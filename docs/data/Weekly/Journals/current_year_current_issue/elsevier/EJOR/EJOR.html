<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EJOR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ejor">EJOR - 24</h2>
<ul>
<li><details>
<summary>
(2026). Optimistic and pessimistic approaches for cooperative games. <em>EJOR</em>, <em>328</em>(2), 725-733. (<a href='https://doi.org/10.1016/j.ejor.2025.09.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative game theory explores how to fairly allocate the joint value generated by a group of decision-makers, but its application is compromised by the large number of counterfactuals needed to compute the value of all coalitions, a problem made even more complicated when externalities are present. We provide a theoretical foundation for a simplification used in many applications, in which the value of a coalition is computed assuming that they either select before or after the complement set of agents, providing optimistic and pessimistic values on what a coalition should receive. In a vast set of problems exhibiting what we call feasibility externalities, we show that ensuring a coalition does not receive more than its optimistic value is always at least as difficult as ensuring it receives its pessimistic value. Furthermore, under the presence of negative externalities, we establish the existence of stable allocations that respect these bounds. Finally, we examine well-known optimization-based applications and their corresponding cooperative games to show how our results lead to new insights and allow the derivation of further results from the existing literature.},
  archive      = {J_EJOR},
  author       = {Ata Atay and Christian Trudeau},
  doi          = {10.1016/j.ejor.2025.09.002},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {725-733},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimistic and pessimistic approaches for cooperative games},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sustainable product development under profit-sharing crowdfunding: An analytical approach to market structure and government policy. <em>EJOR</em>, <em>328</em>(2), 704-724. (<a href='https://doi.org/10.1016/j.ejor.2025.08.020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops an integrated analytical framework to examine how crowdfunding, market structure, and government policy interact to shape environmentally sustainable product development (SPD). Focusing on profit-sharing and securities-based crowdfunding, we model how investor opportunity costs, risk preferences, platform fees, and regulatory schemes (voluntary vs. mandatory under fiscal vs. non-fiscal policy regimes) influence firm strategies and outcomes across economic, social, and environmental (ESE) dimensions. Firm behavior is analyzed under monopoly and duopoly settings to explore variation in market power and competitive intensity. Findings reveal that voluntary greening can achieve strong ESE outcomes in monopolistic markets with low financial frictions and environmentally aware consumers. In contrast, competitive or uncertain environments often require benchmark-based regulation and fiscal instruments to sustain environmental investments. Two dominant firm profiles emerge: the Voluntary sustainability leader, which performs well under favorable market and investor conditions without policy intervention, and the Policy-driven strategist, which depends on regulatory standards and fiscal tools to overcome competitive pressures and risk constraints. By formalizing investor-entrepreneur interactions and embedding environmental quality as a strategic variable, this research advances the literature on crowdfunding and market-driven sustainability. It provides actionable insights for aligning crowdfunding design and policy frameworks with the broader goals of green innovation and public sustainability.},
  archive      = {J_EJOR},
  author       = {Raziyeh Reza-Gharehbagh and Madeleine Pullman},
  doi          = {10.1016/j.ejor.2025.08.020},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {704-724},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Sustainable product development under profit-sharing crowdfunding: An analytical approach to market structure and government policy},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Optimal dividend and scale of business strategies with reinsurance and premium pricing for insurance company. <em>EJOR</em>, <em>328</em>(2), 694-703. (<a href='https://doi.org/10.1016/j.ejor.2025.07.039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the optimal dividend and business scale strategies aimed at maximizing the value of an insurance company. While prior studies typically assume that insurers can only adjust their business scale through reinsurance, this study extends the framework by allowing the insurer to control the premium rate. Under more realistic market assumptions, we examine the joint optimization problem for two common types of reinsurance — proportional and excess-of-loss — across both arbitrage and non-arbitrage scenarios. We derive the optimal strategies for dividends and premium pricing, along with their corresponding value functions. The results show that the insurer should decrease the premium rate and reduce reinsurance coverage as the surplus increases. The optimal dividend policy follows a barrier strategy. Economic interpretations and numerical examples are provided to illustrate the findings.},
  archive      = {J_EJOR},
  author       = {Dingjun Yao and Bo Yang and Xin Xu and Youwei Li and Yizhi Wang},
  doi          = {10.1016/j.ejor.2025.07.039},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {694-703},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal dividend and scale of business strategies with reinsurance and premium pricing for insurance company},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Anticipating delays in recruitment: Explainable machine learning for the prediction of hard-to-fill online job vacancies. <em>EJOR</em>, <em>328</em>(2), 680-693. (<a href='https://doi.org/10.1016/j.ejor.2025.06.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online job vacancy (OJV) platforms have transformed the labor market by enabling employers to advertise jobs to a wide audience. Particularly in tight labor markets, quickly identifying vacancies likely to suffer prolonged durations is crucial. This study utilizes data from the Flemish public employment service's OJV platform to examine the effectiveness of machine learning in predicting hard-to-fill vacancies. We achieve notable predictive performance with XGBoost in forecasting recruitment delays and demonstrate the importance of capturing non-linear patterns in OJV data. SHAP (SHapley Additive exPlanations) values reveal that the textual content of vacancies and latent company characteristics are key predictors of hiring delays. Counterfactual-SHAP insights provide practical guidance for refining recruitment strategies, enhancing labor market forecasts, and informing targeted policies.},
  archive      = {J_EJOR},
  author       = {Wouter Dossche and Sarah Vansteenkiste and Bart Baesens and Wilfried Lemahieu},
  doi          = {10.1016/j.ejor.2025.06.027},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {680-693},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Anticipating delays in recruitment: Explainable machine learning for the prediction of hard-to-fill online job vacancies},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Using diet optimization and machine learning for the design of healthy and acceptable menu plans. <em>EJOR</em>, <em>328</em>(2), 668-679. (<a href='https://doi.org/10.1016/j.ejor.2025.06.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of dietary plans relies on understanding and modelling consumer acceptance, yet quantifying this poses a challenge due to the complexity of individual preferences. Recent research is focused on deriving acceptability constraints directly from data, as demonstrated by its application in designing food baskets with a limited number of commodities. In this study, we applied diet optimization with machine learning to the more complex task of menu planning. This involved considering hundreds of potential food alternatives and assessing their compatibility within a meal using a recipe completion algorithm. Compared to the traditional diet modelling approach of food group filtering, the recipe completion model delivered diets with either higher nutritional adequacy or greater substitute acceptability, depending on the number of food groups used in the traditional method. While more research is needed to further improve the acceptability of substitutions, combining diet optimization with recipe completion presents a promising approach to enhance the nutritional adequacy of individual diets while maintaining the acceptability of food combinations within meals.},
  archive      = {J_EJOR},
  author       = {Dominique van Wonderen and Johanna C. Gerdessen and Alida Melse-Boonstra and Marleen C. Onwezen},
  doi          = {10.1016/j.ejor.2025.06.015},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {668-679},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Using diet optimization and machine learning for the design of healthy and acceptable menu plans},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Project monitoring and control with an empirically grounded budget-release model. <em>EJOR</em>, <em>328</em>(2), 646-667. (<a href='https://doi.org/10.1016/j.ejor.2025.06.007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Project monitoring and control (PMC) is a process of measuring a project’s progress and taking corrective action when necessary to ensure successful project completion. However, most existing models lack empirical validation of their assumptions and effectiveness, which limits their practical use. We fill in this gap by using empirical data from 97 real projects to calibrate activity-duration distributions and assess activity-duration dependencies, integrating these empirical foundations into an enhanced PMC model. We further improve the model by incorporating budget-release timing constraints and introducing two new policies for crashing and fast-tracking based on a project’s specific time and cost characteristics. Extensive computational experiments using empirical and artificial data evaluate the effectiveness of these policies. Because the budget-release policies and corrective action types depend on project characteristics such as activity-duration dependencies and topological network structure, key insights from this study can be usefully applied by project managers as heuristics even without a detailed model of their project.},
  archive      = {J_EJOR},
  author       = {Jie Song and Jinbo Song and Tyson Browning and Mario Vanhoucke},
  doi          = {10.1016/j.ejor.2025.06.007},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {646-667},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Project monitoring and control with an empirically grounded budget-release model},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). The continuous roll-on roll-off dual cycling problem with tugs and driver-handled cargo units. <em>EJOR</em>, <em>328</em>(2), 633-645. (<a href='https://doi.org/10.1016/j.ejor.2025.05.050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Roll-on roll-off vessels are a popular mode of transport in short-sea shipping. In this domain, appropriate stevedoring procedures are crucial to enhance efficiency. This includes dual cycling, where tugs simultaneously load and unload the vessel. Dual cycling reduces turnaround time, thereby giving the vessel more time to travel and allowing for slow steaming and reduced emissions. We extend the roll-on roll-off dual cycling problem by incorporating a continuous time horizon and differentiating cargo units that are handled by their own drivers and units that must be handled by a tug. We propose a mixed integer linear programming model for generating an efficient schedule that minimizes overall makespan by optimizing the sequence of cargo units and the assignment of cargo units to tugs. To solve instances of real-world size with acceptable computational effort, we provide a range of heuristics, including a biased random-key genetic algorithm. Compared to the linear programming model and on instances of real-world size, the genetic algorithm finds good solutions quickly. We derive managerial insights from a sensitivity analysis and show that dual cycling and strategic positioning of driver-handled units can reduce turnaround time by 14.5%, reducing emissions of the considered vessel by more than 8%. We demonstrate the robustness of these insights in uncertain environments through a simulation study.},
  archive      = {J_EJOR},
  author       = {Teresa Marquardt and Arne Heinold and Catherine Cleophas and Frank Meisel},
  doi          = {10.1016/j.ejor.2025.05.050},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {633-645},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The continuous roll-on roll-off dual cycling problem with tugs and driver-handled cargo units},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Optimizing treatment allocation in the presence of interference. <em>EJOR</em>, <em>328</em>(2), 620-632. (<a href='https://doi.org/10.1016/j.ejor.2025.09.015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Influence Maximization (IM), the objective is to — given a budget — select the optimal set of entities in a network to target with a treatment so as to maximize the total effect. For instance, in marketing, the objective is to target the set of customers that maximizes the total response rate, resulting from both direct treatment effects on targeted customers and indirect, spillover, effects that follow from targeting these customers. Recently, new methods to estimate treatment effects in the presence of network interference have been proposed. However, the issue of how to leverage these models to make better treatment allocation decisions has been largely overlooked. Traditionally, in Uplift Modeling (UM), entities are ranked according to estimated treatment effect, and the top entities are allocated treatment. Since, in a network context, entities influence each other, the UM ranking approach will be suboptimal. The problem of finding the optimal treatment allocation in a network setting is NP-hard, and generally has to be solved heuristically. To fill the gap between IM and UM, we propose OTAPI: Optimizing Treatment Allocation in the Presence of Interference to find solutions to the IM problem using treatment effect estimates. OTAPI consists of two steps. First, a causal estimator is trained to predict treatment effects in a network setting. Second, this estimator is leveraged to identify an optimal treatment allocation by integrating it into classic IM algorithms. We demonstrate that this novel method outperforms classic IM and UM approaches on both synthetic and semi-synthetic datasets.},
  archive      = {J_EJOR},
  author       = {Daan Caljon and Jente Van Belle and Jeroen Berrevoets and Wouter Verbeke},
  doi          = {10.1016/j.ejor.2025.09.015},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {620-632},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimizing treatment allocation in the presence of interference},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Soft regression trees: A model variant and a decomposition training algorithm. <em>EJOR</em>, <em>328</em>(2), 607-619. (<a href='https://doi.org/10.1016/j.ejor.2025.08.050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are widely used for classification and regression tasks in a variety of application fields due to their interpretability and good accuracy. During the past decade, growing attention has been devoted to globally optimized decision trees with deterministic or soft splitting rules at branch nodes, which are trained by optimizing the error function over all the tree parameters. In this work, we propose a new variant of soft multivariate regression trees (SRTs) where, for every input vector, the prediction is defined as the linear regression associated to a single leaf node, namely, the leaf node obtained by routing the input vector from the root along the branches with higher probability. SRTs exhibit the conditional computational property, i.e., each prediction depends on a small number of nodes (parameters), and our nonlinear optimization formulation for training them is amenable to decomposition. After showing a universal approximation result for SRTs, we present a decomposition training algorithm including a clustering-based initialization procedure and a heuristic for rerouting the input vectors along the tree. Under mild assumptions, we establish asymptotic convergence guarantees. Experiments on 15 well-known datasets indicate that our SRTs and decomposition algorithm yield higher accuracy and robustness compared with traditional soft regression trees trained using the nonlinear optimization formulation of Blanquero et al. (2021), and a significant reduction in training times as well as a slightly better average accuracy compared with the mixed-integer optimization approach of Bertsimas and Dunn (2019). We also report a comparison with the Random Forest ensemble method.},
  archive      = {J_EJOR},
  author       = {Antonio Consolo and Edoardo Amaldi and Andrea Manno},
  doi          = {10.1016/j.ejor.2025.08.050},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {607-619},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Soft regression trees: A model variant and a decomposition training algorithm},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An explainable machine learning framework for recurrent event data analysis. <em>EJOR</em>, <em>328</em>(2), 591-606. (<a href='https://doi.org/10.1016/j.ejor.2025.09.005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel explainable temporal point process (TPP) model, Stratified Hawkes Point Process (SHPP), for modelling recurrent event data (RED). Unlike existing approaches that treat temporal influence as a black box or rely on post-hoc explanations, SHPP structurally decomposes event intensities into semantically meaningful components for describing self-, Markovian, and joint influences. This decomposition enables direct quantification of how past events contribute to future event risks, termed as influence values. We further provide a sufficient condition for mean-square stability based on kernel decay, ensuring long-term boundedness of intensities and realistic behavioural predictions. Experiments and an e-commerce case study demonstrate SHPP’s ability to deliver accurate, interpretable, and stable modelling of complex event-driven systems.},
  archive      = {J_EJOR},
  author       = {Qi Lyu and Shaomin Wu},
  doi          = {10.1016/j.ejor.2025.09.005},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {591-606},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An explainable machine learning framework for recurrent event data analysis},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Decision space dynamic niching-based method for constrained multiobjective evolutionary optimization. <em>EJOR</em>, <em>328</em>(2), 574-590. (<a href='https://doi.org/10.1016/j.ejor.2025.07.002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a set with a good approximation to the Pareto-optimal solutions in the multiobjective optimization problem (MOP) is a challenging task in terms of convergence toward and diversity across the Pareto optimal front (PoF). In some cases, solving MOPs requires satisfying certain constraints, which significantly increases the complexity of the problem. Such problems are constrained multiobjective optimization problems (CMOPs) and pose considerable computational challenges. Many constrained multiobjective evolutionary algorithms (CMOEAs) face challenges in avoiding becoming trapped in local optima, which impacts convergence, and offer solutions that lack good coverage of the PoF, implying weak diversity. All these nonoptimal or partially optimal solutions in the objective space are essentially clustered in local optimality dilemmas in the decision space. To better eliminate the convergence and diversity challenges caused by clustered solutions, this paper proposes a decision space dynamic niching-based (DSDN) method to better address CMOPs. Specifically, the DSDN method adds a dynamic decision space niche as an additional criterion to the traditional Pareto-constrained dominance principle (Pareto-CDP). The better preserved solutions must satisfy the Pareto-CDP and the condition within the niche radius of other solutions, which strictly meets the original dominance relationship requirement while relaxing the nondominance threshold. As a result, the dynamic adjustment of the niche radius ( N R ) effectively balances the exploitation and exploration of solutions in the decision space while enhancing both convergence and diversity in the objective space. Experiments conducted on four widely recognized test suites and three real-world case studies have demonstrated that the DSDN method yields significantly better results than the original Pareto-CDP algorithms. Furthermore, the proposed approach is competitive with or comparable to seven other state-of-the-art CMOEAs.},
  archive      = {J_EJOR},
  author       = {Fan Yu and Qun Chen and Jinlong Zhou},
  doi          = {10.1016/j.ejor.2025.07.002},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {574-590},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Decision space dynamic niching-based method for constrained multiobjective evolutionary optimization},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Impact of technology spillovers and subsidies on innovation consortia dynamics. <em>EJOR</em>, <em>328</em>(2), 560-573. (<a href='https://doi.org/10.1016/j.ejor.2025.06.031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In innovation consortia, governments and enterprises view technology spillovers and subsidies differently, which reduces the effectiveness of policies and undermines cooperation. This work investigates the dynamics of cooperation among leading enterprises (LEs) and small/medium enterprises (SMEs), using an evolutionary game model with intrinsic, extrinsic and mixed strategies. We show that there may exist more than one evolutionary equilibrium of enterprise cooperation under multi-strategy choices. A high subsidy may induce enterprises to speculate and reach a suboptimal evolutionary equilibrium. Furthermore, our findings indicate that SMEs are particularly sensitive to changes in subsidy levels, income distribution ratios, and project income, with their speculative behavior exhibiting a lagging effect. Also, increasing the income distribution ratio of LEs can promote active cooperation between the groups and reduce the speculative tendency of SMEs. This occurs because it magnifies the effect of technology spillover on the income of SMEs and enhances the income of the leading enterprises. Our work improves understanding of cooperative factors and provides new recommendations for government and leading enterprises to improve cooperation and managerial performance.},
  archive      = {J_EJOR},
  author       = {Zheng Yang and Lin Li and Nicholas G. Hall and Yongzeng Lai and Yijiang Zhou},
  doi          = {10.1016/j.ejor.2025.06.031},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {560-573},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Impact of technology spillovers and subsidies on innovation consortia dynamics},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Online quality endorsement to improve consumer trust: Blockchain or self-hosted livestream?. <em>EJOR</em>, <em>328</em>(2), 545-559. (<a href='https://doi.org/10.1016/j.ejor.2025.04.010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online products have long suffered from consumer distrust, putting them at a disadvantage when online brands compete with offline brands. To address this issue, many online brands have adopted blockchain as a means of quality endorsement to improve consumer trust. Alternatively, livestream e-commerce has shown the capability of both quality endorsement and demand creation by real-time interactions with consumers because the application of AR/VR technology and the use of online sales force can effectively induce the herding mentality. For many small and medium-sized online brands, it remains unclear which approach is better, so we formulate the key tradeoffs in the online brand's choice to improve consumer trust in the presence of offline brand's competition. Our research delves into the influence of three key factors on livestream e-commerce: the cost associated with adopting blockchain technology, the strength of positive network externality, and the potential downside of consumer returns. Contrary to conventional wisdom, we find that when the return cost is high and the network externality in livestream is weak, opting for livestream as a quality endorsement can actually benefit the online brand. We also find that the online brand is capable of mitigating the return cost by transferring it to consumers through charging a high retail price, which increases the likelihood of favoring livestream. Our findings shed light on the building and improvement of online consumer trust, contributing to the high-quality development of online-offline business in the new era of consumption.},
  archive      = {J_EJOR},
  author       = {Baozhuang Niu and Jian Dong and Xinhu Yu and Yulan Wang},
  doi          = {10.1016/j.ejor.2025.04.010},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {545-559},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Online quality endorsement to improve consumer trust: Blockchain or self-hosted livestream?},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Production trade-offs in free disposal hull technologies. <em>EJOR</em>, <em>328</em>(2), 530-544. (<a href='https://doi.org/10.1016/j.ejor.2025.06.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data envelopment analysis, production trade-offs are value judgements that represent simultaneous changes to the inputs and outputs assumed to be technologically possible for any production unit in the technology. The specification of production trade-offs generally leads to an enlargement of the model of technology and increasing its discriminating power on efficiency. In conventional convex variable and constant returns-to-scale models, production trade-offs are the dual forms of weight restrictions. In this paper, we extend the use of production trade-offs to the free disposal hull model of technology and its constant, non-increasing and non-decreasing returns-to-scale variants, in a single unifying development. We provide an axiomatic definition of the new nonconvex technologies, explore the notion of consistent trade-offs in such technologies and develop methods for its testing. We further develop different computational approaches for nonconvex models with production trade-offs. We illustrate the new models by an application in the context of higher education.},
  archive      = {J_EJOR},
  author       = {Mahmood Mehdiloo and Grammatoula Papaioannou and Victor V. Podinovski},
  doi          = {10.1016/j.ejor.2025.06.032},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {530-544},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Production trade-offs in free disposal hull technologies},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Price optimization for round trip car sharing. <em>EJOR</em>, <em>328</em>(2), 511-529. (<a href='https://doi.org/10.1016/j.ejor.2025.06.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Car sharing, car clubs and short-term rentals could support the transition toward net zero but their success depends on them being financially sustainable for service providers and attractive to end users. Dynamic pricing could support this by incentivizing users while balancing supply and demand. We describe the usage of a round trip car sharing fleet by a continuous time Markov chain model, which reduces to a multi-server queuing model where hire duration is assumed independent of the hourly rental price. We present analytical and simulation optimization models that allow the development of dynamic pricing strategies for round trip car sharing systems; in particular identifying the optimal hourly rental price. The analytical tractability of the queuing model enables fast optimization to maximize expected hourly revenue for either a single fare system or a system where the fare depends on the number of cars on hire, while accounting for stochasticity in customer arrival times and durations of hire. Simulation optimization is used to optimize prices where the fare depends on the time of day or hire duration depends on price. We present optimal prices for a given customer population and show how the expected revenue and car availability depend on the customer arrival rate, willingness-to-pay distribution, dependence of the hire duration on price, and size of the customer population. The results provide optimal strategies for pricing of car sharing and inform strategic managerial decisions such as whether to use time- or state-dependent pricing and optimizing the fleet size.},
  archive      = {J_EJOR},
  author       = {Christine S.M. Currie and Rym M’Hallah and Beatriz Brito Oliveira},
  doi          = {10.1016/j.ejor.2025.06.024},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {511-529},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Price optimization for round trip car sharing},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). The development strategy of supply chain intelligent technology considering technology development uncertainty. <em>EJOR</em>, <em>328</em>(2), 496-510. (<a href='https://doi.org/10.1016/j.ejor.2025.07.026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It’s crucial for both manufacturing and logistics industries to improve logistics efficiency and reduce logistics loss during transportation, storage, and other processes through the development and application of intelligent technology. By focusing on three potential modes of intelligent technology development cooperation between a manufacturer and its logistics provider, we examine the impact of such collaboration on reducing Logistics loss, as well as explore the optimal mode of cooperation for both firms. Our analytical results indicate that compared to independent technology development, collaborative development of intelligent technology can mitigate the adverse effects of double-marginalization. Comparing the three modes of cooperation, we find that higher development cost can incentivize the collaboration between two firms, while higher integration cost and price elasticity may make the cost sharing mode preferable. It is noteworthy that the uncertainty of intelligent technology development exerts a significant moderating effect on the choice of cooperation mode. Heightened technology development uncertainty tends to incentivize both firms to pursue joint development in order to alleviate the negative impact of the uncertainty.},
  archive      = {J_EJOR},
  author       = {Peng Han and Yanfang Huo and Weihua Liu and Ershi Qi and Helen Cai},
  doi          = {10.1016/j.ejor.2025.07.026},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {496-510},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The development strategy of supply chain intelligent technology considering technology development uncertainty},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Optimal price, quantity, and return policy decisions of a two-period newsvendor with product reviews. <em>EJOR</em>, <em>328</em>(2), 477-495. (<a href='https://doi.org/10.1016/j.ejor.2025.06.023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online retailers (e-tailers) have high product return rates. To decrease product returns and consumers’ purchase risk, e-tailers can increase available product information through product reviews and/or offer money back guarantees (MBGs). We examine the rational expectations equilibrium of an e-tailer selling to myopic and strategic consumers over two periods. By the end of period 1, product reviews become available, providing a signal to strategic consumers who may wait to purchase in period 2. The e-tailer decides order quantity, prices, and return policy. We find that under both no returns and MBG, as strategic consumers’ patience increases, the optimal range of having them purchase in period 2 increases. For low signal accuracy, it is optimal to have strategic consumers purchase in period 1, whereas for high signal accuracy, it is optimal to have them purchase in period 2. However, under no returns and for a low signal accuracy, it may be optimal to have strategic consumers purchase in period 2 if their proportion is medium and they are patient. We also find that as consumers’ patience increases, the range where MBG dominates no returns increases. For heterogeneous signal accuracy among strategic consumers, equilibrium strategies are similar to the homogeneous case, except that when consumers’ patience is low, high heterogeneity allows the e-tailer to price discriminate, making low-signal accuracy consumers purchase in period 1 and high-signal accuracy consumers purchase in period 2. Also, as the proportion of low-signal accuracy consumers increases, price discrimination increases. Therefore, heterogeneity increases profit.},
  archive      = {J_EJOR},
  author       = {Huirong Fan and Moutaz Khouja and Jing Zhou},
  doi          = {10.1016/j.ejor.2025.06.023},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {477-495},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal price, quantity, and return policy decisions of a two-period newsvendor with product reviews},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Probabilistic forecast aggregation with statistical depth. <em>EJOR</em>, <em>328</em>(2), 460-476. (<a href='https://doi.org/10.1016/j.ejor.2025.06.028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers aggregation methods for interval forecasts and forecasts of cumulative distribution functions (CDFs) when there are many forecasters, and past forecast accuracy may not be known. For aggregation, the median and trimmed means have been proposed as simple and robust alternatives to the mean, with some trimmed mean approaches enabling recalibration to widen or narrow the resulting interval or CDF forecast. For interval forecast aggregation, the median and trimming are applied to each bound separately. To try to use the available information better, we treat the bounds as a bivariate point with statistical depth used to order the points in terms of centrality. The deepest point can be viewed as the median interval forecast, and the depth of each point can be used as the basis for trimming. For CDF forecasts, the literature presents aggregation methods for which the median or trimmed mean are obtained for each point on the domain of the distribution. However, if one part of a CDF forecast is outlying, the appeal of using the rest of the CDF forecast is perhaps reduced. We use functional depth to provide a measure of centrality for each CDF forecast, and hence identify the deepest function, which can be viewed as the median forecast. We also use functional depth as the basis for trimming, and consider weighted depth to control the width of the resulting aggregated interval or CDF forecast. We provide empirical illustration using data from surveys of professional macroeconomic forecasters, and an application to growth-at-risk.},
  archive      = {J_EJOR},
  author       = {James W. Taylor},
  doi          = {10.1016/j.ejor.2025.06.028},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {460-476},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Probabilistic forecast aggregation with statistical depth},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A data-driven approach for strategic inventory placement in multi-echelon supply networks. <em>EJOR</em>, <em>328</em>(2), 446-459. (<a href='https://doi.org/10.1016/j.ejor.2025.06.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a data-driven solution for optimizing inventory buffers in large-scale supply networks. We study the placement and sizing of strategic inventories in multi-echelon supply chains where the decision maker faces uncertain demand with an unknown distribution influenced by explanatory variables. State-of-the-art multi-echelon inventory optimization models, such as the well-known guaranteed-service model (GSM), are non-linear and typically informed by distributional and parametric assumptions. They often rely on dynamic programming and are difficult to solve for large networks. We adapt the GSM to introduce a nonparametric, feature-driven approach to supply chain safety stock optimization that is based on mixed-integer linear programming (MILP). The MILP formulation sets cost-optimal base stocks, which are learned as linear functions of feature data under consideration of service level requirements. This integrated estimation and optimization approach is solved with commercial mathematical programming solvers and is enhanced by a Benders decomposition method for large networks. We extend the literature on data-driven inventory control by a multi-period and multi-echelon approach for safety stock planning in general, acyclic networks. On the real-world networks from Willems (2008), we find that incorporating feature information when setting safety stocks in large supply chains, on average, reduces operational costs out-of-sample. This value of feature information that the proposed model offers to decision-makers increases in demand volatility and is dependent on certain network characteristics.},
  archive      = {J_EJOR},
  author       = {Josef Svoboda and Stefan Minner},
  doi          = {10.1016/j.ejor.2025.06.022},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {446-459},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A data-driven approach for strategic inventory placement in multi-echelon supply networks},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Minimizing the maximum flow loss in the network maintenance scheduling problem with flexible arc outages. <em>EJOR</em>, <em>328</em>(2), 430-445. (<a href='https://doi.org/10.1016/j.ejor.2025.07.056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a network maintenance scheduling problem where maintenance tasks are carried out on the arcs of a network within flexible time windows. During maintenance, an arc is interrupted and no flow can pass through it. Such arc outages introduce flow loss and thus affect network capacity and service capability. For some public services operated on networks, possible blackouts and serious flow loss introducing extreme risks are generally unacceptable. The problem is to find a feasible schedule of maintenance tasks so that the maximum flow loss during the planning horizon is minimized. We introduce a mixed integer programming formulation and a Benders reformulation for the problem. A Benders decomposition algorithm based on a branch-and-cut framework is designed. Strengthened initial cuts and effective cuts are introduced to reduce feasible region so that the exact algorithm is accelerated. An efficient separation procedure is proposed to generate Benders optimality cuts. Computational experiments were conducted on a set of benchmark instances and a set of simulated instances based on telecommunication networks. Computational results show that our algorithm performs much better than applying a solver to the formulation and an existing Benders decomposition algorithm for a related problem. Optimal schedules can reduce extreme risks caused by large flow loss on the network. Since multiple optimal solutions may exist, hierarchical optimization is used to further select a desirable schedule, by either minimizing total flow loss or minimizing the duration of maximum flow loss. With adaptations, our algorithm also performs well for two extensions.},
  archive      = {J_EJOR},
  author       = {Shuang Jin and Ying Liu and Jing Zhou and Qian Hu},
  doi          = {10.1016/j.ejor.2025.07.056},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {430-445},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Minimizing the maximum flow loss in the network maintenance scheduling problem with flexible arc outages},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Stochastic quay partitioning problem. <em>EJOR</em>, <em>328</em>(2), 415-429. (<a href='https://doi.org/10.1016/j.ejor.2025.07.043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider the problem of dividing a quay of a container terminal into berth segments so that the quality of service for future ship arrivals is as good as possible. Since future arrivals are unknown, the alternative solutions are evaluated on various arrival scenarios generated for certain arrival intensity from a stochastic model referred to as a ship traffic model (STM). This problem will be referred to as a stochastic quay partitioning problem (SQPP). SQPP is defined by an STM, arrival intensity, quay length and a set of admissible berth lengths. Evaluation of an SQPP solution on one scenario is a problem of scheduling the arriving vessels on the berths, which is a classic berth allocation problem (BAP). In SQPP the sizes of BAP instances that must be solved by far exceed capabilities of the methods presented in the existing literature. Therefore, a novel approach to solving BAP is applied. Tailored portfolios of algorithms capable of solving very large BAP instances under limited runtime are used. Features of SQPP solutions are studied experimentally: patterns in selected berth lengths, dispersion of solutions quality and solutions similarity. We demonstrate, that partitioning a quay into equal-length berths is not the best approach. The largest vessel traffic is dominating in defining best quay partitions, but dedicating quays for shorter vessels give lower dispersion of solution quality. A set of algorithms to partition a quay is proposed and evaluated: methods based on integer linear programming (ILP) to match vessel classes arrival intensities with berth availability, hill climber, tabu search and a greedy approach. Only under high arrival intensity can these methods show their prowess. ILP methods have an advantage of low solution evaluation cost. Tabu is most flexible, but at high evaluation costs. To the best of our knowledge, SQPP is posed and solved for the first time in the operations research.},
  archive      = {J_EJOR},
  author       = {Jakub Wawrzyniak and Maciej Drozdowski and Éric Sanlaville},
  doi          = {10.1016/j.ejor.2025.07.043},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {415-429},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Stochastic quay partitioning problem},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Scheduling mixed batch machines with inclusive processing set restrictions and non-identical capacities. <em>EJOR</em>, <em>328</em>(2), 407-414. (<a href='https://doi.org/10.1016/j.ejor.2025.07.012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new batch scheduling problem, name mixed batch scheduling problem, is received attentions recently. In a mixed batch scheduling model, the processing time of a job batch H is defined as α max j ∈ H { p j } + ( 1 − α ) ∑ j ∈ H p j , where α ∈ [ 0 , 1 ] is a constant. In other words, the processing time of a job batch is the weighted sum of the maximum processing time and the total processing time of jobs in the batch. In this paper, we study the problem of scheduling mixed batch machines with non-identical capacities under inclusive processing set restrictions, where the objective is to minimize the makespan of finishing all the jobs. We present a fast approximation algorithm with a performance ratio of 4 / 3 + α for the problem, which improves up the existing performance bounds in the literature. By providing a technical lemma, we are able to develop the first polynomial time approximation scheme (PTAS) for the problem. We also design linear-time approximation schemes for two important special cases of the problem.},
  archive      = {J_EJOR},
  author       = {Jinwen Ou and Weidong Li},
  doi          = {10.1016/j.ejor.2025.07.012},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {407-414},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Scheduling mixed batch machines with inclusive processing set restrictions and non-identical capacities},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A matheuristic approach for the robust coloured travelling salesman problem with multiple depots. <em>EJOR</em>, <em>328</em>(2), 390-406. (<a href='https://doi.org/10.1016/j.ejor.2025.06.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a special type of the travelling salesman problem (TSP) called the coloured TSP (CTSP) is considered. The CTSP, which has many real-world applications, involves a set of salesmen, each assigned a specific colour, and cities that may have one or multiple colours. Salesmen are restricted to visiting only cities that share their colour. We consider a specific depot for each salesman, and the edge weights are uncertain, meaning that there is a set of possible scenarios for their values. A robust objective is considered and minimised using an artificial intelligence (AI)-driven matheuristic approach due to the high computational complexity of the problem. This approach integrates a variable neighbourhood search (VNS) framework with genetic algorithm (GA) and simulated annealing (SA) operators. More importantly, local improvements based on mathematical programming are applied to different parts of a proportion of the solutions using the concept of partial optimisation metaheuristic under special intensification conditions (POPMUSIC). A key innovation of our method is the use of an artificial neural network to guide the POPMUSIC procedure by selecting only solution segments with high improvement potential, thereby reducing computation time. Extensive computational experiments demonstrate the effectiveness of the proposed algorithm, which outperforms four state-of-the-art methods in solution quality and runs faster than three of them. We also investigate the contribution of individual algorithmic components and the cost of robustness. Furthermore, our method improves upon the best-known results for the single-depot deterministic version of the CTSP from the literature.},
  archive      = {J_EJOR},
  author       = {Abtin Nourmohammadzadeh and Stefan Voß},
  doi          = {10.1016/j.ejor.2025.06.018},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {390-406},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A matheuristic approach for the robust coloured travelling salesman problem with multiple depots},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fifty years of research on resource-constrained project scheduling explored from different perspectives. <em>EJOR</em>, <em>328</em>(2), 367-389. (<a href='https://doi.org/10.1016/j.ejor.2025.03.024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The resource-constrained project scheduling problem is one of the most investigated problems in the project scheduling literature, and has a rich history. This article provides a perspective on this challenging scheduling problem, without having the ambition to provide a complete overview. Instead, the article does aim to summarize a number of reasons why this problem has been so intensely investigated from different perspectives. It will be shown that this scheduling problem has many faces, and therefore deserves a lot of research time from a computational and theoretical point of view as well as from a practical point of view. An overview of possible extensions to other problems and a detailed overview of the used (both heuristic and exact) solution methods will be given. In addition, the data used will be discussed and interesting avenues for further research will be mentioned throughout the different sections.},
  archive      = {J_EJOR},
  author       = {Christian Artigues and Sönke Hartmann and Mario Vanhoucke},
  doi          = {10.1016/j.ejor.2025.03.024},
  journal      = {European Journal of Operational Research},
  month        = {1},
  number       = {2},
  pages        = {367-389},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of research on resource-constrained project scheduling explored from different perspectives},
  volume       = {328},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
