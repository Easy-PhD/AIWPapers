<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EJOR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ejor">EJOR - 21</h2>
<ul>
<li><details>
<summary>
(2025). Optimal lot-sizing and service level weighting in sequential multi-attribute global transportation service procurement. <em>EJOR</em>, <em>327</em>(3), 1052-1072. (<a href='https://doi.org/10.1016/j.ejor.2025.05.037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of on-demand global transportation service procurement (oGTSP) through digital trading platforms has accelerated due to frequent fluctuations in transport capacity. In the oGTSP model, the exporter must consider logistics service quality and transport prices when sourcing global logistics services. To satisfy the continuous transport needs, procurement is conducted sequentially throughout multiple auction cycles. For a single auction, we constructed a service-level weight-scoring function and analysed the trading parties’ behavioural strategies to obtain an auction equilibrium strategy in a specific context. Then, we developed a multi-cycle sequential decision method based on a single-cycle equilibrium decision by forwarders that can dynamically adjust the auction lot size to help the exporter obtain optimal utility. Finally, based on the real case of a large electronic product exporter, the proposed approach was verified. The results demonstrated that exporters should pay more attention to the quality of service when choosing freight forwarders to improve the utility of transportation service procurement. The exporter can attract more forwarders to participate in auctions to obtain more capacity supply by increasing the weighting of service levels. Besides, the proposed auction system could effectively accommodate strategic forwarders with learning abilities. The exporter’s utility will significantly improve if the freight forwarders have learning ability. There is a marginal diminishing effect in that the benefits from additional participation of learning-oriented bidders are initially large but eventually stabilized. The strategic auction participation of learning-oriented freight forwarders smooths the capacity supply trend, reduces extreme fluctuations and makes multi-cycle predictions more accurate.},
  archive      = {J_EJOR},
  author       = {Xiang T.R. Kong and Zhan He and Kaize Yu and Pengyu Yan},
  doi          = {10.1016/j.ejor.2025.05.037},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {1052-1072},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal lot-sizing and service level weighting in sequential multi-attribute global transportation service procurement},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Government’s optimal inter-temporal subsidy and manufacturer’s dynamic pricing in the presence of strategic consumers. <em>EJOR</em>, <em>327</em>(3), 1039-1051. (<a href='https://doi.org/10.1016/j.ejor.2025.05.027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Governments in many countries offer fiscal incentives—such as subsidies or tax breaks—to consumers to encourage the purchase of environmentally-friendly products like solar panels and electric vehicles. Early adoption by consumers facilitates manufacturers’ learning-by-doing and reduces production cost over time, although the cost reduction itself is subject to uncertainty. Governments face a challenge: should they commit to a multi-period subsidy path (commitment policy) or adjust the subsidy contingent on the realized production cost reduction (dynamic policy)? What are the implications for manufacturers and consumers? We consider a two-period monopoly setting to study these policies. Given the subsidy policy, the manufacturer sets its prices, whereas consumers strategically decide when to purchase the product, if at all. Naturally, the two policies result in different subsidy paths. We find that products with higher initial unit cost (implying higher prices) do not deserve higher subsidies. Our key result is that governments, who seek to maximize expected social welfare, should adopt the dynamic policy. Insightfully, the four components of social welfare—consumer surplus, manufacturer’s profit, environmental benefit and subsidy expenditure—may all be realized higher under the commitment policy than under the dynamic policy when the realized cost reduction falls short of its expected value. This is because the second-period effective price (price minus subsidy) is more sensitive to cost uncertainty under the dynamic policy. Nevertheless, the dominance of the dynamic policy persists also when considering the realized social welfare. We study several extensions demonstrating the robustness of our results, while highlighting certain exceptions.},
  archive      = {J_EJOR},
  author       = {Weichun Chen and Benny Mantin and Bo Li},
  doi          = {10.1016/j.ejor.2025.05.027},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {1039-1051},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Government’s optimal inter-temporal subsidy and manufacturer’s dynamic pricing in the presence of strategic consumers},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated model for predictive maintenance and inventory management under a reliability chance constraint. <em>EJOR</em>, <em>327</em>(3), 1023-1038. (<a href='https://doi.org/10.1016/j.ejor.2025.05.018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new model that integrates opportunistic maintenance and routine maintenance to enhance the effectiveness of predictive maintenance and inventory management in complex manufacturing systems subject to a reliability chance constraint. It considers both hard and soft failure modes and their mutual dependence. When a machine experiences a hard failure, an opportunistic maintenance policy is utilized on the machine’s components. When the soft failure degradation level of a machine component surpasses a threshold, imperfect preventive maintenance or replacement maintenance is carried out. The choice of component supplier, including OEM and aftermarket suppliers, significantly impacts the joint decision model. To improve the model’s realism and applicability, a random variable representing supplier availability intervals is introduced, reflecting a more nuanced understanding of supply chain dynamics. We develop a simulation optimization method to determine the degradation thresholds for opportunistic and regular maintenance, the component inventory policy, and supplier selection. The objective is to minimize the total maintenance and inventory cost, while ensuring a high level of system reliability. The proposed algorithm effectively addresses the system reliability chance constraint by formulating a surrogate model of the quantile of system downtime. A numerical study is conducted to verify the efficacy of the proposed model and to demonstrate the efficiency of the solution method in finding the optimal feasible solution. Furthermore, the influence of critical factors in the model on the optimal policy is analyzed to derive useful managerial insights.},
  archive      = {J_EJOR},
  author       = {Kuo-Hao Chang and Xin-Pei Wu and Robert Cuckler},
  doi          = {10.1016/j.ejor.2025.05.018},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {1023-1038},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An integrated model for predictive maintenance and inventory management under a reliability chance constraint},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ordinal regression meets online learning: Interactive preference learning for multiple criteria choice and ranking with provable guarantees. <em>EJOR</em>, <em>327</em>(3), 1003-1022. (<a href='https://doi.org/10.1016/j.ejor.2025.05.045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a theoretical and practical bridge between ordinal regression for multiple criteria choice and ranking problems and the framework of sequential prediction, also known as online learning. By reframing the ordinal regression as a sequential prediction task, we study a general class of algorithms that assign probabilities to a sequence of the preferences expressed by the Decision Maker (DM). This approach allows us to evaluate various statistical algorithms on a common basis, providing theoretical guarantees on their regret. To model the likelihood, we employ an additive value function that scores pairwise comparisons given by the DM. We explore two likelihood models: (1) a linear model, which we demonstrate is analogous to sequential investment, and (2) the Bradley–Terry model, widely used in statistics and preference learning. For both models, we establish theoretical bounds for the Bayesian method and the Regularized Maximum Likelihood algorithm (also known as Follow the Regularized Leader). We design Monte Carlo Markov Chain methods based on Metropolis–Hastings and Nested Sampling for efficient approximation of the posterior in Bayesian methods. Extensive empirical testing on synthetic and real-world data shows that our methods outperform the best existing approaches in the literature.},
  archive      = {J_EJOR},
  author       = {Marco Grillo and Wojciech Kotłowski and Miłosz Kadziński},
  doi          = {10.1016/j.ejor.2025.05.045},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {1003-1022},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Ordinal regression meets online learning: Interactive preference learning for multiple criteria choice and ranking with provable guarantees},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alternative ranking in trust network group decision-making: A distributionally robust optimization method. <em>EJOR</em>, <em>327</em>(3), 986-1002. (<a href='https://doi.org/10.1016/j.ejor.2025.05.052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In group decision making problems, preference information can be conveniently and productively used to express the decision-makers’ evaluations over the given set of alternatives. However, the inherent imprecision of preference information may lead to fragile priority weights and unreliable alternative ranking. In this study, we propose a distributionally robust ranking model based on social networks to derive stable priorities, which takes into account the influence of uncertain preference information and the strength of relationships among decision-makers. Specifically, to capture the true data-generating distribution of uncertain parameters, we first develop a distributionally robust ranking model with a moment-based ambiguity set that contains all possible probability distributions over a support set. Then, we verify that the solutions exhibit strong finite-sample performance guarantees. Additionally, the developed model can be reformulated into an equivalent semidefinite programming model. To account for the strength of relationships among decision-makers, we employ propagation efficiency based on Shannon’s theorem, and develop the trust propagation and aggregation operators to obtain decision-makers’ weights. Finally, a numerical experiment is provided, in which the justification and robustness of the distributionally robust ranking model outperform several benchmark models by comparative discussions and robustness analyses.},
  archive      = {J_EJOR},
  author       = {Longlong Shao and Jinpei Liu and Chenyi Fu and Ning Zhu and Huayou Chen},
  doi          = {10.1016/j.ejor.2025.05.052},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {986-1002},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Alternative ranking in trust network group decision-making: A distributionally robust optimization method},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When and should streamers choose high-quality products? effects of streamer types. <em>EJOR</em>, <em>327</em>(3), 971-985. (<a href='https://doi.org/10.1016/j.ejor.2025.05.057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of live-streaming commerce, research has largely focused on manufacturers, leaving streamer decision-making underexplored. This study uses game theory to analyze streamers’ product selection strategies, while also examining how streamer types influence these decisions. The findings reveal that: (a) Streamers do not always prioritize high-quality products. Their choices are shaped by various factors, including product pricing, quality gaps, commission ratios, fan-shoppers’ trust, and sales abilities. High-quality manufacturers are advised to collaborate with knowledge-based streamers, while low-quality manufacturers should partner with entertainment-based streamers. Moderate commission ratios can optimize profits for all parties. (b) For well-known products, knowledge-based streamers with strong sales abilities are more likely to select high-quality items, as they can leverage fan-shoppers' willingness to pay. In contrast, entertainment-based streamers do not exhibit this preference. For unknown products, entertainment-based streamers with strong sales abilities may promote low-quality items, even resorting to deception. Interestingly, entertainment-based streamers with weaker sales abilities may promote high-quality products, while knowledge-based streamers may opt for lower-quality options. (c) When product quality is endogenous, streamers with lower sales abilities should focus on entertainment-based content to attract attention. As their sales abilities improve and fan-shoppers’ trust grows, they should transition to knowledge-based content.},
  archive      = {J_EJOR},
  author       = {Shengyan Cheng and Qiang Guo and Chris K Anderson},
  doi          = {10.1016/j.ejor.2025.05.057},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {971-985},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {When and should streamers choose high-quality products? effects of streamer types},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting two-dimensional projection-efficient units in data envelopment analysis under big data scenarios. <em>EJOR</em>, <em>327</em>(3), 957-970. (<a href='https://doi.org/10.1016/j.ejor.2025.05.053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of big data, traditional estimation methods may struggle to process large datasets efficiently. Ali (1993) laid the foundation for improving efficiency assessment using Data Envelopment Analysis (DEA). Building on this work, we demonstrate how to detect two-dimensional projection-efficient units. This is achieved by projecting the multidimensional DEA production frontier onto two-dimensional subspaces and utilizing slope analysis to identify key efficient units. These units are then linked to their full-dimensional counterparts to define projection-efficient units. We propose using these key efficient units as a preliminary step to speed up the identification of full-dimensional efficient units or to estimate the relative density of datasets. Simulations show that our method reduces computation time for the two fastest approaches by an average of 54.2 % across different datasets.},
  archive      = {J_EJOR},
  author       = {Shuqi Xu and Qingyuan Zhu and Zhiyang Shen and Michael Vardanyan and Yinghao Pan},
  doi          = {10.1016/j.ejor.2025.05.053},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {957-970},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Detecting two-dimensional projection-efficient units in data envelopment analysis under big data scenarios},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deck of cards method for hierarchical, robust and stochastic ordinal regression. <em>EJOR</em>, <em>327</em>(3), 937-956. (<a href='https://doi.org/10.1016/j.ejor.2025.05.025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the recently introduced application of the Deck of Cards Method (DCM) to ordinal regression proposing two extensions related to two main research trends in Multiple Criteria Decision Aiding, namely scaling and ordinal regression generalizations. On the one hand, procedures, different from DCM (e.g. AHP, BWM, MACBETH) to collect and elaborate Decision Maker’s (DM’s) preference information are considered to define an overall evaluation of reference alternatives. On the other hand, Robust Ordinal Regression and Stochastic Multicriteria Acceptability Analysis are used to offer the DM more detailed and realistic decision-support outcomes. More specifically, we consider preference imprecision and indetermination through a set of admissible comprehensive evaluations of alternatives provided by the whole set of value functions compatible with DM’s preference information rather than relying on a single definitive evaluation based on one value function. In addition, we also consider alternatives evaluated on a set of criteria hierarchically structured. The methodology we propose allows the DM to provide precise or imprecise information at different levels of the hierarchy of criteria. Like scaling procedures, the compatible value function we consider can be of a different nature, such as weighted sum, linear or general monotone value function, or Choquet integral. Consequently, the approach we propose is versatile and well-equipped to be adapted to DM’s characteristics and requirements. The applicability of the proposed methodology is shown by a didactic example based on a large ongoing research project in which Italian regions are evaluated on criteria representing Circular Economy, Innovation-Driven Development and Smart Specialization Strategies.},
  archive      = {J_EJOR},
  author       = {Salvatore Corrente and Salvatore Greco and Silvano Zappalà},
  doi          = {10.1016/j.ejor.2025.05.025},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {937-956},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Deck of cards method for hierarchical, robust and stochastic ordinal regression},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum likelihood probability measures over sets: Existence, computation, and convergence. <em>EJOR</em>, <em>327</em>(3), 922-936. (<a href='https://doi.org/10.1016/j.ejor.2025.07.054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider maximum likelihood estimation of a distribution over a general measurable space where realizations of the uncertainty are not directly observable but instead are known to lie within observable sets. We show that maximum likelihood estimates concentrate on a collection of maximal intersections (CMI) and can be found by solving a convex optimization problem whose size is linear in the size of the CMI. We provide an enumerative algorithm to compute the estimates and show that there are estimates that assign positive weight only to T + 1 elements of the CMI ( T being the number of observed sets). Motivated by this, we provide a column generation algorithm to compute the estimates that avoids enumerating the CMI. Under the assumption that either the observed sets are mixed-integer representable, or that the range of the underlying distribution is finite and known, we provide formulations of the algorithms that can be solved with commercial solvers. We study convergence properties of the maximum likelihood estimate both in terms of traditional notions of converge, as well as in terms of Wasserstein distances. Our results show that convergence to the underlying distribution cannot be guaranteed in general, but we identify sufficient conditions for convergence. We also perform numerical experiments that show that the estimates can be computed within minutes, that column generation can significantly reduce computational times, and that there is convergence even in cases where no theoretical guarantees are known.},
  archive      = {J_EJOR},
  author       = {Juan S. Borrero and Denis Sauré},
  doi          = {10.1016/j.ejor.2025.07.054},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {922-936},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Maximum likelihood probability measures over sets: Existence, computation, and convergence},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Worst-case values of target semi-variances with applications to robust portfolio selection. <em>EJOR</em>, <em>327</em>(3), 905-921. (<a href='https://doi.org/10.1016/j.ejor.2025.07.057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expected regret and target semi-variance are two of the most important risk measures for downside risk. When the distribution of a loss is uncertain, and only partial information of the loss is known, their worst-case values play important roles in robust risk management for finance, insurance, and many other fields. Jagannathan (1977) derived the worst-case expected regrets when only the mean and variance of a loss are known and the loss is arbitrary, symmetric, or non-negative. While Chen et al. (2011) obtained the worst-case target semi-variances under similar conditions but focusing on arbitrary losses. In this paper, we first complement the study of Chen et al. (2011) on the worst-case target semi-variances and derive the closed-form expressions for the worst-case target semi-variance when only the mean and variance of a loss are known and the loss is symmetric or non-negative. Then, we investigate worst-case target semi-variances over uncertainty sets that represent undesirable scenarios faced by an investor. Our methods for deriving these worst-case values are different from those used in Jagannathan (1977) and Chen et al. (2011). As applications of the results derived in this paper, we propose robust portfolio selection methods that minimize the worst-case target semi-variance of a portfolio loss over different uncertainty sets. To explore the insights of our robust portfolio selection methods, we conduct numerical experiments with real financial data and compare our portfolio selection methods with several portfolio selection models related to the models proposed in this paper.},
  archive      = {J_EJOR},
  author       = {Jun Cai and Zhanyi Jiao and Tiantian Mao},
  doi          = {10.1016/j.ejor.2025.07.057},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {905-921},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Worst-case values of target semi-variances with applications to robust portfolio selection},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint model for longitudinal and spatio-temporal survival data. <em>EJOR</em>, <em>327</em>(3), 892-904. (<a href='https://doi.org/10.1016/j.ejor.2025.07.060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In credit risk analysis, survival models with fixed and time-varying covariates are commonly used to predict a borrower’s time-to-event. When time-varying covariates are endogenous, jointly modeling their evolution with the event time — known as the joint model for longitudinal and time-to-event data — provides a principled approach. In addition to temporal dynamics, incorporating borrowers’ geographical information can enhance predictive accuracy by capturing spatial clustering and its variation over time. We propose the Spatio-Temporal Joint Model (STJM), a Bayesian hierarchical model that accounts for spatial and temporal effects and their interaction. The STJM captures the impact of unobserved heterogeneity across regions, affecting borrowers residing in the same area at a given time. To ensure scalability to large datasets, we implement the model using the Integrated Nested Laplace Approximation (INLA) framework. We apply the STJM to predict the time to full prepayment on a large dataset of 57,258 US mortgage borrowers with more than 2.5 million observations. Empirical results indicate that including spatial effects consistently improves the performance of the joint model. However, the gains are less definitive when we additionally include spatio-temporal interactions.},
  archive      = {J_EJOR},
  author       = {Victor Medina-Olivares and Finn Lindgren and Raffaella Calabrese and Jonathan Crook},
  doi          = {10.1016/j.ejor.2025.07.060},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {892-904},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Joint model for longitudinal and spatio-temporal survival data},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Staggered routing in autonomous mobility-on-demand systems. <em>EJOR</em>, <em>327</em>(3), 875-891. (<a href='https://doi.org/10.1016/j.ejor.2025.06.008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous mobility-on-demand systems, effectively managing vehicle flows to mitigate induced congestion and ensure efficient operations is imperative for system performance and positive customer experience. Against this background, we study the potential of staggered routing, i.e., purposely delaying trip departures from a system perspective, in order to reduce congestion and ensure efficient operations while still meeting customer time windows. We formalize the underlying planning problem and show how to efficiently model it as a mixed integer linear program. Moreover, we present a matheuristic that allows us to efficiently solve large-scale real-world instances both in an offline full-information setting and its online rolling horizon counterpart. We conduct a numerical study for Manhattan, New York City, focusing on low- and highly-congested scenarios. Our results show that in low-congestion scenarios, staggering trip departures allows mitigating, on average, 98 % of the induced congestion in a full information setting. In a rolling horizon setting, our algorithm allows us to reduce 82 % of the induced congestion. In high-congestion scenarios, we observe an average reduction of 60 % as the full information bound and an average reduction of 30 % in our online setting. Surprisingly, we show that these reductions can be reached by shifting trip departures by a maximum of six minutes in both the low and high-congestion scenarios.},
  archive      = {J_EJOR},
  author       = {Antonio Coppola and Gerhard Hiermann and Dario Paccagnan and Maximilian Schiffer},
  doi          = {10.1016/j.ejor.2025.06.008},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {875-891},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Staggered routing in autonomous mobility-on-demand systems},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The storage location assignment and picker routing problem: A generic branch-cut-and-price algorithm. <em>EJOR</em>, <em>327</em>(3), 857-874. (<a href='https://doi.org/10.1016/j.ejor.2025.05.041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Storage Location Assignment Problem (SLAP) and the Picker Routing Problem (PRP) have received significant attention in the literature due to their pivotal role in the performance of the Order Picking (OP) activity, the most resource-intensive process of warehousing logistics. The two problems are traditionally considered at different decision-making levels: tactical for the SLAP, and operational for the PRP. However, this paradigm has been challenged by the emergence of modern practices in e-commerce warehouses, where decisions are more dynamic. This shift makes the integrated problem, called the Storage Location Assignment and Picker Routing Problem (SLAPRP), pertinent to consider. Scholars have investigated several variants of the SLAPRP, including different warehouse layouts and routing policies. Nevertheless, the available computational results suggest that each variant requires an ad-hoc formulation. Moreover, achieving a complete integration of the two problems, where the routing is solved optimally, remains out of reach for commercial solvers, even on trivial instances. In this paper, we propose an exact solution framework that addresses a broad class of variants of the SLAPRP, including all the previously existing ones. This paper proposes a Branch-Cut-and-Price framework based on a novel formulation with an exponential number of variables, which is strengthened with a novel family of non-robust valid inequalities. We have developed an ad-hoc branching scheme to break symmetries and maintain the size of the enumeration tree manageable. Computational experiments show that our framework can effectively solve medium-sized instances of several SLAPRP variants and outperforms the state-of-the-art methods from the literature.},
  archive      = {J_EJOR},
  author       = {Thibault Prunet and Nabil Absi and Diego Cattaruzza},
  doi          = {10.1016/j.ejor.2025.05.041},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {857-874},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The storage location assignment and picker routing problem: A generic branch-cut-and-price algorithm},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust parallel machine selection and scheduling with uncertain release times. <em>EJOR</em>, <em>327</em>(3), 838-856. (<a href='https://doi.org/10.1016/j.ejor.2025.05.032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a parallel machine selection and scheduling (PMSS) problem with uncertain release times. To handle uncertain release times, we propose a two-stage robust PMSS model where the release time deviation (RTD) is characterized by a budget uncertainty set. In the first stage, machine selection and job assignment decisions are made to minimize startup costs before the uncertainties are revealed. In the second stage, once release times are known, job sequences are optimized to minimize the makespan on each machine. Robust constraints are introduced to ensure that the worst-case minimum makespan on each machine does not exceed a pre-specified due date. The proposed model is a tri-level min–max–min optimization problem with mixed-integer recourse decisions, which cannot be solved efficiently by existing algorithms. To this end, we propose a novel logic-based Benders decomposition (LBBD) algorithm with strengthened Benders cuts and speedup techniques. Specifically, we first provide an equivalent mixed-integer linear programming reformulation for the max–min subproblem by analyzing an optimality condition of the worst-case RTD. Second, we design novel combinatorial and analytical Benders cuts, which dominate cuts found in the literature, and we further strengthen them by lifting procedures. Third, we design a relaxation-and-correction procedure and a warm-start procedure to speed up the LBBD algorithm. Numerical experiments show the proposed robust model greatly reduces job tardiness compared with the deterministic model. The proposed cuts efficiently reduce the runtime, and the LBBD algorithm is at least three orders of magnitude faster than the state-of-the-art column-and-constraint-generation algorithm.},
  archive      = {J_EJOR},
  author       = {Linyuan Hu and Yuli Zhang and Muyang Wen and Roel Leus and Ningwei Zhang},
  doi          = {10.1016/j.ejor.2025.05.032},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {838-856},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Robust parallel machine selection and scheduling with uncertain release times},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast optimization approach for a complex real-life 3D multiple bin size bin packing problem. <em>EJOR</em>, <em>327</em>(3), 820-837. (<a href='https://doi.org/10.1016/j.ejor.2025.05.016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a real-life air cargo loading problem which is a variant of the three-dimensional Variable Size Bin Packing Problem with special bin forms of cuboid and non-cuboid unit load devices (ULDs). Packing is constrained by additional practical restrictions, such as load stability, (non-)stackable items, and weight distribution constraints. To solve the problem, we present an insertion heuristic embedded into a Randomized Greedy Search. The solution space is limited by only considering certain candidate points (so-called extreme points), which are promising positions to load an item. We extend the concept of extreme points proposed in the literature and allow moving extreme points for non-cuboid ULDs. A special sorting of the items, which combines a layered structure and free packing, is suggested. Moreover, we propose dividing the space of each ULD into smaller cells to accelerate the collision, non-floating, and stackability check while loading items. In a computational study, we analyze individual algorithm components and show the effectiveness of our method on adapted real-life instances from the literature.},
  archive      = {J_EJOR},
  author       = {Katrin Heßler and Timo Hintsch and Lukas Wienkamp},
  doi          = {10.1016/j.ejor.2025.05.016},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {820-837},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A fast optimization approach for a complex real-life 3D multiple bin size bin packing problem},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning assisted differential evolution for the dynamic resource constrained multi-project scheduling problem with static project schedules. <em>EJOR</em>, <em>327</em>(3), 808-819. (<a href='https://doi.org/10.1016/j.ejor.2025.05.059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large modular construction projects, such as shipbuilding, multiple similar projects arrive stochastically. At project arrival, a schedule has to be created, in which future modifications are difficult and/or undesirable. Since all projects use the same set of shared resources, current scheduling decisions influence future scheduling possibilities. To model this problem, we introduce the Dynamic Resource Constrained Multi-project Scheduling Problem with Static project Schedules. To find schedules, both a greedy approach and simulation-based approach with varying scenarios are introduced. Although the simulation-based approach schedules projects proactively, the computing times are long, even for small instances. Therefore, a method is introduced that learns from schedules obtained in the simulation-based method and uses a neural network to estimate the objective function value. It is shown that this method achieves a significant improvement in objective function value over the greedy algorithm, while only requiring a fraction of the computation time of the simulation-based method.},
  archive      = {J_EJOR},
  author       = {T. van der Beek and J.T. van Essen and J. Pruyn and K. Aardal},
  doi          = {10.1016/j.ejor.2025.05.059},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {808-819},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Machine learning assisted differential evolution for the dynamic resource constrained multi-project scheduling problem with static project schedules},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A flexible mathematical model for home health care problems. <em>EJOR</em>, <em>327</em>(3), 791-807. (<a href='https://doi.org/10.1016/j.ejor.2025.05.055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the health and social care sectors it is common for some specialized teams to travel to patients homes to provide care. These teams are typically made up of by a number of staff members with varying skills, starting locations and working hours. Patients require different types of care, during specific time windows, and may have special requirements, such as needing two staff members, or multiple visits with some sort of temporal dependency between them. Since teams need to decide which staff member will visit each patient, as well as the routes they will take to do so, this kind of planning problem is known in the literature as the Home Health Care Routing and Scheduling Problem (HHCRSP). We introduce a new mixed integer linear programming formulation for the HHCRSP that extends previous models. Our formulation can readily be adapted to address more specific variants in the scientific literature, proving a larger number of optimal solutions and stronger lower bounds on benchmark instances using the same computational framework. We further propose an instance generator for producing scenarios that closely resemble those of the National Health Service in the United Kingdom.},
  archive      = {J_EJOR},
  author       = {Miguel Reula and Consuelo Parreño-Torres and Carlos Lamas-Fernandez and Antonio Martinez-Sykora},
  doi          = {10.1016/j.ejor.2025.05.055},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {791-807},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A flexible mathematical model for home health care problems},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The dial-a-ride problem with limited pickups per trip. <em>EJOR</em>, <em>327</em>(3), 776-790. (<a href='https://doi.org/10.1016/j.ejor.2025.05.051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dial-a-Ride Problem (DARP) is an optimization problem that involves determining optimal routes and schedules for several vehicles to pick up and deliver items at minimum cost. Motivated by real-world carpooling and crowdshipping scenarios, we introduce an additional constraint imposing a maximum number on the number of pickups per trip. This results in the Dial-a-Ride Problem with Limited Pickups per Trip (DARP-LPT). We apply a fragment-based method for DARP-LPT, where a fragment is a partial path. Specifically, we extend two formulations from Rist and Forbes (2021): the Fragment Flow Formulation (FFF) and the Pickup-Space Fragment Formulation (PSFF). Furthermore, our results show that PSFF outperforms FFF, which in turn surpasses traditional arc-based formulations in both solution quality and computational efficiency. Additionally, we compare several existing fragment sets that differ in the length of their partial paths and find that the sets with shorter partial paths yield the best solution times when used with PSFF. In addition, we propose a new mixed fragment set, which is useful when the sets with longer partial paths become too large. In such cases, it yields the lowest CPU time.},
  archive      = {J_EJOR},
  author       = {Boshuai Zhao and Kai Wang and Wenchao Wei and Roel Leus},
  doi          = {10.1016/j.ejor.2025.05.051},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {776-790},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The dial-a-ride problem with limited pickups per trip},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new class of lower bounds for scheduling a batch processing machine to minimize makespan. <em>EJOR</em>, <em>327</em>(3), 754-775. (<a href='https://doi.org/10.1016/j.ejor.2025.05.047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of minimizing makespan on a batch-processing machine with limited capacity. Each job has a size and processing time, and multiple jobs can be processed simultaneously in a batch, provided the machine’s capacity is not exceeded. The batch processing time is determined by the longest processing time in batch. We show that the existing lower bound method has a worst-case performance ratio of 1/2, and propose a class of lower bound procedures ( LB m ) and its improved variant ( ILB m ). The new procedures take integer m , used to partition jobs depending on whether their sizes are greater than B / m or not, and provide tighter bounds as m increases. We prove that the worst-case performance ratio of LB m and ILB m is no worse than 4/7. Additionally, we show that they can be computed efficiently for m ≤3. Based on the structure of the proposed lower bound procedures, we introduce different valid inequalities ( VI ) and embed them into an existing MILP model to achieve a formulation with a tighter LP bound. To gain understanding on the quality of the bounds, we employ them in a branch and bound ( B & B ) algorithm. Results indicate that the B&B with new lower bound methods increases the number of optimally solved problem instances by 44% and 35% compared to the existing B&B and branch and price algorithms, respectively. Furthermore, the lower bound-driven VI s help increase the number of solved problems by more than 30%, achieving an optimality rate exceeding 96% across a wide range of problem instances.},
  archive      = {J_EJOR},
  author       = {Ali Husseinzadeh Kashan and Onur Ozturk},
  doi          = {10.1016/j.ejor.2025.05.047},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {754-775},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A new class of lower bounds for scheduling a batch processing machine to minimize makespan},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EATKG: An open-source efficient exact algorithm for the two-dimensional knapsack problem with guillotine constraints. <em>EJOR</em>, <em>327</em>(3), 735-753. (<a href='https://doi.org/10.1016/j.ejor.2025.05.033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the Two-Dimensional Knapsack Problem with Guillotine Constraints, which is a famous NP -hard problem and is commonly encountered in industries where rectangular raw materials are cut into smaller pieces using guillotine cuts. We propose an efficient exact algorithm (EATKG) to solve this problem, which incorporates advanced techniques and novel elements, including an adapted preprocessing procedure, two enhanced upper bounds, an improved bidirectional tree search approach, and an iterative combination enumeration process. These components effectively balance the computation of upper and lower bounds and handle the issue of memory overflow. We extensively evaluate EATKG on eight classic benchmark sets, comprising 1,277 instances. Our algorithm solves 87% of the instances with an average computing time of 7 seconds, and 93% with an average computing time of 49 seconds. Moreover, EATKG efficiently solves nearly all small- and medium-sized instances, providing better solutions for 46 instances and tighter upper bounds for 109 instances. These results demonstrate the superior performance of our algorithm compared to leading algorithms. To support future research, we have made the source code for the proposed algorithm, along with the corresponding instance data, aggregated results, and detailed solutions, publicly available. This will facilitate further investigations and comparisons of solution methods.},
  archive      = {J_EJOR},
  author       = {Sunkanghong Wang and Roberto Baldacci and Qiang Liu and Lijun Wei},
  doi          = {10.1016/j.ejor.2025.05.033},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {735-753},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {EATKG: An open-source efficient exact algorithm for the two-dimensional knapsack problem with guillotine constraints},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Greedy randomized adaptive search procedures with path relinking. an analytical review of designs and implementations. <em>EJOR</em>, <em>327</em>(3), 717-734. (<a href='https://doi.org/10.1016/j.ejor.2025.02.022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is a comprehensive review of the Greedy Randomized Adaptive Search Procedure (GRASP) metaheuristic and its hybridization with Path Relinking (PR). GRASP with PR has become a widely adopted approach for solving hard optimization problems since its proposal in 1999. The paper covers the historical development of GRASP with PR and its theoretical foundations, as well as recent advances in its implementation and application. The review includes a careful analysis of PR variants, paying special attention to memory-based and randomized designs, with a total of ten different implementations. It identifies the design questions that are still open in the scientific literature. The experimental section applies advanced PR implementations on two well-known combinatorial optimization problems, linear ordering and max-cut, in an effort to answer these open questions. The paper also explores the hybridization of PR and other metaheuristics, such as tabu search, scatter search, and random-keys genetic algorithms. Overall, this review provides valuable insights for researchers and practitioners seeking to implement GRASP with PR for solving optimization problems.},
  archive      = {J_EJOR},
  author       = {Manuel Laguna and Rafael Martí and Anna Martínez-Gavara and Sergio Pérez-Peló and Mauricio G.C. Resende},
  doi          = {10.1016/j.ejor.2025.02.022},
  journal      = {European Journal of Operational Research},
  month        = {12},
  number       = {3},
  pages        = {717-734},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Greedy randomized adaptive search procedures with path relinking. an analytical review of designs and implementations},
  volume       = {327},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
