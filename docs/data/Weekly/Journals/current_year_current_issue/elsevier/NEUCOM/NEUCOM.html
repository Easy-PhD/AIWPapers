<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NEUCOM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neucom">NEUCOM - 47</h2>
<ul>
<li><details>
<summary>
(2025). Dynamic sparse directed graph convolutional network with attention mechanisms for EEG emotion recognition. <em>NEUCOM</em>, <em>658</em>, 131749. (<a href='https://doi.org/10.1016/j.neucom.2025.131749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG)-based emotion recognition is limited by pronounced inter-individual variability and the non-stationarity of neural signals, which leads to key issues such as insufficient representation of existing brain networks and poor generalization performance of cross-subject models. Traditional graph convolution networks (GCNs) rely on symmetric adjacency matrices in modeling the relationships among EEG channels, which cannot characterize the directional transmission of neural signals, and static fully-connected graph structures are susceptible to redundant connection interference. To overcome these limitations, this study proposes a novel model named dynamic sparse directed graph convolutional network with attention mechanisms (DSDirGCN-AM), which adaptively constructs graph structures and preserves key connections through a dynamic sparse graph network, while augmenting the constructed graph structure by assigning importance weights to the channels in each brain region using channel attention. Directed graph convolution with direction awareness is employed to model asymmetric functional connectivity using a bidirectional normalized adjacency matrix to capture signal directions between brain regions. Additionally, a multi-head self-attentive cascade feature mechanism optimizes cascaded multiscale feature fusion through cross-layer correlation weight assignment. Experimental evaluations demonstrate that the proposed method has yielded the accuracies of 93.19 % in SEED and 81.30 % in SEED-IV, which are significantly better than existing methods. The proposed framework offers an EEG emotion recognition approach that better reflects neurophysiological mechanisms.},
  archive      = {J_NEUCOM},
  author       = {Kaiwei Shen and Qingshan She and Xiaoli Yang and Yunyuan Gao and Yingle Fan},
  doi          = {10.1016/j.neucom.2025.131749},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131749},
  shortjournal = {Neurocomputing},
  title        = {Dynamic sparse directed graph convolutional network with attention mechanisms for EEG emotion recognition},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven multi-model predictive control for nonlinear systems under cyber attacks. <em>NEUCOM</em>, <em>658</em>, 131732. (<a href='https://doi.org/10.1016/j.neucom.2025.131732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model predictive control has demonstrated significant potential in managing nonlinear systems, but its effectiveness remains vulnerable to sophisticated cyber attacks. This paper presents a novel data-driven multi-model predictive control (DMMPC) framework that synergistically integrates cyber attack resilience with temporal feature learning. Compared with existing methods that focus on isolated channel attacks, the proposed framework explicitly considers cross-channel interference effects, enabling simultaneous mitigation of cyber attacks in sensor-controller and controller–actuator channels. Firstly, a data-driven anomaly detection system combining historical pattern matching with real-time signal deviation analysis is proposed to decrease the effects of sophisticated cyber attacks. Then, an expectation-based DMMPC method for nonlinear systems is designed to address the cyber attacks, and the bounded-input bounded-output stability of the closed-loop system is theoretically proven. Finally, the effectiveness of the proposed method is validated through numerical simulations and mobile robot experiments. Experimental results show that the proposed framework maintains tracking accuracy and system stability under various attack scenarios.},
  archive      = {J_NEUCOM},
  author       = {Yuesheng Liu and Zhongxian Xu and Ning He and Lile He and Ruoxia Li and Feng Gao},
  doi          = {10.1016/j.neucom.2025.131732},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131732},
  shortjournal = {Neurocomputing},
  title        = {Data-driven multi-model predictive control for nonlinear systems under cyber attacks},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RCMFDUN: Deep unfolding network with range-nullspace decomposition and multi-scale feature fusion for high-fidelity compressed sensing. <em>NEUCOM</em>, <em>658</em>, 131719. (<a href='https://doi.org/10.1016/j.neucom.2025.131719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Unfolding Networks (DUNs) have achieved remarkable success in Compressed Sensing (CS) image reconstruction tasks in recent years, due to their inherent interpretability and strong performance. Nevertheless, current DUNs often suffer from coarse feature granularity and the degradation of high-frequency information. As a result, they tend to lose structural details and produce blurred edges in reconstructed images.To address these issues, this paper proposes a novel Residual Channel-aware Multi-scale Fusion Deep Unfolding Network (RCMFDUN), which is based on nullspace decomposition. This method explicitly decomposes image features into distinct Range and Null subspaces, allowing customized modeling and enhancement in each space. The Range space primarily governs the recovery and preservation of global structures, while the Null space focuses on adaptively enhancing edges and fine details.Specifically, we design a powerful nullspace information extractor. It incorporates a Multi-scale Attention-enhanced Pyramid Fusion (MAPF) module with residual channel attention mechanisms. This module efficiently extracts and integrates structural semantic information across multiple scales, thereby improving both the sensitivity and effectiveness of feature selection. Furthermore, we introduce a cross-stage Feature Refinement Fusion (FRF) strategy. This strategy seamlessly combines shallow structural features from the current stage with enhanced information extracted along the multi-scale path.We propose two scalable variants of the method, both of which are applicable to natural image and medical image datasets. Extensive experiments consistently demonstrate that RCMFDUN significantly outperforms existing state-of-the-art methods in reconstruction accuracy. The code can be accessed at https://github.com/lsh5211/RCMFDUN .},
  archive      = {J_NEUCOM},
  author       = {Qiang Guo and Shihao Li and Tieyuan Li and Yuhang Tian and Mykola Kaliuzhnyi},
  doi          = {10.1016/j.neucom.2025.131719},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131719},
  shortjournal = {Neurocomputing},
  title        = {RCMFDUN: Deep unfolding network with range-nullspace decomposition and multi-scale feature fusion for high-fidelity compressed sensing},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UDA-DDA: Unsupervised domain adaptation with dynamic distribution alignment network for emotion recognition using EEG signals. <em>NEUCOM</em>, <em>658</em>, 131715. (<a href='https://doi.org/10.1016/j.neucom.2025.131715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the challenge of individual variability in affective brain-computer interfaces (aBCI), which employ electroencephalogram (EEG) signals to monitor and recognize human emotional states, thereby facilitating the advancement of emotion-aware technologies. The variability in EEG data across individuals poses a significant barrier to the development of effective and widely applicable aBCI models. To mitigate this issue, we propose a novel transfer learning framework called Unsupervised Domain Adaptation (UDA) with Dynamic Distribution Alignment (UDA-DDA). This approach aligns the marginal and conditional probability distributions of source and target domains by employing maximum mean discrepancy (MMD) and conditional maximum mean discrepancy (CMMD). Firstly, we introduce a dynamic distribution alignment mechanism to adjust differences throughout training and enhance adaptation. Additionally, a pseudo-label confidence filtering module is integrated into the unsupervised process to refine pseudo-label generation and optimize the estimation of conditional distributions. In order to demonstrate the effectiveness and robustness of the proposed UDA-DDA method, extensive experiments are conducted on EEG benchmark databases (SEED, SEED-IV and DEAP). Evaluations of the algorithm’s performance in comparison with other UDA with dynamic distribution alignment network methods indicate the proposed method achieves state-of-the-art results in emotion recognition across various scenarios, including cross-subject and cross-session conditions. This advancement significantly enhances the accuracy and generalization of emotion recognition, potentially fostering the development of personalized aBCI applications. The source code is accessible at https://github.com/XuanSuTrum/UDA-DDA .},
  archive      = {J_NEUCOM},
  author       = {Jiahao Tang and Youjun Li and Chun-Wang Su and Xiangting Fan and Yangxuan Zheng and Haoyu Wang and Hadia Naeem and Peng Fang and Jue Wang and Nan Yao and Xueping Li and Zi-Gang Huang},
  doi          = {10.1016/j.neucom.2025.131715},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131715},
  shortjournal = {Neurocomputing},
  title        = {UDA-DDA: Unsupervised domain adaptation with dynamic distribution alignment network for emotion recognition using EEG signals},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VLPRSDet: A vision–language pretrained model for remote sensing object detection. <em>NEUCOM</em>, <em>658</em>, 131712. (<a href='https://doi.org/10.1016/j.neucom.2025.131712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, numerous excellent vision-language models have emerged in the field of computer vision. These models have demonstrated strong zero-shot detection capabilities and better accuracy after fine-tuning on new datasets in the field of object detection. However, when these models are directly applied to the field of remote sensing, their performance is less than satisfactory. To address this problem, a novel vision-language pretrained model specifically tailored for remote sensing object detection task is proposed. Firstly, we create a new dataset composed of object-text pairs by collecting a large amount of remote sensing image object detection data to train the proposed model. Then, by integrating the CLIP model in the field of remote sensing with the YOLO detector, we propose a vision-language pretrained model for remote sensing object detection (VLPRSDet). VLPRSDet achieves enhanced fusion of visual and textual features through a vision language path aggregation network, and then aligns visual embeddings and textual embeddings through Region Text Matching to achieve the alignment between object regions and text. Experimental results indicate that the proposed VLPRSDet exhibits robust zero-shot capabilities in the field of remote sensing object detection, and can achieve superior detection accuracy after fine-tuning on specific datasets. Specifically, after fine-tuning, VLPRSDet can achieve 76.2 % mAP on the DIOR dataset and 94.2 % mAP on the HRRSD dataset. The code and dataset will be released at https://github.com/dyl96/VLPRSDet .},
  archive      = {J_NEUCOM},
  author       = {Dongyang Liu and Xuejian Liang and Yunxiao Qi and Yunqiao Xi and Jing Jin and Junping Zhang},
  doi          = {10.1016/j.neucom.2025.131712},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131712},
  shortjournal = {Neurocomputing},
  title        = {VLPRSDet: A vision–language pretrained model for remote sensing object detection},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fake news detection framework integrating multi-domain and multimodal features. <em>NEUCOM</em>, <em>658</em>, 131711. (<a href='https://doi.org/10.1016/j.neucom.2025.131711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread dissemination of video-based fake news on social media, identifying the authenticity of information in complex contexts has become increasingly challenging. News from different domains often differs significantly in vocabulary, expression styles, and modality distributions, leading to semantic ambiguity and increasing the difficulty of cross-news modeling. To address these challenges, this paper proposes a Multimodal Multi-Domain Fake News Detection framework (MMMD), which integrates textual, audio, and visual modalities. A domain gating mechanism is introduced to model domain-specific contextual structures, thereby enhancing the discriminative power of weak modalities (such as audio) and improving inter-modal coordination. Experiments conducted on multiple benchmark datasets show that MMMD outperforms mainstream multimodal methods in terms of accuracy, F1-score, and other metrics. Notably, on the FakeSV dataset, MMMD achieves a 6.87 % improvement in accuracy over the representative method SV-FEND. Furthermore, to address the high cost of domain annotation, a K-Means-based pseudo-label generation strategy is adopted. Comparative experiments across different numbers of clusters indicate that setting 10 yields performance close to that of human annotations, validating the method’s feasibility in low-supervision scenarios. Without relying on external user relationships, MMMD leverages domain-aware semantic structures and modality interaction mechanisms, providing an efficient and scalable solution for multimodal fake news detection in complex environments.},
  archive      = {J_NEUCOM},
  author       = {Longqin Guo and Zeqian Chen and Xiaoyang Liu},
  doi          = {10.1016/j.neucom.2025.131711},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131711},
  shortjournal = {Neurocomputing},
  title        = {A fake news detection framework integrating multi-domain and multimodal features},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CM-SQL: A cross-model consistency framework for text-to-SQL. <em>NEUCOM</em>, <em>658</em>, 131708. (<a href='https://doi.org/10.1016/j.neucom.2025.131708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large language models (LLMs) have been widely applied to the task of Text-to-SQL. Currently, most LLM-based Text-to-SQL methods primarily adopt the following approaches to improve the accuracy of generated SQL: (1) schema linking; and (2) leveraging the model’s self-consistency to check, modify, and select the generated SQL. However, due to issues such as hallucinations in LLMs, the database schema generated during the schema linking phase may contain errors or omissions. On the other hand, LLMs often exhibit overconfidence when evaluating the correctness of their outputs. To address these issues, we propose a cross-model consistency SQL generation framework (CM-SQL), which generates SQL outputs from different perspectives by feeding two database schemas into two LLMs. The framework combines the stability of fine-tuned models with the powerful reasoning capabilities of LLMs to evaluate the generated SQL. Additionally, we propose a local modification strategy to correct erroneous SQL. Finally, the outputs of the evaluation module and the LLM are used to select candidate SQLs, yielding the final SQL. We evaluated the proposed framework on the BIRD dev dataset using GPT-4o-mini and DeepSeek-V2.5, achieving an execution accuracy of 65.65 %. On the test set of the Spider dataset, the execution accuracy reached 87.6%, significantly outperforming most methods based on the same LLMs. Furthermore, our performance is comparable to many approaches that rely on more expensive models, such as GPT-4.},
  archive      = {J_NEUCOM},
  author       = {Xiang Li and Jinguo You and Heng Li and Jun Peng and Xi Chen and Ziheng Guo},
  doi          = {10.1016/j.neucom.2025.131708},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131708},
  shortjournal = {Neurocomputing},
  title        = {CM-SQL: A cross-model consistency framework for text-to-SQL},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FaceDisentGAN: Disentangled facial editing with targeted semantic alignment. <em>NEUCOM</em>, <em>658</em>, 131706. (<a href='https://doi.org/10.1016/j.neucom.2025.131706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial attribute editing in generative adversarial networks (GANs) involves two essential objectives: (1) accurately modifying the desired facial attribute, and (2) avoiding the unintended modification of irrelevant facial attributes. To address these challenges, we propose FaceDisentGAN, a novel generative framework for disentangled facial attribute manipulation. Specifically, we introduce: (1) a disentanglement module that decomposes feature maps into orthogonal spatial components (vertical and horizontal) to isolate target-related and unrelated semantics; (2) a two-stage training strategy that first learns general facial representations and then refines them to balance generic feature learning with fine-grained detail preservation; and (3) two novel evaluation metrics—Overall Preservation Score (OPS) and Perfect Match Rate (PMR)—which measure, respectively, the average preservation of non-target attributes and the proportion of perfectly disentangled results. This combination provides both soft and strict assessments of disentanglement quality. Extensive experiments demonstrate that FaceDisentGAN achieves accurate target attribute editing while effectively minimizing feature entanglement, outperforming several existing methods in both visual fidelity and semantic control.},
  archive      = {J_NEUCOM},
  author       = {Meng Xu and Prince Hamandawana and Xiaohan Ma and Zekang Chen and Rize Jin and Tae-Sun Chung},
  doi          = {10.1016/j.neucom.2025.131706},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131706},
  shortjournal = {Neurocomputing},
  title        = {FaceDisentGAN: Disentangled facial editing with targeted semantic alignment},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Node classification via simplicial interaction with augmented maximal clique selection. <em>NEUCOM</em>, <em>658</em>, 131705. (<a href='https://doi.org/10.1016/j.neucom.2025.131705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering higher-order interactions allows for a more comprehensive understanding of network structures beyond simple pairwise connections. While leveraging all cliques in a network to handle higher-order interactions is intuitive, it often leads to computational inefficiencies due to overlapping information between higher-order and lower-order cliques. To address this issue, we propose an augmented maximal clique strategy. Although using only maximal cliques can reduce unnecessary overlap and provide a concise representation of the network, certain nodes may still appear in multiple maximal cliques, resulting in imbalanced training data. Therefore, our augmented maximal clique approach selectively includes some non-maximal cliques to mitigate the overrepresentation of specific nodes and promote more balanced learning across the network. Comparative analyses on synthetic networks and real-world citation datasets demonstrate that our method outperforms approaches based on pairwise interactions, all cliques, or only maximal cliques. Finally, by integrating this strategy into GNN-based semi-supervised learning, we establish a link between maximal clique-based methods and GNNs, showing that incorporating higher-order structures improves predictive accuracy. As a result, the augmented maximal clique strategy offers a computationally efficient and effective solution for higher-order network learning.},
  archive      = {J_NEUCOM},
  author       = {Eunho Koo and Tongseok Lim},
  doi          = {10.1016/j.neucom.2025.131705},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131705},
  shortjournal = {Neurocomputing},
  title        = {Node classification via simplicial interaction with augmented maximal clique selection},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IntSTR: An integrated spatio-temporal relation transformer for video object detection. <em>NEUCOM</em>, <em>658</em>, 131704. (<a href='https://doi.org/10.1016/j.neucom.2025.131704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Transformer-based video object detection (VOD) methods have achieved remarkable progress by replacing the hand-crafted components traditionally used in CNN-based detectors. However, many existing approaches rely on staged spatio-temporal modeling strategies, which increase model complexity and restrict early interaction between spatial and temporal information. To overcome these limitations, we propose IntSTR, a novel framework for unified spatio-temporal modeling. At its core, the spatio-temporal relation encoder (STRE) integrates spatio-temporal feature processing within a single encoder through cascaded attention modules. To strengthen temporal consistency, the temporal query relation (TQR) module explicitly captures geometric relations between object queries across adjacent frames with minimal computational overhead. In addition, the Temporal Feature Memory (TFM) maintains a dynamic memory bank that caches temporal contexts, enabling effective feature aggregation and efficient online processing. Extensive experiments on the ImageNet VID dataset validate the effectiveness of our approach. IntSTR achieves an excellent trade-off between accuracy and efficiency, reaching a competitive 87.2 % mAP 50 with the ResNet-101 backbone while maintaining real-time performance at 33.4 FPS.},
  archive      = {J_NEUCOM},
  author       = {Wentao Zheng and Hong Zheng and Yuquan Sun and Ying Jing},
  doi          = {10.1016/j.neucom.2025.131704},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131704},
  shortjournal = {Neurocomputing},
  title        = {IntSTR: An integrated spatio-temporal relation transformer for video object detection},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SiTrEx: Siamese transformer for feedback and posture correction on workout exercises. <em>NEUCOM</em>, <em>658</em>, 131703. (<a href='https://doi.org/10.1016/j.neucom.2025.131703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying Machine Learning and Deep Learning techniques to sequences of Human Pose Landmarks to recognize workout exercises and count repetitions is widely studied in the computer vision literature. However, existing approaches suffer from two major problems. The first issue is that they lack the ability to provide detailed feedback on the postures performed by the athletes or provide feedback for a limited range of exercises using hand-designed rules and algorithms. The second problem is that these approaches consider only a predefined set of exercises and do not generalize to exercises outside their training data, which limits their usability. In this paper, we aim to address these two shortcomings by proposing a one-shot learning approach that utilizes Siamese Transformers to provide detailed feedback on individual human joints and can generalize to new exercises that are not present in the used dataset. The proposed configuration of the Siamese Transformer model deviates from its standard use in that it outputs a vector of similarity indicators rather than a single similarity score. Additionally, an accompanying binary classification Transformer model is used to assess the usefulness of different parts of the human pose for the input exercise without prior knowledge of the exercise itself. These properties allow the proposed approach to be used in general-purpose fitness applications and coach/athlete training platforms. The proposed approach achieved a 5-fold cross-validation test accuracy of 94.4 % ± 0.8 on the collected dataset.},
  archive      = {J_NEUCOM},
  author       = {Abdellah Sellam and Dounya Kassimi and Abdelhadi Djebana and Sara Mokhtari},
  doi          = {10.1016/j.neucom.2025.131703},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131703},
  shortjournal = {Neurocomputing},
  title        = {SiTrEx: Siamese transformer for feedback and posture correction on workout exercises},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EIAformer: Empowering transformer with enhanced information acquisition for time series forecasting. <em>NEUCOM</em>, <em>658</em>, 131700. (<a href='https://doi.org/10.1016/j.neucom.2025.131700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models have gained significant popularity and demonstrated remarkable performance in long-term time series forecasting. However, existing Transformer-based models are not designed to fully exploit the variation patterns and multiscale information of time series data. Moreover, there is a lack of channel strategy that effectively captures the essential connections between channels for improving the efficiency and accuracy of channel utilization. To overcome these problems, we propose a novel and adaptable architecture, EIAformer, to utilize comprehensive information to enhance the prediction performance. Firstly, hybrid decomposition is proposed to perform different operations on data with different variation patterns using a divide-and-conquer strategy. Then, dynamic patching based on dilated causal convolution is designed to capture multiscale information. Finally, channel fusion based on Granger Causality and DTW distance is constructed to capture the correlation between different channels, and the merged channels are fed into the encoder to perform prediction. Extensive experiments on nine datasets demonstrate that EIAformer achieves superior performance compared to existing Transformer-based models. Meanwhile, the proposed enhancement module as a plug-and-play solution can boost the performance and efficiency of the Transformer family models.},
  archive      = {J_NEUCOM},
  author       = {Weina Wang and Yongjie Wang and Xiaolong Qi and Hui Chen},
  doi          = {10.1016/j.neucom.2025.131700},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131700},
  shortjournal = {Neurocomputing},
  title        = {EIAformer: Empowering transformer with enhanced information acquisition for time series forecasting},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A small-sample cross-domain bearing fault diagnosis method based on knowledge-enhanced domain adversarial learning. <em>NEUCOM</em>, <em>658</em>, 131699. (<a href='https://doi.org/10.1016/j.neucom.2025.131699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional domain adaptation methods often perform poorly in cross-device bearing fault diagnosis when the target domain contains incomplete labels or exhibits imbalanced data. To address this issue, we propose an Adaptive meta-domain transfer learning network (AMTLN), which integrates a self-weighted fusion (SWF) module and a knowledge-enhanced domain adversarial learning (KEDA) framework to improve accuracy and robustness. An AMK-Fast DTW algorithm aligns vibration signals across domains, and kernel density estimation minimizes distributional differences. KEDA introduces auxiliary knowledge and meta-learning to enhance transfer performance in small-sample scenarios and reduce catastrophic forgetting. SWF further strengthens the forward knowledge transfer. Experiments show that AMTLN achieves high accuracy and strong generalization across varying operational conditions, even with incompletely labeled target data.},
  archive      = {J_NEUCOM},
  author       = {Peiming Shi and Yan Zhao and Xuefang Xu and Dongying Han},
  doi          = {10.1016/j.neucom.2025.131699},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131699},
  shortjournal = {Neurocomputing},
  title        = {A small-sample cross-domain bearing fault diagnosis method based on knowledge-enhanced domain adversarial learning},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRIFT: DCT-based robust and intelligent federated learning with trusted privacy. <em>NEUCOM</em>, <em>658</em>, 131697. (<a href='https://doi.org/10.1016/j.neucom.2025.131697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) allows collaborative model training across decentralized clients without sharing private data. However, traditional FL frameworks face dual challenges: vulnerability to Byzantine attacks (where malicious clients submit adversarial model updates) and privacy breaches (where curious clients infer sensitive information from exchanged parameters), exacerbated by decentralized operations and unencrypted communications. While existing work addresses robustness or privacy individually, the interplay between defense mechanisms, particularly the trade-off between attack resilience and utility degradation caused by privacy safeguards, remains understudied. To bridge this gap, we propose DRIFT , a novel FL framework that simultaneously achieves Byzantine robustness and privacy preservation. Our approach uniquely combines spectral analysis with cryptographic protection: By transforming model parameters into the frequency domain through Discrete Cosine Transform, DRIFT identifies malicious updates via spectral clustering while inherently obscuring sensitive parameter patterns. This defense mechanism is further reinforced by a privacy-preserving aggregation protocol leveraging fully homomorphic encryption with floating-point computation. It encrypts client updates during transmission and aggregation without compromising their computational usability. Extensive evaluations on MNIST and PathMNIST demonstrate that DRIFT outperforms baseline methods in resisting state-of-the-art Byzantine attacks while maintaining model utility and providing provable privacy guarantees.},
  archive      = {J_NEUCOM},
  author       = {Qihao Dong and Yang Bai and Mang Su and Yansong Gao and Anmin Fu},
  doi          = {10.1016/j.neucom.2025.131697},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131697},
  shortjournal = {Neurocomputing},
  title        = {DRIFT: DCT-based robust and intelligent federated learning with trusted privacy},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalizable 3D gaussian splatting via multi-view stereo and consistency constraints. <em>NEUCOM</em>, <em>658</em>, 131696. (<a href='https://doi.org/10.1016/j.neucom.2025.131696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent neural rendering methods still struggle with fine-grained detail reconstruction and scene generalization, especially when handling complex geometries and low-texture regions. To address these challenges, we propose a 3D Gaussian Splatting (3DGS) framework enhanced by Multi-view Stereo (MVS), aiming to improve both rendering quality and cross-scene adaptability. Specifically, we first introduce an Adaptive Perception-aware Feature Aggregation (APFA) module, which effectively fuses 2D image features into 3D geometry-aware representations via a Local Feature Adaptive Collaboration (LFAC) mechanism and a global Attention-Aware Module (AAM), significantly improving reconstruction performance in challenging scenes. Subsequently, we propose a depth and normal supervision strategy based on multi-view geometric consistency, where aggregated point clouds are utilized for optimized initialization, enhancing stability and fine-grained detail fidelity. Finally, a Gaussian geometric consistency regularization module is introduced to further enforce the coherence between depth and normal predictions, leading to more refined rendering results. Extensive experiments on standard benchmarks including DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples demonstrate that our method outperforms state-of-the-art approaches in terms of PSNR, SSIM, and LPIPS metrics. Particularly in real-world complex scenes, our approach achieves superior generalization ability and perceptual quality, validating the effectiveness of the proposed framework. The code for our method will be made available at https://github.com/yangyongjuan/MVS-APFA-GS .},
  archive      = {J_NEUCOM},
  author       = {Yongjuan Yang and Jie Cao and Hong Zhao and Weijie Wang},
  doi          = {10.1016/j.neucom.2025.131696},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131696},
  shortjournal = {Neurocomputing},
  title        = {Generalizable 3D gaussian splatting via multi-view stereo and consistency constraints},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TensorProjection layer: A tensor-based dimension reduction method in deep neural networks. <em>NEUCOM</em>, <em>658</em>, 131695. (<a href='https://doi.org/10.1016/j.neucom.2025.131695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a dimension reduction method for features with tensor structure, implemented as a neural network layer called the TensorProjection Layer. This layer applies mode-wise linear projections to the input tensor to reduce its dimensionality, with the projection directions treated as trainable parameters optimized during model training. The method is particularly useful for image data, serving as an alternative to pooling layers that reduce spatial redundancy. It can also reduce channel dimensions, making it applicable to various forms of tensor compression. While especially effective for image-based tasks, its application is not limited to them—as long as the intermediate representation is a tensor. We also demonstrate its use in multi-channel time-series and language data, showcasing its flexibility across diverse modalities. We evaluate the method by replacing specific layers in standard baseline models with TPL, across tasks including medical image classification and segmentation, classification of medical time-series signals, and classification of medical abstract texts. Experimental results suggest that, compared to conventional downsampling techniques such as pooling, the proposed layer offers improved generalization performance, making it a promising alternative for feature summarization in diverse neural network architectures.},
  archive      = {J_NEUCOM},
  author       = {Toshinari Morimoto and Su-Yun Huang},
  doi          = {10.1016/j.neucom.2025.131695},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131695},
  shortjournal = {Neurocomputing},
  title        = {TensorProjection layer: A tensor-based dimension reduction method in deep neural networks},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangled adaptive multi-dimensional dynamic graph convolutional network for skeleton-based action recognition. <em>NEUCOM</em>, <em>658</em>, 131693. (<a href='https://doi.org/10.1016/j.neucom.2025.131693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition plays a key role in computer vision and has gained significant attention due to its broad range of applications. However, most existing methods using graph convolutional networks struggle to effectively learn rich temporal and spatial motion features of body joints. In this work, the disentangled adaptive multi-dimensional dynamic graph convolutional network model that we present consists of three modules: a disentangled adaptive graph convolutional network module, a multi-dimensional dynamic temporal convolutional network module, and an efficient multi-scale attention module. Firstly, the disentangled adaptive graph convolutional network module is able to learn crucial details and interactive relationships of body joints by updating the primitive anatomical structure of the human body and adaptively changing the structural graph topology. Then, the multi-dimensional dynamic temporal convolutional network module is proposed to improve the capability of rich trajectory feature extraction and comprehensive representation. Finally, the efficient multi-scale attention module can concentrate on spatial-temporal information across the temporal and spatial dimensions to strengthen features in critical temporal frames at significant joints. Extensive experiments are performed on three large-scale datasets, including NTU RGB+D, NTU RGB+D 120, and Kinetics-Skeleton, demonstrating that the proposed model achieves state-of-the-art performance and can extract rich trajectory and spatial information from skeleton data.},
  archive      = {J_NEUCOM},
  author       = {Jie Li and Peitao Ye and Yu Xia and Yanwen Wang and Yi Cao},
  doi          = {10.1016/j.neucom.2025.131693},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131693},
  shortjournal = {Neurocomputing},
  title        = {Disentangled adaptive multi-dimensional dynamic graph convolutional network for skeleton-based action recognition},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards explainable trajectory classification: A segment-based perturbation approach. <em>NEUCOM</em>, <em>658</em>, 131691. (<a href='https://doi.org/10.1016/j.neucom.2025.131691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory classification is essential in applications such as transportation analysis, wildlife tracking, and human mobility studies. However, many existing models, especially deep learning-based approaches, suffer from a lack of explainability, making it challenging to understand their decision-making processes. To address this issue, we propose a model-agnostic explainability framework for trajectory classification based on subsegment perturbation. Our method systematically perturbs individual trajectory subsegments and constructs an importance map to highlight their contributions to the classification outcome. Additionally, we also propose a novel fidelity to assess the ability to provide interpretations as well as the quality of the interpretations. We evaluate the framework using multiple benchmark trajectory datasets and various classifiers, including both traditional machine learning models and deep learning models. Experimental results demonstrate that our method provides effective and meaningful explanations, especially the flexibility to be applied to many types of models.},
  archive      = {J_NEUCOM},
  author       = {Le Xuan Tung and Bui Dang Phuc and Vo Nguyen Le Duy},
  doi          = {10.1016/j.neucom.2025.131691},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131691},
  shortjournal = {Neurocomputing},
  title        = {Towards explainable trajectory classification: A segment-based perturbation approach},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid mask generation for infrared small target detection with single-point supervision. <em>NEUCOM</em>, <em>658</em>, 131688. (<a href='https://doi.org/10.1016/j.neucom.2025.131688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-frame infrared small target (SIRST) detection poses a significant challenge due to the requirement to discern minute targets amidst the complex infrared background clutter. In this paper, we focus on a weakly-supervised paradigm to obtain high-quality pseudo masks from the point-level annotation by integrating a novel learning-free method with the hybrid of the learning-based method. The learning-free method adheres to a sequential process, progressing from a point annotation to the bounding box that encompasses the target, and subsequently to detailed pseudo masks, while the hybrid is achieved through filtering out false alarms and retrieving missed detections in the network’s prediction to provide a reliable supplement for learning-free masks. The experimental results show that our learning-free method generates pseudo masks with an average Intersection over Union (IoU) that is 4.3 % higher than the second-best learning-free competitor across three datasets, while the hybrid learning-based method further enhances the quality of pseudo masks, achieving an additional average IoU increase of 3.4 %.},
  archive      = {J_NEUCOM},
  author       = {Weijie He and Mushui Liu and Yunlong Yu},
  doi          = {10.1016/j.neucom.2025.131688},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131688},
  shortjournal = {Neurocomputing},
  title        = {Hybrid mask generation for infrared small target detection with single-point supervision},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The development and future of digital rights management: A review. <em>NEUCOM</em>, <em>658</em>, 131672. (<a href='https://doi.org/10.1016/j.neucom.2025.131672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital rights management (DRM) serves as a critical technological mechanism for copyright protection, ensuring the legitimate use of digital content, and facilitating innovative business models in content distribution and access. This paper begins by introducing the fundamental concepts and typical architecture of DRM systems. It then provides a detailed analysis of the four distinct evolutionary phases of DRM, with a focus on key technologies including usage control, rights expression, content sharing, and decentralization. The paper further examines DRM standards, legal implications, and the tension between enforcement and fair use. Finally, it outlines future challenges and suggests promising directions for future research.},
  archive      = {J_NEUCOM},
  author       = {Xue Feng and Yijie Pan and Nai-an Xiao},
  doi          = {10.1016/j.neucom.2025.131672},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131672},
  shortjournal = {Neurocomputing},
  title        = {The development and future of digital rights management: A review},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight convolution and vision transformer integrated model with multi-scale self-attention mechanism. <em>NEUCOM</em>, <em>658</em>, 131670. (<a href='https://doi.org/10.1016/j.neucom.2025.131670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformer (ViT) has prevailed in computer vision tasks due to its strong long-range dependency modelling ability. However, its large model size and weak local feature modeling ability hinder its application in real scenarios. To balance computational efficiency and performance in downstream vision tasks, we propose an efficient ViT model with sparse attention (dubbed SAEViT) and convolution blocks. Specifically, a Sparsely Aggregated Attention (SAA) module has been proposed to perform adaptive sparse sampling and recover the feature map via deconvolution operation, which significantly reduces the computational complexity of attention operations. In addition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed to enhance inter-channel information exchange through feature decomposition and redistribution, which mitigates the redundancy in traditional feed-forward networks (FFN). Finally, a hierarchical pyramid structure with embedded depth-wise separable convolutional blocks (DWSConv) is devised to further strengthen convolutional features. Extensive experiments on mainstream datasets show that SAEViT achieves Top-1 accuracies of 76.3 % and 79.6 % on the ImageNet-1 K classification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively, demonstrating a lightweight solution for fundamental vision tasks.},
  archive      = {J_NEUCOM},
  author       = {Yi Zhang and Lingxiao Wei and Bowei Zhang and Ziwei Liu and Kai Yi and Shu Hu},
  doi          = {10.1016/j.neucom.2025.131670},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131670},
  shortjournal = {Neurocomputing},
  title        = {A lightweight convolution and vision transformer integrated model with multi-scale self-attention mechanism},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StreamVAD: A streaming framework with progressive context integration for multi-temporal scale video anomaly detection. <em>NEUCOM</em>, <em>658</em>, 131669. (<a href='https://doi.org/10.1016/j.neucom.2025.131669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection (VAD) plays a crucial role in intelligent surveillance systems by identifying abnormal events in video streams. However, most existing methods either rely on isolated feature extraction—failing to model inter-action contextual relationships critical for complex anomaly recognition—or demand full-video processing via graph/hierarchical architectures, which incur high latency, computational burden, and parameter/memory inefficiency with depth. Lightweight designs mitigate costs but sacrifice temporal sensitivity through shallow networks and short-clip inputs, limiting detection of subtle or multi-scale anomalies in streaming scenarios. To address these challenges, we propose StreamVAD, a lightweight streaming anomaly detection framework that achieves low-latency, long-term temporal modeling with minimal computational overhead. A Key Clip Generator (KCG) filters redundant inputs in a streaming manner, allowing the model to focus on informative content while reducing computational cost. A progressive context integration (PCI) module incrementally expands the temporal receptive field by integrating historical context without full-sequence buffering, enabling efficient detection of complex long-term anomalies. Additionally, a multi-scale temporal selection (MTS) strategy dynamically adapts temporal resolution to capture both short- and long-term abnormalities. Extensive experiments on UCF-Crime, XD-Violence, and a supplemental long-term anomaly dataset demonstrate that StreamVAD achieves effective video anomaly detection with fewer parameters and lower latency. The code and dataset are available at https://github.com/Han-lijun/StreamVAD .},
  archive      = {J_NEUCOM},
  author       = {Lijun Han and Gang Liang and Pengcheng Wang and Dingming Liu and Kui Zhao},
  doi          = {10.1016/j.neucom.2025.131669},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131669},
  shortjournal = {Neurocomputing},
  title        = {StreamVAD: A streaming framework with progressive context integration for multi-temporal scale video anomaly detection},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-feature interactive temporal knowledge graph reasoning with evolving retention mechanism. <em>NEUCOM</em>, <em>658</em>, 131663. (<a href='https://doi.org/10.1016/j.neucom.2025.131663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal knowledge graph (TKG) reasoning emphasizes deducing absent connections within evolving knowledge graphs (KGs), which is essential for comprehending dynamic engineering informatics. However, the ongoing dynamic evolution of TKGs presents significant challenges for accurate predictions. To address this challenge, this paper proposes a cross-feature temporal evolution network (CFTENet), which designs an evolving retention mechanism establishing a knowledge forgetting threshold to lock in snapshots of continuous evolution. The importance of knowledge gradually diminishes until the information becomes outdated and is completely forgotten. Historical information at previous time points is preserved in current snapshot to simulate continuous dynamic evolution of knowledge. Moreover, CFTENet incorporates a cross-feature interaction module, leveraging a multilayer dilated convolutional network and a residual network to grasp cross-feature intricate interactions among and across entity and relation characteristics. The proposed model improves the reasoning ability and resilience to unseen data. Comprehensive testing on four benchmark datasets (ICEWS14, ICEWS18, GDELT, WIKI) demonstrates that our model achieves significant performance improvements, surpassing the baseline methods by 1.5 %, 8.8 %, 6.5 %, and 2.2 %, which highlights its effectiveness in TKG reasoning.},
  archive      = {J_NEUCOM},
  author       = {Ying Cui and Xiao Song and Yishi Liu and Ming Liu},
  doi          = {10.1016/j.neucom.2025.131663},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131663},
  shortjournal = {Neurocomputing},
  title        = {Cross-feature interactive temporal knowledge graph reasoning with evolving retention mechanism},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tabular data generation models: An in-depth survey and performance benchmarks with extensive tuning. <em>NEUCOM</em>, <em>658</em>, 131655. (<a href='https://doi.org/10.1016/j.neucom.2025.131655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating realistic, safe, and useful tabular data is important for downstream tasks such as privacy preserving, imputation, oversampling, explainability, and simulation. However, the structure of tabular data, marked by heterogeneous types, non-smooth distributions, complex feature dependencies, and categorical imbalance, poses significant challenges. Although many generative approaches have been proposed, a fair and unified evaluation across datasets remains missing. This work benchmarks five recent model families on 16 diverse datasets (average 80 K rows), with careful optimization of hyperparameters, feature encodings, and architectures. We show that dataset-specific tuning leads to substantial performance gains, particularly for diffusion-based models. We further introduce constrained hyperparameter spaces that retain competitive performance while significantly reducing tuning cost, enabling efficient model selection under fixed GPU budgets. As future perspectives, we can study cross-domain and cross-table generation.},
  archive      = {J_NEUCOM},
  author       = {G.Charbel N. Kindji and Lina M. Rojas-Barahona and Elisa Fromont and Tanguy Urvoy},
  doi          = {10.1016/j.neucom.2025.131655},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131655},
  shortjournal = {Neurocomputing},
  title        = {Tabular data generation models: An in-depth survey and performance benchmarks with extensive tuning},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Algorithmically-designed reward shaping for multiagent reinforcement learning in navigation. <em>NEUCOM</em>, <em>658</em>, 131654. (<a href='https://doi.org/10.1016/j.neucom.2025.131654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practical applicability of multiagent reinforcement learning is hindered by its low sample efficiency and slow learning speed. While reward shaping and expert guidance can partially mitigate these challenges, their efficiency is offset by the need for substantial manual effort. To address these constraints, we introduce Multiagent Environment-aware semi-Automated Guide (MEAG), a novel framework that leverages widely known, highly efficient, and low-resolution single-agent pathfinding algorithms for shaping rewards to guide multiagent reinforcement learning agents. MEAG uses these single-agent solvers over a coarse-grid surrogate that requires minimal manual intervention, and guides agents away from random exploration in a manner that significantly reduces computational costs. When tested across a range of densely and sparsely connected multiagent navigation environments, MEAG consistently outperforms state-of-the-art algorithms, achieving up to 50 % faster convergence and 20 % higher rewards. These improvements enable the consideration of MARL for more complex real-world pathfinding applications ranging from warehouse automation to search and rescue operations, and swarm robotics.},
  archive      = {J_NEUCOM},
  author       = {Ifrah Saeed and Andrew C. Cullen and Zainab Zaidi and Sarah Erfani and Tansu Alpcan},
  doi          = {10.1016/j.neucom.2025.131654},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131654},
  shortjournal = {Neurocomputing},
  title        = {Algorithmically-designed reward shaping for multiagent reinforcement learning in navigation},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual perspective-aware graph neural network for graph-level anomaly detection. <em>NEUCOM</em>, <em>658</em>, 131649. (<a href='https://doi.org/10.1016/j.neucom.2025.131649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-level anomaly detection based on graph neural networks (GAD-GNN) aims to identify graphs exhibiting anomalous characteristics distinct from the majority in a dataset. However, existing GAD-GNN methods face two critical challenges: Aggregation anomaly dilution occurs when the signals of sparsely distributed abnormal nodes are overwhelmed by the dominant influence of normal nodes during message passing. Readout anomaly dilution arises when locally concentrated anomalies are smoothed out in graph readout. To overcome these challenges, we propose the D ual P erspective-Aware G raph N eural N etwork (DPGNN), which integrates two complementary modules. The Global Awareness Module enhances node representations with multi-scale return-probability fingerprints, ensuring that signals of sparsely distributed abnormal nodes are preserved against overwhelming normal patterns. The Local Awareness Module adaptively identifies anomaly subgraphs using structural cues and employs attention-based readout to retain concentrated anomalies from being diluted in graph readout. Extensive experiments on multiple benchmark datasets demonstrate that DPGNN consistently outperforms state-of-the-art methods, validating its effectiveness in detecting graph-level anomalies.},
  archive      = {J_NEUCOM},
  author       = {Jianliang Gao and Xinqiu Zhang and Qiutong Li and Jiamin Chen},
  doi          = {10.1016/j.neucom.2025.131649},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131649},
  shortjournal = {Neurocomputing},
  title        = {Dual perspective-aware graph neural network for graph-level anomaly detection},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinate descent for top-k multi-label feature selection with pseudo-label learning and manifold learning. <em>NEUCOM</em>, <em>658</em>, 131640. (<a href='https://doi.org/10.1016/j.neucom.2025.131640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning plays an increasingly important role in handling complex problems where data instances are associated with multiple labels. However, current methods face significant limitations when dealing with high-dimensional feature spaces. They struggle to preserve the geometric structure among features while failing to fully exploit the latent correlations between labels. To address these key challenges, this paper proposes a novel feature selection method called coordinate descent for top-k multi-label feature selection with pseudo-label learning and manifold learning (CD-MPL), which integrates manifold learning with pseudo-label learning techniques. First, by constructing a feature graph Laplacian matrix, we establish a mathematical representation of the feature manifold structure, effectively preserving the local geometric properties of the feature space. Second, we introduce a pseudo-label learning mechanism, converting discrete binary labels into continuous representations to better model complex label correlations. Notably, to tackle the non-convex optimization problem caused by the ℓ 2 , 0 -norm constraint, we innovatively transform the original problem into the joint optimization of a continuous matrix and a discrete selection matrix. We then employ a coordinate descent (CD) method to efficiently solve the selection matrix, overcoming the non-convexity issue while enhancing model performance, interpretability, and practicality. Experimental results on ten multi-label datasets demonstrate that CD-MPL significantly outperforms existing methods across multiple key evaluation metrics, achieving an average performance improvement of 3.31 %. The algorithm maintains stable performance even with reduced feature subsets and exhibits rapid convergence within 10 iterations, fully validating its efficiency and effectiveness in multi-label classification tasks.},
  archive      = {J_NEUCOM},
  author       = {Ruijia Li and Yingcang Ma and Hong Chen and Xiaofei Yang and Zhiwei Xing},
  doi          = {10.1016/j.neucom.2025.131640},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131640},
  shortjournal = {Neurocomputing},
  title        = {Coordinate descent for top-k multi-label feature selection with pseudo-label learning and manifold learning},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic event-based asymptotic tracking and vibration control for constrained flexible manipulator systems with intermittent faults. <em>NEUCOM</em>, <em>658</em>, 131638. (<a href='https://doi.org/10.1016/j.neucom.2025.131638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The angle constraint and vibration suppression issues of flexible manipulator (FM) systems subjected to intermittent faults are addressed in this article. Firstly, integral barrier Lyapunov functions (BLFs) that can directly constrain the angular position are introduced, eliminating the feasibility conditions of traditional BLFs. Secondly, a triggering mechanism with dynamic variables is provided to reduce the transmission of redundant information, thereby saving communication resources. To mitigate the impact of intermittent faults and handle system, the boundary estimation method and the neural networks (NNs) technology considering the influence of approximation error are adopted, which reduces the conservatism of the developed control algorithm. Through Lyapunov stability theory and Hamiltonian principle, a dynamic event-based fault-tolerant controller is designed, suppressing the offset of the FM while ensuring that the angular position asymptotically tracks the ideal position without exceeding the given constraint boundary. Eventually, the simulation results demonstrate the rationality of the developed control scheme.},
  archive      = {J_NEUCOM},
  author       = {Shan-Lin Liu and Meina Zhai and Rui Wang and Yufeng Tian},
  doi          = {10.1016/j.neucom.2025.131638},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131638},
  shortjournal = {Neurocomputing},
  title        = {Dynamic event-based asymptotic tracking and vibration control for constrained flexible manipulator systems with intermittent faults},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised temporal action segmentation with sample discrimination training and alignment-based boundary refinement. <em>NEUCOM</em>, <em>658</em>, 131636. (<a href='https://doi.org/10.1016/j.neucom.2025.131636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised temporal action segmentation (UTAS) addresses the task of partitioning untrimmed videos into coherent action segments without manual annotations. While boundary-detection-based approaches have demonstrated superior performance, they exhibit two critical limitations. First, these methods often uniformly treat all frames during training, resulting in over-segmentation and suboptimal performance. Second, they primarily rely on intra-video features while neglecting potentially valuable inter-video correlations within the dataset. To address these challenges, we present a comprehensive UTAS framework with three key innovations: (1) A discriminative training mechanism that differentiates between boundary/non-boundary frames in the temporal domain and motion/background pixels in the spatial domain, employing weighted training strategies alongside multiple temporal-scale modeling. (2) A self-validation mechanism for cross-verifying predictions across different input sequences. (3) A boundary refinement approach based on video alignment, which constructs reference video sets according to feature distributions and establishes inter-video correspondences to improve boundary localization. Extensive evaluations on three benchmark datasets, i.e. , the Breakfast, the 50Salads, and the YouTube Instructions, demonstrate that our approach achieves state-of-the-art performance, with quantitative results showing significant improvements over existing methods.},
  archive      = {J_NEUCOM},
  author       = {Feng Huang and Xiao-Diao Chen and Hongyu Chen and Haichuan Song},
  doi          = {10.1016/j.neucom.2025.131636},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131636},
  shortjournal = {Neurocomputing},
  title        = {Unsupervised temporal action segmentation with sample discrimination training and alignment-based boundary refinement},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quaternion reservoir computing for spatiotemporal analysis in polarimetric synthetic aperture radar. <em>NEUCOM</em>, <em>658</em>, 131633. (<a href='https://doi.org/10.1016/j.neucom.2025.131633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quaternion neural networks possess high generalization ability in three-dimensional (3D) information space by representing every 3D data point as a single quaternion entity. In polarimetric synthetic aperture radar (PolSAR) applications such as land surface classification, they are expected to deal with 3D Poincare parameters as inseparable physical entities. With the increasing acquisition frequency, there is a growing demand also for efficient and robust techniques to monitor temporal or spatiotemporal changes. Reservoir computing (RC) is a variation of recurrent neural networks (RNNs) capable of detecting changes in series data with low computational cost. In this context, we propose quaternion reservoir computing (QRC) for spatiotemporal analysis in PolSAR. First, in a benchmark prediction task for chaotic time-series derived from the 3D Lorenz equations, we demonstrate that QRC achieves higher prediction accuracy than real-valued RC and conventional RNNs. Secondly, we conduct spatiotemporal anomalous change detection for actual PolSAR data of (1) rice fields having seasonal changes in Japan and (2) Amazon rainforest suffering from deforestation in Brazil. Compared with real-valued RC, RNNs, one-dimensional convolutional neural networks, Transformer, and non-adaptive methods based on complex Wishart and Pauli RGB, QRC shows a larger area under the curve (AUC) score, demonstrating its high efficacy in capturing spatiotemporal anomalous variations in PolSAR data. Besides such high performance, QRC shows low training cost, which is very suitable for real-time processing in edge computing including highly frequent satellite observations. These experimental results indicate that combining quaternion representation with RC is a promising approach for analyzing the ever-increasing volume of PolSAR data.},
  archive      = {J_NEUCOM},
  author       = {Kitoshi Kawai and Bungo Konishi and Ryo Natsuaki and Akira Hirose},
  doi          = {10.1016/j.neucom.2025.131633},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131633},
  shortjournal = {Neurocomputing},
  title        = {Quaternion reservoir computing for spatiotemporal analysis in polarimetric synthetic aperture radar},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D gaussian splatting technologies and extensions: A review. <em>NEUCOM</em>, <em>658</em>, 131629. (<a href='https://doi.org/10.1016/j.neucom.2025.131629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, 3D Gaussian Splatting (3DGS) has achieved remarkable progress in the field of novel view synthesis. Unlike implicit neural radiance field (NeRF) methods that primarily focus on positional and viewpoint transformations, 3DGS leverages millions of Gaussian ellipsoids for scene reconstruction and employs parallel differentiable rasterization to substantially improve rendering efficiency. Given the rapid advancement and promising prospects of this technique, this survey presents a systematic overview of recent developments in 3DGS. We provide a detailed exposition of the fundamental theory underlying 3DGS, along with relevant benchmark datasets. Uniquely, this work organizes existing optimization strategies according to the stages of the Gaussian splatting pipeline. In addition, we review various downstream applications based on 3DGS and discuss prospective research directions. This survey aims to serve as a valuable reference for researchers across all stages of engagement and to foster further advancements in 3DGS.},
  archive      = {J_NEUCOM},
  author       = {Fengkai Luan and Siliang Sun and Hu Zhang and Yong Yin and Ke Wang and Jiaxing Yang},
  doi          = {10.1016/j.neucom.2025.131629},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131629},
  shortjournal = {Neurocomputing},
  title        = {3D gaussian splatting technologies and extensions: A review},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prototype-based multi-domain self-distillation for unbiased scene graph generation. <em>NEUCOM</em>, <em>658</em>, 131625. (<a href='https://doi.org/10.1016/j.neucom.2025.131625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene Graph Generation (SGG) plays an important role in reinforcing visual image understanding. Existing methods often encounter difficulties in effectively representing implicit relationship features, which limits their capacity to distinguish between predicates. Meanwhile, these approaches are susceptible to imbalanced instance distributions, hindering the efficient training of fine-grained predicates. To address these problems, we propose a novel prototype-based multi-domain self-distillation training framework. Specifically, a Multi-Domain Fusion (MDF) module is introduced to improve predicate feature representation by integrating global contextual information and local spatial-frequency domain information. Then, a Prototype Generation Network (PGN) is designed for building the class prototypes, which consists of the design of different granularity predicates and loss functions. Furthermore, we design two different data balancing strategies under the guidance of class prototypes, which correspond to mining the in-distribution and out-of-distribution information of the original data, respectively. The experimental results demonstrate that the proposed method is superior to the existing methods on VG, GQA and Open Images V6 datasets, which makes it more applicable to generating unbiased scene graph models.},
  archive      = {J_NEUCOM},
  author       = {Yuan Gao and Yaochen Li and Yujie Zang and Jingze Liu and Yuehu Liu},
  doi          = {10.1016/j.neucom.2025.131625},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131625},
  shortjournal = {Neurocomputing},
  title        = {Prototype-based multi-domain self-distillation for unbiased scene graph generation},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DJIST: Decoupled joint image and sequence training framework for sequential visual place recognition. <em>NEUCOM</em>, <em>658</em>, 131622. (<a href='https://doi.org/10.1016/j.neucom.2025.131622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional image-to-image (im2im) visual place recognition (VPR) involves matching a single query image to stored geo-tagged database images. In real-time robotic and autonomous applications, while a continuous stream of frames naturally leads to a simpler sequence-to-sequence (seq2seq) VPR problem, the challenges remain since labeled sequential data is much scarcer than labeled individual images. A recent work addressed this by using a unified network optimized for both seq2seq and im2im tasks, but the resulting sequential descriptors are heavily dependent on the individual descriptors trained on the im2im task. This paper proposes a decoupled joint image and sequence training (DJIST) framework, using a frozen backbone and two independent sequential branches, where one branch is supervised by both im2im and seq2seq losses and the other solely by the seq2seq loss. The feature reduction procedures for generating individual descriptors and sequential descriptors are further separated in the former branch. An attention separation loss is employed between the two branches, which forces them to focus on different parts of the images to produce more informative sequential descriptors. We retrain various existing seq2seq methods using the same backbone and two types of joint training strategies for a fair comparison. Extensive experimental results demonstrate that our proposed DJIST outperforms its original counterpart JIST by 3.9 % to 18.8 % across four benchmark test cases and achieves state-of-the-art Recall@1 scores against retrained baselines on three key benchmarks with robust cross-dataset generalization, negligible degradation under dimensionality reduction, and superior robustness against varying test-time sequence lengths. Code will be available at https://github.com/shuimushan/DJIST .},
  archive      = {J_NEUCOM},
  author       = {Shanshan Wan and Lai Kang and Yingmei Wei and Tianrui Shen and Haixuan Wang and Chao Zuo},
  doi          = {10.1016/j.neucom.2025.131622},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131622},
  shortjournal = {Neurocomputing},
  title        = {DJIST: Decoupled joint image and sequence training framework for sequential visual place recognition},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ABM: Adaptive bias mitigation for class-imbalanced semi-supervised learning. <em>NEUCOM</em>, <em>658</em>, 131617. (<a href='https://doi.org/10.1016/j.neucom.2025.131617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-imbalanced semi-supervised learning (CISSL) poses significant challenges in real-world scenarios, where limited labeled data and skewed class distributions jointly hinder model generalization, especially for minority classes. Existing CISSL methods predominantly focus on adjusting model outputs, often overlooking the crucial role of representation learning in addressing classifier bias. In this paper, we propose a feature-aware adaptive bias mitigation framework that effectively alleviates class imbalance while enhancing representation learning. Specifically, our approach leverages the batch-wise feature mean as an additional input to guide the learning process, enabling the model to calibrate its representations against an unbiased reference. We refine classifier predictions by adjusting logits based on batch-wise feature mean, and further apply post-hoc logit adjustment to correct residual response bias. This combination not only improves pseudo-label quality but also fosters balanced and robust feature learning. Extensive experiments on four benchmark datasets under various class distribution settings demonstrate that our method consistently outperforms state-of-the-art competitors, achieving superior balanced accuracy and robustness against distribution mismatches.},
  archive      = {J_NEUCOM},
  author       = {Hongzhu Yi and Yue Cheng and Weiwei Xing and Xiang Wei},
  doi          = {10.1016/j.neucom.2025.131617},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131617},
  shortjournal = {Neurocomputing},
  title        = {ABM: Adaptive bias mitigation for class-imbalanced semi-supervised learning},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RAAG:Redundancy-adaptive and attention-guided token pruning for efficient video action detection. <em>NEUCOM</em>, <em>658</em>, 131615. (<a href='https://doi.org/10.1016/j.neucom.2025.131615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video action detection faces significant computational challenges, especially with high-resolution and long video sequences. Existing fixed-rate pruning methods are often suboptimal, risking crucial information loss or retaining excessive redundancy. This paper introduces Redundancy-Adaptive and Attention-Guided Token Pruning (RAAG), a novel, adaptive framework for efficient end-to-end video action detection. RAAG integrates Information Redundancy-Adaptive Token Pruning (IRTP), which dynamically adjusts token keep rate based on inter-frame information redundancy, and a Hierarchical Attention-Guided (HAG) strategy, which refines pruning by allocating distinct layer-specific rates to preserve essential features in early layers and aggressively prune in actor-focused middle layers. Comprehensive experiments on AVA 2.2, JHMDB, and UCF101-24 demonstrate RAAG’s superior performance. Notably, RAAG (ViT-L) achieves 40.5 mAP on AVA 2.2, and robustly performs on JHMDB (90.7 mAP) and UCF101-24 (86.5 mAP). These results validate RAAG’s ability to intelligently balance computational efficiency with detection accuracy across diverse video contents.},
  archive      = {J_NEUCOM},
  author       = {Jun Chen and Sailong Deng and Wei Yu and Longsheng Wei},
  doi          = {10.1016/j.neucom.2025.131615},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131615},
  shortjournal = {Neurocomputing},
  title        = {RAAG:Redundancy-adaptive and attention-guided token pruning for efficient video action detection},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPFBL: Modal pairing-based cross-fusion bootstrap learning for multimodal emotion recognition. <em>NEUCOM</em>, <em>658</em>, 131577. (<a href='https://doi.org/10.1016/j.neucom.2025.131577'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition (MER), a key technology in human-computer interaction, deciphers complex emotional states by integrating heterogeneous data sources such as text, audio, and video. However, previous works either retained only private information or focused solely on public information, resulting in a conflict between the strategies used in each approach. Existing methods often lose critical modality-specific attributes during feature extraction or struggle to align semantically divergent representations across modalities during fusion, resulting in incomplete emotional context modeling. To address these challenges, we propose the Modal Pairing-based Cross-Fusion Bootstrap Learning (MPFBL) framework, which integrates modal feature extraction, cross-modal bootstrap learning, and multi-modal cross-fusion into a unified approach. Firstly, the feature extraction module employs a Uni-Modal Transformer (UMT) and a Multi-Modal Transformer (MMT) to jointly capture modality-specific and modality-invariant information, addressing feature degradation in single-encoder paradigms, while alleviating inter-modal heterogeneity by explicitly distinguishing between modality-specific and shared representations. Subsequently, cross-modal bootstrap learning employs attention-guided optimization to align heterogeneous modalities and refine modality-specific representations, enhancing semantic consistency. Finally, a multi-modal cross-fusion network integrates convolutional mapping and adaptive attention to dynamically weight cross-modal dependencies, mitigating spatial-semantic misalignment induced by inter-modal heterogeneity in fusion processes. Extensive experimental results on CMU-MOSEI and CMU-MOSI demonstrate that MPFBL outperforms state-of-the-art methods, while ablation studies further confirm its effectiveness.},
  archive      = {J_NEUCOM},
  author       = {Yong Zhang and Yongqing Liu and HongKai Li and Cheng Cheng and Ziyu Jia},
  doi          = {10.1016/j.neucom.2025.131577},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131577},
  shortjournal = {Neurocomputing},
  title        = {MPFBL: Modal pairing-based cross-fusion bootstrap learning for multimodal emotion recognition},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auto-weighted graph tensor and rank-constrained bipartite graph fusion for multi-view clustering. <em>NEUCOM</em>, <em>658</em>, 131575. (<a href='https://doi.org/10.1016/j.neucom.2025.131575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor multi-view clustering generally outperforms non-tensor counterparts, as the tensor structure can effectively capture the higher-order correlations of data. Although the t-SVD-based tensor nuclear norm has shown remarkable performance, it treats the similar information across all views equally, overlooking the higher-order similarities between similar graphs. To address this issue, we propose a Pearson Correlation Coefficient-based A uto-weighted G raph T ensor and R ank-constrained B ipartite G raph F usion (AGTRBGF) approach for multi-view clustering. Specifically, the P-AGT learning method breaks free from the constraints of predefined weights, automatically assigning optimal weight values for each similarity graph by leveraging the higher-order similarities among the similar graphs of different views. Additionally, the Laplace rank is utilized to constrain the adaptive graph fusion, endowing learned consensus graph with strong diagonal structure and enhancing the model’s robustness. Experiments conducted on distinct datasets validate the effectiveness and superior clustering performance of AGTRBGF.},
  archive      = {J_NEUCOM},
  author       = {Jie Zhang and Xiaoqian Zhang and Jinghao Li and Yongyi Yang and Zhenwen Ren and Rong Tang and Dong Wang},
  doi          = {10.1016/j.neucom.2025.131575},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131575},
  shortjournal = {Neurocomputing},
  title        = {Auto-weighted graph tensor and rank-constrained bipartite graph fusion for multi-view clustering},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Encryption-decryption-based distributed state estimation against eavesdropping attacks over sensor networks with communication protocol. <em>NEUCOM</em>, <em>658</em>, 131570. (<a href='https://doi.org/10.1016/j.neucom.2025.131570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The secure distributed state estimation problem is investigated for a class of discrete time-varying systems over sensor networks regulated by encryption–decryption mechanism and round-robin protocol. To save energy and alleviate network congestion, the round-robin protocol is introduced to schedule the transmission order of the measurement data. To mitigate privacy leakage, an encryptor is designed to encrypt the measurement information of each sensor node, and then the encrypted measurements can be decrypted by the user. The primary objective of this paper is to present a distributed state estimation algorithm with recursive format for such time-varying systems, in which an upper bound on the estimation error covariance is derived, and appropriate estimator gains are determined to minimize this upper bound. In addition, a sufficient condition is provided to ensure that the estimation error of the user is exponentially bounded in the mean-square sense. Particularly, the properly designed encryption–decryption parameters guarantee that the state estimation error of the eavesdropper is unbounded. Finally, two simulation experiments are conducted to demonstrate the feasibility of the developed encryption–decryption-based distributed state estimation algorithm.},
  archive      = {J_NEUCOM},
  author       = {Xiaolong Yang and Wen Chen and Hongxu Zhang and Jiawen Zhang and Yuxin Guo},
  doi          = {10.1016/j.neucom.2025.131570},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131570},
  shortjournal = {Neurocomputing},
  title        = {Encryption-decryption-based distributed state estimation against eavesdropping attacks over sensor networks with communication protocol},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSF-GODE: Multi-scale frequency-domain learning in graph neural ODEs for accurate traffic flow forecasting. <em>NEUCOM</em>, <em>658</em>, 131566. (<a href='https://doi.org/10.1016/j.neucom.2025.131566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality traffic forecasting plays a critical role in intelligent transportation systems (ITS) and the development of smart cities. However, the pervasive spatiotemporal heterogeneity in traffic data poses significant challenges for existing models in reliably capturing complex and evolving traffic dynamics. In addition, the frequent neglect of features from non-hotspot regions and the absence of effective cross-channel feature fusion mechanisms further hinder both predictive accuracy and generalization capabilities. To address these challenges, we propose a novel framework named Multi-Scale Spatiotemporal Frequency-aware Graph Ordinary Differential Equation network (MSF-GODE), which offers a unified and systematic modeling strategy to tackle the above limitations. Specifically, the model first utilizes a multi-scale frequency sample generator that leverages time–frequency decomposition to extract periodic structures and capture temporal dependencies across multiple resolutions. It then incorporates a spatiotemporal feature extractor that combines key feature selection and contrastive learning, thereby enhancing the model’s ability to represent non-key regions. Finally, a spatiotemporal frequency-domain feature fusion module is employed to model structural evolution and integrate multi-channel features more effectively. Extensive experiments conducted on six real-world traffic datasets demonstrate that MSF-GODE significantly outperforms existing state-of-the-art methods in terms of both prediction accuracy and generalization, offering a robust and effective solution for traffic forecasting in heterogeneous environments.},
  archive      = {J_NEUCOM},
  author       = {Peng Liu and Yaodong Zhu and Yang Yang and Jilong Tang and Xiaojiao Jiang and Jinquan Wang},
  doi          = {10.1016/j.neucom.2025.131566},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131566},
  shortjournal = {Neurocomputing},
  title        = {MSF-GODE: Multi-scale frequency-domain learning in graph neural ODEs for accurate traffic flow forecasting},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fused adaptive tensor log-determinant and local smoothness regularizer for multi-view clustering. <em>NEUCOM</em>, <em>658</em>, 131564. (<a href='https://doi.org/10.1016/j.neucom.2025.131564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevailing techniques for multi-view subspace clustering (MVC) methods often depend on the assumption of low-rankness, which asserts that data can be effectively represented in a low-dimensional subspace. While these approaches capture the structure of the data globally and remove noise and redundancy, they all neglect local smoothness prior, which has been extensively used to reduce noise in the image field. Besides, existing techniques often depend on the tensor nuclear norm (TNN)to approximate the intrinsically non-convex tensor rank function. However, the TNN approach equates all singular values, which gives rise to excessive penalization of the principal rank components and ultimately leads to sub-optimal tensor representations. In response to these challenges, we introduce an innovative method called fused adaptive tensor Log-determinant and local smoothness regularizer (FATLLSR) for multi-view clustering. Specifically, we initially derive the self-expressive matrix for each view and subsequently integrate these matrices into a tensor. Then in order to simultaneously explore low-rankness and local smoothness prior, FATLLSR is designed and is used to constrain the obtained tensor. By using FATLLSR, we can not only relax tensor multi-rank constraint better than TNN but also utilize the local smoothness information hidden in multi-view data, making our method more robust to noise and redundancy. These techniques are integrated to constitute a unified model that is effectively handled using the augmented Lagrange multiplier (ALM). As demonstrated by its performance on different datasets, FATLLSR achieves outstanding clustering performance compared to the most advanced methods. The code is publicly available at https://github.com/wangfii/FATLLSR .},
  archive      = {J_NEUCOM},
  author       = {Fei Wang and Gui-Fu Lu},
  doi          = {10.1016/j.neucom.2025.131564},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131564},
  shortjournal = {Neurocomputing},
  title        = {Fused adaptive tensor log-determinant and local smoothness regularizer for multi-view clustering},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised multi-blind network for real image denoising via multivariate gaussian-poisson noise. <em>NEUCOM</em>, <em>658</em>, 131557. (<a href='https://doi.org/10.1016/j.neucom.2025.131557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The noise in real images exhibits more complex distributions than the synthetic noise and distinguishes across different scenarios. Furthermore, the scarcity of "clean-to-noisy" paired image datasets makes the current models difficult to denoise successfully. To address these challenges, we propose MGP-MBF M 2 ANet, a self-supervised multi-blind feature multi-modulation attention network based on multivariate Gaussian-Poisson noise prior for real image denoising. Firstly, we propose a multivariate Gaussian-Poisson distribution to construct noisy images that contain more complex pixel spatial positions and intensity correlations, which expand the training domain and improve the model’s ability to generalize across diverse real noisy images. Building on this, we implement a random sampling mechanism based on four-neighborhood similarity to construct "noise-noise" training pairs, effectively exploiting the statistical properties of local structures in noisy images, without relying on any clean reference image. During the network design phase, a multi-blind feature multi-modulation attention module successfully enhances the representation of local features, which introduces multi-masked strategy to force network to learn more information to address the challenge of feature identity mapping. Experimental results demonstrate that the proposed method effectively suppresses noise and recovers high-frequency details within an unsupervised learning paradigm, achieving superior performance in both objective evaluation metrics and subjective visual quality across multiple real-world datasets.},
  archive      = {J_NEUCOM},
  author       = {Hang Zhao and Zitong Wang and Xiaoli Zhang and Zhaojun Liu},
  doi          = {10.1016/j.neucom.2025.131557},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131557},
  shortjournal = {Neurocomputing},
  title        = {Self-supervised multi-blind network for real image denoising via multivariate gaussian-poisson noise},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interactive attention and contrastive learning for few-shot relation extraction. <em>NEUCOM</em>, <em>658</em>, 131551. (<a href='https://doi.org/10.1016/j.neucom.2025.131551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction is a critical task in natural language processing, often challenged by the problem of insufficient samples in real world scenarios. Therefore, studying few-shot relation extraction is of great significance. Currently, prototype networks and meta-learning-based parameter optimization are the mainstream methods to study this kind of problem. However, these methods still face sample confusion during classification, and the trained models are prone to overfitting. To solve these problems, this paper proposes a few-shot relation extraction method based on interactive attention. During the model training stage, we introduce two contrastive learning approaches to better capture sample features and reduce sample confusion. Contrastive learning strengthens the connections between instances and their corresponding relationship descriptions, thus improving relation extraction. In the testing phase, the model employs an attention mechanism to calculate the attention scores between the query set and the support set and employs a new classification layer to mitigate overfitting. We conducted experiments on two real-world few-shot relation extraction datasets, and the results demonstrate that our method achieved superior performance on both in-domain and cross-domain datasets, proving the effectiveness of the proposed approach. The code is available at https://github.com/xyzew/IACL.git .},
  archive      = {J_NEUCOM},
  author       = {Yan Li and Yao Wang and Zhaojie Wang and Wei Wang and Bailing Wang and Guodong Xin},
  doi          = {10.1016/j.neucom.2025.131551},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131551},
  shortjournal = {Neurocomputing},
  title        = {Interactive attention and contrastive learning for few-shot relation extraction},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAGNet: A multimodal knowledge-augmented graph network for early-stage misinformation detection. <em>NEUCOM</em>, <em>658</em>, 131533. (<a href='https://doi.org/10.1016/j.neucom.2025.131533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid proliferation of multimodal misinformation on social media, detecting such content has become increasingly challenging. Existing approaches often rely on flat or shallow fusion strategies, which fail to capture structured semantic interactions across modalities. Moreover, most methods lack controllable, task-relevant mechanisms for integrating external knowledge, limiting their adaptability to emerging misinformation. In this paper, we present MAGNet, a Multimodal Augmented Graph Network that models fine-grained features with LLM-enhanced contextual knowledge through a hierarchical graph attention framework. MAGNet constructs heterogeneous graphs with modality- and context-specific edge weights based on semantic and affective alignment, enabling progressive reasoning from local features to global representations. Extensive experiments on three real-world datasets demonstrate that MAGNet consistently outperforms strong baselines across multiple evaluation metrics. The results underscore the effectiveness of combining graph-based modeling, fine-grained fusion, and structured knowledge integration in developing scalable and robust solutions for multimodal misinformation detection.},
  archive      = {J_NEUCOM},
  author       = {Wang Jinghong and Yang Hongbo and Wang Xizhao and Wang Wei and Li Yanan},
  doi          = {10.1016/j.neucom.2025.131533},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131533},
  shortjournal = {Neurocomputing},
  title        = {MAGNet: A multimodal knowledge-augmented graph network for early-stage misinformation detection},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rotated graph similarity computation via graph transformer networks. <em>NEUCOM</em>, <em>658</em>, 131474. (<a href='https://doi.org/10.1016/j.neucom.2025.131474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph similarity learning has always been a prominent topic in various graph data analysis tasks, such as graph classification and graph similarity search. Recently, graph similarity learning methods with graph neural networks measure the similarity between graphs through node-level or graph-level embedding comparisons, but neglect to capture the complex information within data. Moreover, they employ cross-layer interactions (e.g., between each node of one graph and the other entire graph) for graph similarity learning, which are computationally expensive. In light of these limitations, this paper proposes a R otated G raph Sim ilarity computation ( RGSim ) approach for end-to-end computation of graph similarity between any pair of graph structure objects. This RGSim combines two strategies. Firstly, to provide a global summary of the graphs, RGSim utilizes graph transformer network and node selection strategy to map each node into the embedded vectors. It then employs the Softmax aggregation operator for multi-level union graph embedding. Secondly, RGSim employs Neural Tensor Network to learn the pseudo-similarity scores between graph pairs, defining pseudo-similarity scores as rotations in the complex vector space between graph pairs, aiming to reduce the computational overhead. Comprehensive experiments demonstrate that RGSim consistently outperforms state-of-the-art baseline models on both the graph–graph classification and graph–graph regression tasks, while also exhibiting superior efficiency.},
  archive      = {J_NEUCOM},
  author       = {Zhaoyao Yan and Cangfeng Ding and Lerong Ma and Lu Cao and Hao You},
  doi          = {10.1016/j.neucom.2025.131474},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131474},
  shortjournal = {Neurocomputing},
  title        = {Rotated graph similarity computation via graph transformer networks},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive reverse perturbation network for audio deepfake detection. <em>NEUCOM</em>, <em>658</em>, 131466. (<a href='https://doi.org/10.1016/j.neucom.2025.131466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing prevalence of audio deepfakes underscores the urgent need for advanced detection frameworks capable of identifying subtle synthetic artifacts. In response to this challenge, we propose an Adaptive Reverse Perturbation Network, a novel architecture that leverages partial reversal strategies on speech segments and incorporates hierarchical feature discrepancy analysis to enhance deepfake detection. Specifically, the proposed framework employs learnable reversal modules to capture phase discontinuities and spectral anomalies, and utilizes Prime-window reversal to reveal synthetic artifacts that emerge exclusively in reversed speech. Evaluations conducted on five benchmark datasets demonstrate the superior performance of the proposed method, achieving an equal error rate of 1.98 %, representing a 39.6 % improvement over previous systems, as well as a t-DCF of 0.237. Further analysis reveals an inverse correlation between language-specific weight similarity and detection accuracy. These results validate the effectiveness of the trainable differential convolution and reverse perturbation strategies in combating the evolving threat of audio deepfakes, and provide novel insights into phonological artifact patterns associated with synthetic speech.},
  archive      = {J_NEUCOM},
  author       = {Xue Ouyang and Chunhui Wang and Bin Zhao and Hao Li},
  doi          = {10.1016/j.neucom.2025.131466},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131466},
  shortjournal = {Neurocomputing},
  title        = {Adaptive reverse perturbation network for audio deepfake detection},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RDNet: Region specific iterative deformation with multi-scale attention for medical image registration. <em>NEUCOM</em>, <em>658</em>, 131455. (<a href='https://doi.org/10.1016/j.neucom.2025.131455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deformable medical image registration is essential for various clinical applications, including diagnosis, treatment planning, and disease monitoring. Although significant progress has been made with pyramid architecture, they often struggle to effectively capture the complex variations in deformation fields at feature maps with different resolutions. However, conventional skip connection designs inadequately address the asymmetric roles of moving and fixed images in deformation estimation, as they treat both images symmetrically without accounting for their distinct contributions to the alignment process. To address these challenges, we present RDNet, a learning-based dual-stream pyramid-based framework incorporating two key components: the Mapping Block (MB) and the Region Specific Layer (RSL). The MB module is carefully integrated into the fixed image skip connections to improve hierarchical feature alignment between the encoder and decoder. The high-level hierarchical semantic gap is efficiently minimized by MB through spatial and channel-wise attention methods, improving feature correspondence and registration accuracy. Additionally, to address the challenges caused by complex variations in the pyramid architecture, we present the RSL module in a multi-scale framework. This incorporation improves the capture of long-range dependencies specific to a region, resulting in more precise deformation estimation and improved registration accuracy while minimizing deformation loss. We conducted comprehensive experiments on two publicly available Brain MRI datasets, OASIS and LPBA40, and one Lung CT dataset to demonstrate that our proposed framework achieves state-of-the-art registration results.},
  archive      = {J_NEUCOM},
  author       = {Wenming Cao and Naeem Hussain and Zhiyue Yan},
  doi          = {10.1016/j.neucom.2025.131455},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131455},
  shortjournal = {Neurocomputing},
  title        = {RDNet: Region specific iterative deformation with multi-scale attention for medical image registration},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OMR-diffusion: Optimizing multi-round enhanced training in diffusion models for improved intent understanding. <em>NEUCOM</em>, <em>658</em>, 131452. (<a href='https://doi.org/10.1016/j.neucom.2025.131452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative AI has significantly advanced text-driven image generation, but it still faces challenges in producing outputs that consistently align with evolving user preferences and intents, particularly in multi-turn dialogue scenarios. In this research, we present a Visual Co-Alignment (VCA) framework that incorporates human-in-the-loop feedback, utilizing a well-trained reward model specifically designed to closely align with human preferences. Using a diverse multi-turn dialogue dataset, the framework applies multiple reward functions (such as diversity, consistency, and preference feedback) to refine the diffusion model through LoRA, effectively optimizing image generation based on user input. We also constructed multi-round dialogue datasets with prompts and image pairs that well-fit user intent. Experiments show the model achieved 508 wins in human evaluation, outperforming DALLE ⋅ 3 (463 wins) and others. It also achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL ⋅ E 3) and excels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments demonstrate the effectiveness of the proposed method over state-of-the-art baselines, with significant improvements in image consistency and alignment with user intent. The project page is https://tathataai.github.io/OMR-Diffusion/ .},
  archive      = {J_NEUCOM},
  author       = {Kun Li and Jianhui Wang and Yangfan He and Miao Zhang and Xueqian Wang},
  doi          = {10.1016/j.neucom.2025.131452},
  journal      = {Neurocomputing},
  month        = {12},
  pages        = {131452},
  shortjournal = {Neurocomputing},
  title        = {OMR-diffusion: Optimizing multi-round enhanced training in diffusion models for improved intent understanding},
  volume       = {658},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
