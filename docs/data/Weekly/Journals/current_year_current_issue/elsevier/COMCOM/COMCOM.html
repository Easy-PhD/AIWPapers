<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMCOM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comcom">COMCOM - 40</h2>
<ul>
<li><details>
<summary>
(2025). A scalable blockchain framework for IoT based on restaking and incentive mechanisms. <em>COMCOM</em>, <em>242</em>, 108317. (<a href='https://doi.org/10.1016/j.comcom.2025.108317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a scalable blockchain framework based on sidechain solution for the Internet of Things (IoT). Considering the low-trust models of existing sidechains, we present a restaking-based trust aggregation method for Proof of Stake (PoS). By allowing mainchain validators to duplicate their stake on the sidechain network, we enhance the cryptoeconomics security of the sidechain while reducing costs. Given the potential conflicts between risks and rewards of trust aggregation, and the challenges posed by the heterogeneity of IoT devices for quantitative analysis, we propose an incentive analysis framework based on contract. By analyzing the optimal strategies of different risk-preference validators, design differentiated contracts to promote incentive-compatible outcomes. Additionally, we account for the uncertainty in the distribution of sidechain validators and discuss optimal configurations under various conditions. To address potential collusion attacks, we introduce a quantifiable exemption mechanism to limit the security risks. Finally, numerical simulations verify the feasibility and effectiveness of our proposed method.},
  archive      = {J_COMCOM},
  author       = {Fang Ye and Zitao Zhou and Yifan Wang and Yibing Li},
  doi          = {10.1016/j.comcom.2025.108317},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108317},
  shortjournal = {Comput. Commun.},
  title        = {A scalable blockchain framework for IoT based on restaking and incentive mechanisms},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive retention-aware online video caching scheme in mobile edge computing. <em>COMCOM</em>, <em>242</em>, 108313. (<a href='https://doi.org/10.1016/j.comcom.2025.108313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current massive video requests have caused severe network congestion. To reduce transmission latency and improve user Quality of Experience (QoE), caching infrastructures are deployed closer to the edge. Nowadays, most caching systems tend to cache content with a high programming voltage to ensure a long retention time, which leads to significant cache damage. However, as new videos emerge every second, the rapidly changing popularity makes long retention time wasteful in terms of caching resource. Moreover, with the rise of emerging video formats (such as virtual reality content), the diverse requirements for transmission latency across various video categories make balancing user QoE more challenging. To tackle these challenges, we propose a joint optimization framework that balances user QoE and operational costs through video category recognition and adaptive retention time selection. First, we model user QoE as transmission latency cost and further formulate the optimization problem as a Markov Decision Process (MDP) to minimize the system cost. To solve the proposed problem, we design a two-step Double Deep Q-Network (DDQN)-based scheme. The scheme first determines the optimal retention time through unifying the process of action selection and state-value evaluation. Secondly, it makes replacement decisions according to the computed caching value of each content. By validating on three datasets, the experiments show that the proposed scheme outperforms the baseline algorithms in both cache hit rate and system cost.},
  archive      = {J_COMCOM},
  author       = {Guangzhou Liu and Zhen Qian and Guanghui Li},
  doi          = {10.1016/j.comcom.2025.108313},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108313},
  shortjournal = {Comput. Commun.},
  title        = {Proactive retention-aware online video caching scheme in mobile edge computing},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust-defined network: A panoramic P2P framework for distributed ledger systems. <em>COMCOM</em>, <em>242</em>, 108311. (<a href='https://doi.org/10.1016/j.comcom.2025.108311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology has revolutionized distributed ledger systems by offering superior security and transparency compared to traditional centralized systems. Despite its advantages, current blockchain systems face significant challenges such as network congestion, communication errors, and scalability issues, largely due to the limitations of blockchain peer-to-peer (P2P) protocols. These problems hinder the performance, reliability, and widespread adoption of blockchain technology. In this paper, we propose a Trust-Defined Network (TDN) framework designed to solve these challenges by reflecting the physical network information to the blockchain. This approach enables the precise diagnosis of existing blockchain P2P protocol limitations and facilitates the objective verification of new improvement measures. Our proposed framework supports various blockchain network environments, particularly Ethereum-based networks, and ensures enhanced network stability and performance. Through extensive simulations and real-world case studies in IoT-enabled blockchain applications, we demonstrate that TDN significantly reduces network congestion, improves transaction finality, and enhances the reliability of blockchain communication channels. These findings highlight the framework’s potential to optimize blockchain infrastructure, making it more robust for large-scale deployment and real-world applications.},
  archive      = {J_COMCOM},
  author       = {Taehoon Yoo and Kiseok Kim and Hwangnam Kim},
  doi          = {10.1016/j.comcom.2025.108311},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108311},
  shortjournal = {Comput. Commun.},
  title        = {Trust-defined network: A panoramic P2P framework for distributed ledger systems},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-reinforcement learning driven model architecture and algorithm optimization in intelligent driving task offloading. <em>COMCOM</em>, <em>242</em>, 108310. (<a href='https://doi.org/10.1016/j.comcom.2025.108310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of rapid development of intelligent driving technology, the amount of data generated by vehicles increases dramatically, while the bottleneck of storage and computation capacity of in-vehicle devices becomes more and more prominent, and task offloading becomes the key to improve the performance of intelligent driving systems. In this context, this paper proposes the MRL-ADTO algorithm, which innovatively applies meta-reinforcement learning (MRL) to the field of intelligent driving task offloading, optimizes the directed acyclic graph (DAG) synthesis logic and the task priority ranking algorithm, designs a neural network model based on the sequence to sequence (Seq2Seq) structure, and introduces the mechanism of multi-head attention at the same time. The experimental results show that MRL-ADTO can significantly reduce the task execution delay in multiple scenarios compared with the existing algorithms, and has obvious advantages in terms of training efficiency and convergence performance, providing an efficient and reliable solution for smart driving task offloading.},
  archive      = {J_COMCOM},
  author       = {Peiying Zhang and Jiamin Liu and Zhiyuan Ren and Lizhuang Tan and Neeraj Kumar and Konstantin Igorevich Kostromitin},
  doi          = {10.1016/j.comcom.2025.108310},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108310},
  shortjournal = {Comput. Commun.},
  title        = {Meta-reinforcement learning driven model architecture and algorithm optimization in intelligent driving task offloading},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed learning-based context-aware SFC deployment in the artificial intelligence of things. <em>COMCOM</em>, <em>242</em>, 108309. (<a href='https://doi.org/10.1016/j.comcom.2025.108309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Things (IoT) and Artificial Intelligence (AI) technologies, the Artificial Intelligence of Things (AIoT) has become a key driving force for realizing intelligent and automated applications. The deployment of Service Function Chains (SFCs) is crucial in dynamic AIoT environments, where efficiently and flexibly deploying SFCs to meet real-time application demands is a research focus. However, existing SFC deployment methods often face challenges such as dynamic variations and uncertainty in contextual information, resource allocation inefficiencies, and limited adaptability to changing network conditions. To address these issues, we propose a learning-based context-aware dynamic SFC deployment method tailored for AIoT environments. Specifically, we introduce an attention-based contextual feature extraction method to capture dynamic changes (e.g., link latency variations) and prioritize key contextual information, improving the rate of served requests by 17.90% (69.60% vs. 59.03% for MADDPG) and enhancing the flexibility of SFC deployment decisions. Additionally, to address resource allocation bottlenecks and adaptability challenges in SFC deployment, we propose a distributed learning-based context-aware approach that uses collaborative learning and periodic updates (every 200 ms) to adjust SFC deployment strategies in response to topology changes and load variations and optimize system performance. Extensive experimental results demonstrate the efficacy of the proposed algorithm. Numerical results demonstrate that our algorithm reduces SFC deployment latency by 8% (46 ms vs. 50 ms for MADDPG), achieves 98.3% computational resource utilization, processes 211 Mbit/s service data volume, and improves adaptability to network changes, as validated in simulations.},
  archive      = {J_COMCOM},
  author       = {Wenlin Cheng and Xingwei Wang and Fuliang Li and Bo Yi and Qiang He and Chuangchuang Zhang and Chengxi Gao and Min Huang},
  doi          = {10.1016/j.comcom.2025.108309},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108309},
  shortjournal = {Comput. Commun.},
  title        = {Distributed learning-based context-aware SFC deployment in the artificial intelligence of things},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-feature fusion approach for physical layer authentication in LEO satellites. <em>COMCOM</em>, <em>242</em>, 108308. (<a href='https://doi.org/10.1016/j.comcom.2025.108308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial information networks (SINs) have emerged as a means to enhance the expanse and dependability of communication and data transmission services. SINs rely on satellite systems to provide these services, among which low earth orbit (LEO) satellites are widely concerned because of their advantages of low orbital altitude, small network transmission delay, small path loss, and high signal strength. However, due to the frequent switching of communication links between LEO satellites and the ground, the authentication mechanism of the ground users to the satellites is vulnerable to spoofing attacks, and the traditional upper layer authentication method based on encryption usually requires a lot of overhead and delay. In this case, the lightweight physical layer authentication (PLA) mechanism utilizes the inherent distinctiveness and unpredictable nature of channel physical properties, serving as a vital application in SINs for ensuring authentication. Therefore, our work introduces a PLA method incorporating multi-feature integration, aimed at delivering effective identity verification tailored for LEO satellites. The approach employs doppler frequency shift (DS), angles of arrival (AOAs), and received power (RP) features, fusing an support vector machine (SVM) classifier, to distinguish between legal and illegal satellites in different simulation scenarios. The satellite toolkit (STK) is used to collect data from the actual orbit of satellites and assess the efficacy of the scheme. The findings indicate that the scheme offers enhanced authentication capabilities.},
  archive      = {J_COMCOM},
  author       = {Rongjun Yan and Fan Jia},
  doi          = {10.1016/j.comcom.2025.108308},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108308},
  shortjournal = {Comput. Commun.},
  title        = {A multi-feature fusion approach for physical layer authentication in LEO satellites},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating blockchain with IoT: Evaluating the feasibility of lightweight bitcoin wallets on resource-constrained devices. <em>COMCOM</em>, <em>242</em>, 108300. (<a href='https://doi.org/10.1016/j.comcom.2025.108300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of blockchain technology with IoT architectures holds immense potential for advancing application design and enhancing security properties. However, the resource constraints typically present in IoT devices pose a challenge. This paper explores the feasibility of running a lightweight Bitcoin wallet on IoT devices and identifies the minimum requirements for their successful operation. A review of the literature is used to identify existing integration architectures and derive the wallet needs. The study evaluates performance metrics such as execution time, memory usage, network data transmission, and power consumption to determine the feasibility of deploying these architectures.},
  archive      = {J_COMCOM},
  author       = {Mohsen Rahmanikivi and Cristina Pérez-Solà and Víctor Garcia-Font},
  doi          = {10.1016/j.comcom.2025.108300},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108300},
  shortjournal = {Comput. Commun.},
  title        = {Integrating blockchain with IoT: Evaluating the feasibility of lightweight bitcoin wallets on resource-constrained devices},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-rate planning in self-powered wireless multi-hop D2D settings under stochasticity: A scenario-based iterative optimization approach. <em>COMCOM</em>, <em>242</em>, 108299. (<a href='https://doi.org/10.1016/j.comcom.2025.108299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-hop Device-to-Device (D2D) communications are emerging as the foundation for numerous compelling 6G applications, enabling seamless information flow between distributed nodes. In the context of such uncertain wireless multi-hop D2D settings, jointly optimizing source data rates, routing, and transmission power decisions is both an essential task and a highly complex problem, particularly due to uncertainties introduced by the wireless channel states and the energy harvesting processes on the nodes. In the current literature, this problem is mostly tackled in a future agnostic sense, and/or using specific distributions to model the uncertainties. In contrast, in this paper, we compute a future energy and resource allocation plan of the network’s operation, using scenario-based optimization techniques to account for stochasticities. Scenarios can model generic distributions of uncertain quantities in a tractable manner. The formulated problem is inherently non-convex and to solve it, we propose CoNetPlan-E, a heuristic iterative method that at each iteration solves appropriately parameterized convex approximations of the original problem. We prove that CoNetPlan-E converges under realistic assumptions, while ensuring that the obtained solution at convergence is feasible for the original non-convex problem. Numerical evaluations showcase the effectiveness of the proposed method compared to existing baseline solutions, while considering three levels of increasing network topology complexity. Importantly, CoNetPlan-E is superior with respect to scalability and runtime while leading to close-to-optimal solutions as these are determined by the standard non-convex solver Ipopt.},
  archive      = {J_COMCOM},
  author       = {Georgia Stavropoulou and Eleni Stai and Maria Diamanti and Symeon Papavassiliou},
  doi          = {10.1016/j.comcom.2025.108299},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108299},
  shortjournal = {Comput. Commun.},
  title        = {Source-rate planning in self-powered wireless multi-hop D2D settings under stochasticity: A scenario-based iterative optimization approach},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed denial of service attack analysis and mitigation for MQTTv5 shared subscription. <em>COMCOM</em>, <em>242</em>, 108298. (<a href='https://doi.org/10.1016/j.comcom.2025.108298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sixth-generation (6G) networks will feature Massive IoT (M-IoT) deployments with a huge number of interconnected devices, enabling fast and reliable IoT applications. To address scalability and enhance message delivery, MQTTv5 introduces the shared subscription mechanism. However, the increased interconnectivity amplifies security vulnerabilities, posing significant risks with potentially severe consequences. In light of these challenges, this work aims to conduct a security-focused analysis of the shared subscription feature. Our study highlights the potential extent of damage from such an attack, which can potentially lead to indefinite starvation among legacy subscribers, and proposes a countermeasure to mitigate its impact. Additionally, to provide comprehensive security to the proposed mitigation mechanism, we design an Authenticated Encryption with Associated Data (AEAD)-based protection to counteract external malicious entities as well as an attacker detection mechanism based on a trust-based approach combined with the z-score statistical method to protect the proposed mitigation against internal attackers. These countermeasures are designed to accommodate the lightweight nature of MQTT and are characterized by a low protocol footprint while effectively mitigating the impact of the attack. Through an extensive experimental campaign, we tested this solution under real IoT traffic patterns to demonstrate its effectiveness and capability to restore the performance of the MQTT system targeted by the discovered attack.},
  archive      = {J_COMCOM},
  author       = {Graziano Rizzo and Mattia Giovanni Spina and Floriano De Rango},
  doi          = {10.1016/j.comcom.2025.108298},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108298},
  shortjournal = {Comput. Commun.},
  title        = {Distributed denial of service attack analysis and mitigation for MQTTv5 shared subscription},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying reinforcement learning in slotted LoRaWAN: From concept to implementation. <em>COMCOM</em>, <em>242</em>, 108297. (<a href='https://doi.org/10.1016/j.comcom.2025.108297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Low Power Wide Area Networks (LPWANs) are increasingly adopted for Internet of Things (IoT) applications, they face significant challenges related to interference and scalability, which can lead to high collision rates and reduced network throughput. This paper presents a novel approach to enhancing the performance of LoRaWAN, one of the dominant LPWAN protocols, by leveraging Reinforcement Learning (RL). The proposed solution introduces a synchronization framework designed to operate under LoRaWAN principles, coupled with a low-cost, on-device RL mechanism that autonomously mitigates collisions. Through extensive simulations and real-world experiments, the effectiveness of the RL approach is demonstrated, showing an over 30% improvement in terms of packet delivery ratio (PDR) compared to traditional multiple access methods such as Pure-Aloha, Slotted-Aloha, and Carrier Sense Multiple Access (CSMA). Additionally, open-source implementations for both simulation and experimental validation are provided, ensuring reproducibility and facilitating further research in this domain.},
  archive      = {J_COMCOM},
  author       = {Dimitrios Zorbas and Sultan Kasenov and Kamila Salimzhanova and Dias Gaziz and Timur Ismailov and Batyrkhan Baimukhanov},
  doi          = {10.1016/j.comcom.2025.108297},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108297},
  shortjournal = {Comput. Commun.},
  title        = {Applying reinforcement learning in slotted LoRaWAN: From concept to implementation},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on grouping strategy for NOMA downlink based on pointer network. <em>COMCOM</em>, <em>242</em>, 108296. (<a href='https://doi.org/10.1016/j.comcom.2025.108296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the application of Non-Orthogonal Multiple Access (NOMA) technology in 5G and beyond communication systems, how to effectively group users to optimize power allocation has become a key challenge. This paper proposes a user grouping method based on a Pointer Network, which efficiently extracts user location information through embedding layers, encoder–decoder structures, and attention mechanisms, achieving the goal of precise grouping decisions and power optimization. The embedding layer maps users’ two-dimensional coordinates into a high-dimensional space, enhancing the model’s spatial awareness. The encoder–decoder structure, combined with Long Short-Term Memory (LSTM) networks and attention mechanisms, captures the spatiotemporal dependencies between users and dynamically selects the optimal path during the grouping process. Experimental results show that when users are located 300 meters from the base station, the recognition accuracy of a 4-user grouping reaches 94.85%, and that of a 6-user grouping reaches 89.3%. The method also demonstrates strong robustness under multipath fading channels and low signal-to-noise ratio conditions. Compared to random grouping methods, the proposed grouping strategy exhibits better adaptability and scalability in complex communication environments, significantly reducing power consumption, and providing new technical support for resource allocation and energy management in NOMA systems.},
  archive      = {J_COMCOM},
  author       = {Lingfeng Wu and Rui Zhu and Yarong Chen and Peng Chu and Juan Tian and Jiuxiao Cao},
  doi          = {10.1016/j.comcom.2025.108296},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108296},
  shortjournal = {Comput. Commun.},
  title        = {Research on grouping strategy for NOMA downlink based on pointer network},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Private networks: Evolution, ecosystem, use cases, architecture, spectrum, and deployment challenges. <em>COMCOM</em>, <em>242</em>, 108295. (<a href='https://doi.org/10.1016/j.comcom.2025.108295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Private networks have reshaped enterprise communications by providing unmatched control, security, and tailored solutions for various industries. This paper presents an in-depth survey of private networks, covering their evolution, current landscape, and future outlook. Key topics include the use cases, architecture, spectrum management, and deployment strategies. The study examines the transition from private 4G/LTE to private 5G networks, fueled by demands for higher data throughput and ultra-low latency across sectors. It highlights the advantages of private 5G over public mobile networks (MNOs) and Wi-Fi, with a special focus on spectrum sharing as a means to optimize frequency use. Additionally, the paper reviews global spectrum allocations for private 5G, providing an overview of regulatory frameworks and available frequency bands across countries. It also explores future prospects, including private 6G networks and emerging spectrum technologies. Key challenges such as high deployment costs, interoperability issues, and security concerns are discussed alongside potential solutions. Through this comprehensive analysis, the paper aims to provide valuable insights for researchers, practitioners, and policymakers in the field of private networks.},
  archive      = {J_COMCOM},
  author       = {Onur Sahin and Vanlin Sathya and Mehmet Yavuz},
  doi          = {10.1016/j.comcom.2025.108295},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108295},
  shortjournal = {Comput. Commun.},
  title        = {Private networks: Evolution, ecosystem, use cases, architecture, spectrum, and deployment challenges},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simplifying distributed application deployment at the edge through software-defined overlay networks. <em>COMCOM</em>, <em>242</em>, 108294. (<a href='https://doi.org/10.1016/j.comcom.2025.108294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for low latency, bandwidth efficiency, and privacy has driven the deployment of distributed applications to the network edge. However, edge environments introduce concrete challenges such as limited infrastructure control, constrained connectivity due to NAT or firewalls, and the heterogeneity of devices and network conditions. This paper introduces a software-defined overlay networking (SDON) middleware that addresses these issues by simplifying the development and deployment of edge applications through centralized control and dynamic overlay management. SDON allows applications to define high-level requirements, such as node and link characteristics and the network topology. These requirements are translated into device-specific configurations and enforced across suitable edge devices. We implemented our SDON middleware as a fully functional software and evaluated it in two edge computing use cases: i) routing for video streaming across middleboxed edge devices and ii) computation offloading on heterogeneous edge devices. Our results show that deployments via SDON, with centrally enforced optimizations, improve application performance by reducing mean streaming latency by 20 % and computation times by 22 %.},
  archive      = {J_COMCOM},
  author       = {Heiko Bornholdt and Kevin Röbert and Stefan Schulte and Janick Edinger and Mathias Fischer},
  doi          = {10.1016/j.comcom.2025.108294},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108294},
  shortjournal = {Comput. Commun.},
  title        = {Simplifying distributed application deployment at the edge through software-defined overlay networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving satellite network efficiency with terminal traffic prediction and SQP-SRA algorithm. <em>COMCOM</em>, <em>242</em>, 108293. (<a href='https://doi.org/10.1016/j.comcom.2025.108293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the low resource utilization in satellite networks caused by heterogeneous regional traffic demands, this paper proposes a resource allocation strategy for LEO satellite internet based on terminal traffic prediction. An improved LSTM-GRU hybrid model is developed using real-world datasets to forecast ground traffic, accounting for periodic patterns and weather effects. A leaseable EOSN differentiated transmission framework is designed to enable targeted resource allocation and inter-satellite leasing, enhancing network coverage. To optimize data transmission ratios, user bandwidth, and service pricing, we introduce a sequential quadratic programming-based satellite resource allocation (SQP-SRA) algorithm that balances latency and energy consumption. Compared with LSTM, GRU, Transformer, and wavelet neural networks, the proposed model reduces traffic prediction error by approximately 26%. Simulation results demonstrate that, relative to the DDTOA, FCFS, and TOMRA algorithms, the proposed strategy improves user benefits by approximately 60% and enhances satellite service provider revenues by approximately 80%.},
  archive      = {J_COMCOM},
  author       = {Liangang Qi and Enqiang Wang and Tianfang Xu and Yuan Zhu and Yun Zhao},
  doi          = {10.1016/j.comcom.2025.108293},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108293},
  shortjournal = {Comput. Commun.},
  title        = {Improving satellite network efficiency with terminal traffic prediction and SQP-SRA algorithm},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved AVB-aware scheduling of time-triggered traffic in time-sensitive networks. <em>COMCOM</em>, <em>242</em>, 108292. (<a href='https://doi.org/10.1016/j.comcom.2025.108292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-Sensitive Networking (TSN) provides deterministic services for diverse traffic types within a unified network. Among them, time-triggered (TT) traffic requires stringent timing guarantees, typically achieved through precise scheduling using Gate Control Lists (GCLs) in Time-Aware Shapers (TASs). However, most existing studies primarily focus on TT scheduling, often overlooking its impact on Audio Video Bridging (AVB) traffic, which demands worst-case delay (WCD) guarantees. This paper proposes an improved AVB-aware scheduling approach for TT traffic that enhances AVB performance without compromising TT schedulability. A rigorous network calculus analysis identifies two critical factors influencing WCD of AVB traffic: the maximum TT window length and the minimum relative offset between adjacent TT windows. Building on these insights, we develop a lightweight objective function for TT flow scheduling, enabling efficient evaluation of the impact on AVB traffic. This objective function is embedded into a Greedy Randomized Adaptive Search Procedure (GRASP)-based scheduling framework, further enhanced by a flow sorting strategy and a flexible local search mechanism that prioritizes high-impact TT flows and adaptively escapes local optima. Simulation results demonstrate that the proposed method significantly improves AVB performance and reduces runtime overhead, while consistently maintaining full TT schedulability across diverse TSN scenarios.},
  archive      = {J_COMCOM},
  author       = {Meng Wang and Yiqin Lu},
  doi          = {10.1016/j.comcom.2025.108292},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108292},
  shortjournal = {Comput. Commun.},
  title        = {An improved AVB-aware scheduling of time-triggered traffic in time-sensitive networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing vertical handover of energy efficient sleep mode schemes in heterogeneous networks. <em>COMCOM</em>, <em>242</em>, 108291. (<a href='https://doi.org/10.1016/j.comcom.2025.108291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous networks (HetNets) are a promising solution for the growing traffic demands of 5G. However, the continuous construction of small base stations (SBSs) increases the number of vertical handovers, which is closely related to a decrease in quality of service (QoS) due to the challenges in handover management. Therefore, many studies calculate the vertical handover rate using simulation-based or numerical methods. The sleep mode schemes, which dynamically put some SBSs into sleep mode, aim to address the concerns of excessive power consumption and also reduce the number of vertical handovers. Sleep modes are cost-effective and have become one of the popular methods for reducing energy consumption in HetNets. However, there is currently no model to analyze the number of vertical handovers when sleep modes are applied, making it difficult for internet service providers (ISPs) to estimate the impact of vertical handovers. In this paper, we analyze the number of vertical handovers in sleep mode schemes for heterogeneous networks and calculate the energy savings of three different schemes. The average errors of our proposed mathematical equations are less than 5%.},
  archive      = {J_COMCOM},
  author       = {Ting-Yu Lin and Hao-Zhong Zheng and Chun-Hao Yang and Fang-Yi Lee and Chia-Heng Tu and Meng-Hsun Tsai},
  doi          = {10.1016/j.comcom.2025.108291},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108291},
  shortjournal = {Comput. Commun.},
  title        = {Analyzing vertical handover of energy efficient sleep mode schemes in heterogeneous networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network traffic classification through high-order L-moments and multi-objective optimization. <em>COMCOM</em>, <em>242</em>, 108290. (<a href='https://doi.org/10.1016/j.comcom.2025.108290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of encrypted and dynamic network traffic poses significant challenges to traditional traffic analysis methods, underscoring the need for robust and scalable solutions. Statistical approaches like L-moments have demonstrated exceptional potential in characterizing traffic flows, offering reduced sensitivity to outliers and the ability to capture higher-order distributional properties with minimal data. Building on previous work by the authors, this study introduces significant enhancements to the L-moment-based methodology for flow analysis and classification, specifically addressing limitations in feature selection and sample size requirements, aspects crucial for achieving deployable configurations in high-performance network environments. Key contributions include the integration of the fifth-order L-moment ratio ( τ 5 ) for enriched traffic representation and a multi-objective optimization framework based on a multi-objective evolutionary algorithm that balances competing goals: minimizing flow features selected for flow classification, reducing sample sizes for L-moment estimation, and maximizing classification quality. The enhanced methodology was applied to the CIC-DDoS2019 dataset, previously used in the authors’ earlier work, enabling direct comparison. Results show a reduction in sample size requirements from 200 to as few as 10, while simultaneously improving classification accuracy and selecting minimal features. These findings demonstrate the scalability and effectiveness of the proposed framework, designed for resource-constrained environments in Next-Generation Networks (NGNs), and make it publicly available for reproducibility and future research.},
  archive      = {J_COMCOM},
  author       = {Jesús Galeano-Brajones and Mihaela I. Chidean and Francisco Luna and Jesús Calle-Cancho and Javier Carmona-Murillo},
  doi          = {10.1016/j.comcom.2025.108290},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108290},
  shortjournal = {Comput. Commun.},
  title        = {Network traffic classification through high-order L-moments and multi-objective optimization},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A blockchain solution for decentralized training in machine learning for IoT. <em>COMCOM</em>, <em>242</em>, 108289. (<a href='https://doi.org/10.1016/j.comcom.2025.108289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of Internet of Things (IoT) devices and applications has led to an increased demand for advanced analytics and machine learning techniques capable of handling the challenges associated with data privacy, security, and scalability. Federated learning (FL) and blockchain technologies have emerged as promising approaches to address these challenges by enabling decentralized, secure, and privacy-preserving model training on distributed data sources. In this paper, we present a novel IoT solution that combines the incremental learning vector quantization algorithm (XuILVQ) with Ethereum blockchain technology to facilitate secure and efficient data sharing, model training, and prototype storage in a distributed environment. Our proposed architecture addresses the shortcomings of existing blockchain-based FL solutions by reducing computational and communication overheads while maintaining data privacy and security. We assess the performance of our system through a series of experiments, showing its potential to enhance the accuracy and efficiency of machine learning tasks in IoT settings.},
  archive      = {J_COMCOM},
  author       = {Carlos Beis-Penedo and Francisco Troncoso-Pastoriza and Rebeca P. Díaz-Redondo and Ana Fernández-Vilas and Manuel Fernández-Veiga and Martín González Soto},
  doi          = {10.1016/j.comcom.2025.108289},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108289},
  shortjournal = {Comput. Commun.},
  title        = {A blockchain solution for decentralized training in machine learning for IoT},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLoV2T: A fine-grained malicious traffic classification method based on federated learning for AIoT. <em>COMCOM</em>, <em>242</em>, 108288. (<a href='https://doi.org/10.1016/j.comcom.2025.108288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Artificial Intelligence of Things (AIoT), the network security risks associated with AIoT have surged, making precise fine-grained malicious traffic classification (MTC) technology essential, but the reliance on large datasets raises privacy concerns. Federated Learning (FL) offers a privacy-preserving alternative, but existing FL-based solutions still suffer from suboptimal classification accuracy, limited terminal resources, and the non-independent and identically distributed (non-IID) IoT data that hinder effective global model aggregation. To address these issues, this paper introduces FLoV2T — a FL-based fine-grained MTC method for AIoT. To improve classification performance, we first employ a pretrained Vision Transformer (ViT) to extract discriminative features by visualizing raw network traffic as images, thereby tackling the problem of inadequate feature representation. To alleviate the burden of resource constraints and high communication costs, we then implement a local parameter fine-tuning mechanism based on Low-Rank Adaptation (LoRA), significantly reducing the parameter for model learning and communication at the edge. Furthermore, to counteract the model bias towards clients’ non-IID data on model aggregation, we design a regularized parameter aggregation strategy to enhance global model robustness. Experimental results show that FLoV2T achieves an average accuracy of 97.26% and an F1 score of 96.99%, surpassing the baseline by 10.94% and 11.47%. Moreover, LoRA reduces parameter count by approximately 64 times while maintaining high classification performance, and under non-IID conditions, overall performance reaches an average accuracy of 96.17% and an average F1 score of 95.81%, underscoring FLoV2T’s potential in future AIoT communication networks.},
  archive      = {J_COMCOM},
  author       = {Fanyi Zeng and Chen Xu and Dapeng Man and Junhui Jiang and Wu Yang},
  doi          = {10.1016/j.comcom.2025.108288},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108288},
  shortjournal = {Comput. Commun.},
  title        = {FLoV2T: A fine-grained malicious traffic classification method based on federated learning for AIoT},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ILoRa: Interleaving-driven neural network for rate adaptation in LoRa communications. <em>COMCOM</em>, <em>242</em>, 108287. (<a href='https://doi.org/10.1016/j.comcom.2025.108287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate adaptation in LoRa communications is crucial for improving the channel throughput by adjusting the data rate according to varying channel conditions. Existing methods typically operate at the packet or symbol level, which limits their ability to achieve fine-grained rate adaptation. In this paper, we propose ILoRa, an Interleaving-driven partial transmission method that automatically adjusts transmission rates according to real-time channel conditions. To be specific, we first introduce intra-symbol interleaving that leverages a progressive inorder traversal method to determine the transmission order within a symbol. Then inter-symbol interleaving is applied to coordinate the order across symbols. To manage the interleaving-induced partial transmission and improve communication performance under noisy conditions, we employ a multi-task convolutional recurrent neural network (MT-CRNN). This network leverages advanced data augmentation methods to further enhance channel robustness: time-spectral augmentation to mitigate information loss and synthetic noisy data to simulate various channel conditions. Extensive experimental results demonstrate that ILoRa significantly enhance transmission efficiency while maintaining reliable performance even in challenging environments.},
  archive      = {J_COMCOM},
  author       = {Xiaoke Qi and Haiyang Li and Dian Zhang and Lu Wang},
  doi          = {10.1016/j.comcom.2025.108287},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108287},
  shortjournal = {Comput. Commun.},
  title        = {ILoRa: Interleaving-driven neural network for rate adaptation in LoRa communications},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning based interference optimization for coordinated beamforming in ultra-dense wi-fi networks. <em>COMCOM</em>, <em>242</em>, 108286. (<a href='https://doi.org/10.1016/j.comcom.2025.108286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation Wi-Fi networks are expected to have an ultra-dense deployment of access points (APs), thus, interference from overlapping basic service sets (OBSSs) poses challenges for interference management. Wi-Fi 8 aims at mitigating such interference using multi-access point coordination (MAPC). One of the MAPC variants is coordinated beamforming (Co-BF), where neighboring APs direct their signals towards specific users. Besides beam steering, APs can also perform null steering, which is more complex but can bring greater performance gains. In this paper, we present a centralized approach named intelligent null steering by reinforcement learning (IntelliNull), designed to reduce interference from neighboring transmitters by coordinated nulling while maximizing the signal quality at each station. We show that training the beam and null steering mechanism with a deep deterministic policy gradient (DDPG), it is possible to steer beams toward associated stations while intelligently nulling the most destructive interference from OBSS rather than nulling random interference directions. This method enhances communication between the AP and neighboring stations by reducing channel access contention, enabling transmissions at full power, and reducing worst-case latency. The proposed IntelliNull agent continuously adapts to changes in the network environment, including node mobility using channel state information (CSI) collected in real-time. We also compare our IntelliNull, which is based on beamforming plus nulling, with the baseline which is based on beamforming only. Our results demonstrate that IntelliNull outperforms the baseline by effectively mitigating interference, leading to higher throughput and better signal-to-interference-plus-noise ratio (SINR), especially in dense deployment scenarios where beamforming alone fails to sufficiently suppress OBSS interference.},
  archive      = {J_COMCOM},
  author       = {Jamshid Bacha and Anatolij Zubow and Szymon Szott and Katarzyna Kosek-Szott and Falko Dressler},
  doi          = {10.1016/j.comcom.2025.108286},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108286},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning based interference optimization for coordinated beamforming in ultra-dense wi-fi networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient security service function chaining based on federated learning in edge networks. <em>COMCOM</em>, <em>242</em>, 108285. (<a href='https://doi.org/10.1016/j.comcom.2025.108285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating demand for network services has prompted the evolution of Service Function Chaining (SFC) within 6G networks to deliver sophisticated, customized services while ensuring robust cybersecurity. This paper introduces an efficient and secure framework for SFC in Mobile Edge Computing (MEC) environments, termed the Federated Learning-based SFC (FL-SFC), which integrates SFC, MEC, and Federated Learning (FL) to enhance service policy decision-making and safeguard user privacy. The FL-SFC framework enables dynamic updating of service policies and optimizes communication efficiency. We propose an anomaly detection model, CNN-GRU, which combines Convolutional Neural Networks (CNNs) and Gated Recurrent Units (GRUs) to significantly improve anomaly detection performance at the network edge. Additionally, to address the high communication costs associated with service policy models, we have designed a model compression mechanism leveraging sparsification and quantization techniques, which substantially reduces communication overhead during model training. Simulation experiments demonstrated the superiority of the FL-SFC framework and the CNN-GRU model in detection performance over existing methods. Results indicate that our model excels in accuracy, precision, recall, and F1-score while significantly reducing the number of communication bits, thereby validating the effectiveness of our approach.},
  archive      = {J_COMCOM},
  author       = {Yunjian Jia and Jian Yu and Liang Liang and Fang Fang and Wanli Wen},
  doi          = {10.1016/j.comcom.2025.108285},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108285},
  shortjournal = {Comput. Commun.},
  title        = {Efficient security service function chaining based on federated learning in edge networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correctness of flow migration across network function instances. <em>COMCOM</em>, <em>242</em>, 108284. (<a href='https://doi.org/10.1016/j.comcom.2025.108284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Functions (NFs) improve the safety and efficiency of networks. Flows traversing NFs may need to be migrated from a source NF instance (sNF) to a destination NF instance (dNF) to balance load, conserve energy, etc. When NFs are stateful, the information stored on an sNF per flow must be migrated to the corresponding dNF before the flow is migrated, to avoid problems of consistency. Our main contribution is to examine what it means to correctly migrate flows from a stateful NF instance. We define the property of Weak-O, where only the state information required for packets to be correctly forwarded from an sNF is migrated first to the corresponding dNF, while the remaining states are eventually migrated. Weak-O can be preserved without buffering or dropping packets, unlike existing algorithms. We propose an algorithm that preserves Weak-O and prove its correctness. Even though this may cause packet re-ordering, we experimentally demonstrate that the goodputs with and without migration are comparable when the old and new paths have the same delays and bandwidths. This is also true when the new path has larger bandwidth or at most 5 times longer delays. Thus flow migration without buffering is practical, contrary to what was thought before. We also prove that no criterion stronger than Weak-O can be preserved in a flow migration system that requires no buffering or dropping of packets and eventually synchronizes its states.},
  archive      = {J_COMCOM},
  author       = {Ranjan Patowary and Gautam Barua and Radhika Sukapuram},
  doi          = {10.1016/j.comcom.2025.108284},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108284},
  shortjournal = {Comput. Commun.},
  title        = {Correctness of flow migration across network function instances},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDR: Stackelberg-based deep reinforcement learning for multi-skill spatiotemporal task allocation in AIoT systems. <em>COMCOM</em>, <em>242</em>, 108283. (<a href='https://doi.org/10.1016/j.comcom.2025.108283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In AIoT-based multi-skill environments, task allocation is a complex process that involves multiple constraints and worker acceptance rates. However, existing studies often overlook worker acceptance rates and fail to properly balance the interests of both workers and requesters. To address this, we propose SDR, a system based on a dual Dueling DQN model in deep reinforcement learning, designed to maximize the long-term utility of all participants while considering user acceptance rates and demand constraints. SDR introduces targeted enhancements in state, action, and reward design to balance acceptance rates with spatiotemporal and skill constraints, optimizing both immediate and long-term task allocation performance. To resolve conflicts of interest, we integrate Pareto optimization into the Q-value computation and action selection. For scenarios where interests align, we adopt Stackelberg game theory to refine the reward mechanism. Extensive simulations on both synthetic and real-world datasets validate the effectiveness of our approach in improving task allocation and pricing strategies.},
  archive      = {J_COMCOM},
  author       = {Yu Li and Fengya Yin and Yihao Zheng and Wenjian Xu and Jung Yoon Kim and Zhe Peng},
  doi          = {10.1016/j.comcom.2025.108283},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108283},
  shortjournal = {Comput. Commun.},
  title        = {SDR: Stackelberg-based deep reinforcement learning for multi-skill spatiotemporal task allocation in AIoT systems},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive handover for task offloading in UAVs. <em>COMCOM</em>, <em>242</em>, 108282. (<a href='https://doi.org/10.1016/j.comcom.2025.108282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) are usually deployed alongside Internet of Things (IoT) devices in smart city applications, particularly for critical tasks such as disaster management that require continuous service. UAVs often handle resource-intensive and sensitive tasks through offloading, but unexpected task interruptions due to UAV dropouts can generate safety risks and increase costs. Although existing approaches in the literature have already addressed proactive handovers to mitigate such disruptions, their primary focus is on communication issues arising from UAV movement and are unable to handle offloading related issues. In this paper, we include in our model, in addition to communication, factors such as energy, computation requirements, and dynamic environmental conditions (e.g., wind speed and incentive), pushing toward a comprehensive solution for UAV task offloading and resource allocation. In fact, we formulate our problematic as a Markov game, which we solve using a Multi Agent Deep Q Network (MADQN). In our experiments, we assessed our approach using a federated learning scenario to illustrate its effectiveness in a realistic distributed application setting against several baselines from the state of the art. Results showed that our approach outperforms its peers in terms of system utility, and tradeoff between cost and dropout rates, leading to an improved handover management of computational and energy resources in UAV-IoT based systems. In fact, it reduces the dropout rate by approximately 45% compared to the second-best baseline, leading to a 2% improvement in model accuracy and a 50% reduction in deployment costs.},
  archive      = {J_COMCOM},
  author       = {Mohammed Riyadh Abdmeziem and Amina Ahmed Nacer and Soumeya Demil},
  doi          = {10.1016/j.comcom.2025.108282},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108282},
  shortjournal = {Comput. Commun.},
  title        = {Proactive handover for task offloading in UAVs},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance model and system optimization of an energy-saving strategy based on adaptive service rate tuning in cloud data centers with micro-burst traffic. <em>COMCOM</em>, <em>242</em>, 108281. (<a href='https://doi.org/10.1016/j.comcom.2025.108281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing competition in cloud market, reducing operating costs and improving Quality of Service (QoS) are two of the key issues that cloud vendors need to consider. In order to reduce the power consumption while mitigating the negative impact of micro-burst traffic in Cloud Data Centers (CDCs) on performance, and make cloud vendors more competitive, we design an Energy-saving Strategy based on Sleep and Adaptive Service-rate Tuning (ES-SAST) in this paper. We model the arrivals of the cloud task requests as an environment-dependent R -phase Markov Arrival Process (MAP ( R ) ), and we establish a multi-server synchronous multi-vacation queue with adaptive service rate tuning. We construct a four-dimensional Markov chain to analyze the queue, and we calculate some measures to evaluate the energy efficiency and QoS in the steady state. Then we develop an objective function composed of three performance measures. Finally, we propose an Improved Fire Hawk Optimizer (IFHO) with multi-strategy integration, and IFHO jointly optimizes two system parameters. An empirical study shows that IFHO chooses a lower system expected cost, where the power consumption of the system falls by 3%, the latency of tasks decreases by 19%, and the loss rate of the system reduces by 37%, on average.},
  archive      = {J_COMCOM},
  author       = {Xuena Yan and Shunfu Jin},
  doi          = {10.1016/j.comcom.2025.108281},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108281},
  shortjournal = {Comput. Commun.},
  title        = {Performance model and system optimization of an energy-saving strategy based on adaptive service rate tuning in cloud data centers with micro-burst traffic},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overcoming data limitations in internet traffic forecasting: LSTM models with transfer learning and wavelet augmentation. <em>COMCOM</em>, <em>242</em>, 108280. (<a href='https://doi.org/10.1016/j.comcom.2025.108280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate internet traffic prediction in smaller ISP networks is challenged by limited data availability. This paper explores this issue using transfer learning and data augmentation techniques with two LSTM-based models, LSTMSeq2Seq and LSTMSeq2SeqAtn, initially trained on a comprehensive dataset provided by Juniper Networks, Inc. and subsequently applied to smaller datasets. The datasets represent real internet traffic telemetry, offering insights into diverse traffic patterns across different network domains. Our study found that although both models performed well in single-step predictions, multi-step forecasting was more challenging, especially regarding long-term accuracy. Empirical results demonstrated that LSTMSeq2Seq outperformed LSTMSeq2SeqAtn on smaller datasets, with improvements in forecasting accuracy by up to 36.70% in MAE and 27.66% in WAPE after applying data augmentation using Discrete Wavelet Transform. The LSTMSeq2Seq model achieved an accuracy improvement from 83% to 88% for 6-step forecasts, 82% to 88% for 9-step forecasts, and 81% to 87% for 12-step forecasts, whereas LSTMSeq2SeqAtn exhibited a more stable short-term performance but higher variability in longer forecasts. Additionally, the mean absolute percentage error (MAPE) of multi-step predictions increased over longer horizons, with LSTMSeq2Seq reaching 6.74% at 12 steps and LSTMSeq2SeqAtn at 6.77%, highlighting the challenge of long-term forecasting. Variability analysis showed that while the attention mechanism in LSTMSeq2SeqAtn improved short-term prediction consistency, it also increased uncertainty in longer forecasts, as seen in the interquartile range (IQR) rising from 0.578 at 6 steps to 1.237 at 9 steps. Outlier analysis further confirmed that LSTMSeq2Seq exhibited more stable improvements, whereas LSTMSeq2SeqAtn showed increased dispersion in forecast accuracy. These findings underscore the importance of transfer learning and data augmentation in enhancing forecasting accuracy, particularly for smaller ISP networks with limited data availability. Furthermore, our analysis highlights the trade-offs between model complexity, short-term consistency, and long-term stability in internet traffic prediction.},
  archive      = {J_COMCOM},
  author       = {Sajal Saha and Anwar Haque and Greg Sidebottom},
  doi          = {10.1016/j.comcom.2025.108280},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108280},
  shortjournal = {Comput. Commun.},
  title        = {Overcoming data limitations in internet traffic forecasting: LSTM models with transfer learning and wavelet augmentation},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring traffic pattern variability in vehicular federated learning. <em>COMCOM</em>, <em>242</em>, 108279. (<a href='https://doi.org/10.1016/j.comcom.2025.108279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of software-defined vehicles has brought machine learning into the vehicular domain. To support these data-driven applications, techniques to incentivize users to share their vehicle data are crucial. Federated learning trains machine learning models in a distributed manner, leveraging client data without compromising its privacy. Nonetheless, in vehicular networks, the dynamic behavior of nodes affects client availability and the global model’s performance. Accordingly, this paper evaluates federated learning (FL) in a realistic vehicular network topology, accounting for real vehicle traffic in two Brazilian urban areas. The network simulation covers 3 . 7 km 2 with 1290 vehicles per hour and road speeds, based on real data. Our paper provides a comprehensive analysis of the impact that different traffic behaviors can yield during the training phase of a federated learning model. We observe that there is a performance decay in urban areas with longer vehicle permanence. Interestingly, longer vehicle participation in FL training leads to a biased final model with reduced generalization. We propose a novel approach to verify vehicle variability over time, by using the Dice-Sørensen coefficient to compare the set of clients participating in different rounds of training. By maintaining the vehicle variability over the rounds we can reduce the effect of the bias on the model, and – with a 47% reduction of the communication overhead – achieve faster learning, higher convergence in the first 15 rounds, and an equivalent final accuracy. Additionally, we extend our analysis by conducting simulations under more extreme traffic scenarios across multiple datasets, using a MobileNetV3. The results confirm that sustaining high vehicle variability – in scenarios with a brief participation of vehicles in the training – yields comparable model performance while saving up to 83.5 GB in communication costs.},
  archive      = {J_COMCOM},
  author       = {Giuliano Fittipaldi and Rodrigo S. Couto and Luís H.M.K. Costa},
  doi          = {10.1016/j.comcom.2025.108279},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108279},
  shortjournal = {Comput. Commun.},
  title        = {Exploring traffic pattern variability in vehicular federated learning},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay analysis of BFT consensus: Case study of narwhal and bullshark protocols. <em>COMCOM</em>, <em>242</em>, 108278. (<a href='https://doi.org/10.1016/j.comcom.2025.108278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acknowledging the critical influence of consensus delays on blockchain performance, this paper presents an analytical and simulation-based exploration of delay characteristics in Byzantine Fault Tolerant (BFT) consensus mechanisms. Our focus is on SUI, a blockchain system that employs a Directed Acyclic Graph (DAG) structure to support parallel transaction execution. SUI relies on two integrated protocols: Narwhal, a mempool protocol responsible for efficient block dissemination and DAG construction; and Bullshark, which organizes DAG vertices to produce a consistent total order of transactions without incurring additional communication overhead. While our previous work modeled Narwhal’s delay characteristics under various message propagation distributions, this study shifts attention to Bullshark—the protocol responsible for reaching consensus. We propose a probabilistic analytical model that estimates the number of rounds required to reach consensus. In this model, each validator’s decision is treated as a Bernoulli trial, and we apply the binomial distribution to determine the probability of reaching quorum. This framework enables us to analyze the expected delay of the protocol. To validate our model, we implemented both Narwhal and Bullshark and conducted extensive simulations. The simulation results show strong agreement with our analytical predictions, confirming the accuracy of our model. For instance, under a Gaussian delay model with mean μ = 1 ms and standard deviation σ = 0 . 25 ms—values representative of short-range wireless communication in real-world IoT or LAN settings [1] —we predict an average round duration of approximately 3.26 ms. Furthermore, based on our binomial-based model of block commitment, the expected number of rounds to reach consensus is approximately 1 when f = 10 , indicating that blocks typically commit in a single round with high probability. To the best of our knowledge, this is the first study to model Bullshark’s consensus process using Bernoulli trials and binomial distributions. Our contributions offer a novel framework for evaluating its efficiency and provide insights that can guide future optimization and scalability efforts for DAG-based BFT protocols.},
  archive      = {J_COMCOM},
  author       = {Khouloud Hwerbi and Ichrak Amdouni and Cédric Adjih and Leila Azouz Saidane and Anis Laouiti},
  doi          = {10.1016/j.comcom.2025.108278},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108278},
  shortjournal = {Comput. Commun.},
  title        = {Delay analysis of BFT consensus: Case study of narwhal and bullshark protocols},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic split federated learning for resource-constrained IoT systems. <em>COMCOM</em>, <em>242</em>, 108275. (<a href='https://doi.org/10.1016/j.comcom.2025.108275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient resource utilization in Internet of Things (IoT) systems is challenging due to device limitations. These limitations restrict on-device machine learning (ML) model training, leading to longer processing times and inefficient metadata analysis. Additionally, conventional centralized data collection poses privacy concerns, as raw data has to leave the device to the server for processing. Combining Federated Learning (FL) and Split Learning (SL) offers a promising solution by enabling effective machine learning on resource-constrained devices while preserving user privacy. However, the dynamic nature of IoT resources and device heterogeneity can complicate the application of these solutions, as some IoT devices cannot complete the training task on time. To address these concerns, we have developed a Dynamic Split Federated Learning (DSFL) architecture that dynamically adjusts to the real-time resource availability of individual clients. Combining real-time split-point selection with a Genetic Algorithm (GA) for client selection, tailored to heterogeneous, resource-constrained IoT devices. DSFL ensures optimal utilization of resources and efficient training across heterogeneous IoT devices and servers. Our architecture detects each IoT device’s training capabilities by identifying the number of layers it can train. Moreover, an effective Genetic Algorithm (GA) process strategically selects the clients required to complete the split federated learning round. Cooperatively, the clients and servers train their parts of the model, aggregate them, and then combine the results before moving to the next round. Our proposed architecture enables collaborative model training across devices while preserving data privacy by combining FL’s parallelism with SL’s dynamic modeling. We evaluated our architecture on sensory and image-based datasets, showing improved accuracy and reduced overhead compared to baseline methods.},
  archive      = {J_COMCOM},
  author       = {Mohamad Wazzeh and Ahmad Hammoud and Azzam Mourad and Hadi Otrok and Chamseddine Talhi and Zbigniew Dziong and Chang-Dong Wang and Mohsen Guizani},
  doi          = {10.1016/j.comcom.2025.108275},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108275},
  shortjournal = {Comput. Commun.},
  title        = {Dynamic split federated learning for resource-constrained IoT systems},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical layer security in FAS-aided wireless powered NOMA systems. <em>COMCOM</em>, <em>242</em>, 108274. (<a href='https://doi.org/10.1016/j.comcom.2025.108274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of communication technologies and the emergence of sixth-generation (6G) networks have introduced unprecedented opportunities for ultra-reliable, low-latency, and energy-efficient communication. Integrating technologies like non-orthogonal multiple access (NOMA) and wireless powered communication networks (WPCNs) brings new challenges. These include energy constraints and increased security vulnerabilities. Traditional antenna systems and orthogonal multiple access schemes struggle to meet the increasing demands for performance and security in such environments. To address this gap, this paper investigates the impact of emerging fluid antenna systems (FAS) on the performance of physical layer security (PLS) in WPCNs. Specifically, we consider a scenario in which a transmitter, powered by a power beacon via an energy link, transmits confidential messages to legitimate FAS-aided users over information links while an external eavesdropper attempts to decode the transmitted signals. Additionally, users leverage the NOMA scheme, where the far user may also act as an internal eavesdropper. For the proposed model, we first derive the distributions of the equivalent channels at each node and subsequently obtain compact expressions for the secrecy outage probability (SOP) and average secrecy capacity (ASC), using the Gaussian quadrature methods. Our results reveal that incorporating the FAS for NOMA users, instead of the TAS, enhances the performance of the proposed secure WPCN.},
  archive      = {J_COMCOM},
  author       = {Farshad Rostami Ghadi and Masoud Kaveh and Kai-Kit Wong and Diego Martín and Riku Jäntti and Zheng Yan},
  doi          = {10.1016/j.comcom.2025.108274},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108274},
  shortjournal = {Comput. Commun.},
  title        = {Physical layer security in FAS-aided wireless powered NOMA systems},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A slot-based energy storage decision-making approach for optimal off-grid telecommunication operator. <em>COMCOM</em>, <em>242</em>, 108273. (<a href='https://doi.org/10.1016/j.comcom.2025.108273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a slot-based energy storage approach for decision-making in the context of an Off-Grid telecommunication operator. We consider network systems powered by solar panels, where harvest energy is stored in a battery that can also be sold when fully charged. To reflect real-world conditions, we account for non-stationary energy arrivals and service demands that depend on the time of day, as well as the failure states of PV panel. The network operator we model faces two conflicting objectives: maintaining the operation of its infrastructure and selling (or supplying to other networks) surplus energy from fully charged batteries. To address these challenges, we developed a slot-based Markov Decision Process (MDP) model that incorporates positive rewards for energy sales, as well as penalties for energy loss and battery depletion. This slot-based MDP follows a specific structure we have previously proven to be efficient in terms of computational performance and accuracy. From this model, we derive the optimal policy that balances these conflicting objectives and maximizes the average reward function. Additionally, we present results comparing different cities and months, which the operator can consider when deploying its infrastructure to maximize rewards based on location-specific energy availability and seasonal variations. Experimental results show that our proposed algorithm outperforms classical methods in large-scale scenarios. While Relative Value Iteration algorithm remains competitive on smaller instances, its convergence time increases significantly under strict precision requirements (e.g., ϵ < 1 0 − 10 ). In contrast, our method maintains both speed and robustness, solving MDPs with up to 2 × 1 0 5 states and 100 actions in under one hour, whereas standard approaches exceed 1 0 4 seconds.},
  archive      = {J_COMCOM},
  author       = {Youssef Ait El Mahjoub and Jean-Michel Fourneau},
  doi          = {10.1016/j.comcom.2025.108273},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108273},
  shortjournal = {Comput. Commun.},
  title        = {A slot-based energy storage decision-making approach for optimal off-grid telecommunication operator},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight secret-sharing-based defense against model poisoning attacks in privacy-preserving federated learning. <em>COMCOM</em>, <em>242</em>, 108272. (<a href='https://doi.org/10.1016/j.comcom.2025.108272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Artificial Intelligence of Things (AIoT) converges with Privacy-Preserving Federated Learning (PPFL), the challenge of defending against model poisoning attacks emerges as increasingly critical. Due to PPFL’s cryptographic protocols for protecting gradient exchanges, detecting poisoning attacks becomes challenging. Traditional defense mechanisms rely on plaintext gradient analysis and thus cannot be directly applied to encrypted gradients. Although homomorphic encryption-based defense schemes enable secure computations on encrypted data, their substantial computational overhead makes them impractical for resource-constrained Internet of Things (IoT) deployments. To address these challenges, we propose a Secret-Sharing-based Defense Framework (SSDF), a lightweight scheme that enables efficient similarity calculations on encrypted gradients under secure aggregation protocols. Our scheme facilitates robust aggregation of encrypted parameters in resource-constrained edge computing environments while protecting the privacy of local model updates. Extensive experiments on four datasets demonstrate that our proposed scheme provides robust defense capabilities against poisoning attacks for both Independent and Identically Distributed (IID) and non-IID data.},
  archive      = {J_COMCOM},
  author       = {Hengheng Xiong and Jiguang Lv and Dapeng Man and Yukun Zhu and Tao Liu and Huanran Wang and Chen Xu and Wu Yang},
  doi          = {10.1016/j.comcom.2025.108272},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108272},
  shortjournal = {Comput. Commun.},
  title        = {A lightweight secret-sharing-based defense against model poisoning attacks in privacy-preserving federated learning},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A homomorphic MAC-based verifiable secure aggregation for federated learning in cloud–edge AIoT. <em>COMCOM</em>, <em>242</em>, 108271. (<a href='https://doi.org/10.1016/j.comcom.2025.108271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud–edge collaborative Artificial Intelligence of Things (AIoT) architecture addresses challenges in managing vast data storage, intelligent information processing, device interconnectivity within the Internet of Things. For its security risks and data privacy, federated learning emerges as a promising solution for ensuring data privacy in AIoT. However, susceptibility to malicious attacks during data transmission poses a significant challenge and a semi-trusted server may deviate from the specified protocol leading to inaccurate aggregation parameters returned to clients. Our proposed solution introduces a federated learning integrity verification scheme based on homomorphic Message Authentication Code (MAC) within a cloud–edge collaborative AIoT architecture. Homomorphic MAC ensures secure aggregation and integrity verification, even when distinct clients possess different keys, emphasizing integrity verification by edge node, contributes to reduced client computing costs. Further verifying of the aggregated parameters by users prevents untrusted transmission from edge node. Leveraging data integrity verification proves effective in mitigating challenges associated with parameter security, especially in scenarios involving inaccurate aggregation of local model parameters within federated learning. Our solution is free bilinear pairing, resulting in a significant reduction in computational overhead. We evaluate accuracy on the MNIST dataset through comparison with the FedAVG plaintext scheme, showing that our approach ensures parameter integrity while maintaining model performance, numerical simulations also confirm its efficiency.},
  archive      = {J_COMCOM},
  author       = {Shufen Niu and Weiying Kong and Lihua Chen and Xusheng Zhou and Ning Wang},
  doi          = {10.1016/j.comcom.2025.108271},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108271},
  shortjournal = {Comput. Commun.},
  title        = {A homomorphic MAC-based verifiable secure aggregation for federated learning in cloud–edge AIoT},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance of UAV-assisted C-V2X communications with 3D antenna beam-width fluctuations. <em>COMCOM</em>, <em>242</em>, 108267. (<a href='https://doi.org/10.1016/j.comcom.2025.108267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The antenna’s three-dimensional (3D) beam-width orientation is crucial in assessing the effectiveness of vehicular communications. This paper investigates the influence of variations of millimeter waveband antenna 3D beam-width on the performance of un-crewed aerial vehicle (UAV)-assisted cellular vehicle-to-everything (C-V2X) communications. The cellular base-stations are represented using a two-dimensional Poisson point process (PPP), while vehicular nodes (V-Ns) are represented using a Poisson line process, and UAVs are represented using a 3D PPP. The typical transmitting V-N can connect with the nearest V-N in direct mode transmission or with the (macro base-station) MBS, line-of-sight (LOS) UAV, or non-LOS (NLOS) UAV in shared mode transmission. The efficiency of the system is measured by using the antenna’s 3D beam-width relative to coverage and spectrum efficiency. To that aim, analytical equations for the association and coverage probability of vehicular-to-vehicular, vehicular-to-MBS, vehicular-to-LOS UAV, and vehicular-to-NLOS UAV connections are obtained in the setting of variation in beam-width. The efficiency is also measured in terms of V-Ns, MBS, and UAVs. The findings revealed that our system, considering millimeter waveband-based UAV-assisted C-V2X network leveraging the benefits of MBSs and UAVs, performs better than the conventional V2X network. The findings reveal that the efficiency of the UAV-assisted C-V2X networks is affected by the variable 3D beam-width, hence, it needs to be thoroughly specified. Furthermore, the network’s performance degrades when the UAV’s beam-width variations grow.},
  archive      = {J_COMCOM},
  author       = {Mohammad Arif and Wooseong Kim and Asif Mehmood},
  doi          = {10.1016/j.comcom.2025.108267},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108267},
  shortjournal = {Comput. Commun.},
  title        = {Performance of UAV-assisted C-V2X communications with 3D antenna beam-width fluctuations},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ACSFL: An adaptive client selection-based federated learning with personalized differential privacy for heterogeneous AIoT environments. <em>COMCOM</em>, <em>242</em>, 108264. (<a href='https://doi.org/10.1016/j.comcom.2025.108264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the rapid development of Artificial Intelligence (AI) and the Internet of Things (IoT), the Artificial Intelligence of Things (AIoT) is increasingly applied in smart environments. Federated Learning (FL) meets the need for intelligent data processing in these environments by providing powerful training capabilities while preserving privacy. However, AIoT environments pose new challenges for FL, particularly due to the heterogeneity of edge devices, which vary in hardware, software, network conditions, and data distribution. These factors degrade model performance and hinder convergence. Additionally, communication overhead and data privacy risks are also critical concerns. Although Differential Privacy (DP) can offer protection, they often apply uniform privacy levels, overlooking the diversity of AIoT devices. On the other hand, while current client-selection approaches partially address the heterogeneity of AIoT devices, they also tend to ignore the impact of the noising mechanisms. In this paper, we propose ACSFL, an adaptive client selection-based FL framework that integrates personalized local DP. By a novel, dynamic evaluation metric of node heterogeneity, privacy budget, and contribution, ACSFL can jointly optimize model performance, privacy preservation, and communication efficiency. We further propose a personalized local differential privacy mechanism in ACSFL, to filter and allocate each client’s budget per round, substantially enhancing privacy preservation and yielding significant accuracy gains under identical overall privacy constraints. All the above assertions are also well supported by theoretical and experimental demonstration. Specifically, our experiments show that ACSFL improves model convergence and generalization by 14% on average, achieves comparable model accuracy with 20% fewer clients, reduces communication overhead by over 25%, and saves about 26% of the privacy budget compared to other client selection methods.},
  archive      = {J_COMCOM},
  author       = {Zhousheng Wang and Junjie Chen and Hua Dai and Jian Xu and Geng Yang and Hao Zhou},
  doi          = {10.1016/j.comcom.2025.108264},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108264},
  shortjournal = {Comput. Commun.},
  title        = {ACSFL: An adaptive client selection-based federated learning with personalized differential privacy for heterogeneous AIoT environments},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient blockchain synchronization mechanism over NDN based on directed interest forwarding. <em>COMCOM</em>, <em>242</em>, 108258. (<a href='https://doi.org/10.1016/j.comcom.2025.108258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology, as a decentralized technology, has been applied across various industries due to its immutability and information security features. With the increasing adoption of blockchain technology, network scale and transaction volumes have increased rapidly. The growing data transmission demands have exposed network performance issues in blockchain systems, creating a bottleneck for further improvements. While Named Data Networking (NDN) offers strong support for blockchain networks, some existing designs lack efficient synchronization methods, resulting in redundancies and limiting the full potential of NDN in blockchain networks. To address this issue, this paper proposes a directed Interest forwarding-based synchronization mechanism for NDN-based blockchain networks. In this mechanism, we design a Block Synchronous Forward Table (BSFT) to record the synchronization status of upstream and downstream nodes. Through the structure of this table, nodes can obtain information about other nodes in the network via six specifically designed NDN Interests. During synchronization, nodes dynamically select the appropriate peers to send data request Interest based on the actual network state and synchronization status, thereby reducing the large number of redundant Interest packets and corresponding response Data packets caused by Interest broadcasts. Experimental results demonstrate that our proposed synchronization mechanism can effectively reduce network traffic, lowering traffic by about 30% or more compared to traditional IP-based blockchain and other NDN-based blockchain solutions. This also accelerates the synchronization of Data packets across the entire network, thereby enhancing the overall performance of blockchain networks.},
  archive      = {J_COMCOM},
  author       = {Dehao Zhang and Jiapeng Xiu and Zhengqiu Yang and Huixin Liu and Shaoyong Guo},
  doi          = {10.1016/j.comcom.2025.108258},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108258},
  shortjournal = {Comput. Commun.},
  title        = {Efficient blockchain synchronization mechanism over NDN based on directed interest forwarding},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing mobility prediction in 5G for enhanced C-V2X applications: A multidisciplinary research survey. <em>COMCOM</em>, <em>242</em>, 108254. (<a href='https://doi.org/10.1016/j.comcom.2025.108254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of 5G New Radio technologies, autonomous vehicles are evolving from isolated units into components of a larger, interconnected system. This transformation is enabled by robust Vehicle-to-Everything (V2X) communications, facilitating applications such as high-definition sensor data sharing and collision avoidance within the Cellular-V2X (C-V2X) framework. Nationwide, ultra-reliable, low-latency coverage is crucial for these applications, necessitating a smart, flexible network to manage mobility uncertainties effectively. To achieve this, mobility prediction will play a pivotal role by preparing the network for anticipated traffic patterns and optimizing its radio and computational resources, thereby enhancing overall efficiency. This survey provides a comprehensive review and analysis of current and emerging mobility prediction methodologies essential for enhancing these networks. We explore these methodologies along with the standards and requirements set by key organizations like the 3rd Generation Partnership Project (3GPP) and industry leaders such as the 5G Automotive Association (5GAA). By reviewing state-of-the-art mobility prediction techniques, this survey critically analyzes their role in forecasting key network performance indicators (KPIs), enabling proactive resource allocation, robust edge-computing strategies, and slice orchestration, all crucial for optimizing 5G performance and ensuring ultra-reliable, low-latency C-V2X communications.},
  archive      = {J_COMCOM},
  author       = {Mario Bou Abboud and Maroua Drissi and Oumaya Baala and Sylvain Allio},
  doi          = {10.1016/j.comcom.2025.108254},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108254},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing mobility prediction in 5G for enhanced C-V2X applications: A multidisciplinary research survey},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing bandwidth allocation in mmWave/sub-THz cellular networks using maximum flow algorithms. <em>COMCOM</em>, <em>242</em>, 108221. (<a href='https://doi.org/10.1016/j.comcom.2025.108221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploitation of millimeter wave (mmWave) and sub-Terahertz (sub-THz) bands is expected to be one of the main pillars for the development of future cellular networks due to the high available bandwidth they provide. The existence of Line-of-Sight (LOS) link between a user equipment (UE) and an access point (AP) is a prerequisite for connection establishment in these networks, as the wireless links in these bands are very sensitive to blockage effects. This can be achieved by densifying APs within a network area. An arising challenge is the efficient exploitation of the available bandwidth of a given network. In this paper, the maximization of the number of served UEs in modern mmWave and sub-THz cellular networks is investigated and achieved by deploying a Maximum Flow Algorithm for UE-AP association (MFUA) to optimize bandwidth allocation, assuming that every AP will have a finite and predefined amount of bandwidth which they can share among UEs. MFUA determines the maximum flow between two given nodes of a graph corresponding to a specific network, where the capacity of its edges is known. An extensive simulation campaign was carried out revealing that the use of MFUA utilizes bandwidth more effectively compared to the reference method and improves the system performance, leading to the maximization of number of served UEs. The examined test cases include static and time-evolving scenarios.},
  archive      = {J_COMCOM},
  author       = {Kyriakos N. Manganaris and Panagiotis Promponas and Aris Tsolis and Fotis I. Lazarakis and Kostas P. Peppas},
  doi          = {10.1016/j.comcom.2025.108221},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108221},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing bandwidth allocation in mmWave/sub-THz cellular networks using maximum flow algorithms},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accountable privacy-enhanced multi-authority attribute-based authentication scheme for cloud services. <em>COMCOM</em>, <em>242</em>, 108205. (<a href='https://doi.org/10.1016/j.comcom.2025.108205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current attribute-based authentication (ABA) schemes have three major drawbacks: first, the single attribute authority (AA) becomes the system bottleneck, i.e., if the AA is corrupted, the entire system will stop working; second, user privacy is not completely secured; and third, malicious users may exploit their anonymity. To overcome these defects, we improved a previously established privacy-preserving decentralized ciphertext policy attribute-based encryption (PPD-CP-ABE) scheme, obtaining a PPD-CP-ABE with verifiable outsourced decryption (PPD-CP-ABE-VOD). This improved scheme uses outsourced decryption, secure two-party computation protocol, and zero-knowledge proofs. We transformed the PPD-CP-ABE-VOD scheme into a new privacy-enhanced multi-authority ABA scheme using an identity tracing mechanism based on linear encryption. This new scheme has the following advantages over similar schemes. First, it introduces multiple AAs and does not require users to trust AA fully. Second, it protects users’ attributes, global identifiers, and access behavior, thus strengthening user privacy protection. Finally, it balances user privacy protection and user accountability. Theoretical and experimental analyses have shown that the new scheme is comparable to recently proposed ABA systems in terms of performance in the key generation and authentication phases, despite appending multiple security properties.},
  archive      = {J_COMCOM},
  author       = {Xin Liu and Hao Wang and Bo Zhang and Bin Zhang},
  doi          = {10.1016/j.comcom.2025.108205},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108205},
  shortjournal = {Comput. Commun.},
  title        = {Accountable privacy-enhanced multi-authority attribute-based authentication scheme for cloud services},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
