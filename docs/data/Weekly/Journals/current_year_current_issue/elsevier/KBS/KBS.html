<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>KBS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="kbs">KBS - 17</h2>
<ul>
<li><details>
<summary>
(2025). Potential subgraph rule and reasoning context enhancement for sparse multi-hop knowledge graph reasoning. <em>KBS</em>, <em>330</em>, 114483. (<a href='https://doi.org/10.1016/j.knosys.2025.114483'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-hop knowledge graph reasoning aims to leverage the relations between multiple nodes in a knowledge graph to reason information about an event or entity. This reasoning process requires traversing multiple interconnected facts or knowledge points, which aids in understanding the model’s decision-making process. Multi-hop knowledge graph reasoning has driven the development of knowledge-based technologies, such as question-answering systems and recommendation systems. However, multi-hop reasoning relies on the connectivity between different entities in the knowledge graph. This characteristic makes multi-hop reasoning lack robustness when dealing with sparse data. To address the challenges of sparsity, recent studies pre-train knowledge graph embedding models to complete potential triples. The completion methods introduce noisy triples, which increases the risk of model selection errors and spurious paths. In this work, we propose a framework based on potential subgraph rule and reasoning context enhancement to mitigate the challenges of sparsity. On one hand, we leverage reasoning context to enhance state information and the reasoning process; on the other hand, we design an action perceptron based on the importance of reasoning context to reduce the introduction of noisy triples. Additionally, we analyze the phenomenon of data augmentation introducing spurious paths, and further utilize data augmentation-based potential subgraph rules to guide the reasoning process. This dual mechanism demonstrates stronger robustness in addressing sparsity challenges and spurious paths. Diverse experiments demonstrate that our model outperforms the existing multi-hop reasoning models across five datasets. Our implementations will be publicly available at: https://github.com/jianruichen/PreKGR .},
  archive      = {J_KBS},
  author       = {Congcong Sun and Jianrui Chen and Deguang Chen and Junjie Huang},
  doi          = {10.1016/j.knosys.2025.114483},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114483},
  shortjournal = {Knowl. Based Syst.},
  title        = {Potential subgraph rule and reasoning context enhancement for sparse multi-hop knowledge graph reasoning},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAMR: Multi-scale graph contrastive learning with dynamic adjustment and mutual rectification. <em>KBS</em>, <em>330</em>, 114482. (<a href='https://doi.org/10.1016/j.knosys.2025.114482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) has emerged as a powerful self-supervised approach for learning generalized graph representations, achieving remarkable advancements in recent years. However, most existing GCL methods ignore the noise of the augmented global structure and the dynamic change in training, and lack detailed consideration in calculating local structural homogeneity. These limitations may lead to the model’s insufficient performance in capturing fine-grained semantic features at the node level, making it difficult to fully explore the potential semantic associations between adjacent nodes. Meanwhile, on a global scale, there is also a lack of the ability to model complex topological structures. To this end, we propose a new multi-scale graph contrastive learning with dynamic adjustment and mutual rectification. This method dynamically adjusts the global structure via graph reconstruction and adaptively learns node representations; Meanwhile, a mutual rectification module is designed to predict the support scores of neighbors relative to anchors and quantify each neighbor’s contribution to view agreement. Both reconstruction and rectification are integrated into the training objective and effectively capture the graph structure information from both global and local scales, improving the quality and robustness of graph representations. We conduct extensive experiments on three downstream tasks: node classification, node clustering, and link prediction. The experimental results demonstrate that our method outperforms existing GCL methods across multiple tasks and datasets, validating the effectiveness and generalizability of the proposed model.},
  archive      = {J_KBS},
  author       = {Dengdi Sun and Zhixiang Wu and Mingwei Cao and Zhifu Tao and Zhuanlian Ding},
  doi          = {10.1016/j.knosys.2025.114482},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114482},
  shortjournal = {Knowl. Based Syst.},
  title        = {DAMR: Multi-scale graph contrastive learning with dynamic adjustment and mutual rectification},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing leadership-based metaheuristics using reinforcement learning: A case study in grey wolf optimizer. <em>KBS</em>, <em>330</em>, 114471. (<a href='https://doi.org/10.1016/j.knosys.2025.114471'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristics are widely applied in optimization because of their flexibility and ability to address complex and high-dimensional problems. Nevertheless, they face persistent challenges, including susceptibility to local optima, limited parameter adaptability, and premature convergence. Leadership-based metaheuristics, in which leaders guide the search process, encounter additional difficulties such as limited exploration capacity, leader stagnation, and reduced diversity, often stemming from underutilization of data generated during the search. To overcome these limitations, this study proposes a reinforcement learning–based approach, RL-LGWO, which enhances the Grey Wolf Optimizer (GWO) by integrating multi-agent reinforcement learning. In RL-LGWO, agents share experiences to improve decision-making, and reinforcement learning is employed to decouple and adapt the leader update mechanism, thereby improving the exploration–exploitation balance and enabling leaders to dynamically escape local optima. The proposed method was evaluated against two GWO-enhancing algorithms, three RL-based GWO variants, PSO, WOA, and the original GWO across 23 well-known benchmark functions, in addition to the recent CEC2022 benchmark suite. Experimental results show that RL-LGWO achieved the best solutions on 17 of the 23 benchmark functions, with superior convergence speed and improved stability, while incurring only a minor runtime increase compared with the original GWO. Furthermore, on the CEC2022 suite, RL-LGWO outperformed competing algorithms on 10 of 12 test functions, underscoring its robustness and adaptability to recent and challenging benchmarks. Overall, the findings indicate that RL-LGWO delivers a substantive improvement over state-of-the-art alternatives and holds strong potential to advance leadership-based metaheuristics for a wide range of optimization problems.},
  archive      = {J_KBS},
  author       = {Afifeh Maleki and Mehdy Roayaei and Seyedali Mirjalili},
  doi          = {10.1016/j.knosys.2025.114471},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114471},
  shortjournal = {Knowl. Based Syst.},
  title        = {Enhancing leadership-based metaheuristics using reinforcement learning: A case study in grey wolf optimizer},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing dual network based semi-supervised medical image segmentation with uncertainty-guided pseudo-labeling. <em>KBS</em>, <em>330</em>, 114454. (<a href='https://doi.org/10.1016/j.knosys.2025.114454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable performance of supervised medical image segmentation models, relying on a large amount of labeled data is impractical in real-world situations. Semi-supervised learning approaches aim to alleviate this challenge using unlabeled data through pseudo-label generation. Yet, existing semi-supervised segmentation methods still suffer from noisy pseudo-labels and insufficient supervision within the feature space. To solve these challenges, this paper proposes a novel semi-supervised 3D medical image segmentation framework based on a dual-network architecture. Specifically, we investigate a Cross Consistency Enhancement module using both cross pseudo and entropy-filtered supervision to reduce the noisy pseudo-labels, while we design a dynamic weighting strategy to adjust the contributions of pseudo-labels using an uncertainty-aware mechanism (i.e., Kullback–Leibler divergence). In addition, we use a self-supervised contrastive learning mechanism to align uncertain voxel features with reliable class prototypes by effectively differentiating between trustworthy and uncertain predictions, thus reducing prediction uncertainty. Extensive experiments are conducted on three 3D segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed approach consistently exhibits superior performance across various settings (e.g., 89.95 % Dice score on left Atrial with 10 % labeled data) compared to the state-of-the-art methods. Furthermore, the usefulness of the proposed modules is further validated via ablation experiments. The code repository is available at https://github.com/AIPMLab/Semi-supervised-Segmentation .},
  archive      = {J_KBS},
  author       = {Yunyao Lu and Yihang Wu and Ahmad Chaddad and Tareef Daqqaq and Reem Kateb},
  doi          = {10.1016/j.knosys.2025.114454},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114454},
  shortjournal = {Knowl. Based Syst.},
  title        = {Enhancing dual network based semi-supervised medical image segmentation with uncertainty-guided pseudo-labeling},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-domain weakly decoupled domain generalization network for fault diagnosis under unknown operating conditions. <em>KBS</em>, <em>330</em>, 114452. (<a href='https://doi.org/10.1016/j.knosys.2025.114452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of transfer learning strategies to solve cross-domain fault diagnosis problems has achieved significant results. However, most existing multi-source domain generalization fault diagnosis methods use a single classifier or introduce auxiliary classifiers, focusing on learning domain-invariant features or global feature distribution matching. Furthermore, since the data distributions of different source domains may be significantly different, this may lose the data distribution information specific to each source domain. In addition, how to reduce the variation in risk between samples within the same domain training is also a challenging issue. Finally, it is also crucial to balance the predictive outputs of multiple classifiers to adapt them to the data distribution of the target domain. Based on the above challenges, this paper proposes a multi-domain weakly decoupled domain generalization network for fault diagnosis under unknown operating conditions. Feature weakly decoupled mechanism is achieved by employing multiple classifiers and incorporating the variance of samples within the same sample domain as a penalty term. This reduces the model’s sensitivity to changes in the extreme distribution of samples within the domain. Classifier weakly decoupled mechanism, on the other hand, reduces the inter-domain risk variance by minimizing the loss of variance in the predicted output of the source domain classifiers. This improves the robustness of the model to inter-domain distributional changes and covariate changes. Experimental results on three datasets validate the effectiveness and general applicability of the proposed approach.},
  archive      = {J_KBS},
  author       = {Yawei Sun and Hongfeng Tao and Vladimir Stojanovic},
  doi          = {10.1016/j.knosys.2025.114452},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114452},
  shortjournal = {Knowl. Based Syst.},
  title        = {Multi-domain weakly decoupled domain generalization network for fault diagnosis under unknown operating conditions},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing large language models for bitcoin time series forecasting. <em>KBS</em>, <em>330</em>, 114449. (<a href='https://doi.org/10.1016/j.knosys.2025.114449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the recent advancements in application of deep learning to time series forecasting, focus has shifted from training transformers end-to-end to efficiently leveraging the predictive capabilities of Large Language Models (LLMs). Models that encode the time series data to interact with a frozen LLM backbone have been shown to outperform transformers on all benchmark datasets. However, their efficiency on complex datasets, which do not show clear seasonality or trend, remains an open question. In this work, we seek to evaluate the performance of reprogrammed LLMs on the Bitcoin price chart, a financial time series known for its complexity and high volatility. We propose effective methods to improve the performance of Time-LLM, a State-of-the-art (SOTA) method, on such a time series. First, we propose structural improvements to Time-LLM. Second, we suggest an efficient way to handle the non-stationarity of the dataset. Finally, we propose an efficient method for passing additional financial information to the LLM. Our results demonstrate a 50 % improvement on the average percentage loss and a 5 % increase on accuracy of our adapted Time-LLM architecture on Bitcoin data when compared to SOTA models, including the original Time-LLM model. This highlights the impact on forecast accuracy of domain-specific decision making in data processing and feature selection.},
  archive      = {J_KBS},
  author       = {Owen Chaffard and Pablo Mollá and Marc Cavazza and Helmut Prendinger},
  doi          = {10.1016/j.knosys.2025.114449},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114449},
  shortjournal = {Knowl. Based Syst.},
  title        = {Enhancing large language models for bitcoin time series forecasting},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FCAT: Federated causal adversarial training. <em>KBS</em>, <em>330</em>, 114440. (<a href='https://doi.org/10.1016/j.knosys.2025.114440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference has been proven to be a crucial technique for improving the efficacy and explainability of adversarial training (AT). However, its applicability in the decentralized adversarial training paradigm has not been fully explored. Where one potential challenge is to apply the causal inference in the settings of non-independent and identically distributed (Non-IID) federated learning. In particular, the imbalanced data distributions among various clients will unavoidably hinder the efficacy and adaptability of causal inference. To address this issue, this paper proposes a novel yet practical method dubbed Federated Causal Adversarial Training (FCAT), which seeks to improve causal models via calibrated correction information. Additionally, we introduce a lightweight slack aggregation method aimed at addressing client model disparities and minimizing the communication overhead in each iteration. Extensive experimental results demonstrate that FCAT significantly improves the efficacy of causal models in federated adversarial training, and remarkably outperforms the current state-of-the-art (SOTA) competitors on multiple widely-adopted benchmarks.},
  archive      = {J_KBS},
  author       = {Yunhao Feng and Yanming Guo and Mingrui Lao and Yulun Wu and Yishan Li and Yuxiang Xie},
  doi          = {10.1016/j.knosys.2025.114440},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114440},
  shortjournal = {Knowl. Based Syst.},
  title        = {FCAT: Federated causal adversarial training},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural network-based iterative heuristic algorithm for the polynomial robust knapsack problem. <em>KBS</em>, <em>330</em>, 114439. (<a href='https://doi.org/10.1016/j.knosys.2025.114439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The polynomial robust knapsack problem (PRKP) is a variant of the classic knapsack problem by incorporating uncertain costs and benefits from item combinations, leading to a nonlinear objective function and exponential solution space. These complexities make the PRKP suitable for real-world scenarios where interactions between items unpredictably impact outcomes. However, existing algorithms struggle to efficiently solve large instances of the PRKP due to its computational complexity. Therefore, this paper presents an iterative heuristic algorithm leveraging a neural network (NN) to address the PRKP, reducing the solution space and enabling efficient resolution of subproblems. The framework integrates an NN trained in two steps: general training and fine-tuning. The trained model is then embedded in the iterative heuristic algorithm to tackle the PRKP. A synthetic dataset comprising 2500 instances, ranging from 100 to 1500 items, is created to train the NN. Comparative evaluations are conducted using 1600 benchmark instances from the literature and 140 larger instances containing between 2000 and 15,000 items. We compare our approach against two state-of-the-art algorithms for the PRKP: a genetic algorithm and a random forest-based heuristic. Computational results demonstrate that the proposed algorithm outperforms the genetic algorithm, providing superior solution quality with significantly reduced computing times. Meanwhile, against random forest-based heuristic, it delivers better solution quality with only a moderate increase in computing time. For larger instances, it maintains its advantage in solution quality while remaining computationally efficient. These results highlight the algorithm’s scalability, effectiveness, and potential to address the PRKP.},
  archive      = {J_KBS},
  author       = {José González-Cortés and Carlos Contreras-Bolton},
  doi          = {10.1016/j.knosys.2025.114439},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114439},
  shortjournal = {Knowl. Based Syst.},
  title        = {A neural network-based iterative heuristic algorithm for the polynomial robust knapsack problem},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TWDT: Training-free word-level controllable diffusion model for text generation. <em>KBS</em>, <em>330</em>, 114437. (<a href='https://doi.org/10.1016/j.knosys.2025.114437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing controlled text generation (CTG) methods typically require the training of additional components, whereas diffusion models have already achieved fine control in image generation by adjusting latent feature information during the inference process. However, existing diffusion models still face issues such as “attribute leakage” and “overgeneration” when applied to text generation, leading to generated texts lacking precise control. To address these problems, we propose a training-free word-level controllable diffusion language network (TWDT). This network achieves fine-grained control of text generation by adjusting latent space features during the inference process. Specifically, TWDT introduces an Alignment and Word Evaluation (AWE) module, which ensures accurate mapping of the text to a predefined set of feature words through syntactic segmentation and multi-level semantic alignment. At the same time, a similarity threshold filtering mechanism is applied to inject Gaussian noise into low-consistency nodes, ensuring semantic consistency and stability during generation. To evaluate the rigor and accuracy of the model, we have developed a high-quality multi-disease dental diagnostic dataset, all of which are annotated by experienced dental experts, serving as the benchmark for model evaluation. Experimental results show that TWDT outperforms existing diffusion models in terms of generation accuracy and rigor.},
  archive      = {J_KBS},
  author       = {Nan Gao and Yangjie Lu and Peng Chen and Guodao Sun and Ronghua Liang and Yilong Zhang},
  doi          = {10.1016/j.knosys.2025.114437},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114437},
  shortjournal = {Knowl. Based Syst.},
  title        = {TWDT: Training-free word-level controllable diffusion model for text generation},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-class intrusion detection system for in-vehicle networks using few-shot learning and convolutional anomaly transformer network. <em>KBS</em>, <em>330</em>, 114436. (<a href='https://doi.org/10.1016/j.knosys.2025.114436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern vehicles depend on the Controller Area Network (CAN) for electronic control unit (ECU) communication, but its inherent vulnerabilities necessitate robust intrusion detection systems (IDS). Current machine learning and deep learning IDS solutions struggle with limited labeled data, class imbalances, and costly data collection processes. Few-shot learning, effective with few labeled samples, remains underexplored for in-vehicle networks (IVNs) despite its potential in data-scarce automotive cybersecurity scenarios. To bridge this gap, we introduce the first few-shot learning approach for multi-class intrusion detection in IVNs, leveraging a novel, lightweight Convolutional Anomaly Transformer. By integrating a 1D convolutional layer with an Anomaly Transformer, our model effectively classifies diverse attack types with minimal training data, mitigating class imbalance. Experiments on the widely-used real-world Car Hacking dataset, the complex ROAD dataset, and the distinct CAN-ML dataset validate its efficacy. On the Car Hacking dataset, we achieve an exceptional F1 score of 0.9994 with only 2 % of training data, improving to 0.9999 with 10 %. On the challenging ROAD dataset, characterized by diverse attacks and high variability, the model achieves an F1 score of up to 0.9980 using just 10 % of training data. Demonstrating strong generalization capabilities, the model also attains an impressive F1 score of 0.9918 on the CAN-ML dataset, which features entirely different vehicles and attack distributions. Furthermore, the lightweight architecture of our proposed IDS enables practical deployment in resource-constrained automotive environments.},
  archive      = {J_KBS},
  author       = {Nguyen Thanh Minh Duy and Truong Hoang Bao Huy and Pham Van Phu and Tien-Dat Le and Daehee Kim},
  doi          = {10.1016/j.knosys.2025.114436},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114436},
  shortjournal = {Knowl. Based Syst.},
  title        = {Multi-class intrusion detection system for in-vehicle networks using few-shot learning and convolutional anomaly transformer network},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of an enhanced heart attack diagnosis model using knowledge distillation and frequent sequence pattern mining. <em>KBS</em>, <em>330</em>, 114434. (<a href='https://doi.org/10.1016/j.knosys.2025.114434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) algorithms have displayed their effectiveness in predicting sequence modelling compared to various systems. Nevertheless, some limitations of existing methods are the demand for enormous databases, computational expense, and the risk of overfitting. To address these problems, this study proposes a novel DL technique using knowledge distillation and sequence illness pattern recognition from medical databases. Firstly, the input data is pre-processed using the data cleaning method. The size of the sequence dataset and the duration of the sequential patterns are both considered during the process of using PREFIXSPAN to manage long sequential patterns. In the proposed strategy, a lightweight student network is employed to train a strong teacher network, which is produced by a Knowledge Distillation framework. A teacher network is assessed by the Attention Based Densely Connected Capsule Model (Attention-DC). An efficient, low-weight Depthwise Separable Convolutional Neural Network (DSCNN) model is then chosen as the student network. This study uses three datasets to solve enormous database issues. The KD helps prevent the student model from overfitting to noise or specific patterns in the training data. The Improved Coot Optimization Algorithm (ICOA) is applied to adjust the parameter. The hyperparameters used to optimize the performance of the proposed model are Epochs (300), learning rate (0.001), and batch size (32), respectively. The experiments use the resources of three different datasets, and Python is employed to analyze the results. The proposed technique achieves accuracy of 99.512 %, 99.329 % and 99.351 % for the heart disease, cardiovascular disease, and Diabetes dataset.},
  archive      = {J_KBS},
  author       = {Dinesh Kumar Bhawnani and Sunita Soni and Arpana Rawal},
  doi          = {10.1016/j.knosys.2025.114434},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114434},
  shortjournal = {Knowl. Based Syst.},
  title        = {Development of an enhanced heart attack diagnosis model using knowledge distillation and frequent sequence pattern mining},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural chain of thoughts for radiology education. <em>KBS</em>, <em>330</em>, 114433. (<a href='https://doi.org/10.1016/j.knosys.2025.114433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiology education requires trainees to develop both perceptual and interpretive expertise. However, refinement of these skills is often impeded by the limited availability of mentorship, a consequence of the demanding schedules of experienced radiologists. This lack of personalized guidance makes it difficult for learners to recognize the mistakes they make, understand why those errors occurred and how to refine their perceptual processes. Many of these errors arise from subtle differences in visual attention, such as failing to fixate on an abnormality, allocating an insufficient fixation time, or overlooking an abnormality despite scanning the correct region. Although Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been explored for radiology tasks, they often struggle to detect such fine-grained multimodal variations, particularly when comparing gaze behavior between experts and trainees. To address these limitations, we introduce Structural Chain of Thoughts (SCoT), a novel framework that enhances LLMs and LMMs sensitivity to nuanced multimodal differences by structuring gaze data and radiology report into a thought graph. By leveraging a structural prior, SCoT systematically identifies key perceptual and interpretive discrepancies, allowing models to provide targeted, context-aware feedback. This structured approach not only highlights missed findings but also explains the reasoning behind perceptual errors, turning them into learning opportunities. Applied within radiology education, SCoT bridges the gap between expert and novice performance, offering a scalable solution for AI-driven diagnostic training. We further contribute a simulated dataset of perceptual errors in chest X-ray (CXR) interpretation, facilitating future research into multimodal reasoning and AI-driven medical education. Unlike conventional Chain-of-Thought approaches, SCoT explicitly integrates gaze and textual information into a structured reasoning process, yielding interpretable, fine-grained, and personalized feedback tailored to the unique needs of radiology training. The code and data will be available here: GitHub Repository .},
  archive      = {J_KBS},
  author       = {Akash Awasthi and Brandon Chung and Anh Mai Vu and Saba Khan and Ngan Le and Zhigang Deng and Rishi Agrawal and Carol C. Wu and Hien Van Nguyen},
  doi          = {10.1016/j.knosys.2025.114433},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114433},
  shortjournal = {Knowl. Based Syst.},
  title        = {Structural chain of thoughts for radiology education},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized task offloading with energy efficient communication and optimal offloading network: A mobility and energy-efficient approach for augmented reality in mobile edge computing. <em>KBS</em>, <em>330</em>, 114431. (<a href='https://doi.org/10.1016/j.knosys.2025.114431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing enables the efficient execution of compute-intensive tasks by offloading them to edge servers. However, frequent user mobility in 5 G urban networks leads to increased latency, energy consumption, and resource wastage due to continuous handovers. To address these challenges, Energy Efficient Communication and Optimal Offloading Network, a framework is proposed that combines user mobility prediction and hybrid optimization for task offloading. Energy Efficient Communication and Optimal Offloading Network utilizes a modified Long Short-Term Memory model to predict user movement with high accuracy, achieving an accuracy improvement from 65 % to 95 % over ten iterations. Additionally, a Hybrid Grey Wolf Optimization Algorithm optimizes task allocation, resulting in a 30 % reduction in energy consumption and a 25 % improvement in server utilization compared to baseline methods. The framework achieves latency as low as 5 milliseconds for augmented reality tasks while maintaining scalability in high-traffic 5 G environments. The proposed model also outperforms baseline approaches in terms of task completion time, throughput, and communication efficiency, and it achieves a 94.5 % offloading success rate and 98 % augmented reality delay compliance. The proposed model provides a scalable and useful solution for real-time Augmented Reality by combining energy-constrained task allocation with mobility-aware predictions.},
  archive      = {J_KBS},
  author       = {Anitha Jebamani Soundararaj and Godfrey Winster Sathianesan},
  doi          = {10.1016/j.knosys.2025.114431},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114431},
  shortjournal = {Knowl. Based Syst.},
  title        = {Optimized task offloading with energy efficient communication and optimal offloading network: A mobility and energy-efficient approach for augmented reality in mobile edge computing},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified multi-subgraph pre-training framework for spatio-temporal graph. <em>KBS</em>, <em>330</em>, 114428. (<a href='https://doi.org/10.1016/j.knosys.2025.114428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal graph (STG) learning has shown great potential in capturing complex spatio-temporal dependencies and has achieved significant success in various fields such as traffic flow prediction, climate forecasting, and epidemiological spread research. By learning general features from spatio-temporal graphs, pre-trained graph models can capture hidden semantic information in the data, thereby enhancing the learning effect of downstream tasks and improving overall model performance. However, most existing spatio-temporal graph learning methods use the entire graph for training, which may not fully capture local structure and feature information. In addition, existing methods usually adopt sequence modeling techniques without fully considering the time decay effect, i.e., the need to apply decaying attention to distant time steps. To address these issues, this paper proposes a u nified dual-phase m ulti- s ubgraph pre-training s patio- t emporal graph framework (UMSST). Specifically, in the first phase, the framework learns the global representation of the spatio-temporal graph and locates key graph nodes, while learning the “unit representations” of these key nodes. In the second phase, multiple spatio-temporal subgraphs are constructed based on these “unit representations” to further capture the implicit encoding information of more general features around the corresponding subgraphs, thereby helping the model make full use of general features. Experimental results on real datasets show that the proposed pre-trained spatio-temporal graph framework significantly improves the performance of downstream tasks and demonstrates its effectiveness in comparison with recent strong baseline models.},
  archive      = {J_KBS},
  author       = {Mingze Zhong and Zexuan Long and Xinglei Wang and Tao Cheng and Meng Fang and Ling Chen},
  doi          = {10.1016/j.knosys.2025.114428},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114428},
  shortjournal = {Knowl. Based Syst.},
  title        = {A unified multi-subgraph pre-training framework for spatio-temporal graph},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Provide explainable clues: A generative traceable method for knowledge graph completion. <em>KBS</em>, <em>330</em>, 114426. (<a href='https://doi.org/10.1016/j.knosys.2025.114426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the quality of Knowledge Graph Completion (KGC) results is an essential topic in the field of knowledge graphs. Recently, generative models (GMs) have gained widespread attention for addressing the generalization issues of traditional approaches. However, the black-box nature of generative models often leads to hallucinations, which reduce the model’s performance. Most methods attempt to mitigate this issue through retrieval enhancement and decoding constraints. However, they overlook one major cause of hallucinations–poor explainability. Based on this concept, we propose a G enerative T raceable M ethod, namely GTM, which aims to improve the KGC capability of GMs by exploring the inhibitory effect of explainability on hallucinations. In GTM, a clue tracker is used to find contextual evidence for explainability. In addition, to measure explainability clues, we propose a context-aware analyzer, which enhances the understanding of context through group analogy. In the reasoning phase, we ensure the validity of the generated results by integrating the interpretive capability of clues. Extensive experiments have demonstrated that GTM can adapt to various KGC tasks and significantly enhance the performance of KGC models.},
  archive      = {J_KBS},
  author       = {Ziqi Ma and Jinpeng Li and Hang Yu},
  doi          = {10.1016/j.knosys.2025.114426},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114426},
  shortjournal = {Knowl. Based Syst.},
  title        = {Provide explainable clues: A generative traceable method for knowledge graph completion},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IRTF: A new tensor factorization for irregular multidimensional data recovery. <em>KBS</em>, <em>330</em>, 114372. (<a href='https://doi.org/10.1016/j.knosys.2025.114372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor factorizations, although serving as paramount tools for exploiting prior knowledge of multidimensional data, are unsuitable for emerging irregular multidimensional data with the arbitrary shape spatial domain (i.e., spatial-irregular tensor), such as superpixels and spatial transcriptomics. Developing new tensor factorizations suitable for spatial-irregular tensors poses a compelling challenge. To meet this challenge, we introduce a novel Irregular Tensor Factorization (IRTF), which can fully capture the intrinsic spatial and channel information behind the spatial-irregular tensor. Concretely, a spatial-irregular tensor can be decomposed into the product of an intrinsic regular tensor, learnable channel transform matrices, and a learnable spatial transform matrix. Accompanying IRTF, we suggest the Total Variation on Channel and Spatial Transforms (TV-CST) to exploit the local information of spatial-irregular tensors, which is hardly excavated by traditional total variation methods. Combining the proposed IRTF and TV-CST, we built a spatial-irregular tensor recovery model. Extensive experiments on real-world spatial-irregular tensors demonstrate the promising performance of our IRTF and its significant advantages on downstream tasks.},
  archive      = {J_KBS},
  author       = {Jin-Yu Xie and Hao Zhang and Xi-Le Zhao and Yi-Si Luo},
  doi          = {10.1016/j.knosys.2025.114372},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114372},
  shortjournal = {Knowl. Based Syst.},
  title        = {IRTF: A new tensor factorization for irregular multidimensional data recovery},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-driven deep learning network for image splicing forgery detection. <em>KBS</em>, <em>330</em>, 114365. (<a href='https://doi.org/10.1016/j.knosys.2025.114365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image splicing is a widely used technique for manipulating images in various social activities. Detecting splicing forgery is crucial in digital forensics to identify malicious image manipulation and protect information security. However, existing methods for detecting splicing forgery typically learn features in the spatial domain and struggle to effectively capture subtle features indicative of forgery, resulting in insufficient image splicing forgery detection accuracy. To address this challenge, we propose a novel deep-learning network named the frequency-driven deep-learning network (FreNet). Specifically, FreNet comprises three innovative modules: the frequency learnable module (FLM), the spatial-aware frequency learning module (SFLM), and the high-level feature-enhancement module (HFEM). The FLM effectively extracts high- and low-frequency features, thus enhancing frequency-domain representation and capturing subtle tampered features in splicing forgery images. The SFLM utilizes spatial information to guide frequency feature learning, thus enabling spatial-aware frequency feature learning. The HFEM enhances rich contextual and high-level semantic information through multilevel and multipath extraction and fusion. Extensive experiments on five benchmark datasets indicate that FreNet can achieve superior performance. Additionally, robustness experiments demonstrate the superior robustness of FreNet against various common attacks.},
  archive      = {J_KBS},
  author       = {Enji Liang and Kuiyuan Zhang and Zhongyun Hua and Xiaohua Jia},
  doi          = {10.1016/j.knosys.2025.114365},
  journal      = {Knowledge-Based Systems},
  month        = {11},
  pages        = {114365},
  shortjournal = {Knowl. Based Syst.},
  title        = {Frequency-driven deep learning network for image splicing forgery detection},
  volume       = {330},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
