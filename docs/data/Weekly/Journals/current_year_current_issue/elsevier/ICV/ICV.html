<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv">ICV - 29</h2>
<ul>
<li><details>
<summary>
(2025). EDFusion: Edge-guided attention and dynamic receptive field with dense residual for multi-focus image fusion. <em>ICV</em>, <em>163</em>, 105763. (<a href='https://doi.org/10.1016/j.imavis.2025.105763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion (MFIF) synthesizes a fully focused image by integrating multiple partially focused images captured at distinct focal planes of the same scene. However, existing methods often fall short in preserving edge and texture details. To address this issue, this paper proposes a network for multi-focus image fusion that incorporates edge-guided attention and dynamic receptive field dense residuals. The network employs a specially designed dynamic receptive field dense residual block (DRF-DRB) to achieve adaptive multi-scale feature extraction, providing rich contextual information for subsequent fine fusion. Building on this, an edge-guided fusion module (EGFM) explicitly leverages the differences in source images as edge priors to generate dedicated weight maps for each feature channel, enabling precise boundary preservation. To efficiently model global dependencies, we introduce a multi-scale token mixing transformer (MSTM-Transformer), designed to reduce computational complexity while enhancing cross-scale semantic interactions. Finally, a refined multi-scale context upsampling module (MSCU) reconstructs high-frequency details. Experiments were conducted on five public datasets, comparing against twelve state-of-the-art methods and evaluated using nine metrics. Both quantitative and qualitative results demonstrate that the proposed method significantly outperforms existing approaches in fusion performance. Notably, on the Lytro dataset, the proposed method ranked first across eight core metrics, achieving high scores of 1.1946 in the information preservation metric ( Q N M I ) and 0.7629 in the edge information fidelity metric ( Q A B / F ).},
  archive      = {J_ICV},
  author       = {Hao Zhai and Zhendong Xu and Zhi Zeng and Lei Yu and Bo Lin},
  doi          = {10.1016/j.imavis.2025.105763},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105763},
  shortjournal = {Image Vis. Comput.},
  title        = {EDFusion: Edge-guided attention and dynamic receptive field with dense residual for multi-focus image fusion},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GNN-based primitive recombination for compositional zero-shot learning. <em>ICV</em>, <em>163</em>, 105762. (<a href='https://doi.org/10.1016/j.imavis.2025.105762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional Zero-Shot Learning (CZSL) aims to recognize unseen attribute–object combinations, with the core challenge being the complex visual manifestations across compositions. We posit that the key to address this challenge lies in enabling models to simulate human recognition processes by decomposing and dynamically recombining primitives (attributes and objects). Existing methods merely concatenate primitives after extraction to form new combinations, without achieving deep integration between attributes and objects to create truly novel compositions. To address this issue, we propose Graph Neural Network-based Primitive Recombination (GPR) framework. This framework innovatively designs a Primitive Recombination Module (PRM) based on the Compositional Matching Module (CMM). Specifically, we first extract primitives, and build independent attribute and object space based on the CLIP model, enabling more precise learning of primitive-level visual features and reducing information residuals. Additionally, we introduce a Virtual Composition Unit (VCU), which inputs optimized primitive features as nodes into GNN and models complex interaction relationships between attributes and objects through message propagation. The module performs mean pooling on the updated node features to obtain a recombined representation and fuses the global visual information from the original image through residual connections, generating semantically rich virtual compositional features while preserving key visual cues. We conduct extensive experiments on three CZSL benchmark datasets to show that GPR achieves state-of-the-art or competitive performance in both closed-world and open-world settings.},
  archive      = {J_ICV},
  author       = {Fuqin Deng and Caiyun Tang and Lanhui Fu and Wei Jin and Jiaming Zhong and Hongming Wang and Nannan Li},
  doi          = {10.1016/j.imavis.2025.105762},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105762},
  shortjournal = {Image Vis. Comput.},
  title        = {GNN-based primitive recombination for compositional zero-shot learning},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging spatial-channel attention in U-net for enhanced segmentation of martian dust storms. <em>ICV</em>, <em>163</em>, 105754. (<a href='https://doi.org/10.1016/j.imavis.2025.105754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated detection of Martian dust storms is critical for analyzing planetary climate dynamics, yet segmentation remains challenging due to diffuse storm boundaries and data artifacts. This study presents a Convolutional Block Attention Module-enhanced (CBAM-enhanced) U-Net architecture for dust storm segmentation using Mars Reconnaissance Orbiter (MRO) MARCI Mars Daily Global Maps (MDGMs) from the Mars Dust Activity Database (MDAD v1.1). The approach combines attention-driven feature refinement with class-imbalance mitigation and a patching strategy to handle missing data in global maps. The model achieves 0.6502 Intersection over Union (IoU) and 0.6883 Dice scores on MDAD data, outperforming baseline U-Net by 3%, while using 8x fewer parameters (1.95M vs 23M) in comparison to state-of-the-art methods, significantly reducing computational costs. Ablation experiments confirm CBAM reduces false positives and preserves fine boundaries; case studies show the model, in some cases, detects sub-visual dust features missed in ground truth annotations, suggesting potential utility for discovering marginal atmospheric phenomena. This work establishes an efficient framework for processing planetary image data while balancing accuracy and computational practicality.},
  archive      = {J_ICV},
  author       = {Daniele Venturini and Marco Raoul Marini and Luigi Cinque and Gian Luca Foresti},
  doi          = {10.1016/j.imavis.2025.105754},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105754},
  shortjournal = {Image Vis. Comput.},
  title        = {Leveraging spatial-channel attention in U-net for enhanced segmentation of martian dust storms},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified ResNet model for medical image-based lung cancer detection. <em>ICV</em>, <em>163</em>, 105752. (<a href='https://doi.org/10.1016/j.imavis.2025.105752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is still the most common cause of tumor death in the world. Therefore, there is a great demand to develop diagnostic tools for lung cancer. This research proposes a diagnostically tuned modified ResNet 50 model for detecting and diagnosing lung cancer from chest X-ray images. The architecture of ResNet 50 is adapted to be more suitable for the unique challenges presented by medical imaging data. The modifications include adding extra batch normalization layers for stabilizing training, replacing fully connected layers with global average pooling to reduce overfitting, and adding a squeeze-and-excitation (SE) block that enhances the model's focus on key features such as nodules and lesions. Furthermore, transfer learning was performed on the pre-trained ResNet 50 weights, and the model was fine-tuned to the dataset of images of lungs for better sensitivity regarding cancerous patterns. This modified ResNet 50 was evaluated on a publicly available dataset of lung images from the JSRT dataset, which outperforms the original ResNet 50 and state-of-the-art research. The proposed model achieves high sensitivity, specificity, precision, F1-score and accuracy, which are considered the most important factors in clinical settings. Accuracy reached as high as 98.77% in the detection of lung cancer, as shown by the results. The results also show that the modified ResNet model can be a highly reliable and efficient tool for the early detection of lung cancer. As a result, the improved architecture leads to better diagnostic accuracy and reduced computational complexity so it can be used in medical imaging with real-time applications.},
  archive      = {J_ICV},
  author       = {Zeyad Q. Habeeb and Branislav Vuksanovic and Imad Q. Alzaydi},
  doi          = {10.1016/j.imavis.2025.105752},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105752},
  shortjournal = {Image Vis. Comput.},
  title        = {Modified ResNet model for medical image-based lung cancer detection},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFET: Multi-frequency enhancement transformer for single-image super-resolution. <em>ICV</em>, <em>163</em>, 105751. (<a href='https://doi.org/10.1016/j.imavis.2025.105751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-Image Super-Resolution (SISR) aims to reconstruct a high-resolution image from a low-resolution input while effectively preserving structural integrity and fine details. However, (i) low-frequency structural cues progressively fade during deep-layer propagation, and (ii) existing upsampling modules either ignore multi-scale context or incur excessive computation, leading to unsatisfactory high-frequency texture recovery. To address these limitations, we propose the Multi-Frequency Enhancement Transformer (MFET), a novel Transformer-based network tailored for efficient SISR. MFET seamlessly integrates low-frequency structural preservation with high-frequency detail recovery through its Multi-Frequency Block (MFB). The MFB employs a Residual Attention Mechanism (RAM) to propagate fine-grained features across layers, ensuring robust retention of low-level details, and an Efficient Upscale Module (EUM) with a pyramidal structure and depthwise separable convolutions to enhance high-frequency components with minimal computational cost. Extensive experiments on benchmark datasets demonstrate that MFET achieves superior performance in PSNR and SSIM, particularly at ×3 and ×4 scales, excelling in texture and edge reconstruction. MFET strikes an optimal balance between quality and efficiency, offering a promising solution for high-quality super-resolution. Our code is available at https://github.com/snh4/MFET .},
  archive      = {J_ICV},
  author       = {Yunlei Sun and Pengxiao Shi and Tiancheng Chen and Danning Qi and Ke Xu},
  doi          = {10.1016/j.imavis.2025.105751},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105751},
  shortjournal = {Image Vis. Comput.},
  title        = {MFET: Multi-frequency enhancement transformer for single-image super-resolution},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced crowd counting with weighted attention network and multi-scale feature integration. <em>ICV</em>, <em>163</em>, 105750. (<a href='https://doi.org/10.1016/j.imavis.2025.105750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting plays a crucial role in the field of computer vision, particularly in practical applications such as traffic monitoring. However, current methods that establish mappings between original images and density maps are not only prone to overfitting but also struggle with occlusion and scale variation in crowded scenes. In this paper, we propose a novel Weighted Attention Focusing Network (WAFNet) to enhance crowd counting performance by decoupling the image-density mapping. Our approach first employs a two-stage model to separate the image density map. It then introduces a weight map, generated by the front-end network, to address the issue of scale variation. Additionally, we incorporate a Multi-Layer Feature Compilation Module (MLFCM) to better preserve and fuse features from multiple layers and adopt a Low-Resolution Feature Enhancement Module (LRFEM) to enhance the low-resolution features of the crowd. Experiments conducted on six benchmark crowd counting datasets demonstrate that our method achieves improved performance, particularly in dense and occluded scenes.},
  archive      = {J_ICV},
  author       = {Lifang Zhou and Zhen Hu},
  doi          = {10.1016/j.imavis.2025.105750},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105750},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhanced crowd counting with weighted attention network and multi-scale feature integration},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fall detection using deep learning with features computed from recursive quadratic splits of video frames. <em>ICV</em>, <em>163</em>, 105749. (<a href='https://doi.org/10.1016/j.imavis.2025.105749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accidental falls are a leading cause of injury and death worldwide, particularly among the elderly. Despite extensive research on fall detection, many existing systems remain limited by reliance on wearable sensors that are inconvenient for continuous use, or vision-based approaches that require full video decoding, human pose estimation, or simplified datasets that fail to capture the complexity of real-life environments. As a result, their accuracy often deteriorates in realistic scenarios such as nursing homes or crowded public spaces. In this paper, we introduce a novel fall detection framework that leverages information embedded in the High Efficiency Video Coding (HEVC) standard. Unlike traditional vision-based methods, our approach extracts spatio-temporal features directly from recursive block splits and other HEVC coding information. This includes creating a sequence of four RGB input images which capture block sizes and splits of the video frames in a visual manner. The block sizes in video coding are determined based on the spatio-temporal activities in the frames, hence the suitability of using them as features. Other features are also derived from the coded videos, including compression modes, motion vectors, and prediction residuals. To enhance robustness, we integrate these features into deep learning models and employ fusion strategies that combine complementary representations. Extensive evaluations on two challenging datasets: the Real-World Fall Dataset (RFDS) and the High-Quality Fall Simulation Dataset (HQFSD), demonstrate that our method achieves superior accuracy and robustness compared to prior work. In addition, our method requires only around 23 GFLOPs per video because the deep learning network is executed on just four fixed-frame representations, whereas traditional pipelines process every frame individually, often amounting to hundreds of frames per video and orders of magnitude higher FLOPs.},
  archive      = {J_ICV},
  author       = {Zahra Solatidehkordi and Tamer Shanableh},
  doi          = {10.1016/j.imavis.2025.105749},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105749},
  shortjournal = {Image Vis. Comput.},
  title        = {Fall detection using deep learning with features computed from recursive quadratic splits of video frames},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHMG: Prompt-based human motion generation for action recognition. <em>ICV</em>, <em>163</em>, 105748. (<a href='https://doi.org/10.1016/j.imavis.2025.105748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data generation is an effective method to address inefficient and costly data collection in action recognition. Skeleton data is more robust to illumination and background than RGB data. Therefore, the generation of skeleton motions holds greater value. Existing skeleton motion generation methods generate motions that deviate from the real motion data distribution, leading to blurred inter-class boundaries and adversely affecting action recognition accuracy. In this paper, we propose a Prompt-based Human Motion Generation Network (PHMG), which consists of a Prompt-based Generation Module (PGM) and an Active Optimization Module (AOM). The encoder within the PGM integrates spatio-temporal dual-branch self-attention with graph convolution, effectively capturing both local and global motion features while maintaining the independence of spatio-temporal representations. Moreover, the PGM integrates Contrastive Language–Image Pre-Training (CLIP) encoded textual prompts into the generation process adaptively through the proposed Adaptive Weight(AW). The AOM comprises a recognition network and an active optimization layer. The recognition network produces prediction vectors for the motions generated by the PGM, while the active optimization layer evaluates these vectors using an uncertainty metric to optimize the generated motions. The PGM and AOM operate alternately to generate a refined set of motions iteratively. Extensive experiments on public datasets, namely NTU-RGB+D and NTU-RGB+D 120, reveals that our PHMG achieves excellent results in both qualitative and quantitative assessments. Notably, we attain 2.48 FMD, 92.98% accuracy on NTU-RGB+D, and 9.24 FMD, 58.47% accuracy on NTU-RGB+D 120.},
  archive      = {J_ICV},
  author       = {Kai Lu and Long Liu and Xin Wang and Siying Ren},
  doi          = {10.1016/j.imavis.2025.105748},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105748},
  shortjournal = {Image Vis. Comput.},
  title        = {PHMG: Prompt-based human motion generation for action recognition},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MBT-polyp: A new multi-branch memory-augmented transformer for polyp segmentation. <em>ICV</em>, <em>163</em>, 105747. (<a href='https://doi.org/10.1016/j.imavis.2025.105747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polyp segmentation plays a critical role in the early diagnosis and precise clinical intervention of colorectal cancer (CRC). Despite significant advancements in deep learning for medical image segmentation, accurate localization of small polyps and precise delineation of polyp boundaries remain challenges in colorectal polyp segmentation. In this study, we introduce MBT-Polyp, a Multi-branch Memory-augmented Transformer architecture designed to improve segmentation sensitivity for small polyps and enhance the delineation accuracy of ambiguous polyp boundaries. At the core of our framework is MemoryFormer, a Transformer-based U-shaped architecture that incorporates three key components: a Dynamic Focal Attention block (DFA) for efficient small target enhancement and edge refinement, a High-Level Memory Attention Module (HMAM) for preserving boundary details via cross-resolution fusion, and a Multi-View Channel Memory Attention Module (MCMAM) for suppressing background noise and modeling local spatial context. To guide specialized learning, we derive small polyp and edge labels alongside ground truth, enabling MemoryFormer to process them through dedicated branches. The outputs are fused using a Small Polyp Fusion Strategy (SPFS) and an Edge Correction Strategy (ECS) to alleviate over- and under-segmentation. The quantitative results on Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300, and ETIS-Larib yield mean Dice scores of 0.930, 0.818, 0.943, 0.912, and 0.763, respectively, demonstrating strong generalization across diverse polyp segmentation scenarios. Code and datasets are available at: https://github.com/taojlu/PolypSeg .},
  archive      = {J_ICV},
  author       = {Tao Wang and Weijie Wang and Fausto Giunchiglia and Fengzhi Zhao and Ye Zhang and Duo Yu and Guixia Liu},
  doi          = {10.1016/j.imavis.2025.105747},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105747},
  shortjournal = {Image Vis. Comput.},
  title        = {MBT-polyp: A new multi-branch memory-augmented transformer for polyp segmentation},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PixTention: Dynamic pixel-level adapter using attention maps. <em>ICV</em>, <em>163</em>, 105746. (<a href='https://doi.org/10.1016/j.imavis.2025.105746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in image generation have popularized adapter-based fine-tuning, where Low-Rank Adaptation (LoRA) modules enable efficient personalization with minimal storage costs. However, current approaches often suffer from two key limitations: (1) manually selecting suitable LoRA adapters is time-consuming and requires expert knowledge, and (2) applying multiple adapters globally can introduce style interference and reduce image fidelity, especially for prompts with multiple distinct concepts. We propose PixTention , a framework that addresses these challenges via a novel three-stage process: Curator , Selector , and Integrator . The Curator uses a vision-language model to generate enriched semantic descriptions of LoRA adapters and clusters their embeddings based on shared visual themes, enabling efficient hierarchical retrieval. The Selector embeds user prompts and first selects the most relevant adapter clusters, then identifies top-K adapters within them via cosine similarity. The Integrator leverages cross-attention maps from diffusion models to assign each retrieved adapter to specific semantic regions in the output image, ensuring localized, prompt-aligned transformations without global style overwriting. Through experiments on COCO-Multi and a custom StyleCompose dataset, PixTention achieves higher CLIP scores, IoU and lower FID than baseline retrieval and reranking methods, demonstrating superior text-image alignment and image realism. Our results highlight the importance of semantic clustering, region-specific adapter composition, and cross-modal alignment in advancing controllable, high-fidelity image generation.},
  archive      = {J_ICV},
  author       = {Dooho Choi and Yunsick Sung},
  doi          = {10.1016/j.imavis.2025.105746},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105746},
  shortjournal = {Image Vis. Comput.},
  title        = {PixTention: Dynamic pixel-level adapter using attention maps},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PR-DETR: Extracting and utilizing prior knowledge for improved end-to-end object detection. <em>ICV</em>, <em>163</em>, 105745. (<a href='https://doi.org/10.1016/j.imavis.2025.105745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The query initialization in the Transformer-based target detection algorithm has static characteristics, resulting in a limitation to flexibly adjust the degree of attention to different image features during the learning process. In addition, without the guidance of global spatial semantic information, it will cause the model to disregard the relationship between the target and the surrounding environment due to relying on local features for target detection, causing the problem of false detection or missed detection of the target. In order to solve the above problems, this paper proposes a query-optimized target detection model PR-DETR based on feature map guidance. PR-DETR designs the Aggregating Global Spatial Semantic Information module (AGSSI module) to extract and enhance global spatial semantic information. Afterwards, we design queries that participate in the interaction of local and global spatial semantic information in the encoding part in advance, so as to obtain sufficient prior knowledge and provide more accurate and efficient queries for subsequent decoding feature maps. Experiment results show that PR-DETR has significantly improved detection accuracy on the MS COCO data set compared with existing related research work. The mAP is 3.5, 2.3 and 2.0 higher than Conditional-DETR, Anchor-DETR and DAB-DETR respectively.},
  archive      = {J_ICV},
  author       = {Yukang Huo and Mingyuan Yao and Tonghao Wang and Qingbin Tian and Jiayin Zhao and Xiao Liu and Haihua Wang},
  doi          = {10.1016/j.imavis.2025.105745},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105745},
  shortjournal = {Image Vis. Comput.},
  title        = {PR-DETR: Extracting and utilizing prior knowledge for improved end-to-end object detection},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DECF-FGVC: A discriminative enhancement and complementary fusion approach for fine-grained bird visual classification. <em>ICV</em>, <em>163</em>, 105744. (<a href='https://doi.org/10.1016/j.imavis.2025.105744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained bird image recognition plays a critical role in species conservation. However, existing approaches are constrained by complex background interference, insufficient extraction of discriminative features, and limited integration of hierarchical information. While Vision Transformers (ViTs) demonstrate superior performance over CNNs in fine-grained classification tasks, they remain vulnerable to background noise, with class tokens often failing to capture key regions - overlooking the complementarity between low-level details and high-level semantics. This study proposes DECF-FGVC, a novel model incorporating three modules: Patch Contrast Enhancement (PCE), Contrast Token Refiner (CTR), and Hierarchical Token Synthesizer (HTS). These modules synergistically suppress background noise, emphasize key regions, and integrate multi-layer features through attention-weighted image reconstruction, counterfactual learning-based token refinement, and hierarchical token fusion. Extensive experiments on CUB-200-2011, NABirds, and iNaturalist2017 datasets achieve classification accuracies of 91.9%, 91.4%, and 77.92% respectively, consistently outperforming state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {ShuaiShuai Deng and Tianhua Chen and Qinghua Qiao},
  doi          = {10.1016/j.imavis.2025.105744},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105744},
  shortjournal = {Image Vis. Comput.},
  title        = {DECF-FGVC: A discriminative enhancement and complementary fusion approach for fine-grained bird visual classification},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding adversarial robustness of deep neural networks via decision reliance. <em>ICV</em>, <em>163</em>, 105743. (<a href='https://doi.org/10.1016/j.imavis.2025.105743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial robustness has become a major concern as machine learning models are increasingly deployed in high-risk and high-impact applications. Accordingly, various adversarial training strategies are proposed, making the model more robust under adversarial attack. However, similar to deep neural networks (DNNs) themselves, the mechanisms through which adversarial training strategies improve model robustness remain opaque. In this paper, we reveal how adversarial training alters the internal workings of deep neural networks by conducting neuron-wise decision reliance analysis. We find that adversarially vulnerable models predominantly rely on a small subset of predictive neurons while adversarially robust models tend to distribute their reliance across a broader range of neurons. We validate the relationship between decision reliance and adversarial robustness through comprehensive experiments across various models, training objectives, and attack scenarios. We observe that this relationship also holds for standard trained models, including those trained with Mixup or CutMix, which demonstrate improved performance against one-step adversarial attacks. Furthermore, we show that minimizing decision reliance leads to improved adversarial robustness. Our findings enrich the understanding of adversarially trained models and offer an interpretable and efficient approach to analyzing their internal mechanisms.},
  archive      = {J_ICV},
  author       = {Soyoun Won and Hyeon Bae Kim and Yong Hyun Ahn and Hong Joo Lee and Seong Tae Kim},
  doi          = {10.1016/j.imavis.2025.105743},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105743},
  shortjournal = {Image Vis. Comput.},
  title        = {Understanding adversarial robustness of deep neural networks via decision reliance},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepDCT-VO: 3D directional coordinate transformation for low-complexity monocular visual odometry using deep learning. <em>ICV</em>, <em>163</em>, 105742. (<a href='https://doi.org/10.1016/j.imavis.2025.105742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based monocular visual odometry has gained importance in robotics and autonomous navigation due to its robustness in visually challenging environments and minimal sensor requirements. However, many existing deep learning-based MVO methods suffer from high computational costs and large model sizes, making them less suitable for real-time applications in resource-limited systems. In this study, we propose DeepDCT-VO, a lightweight visual odometry method that combines three-dimensional directional coordinate transformation with a compact deep learning architecture. Unlike traditional approaches that estimate translation in a global coordinate system and are prone to drift accumulation, DeepDCT-VO uses local directional motion derived from composite rotations. This approach avoids global trajectory reconstruction, thereby improving the method’s stability and reliability. The proposed model operates on input images at multiple resolutions (120 × 120, 240 × 240, 360 × 360, and 480 × 480), leveraging attention-guided residual learning to extract robust features. Additionally, it incorporates multi-modal information—specifically depth and semantic maps—to further improve the accuracy of pose estimation. Evaluations on the KITTI odometry benchmark demonstrate that DeepDCT-VO achieves competitive trajectory estimation accuracy while maintaining real-time performance—8 ms per frame on GPU and 12 ms on CPU. Compared to the existing method with the lowest translational drift ( t rel ), DeepDCT-VO reduces model size by approximately 96.3% (from 37.5 million to 1.4 million parameters). Conversely, when compared to the lightest model in terms of parameter count, DeepDCT-VO reduces t rel from 8.57% to 1.69%, achieving an 80.3% reduction in translational drift. These results underscore the effectiveness of DeepDCT-VO in delivering accurate and efficient monocular visual odometry, particularly suited for embedded and resource-limited applications, while the proposed transformation method offers an auxiliary function in reducing translational complexity.},
  archive      = {J_ICV},
  author       = {E. Simsek and B. Ozyer},
  doi          = {10.1016/j.imavis.2025.105742},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105742},
  shortjournal = {Image Vis. Comput.},
  title        = {DeepDCT-VO: 3D directional coordinate transformation for low-complexity monocular visual odometry using deep learning},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OneN: Guided attention for natively-explainable anomaly detection. <em>ICV</em>, <em>163</em>, 105741. (<a href='https://doi.org/10.1016/j.imavis.2025.105741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial computer vision applications, anomaly detection (AD) is a critical task for ensuring product quality and system reliability. However, many existing AD systems follow a modular design that decouples classification from detection and localization tasks. Although this separation simplifies model development, it often limits generalizability and reduces practical effectiveness in real-world scenarios. Deep neural networks offer strong potential for unified solutions. Nonetheless, most current approaches still treat detection, localization and classification as separate components, hindering the development of more integrated and efficient AD pipelines. To bridge this gap, we propose OneN (One Network), a unified architecture that performs detection, localization, and classification within a single framework. Our approach distills knowledge from a high-capacity convolutional neural network (CNN) into an attention-based architecture trained under varying levels of supervision. The resulting attention maps act as interpretable pseudo-segmentation masks, enabling accurate localization of anomalous regions. To further enhance localization quality, we introduce a progressive focal loss that guides attention maps at each layer to focus on critical features. We validate our method through extensive experiments on both standardized and custom-defined industrial benchmarks. Even under weak supervision, it improves performance, reduces annotation effort, and facilitates scalable deployment in industrial environments.},
  archive      = {J_ICV},
  author       = {Pasquale Coscia and Angelo Genovese and Vincenzo Piuri and Fabio Scotti},
  doi          = {10.1016/j.imavis.2025.105741},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105741},
  shortjournal = {Image Vis. Comput.},
  title        = {OneN: Guided attention for natively-explainable anomaly detection},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test-time adaptation for object detection via dynamic dual teaching. <em>ICV</em>, <em>163</em>, 105740. (<a href='https://doi.org/10.1016/j.imavis.2025.105740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-Time Adaptation (TTA) is a practical setting in real-world applications, which aims to adapt a source-trained model to target domains with online unlabeled test data streams. Current approaches often rely on self-training, utilizing supervision signals from the source-trained model, suffering from poor adaptation due to diverse domain shifts. In this paper, we propose a novel test-time adaptation method for object detection guided by dual teachers, termed D ynamic D ual T eaching ( DDT ). Inspired by the generalization potentials of Vision-Language Models (VLMs), we integrate the VLM as an additional language-driven instructor. This integration exploits the domain-robustness of language prompts to mitigate domain shifts, collaborating with the teacher of source information within the teacher–student framework. Firstly, we utilize an ensemble prompt to guide the prediction process of the language-driven instructor. Secondly, a dynamic fusion strategy of the dual teachers is designed to generate high-quality pseudo-labels for student learning. Moreover, we incorporate a dual prediction consistency regularization to further mitigate the sensitivity of the adapted detector to domain shifts. Experiments on diverse domain adaptation benchmarks demonstrate that the proposed DDT method achieves state-of-the-art performance on both online and offline domain adaptation settings.},
  archive      = {J_ICV},
  author       = {Siqi Zhang and Lu Zhang and Zhiyong Liu},
  doi          = {10.1016/j.imavis.2025.105740},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105740},
  shortjournal = {Image Vis. Comput.},
  title        = {Test-time adaptation for object detection via dynamic dual teaching},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining fine-grained attributes for vision–semantics integration in few-shot learning. <em>ICV</em>, <em>163</em>, 105739. (<a href='https://doi.org/10.1016/j.imavis.2025.105739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Few-Shot Learning (FSL) have been significantly driven by leveraging semantic descriptions to enhance feature discrimination and recognition performance. However, existing methods, such as SemFew, often rely on verbose or manually curated attributes and apply semantic guidance only to the support set, limiting their effectiveness in distinguishing fine-grained categories. Inspired by human visual perception, which emphasizes crucial features for accurate recognition, this study introduces concise, fine-grained semantic attributes to address these limitations. We propose a Visual Attribute Enhancement (VAE) mechanism that integrates enriched semantic information into visual features, enabling the model to highlight the most relevant visual attributes and better distinguish visually similar samples. This module enhances visual features by aligning them with semantic attribute embeddings through a cross-attention mechanism and optimizes this alignment using an attribute-based cross-entropy loss. Furthermore, to mitigate the performance degradation caused by methods that supply semantic information exclusively to the support set, we propose a semantic attribute reconstruction (SAR) module. This module predicts and integrates semantic features for query samples, ensuring balanced information distribution between the support and query sets. Specifically, SAR enhances query representations by aligning and reconstructing semantic and visual attributes through regression and optimal transport losses to ensure semantic–visual consistency. Experiments on five benchmark datasets, including both general datasets and more challenging fine-grained Few-Shot datasets consistently demonstrate that our proposed method outperforms state-of-the-art methods in both 5-way 1-shot and 5-way 5-shot settings.},
  archive      = {J_ICV},
  author       = {Juan Zhao and Lili Kong and Deshang Sun and Deng Xiong and Jiancheng Lv},
  doi          = {10.1016/j.imavis.2025.105739},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105739},
  shortjournal = {Image Vis. Comput.},
  title        = {Mining fine-grained attributes for vision–semantics integration in few-shot learning},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable deepfake detection across different modalities: An overview of methods and challenges. <em>ICV</em>, <em>163</em>, 105738. (<a href='https://doi.org/10.1016/j.imavis.2025.105738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of deepfake technology enables the creation of realistic and deceptive content, raising concerns about several serious issues, including biometric authentication, misinformation, politics, privacy, and trust. Many Deepfake Detection (DD) models are entering the market to combat the misuse of deepfakes. With these developments, one primary issue occurs in ensuring the explainability of the proposed detection models to understand the rationale of the decision. This paper aims to investigate the state-of-the-art explainable DD models across multiple modalities, including image, video, audio, and text. Unlike existing surveys that focus on detection methodologies with minimal attention to explainability and limited modality coverage, this paper directly focuses on these gaps. It offers a comprehensive analysis of advanced explainability techniques, including Grad-CAM, LIME, SHAP, LRP, Saliency Maps, and Anchors, for detecting deceptive content across the modalities. It identifies the strengths and limitations of existing models and outlines research directions to enhance explainability and interpretability in future works. By exploring these models, we aim to enhance transparency, provide deeper insights into model decisions, and bridge the gap between detection accuracy with explainability in DD models.},
  archive      = {J_ICV},
  author       = {MD Sarfaraz Momin and Abu Sufian and Debaditya Barman and Marco Leo and Cosimo Distante and Naser Damer},
  doi          = {10.1016/j.imavis.2025.105738},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105738},
  shortjournal = {Image Vis. Comput.},
  title        = {Explainable deepfake detection across different modalities: An overview of methods and challenges},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic sparse and weight allocation-based text-driven person retrieval. <em>ICV</em>, <em>163</em>, 105737. (<a href='https://doi.org/10.1016/j.imavis.2025.105737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image person retrieval aims to find the most matching personimages in a large-scale persondataset through textual descriptions. However, most of the existing methods have the following problems: (1) There are still some inaccurate matching pairs in the retrieval system, and the errors of these matching pairs negatively affect the performance of the whole retrieval system. (2) In the whole training process of the model, the whole text is used directly, but there are still non-important parts of the text that are not important for recognizing the images, and how to process the text effectively is still a hot topic in current research. These critical issues significantly degrade the retrieval performance. To this end, we propose a new alignment optimization framework for text-based person retrieval. Precisely, our framework consists of three key components: (1) progressive enhancement for a multimodal integration, which not only simulates coarse-grained alignment through mathematical modeling, but also appropriately combines coarse-grained and fine-grained alignment through progressive learning; (2) global bidirectional match filtering, which utilizes subjective logic to effectively mitigate the interference of incorrectly matched pairs of image text, and at the same time utilizes a bidirectional KL match filtering algorithm so as to select the matching pairs with high degree of image text matching for training; (3) fine-grained dynamic sparse mask modeling, which uses mask language modeling and constructs a dynamic spatial sparsification module, which not only applies more expressive modules to important positions but also mines the relationship between image text pairs at a fine-grained level, thus improving retrieval performance. Extensive experiments show that the method achieves state-of-the-art results on three benchmark datasets and performs well on domain generalization tasks.},
  archive      = {J_ICV},
  author       = {Shuren Zhou and Qihang Zhou and Jiao Liu},
  doi          = {10.1016/j.imavis.2025.105737},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105737},
  shortjournal = {Image Vis. Comput.},
  title        = {Dynamic sparse and weight allocation-based text-driven person retrieval},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MITS: A large-scale multimodal benchmark dataset for intelligent traffic surveillance. <em>ICV</em>, <em>163</em>, 105736. (<a href='https://doi.org/10.1016/j.imavis.2025.105736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General-domain large multimodal models (LMMs) have achieved significant advances in various image-text tasks. However, their performance in the Intelligent Traffic Surveillance (ITS) domain remains limited due to the absence of dedicated multimodal datasets. To address this gap, we introduce MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale multimodal benchmark dataset specifically designed for ITS. MITS includes 170,400 independently collected real-world ITS images sourced from traffic surveillance cameras, annotated with eight main categories and 24 subcategories of ITS-specific objects and events under diverse environmental conditions. Additionally, through a systematic data generation pipeline, we generate high-quality image captions and 5 million instruction-following visual question-answer pairs , addressing five critical ITS tasks : object and event recognition, object counting, object localization, background analysis, and event reasoning. To demonstrate MITS’s effectiveness, we fine-tune mainstream LMMs on this dataset, enabling the development of ITS-specific applications. Experimental results show that MITS significantly improves LMM performance in ITS applications, increasing LLaVA-1.5’s performance from 0.494 to 0.905 (+83.2%), LLaVA-1.6’s from 0.678 to 0.921 (+35.8%), Qwen2-VL’s from 0.584 to 0.926 (+58.6%), and Qwen2.5-VL’s from 0.732 to 0.930 (+27.0%). We release the dataset, code, and models as open-source , providing high-value resources to advance both ITS and LMM research.},
  archive      = {J_ICV},
  author       = {Kaikai Zhao and Zhaoxiang Liu and Peng Wang and Xin Wang and Zhicheng Ma and Yajun Xu and Wenjing Zhang and Yibing Nan and Kai Wang and Shiguo Lian},
  doi          = {10.1016/j.imavis.2025.105736},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105736},
  shortjournal = {Image Vis. Comput.},
  title        = {MITS: A large-scale multimodal benchmark dataset for intelligent traffic surveillance},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CDAF: Cross-modal and dual-channel upsample adaptive fusion network for point cloud completion. <em>ICV</em>, <em>163</em>, 105735. (<a href='https://doi.org/10.1016/j.imavis.2025.105735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, point cloud data often suffers from incompleteness due to limitations in sensor viewpoints, resolution constraints, and self-occlusions, which hinders its applications in domains such as autonomous driving and robotics. To address these challenges, this paper proposes a novel Cross-Modal and Dual-channel Upsample Adaptive Fusion network (CDAF), Our framework innovatively integrates depth maps with point clouds through dual-channel attention and gating units, significantly improving completion accuracy and detail recovery. The framework comprises two core modules: Cross-Modal Feature Enhancement (CMFE) and Dual-channel Upsampling Adaptive Fusion (DUAF). CMFE enhances point cloud feature representation by leveraging Spatial-activated Channel Attention to model channel-wise dependencies and Max-Sigmoid Attention to align cross-modal features between depth maps and point clouds, DUAF progressively refines coarse point clouds through a parallel structural analysis and similarity alignment branches, enabling adaptive fusion of local geometric priors and global shape consistency. Experimental results on multiple benchmark datasets demonstrate that CDAF surpasses existing state-of-the-art methods in point cloud completion tasks, showcasing superior global shape understanding and detail recovery.},
  archive      = {J_ICV},
  author       = {Ming Lu and Jian Li and Duo Han Zhao and Qin Wang},
  doi          = {10.1016/j.imavis.2025.105735},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105735},
  shortjournal = {Image Vis. Comput.},
  title        = {CDAF: Cross-modal and dual-channel upsample adaptive fusion network for point cloud completion},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UNIR-net: A novel approach for restoring underwater images with non-uniform illumination using synthetic data. <em>ICV</em>, <em>163</em>, 105734. (<a href='https://doi.org/10.1016/j.imavis.2025.105734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring underwater images affected by non-uniform illumination (NUI) is essential to improve visual quality and usability in marine applications. Conventional methods often fall short in handling complex illumination patterns, while learning-based approaches face challenges due to the lack of targeted datasets. To address these limitations, the Underwater Non-uniform Illumination Restoration Network (UNIR-Net) is proposed. UNIR-Net integrates multiple components, including illumination enhancement, attention mechanisms, visual refinement, and contrast correction, to effectively restore underwater images affected by NUI. In addition, the Paired Underwater Non-uniform Illumination (PUNI) dataset is introduced, specifically designed for training and evaluating models under NUI conditions. Experimental results on PUNI and the large-scale real-world Non-Uniform Illumination Dataset (NUID) show that UNIR-Net achieves superior performance in both quantitative metrics and visual outcomes. UNIR-Net also improves downstream tasks such as underwater semantic segmentation, highlighting its practical relevance. The code is available at https://github.com/xingyumex/UNIR-Net .},
  archive      = {J_ICV},
  author       = {Ezequiel Pérez-Zarate and Chunxiao Liu and Oscar Ramos-Soto and Diego Oliva and Marco Pérez-Cisneros},
  doi          = {10.1016/j.imavis.2025.105734},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105734},
  shortjournal = {Image Vis. Comput.},
  title        = {UNIR-net: A novel approach for restoring underwater images with non-uniform illumination using synthetic data},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FFENet: A frequency fusion and enhancement network for camouflaged object detection. <em>ICV</em>, <em>163</em>, 105733. (<a href='https://doi.org/10.1016/j.imavis.2025.105733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of camouflaged object detection (COD) is to accurately find camouflaged objects hidden in their surroundings. Although most of the existing frequency-domain based COD models can boost the performance of COD to a certain extent by utilizing the frequency domain information, the frequency feature fusion strategies they adopt tend to ignore the complementary effects between high-frequency features and low-frequency features. In addition, most of the existing frequency-domain based COD models also do not consider enhancing camouflaged objects using low-level frequency-domain features. In order to solve these problems, we present a frequency fusion and enhancement network (FFENet) for camouflaged object detection, which mainly includes three stages. In the frequency feature extraction stage, we design a frequency feature learning module (FLM) to extract corresponding high-frequency features and low-frequency features. In the frequency feature fusion stage, we design a frequency feature fusion module (FFM) that can increase the representation ability of the fused features by adaptively assigning weights to the high-frequency features and the low-frequency features using a cross-attention mechanism. In the frequency feature guidance information enhancement stage, we design a frequency feature guidance information enhancement module (FGIEM) to enhance the contextual information and detail information of camouflaged objects in the fused features under the guidance of the low-level frequency features. Extensive experimental results on the COD10K, CHAMELEON, NC4K and CAMO datasets show that our model is superior to most existing COD models.},
  archive      = {J_ICV},
  author       = {Haishun Du and Wenzhe Zhang and Sen Wang and Zhengyang Zhang and Linbing Cao},
  doi          = {10.1016/j.imavis.2025.105733},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105733},
  shortjournal = {Image Vis. Comput.},
  title        = {FFENet: A frequency fusion and enhancement network for camouflaged object detection},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UpAttTrans: Upscaled attention based transformer for facial image super-resolution. <em>ICV</em>, <em>163</em>, 105731. (<a href='https://doi.org/10.1016/j.imavis.2025.105731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) aims to reconstruct high-quality images from low-resolution inputs, a task particularly challenging in face-related applications due to extreme degradations and modality differences (e.g., visible, low-resolution, near-infrared). Conventional convolutional neural networks (CNNs) and GAN-based approaches have achieved notable success; however, they often struggle with preserving identity and fine structural details at high upscaling factors. In this work, we introduce UpAttTrans, a novel attention mechanism that connects original and upsampled features for better detail recovery based on vision transformer for SR. The core generator leverages a custom UpAttTrans module that translates input image patches into embeddings, processes them through transformer layers enhanced with connector-up attention, and reconstructs high-resolution outputs with improved detail retention. We evaluate our model on the CelebA dataset across multiple upscaling factors ( 4 × , 8 × , 16 × , 32 × , and 64 × ). UpAttTrans achieves a 24.63% increase in PSNR, 21.56% in SSIM, and 19.61% reduction in FID for 4 × and 8 × SR, outperforming state-of-the-art baselines. Additionally, for higher magnification levels, our model maintains strong performance, with average gains of 6.20% in PSNR and 21.49% in SSIM, indicating its robustness in extreme SR settings. These findings suggest that UpAttTrans holds significant promise for real-world applications such as face recognition in surveillance, forensic image enhancement, and cross-spectral matching, where high-quality reconstruction from severely degraded inputs is critical.},
  archive      = {J_ICV},
  author       = {Neeraj Baghel and Shiv Ram Dubey and Satish Kumar Singh},
  doi          = {10.1016/j.imavis.2025.105731},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105731},
  shortjournal = {Image Vis. Comput.},
  title        = {UpAttTrans: Upscaled attention based transformer for facial image super-resolution},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel extraction of discriminative fine-grained feature to improve retinal vessel segmentation. <em>ICV</em>, <em>163</em>, 105729. (<a href='https://doi.org/10.1016/j.imavis.2025.105729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation is a vital early detection method for several severe ocular diseases. Despite significant progress in retinal vessel segmentation with the advancement of Neural Networks, there are still challenges to overcome. Specifically, retinal vessel segmentation aims to predict the class label for every pixel within a fundus image, with a primary focus on intra-image discrimination, making it vital for models to extract more discriminative features. Nevertheless, existing methods primarily focus on minimizing the difference between the output from the decoder and the label, but ignore fully using feature-level fine-grained representations from the encoder. To address these issues, we propose a novel Attention U-shaped Kolmogorov–Arnold Network named AttUKAN along with a novel Label-guided Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we implement Attention Gates into Kolmogorov–Arnold Networks to enhance model sensitivity by suppressing irrelevant feature activations and model interpretability by non-linear modeling of KAN blocks. Additionally, we also design a novel Label-guided Pixel-wise Contrastive Loss to supervise our proposed AttUKAN to extract more discriminative features by distinguishing between foreground vessel-pixel pairs and background pairs. Experiments are conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%, 80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and 66.94% in the above datasets, which are the highest compared to 11 networks for retinal vessel segmentation. Quantitative and qualitative results show that our AttUKAN achieves state-of-the-art performance and outperforms existing retinal vessel segmentation methods. Our code will be available at https://github.com/stevezs315/AttUKAN .},
  archive      = {J_ICV},
  author       = {Shuang Zeng and Chee Hong Lee and Micky C. Nnamdi and Wenqi Shi and J. Ben Tamo and Hangzhou He and Xinliang Zhang and Qian Chen and May D. Wang and Lei Zhu and Yanye Lu and Qiushi Ren},
  doi          = {10.1016/j.imavis.2025.105729},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105729},
  shortjournal = {Image Vis. Comput.},
  title        = {Novel extraction of discriminative fine-grained feature to improve retinal vessel segmentation},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence content detection techniques using watermarking: A survey. <em>ICV</em>, <em>163</em>, 105728. (<a href='https://doi.org/10.1016/j.imavis.2025.105728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement in AI-generated content has catalyzed artistic creation, advertising, and media dissemination. Despite their widespread applications across several domains, AI-generated content inherently poses risks of identity fraud, copyright violation and unauthorized use. Watermarking has emerged as a critical tool for copyright protection, allowing embedding of identification information in AI-generated content, and enhances traceability and verification without hurting user experience. In this study, we provide a systematic literature review of the technique for detecting AI content, especially text and images, using watermarking, spanning studies from 2010 to 2025. Studies included in this review were peer-reviewed articles that applied watermarking to effectively distinguish AI-generated content from real or human-written content. We report strong past and current approaches to detecting watermarking-based AI content, especially text and images. This includes an analysis of how watermarking methods are used on AI-generated content, their role in enhancing performance, and a detail comparative analysis of notable techniques. Furthermore, we discuss how these methods have been evaluated, identify the research gaps and potential solutions. Our findings provide valuable insights for future watermarking-based AI content detection researchers, applications and organizations seeking to implement watermarking solutions in potential applications. To the best of our knowledge, we are the first to explore the detection of AI content, especially text and image, detection using watermarking.},
  archive      = {J_ICV},
  author       = {Nishant Kumar and Amit Kumar Singh},
  doi          = {10.1016/j.imavis.2025.105728},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105728},
  shortjournal = {Image Vis. Comput.},
  title        = {Artificial intelligence content detection techniques using watermarking: A survey},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Your image generator is your new private dataset. <em>ICV</em>, <em>163</em>, 105727. (<a href='https://doi.org/10.1016/j.imavis.2025.105727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative diffusion models have emerged as powerful tools to synthetically produce training data, offering potential solutions to data scarcity and reducing labelling costs for downstream supervised deep learning applications. However, existing approaches for synthetic dataset generation face significant limitations: previous methods like Knowledge Recycling rely on label-conditioned generation with models trained from scratch, limiting flexibility and requiring extensive computational resources, while simple class-based conditioning fails to capture the semantic diversity and intra-class variations found in real datasets. Additionally, effectively leveraging text-conditioned image generation for building classifier training sets requires addressing key issues: constructing informative textual prompts, adapting generative models to specific domains, and ensuring robust performance. This paper proposes the Text-Conditioned Knowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines dynamic image captioning, parameter-efficient diffusion model fine-tuning, and Generative Knowledge Distillation techniques to create synthetic datasets tailored for image classification. The pipeline is rigorously evaluated on ten diverse image classification benchmarks. The results demonstrate that models trained solely on TCKR-generated data achieve classification accuracies on par with (and in several cases exceeding) models trained on real images. Furthermore, the evaluation reveals that these synthetic-data-trained models exhibit substantially enhanced privacy characteristics: their vulnerability to Membership Inference Attacks is significantly reduced, with the membership inference AUC lowered by 5.49 points on average compared to using real training data, demonstrating a substantial improvement in the performance-privacy trade-off. These findings indicate that high-fidelity synthetic data can effectively replace real data for training classifiers, yielding strong performance whilst simultaneously providing improved privacy protection as a valuable emergent property. The code and trained models are available in the accompanying open-source repository .},
  archive      = {J_ICV},
  author       = {Nicolò Francesco Resmini and Eugenio Lomurno and Cristian Sbrolli and Matteo Matteucci},
  doi          = {10.1016/j.imavis.2025.105727},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105727},
  shortjournal = {Image Vis. Comput.},
  title        = {Your image generator is your new private dataset},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the noise robustness of class activation maps: A framework for reliable model interpretability. <em>ICV</em>, <em>163</em>, 105717. (<a href='https://doi.org/10.1016/j.imavis.2025.105717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class Activation Maps (CAMs) are one of the important methods for visualizing regions used by deep learning models. Yet their robustness to different noise remains underexplored. In this work, we evaluate and report the resilience of various CAM methods for different noise perturbations across multiple architectures and datasets. By analyzing the influence of different noise types on CAM explanations, we assess the susceptibility to noise and the extent to which dataset characteristics may impact explanation stability. The findings highlight considerable variability in noise sensitivity for various CAMs. We propose a robustness metric for CAMs that captures two key properties: consistency and responsiveness. Consistency reflects the ability of CAMs to remain stable under input perturbations that do not alter the predicted class, while responsiveness measures the sensitivity of CAMs to changes in the prediction caused by such perturbations. The metric is evaluated empirically across models, different perturbations, and datasets along with complementary statistical tests to exemplify the applicability of our proposed approach.},
  archive      = {J_ICV},
  author       = {Syamantak Sarkar and Revoti P. Bora and Bhupender Kaushal and Sudhish N. George and Kiran Raja},
  doi          = {10.1016/j.imavis.2025.105717},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105717},
  shortjournal = {Image Vis. Comput.},
  title        = {Assessing the noise robustness of class activation maps: A framework for reliable model interpretability},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InceptionWTMNet: A hybrid network for alzheimer’s disease detection using wavelet transform convolution and mixed local channel attention on finely fused multimodal images. <em>ICV</em>, <em>163</em>, 105693. (<a href='https://doi.org/10.1016/j.imavis.2025.105693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal fusion has emerged as a critical technique for the diagnosis of Alzheimer’s Disease (AD), with the aim of effectively extracting and utilising complementary information from diverse modalities. Current fusion methods frequently cause the precise alignment of source images and do not adequately address parallax issues. This oversight can result in artifacts during the fusion process when images are misaligned. In response to this challenge, we propose a refined registration fusion technique, termed MURF, which integrates multimodal image registration and fusion within a cohesive framework. The Vision Transformer (ViT) has inspired the application of large-kernel convolutions in the diagnosis of Alzheimer’s disease (AD) because of its ability to model long-range dependencies. This approach aims to expand the receptive field and enhance the performance of diagnostic models. Despite requiring a minimal number of floating-point operations (FLOPs), these deep operators encounter challenges associated with over-parameterisation because of high memory access costs, which ultimately compromises computational efficiency. By utilising wavelet transform convolutions (WTConv), we decompose large-kernel depth-wise convolutions into four parallel branches. One branch employs a wavelet-transform convolution with square kernels, while the other two branches incorporate orthogonal wavelet-transform kernels with an identity mapping. This innovative method, with a Mixed Local Channel Attention mechanism, has facilitated the development of the InceptionWTConvolutions network. This network maintains a receptive field comparable to that of large-kernel convolutions, while concurrently minimising over-parameterisation and enhancing computational efficiency. InceptionWTMNet classified AD, MCI, and NC using MRI and PET data from ADNI dataset with 98.69% accuracy, 98.65% recall, 98.70% F1-score, and 98.98% AUC. and provide Graphical abstract in correct format.},
  archive      = {J_ICV},
  author       = {Zenan Xu and Zhengyao Bai and Han Ma and Mingqiang Xu and Qiqin Huang and Tao Lin},
  doi          = {10.1016/j.imavis.2025.105693},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105693},
  shortjournal = {Image Vis. Comput.},
  title        = {InceptionWTMNet: A hybrid network for alzheimer’s disease detection using wavelet transform convolution and mixed local channel attention on finely fused multimodal images},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
