<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ARTMED</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="artmed">ARTMED - 9</h2>
<ul>
<li><details>
<summary>
(2025). The interpretable deep learning framework and validation for seizure detection in pediatric electroencephalography: An improved accuracy and performance analysis. <em>ARTMED</em>, <em>170</em>, 103276. (<a href='https://doi.org/10.1016/j.artmed.2025.103276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an interpretable deep learning framework and compares the two novel models. A fully convolutional network with squeeze-and-excitation modules (SE-FCN) is designed to enhance spatial sensitivity and retain temporal resolution. In addition, a transformer-based model (TransNet) is developed to capture temporal and channel-wise dependencies via self-attention. These two models output channel saliency weights to the EEG electrode space and generate heatmaps for inferring potential epileptogenic zones. Deep learning primarily adopts convolutional neural networks (CNNs) or sequence generation networks (SGNs) and faces the limitations. For instance, CNN-based models often lack hierarchical modeling and fail to quantify channel-wise contributions, hindering spatial localization. SGN-based models struggle to capture complex spatiotemporal dependencies and typically lack adaptive attention tailored to electroencephalography (EEG) characters. Epileptic seizure detection is vital for effective clinical intervention and existing methods operated as black boxes, limiting clinical interpretability. This study evaluates the models on the CHB-MIT pediatric EEG dataset using a subject-independent cross-validation protocol. SE-FCN achieves an AUC of 0.89 and accuracy of 86.7 %, while TransNet achieves an AUC of 0.92 and accuracy of 86.4 %. Saliency maps from both models demonstrate high consistency and enable categorization of 22 patients into five groups based on inferred seizure origins.},
  archive      = {J_ARTMED},
  author       = {Yu Zhou and Yuxin Gao and Qiang Li and Ruiheng Wu and Aiping Yang and Ming-Lang Tseng},
  doi          = {10.1016/j.artmed.2025.103276},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103276},
  shortjournal = {Artif. Intell. Med.},
  title        = {The interpretable deep learning framework and validation for seizure detection in pediatric electroencephalography: An improved accuracy and performance analysis},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding the cortical responses to mechanical wrist perturbations: A two-step shared structure NARX method. <em>ARTMED</em>, <em>170</em>, 103273. (<a href='https://doi.org/10.1016/j.artmed.2025.103273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared structure nonlinear autoregressive with exogenous input (NARX) model is a promising tool for exploring cortical responses mechanism to external stimuli, essential for advancing our understanding of brain function and developing methods for direct brain information encoding. In this paper, we proposed a two-step method to overcome limitations in existing method, which neglect data relationships and rely on a greedy search for regression terms, leading to less accurate models. In our approach, data from multiple trials are concatenated, and then the orthogonal forward regression (OFR) algorithm identifies model terms in first step, enhancing inter-trial connections and establishing a preliminary model for each subject. Shared model terms across subjects are then used to construct a general target model. Next, non-shared regression terms that best represent population-level information are identified, using adaptive multi-population genetic algorithms, and use to enhance the target models' descriptive power. Simulations results show significant competitiveness in terms of accuracy as compared to other state-of-the-art methods. When applied to real electroencephalography signals under mechanical disturbance, structural and parameter analysis revealed consistent neural response patterns across subjects, with subject-specific responses likely stemming from muscle feedback. Frequency response analysis further suggests that the brain may generate motor inhibition signals based on sensory inputs to maintain a pre-disturbance resting state. These findings provide valuable insights into cortical response mechanisms and have potential implications for future brain information encoding research.},
  archive      = {J_ARTMED},
  author       = {Nan Zheng and Yurong Li and Wuxiang Shi and Jiyu Tan},
  doi          = {10.1016/j.artmed.2025.103273},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103273},
  shortjournal = {Artif. Intell. Med.},
  title        = {Decoding the cortical responses to mechanical wrist perturbations: A two-step shared structure NARX method},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven dynamic grouping for adaptive clinical trials: Rethinking randomization in precision medicine. <em>ARTMED</em>, <em>170</em>, 103272. (<a href='https://doi.org/10.1016/j.artmed.2025.103272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating artificial intelligence into biomedical human subjects research is transforming traditional experimental paradigms. This perspective introduces the concept of “dynamic grouping,” wherein artificial intelligence (AI) systems continuously reassign participants across experimental conditions based on real-time biomarker data and clinical response patterns. Unlike traditional biomedical research designs that rely on fixed treatment and control groups, dynamic grouping allows participant assignments to evolve throughout the study. We examine the ethical implications, methodological challenges, and research opportunities associated with this paradigm, particularly in clinical trials, precision medicine, and digital therapeutics. To support this analysis, we present three computational simulations that quantify its impact: (i) a heterogeneity simulation demonstrating how patient variability affects the advantage of dynamic grouping, (ii) a statistical power analysis showing potential sample size reductions in adaptive designs, and (iii) a clinical outcome distribution analysis highlighting how dynamic grouping reduces negative treatment outcomes and optimizes patient responses. Our findings suggest that dynamic grouping can improve treatment effectiveness, enhance resource allocation, and increase statistical efficiency, although it also raises new challenges for causal inference, informed consent, and regulatory oversight. As AI continues to reshape medical research, adapting ethical and methodological frameworks will be essential for its responsible implementation.},
  archive      = {J_ARTMED},
  author       = {Madhur Mangalam},
  doi          = {10.1016/j.artmed.2025.103272},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103272},
  shortjournal = {Artif. Intell. Med.},
  title        = {AI-driven dynamic grouping for adaptive clinical trials: Rethinking randomization in precision medicine},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining multi-electrode and multi-wave electroencephalogram based time-interval temporal patterns for improved classification capabilities and explainability. <em>ARTMED</em>, <em>170</em>, 103269. (<a href='https://doi.org/10.1016/j.artmed.2025.103269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-computer interface (BCI) systems, and particularly electroencephalogram (EEG) based BCI systems, have become more widely used in recent years and are utilized in various applications and domains ranging from medicine and marketing to games and entertainment. While different algorithms have been used to analyze EEG data and enable its classification, existing algorithms have two main drawbacks; both their classification and explainability capabilities are limited. Lacking in explainability, they cannot indicate which electrodes and waves led to a classification decision or explain how areas and frequencies of the brain's activity correlate to a specific task. In this study, we propose a novel extension for the time-interval temporal patterns mining algorithms aimed at enhancing the data mining process by enabling a richer set of patterns to be learned from the EEG data, thereby contributing to improved classification and explainability capabilities. The extended algorithm is designed to capture and leverage the unique nature of EEG data by decomposing it into different brain waves and modeling the relations among them and between different electrodes. Our evaluation of the proposed extended algorithm on multiple learning tasks and three EEG datasets demonstrated the extended algorithm's ability to mine richer patterns that improve the classification performance by 4–11 % based on the Area-Under the receiver operating characteristic Curve (AUC) metric, compared to the original version of the algorithm. Moreover, the algorithm was shown to shed light on the areas and frequencies of the brain's activity that are correlated with specific tasks.},
  archive      = {J_ARTMED},
  author       = {Ofir Landau and Nir Nissim},
  doi          = {10.1016/j.artmed.2025.103269},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103269},
  shortjournal = {Artif. Intell. Med.},
  title        = {Mining multi-electrode and multi-wave electroencephalogram based time-interval temporal patterns for improved classification capabilities and explainability},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical multimodal foundation models in clinical diagnosis and treatment: Applications, challenges, and future directions. <em>ARTMED</em>, <em>170</em>, 103265. (<a href='https://doi.org/10.1016/j.artmed.2025.103265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning have significantly revolutionized the field of clinical diagnosis and treatment, offering novel approaches to improve diagnostic precision and treatment efficacy across diverse clinical domains, thus driving the pursuit of precision medicine. The growing availability of multi-organ and multimodal datasets has accelerated the development of large-scale Medical Multimodal Foundation Models (MMFMs). These models, known for their strong generalization capabilities and rich representational power, are increasingly being adapted to address a wide range of clinical tasks, from early diagnosis to personalized treatment strategies. This review offers a comprehensive analysis of recent developments in MMFMs, focusing on three key aspects: datasets, model architectures, and clinical applications. We also explore the challenges and opportunities in optimizing multimodal representations and discuss how these advancements are shaping the future of healthcare by enabling improved patient outcomes and more efficient clinical workflows.},
  archive      = {J_ARTMED},
  author       = {Kai Sun and Siyan Xue and Fuchun Sun and Haoran Sun and Yu Luo and Ling Wang and Siyuan Wang and Na Guo and Lei Liu and Tian Zhao and Xinzhou Wang and Lei Yang and Shuo Jin and Jun Yan and Jiahong Dong},
  doi          = {10.1016/j.artmed.2025.103265},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103265},
  shortjournal = {Artif. Intell. Med.},
  title        = {Medical multimodal foundation models in clinical diagnosis and treatment: Applications, challenges, and future directions},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end solution for out-of-hospital emergency medical dispatch triage based on multimodal and continual deep learning. <em>ARTMED</em>, <em>170</em>, 103264. (<a href='https://doi.org/10.1016/j.artmed.2025.103264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this study was to build a multimodal, multitask predictive model—named E2eDeepEMC 2 —to improve out-of-hospital emergency incident severity assessments while coping with shifts in data distributions over time. We drew on 2 054 694 independent incidents recorded by the Valencian emergency medical dispatch service between 2009 and 2019 (excluding 2013), combining demographic, temporal, clinical and free-text inputs. To handle temporal drift, our model integrates continual learning strategies and comprises three encoder modules (for context, clinical data and text), whose outputs are merged to predict the life-threatening level, admissible response delay and emergency system jurisdiction. Compared with the Valencian Region’s existing in-house triage protocol, E2eDeepEMC 2 achieved absolute F1-score gains of 18.46% for life-threatening level, 25.96% for response delay and 3.63% for jurisdiction. Compared to non-continual learning baselines, it also outperformed them by 3.04%, 9.66% and 0.58%, respectively. Deployment of E2eDeepEMC 2 is currently underway in the Valencian Region, underscoring its practical impact on real-world emergency dispatch decision-making.},
  archive      = {J_ARTMED},
  author       = {Pablo Ferri and Carlos Sáez and Antonio Félix-De Castro and Purificación Sánchez-Cuesta and Juan M. García-Gómez},
  doi          = {10.1016/j.artmed.2025.103264},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103264},
  shortjournal = {Artif. Intell. Med.},
  title        = {An end-to-end solution for out-of-hospital emergency medical dispatch triage based on multimodal and continual deep learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoRA-PT: Low-rank adapting UNETR for hippocampus segmentation using principal tensor singular values and vectors. <em>ARTMED</em>, <em>170</em>, 103254. (<a href='https://doi.org/10.1016/j.artmed.2025.103254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hippocampus is an important brain structure involved in various psychiatric disorders, and its automatic and accurate segmentation is vital for studying these diseases. Recently, deep learning-based methods have made significant progress in hippocampus segmentation. However, training deep neural network models requires substantial computational resources, time, and a large amount of labeled training data, which is frequently scarce in medical image segmentation. To address these issues, we propose LoRA-PT, a novel parameter-efficient fine-tuning (PEFT) method that transfers the pre-trained UNETR model from the BraTS2021 dataset to the hippocampus segmentation task. Specifically, LoRA-PT divides the parameter matrix of the transformer structure into three distinct sizes, yielding three third-order tensors. These tensors are decomposed using tensor singular value decomposition to generate low-rank tensors consisting of the principal singular values and vectors, with the remaining singular values and vectors forming the residual tensor. During fine-tuning, only the low-rank tensors (i.e., the principal tensor singular values and vectors) are updated, while the residual tensors remain unchanged. We validated the proposed method on three public hippocampus datasets, and the experimental results show that LoRA-PT outperformed state-of-the-art PEFT methods in segmentation accuracy while significantly reducing the number of parameter updates. Our source code is available at https://github.com/WangangCheng/LoRA-PT/tree/LoRA-PT .},
  archive      = {J_ARTMED},
  author       = {Guanghua He and Wangang Cheng and Hancan Zhu and Gaohang Yu},
  doi          = {10.1016/j.artmed.2025.103254},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103254},
  shortjournal = {Artif. Intell. Med.},
  title        = {LoRA-PT: Low-rank adapting UNETR for hippocampus segmentation using principal tensor singular values and vectors},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMSupcon: An image fusion-based multi-modal supervised contrastive method for brain tumor diagnosis. <em>ARTMED</em>, <em>170</em>, 103253. (<a href='https://doi.org/10.1016/j.artmed.2025.103253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis of brain tumors is pivotal for effective treatment, with MRI serving as a commonly used non-invasive diagnostic modality in clinical practices. Fundamentally, brain tumor diagnosis is a type of pattern recognition task that requires the integration of information from multi-modal MRI images. However, existing fusion strategies are hindered by the scarcity of multi-modal imaging samples. In this paper, we propose a new training paradigm tailored for the scenario of multi-modal imaging in brain tumor diagnosis, called multi-modal supervised contrastive learning method (MMSupcon). This method significantly enhances diagnostic accuracy through two key components: multi-modal medical image fusion and multi-modal supervised contrastive loss. First, the fusion component integrates complementary imaging modalities to generate information-rich samples. Second, by introducing fused samples to guide original samples in learning feature consistency or inconsistency among classes, our loss component effectively preserves the integrity of cross-modal information while maintaining the distinctiveness of individual modalities. Finally, MMSupcon is validated on a real-world brain tumor dataset collected from Beijing Tiantan Hospital, achieving state-of-the-art performance. Furthermore, additional experiments on two public BraTS glioma classification datasets also demonstrate our substantial performance improvements. The source code is released at https://github.com/hywang02/MMSupcon .},
  archive      = {J_ARTMED},
  author       = {Haoyu Wang and Jing Zhang and Siying Wu and Haoran Wei and Xun Chen and Yunwei Ou and Xiaoyan Sun},
  doi          = {10.1016/j.artmed.2025.103253},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103253},
  shortjournal = {Artif. Intell. Med.},
  title        = {MMSupcon: An image fusion-based multi-modal supervised contrastive method for brain tumor diagnosis},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathway information on methylation analysis using deep neural network (PROMINENT): An interpretable deep learning method with pathway prior for phenotype prediction using gene-level DNA methylation. <em>ARTMED</em>, <em>170</em>, 103236. (<a href='https://doi.org/10.1016/j.artmed.2025.103236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background DNA methylation is a key epigenetic marker that influences gene expression and phenotype regulation, and is affected by both genetic and environmental factors. Traditional linear regression methods such as elastic nets have been employed to assess the cumulative effects of multiple DNA methylation markers on phenotypes. However, these methods often fail to capture the complex nonlinear nature of the data. Recent deep learning approaches, such as MethylNet, have improved the prediction accuracy but lack interpretability and efficiency. Findings To address these limitations, we introduced P athway Info r mati o n on M ethylat i on Analysis using a Deep Ne ural N e t work (PROMINENT), a novel interpretable deep learning method that integrates gene-level DNA methylation data with biological pathway information for phenotype prediction. PROMINENT enhances interpretability and prediction accuracy by incorporating gene- and pathway-level priors from databases such as Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG). It employs SHapley Additive exPlanations (SHAP) to prioritize significant genes and pathways. Evaluated across various datasets, childhood asthma, idiopathic pulmonary fibrosis (IPF), and first-episode psychosis (FEP)—PROMINENT consistently outperformed existing methods in terms of prediction accuracy and computational efficiency. PROMINENT also identified crucial genes and pathways involved in disease mechanisms. Conclusions PROMINENT represents a significant advancement in leveraging DNA methylation data for phenotype prediction, offering both high accuracy and interpretability within reasonable computational time. This method holds promise for elucidating the epigenetic underpinnings of complex diseases and enhancing the utility of DNA methylation data in biomedical research.},
  archive      = {J_ARTMED},
  author       = {Soyeon Kim and Laizhi Zhang and Yidi Qin and Rebecca I. Caldino Bohn and Hyun Jung Park},
  doi          = {10.1016/j.artmed.2025.103236},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103236},
  shortjournal = {Artif. Intell. Med.},
  title        = {Pathway information on methylation analysis using deep neural network (PROMINENT): An interpretable deep learning method with pathway prior for phenotype prediction using gene-level DNA methylation},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
