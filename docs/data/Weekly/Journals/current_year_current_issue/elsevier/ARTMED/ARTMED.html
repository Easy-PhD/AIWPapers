<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ARTMED</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="artmed">ARTMED - 20</h2>
<ul>
<li><details>
<summary>
(2025). Preprocessing narrative texts in electronic medical records to identify hospital adverse events: A scoping review. <em>ARTMED</em>, <em>170</em>, 103281. (<a href='https://doi.org/10.1016/j.artmed.2025.103281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Narrative electronic medical records (EMR), which include textual notes created by clinicians within healthcare environments, represent a significant resource for documenting various facets of patient care. This form of text exhibits distinctive characteristics, such as the occurrence of grammatically incorrect sentences, abbreviations, frequent acronyms, specialized characters with particular meanings, negation expressions, and sporadic misspellings. As a result, a primary goal in processing these textual notes is to implement effective preprocessing techniques that enhance data quality and ensure consistency across all entries. Recent advancements in algorithms and methodologies within the fields of natural language processing (NLP), machine learning (ML), and large language models (LLM) have prompted researchers to leverage narrative EMR for the detection of hospital adverse events (HAE). Methods: The scoping review adhered to the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines. A scoping review protocol was developed and utilized to guide the research process, clearly outlining the eligibility criteria, information sources, search strategies, data management, selection process, data collection procedures, data items, outcomes and prioritization, data synthesis, and meta-bias considerations. The search strategy was implemented across nine engineering and medical electronic databases. Results: The results have indicated that from a total of 3,264 studies retrieved, 48 unique studies were included in the review. Responses to the research questions were systematically extracted from these studies. The review has identified challenges associated with the preprocessing of narrative texts in EMR for HAE identification. Additionally, three research gaps have been identified: (1) the imperative need for a pipeline to preprocess narrative EMR for the identification of HAE, (2) the necessity for a robust system capable of managing the extensive volume of narrative EMR data, and (3) the requirement for temporal event system, which are essential for effective HAE detection. The study also has underscored the essential role of preprocessing tasks in enhancing the performance of HAE detection. The study has emphasized the importance of extracting N-grams from clinical text, normalizing these N-grams through lemmatization and/or stemming, and establishing semantic feature extraction in preprocessing tasks that significantly affect HAE detection performance. While LLM-based systems naturally incorporate tokenization and normalization processes within their frameworks, it remains crucial to address features that hold semantic relevance to the specific type of HAE during preprocessing. Conclusion: This scoping review has provided valuable insights for researchers focused on HAE detection utilizing narrative EMR data. It has elucidated how preprocessing tasks can elevate the performance of HAE detection and draws attention to neglected research gaps within the field. Addressing these gaps will necessitate further investigation in subsequent research endeavors.},
  archive      = {J_ARTMED},
  author       = {Hamed Jafarpour and Guosong Wu and Cheligeer (Ken) Cheligeer and Jun Yan and Yuan Xu and Danielle A. Southern and Cathy A. Eastwood and Yong Zeng and Hude Quan},
  doi          = {10.1016/j.artmed.2025.103281},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103281},
  shortjournal = {Artif. Intell. Med.},
  title        = {Preprocessing narrative texts in electronic medical records to identify hospital adverse events: A scoping review},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCLResNet and DSAF: A self-supervised contrastive learning and deep self-attention fusion-based multimodal network for predicting central lymph node metastasis in papillary thyroid carcinoma. <em>ARTMED</em>, <em>170</em>, 103280. (<a href='https://doi.org/10.1016/j.artmed.2025.103280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of central lymph node metastasis (CLNM) in papillary thyroid carcinoma (PTC) is crucial to avoid unnecessary invasive procedures, yet existing models often fall short. We constructed the SCLResNet101 model based on a contrastive learning framework to extract network features of tumor ultrasound (US). SeResnet101 was used to extract network features of peri-vascular adipose tissue (PVAT) from the computed tomography (CT) of C6 (the arterial and venous layers beneath the thyroid). Univariate and multivariate analyses were performed using binary logistic regression to select clinical features. Finally, we constructed a Deep Self-Attention Fusion (DSAF) network to integrate features from these three modalities for CLNM prediction. Univariate and multivariate analyses revealed that Gender, Age, Size of US, and Extrathyroidal Extension (ETE) were independent risk factors for CLNM. In the internal test cohort (I-T), the area under the curve (AUC) of model was 0.863 (95 % CI: 0.779–0.932). In the external test cohort (E-T), the AUC was 0.839 (95 % CI: 0.755–0.905). Compared to all radiologists, the model significantly reduced both false-positive and false-negative rates in both the I-T and E-T. This study incorporates PVAT, which significantly enhances the performance of the multimodal deep learning model and may assist surgeons in making more informed and precise surgical decisions in the treatment of PTC.},
  archive      = {J_ARTMED},
  author       = {Shidi Miao and Yuyang Jiang and Wenjuan Huang and Yuxin Jiang and Mengzhuo Sun and Mingxuan Wang and Hongzhuo Qi and Ao Li and Zengyao Liu and Qiujun Wang and Ruitao Wang and Xuemei Ding},
  doi          = {10.1016/j.artmed.2025.103280},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103280},
  shortjournal = {Artif. Intell. Med.},
  title        = {SCLResNet and DSAF: A self-supervised contrastive learning and deep self-attention fusion-based multimodal network for predicting central lymph node metastasis in papillary thyroid carcinoma},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSFNet: A Temporal–Spectral fusion network for advanced speech emotion recognition in medical applications. <em>ARTMED</em>, <em>170</em>, 103279. (<a href='https://doi.org/10.1016/j.artmed.2025.103279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition (SER) is a critical component in enhancing communication systems and human–machine interaction, with significant potential for applications in the medical field. Although existing SER methods that combine temporal and spectral features have achieved notable advancements, they still encounter a big challenge in capturing emotional nuances, which are vital in medical diagnostics and patient care. In this study, we introduce a straightforward yet highly efficient network called TSFNet, which is the Temporal–Spectral Fusion Network via a Large-scale Pre-trained Model. This network is specifically designed to effectively process intricate emotional nuances by seamlessly integrating temporal and spectral information present in speech signals. By leveraging the capabilities of a large-scale pre-trained model, which serves as a powerful plug-and-play component for extracting and learning the temporal characteristics of speech, TSFNet enables a more accurate capture of complex emotional details crucial for medical applications. Extensive experiments are conducted on publicly available datasets, to evaluate the performance of TSFNet. Extensive experiments conducted on six public datasets demonstrate that TSFNet significantly outperforms existing baselines, achieving unweighted accuracies of 95.57% for Savee, 92.67% for Crema-D, 85.71% for IEMOCAP, 100.00% for Tess, 95.86% for Emovo, and 80.43% for Meld. It means that TSFNet has the potential in advancing medical diagnostic tools and patient monitoring systems.},
  archive      = {J_ARTMED},
  author       = {Xinran Li and Peilin Huang and Xiaojiang Peng and Feng Sha and Xiaomao Fan and Ye Li},
  doi          = {10.1016/j.artmed.2025.103279},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103279},
  shortjournal = {Artif. Intell. Med.},
  title        = {TSFNet: A Temporal–Spectral fusion network for advanced speech emotion recognition in medical applications},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture-attention siamese transformer for video polyp segmentation. <em>ARTMED</em>, <em>170</em>, 103278. (<a href='https://doi.org/10.1016/j.artmed.2025.103278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of polyps from colonoscopy videos is of great significance to polyp treatment and early prevention of colorectal cancer. However, it is challenging due to the difficulties associated with modeling long-range spatio-temporal relationships within a colonoscopy video. In this paper, we address this challenging task with a novel M ixture- A ttention S iamese T ransformer ( MAST ), which explicitly models the long-range spatio-temporal relationships with a mixture-attention mechanism for accurate polyp segmentation. Specifically, we first construct a Siamese transformer architecture to jointly encode paired video frames for their feature representations. We then design a mixture-attention module to exploit the intra-frame and inter-frame correlations, enhancing the features with rich spatio-temporal relationships. Finally, the enhanced features are fed to two parallel decoders for predicting the segmentation maps. Extensive experiments on the large-scale SUN-SEG benchmark demonstrate the superior performance of MAST in comparison with the cutting-edge competitors. Our code is publicly available at https://github.com/Junqing-Yang/MAST .},
  archive      = {J_ARTMED},
  author       = {Geng Chen and Junqing Yang and Xiaozhou Pu and Ge-Peng Ji and Huan Xiong and Yongsheng Pan and Hengfei Cui and Yong Xia},
  doi          = {10.1016/j.artmed.2025.103278},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103278},
  shortjournal = {Artif. Intell. Med.},
  title        = {Mixture-attention siamese transformer for video polyp segmentation},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BPINet: Synchronous blood pressure estimation and user authentication based on ECG and PPG signal with multi-task learning. <em>ARTMED</em>, <em>170</em>, 103277. (<a href='https://doi.org/10.1016/j.artmed.2025.103277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a vital indicator of health, blood pressure is particularly important for elderly individuals with chronic illnesses who live alone. Daily monitoring is essential to prevent hypertension and related complications. Nevertheless, most current home blood pressure monitors depend on cuff-based techniques, which can produce inaccurate results through improper use or cuff placement. Moreover, these devices generally cannot identify the specific user being measured, which hinders the development of personalized long-term health monitoring reports. In this paper, we propose BPINet, a multi-task model based on the Multi-gate Mixture-of-Experts (MMoE) framework that utilizes CNN-BiLSTM to extract features from ECG/PPG signals for simultaneous blood pressure estimation and user authentication (identity recognition). We also compile a dataset of ECG/PPG signals from multiple families, along with their blood pressure measurements, and incorporate it with the University of Queensland Vital Signs Dataset (UQVS) to evaluate the performance of BPINet. On the UQVS dataset, BPINet achieves a 97.54% user identity recognition accuracy. For systolic blood pressure (SBP) estimation, BPINet yields an MAE ± STD of 3.317 ± 5.771 mmHg. For diastolic blood pressure (DBP) estimation, the corresponding values are 2.444 ± 4.147 mmHg. On our customized dataset, BPINet achieves a 94.30% user identity recognition accuracy. For SBP estimation, it yields an MAE ± STD of 2.940 ± 4.753 mmHg. These results meet both the British Hypertension Society (BHS) Grade A standard and the Association for the Advancement of Medical Instrumentation (AAMI) standard. BPINet not only performs blood pressure estimation effectively but also enables simultaneous user identity recognition, facilitating the creation of personalized health records. The experimental results demonstrate the clinical feasibility and effectiveness of our proposed scheme.},
  archive      = {J_ARTMED},
  author       = {Xianliang Jiang and Dingxin Yu and Guang Jin and Fei Lei and Weihao Zhang and Xinyan Zhou},
  doi          = {10.1016/j.artmed.2025.103277},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103277},
  shortjournal = {Artif. Intell. Med.},
  title        = {BPINet: Synchronous blood pressure estimation and user authentication based on ECG and PPG signal with multi-task learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The interpretable deep learning framework and validation for seizure detection in pediatric electroencephalography: An improved accuracy and performance analysis. <em>ARTMED</em>, <em>170</em>, 103276. (<a href='https://doi.org/10.1016/j.artmed.2025.103276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an interpretable deep learning framework and compares the two novel models. A fully convolutional network with squeeze-and-excitation modules (SE-FCN) is designed to enhance spatial sensitivity and retain temporal resolution. In addition, a transformer-based model (TransNet) is developed to capture temporal and channel-wise dependencies via self-attention. These two models output channel saliency weights to the EEG electrode space and generate heatmaps for inferring potential epileptogenic zones. Deep learning primarily adopts convolutional neural networks (CNNs) or sequence generation networks (SGNs) and faces the limitations. For instance, CNN-based models often lack hierarchical modeling and fail to quantify channel-wise contributions, hindering spatial localization. SGN-based models struggle to capture complex spatiotemporal dependencies and typically lack adaptive attention tailored to electroencephalography (EEG) characters. Epileptic seizure detection is vital for effective clinical intervention and existing methods operated as black boxes, limiting clinical interpretability. This study evaluates the models on the CHB-MIT pediatric EEG dataset using a subject-independent cross-validation protocol. SE-FCN achieves an AUC of 0.89 and accuracy of 86.7 %, while TransNet achieves an AUC of 0.92 and accuracy of 86.4 %. Saliency maps from both models demonstrate high consistency and enable categorization of 22 patients into five groups based on inferred seizure origins.},
  archive      = {J_ARTMED},
  author       = {Yu Zhou and Yuxin Gao and Qiang Li and Ruiheng Wu and Aiping Yang and Ming-Lang Tseng},
  doi          = {10.1016/j.artmed.2025.103276},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103276},
  shortjournal = {Artif. Intell. Med.},
  title        = {The interpretable deep learning framework and validation for seizure detection in pediatric electroencephalography: An improved accuracy and performance analysis},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel memory interaction neural network for multi-label drug–drug interaction prediction with neighbor importance sampling. <em>ARTMED</em>, <em>170</em>, 103275. (<a href='https://doi.org/10.1016/j.artmed.2025.103275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-administration of multiple drugs can frequently cause drug–drug interactions (DDIs), including adverse drug reactions (ADRs) that may increase the likelihood of morbidity and mortality. Identifying potential DDIs presents a significant challenge, due to the complexity of pharmacology. Recent advances in knowledge graphs have contributed to DDI prediction by providing a robust framework for representing various relationships between drugs and other entities, such as proteins, diseases, and drug attributes. However, current network-based models often fail to uncover interaction information among DDI triplets, as they typically encode triplets independently. Additionally, uniform sampling methods may overlook differences in neighboring node properties. In this work, we propose a novel memory interaction neural network for DDI prediction, which integrates drug molecular sequences with semantic information from the drug knowledge graph. Specifically, we introduce a neighbor importance sampling strategy that selectively samples highly connected neighbors, improving computational efficiency and reducing noise. We also design a memory interaction module that utilizes multi-head attention mechanisms and deep neural networks to capture interactions among DDI triplets. Experimental evaluation on KEGG and OGB-biokg datasets demonstrates the superiority of our model compared to classical and state-of-the-art methods in predicting DDIs. Datasets and code for this proposed DDIs prediction model are freely accessible at https://github.com/wj1108114106/Multi-label-DDIs .},
  archive      = {J_ARTMED},
  author       = {Jing Wang and Runzhi Li and Shuo Zhang and YunLi Xing and Siyu Yan and Lihong Ma},
  doi          = {10.1016/j.artmed.2025.103275},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103275},
  shortjournal = {Artif. Intell. Med.},
  title        = {A novel memory interaction neural network for multi-label drug–drug interaction prediction with neighbor importance sampling},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Types, functions, and mechanisms of machine learning for personalizing smoking cessation interventions: A systematic scoping review. <em>ARTMED</em>, <em>170</em>, 103274. (<a href='https://doi.org/10.1016/j.artmed.2025.103274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Purpose Artificial intelligence can realize personalization. This systematic scoping review provides the types, functions, and mechanisms of machine learning (ML) for personalizing smoking cessation interventions. Methodology We searched fourteen databases including PubMed, CINAHL, EMBASE, the Cochrane Library, IEEE Xplore, PsycINFO, Scopus, Web of Science, AAAI, ACM Digital Library, ArXIV, Mednar, ProQuest, and Science.gov . We selected 98 articles from 4073 records that met the criteria. Two independent reviewers screened and selected the articles. Two reviewers extracted the data using a self-developed data charting form independently. Results The findings are reported in narrative syntheses, tables, and figures. The types of ML included artificial neural networks, Bayesian algorithms, clustering algorithms, decision tree algorithms, deep learning (DL) algorithms, ensemble algorithms, linear classifiers, others, and unspecified. The most common ML technique used was supervised learning (81 %), and the ML functions included (1) message tailoring (17 %), (2) prediction and detection of smoking events (34 %), (3) social media surveillance (14 %), (4) predictive models (24 %), and (5) biomarker analysis (10 %). The ML mechanisms involved the following sequence: data input, data preprocessing, feature extraction and selection, training and validation, and data output. Conclusion This review is the first to describe the potential use of ML for personalizing smoking cessation interventions. We provide recommendations for future research by identifying the limitations and gaps in the studies. Future studies should refine, validate, and test ML models using robust experimental methods to conclude their effectiveness.},
  archive      = {J_ARTMED},
  author       = {Yu Jie Xavia Ng and Shing Hui Reina Cheong and Wen Wei Ang and Ying Lau and Siew Tiang Lau},
  doi          = {10.1016/j.artmed.2025.103274},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103274},
  shortjournal = {Artif. Intell. Med.},
  title        = {Types, functions, and mechanisms of machine learning for personalizing smoking cessation interventions: A systematic scoping review},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding the cortical responses to mechanical wrist perturbations: A two-step shared structure NARX method. <em>ARTMED</em>, <em>170</em>, 103273. (<a href='https://doi.org/10.1016/j.artmed.2025.103273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared structure nonlinear autoregressive with exogenous input (NARX) model is a promising tool for exploring cortical responses mechanism to external stimuli, essential for advancing our understanding of brain function and developing methods for direct brain information encoding. In this paper, we proposed a two-step method to overcome limitations in existing method, which neglect data relationships and rely on a greedy search for regression terms, leading to less accurate models. In our approach, data from multiple trials are concatenated, and then the orthogonal forward regression (OFR) algorithm identifies model terms in first step, enhancing inter-trial connections and establishing a preliminary model for each subject. Shared model terms across subjects are then used to construct a general target model. Next, non-shared regression terms that best represent population-level information are identified, using adaptive multi-population genetic algorithms, and use to enhance the target models' descriptive power. Simulations results show significant competitiveness in terms of accuracy as compared to other state-of-the-art methods. When applied to real electroencephalography signals under mechanical disturbance, structural and parameter analysis revealed consistent neural response patterns across subjects, with subject-specific responses likely stemming from muscle feedback. Frequency response analysis further suggests that the brain may generate motor inhibition signals based on sensory inputs to maintain a pre-disturbance resting state. These findings provide valuable insights into cortical response mechanisms and have potential implications for future brain information encoding research.},
  archive      = {J_ARTMED},
  author       = {Nan Zheng and Yurong Li and Wuxiang Shi and Jiyu Tan},
  doi          = {10.1016/j.artmed.2025.103273},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103273},
  shortjournal = {Artif. Intell. Med.},
  title        = {Decoding the cortical responses to mechanical wrist perturbations: A two-step shared structure NARX method},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven dynamic grouping for adaptive clinical trials: Rethinking randomization in precision medicine. <em>ARTMED</em>, <em>170</em>, 103272. (<a href='https://doi.org/10.1016/j.artmed.2025.103272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating artificial intelligence into biomedical human subjects research is transforming traditional experimental paradigms. This perspective introduces the concept of “dynamic grouping,” wherein artificial intelligence (AI) systems continuously reassign participants across experimental conditions based on real-time biomarker data and clinical response patterns. Unlike traditional biomedical research designs that rely on fixed treatment and control groups, dynamic grouping allows participant assignments to evolve throughout the study. We examine the ethical implications, methodological challenges, and research opportunities associated with this paradigm, particularly in clinical trials, precision medicine, and digital therapeutics. To support this analysis, we present three computational simulations that quantify its impact: (i) a heterogeneity simulation demonstrating how patient variability affects the advantage of dynamic grouping, (ii) a statistical power analysis showing potential sample size reductions in adaptive designs, and (iii) a clinical outcome distribution analysis highlighting how dynamic grouping reduces negative treatment outcomes and optimizes patient responses. Our findings suggest that dynamic grouping can improve treatment effectiveness, enhance resource allocation, and increase statistical efficiency, although it also raises new challenges for causal inference, informed consent, and regulatory oversight. As AI continues to reshape medical research, adapting ethical and methodological frameworks will be essential for its responsible implementation.},
  archive      = {J_ARTMED},
  author       = {Madhur Mangalam},
  doi          = {10.1016/j.artmed.2025.103272},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103272},
  shortjournal = {Artif. Intell. Med.},
  title        = {AI-driven dynamic grouping for adaptive clinical trials: Rethinking randomization in precision medicine},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H-SynEx: Using synthetic images and ultra-high resolution ex vivo MRI for hypothalamus subregion segmentation. <em>ARTMED</em>, <em>170</em>, 103271. (<a href='https://doi.org/10.1016/j.artmed.2025.103271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hypothalamus is a small structure located in the center of the brain and is involved in significant functions such as sleeping, temperature, and appetite control. Various neurological disorders are also associated with hypothalamic abnormalities. Automated image analysis of this structure from brain MRI is thus highly desirable to study the hypothalamus in vivo . However, most of the automated segmentation tools currently available focus exclusively on T1w images. In this study, we introduce H-SynEx, a machine learning method for automated segmentation of hypothalamic subregions that generalizes across different MRI sequences and resolutions without retraining. H-synEx was trained with synthetic images built from label maps derived from ultra-high resolution ex vivo MRI scans, allowing finer-grained manual segmentation when compared with 1 mm isometric in vivo images. We validated our method using Dice Coefficient (DSC) and Average Hausdorff distance (AVD) across in vivo images from six different datasets with six different MRI sequences (T1, T2, proton density, quantitative T1, fractional anisotropy, and FLAIR). Statistical analysis compared hypothalamic subregion volumes in controls, Alzheimer’s disease (AD), and behavioral variant frontotemporal dementia (bvFTD) subjects using the Area Under the Receiver Operating Characteristic curve (AUROC) and the Wilcoxon rank sum test. Our results show that H-SynEx successfully leverages information from ultra-high resolution scans to segment in vivo from different MRI sequences. Our automated segmentation was able to discriminate controls versus patients with Alzheimer’s disease on FLAIR images with 5 mm spacing. H-SynEx is openly available at https://github.com/liviamarodrigues/hsynex .},
  archive      = {J_ARTMED},
  author       = {Livia Rodrigues and Martina Bocchetta and Oula Puonti and Douglas Greve and Ana Carolina Londe and Marcondes França and Simone Appenzeller and Juan Eugenio Iglesias and Leticia Rittner},
  doi          = {10.1016/j.artmed.2025.103271},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103271},
  shortjournal = {Artif. Intell. Med.},
  title        = {H-SynEx: Using synthetic images and ultra-high resolution ex vivo MRI for hypothalamus subregion segmentation},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining multi-electrode and multi-wave electroencephalogram based time-interval temporal patterns for improved classification capabilities and explainability. <em>ARTMED</em>, <em>170</em>, 103269. (<a href='https://doi.org/10.1016/j.artmed.2025.103269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-computer interface (BCI) systems, and particularly electroencephalogram (EEG) based BCI systems, have become more widely used in recent years and are utilized in various applications and domains ranging from medicine and marketing to games and entertainment. While different algorithms have been used to analyze EEG data and enable its classification, existing algorithms have two main drawbacks; both their classification and explainability capabilities are limited. Lacking in explainability, they cannot indicate which electrodes and waves led to a classification decision or explain how areas and frequencies of the brain's activity correlate to a specific task. In this study, we propose a novel extension for the time-interval temporal patterns mining algorithms aimed at enhancing the data mining process by enabling a richer set of patterns to be learned from the EEG data, thereby contributing to improved classification and explainability capabilities. The extended algorithm is designed to capture and leverage the unique nature of EEG data by decomposing it into different brain waves and modeling the relations among them and between different electrodes. Our evaluation of the proposed extended algorithm on multiple learning tasks and three EEG datasets demonstrated the extended algorithm's ability to mine richer patterns that improve the classification performance by 4–11 % based on the Area-Under the receiver operating characteristic Curve (AUC) metric, compared to the original version of the algorithm. Moreover, the algorithm was shown to shed light on the areas and frequencies of the brain's activity that are correlated with specific tasks.},
  archive      = {J_ARTMED},
  author       = {Ofir Landau and Nir Nissim},
  doi          = {10.1016/j.artmed.2025.103269},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103269},
  shortjournal = {Artif. Intell. Med.},
  title        = {Mining multi-electrode and multi-wave electroencephalogram based time-interval temporal patterns for improved classification capabilities and explainability},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey for large language models in biomedicine. <em>ARTMED</em>, <em>170</em>, 103268. (<a href='https://doi.org/10.1016/j.artmed.2025.103268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent breakthroughs in large language models (LLMs) offer unprecedented natural language understanding and generation capabilities. However, existing surveys on LLMs in biomedicine often focus on specific applications or model architectures, lacking a comprehensive analysis that integrates the latest advancements across various biomedical domains. This review, based on an analysis of 484 publications sourced from databases including PubMed, Web of Science, and arXiv, provides an in-depth examination of the current landscape, applications, challenges, and prospects of LLMs in biomedicine, distinguishing itself by focusing on the practical implications of these models in real-world biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot learning across a broad spectrum of biomedical tasks, including diagnostic assistance, drug discovery, and personalized medicine, among others, with insights drawn from 137 key studies. Then, we discuss adaptation strategies of LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to enhance their performance in specialized biomedical contexts where zero-shot fails to achieve, such as medical question answering and efficient processing of biomedical literature. Finally, we discuss the challenges that LLMs face in the biomedicine domain including data privacy concerns, limited model interpretability, issues with dataset quality, and ethics due to the sensitive nature of biomedical data, the need for highly reliable model outputs, and the ethical implications of deploying AI in healthcare. To address these challenges, we also identify future research directions of LLM in biomedicine including federated learning methods to preserve data privacy and integrating explainable AI methodologies to enhance the transparency of LLMs. As this field of LLM rapidly evolves, continued research and development are essential to fully harness the capabilities of LLMs in biomedicine while ensuring their responsible and effective deployment.},
  archive      = {J_ARTMED},
  author       = {Chong Wang and Mengyao Li and Junjun He and Zhongruo Wang and Erfan Darzi and Zan Chen and Jin Ye and Tianbin Li and Yanzhou Su and Jing Ke and Kaili Qu and Shuxin Li and Yi Yu and Pietro Liò and Tianyun Wang and Yu Guang Wang and Yiqing Shen},
  doi          = {10.1016/j.artmed.2025.103268},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103268},
  shortjournal = {Artif. Intell. Med.},
  title        = {A survey for large language models in biomedicine},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-quality triage and diagnosis of gynecological diseases via artificial intelligence. <em>ARTMED</em>, <em>170</em>, 103267. (<a href='https://doi.org/10.1016/j.artmed.2025.103267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely detection and diagnosis of diseases are key elements of an efficient healthcare system. In recent years, artificial intelligence (AI) has played an increasingly important role in improving the accuracy and efficiency of disease diagnosis in clinical practice. However, most existing AI systems for disease diagnosis have focused on either classifying patients into broad disease categories or diagnosing a specific disease, leaving a gap in the development of a coherent AI system for both triage and diagnosis in a department of a general hospital. In this study, we fill this gap with SmartGyne, an advanced AI system that can achieve high-quality triage and diagnosis for a full spectrum of gynecological diseases. By extracting useful clinical evidence for diagnosis from a large amount of electronic medical records, SmartGyne establishes an effective framework to integrate real-world clinical evidence and knowledge into a coherent AI system that can effectively handle a full spectrum of complex diseases in a department of a general hospital. Validation experiments demonstrated that SmartGyne achieved an overall accuracy of 80.1 % in triage for gynecological diseases, and 99.4 % in diagnosis for a gynecological subspecialty. In comparison with human physicians, SmartGyne showed competitive triage and diagnostic performance, and improved consultation efficiency and accuracy for physicians with limited specialized experience. These results show that SmartGyne achieves high-quality triage and diagnosis, holding the potential to improve the efficiency of the healthcare system in China, as well as other countries lacking professional gynecologists.},
  archive      = {J_ARTMED},
  author       = {Linru Fu and Che Wang and Zhaoyang Liu and Changzai Pan and Zhe Du and Zhijing Sun and Lan Zhu and Ke Deng},
  doi          = {10.1016/j.artmed.2025.103267},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103267},
  shortjournal = {Artif. Intell. Med.},
  title        = {High-quality triage and diagnosis of gynecological diseases via artificial intelligence},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving prototypical parts abstraction for case-based reasoning explanations designed for the kidney stone type recognition. <em>ARTMED</em>, <em>170</em>, 103266. (<a href='https://doi.org/10.1016/j.artmed.2025.103266'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The in-vivo identification of the kidney stone types during an ureteroscopy would be a major medical advance in urology, as it could reduce the time of the tedious renal calculi extraction process, while diminishing infection risks. Furthermore, such an automated procedure would make possible to prescribe anti-recurrence treatments immediately. Nowadays, only few experienced urologists are able to recognize the kidney stone types in the images of the videos displayed on a screen during the endoscopy. This visual recognition by urologists is also highly operator dependent. Thus, several deep learning (DL) models have recently been proposed to automatically recognize the kidney stone types using ureteroscopic images. However, these DL models are of black box nature and do not establish the relationship of the visual features they used to take the decision with the color, texture and morphological features visually analyzed in biological laboratories to determine the type of extracted kidney stone fragments using the reference morphoconstitutional analysis (MCA) procedure. This contribution proposes a case-based reasoning DLmodel which uses prototypical parts (PPs) and generates local and global descriptors. The PPs encode for each class (i.e., kidney stone type) visual feature information (hue, saturation, intensity and textures) similar to that used by biologists during MCA. The PPs are optimally generated due a new loss function used during the model training. Moreover, the local and global descriptors of PPs allow to explain the decisions (“what” information, “where in the images”) in an understandable way for biologists and urologists. The proposed DL model has been tested on a database including images of the six most widespread kidney stone types in industrialized countries. The overall average classification accuracy was 90 . 37 ± 0 . 6 % . When comparing this results with that of the eight other DL models of the kidney stone state-of-the-art, it can be seen that the valuable gain in explanability was not reached at the expense of accuracy which was even slightly increased with respect to that ( 88 . 2 ± 2 . 1 % ) of the best method of the literature. These promising and interpretable results also encourage urologists to put their trust in AI-based solutions.},
  archive      = {J_ARTMED},
  author       = {Daniel Flores-Araiza and Francisco Lopez-Tiro and Clément Larose and Salvador Hinojosa and Andres Mendez-Vazquez and Miguel Gonzalez-Mendoza and Gilberto Ochoa-Ruiz and Christian Daul},
  doi          = {10.1016/j.artmed.2025.103266},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103266},
  shortjournal = {Artif. Intell. Med.},
  title        = {Improving prototypical parts abstraction for case-based reasoning explanations designed for the kidney stone type recognition},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical multimodal foundation models in clinical diagnosis and treatment: Applications, challenges, and future directions. <em>ARTMED</em>, <em>170</em>, 103265. (<a href='https://doi.org/10.1016/j.artmed.2025.103265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning have significantly revolutionized the field of clinical diagnosis and treatment, offering novel approaches to improve diagnostic precision and treatment efficacy across diverse clinical domains, thus driving the pursuit of precision medicine. The growing availability of multi-organ and multimodal datasets has accelerated the development of large-scale Medical Multimodal Foundation Models (MMFMs). These models, known for their strong generalization capabilities and rich representational power, are increasingly being adapted to address a wide range of clinical tasks, from early diagnosis to personalized treatment strategies. This review offers a comprehensive analysis of recent developments in MMFMs, focusing on three key aspects: datasets, model architectures, and clinical applications. We also explore the challenges and opportunities in optimizing multimodal representations and discuss how these advancements are shaping the future of healthcare by enabling improved patient outcomes and more efficient clinical workflows.},
  archive      = {J_ARTMED},
  author       = {Kai Sun and Siyan Xue and Fuchun Sun and Haoran Sun and Yu Luo and Ling Wang and Siyuan Wang and Na Guo and Lei Liu and Tian Zhao and Xinzhou Wang and Lei Yang and Shuo Jin and Jun Yan and Jiahong Dong},
  doi          = {10.1016/j.artmed.2025.103265},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103265},
  shortjournal = {Artif. Intell. Med.},
  title        = {Medical multimodal foundation models in clinical diagnosis and treatment: Applications, challenges, and future directions},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end solution for out-of-hospital emergency medical dispatch triage based on multimodal and continual deep learning. <em>ARTMED</em>, <em>170</em>, 103264. (<a href='https://doi.org/10.1016/j.artmed.2025.103264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this study was to build a multimodal, multitask predictive model—named E2eDeepEMC 2 —to improve out-of-hospital emergency incident severity assessments while coping with shifts in data distributions over time. We drew on 2 054 694 independent incidents recorded by the Valencian emergency medical dispatch service between 2009 and 2019 (excluding 2013), combining demographic, temporal, clinical and free-text inputs. To handle temporal drift, our model integrates continual learning strategies and comprises three encoder modules (for context, clinical data and text), whose outputs are merged to predict the life-threatening level, admissible response delay and emergency system jurisdiction. Compared with the Valencian Region’s existing in-house triage protocol, E2eDeepEMC 2 achieved absolute F1-score gains of 18.46% for life-threatening level, 25.96% for response delay and 3.63% for jurisdiction. Compared to non-continual learning baselines, it also outperformed them by 3.04%, 9.66% and 0.58%, respectively. Deployment of E2eDeepEMC 2 is currently underway in the Valencian Region, underscoring its practical impact on real-world emergency dispatch decision-making.},
  archive      = {J_ARTMED},
  author       = {Pablo Ferri and Carlos Sáez and Antonio Félix-De Castro and Purificación Sánchez-Cuesta and Juan M. García-Gómez},
  doi          = {10.1016/j.artmed.2025.103264},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103264},
  shortjournal = {Artif. Intell. Med.},
  title        = {An end-to-end solution for out-of-hospital emergency medical dispatch triage based on multimodal and continual deep learning},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoRA-PT: Low-rank adapting UNETR for hippocampus segmentation using principal tensor singular values and vectors. <em>ARTMED</em>, <em>170</em>, 103254. (<a href='https://doi.org/10.1016/j.artmed.2025.103254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hippocampus is an important brain structure involved in various psychiatric disorders, and its automatic and accurate segmentation is vital for studying these diseases. Recently, deep learning-based methods have made significant progress in hippocampus segmentation. However, training deep neural network models requires substantial computational resources, time, and a large amount of labeled training data, which is frequently scarce in medical image segmentation. To address these issues, we propose LoRA-PT, a novel parameter-efficient fine-tuning (PEFT) method that transfers the pre-trained UNETR model from the BraTS2021 dataset to the hippocampus segmentation task. Specifically, LoRA-PT divides the parameter matrix of the transformer structure into three distinct sizes, yielding three third-order tensors. These tensors are decomposed using tensor singular value decomposition to generate low-rank tensors consisting of the principal singular values and vectors, with the remaining singular values and vectors forming the residual tensor. During fine-tuning, only the low-rank tensors (i.e., the principal tensor singular values and vectors) are updated, while the residual tensors remain unchanged. We validated the proposed method on three public hippocampus datasets, and the experimental results show that LoRA-PT outperformed state-of-the-art PEFT methods in segmentation accuracy while significantly reducing the number of parameter updates. Our source code is available at https://github.com/WangangCheng/LoRA-PT/tree/LoRA-PT .},
  archive      = {J_ARTMED},
  author       = {Guanghua He and Wangang Cheng and Hancan Zhu and Gaohang Yu},
  doi          = {10.1016/j.artmed.2025.103254},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103254},
  shortjournal = {Artif. Intell. Med.},
  title        = {LoRA-PT: Low-rank adapting UNETR for hippocampus segmentation using principal tensor singular values and vectors},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMSupcon: An image fusion-based multi-modal supervised contrastive method for brain tumor diagnosis. <em>ARTMED</em>, <em>170</em>, 103253. (<a href='https://doi.org/10.1016/j.artmed.2025.103253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis of brain tumors is pivotal for effective treatment, with MRI serving as a commonly used non-invasive diagnostic modality in clinical practices. Fundamentally, brain tumor diagnosis is a type of pattern recognition task that requires the integration of information from multi-modal MRI images. However, existing fusion strategies are hindered by the scarcity of multi-modal imaging samples. In this paper, we propose a new training paradigm tailored for the scenario of multi-modal imaging in brain tumor diagnosis, called multi-modal supervised contrastive learning method (MMSupcon). This method significantly enhances diagnostic accuracy through two key components: multi-modal medical image fusion and multi-modal supervised contrastive loss. First, the fusion component integrates complementary imaging modalities to generate information-rich samples. Second, by introducing fused samples to guide original samples in learning feature consistency or inconsistency among classes, our loss component effectively preserves the integrity of cross-modal information while maintaining the distinctiveness of individual modalities. Finally, MMSupcon is validated on a real-world brain tumor dataset collected from Beijing Tiantan Hospital, achieving state-of-the-art performance. Furthermore, additional experiments on two public BraTS glioma classification datasets also demonstrate our substantial performance improvements. The source code is released at https://github.com/hywang02/MMSupcon .},
  archive      = {J_ARTMED},
  author       = {Haoyu Wang and Jing Zhang and Siying Wu and Haoran Wei and Xun Chen and Yunwei Ou and Xiaoyan Sun},
  doi          = {10.1016/j.artmed.2025.103253},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103253},
  shortjournal = {Artif. Intell. Med.},
  title        = {MMSupcon: An image fusion-based multi-modal supervised contrastive method for brain tumor diagnosis},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathway information on methylation analysis using deep neural network (PROMINENT): An interpretable deep learning method with pathway prior for phenotype prediction using gene-level DNA methylation. <em>ARTMED</em>, <em>170</em>, 103236. (<a href='https://doi.org/10.1016/j.artmed.2025.103236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background DNA methylation is a key epigenetic marker that influences gene expression and phenotype regulation, and is affected by both genetic and environmental factors. Traditional linear regression methods such as elastic nets have been employed to assess the cumulative effects of multiple DNA methylation markers on phenotypes. However, these methods often fail to capture the complex nonlinear nature of the data. Recent deep learning approaches, such as MethylNet, have improved the prediction accuracy but lack interpretability and efficiency. Findings To address these limitations, we introduced P athway Info r mati o n on M ethylat i on Analysis using a Deep Ne ural N e t work (PROMINENT), a novel interpretable deep learning method that integrates gene-level DNA methylation data with biological pathway information for phenotype prediction. PROMINENT enhances interpretability and prediction accuracy by incorporating gene- and pathway-level priors from databases such as Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG). It employs SHapley Additive exPlanations (SHAP) to prioritize significant genes and pathways. Evaluated across various datasets, childhood asthma, idiopathic pulmonary fibrosis (IPF), and first-episode psychosis (FEP)—PROMINENT consistently outperformed existing methods in terms of prediction accuracy and computational efficiency. PROMINENT also identified crucial genes and pathways involved in disease mechanisms. Conclusions PROMINENT represents a significant advancement in leveraging DNA methylation data for phenotype prediction, offering both high accuracy and interpretability within reasonable computational time. This method holds promise for elucidating the epigenetic underpinnings of complex diseases and enhancing the utility of DNA methylation data in biomedical research.},
  archive      = {J_ARTMED},
  author       = {Soyeon Kim and Laizhi Zhang and Yidi Qin and Rebecca I. Caldino Bohn and Hyun Jung Park},
  doi          = {10.1016/j.artmed.2025.103236},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103236},
  shortjournal = {Artif. Intell. Med.},
  title        = {Pathway information on methylation analysis using deep neural network (PROMINENT): An interpretable deep learning method with pathway prior for phenotype prediction using gene-level DNA methylation},
  volume       = {170},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
