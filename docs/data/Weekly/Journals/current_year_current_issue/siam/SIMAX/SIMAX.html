<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMAX</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simax">SIMAX - 5</h2>
<ul>
<li><details>
<summary>
(2025). Splitting alternating algorithms for sparse solutions of linear systems with concatenated orthogonal matrices. <em>SIMAX</em>, <em>46</em>(4), 2310-2330. (<a href='https://doi.org/10.1137/24M1707341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A class of splitting alternating algorithms is proposed for finding the sparse solution of linear systems with concatenated orthogonal matrices. Depending on the number of matrices concatenated, the proposed algorithms are classified into the two-block splitting alternating algorithm (TSAA) and the multiblock splitting alternating algorithm (MSAA). These algorithms aim to decompose a large-scale linear system into two or more coupled subsystems, each significantly smaller than the original system, and then combine the solutions of these subsystems to produce the sparse solution of the original system. The proposed algorithms only involve matrix-vector products and reduced orthogonal projections. It turns out that the proposed algorithms are globally convergent to the sparse solution of a linear system if the matrix (along with the sparsity level of the solution) satisfies a coherence-type condition. Numerical experiments indicate that the proposed algorithms are very promising and can quickly and accurately locate the sparse solution of a linear system with significantly fewer iterations than several mainstream iterative methods. Reproducibility of computational results. This paper has been awarded the “SIAM Reproducibility Badge: Code and data available” as a recognition that the authors have followed reproducibility principles valued by SIMAX and the scientific computing community. Code and data that allow readers to reproduce the results in this paper are available at https://zhongfengsun.github.io/.},
  archive      = {J_SIMAX},
  author       = {Yun-Bin Zhao and Zhong-Feng Sun},
  doi          = {10.1137/24M1707341},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2310-2330},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Splitting alternating algorithms for sparse solutions of linear systems with concatenated orthogonal matrices},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Butterfly factorization with error guarantees. <em>SIMAX</em>, <em>46</em>(4), 2253-2309. (<a href='https://doi.org/10.1137/24M1708796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we investigate the butterfly factorization problem, i.e., the problem of approximating a matrix by a product of sparse and structured factors. We propose a new formal mathematical description of such factors, that encompasses many different variations of butterfly factorization with different choices of the prescribed sparsity patterns. Among these choices we identify those that ensure that the factorization problem admits an optimum, thanks to a new property called “chainability”. For those supports we propose a new butterfly algorithm that yields an approximate solution to the butterfly factorization problem and that is supported by stronger theoretical guarantees than existing factorization methods. Specifically, we show that the ratio of the approximation error by the minimum value is bounded by a constant, independent of the target matrix. Reproducibility of computational results. This paper has been awarded the “SIAM Reproducibility Badge: Code and data available” as a recognition that the authors have followed reproducibility principles valued by SIMAX and the scientific computing community. Code and data that allow readers to reproduce the results in this paper are available at https://inria.hal.science/hal-04720713v1.},
  archive      = {J_SIMAX},
  author       = {Quoc-Tung Le and Léon Zheng and Elisa Riccietti and Rémi Gribonval},
  doi          = {10.1137/24M1708796},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2253-2309},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Butterfly factorization with error guarantees},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The generalized matrix norm problem. <em>SIMAX</em>, <em>46</em>(4), 2226-2252. (<a href='https://doi.org/10.1137/23M1605545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the computability of the operator norm of a matrix with respect to norms induced by linear operators. Our findings reveal that this problem can be solved in polynomial time in certain situations, and we discuss how it can be approximated in other cases. Along the way, we investigate the concept of push-forward and pull-back of seminorms, which leads us to uncover novel duality principles that come into play when optimizing over the unit ball of norms. Reproducibility of computational results. This paper has been awarded the “SIAM Reproducibility Badge: Code and data available” as a recognition that the authors have followed reproducibility principles valued by SIMAX and the scientific computing community. Code and data that allow readers to reproduce the results in this paper are available at https://codeocean.com/capsule/8982170/tree and in the supplementary materials (RBcode.zip [43.0MB]).},
  archive      = {J_SIMAX},
  author       = {Adrian Kulmburg},
  doi          = {10.1137/23M1605545},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2226-2252},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The generalized matrix norm problem},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A subspace-conjugate gradient method for linear matrix equations. <em>SIMAX</em>, <em>46</em>(4), 2197-2225. (<a href='https://doi.org/10.1137/25M1723402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The efficient solution of large-scale multiterm linear matrix equations is a challenging task in numerical linear algebra, and it is a largely open problem. We propose a new iterative scheme for symmetric and positive definite operators, significantly advancing methods such as truncated matrix-oriented conjugate gradients (cg). The new algorithm capitalizes on the low-rank matrix format of its iterates by fully exploiting the subspace information of the factors as iterations proceed. The approach implicitly relies on orthogonality conditions imposed over much larger subspaces than in cg, unveiling insightful connections with subspace projection methods. The new method is also equipped with memory-saving strategies. In particular, we show that for a given matrix , the action in low-rank format may not be evaluated exactly due to memory constraints. This problem is often underestimated, though it will eventually produce out-of-memory breakdowns for a sufficiently large number of terms. We propose an ad hoc randomized range-finding strategy that appears to fully resolve this shortcoming. Experimental results with typical application problems illustrate the potential of our approach over various methods developed in the recent literature. Reproducibility of computational results. This paper has been awarded the “SIAM Reproducibility Badge: Code and data available” as a recognition that the authors have followed reproducibility principles valued by SIMAX and the scientific computing community. Code and data that allow readers to reproduce the results in this paper are available at https://github.com/palittaUniBO.},
  archive      = {J_SIMAX},
  author       = {Davide Palitta and Martina Iannacito and Valeria Simoncini},
  doi          = {10.1137/25M1723402},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2197-2225},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A subspace-conjugate gradient method for linear matrix equations},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error norm estimates for the block conjugate gradient algorithms. <em>SIMAX</em>, <em>46</em>(4), 2175-2196. (<a href='https://doi.org/10.1137/25M1735408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In the book [G. Meurant and P. Tichý, Error Norm Estimation in the Conjugate Gradient Algorithm, SIAM, 2024], we discussed the estimation of error norms in the conjugate gradient (CG) algorithm for solving linear systems with a symmetric positive definite matrix , where and are vectors. In this paper, we generalize the most important formulas for estimating the -norm of the error to the block case. First, we discuss in detail the derivation of various variants of the block CG (BCG) algorithm from the block Lanczos algorithm. We then consider BCG and derive the related block Gauss and block Gauss–Radau quadrature rules. We show how to obtain lower and upper bounds on the -norm of the error of each system, both in terms of the quantities computed in BCG and in terms of the underlying block Lanczos algorithm. Numerical experiments demonstrate the behavior of the bounds in practical computations.},
  archive      = {J_SIMAX},
  author       = {Gérard Meurant and Petr Tichý},
  doi          = {10.1137/25M1735408},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {2175-2196},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Error norm estimates for the block conjugate gradient algorithms},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
