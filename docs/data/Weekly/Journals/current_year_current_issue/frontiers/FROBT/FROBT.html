<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FROBT</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="frobt">FROBT - 12</h2>
<ul>
<li><details>
<summary>
(2025). Reinventing the wheel: A simulation-aided design of a soft, shape-adapting, lugged wheel for locomotion on sandy terrains. <em>FROBT</em>, <em>12</em>, 1686519. (<a href='https://doi.org/10.3389/frobt.2025.1686519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locomotion over granular terrain poses significant challenges for autonomous robotic systems, particularly in coastal regions characterized by loose, shifting sands. To optimize the locomotion on these challenging terrains, a simulation-aided design approach was used to develop a soft, shape-adapting, wheeled locomotion system. A co-simulation framework combining the discrete element method (DEM) and multibody dynamics (MBD) is employed to simulate the locomotion of a wheeled robot on varying sandy soils, covering both dry and wet sandy soil conditions. A shape-adapting wheel design is proposed, incorporating soft, inflatable elements that enable the wheel to transform between lugged and circular configurations. A discretized flexbody approach is adopted to model the interactions between the sandy soil and the soft, flexible bodies of the shape-adapting wheel design. Simulation results demonstrate improved performance of the shape-adapting wheels across a variety of sandy terrains, including slopes and obstacles. Integrating softness into the wheel improves obstacle climbing performance, while a lugged wheel configuration performs particularly well on loose, dry sandy slopes. This DEM-MBD co-simulation further enables efficient evaluation of locomotion strategies without the need for extensive physical prototyping.},
  archive      = {J_FROBT},
  author       = {Shi, H. and Klaassen, P. and Schott, D. L. and Jovanova, J.},
  doi          = {10.3389/frobt.2025.1686519},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1686519},
  shortjournal = {Front. Robot. AI},
  title        = {Reinventing the wheel: A simulation-aided design of a soft, shape-adapting, lugged wheel for locomotion on sandy terrains},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Agency-preserving robotic assistance for grasp slip recovery in body-powered prostheses. <em>FROBT</em>, <em>12</em>, 1675955. (<a href='https://doi.org/10.3389/frobt.2025.1675955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies demonstrate that performance in reaction-based tasks can be improved using external robotic assistance without reducing the user’s sense of agency, particularly when assistance is delivered near the user’s natural reaction time. This finding has promise for assistive technologies like upper limb prostheses, where agency contributes to long-term use and users’ natural slip reflexes are hindered by reduced feedback and proprioception. However, prior studies lack the physical feedback of device movement inherent to many assistive devices like body-powered prostheses or exoskeletons where user and device are physically coupled. In this work, we explore the relationship between robotic assistance, performance, and agency when such feedback is present. We study how the timing of robotic assistance alters performance and agency, as experienced through the feedback of a body-powered transmission. We collect data from twenty participants in a simulated slip reaction task using a custom body-powered prosthesis emulator, with robotic grasp assistance provided at various delays relative to the onset of slip. Results show that, as assistance becomes more aligned with reaction times, agency increases while performance benefits are still obtained, even if users are aware of the assistance and perceive an increase in performance. Our findings suggest that in scenarios where users can physically perceive robotic assistance and its benefits, such as in body-grounded assistive technologies like body-powered prostheses or exoskeletons, temporal alignment between the user and robotic assistance plays a role in both performance and user experience.},
  archive      = {J_FROBT},
  author       = {Davis, Benjamin and Abbott, Michael and Stuart, Hannah S.},
  doi          = {10.3389/frobt.2025.1675955},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1675955},
  shortjournal = {Front. Robot. AI},
  title        = {Agency-preserving robotic assistance for grasp slip recovery in body-powered prostheses},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multimodal collaborative storytelling with pepper: A preliminary study with zero-shot LLMs. <em>FROBT</em>, <em>12</em>, 1662819. (<a href='https://doi.org/10.3389/frobt.2025.1662819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of large language models (LLMs), collaborative storytelling in virtual agents or chatbots has gained popularity. Despite storytelling has long been employed in social robotics as a means to educate, entertain, and persuade audiences, the integration of LLMs into such platforms remains largely unexplored. This paper presents the initial steps for a novel multimodal collaborative storytelling system in which users co-create stories with the social robot Pepper through natural language interaction and by presenting physical objects. The robot employs a YOLO-based vision system to recognize these objects and seamlessly incorporate them into the narrative. Story generation and adaptation are handled autonomously using the Llama model in a zero-shot setting, aiming to assess the usability and maturity of such models in interactive storytelling. To enhance immersion, the robot performs the final story using expressive gestures, emotional cues, and speech modulation. User feedback, collected through questionnaires and semi-structured interviews, indicates a high level of acceptance.},
  archive      = {J_FROBT},
  author       = {Zabala, Unai and Echevarria, Juan and Rodriguez, Igor and Lazkano, Elena},
  doi          = {10.3389/frobt.2025.1662819},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1662819},
  shortjournal = {Front. Robot. AI},
  title        = {Exploring multimodal collaborative storytelling with pepper: A preliminary study with zero-shot LLMs},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trustworthy navigation with variational policy in deep reinforcement learning. <em>FROBT</em>, <em>12</em>, 1652050. (<a href='https://doi.org/10.3389/frobt.2025.1652050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionDeveloping a reliable and trustworthy navigation policy in deep reinforcement learning (DRL) for mobile robots is extremely challenging, particularly in real-world, highly dynamic environments. Particularly, exploring and navigating unknown environments without prior knowledge, while avoiding obstacles and collisions, is very cumbersome for mobile robots. MethodsThis study introduces a novel trustworthy navigation framework that utilizes variational policy learning to quantify uncertainty in the estimation of the robot’s action, localization, and map representation. Trust-Nav employs the Bayesian variational approximation of the posterior distribution over the policy-based neural network’s parameters. Policy-based and value-based learning are combined to guide the robot’s actions in unknown environments. We derive the propagation of variational moments through all layers of the policy network and employ a first-order approximation for the nonlinear activation functions. The uncertainty in robot action is measured by the propagated variational covariance in the DRL policy network. At the same time, the uncertainty in the robot’s localization and mapping is embedded in the reward function and stems from the traditional Theory of Optimal Experimental Design. The total loss function optimizes the parameters of the policy and value networks to maximize the robot’s cumulative reward in an unknown environment.ResultsExperiments conducted using the Gazebo robotics simulator demonstrate the superior performance of the proposed Trust-Nav model in achieving robust autonomous navigation and mapping.DiscussionTrust-Nav consistently outperforms deterministic DRL approaches, particularly in complicated environments involving noisy conditions and adversarial attacks. This integration of uncertainty into the policy network promotes safer and more reliable navigation, especially in complex or unpredictable environments. Trust-Nav offers a step toward deployable, self-aware robotic systems capable of recognizing and responding to their own limitations.},
  archive      = {J_FROBT},
  author       = {Bockrath, Karla and Ernst, Liam and Nadeem, Rohaan and Pedraza, Bryan and Dera, Dimah},
  doi          = {10.3389/frobt.2025.1652050},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1652050},
  shortjournal = {Front. Robot. AI},
  title        = {Trustworthy navigation with variational policy in deep reinforcement learning},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing socially assistive robots for clinical practice: Insights from an asynchronous remote community of speech-language pathologists. <em>FROBT</em>, <em>12</em>, 1646880. (<a href='https://doi.org/10.3389/frobt.2025.1646880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionSocially Assistive Robots (SARs) hold promise for augmenting speech-language therapy by addressing high caseloads and enhancing child engagement. However, many implementations remain misaligned with clinician practices and overlook expressive strategies central to speech-language pathology.MethodsWe conducted a 4-week Asynchronous Remote Community (ARC) study with thirteen licensed speech-language pathologists (SLPs). Participants engaged in weekly activities and asynchronous discussions, contributing reflective insights on emotional expression, domain-specific needs, and potential roles for SARs. The ARC format supported distributed, flexible engagement and facilitated iterative co-design through longitudinal peer dialogue. Data were analyzed using thematic analysis to identify emerging patterns.ResultsAnalysis revealed five clinician-driven design considerations for SARs: (1) the need for expressive and multi-modal communication; (2) customization of behaviors to accommodate sensory and developmental profiles; (3) adaptability of roles across therapy contexts; (4) ethical concerns surrounding overuse and fears of clinician replacement; and (5) opportunities for data tracking and personalization.DiscussionFindings highlight clinician-informed design implications that can guide the development of socially intelligent, adaptable, and ethically grounded SARs. The ARC approach proved a viable co-design framework, enabling deeper reflection and peer-driven requirements than traditional short-term methods. This work bridges the gap between robotic capabilities and clinical expectations, underscoring the importance of embedding clinician expertise in SAR design to foster meaningful integration into speech-language interventions.},
  archive      = {J_FROBT},
  author       = {Oliva, Denielle and Olszewski, Abbie and Sadeghi, Shekoufeh and Dantu, Karthik and Feil-Seifer, David},
  doi          = {10.3389/frobt.2025.1646880},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1646880},
  shortjournal = {Front. Robot. AI},
  title        = {Designing socially assistive robots for clinical practice: Insights from an asynchronous remote community of speech-language pathologists},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized causal explanations of a robot’s behavior. <em>FROBT</em>, <em>12</em>, 1637574. (<a href='https://doi.org/10.3389/frobt.2025.1637574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of robots in environments shared with humans implies that they must be able to justify or explain their behavior to nonexpert users when the user, or the situation itself, requires it. We propose a framework for robots to generate personalized explanations of their behavior by integrating cause-and-effect structures, social roles, and natural language queries. Robot events are stored as cause–effect pairs in a causal log. Given a human natural language query, the system uses machine learning to identify the matching cause-and-effect entry in the causal log and determine the social role of the inquirer. An initial explanation is generated and is then further refined by a large language model (LLM) to produce linguistically diverse responses tailored to the social role and the query. This approach maintains causal and factual accuracy while providing language variation in the generated explanations. Qualitative and quantitative experiments show that combining the causal information with the social role and the query when generating the explanations yields the most appreciated explanations.},
  archive      = {J_FROBT},
  author       = {Galeas, José and Bensch, Suna and Hellström, Thomas and Bandera, Antonio},
  doi          = {10.3389/frobt.2025.1637574},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1637574},
  shortjournal = {Front. Robot. AI},
  title        = {Personalized causal explanations of a robot’s behavior},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New avenues for understanding what deep networks learn from EEG. <em>FROBT</em>, <em>12</em>, 1625732. (<a href='https://doi.org/10.3389/frobt.2025.1625732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important but unresolved question in deep learning for EEG decoding is which features neural networks learn to solve the task. Prior interpretability studies have mainly explained individual predictions, analyzed the use of established EEG features, or examined subnetworks of larger models. In contrast, we apply interpretability methods to uncover features learned by the complete network. Specifically, we introduce two complementary architectures with dedicated visualization techniques to obtain an approximate understanding of the full network trained on binary classification into nonpathological and pathological EEG. First, we use invertible networks—networks that are designed to be invertible—to generate prototypical input signals for each class. Second, we design a very compact network that is fully visualizable, while still retaining reasonable decoding performance. Through these visualizations, we find both expected features like higher-amplitude oscillations in the delta and theta frequency bands in the temporal region for the pathological class as well as surprising differences in the very low sub-delta frequencies below 0.5 Hz. Closer investigation reveals higher spectral amplitudes for the healthy class at the frontal sensors in these sub-delta frequencies, an unexpected feature that the proposed visualizations helped identify. Overall, the study shows the potential of visualizations to understand the network prediction function without relying on specific predefined features.},
  archive      = {J_FROBT},
  author       = {Schirrmeister, Robin T. and Ball, Tonio},
  doi          = {10.3389/frobt.2025.1625732},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1625732},
  shortjournal = {Front. Robot. AI},
  title        = {New avenues for understanding what deep networks learn from EEG},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling trust and its dynamics from physiological signals and embedded measures for operational human-autonomy teaming. <em>FROBT</em>, <em>12</em>, 1624777. (<a href='https://doi.org/10.3389/frobt.2025.1624777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-autonomy teaming is an increasingly integral component of operational environments, including crewed and remotely operated space missions, military settings, and public safety. The performance of such teams relies on proper trust in the autonomous system, thus creating an urgent need to capture the dynamic nature of trust and devise objective, non-disruptive means of precisely modeling trust. This paper describes the use of bio-signals and embedded measures to create a model capable of inferring and predicting trust. Data (2304 observations) was collected via human subject testing (n = 12, 7M/5F) during which participants interacted with a simulated autonomous system in an operationally relevant, human-on-the-loop, remote monitoring task and reported their subjective trust via visual analog scales. Electrocardiogram, respiration, electrodermal activity, electroencephalogram, functional near-infrared spectroscopy, eye-tracking, and button click data were collected during each trial. Operator background information were collected prior to the experiment. Features were extracted and algorithmically down-selected, then ordinary least squares regression was used to fit the model, and predictive capabilities were assessed on unseen trials. Model predictions achieved a high level of accuracy with a Q2 of 0.64 and captured rapid changes in trust during an operationally relevant human-autonomy teaming task. The model advances the field of non-disruptive means of inferring trust by incorporating a broad suite of physiological signals into a model that is predictive, while many current models are purely descriptive. Future work should assess model performance on unseen participants.},
  archive      = {J_FROBT},
  author       = {Rindfuss, Abigail and Leary, Sarah and Dutta, Prachi and Chen, Ryan and Clark, Torin K. and Kong, Zhaodan and Hayman, Allison P. A.},
  doi          = {10.3389/frobt.2025.1624777},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1624777},
  shortjournal = {Front. Robot. AI},
  title        = {Modeling trust and its dynamics from physiological signals and embedded measures for operational human-autonomy teaming},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive emergency response and dynamic crowd navigation for mobile robot using deep reinforcement learning. <em>FROBT</em>, <em>12</em>, 1612392. (<a href='https://doi.org/10.3389/frobt.2025.1612392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile robots have emerged as a reliable solution for dynamic navigation in real-world applications. Effective deployment in high-density crowds and emergency scenarios requires not only accurate path planning but also rapid adaptation to changing environments. However, autonomous navigation in such environments remains a significant challenge, particularly in time-sensitive applications such as emergency response. Existing path planning and reinforcement learning approaches often lack adaptability to uncertainties and time-varying obstacles, thereby making them less suitable for unstructured real-world scenarios. To address these limitations, a Deep Reinforcement Learning (DRL) framework for dynamic crowd navigation using three algorithms, Deep Deterministic Policy Gradient (DDPG), Twin Delayed Deep Deterministic Policy Gradient (TD3), and Deep Q-Network (DQN), is proposed. A context-aware state representation that combines Light Detection and Ranging (LiDAR)-based obstacle perception, goal orientation, and robot kinematics to enhance situational awareness is developed. The proposed framework is implemented in a ROS2 Gazebo simulation environment using the TurtleBot3 platform and tested in challenging scenarios to identify the most effective algorithm. Extensive simulation analysis demonstrates that TD3 outperforms the other approaches in terms of success rate, path efficiency, and collision avoidance. This study contributes a reproducible, constraint-aware DRL navigation architecture suitable for real-time, emergency-oriented mobile robot applications.},
  archive      = {J_FROBT},
  author       = {Alexander, Anusha and Vangaveeti, V. N. Suchir and Venkatesan, Kalaichelvi and Mounsef, Jinane and Ramanujam, Karthikeyan},
  doi          = {10.3389/frobt.2025.1612392},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1612392},
  shortjournal = {Front. Robot. AI},
  title        = {Adaptive emergency response and dynamic crowd navigation for mobile robot using deep reinforcement learning},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error recovery in wearable robotic co-grasping: The role of human-led correction. <em>FROBT</em>, <em>12</em>, 1598296. (<a href='https://doi.org/10.3389/frobt.2025.1598296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionTrust in automated systems influences the use and disuse of new technologies. Although recent advances in robotics have improved wearable devices designed to assist in grasping, perfectly reliable systems have yet to be achieved. In this work, we introduce a new strategy for wearable devices called Co-Grasping, where both body power and robotics can contribute to grasping, but the user controls the allocation of the human and robot roles.MethodsOur implementation of a Co-Grasping device successfully allows the human operator to intervene using body power during simulated robot errors, in order to aid in error recovery and continue performing grasping tasks without drops.ResultsHere, we also show that the presence of recoverable errors lowers trust perception and increases physical engagement behaviors. However, when the robot becomes reliable once again, trust rebounds and most behavioral metrics return to baseline as well.DiscussionThese results indicate that trust in faulty automation can be repaired and that enabling users to assume control over system actuation in response to such faults can prevent errors from negatively affecting overall device function. Facilitating human-led dynamic changes in human and robot role allocation through this Co-Grasping device lays a promising foundation for unique human-robot interactions that promote high performance and where trust can recover quickly, despite existing challenges in developing perfect automated systems.},
  archive      = {J_FROBT},
  author       = {Chang, Erin Y. and Torres, Wilson O. and Stuart, Hannah S.},
  doi          = {10.3389/frobt.2025.1598296},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1598296},
  shortjournal = {Front. Robot. AI},
  title        = {Error recovery in wearable robotic co-grasping: The role of human-led correction},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The future of robotic disassembly: A systematic review of techniques and applications in the age of AI. <em>FROBT</em>, <em>12</em>, 1584657. (<a href='https://doi.org/10.3389/frobt.2025.1584657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s era of digital transformation, industries have made a decisive leap by adopting data-driven, robot-assisted disassembly solutions that cut cycle time and cost relative to labor-intensive manual tear-down. Thus, including robots not only improved production activities but also strengthened the safety measures that once the human operator was handling. Minimizing the impact of the human factor in the process means minimizing incidents related to it. The disassembly of Waste Electrical and Electronic Equipment (WEEE) poses complex technical, economic, and safety challenges that traditional manual methods struggle to meet. Thus, there is a need for a decision-making tool harmonized with human cooperation, in which Artificial Intelligence (AI) plays a pivotal role by providing financially viable solutions while ensuring a secure collaborative environment for both humans and robots. This review synthesizes recent advances in AI-enabled robotic disassembly by focusing on four main research areas: i optimization and strategic planning, ii human–robot collaboration (HRC), iii computer vision (CV) integration, and (iv) Safety for Collaborative Applications. A supplementary subsection is also included to briefly acknowledge emerging topics such as reinforcement learning that lie outside the main scope but represent promising future directions. By analyzing 62 peer-reviewed studies published between 2000 and 2024, the results identify how these themes converge, highlight open challenges, and map out future research directions.},
  archive      = {J_FROBT},
  author       = {Ameur, Soufiane and Tabaa, Mohamed and Hidila, Zineb and Hamlich, Mohamed and Karboub, Kaouter and Bearee, Richard},
  doi          = {10.3389/frobt.2025.1584657},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1584657},
  shortjournal = {Front. Robot. AI},
  title        = {The future of robotic disassembly: A systematic review of techniques and applications in the age of AI},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An approach for unsupervised interaction clustering in human–robot co-work using spatiotemporal graph convolutional networks. <em>FROBT</em>, <em>12</em>, 1545712. (<a href='https://doi.org/10.3389/frobt.2025.1545712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an approach to cluster interaction forms in industrial human–robot co-work using spatiotemporal graph convolutional networks (STGCNs). Humans will increasingly work with robots in the future, whereas previously, humans worked side by side, hand in hand, or alone. The growing frequency of robotic and human–robot co-working applications and the requirement to increase flexibility affect the variety and variability of interactions between humans and robots, which can be observed at production workplaces. In this paper, we investigate the variety and variability of human–robot interactions in industrial co-work scenarios where full automation is impractical. To address the challenges of interaction modeling and clustering, we present an approach that utilizes STGCNs for interaction clustering. Data were collected from 12 realistic human–robot co-work scenarios using a high-accuracy tracking system. The approach identified 10 distinct interaction forms, revealing more granular interaction patterns than established taxonomies. These results support continuous, data-driven analysis of human–robot behavior and contribute to the development of more flexible, human-centered systems that are aligned with Industry 5.0.},
  archive      = {J_FROBT},
  author       = {Heuermann, Aaron and Ghrairi, Zied and Zitnikov, Anton and Al Noman, Abdullah and Thoben, Klaus-Dieter},
  doi          = {10.3389/frobt.2025.1545712},
  journal      = {Frontiers in Robotics and AI},
  month        = {10},
  pages        = {1545712},
  shortjournal = {Front. Robot. AI},
  title        = {An approach for unsupervised interaction clustering in human–robot co-work using spatiotemporal graph convolutional networks},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
