<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>aaai</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jair">JAIR - 6</h2>
<ul>
<li><details>
<summary>
(2025). Viewpoint: The future of human-centric explainable artificial intelligence (XAI) is not post-hoc explanations. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.17970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable Artificial Intelligence (XAI) plays a crucial role in enabling human understanding and trust in deep learning systems. As models get larger, more ubiquitous, and pervasive in aspects of daily life, explainability is necessary to minimize adverse effects of model mistakes. Unfortunately, current approaches in human-centric XAI (e.g. predictive tasks in healthcare, education, or personalized ads) tend to rely on a single post-hoc explainer, whereas recent work has identified systematic disagreement between post-hoc explainers when applied to the same instances of underlying black-box models. In this viewpoint paper, we therefore present a call for action to address the limitations of current state-of-the-art explainers. We propose a shift from post-hoc explainability to designing interpretable neural network architectures. We identify five needs of human-centric XAI (real-time, accurate, actionable, human-interpretable, and consistent) and propose two possible routes forward for interpretable-by-design neural network workflows (adaptive routing and temporal diagnostics). We postulate that the future of human-centric XAI is neither in explaining black-boxes nor in reverting to traditional, interpretable models, but in neural networks that are intrinsically interpretable.},
  archive      = {J_JAIR},
  author       = {Vinitra Swamy and Jibril Frej and Tanja Käser},
  doi          = {10.1613/jair.1.17970},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Viewpoint: The future of human-centric explainable artificial intelligence (XAI) is not post-hoc explanations},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal decision trees for interpretable and constrained clustering. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.18144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained clustering is a semi-supervised approach to determining meaningful groupings of data that respect user-specified constraints. Such constraints are typically used to enforce desirable structural and domain-specific properties of the resulting clusters. Notably, such constraints can significantly improve the quality and accuracy of clustering. Data clustering solutions can take on many different forms. Decision trees are a particularly desirable solution form because of their inherent interpretability. Unfortunately, existing decision tree clustering approaches do not support clustering constraints and do not provide strong theoretical guarantees with respect to solution quality. To address the task of decision tree clustering with constraints, we present a novel SAT-based encoding that solves the problem to an approximated optimality in relation to a well-known bi-criteria objective. Our framework is the first exact approach for interpretable constrained clustering with decision trees. Experiments involving a range of real-world and synthetic datasets demonstrate that our approach can produce interpretable clustering solutions that are of superior quality compared to their non-interpretable counterparts, with or without the addition of constraints. We further provide new insights into the trade-off between interpretability and the satisfaction of user-specified constraints, presenting extensions to our clustering approach that treat the satisfaction of constraints as an additional optimization objective.},
  archive      = {J_JAIR},
  author       = {Pouya Shati and Yuliang Song and Eldan Cohen and Sheila McIlraith},
  doi          = {10.1613/jair.1.18144},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Optimal decision trees for interpretable and constrained clustering},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On generating monolithic and model reconciling explanations in probabilistic scenarios. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.18820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explanation generation frameworks aim to make AI systems’ decisions transparent and understandable to human users. However, generating explanations in uncertain environments characterized by incomplete information and probabilistic models remains a significant challenge. In this paper, we propose a novel framework for generating probabilistic monolithic explanations and model reconciling explanations. Monolithic explanations provide self-contained reasons for an explanandum without considering the agent receiving the explanation, while model reconciling explanations account for the knowledge of the agent receiving the explanation. For monolithic explanations, our approach integrates uncertainty by utilizing probabilistic logic to increase the probability of the explanandum. For model reconciling explanations, we propose a framework that extends the logic-based variant of the model reconciliation problem to account for probabilistic human models, where the goal is to find explanations that increase the probability of the explanandum while minimizing conflicts between the explanation and the probabilistic human model. We introduce explanatory gain and explanatory power as quantitative metrics to assess the quality of these explanations. Further, we present algorithms that exploit the duality between minimal correction sets and minimal unsatisfiable sets to efficiently compute both types of explanations in probabilistic contexts. Extensive experimental evaluations on various benchmarks demonstrate the effectiveness and scalability of our approach in generating explanations under uncertainty.},
  archive      = {J_JAIR},
  author       = {Stylianos Loukas Vasileiou and William Yeoh and Alessandro Previti and Tran Cao Son},
  doi          = {10.1613/jair.1.18820},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On generating monolithic and model reconciling explanations in probabilistic scenarios},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust reward design for markov decision processes. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.19154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of reward design examines the interaction between a leader and a follower, where the leader aims to shape the follower’s behavior to maximize the leader’s payoff by modifying the follower’s reward function. Current approaches to reward design rely on an accurate model of how the follower responds to reward modifications, which can be sensitive to modeling inaccuracies. To address this issue of sensitivity, we present a solution that offers robustness against uncertainties in modeling the follower, including 1) how the follower breaks ties in the presence of nonunique best responses, 2) inexact knowledge of how the follower perceives reward modifications, and 3) bounded rationality of the follower. Our robust solution is guaranteed to exist under mild conditions and can be obtained numerically by solving a mixed-integer linear program. Numerical experiments on multiple test cases demonstrate that our solution improves robustness compared to the standard approach without incurring significant additional computing costs.},
  archive      = {J_JAIR},
  author       = {Shuo Wu and Haoxiang Ma and Jie Fu and Shuo Han},
  doi          = {10.1613/jair.1.19154},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Robust reward design for markov decision processes},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a simple hedonic game with graph-restricted communication. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.14956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a hedonic game for which feasible coalitions are prescribed by a graph representing the agents’ social relations. A group of agents can form a feasible coalition if and only if their corresponding vertices can be spanned with a star. This requirement guarantees that agents are connected, close to each other, and one central agent can coordinate the actions of the group. In our game, everyone strives to join the largest feasible coalition. We study the existence and computational complexity of both Nash stable and core stable partitions. Then, we provide tight or asymptotically tight bounds on their efficiency, measured in terms of the price of anarchy and the price of stability, under two natural social functions, namely, the number of agents who are not in a singleton coalition, and the number of coalitions. We also derive refined bounds for games in which the social graph is claw-free. Finally, we investigate the complexity of computing socially optimal partitions, as well as extreme Nash stable ones.},
  archive      = {J_JAIR},
  author       = {Vittorio Bilò and Laurent Gourvès and Jérôme Monnot},
  doi          = {10.1613/jair.1.14956},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On a simple hedonic game with graph-restricted communication},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correct explanations and how to define them: Properties and metrics for measuring correctness of three forms of ML model Input/Output behaviour explanations. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.18691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In explainable AI, many explanation methods generate similar yet diverging explanations for machine learning (ML) models. How fair is it then to explain ML model behaviour by such explanations? Arguably, one needs to judge whether those explanations are good at explaining ML model input/output behaviour. We here attempt to formalise ways to judge goodness of such explanations in terms of their correctness. For assessing correctness, one needs to have desirable properties of explanation correctness in mind, as well as was to measure satisfaction of those properties. We submit two high-level properties of soundness and completeness for assessing explanation correctness: explaining is sound if the model behaves the way the explanations say; explaining is complete if explanations can be given for model’s outputs on any inputs. We formulate soundness and completeness properties for three forms of explanations: feature importance, counterfactuals and rules. We further formalise multiple general metrics, at least one for each property and form of explanation, for quantitatively measuring satisfaction of soundness and completeness. We argue that explanations are correct in as much as various aspects of the different forms of explanations are met as quantified by those metrics. We hope that being able to assess correctness of ML model input/output behaviour explanations against formal properties and metrics is a substantial step towards fairly explaining ML-based inference.},
  archive      = {J_JAIR},
  author       = {Vandita Singh and Kristijonas Cyras and Muhammad Zain Akram and Rafia Inam},
  doi          = {10.1613/jair.1.18691},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Correct explanations and how to define them: Properties and metrics for measuring correctness of three forms of ML model Input/Output behaviour explanations},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
