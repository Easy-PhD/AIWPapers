<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PEERJCS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="peerjcs">PEERJCS - 69</h2>
<ul>
<li><details>
<summary>
(2025). SA3C-ID: A novel network intrusion detection model using feature selection and adversarial training. <em>PEERJCS</em>, <em>11</em>, e3089. (<a href='https://doi.org/10.7717/peerj-cs.3089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous proliferation of emerging technologies such as cloud computing, 5G networks, and the Internet of Things, the field of cybersecurity is facing an increasing number of complex challenges. Network intrusion detection systems, as a fundamental part of network security, have become increasingly significant. However, traditional intrusion detection methods exhibit several limitations, including insufficient feature extraction from network data, high model complexity, and data imbalance, which result in issues like low detection efficiency, as well as frequent false positives and missed alarms. To address the above issues, this article proposed an adversarial intrusion detection model (Soft Adversarial Asynchronous Actor-Critic Intrusion Detection, SA3C-ID) based on reinforcement learning. Firstly, the raw dataset is preprocessed via one-hot encoding and standardization. Subsequently, the refined data undergoes feature selection employing an improved pigeon-inspired optimizer (PIO) algorithm. This operation eliminates redundant and irrelevant features, consequently reducing data dimensionality while maintaining critical information. Next, the network intrusion detection process is modeled as a Markov decision process and integrated with the Soft Actor-Critic (SAC) reinforcement learning algorithm, with a view to constructing agents; In the context of adversarial training, two agents, designated as the attacker and the defender, are defined to perform asynchronous adversarial training. During this training process, both agents calculate the reward value, update their respective strategies, and transfer parameters based on the classification results. Finally, to verify the robustness and generalization ability of the SA3C-ID model, ablation experiments and comparative evaluations are conducted on two benchmark datasets, NSL-KDD and CSE-CIC-IDS2018. The experimental results demonstrate that SA3C-ID exhibits superior performance in comparison to other prevalent intrusion detection models. The F1-score attained by SA3C-ID was 92.58% and 98.76% on the NSL-KDD and CSE-CIC-IDS2018 datasets, respectively.},
  archive      = {J_PEERJCS},
  author       = {Wanwei Huang and Haobin Tian and Lei Wang and Sunan Wang and Kun Wang and Songze Li},
  doi          = {10.7717/peerj-cs.3089},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3089},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SA3C-ID: A novel network intrusion detection model using feature selection and adversarial training},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Music audio emotion regression using the fusion of convolutional neural networks and bidirectional long short-term memory models. <em>PEERJCS</em>, <em>11</em>, e3086. (<a href='https://doi.org/10.7717/peerj-cs.3086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music emotion regression (MER) is a vital field that bridges psychology and music information retrieval. Music has the powerful ability to evoke a wide range of human emotions, from joy and sadness to anger and calmness. Understanding how music influences emotional states is essential for grasping its psychological effects on individuals. This research presents an innovative model that combines convolutional neural networks (CNNs) with bidirectional long short-term memory (BiLSTM) networks to analyze and predict the emotional impact of musical audio. The model uses CNNs to detect temporal patterns and BiLSTMs to interpret sequences in both forward and backward directions, enhancing its ability to capture the complex structure of musical data. Additionally, a multi-head attention mechanism is incorporated to improve the model’s expressiveness and generalizability, making it especially effective for handling intricate sequential tasks and large datasets. The model’s performance was evaluated through sentiment prediction using extensive, publicly available datasets comprising over 9,000 musical excerpts. Results show that the proposed model significantly outperforms existing methods in MER, achieving an R-squared value of 0.845, indicating an excellent fit with the empirical data.},
  archive      = {J_PEERJCS},
  author       = {Yi Qiu and Yu Lin and Yun Lin},
  doi          = {10.7717/peerj-cs.3086},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3086},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Music audio emotion regression using the fusion of convolutional neural networks and bidirectional long short-term memory models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond buzzwords: NLP reveals common threads in sustainable and circular construction discourse. <em>PEERJCS</em>, <em>11</em>, e3085. (<a href='https://doi.org/10.7717/peerj-cs.3085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular economy and sustainability have both seen rapid growth in academic literature, often leading to ambiguity and the overuse of these terms. This obscures their true objectives and makes it challenging to discern their distinct intentions. Manually analyzing the vast body of recent publications to understand how these concepts connect to environmentally beneficial practices is laborious and time-consuming. This study aims to compare and analyze existing literature on sustainable and circular construction using natural language processing (NLP) techniques to elucidate the similarities and overlaps between these concepts within the construction industry. To achieve this, we employed three NLP methods: (1) TextRank, a graph-based ranking algorithm that extracts key structural relationships between terms in a document; (2) term frequency–inverse document frequency, a statistical measure that identifies the most significant terms based on their frequency and uniqueness within the corpus; and (3) semantic annotation (Wikifier), a method that links text tokens to structured knowledge bases such as Wikipedia for better contextual understanding. These methods are used to analyze a dataset of 480 academic articles focusing on sustainability and circular economy in the construction sector. Our analysis revealed that circular construction is more specific and practical, emphasizing resource efficiency, waste management, and industry-specific processes, targeting the operational aspects of recycling and resource recovery. In contrast, sustainable construction encompasses a broader and more holistic scope, including urban planning, community development, and long-term environmental impacts. This study demonstrates how NLP methods can systematically disentangle closely related frameworks in construction literature, providing a replicable methodological framework for future data-driven investigations. By clarifying the distinctions and overlaps between the terms “circular construction” and “sustainable construction”, our research offers enhanced understanding for policymakers, industry practitioners, and academics aiming to integrate sustainable and circular principles effectively within the construction sector.},
  archive      = {J_PEERJCS},
  author       = {Shakarim Aubakirov and Alexandr Pak and Iskander Akhmetov and Aidana Tleuken and Huseyin Atakan Varol and Assel Akzhalova and Ferhat Karaca},
  doi          = {10.7717/peerj-cs.3085},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3085},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Beyond buzzwords: NLP reveals common threads in sustainable and circular construction discourse},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing thyroid diagnosis: Integrating AI-driven CAD framework with numerical data and ultrasound images. <em>PEERJCS</em>, <em>11</em>, e3063. (<a href='https://doi.org/10.7717/peerj-cs.3063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an advanced computer-aided diagnosis (CAD) framework for thyroid disease diagnosis that integrates numerical patient data and ultrasound images. The framework uses cutting edge technologies, including Vision Transformers (ViTs) and SHapley Additive exPlanations (SHAPs), to increase diagnostic accuracy, interpretability, and clinical applicability. The proposed CAD framework employs the sparse search algorithm (SSA) for optimized feature selection from numerical data and the tree-structured Parzen estimator for tuning the hyperparameters. ViTs are utilized for analyzing thyroid ultrasound images, whereas SHAP provides explainable AI insights into model predictions. Extensive experiments were conducted on two datasets: the thyroid disease patient dataset and the DDTI: Thyroid Ultrasound Images dataset. Performance was evaluated via five-fold and ten-fold cross-validation utilizing metrics including accuracy, precision, and recall. The framework achieved promising performance, with models trained without data augmentation consistently outperforming their augmented counterparts. For the thyroid disease patient dataset, the best-performing model reported an accuracy of 99.71%, precision of 97.05%, recall of 99.29%, and F1-score of 98.16%. For the DDTI dataset, ViTs achieved an accuracy of 95.06% without augmentation, surpassing existing methodologies. Key features such as thyroxine, thyroid surgery, and thyroid-stimulating hormone (TSH) were identified as critical predictors of thyroid conditions. This study underscores the potentiality of AI-driven approaches in healthcare, paving the way for improved diagnostic outcomes and personalized treatment strategies.},
  archive      = {J_PEERJCS},
  author       = {Saleh Ateeq Almutairi},
  doi          = {10.7717/peerj-cs.3063},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3063},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Advancing thyroid diagnosis: Integrating AI-driven CAD framework with numerical data and ultrasound images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precise distance measurement with stereo camera: Experimental results. <em>PEERJCS</em>, <em>11</em>, e3057. (<a href='https://doi.org/10.7717/peerj-cs.3057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, image processing is used in many areas, especially artificial intelligence. This is because images are thought to contain a lot of information. In addition, many distance measurement studies have used image processing techniques. However, no studies have reached these high sensitivity and accuracy rates that can be used in engineering. The motivation of the study is to obtain the results of the experimental application of the image processing method, which can measure distances with high sensitivity and can also be used in engineering fields. In the study, the distances of 19 different target objects were measured using Total Station, Laser Meter, and Developed Prototype (Image Meter). Total Station measurement results were used as a reference and the Laser Meter and Image Meter results were compared. As a result of the comparison, it was determined that the developed Image Meter had a smaller error rate in 11 of the 19 comparisons. Results were obtained with an average error of 1.24% as a result of 19 measurements made with the developed Image Meter. The experimental results were also compared with theoretical calculation. As a result of the comparisons, it was determined that the results with the developed Image Meter were acceptable and could be improved with mechanical arrangements.},
  archive      = {J_PEERJCS},
  author       = {Haydar Yanık and Bülent Turan},
  doi          = {10.7717/peerj-cs.3057},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3057},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Precise distance measurement with stereo camera: Experimental results},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EquiRate: Balanced rating injection approach for popularity bias mitigation in recommender systems. <em>PEERJCS</em>, <em>11</em>, e3055. (<a href='https://doi.org/10.7717/peerj-cs.3055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems often suffer from popularity bias problem, favoring popular items and overshadowing less known or niche content, which limits recommendation diversity and content exposure. The root reason for this issue is the imbalances in the rating distribution; a few popular items receive a disproportionately large share of interactions, while the vast majority garner relatively few. In this study, we propose the EquiRate method as a pre-processing approach, addressing this problem by injecting synthetic ratings into less popular items to make the dataset regarding rating distribution more balanced. More specifically, this method utilizes several synthetic rating injection and synthetic rating generation strategies: (i) the first ones focus on determining which items to inject synthetic ratings into and calculating the total number of these ratings, while (ii) the second ones concentrate on computing the concrete values of the ratings to be included. We also introduce a holistic and highly efficient evaluation metric, i.e., the FusionIndex, concurrently measuring accuracy and several beyond-accuracy aspects of recommendations. The experiments realized on three benchmark datasets conclude that several EquiRate’s variants, with proper parameter-tuning, effectively reduce popularity bias and enhance recommendation diversity. We also observe that some prominent popularity-debiasing methods, when assessed using the FusionIndex, often fail to balance the referrals’ accuracy and beyond-accuracy factors. On the other hand, our best-performing EquiRate variants significantly outperform the existing methods regarding the FusionIndex, and their superiority is more apparent for the high-dimension data collections, which are more realistic for real-world scenarios.},
  archive      = {J_PEERJCS},
  author       = {Mert Gulsoy and Emre Yalcin and Alper Bilge},
  doi          = {10.7717/peerj-cs.3055},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3055},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {EquiRate: Balanced rating injection approach for popularity bias mitigation in recommender systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D reconstruction of toys based on adaptive scaled neural radiation field. <em>PEERJCS</em>, <em>11</em>, e3053. (<a href='https://doi.org/10.7717/peerj-cs.3053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of computer vision technology, 3D reconstruction of toys under single-view conditions still faces significant challenges in terms of detail loss and color distortion. For this reason, this article proposes an adaptive scale neural radiance fields (AS-NeRF) model to enhance the accuracy and realism of 3D toy reconstruction. The method constructs a multi-task feature extraction network based on the Vision Transformer, which simultaneously extracts and fuses multidimensional features such as texture, shape, color, and depth through a task dynamic modulation mechanism and a dynamic adapter layer, providing a rich and accurate contextual feature representation. The NeRF model is enhanced to incorporate an adaptive scaling mechanism that dynamically optimizes rendering sampling accuracy according to the local complexity of the scene. Spectral sensing techniques are integrated to reproduce the true colors of materials accurately. Finally, the conditional diffusion model is deeply integrated with NeRF, and high-dimensional conditional vectors are used to guide the inverse diffusion process in generating unobserved images with consistent geometric structure and physical properties. Experiments on the Co3D toy dataset demonstrate that AS-NeRF significantly outperforms existing mainstream methods in terms of peak signal-to-noise ratio (PSNR), structural similarity (SSIM), loss of perceptions (LPIPS), and Chamfer distance, thereby verifying the validity and advantages of the proposed method for high-quality toy 3D reconstruction tasks.},
  archive      = {J_PEERJCS},
  author       = {Jiajun Zou and Shaojiang Liu and Feng Wang and Weichuan Ni and Shitong Ye},
  doi          = {10.7717/peerj-cs.3053},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3053},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {3D reconstruction of toys based on adaptive scaled neural radiation field},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JDroid: Android malware detection using hybrid opcode feature vector. <em>PEERJCS</em>, <em>11</em>, e3051. (<a href='https://doi.org/10.7717/peerj-cs.3051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of devices using the Android operating system makes these devices the primary target for malware developers. Researchers are investigating different techniques to protect end users from these attackers. While many of these techniques are successful in detecting malware, they also have some limitations. Because many applications today use advanced obfuscation techniques, advanced disguise, and variant generation techniques to bypass detection tools, this creates difficulties for security experts. However, the rich semantic information hidden in opcodes offers a promising way to distinguish benign applications from malicious ones. In this study, we propose a tool called JDroid that treats opcodes (Dalvik Opcode and Java ByteCode) as features based on static analysis. The proposed tool aims to detect malicious applications with a unique ensemble model in a stacked generalised structure that uses different opcode sequences as a hybrid, and where each feature is first trained separately and then used by an ensemble decision. For this purpose, opcodes are extracted from APK files by code analysis and directly converted into vectors as 0 and 1 according to their usage cases. A subset of 461 features, obtained through filtering and feature selection processes, is then created using fewer features. This increases efficiency and performance, avoids overfitting, and reduces computational cost. The datasets Drebin, Genome, MalDroid2020, CICInvesAndMal2019, and Omer are tested with an application pool consisting of 14 thousand applications, and the classification performance is compared with different machine learning methods. Experimental results show that the proposed approach has an accuracy value of 98.6% and an area under the curve (AUC) value of 99.6% in malware detection without being affected by the obfuscation process.},
  archive      = {J_PEERJCS},
  author       = {Recep Sinan Arslan},
  doi          = {10.7717/peerj-cs.3051},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3051},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {JDroid: Android malware detection using hybrid opcode feature vector},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight graph convolutional network with multi-attention mechanisms for intelligent action recognition in online physical education. <em>PEERJCS</em>, <em>11</em>, e3050. (<a href='https://doi.org/10.7717/peerj-cs.3050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of online physical education in higher education has improved accessibility but presents challenges in recognizing complex movements and delivering individualized feedback. Existing action recognition models are often computationally intensive and struggle to generalize across diverse skeletal patterns. To address this, we propose a lightweight graph convolutional network (GCN) that integrates an improved Ghost module with multi-attention mechanisms, including a global attention mechanism (GAM) and a channel attention mechanism (CAM), to enhance spatial and temporal feature extraction. The model is trained end-to-end on 3D skeleton sequences and optimized for real-time efficiency. The computational cost is evaluated in terms of giga floating-point operations (GFLOPs), with the proposed model requiring only 6.2 GFLOPs per inference, over 60% less than the baseline ST-GCN. Experimental results on the NTU60RGB+D dataset demonstrate that the model achieves 90.8% accuracy in cross-subject and 96.8% in cross-view settings. These findings highlight the model’s effectiveness in balancing accuracy and efficiency, with promising applications in online physical education, rehabilitation monitoring, elderly movement analysis, and VR-based interfaces.},
  archive      = {J_PEERJCS},
  author       = {Yuhao You},
  doi          = {10.7717/peerj-cs.3050},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3050},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Lightweight graph convolutional network with multi-attention mechanisms for intelligent action recognition in online physical education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification of core sub-team on scientific collaboration networks with shapley method. <em>PEERJCS</em>, <em>11</em>, e3048. (<a href='https://doi.org/10.7717/peerj-cs.3048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the core sub-teams that drive productivity in scientific collaboration networks is essential for research evaluation and team management. However, existing methods typically rank individual researchers by bibliometric impact or select structurally cohesive clusters, but rarely account for both collaboration patterns and joint scientific output. To address this limitation, we propose a novel two-dimensional framework that integrates network topology with research performance to identify core sub-teams. Specifically, we measure each sub-team’s marginal structural contribution using the Shapley value and quantify its collective impact using a sub-team H-index. To efficiently identify high-contributing sub-teams, we employ the Monte Carlo Tree Search algorithm, along with an approximation strategy to estimate Shapley values under computational constraints. We evaluate our method on 61 real-world scientific collaboration teams from Web of Science and Baidu Scholar data. Experimental results validate the effectiveness of our method in identifying core sub-teams, with the highest collaborative and citation impact. The proposed method offers a valuable analytical tool for research managers and funding agencies seeking to locate high-impact collaborative clusters, and it provides a generalizable framework for studies requiring the integration of structural and performance-based indicators in network analysis.},
  archive      = {J_PEERJCS},
  author       = {Lixin Zhou and Chen Liu and Xue Song},
  doi          = {10.7717/peerj-cs.3048},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3048},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Identification of core sub-team on scientific collaboration networks with shapley method},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of psychiatry clinical notes by diagnosis: A deep learning and machine learning approach. <em>PEERJCS</em>, <em>11</em>, e3045. (<a href='https://doi.org/10.7717/peerj-cs.3045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of clinical notes into specific diagnostic categories is critical in healthcare, especially for mental health conditions like anxiety and adjustment disorder. In this study, we compare the performance of various artificial intelligence models, including both traditional machine learning approaches (random forest, support vector machine, K-nearest neighbors, decision tree, and eXtreme Gradient Boost) and deep learning models (DistilBERT and SciBERT), to classify clinical notes into these two diagnoses. Additionally, we implemented three oversampling strategies: No Oversampling, Random Oversampling, and Synthetic Minority Over-sampling Technique (SMOTE), to assess their impact on model performance. Hyperparameter tuning was also applied to optimize model accuracy. Our results indicate that oversampling techniques had minimal impact on model performance overall. The only exception was SMOTE, which showed a positive effect specifically with Bidirectional Encoder Representations from Transformers (BERT)-based models. However, hyperparameter optimization significantly improved accuracy across the models, enhancing their ability to generalize and perform on the dataset. The decision tree and eXtreme Gradient Boost models achieved the highest accuracy among machine learning approaches, both reaching 96%, while the DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category. These findings underscore the importance of hyperparameter tuning in maximizing model performance. This study contributes to the ongoing research on AI-assisted diagnostic tools in mental health by providing insights into the efficacy of different model architectures and data balancing methods.},
  archive      = {J_PEERJCS},
  author       = {Sergio Rubio-Martín and María Teresa García-Ordás and Antonio Serrano-García and Clara Margarita Franch-Pato and Arturo Crespo-Álvaro and José Alberto Benítez-Andrades},
  doi          = {10.7717/peerj-cs.3045},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3045},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classification of psychiatry clinical notes by diagnosis: A deep learning and machine learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grammatical error correction for low-resource languages: A review of challenges, strategies, computational and future directions. <em>PEERJCS</em>, <em>11</em>, e3044. (<a href='https://doi.org/10.7717/peerj-cs.3044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grammatical error correction (GEC) is crucial for enhancing the readability and comprehension of texts, particularly in improving text quality in low-resource languages. However, challenges such as data scarcity, linguistic diversity, and limited computational resources hinder advancements in this domain. To address these challenges, researchers have developed strategies such as synthetic data generation, multilingual pre-trained models, and cross-lingual transfer learning. This review synthesizes findings from key studies to explore effective GEC methods for low-resource languages, emphasizing approaches for handling limited annotated corpora, typological complexities, and evaluation challenges. Synthetic data generation techniques, including noise injection, adversarial error generation, and translationese-based augmentation, have proven vital for overcoming data scarcity. Multilingual and transfer learning approaches demonstrate effectiveness in adapting knowledge from high-resource languages to low-resource settings, especially when combined with fine-tuning on curated datasets. Additionally, linguistic diversity has been partially addressed through methods like morphology-aware embeddings, byte-level tokenization, and contextual data preprocessing. However, limited research exists on robust evaluation metrics tailored to diverse typologies, such as agglutinative and morphologically rich languages, and the creation of gold-standard datasets remains an ongoing challenge. Recent advancements in dataset construction and the use of large language models further enrich this field, offering scalable solutions for low-resource contexts. Despite notable progress, this review identifies gaps in evaluation methodologies and typology-specific solutions, calling for future innovations in multilingual modeling, dataset creation, and computationally efficient GEC systems tailored to the unique needs of low-resource languages.},
  archive      = {J_PEERJCS},
  author       = {Syauqie Muhammad Marier and Xiangfan Chen and Linan Zhu and Xiangjie Kong},
  doi          = {10.7717/peerj-cs.3044},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3044},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Grammatical error correction for low-resource languages: A review of challenges, strategies, computational and future directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task advanced convolutional neural network for robust lymphoblastic leukemia diagnosis, classification, and segmentation. <em>PEERJCS</em>, <em>11</em>, e3043. (<a href='https://doi.org/10.7717/peerj-cs.3043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute lymphoblastic leukemia (ALL), a hematologic malignancy characterized by the overproduction of immature lymphocytes, a type of white blood cell. Accurate and timely diagnosis of ALL is crucial for effective management. This article introduces a novel multi-task advanced convolutional neural network (MTA-CNN) framework for ALL detection in medical imaging data by simultaneously performing, expression classification, and disease detection. The MTA-CNN is based on a deep learning architecture that can handle multiple tasks simultaneously, allowing it to learn more comprehensive and generalizable features. With, expression classification, and disease detection tasks, the MTA-CNN effectively leverages the complementary information from each task to improve overall performance. The proposed framework employs CNNs to extract informative features from medical images. These features capture the spatial and temporal characteristics of the data, which are essential for accurate ALL diagnosis. The cascaded structure of the MTA-CNN allows the model to learn features at different levels of abstraction, from low-level to high-level, enabling it to capture both fine-grained and coarse-grained information. To ensure the reliability of the detection results, non-maximum suppression is employed to eliminate redundant detections, focusing only on the most likely candidates. Additionally, the MTA-CNN’s ability to accurately localize key facial landmarks provides valuable information for further analysis, including identifying abnormal structures or changes in anatomical features associated with ALL. Experimental results on a comprehensive dataset of medical images demonstrate the superiority of the MTA-CNN over other learning methods. The proposed framework achieved an accuracy of 0.978, precision of 0.979, recall of 0.967, F1-score of 0.973, specificity of 0.991, Cohen’s kappa of 0.979, and negative predictive value (NPV) of 0.990. These metrics significantly outperform baseline models, highlighting the MTA-CNN’s ability to accurately identify and classify ALL cases. The MTA-CNN offers a promising approach for improving the efficiency and accuracy of ALL diagnosis.},
  archive      = {J_PEERJCS},
  author       = {Sercan Yalcin and Zuhal Cetin Yalcin and Muhammed Yildirim and Bilal Alatas},
  doi          = {10.7717/peerj-cs.3043},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3043},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-task advanced convolutional neural network for robust lymphoblastic leukemia diagnosis, classification, and segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective optimization for smart cities: A systematic review of algorithms, challenges, and future directions. <em>PEERJCS</em>, <em>11</em>, e3042. (<a href='https://doi.org/10.7717/peerj-cs.3042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing complexity and interdependence of urban systems, multi-objective optimization (MOO) has become a critical tool for smart-city planning, sustainability, and real-time decision-making. This article presents a systematic literature review (SLR) of 117 peer-reviewed studies published between 2015 and 2025, assessing the evolution, classification, and performance of MOO techniques in smart-city contexts. Existing algorithms are organised into four families—bio-inspired, mathematical theory-driven, physics-inspired, and machine-learning-enhanced—and benchmarked for computational efficiency, scalability, and scenario suitability across six urban domains: infrastructure, energy, transportation, Internet of Things (IoT)/cloud systems, agriculture, and water management. While established methods such as Non-dominated Sorting Genetic Algorithm II (NSGA-II) and Multiobjective Evolutionary Algorithm based on Decomposition (MOED/D) remain prevalent, hybrid frameworks that couple deep learning with evolutionary search display superior adaptability in high-dimensional, dynamic environments. Persistent challenges include limited cross-domain generalisability, inadequate uncertainty handling, and low interpretability of artificial intelligence (AI)-assisted models. Twelve research gaps are synthesised—from privacy-preserving optimisation and sustainable trade-off resolution to integration with digital twins, large language models, and neuromorphic computing—and a roadmap towards scalable, interpretable, and resilient optimisation frameworks is outlined. Finally, a ready-to-use benchmarking toolkit and a deployment-oriented algorithm-selection matrix are provided to guide researchers, engineers, and policy-makers in real-world smart-city applications. This review targets interdisciplinary researchers, optimisation developers, and smart-city practitioners seeking to apply or advance MOO techniques in complex urban systems.},
  archive      = {J_PEERJCS},
  author       = {YiFan Chen and Weng Howe Chan and Eileen Lee Ming Su and Qi Diao},
  doi          = {10.7717/peerj-cs.3042},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3042},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-objective optimization for smart cities: A systematic review of algorithms, challenges, and future directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive filter parameter reconstruction technology for rocket inertial navigation/satellite integrated navigation system. <em>PEERJCS</em>, <em>11</em>, e3040. (<a href='https://doi.org/10.7717/peerj-cs.3040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of micro-electro-mechanical systems (MEMS) strapdown inertial navigation systems (SINS) with global navigation satellite systems (GNSS) has emerged as a significant area of research due to its compact size, affordability, and high precision. In the context of guided rocket-borne MEMS-SINS/GNSS integrated navigation systems, the performance of navigation is characterized by the need for high overload, accuracy, and real-time capability. A variety of enhanced algorithms based on Kalman filtering are currently employed as integrated filtering methods, which comprehensively address deviations in the system model to improve navigation performance. The noise characteristics of MEMS inertial guidance devices change dramatically under long-term storage conditions, while the dynamic flight environment of rockets and the high real-time requirements of navigation solving make the design of on-board combined navigation filters challenging. To address this issue, this article introduces the Adaptive Reconfigurable Extended Kalman Filter (AREKF) method. Initially, a precise system state model is developed to reflect the unique characteristics of the rocket flight environment, facilitating rapid convergence of the filtering process. Subsequently, during the rocket alignment process, a real-time reconstruction of filter parameters is implemented to enable adaptive and precise modeling of navigation parameters. This strategy ensures lower computational costs during rocket flight, enhances the accuracy of the navigation system, and produces real-time navigation outputs that exhibit high overload and precision. The results from the Six-Degree (6D) Model simulation and car-mounted experiments demonstrate that, compared to the traditional Extended Kalman Filter (EKF) algorithm and existing improved algorithms, the AREKF method significantly enhances the real-time navigation accuracy of rockets under high overload conditions.},
  archive      = {J_PEERJCS},
  author       = {Zhijie Yang and Guoguang Chen and Mingli Niu and Xiaolong Yan and Xiaoli Tian and Guocui Zhang},
  doi          = {10.7717/peerj-cs.3040},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3040},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adaptive filter parameter reconstruction technology for rocket inertial navigation/satellite integrated navigation system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A progressive attention-based cross-modal fusion network for cardiovascular disease detection using synchronized electrocardiogram and phonocardiogram signals. <em>PEERJCS</em>, <em>11</em>, e3038. (<a href='https://doi.org/10.7717/peerj-cs.3038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synchronized electrocardiogram (ECG) and phonocardiogram (PCG) signals provide complementary diagnostic insights crucial for improving the accuracy of cardiovascular disease (CVD) detection. However, existing deep learning methods often utilize single-modal data or employ simplistic early or late fusion strategies, which inadequately capture the complex, hierarchical interdependencies between these modalities, thereby limiting detection performance. This study introduces PACFNet, a novel progressive attention-based cross-modal feature fusion network, for end-to-end CVD detection. PACFNet features a three-branch architecture: two modality-specific encoders for ECG and PCG, and a progressive selective attention-based cross-modal fusion encoder. A key innovation is its four-layer progressive fusion mechanism, which integrates multi-modal information from low-level morphological details to high-level semantic representations. This is achieved by selective attention-based cross-modal fusion (SACMF) modules at each progressive level, employing cascaded spatial and channel attention to dynamically emphasize salient feature contributions across modalities, thus significantly enhancing feature learning. Signals are pre-processed using a beat-to-beat segmentation approach to analyze individual cardiac cycles. Experimental validation on the public PhysioNet 2016 dataset demonstrates PACFNet’s state-of-the-art performance, with an accuracy of 97.7%, sensitivity of 98%, specificity of 97.3%, and an F1-score of 99.7%. Notably, PACFNet not only excels in multi-modal settings but also maintains robust diagnostic capabilities even with missing modalities, underscoring its practical effectiveness and reliability. The source code is publicly available on Zenodo (https://zenodo.org/records/15450169).},
  archive      = {J_PEERJCS},
  author       = {Wei Peng Li and Joon Huang Chuah and Guo Jeng Tan and Chengyu Liu and Hua-Nong Ting},
  doi          = {10.7717/peerj-cs.3038},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3038},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A progressive attention-based cross-modal fusion network for cardiovascular disease detection using synchronized electrocardiogram and phonocardiogram signals},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based BiLSTM-XGBoost model for reliability assessment and lifetime prediction of digital microfluidic systems. <em>PEERJCS</em>, <em>11</em>, e3037. (<a href='https://doi.org/10.7717/peerj-cs.3037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods for reliability and lifetime testing of digital microfluidic systems heavily rely on real-time monitoring data. This often leads to evaluation lag and limits their application, especially for complex droplets. To address these issues, this study proposes a novel prediction model for digital microfluidic (DMF) devices. The model combines an attention-based bidirectional long short-term memory (BiLSTM) with eXtreme Gradient Boosting (XGBoost) using a Stacking approach. This integrated model efficiently identifies the health state and predicts the failure time of digital microfluidic devices. This approach overcomes the limitations of traditional methods, such as over-reliance on sensor feedback and detection hysteresis. Experimental results demonstrate high prediction accuracy. The model achieved a mean absolute percentage error (MAPE) of 1.6464, Root mean squared error (RMSE) of 0.3667, mean absolute error (MAE) of 0.2557, and a coefficient of determination (R-squared) of 0.9949. Compared to baseline methods, the proposed BiLSTM-XGBoost model achieves the highest prediction accuracy, enabling effective health monitoring, problem identification, and failure prediction. Ultimately, this improves system reliability and lifetime with greater timeliness and accuracy.},
  archive      = {J_PEERJCS},
  author       = {Lifeng He and Qili Yang and Junxi Chen and Wenjing Liu and Zhijie Luo},
  doi          = {10.7717/peerj-cs.3037},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3037},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Attention-based BiLSTM-XGBoost model for reliability assessment and lifetime prediction of digital microfluidic systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alpha-DehazeNet: Single image dehazing via RGBA haze modeling and adaptive learning. <em>PEERJCS</em>, <em>11</em>, e3036. (<a href='https://doi.org/10.7717/peerj-cs.3036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is a vital research area in computer vision. Many existing deep learning-based dehazing methods rely on atmospheric scattering models with manually predefined, non-trainable parameters, which limits their adaptability and transferability. We propose Alpha-DehazeNet, a novel model that leverages red green blue alpha (RGBA) haze layer effect maps by defining a grayscale transparency map in the RGBA color space as the initial haze layer. Alpha-DehazeNet employs a U-Net generator enhanced with a spatial attention mechanism to encode haze-related features. This generator is integrated into an adversarial architecture with residual connections, enabling end-to-end training. Additionally, a depth consistency loss is introduced to improve dehazing accuracy. Alpha-DehazeNet outperforms several state-of-the-art models on synthetic datasets (ITS and OTS from RESIDE), achieving 37.35 dB peak signal-to-noise ratio (PSNR) on SOTS-indoor and 37.39 dB PSNR on SOTS-outdoor, while using only 8.86 million parameters. On real-world datasets, Alpha-DehazeNet delivers competitive results, although it shows limitations in handling non-white fog and cloud conditions. The code is publicly available at: https://doi.org/10.5281/zenodo.15361810.},
  archive      = {J_PEERJCS},
  author       = {Jin He and Ruibin Li},
  doi          = {10.7717/peerj-cs.3036},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3036},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Alpha-DehazeNet: Single image dehazing via RGBA haze modeling and adaptive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local-global multi-scale attention network for medical image segmentation. <em>PEERJCS</em>, <em>11</em>, e3033. (<a href='https://doi.org/10.7717/peerj-cs.3033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of deep learning technologies, deep learning-based medical image segmentation methods have achieved remarkable results. However, existing segmentation approaches still face several key challenges, including the insufficient extraction of local and global information from images and the inaccurate selection of core features. To address these challenges, this article proposes a novel medical image segmentation architecture—local-global multi-scale attention network (LGMANet). LGMANet introduces an innovative local-global information processing block (LGIPB) to effectively facilitate the deep mining of both local and global information during the downsampling process. In addition, an efficient multi-scale reconstruction attention (EMRA) module is designed to help the model accurately extract core features and multi-scale information while effectively suppressing irrelevant content. Experiments on the ISIC2018, CVC-ClinicDB, BUSI, and GLaS datasets demonstrate that LGMANet achieves IoU scores of 85.28%, 82.67%, 70.07%, and 88.90%, respectively, showcasing its superior segmentation performance.},
  archive      = {J_PEERJCS},
  author       = {Minghui Zhu and Dapeng Cheng and Yanyan Mao and Lu Sun and Wanting Jing},
  doi          = {10.7717/peerj-cs.3033},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3033},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Local-global multi-scale attention network for medical image segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic token encryption for preventing permission leakage in serverless architectures. <em>PEERJCS</em>, <em>11</em>, e3029. (<a href='https://doi.org/10.7717/peerj-cs.3029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless architecture simplifies application development and operation, but its permission control model based on static execution roles struggles to adapt to highly dynamic runtime environments, which can easily lead to the risk of permission and key leakage. To address this challenge, this article proposes a runtime dynamic token-based access control scheme. The scheme combines function context and user-defined security rules to achieve function-level dynamic authorization and request-level identity authentication. The generated dynamic tokens possess strong randomness, unpredictability, and one-time use characteristics, effectively reducing the harm caused by token leakage. Moreover, the designed multi-factor token verification model integrates dynamic factors such as call chain features and behavior patterns, which can defend against various security threats. Through social surveys, qualitative analysis, and extensive experiments, this article confirms that the proposed scheme significantly enhances the security of serverless applications while maintaining a controllable impact on platform performance. This research enriches the theoretical knowledge in the field of serverless security and provides new ideas for development practices, which is expected to promote the expansion of serverless architecture to enterprise-level scenarios and contribute to the healthy development of its ecosystem.},
  archive      = {J_PEERJCS},
  author       = {Yu Liu and Fu Li and Chenhao Sun},
  doi          = {10.7717/peerj-cs.3029},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3029},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic token encryption for preventing permission leakage in serverless architectures},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid TCN-TE-ANN model for high-precision solar irradiance prediction. <em>PEERJCS</em>, <em>11</em>, e3026. (<a href='https://doi.org/10.7717/peerj-cs.3026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of solar irradiance is critical for optimizing solar energy systems, enhancing grid stability, and supporting sustainable energy transitions. While numerous studies have explored various methodologies for solar radiation prediction, challenges remain in achieving high accuracy across diverse geographic locations and temporal resolutions. This study presents a novel hybrid model combining temporal convolutional networks (TCN), Transformer encoders (TE), and artificial neural networks (ANN) to predict global horizontal irradiance (GHI) with high precision. Utilizing a comprehensive dataset from three significant U.S. solar energy sites—Desert Sunlight, Copper Mountain, and Solar Star—spanning 22 years at a 30-min temporal resolution, the proposed model demonstrated superior performance metrics, with R2 ranging from 0.94768 to 0.97417, root mean square error (RMSE) between 0.04776 and 0.06543 W/m2, and mean absolute error (MAE) between 0.02510 and 0.03526 W/m2. By leveraging TCN’s temporal feature extraction, TE’s attention mechanisms, and ANN’s dense layer refinements, the model demonstrates significant advancements over existing methods.},
  archive      = {J_PEERJCS},
  author       = {Murat Isik},
  doi          = {10.7717/peerj-cs.3026},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3026},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel hybrid TCN-TE-ANN model for high-precision solar irradiance prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive review of dimensionality reduction algorithms: Challenges, limitations, and innovative solutions. <em>PEERJCS</em>, <em>11</em>, e3025. (<a href='https://doi.org/10.7717/peerj-cs.3025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction (DR) simplifies complex data from genomics, imaging, sensors, and language into interpretable forms that support visualization, clustering, and modeling. Yet widely used methods like principal component analysis, t-distributed stochastic neighbor embedding, uniform manifold approximation and projection, and autoencoders are often applied as “black boxes,” neglecting interpretability, fairness, stability, and privacy. This review introduces a unified classification—linear, nonlinear, hybrid, and ensemble approaches—and assesses them against eight core challenges: dimensionality selection, overfitting, instability, noise sensitivity, bias, scalability, privacy risks, and ethical compliance. We outline solutions such as intrinsic dimensionality estimation, robust neighborhood graphs, fairness-aware embeddings, scalable algorithms, and automated tuning. Drawing on case studies from bioinformatics, vision, language, and Internet of Things analytics, we offer a practical roadmap for deploying dimensionality reduction methods that are scalable, interpretable, and ethically sound—advancing responsible artificial intelligence in high-stakes applications.},
  archive      = {J_PEERJCS},
  author       = {Aasim Ayaz Wani},
  doi          = {10.7717/peerj-cs.3025},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3025},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comprehensive review of dimensionality reduction algorithms: Challenges, limitations, and innovative solutions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced approach for automatic annotation of error codes based on seq2edit. <em>PEERJCS</em>, <em>11</em>, e3024. (<a href='https://doi.org/10.7717/peerj-cs.3024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep natural language translation models have been used for automatic code error correction and have demonstrated outstanding potential. However, a large and accurately annotated training dataset is essential for these models to perform well. The key to improving the performance of these models lies in automatically and accurately annotating code errors and establishing a larger training dataset. Recently, a code error automatic annotation method based on Seq2edit has been proposed to optimize the dataset. However, the accuracy of the annotation is affected because tokens in the input code from the same statement may be aligned to different statements. This article proposes a Seq2edit annotation method based on the source code’s sentence structure. By dividing the code into statements with independent meanings and introducing a cost coefficient to improve the Levenshtein algorithm, this method optimizes the calculation of edit distance and enhances the ability to align tokens. Experimental results show that this method can fully utilize the contextual information of the source code during the automatic annotation process, leading to a significant improvement in annotation accuracy.},
  archive      = {J_PEERJCS},
  author       = {Jian Wang and Tao Lin and Rongsen Zhao and Huiling Zhao},
  doi          = {10.7717/peerj-cs.3024},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3024},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An enhanced approach for automatic annotation of error codes based on seq2edit},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intent-aware knowledge graph-based model for electrical power material recommendation. <em>PEERJCS</em>, <em>11</em>, e3023. (<a href='https://doi.org/10.7717/peerj-cs.3023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of electrical power material management, it is paramount that users receive accurate recommendations regarding the electrical power materials they require. Recently, a growing number of studies have been dedicated to graph neural network (GNN)-based recommendation systems due to their ability to seamlessly combine node information with topological structure, enhancing the effectiveness of recommendations. However, a notable drawback of current GNN-based recommendation is their inability to explicitly capture users’ intent in recommendations, which limits the performance. In fact, users’ intent is crucial in determining their actions. One example is when users first form an intent to buy a particular set of items and then choose a specific item from the set based on their preferences. To fill this gap, this article proposes an intent-aware knowledge graph-based model for electrical material recommendation, named IKG-EMR. IKG-EMR models user preferences and intent by leveraging knowledge graph and user behavior sequences, respectively. Specifically, a graph neural network is adopted to generate user intent embedding and item embedding from the tripartite graph of “User-Item-Topic”, and a multi-head attention network (Transformer) is used for extracting preference from user behavior sequences. Finally, an adaptive fusion with attention network is devised to generate comprehensive user representation by integrating user preference and intent features. Extensive experiments conducted on the real-life electric power materials show that our proposed model outperforms state-of-the-art methods.},
  archive      = {J_PEERJCS},
  author       = {Lin Zhao and Ning Luan and Weihua Cheng and Shuming Feng and Hui Wang and Yongcheng Yang and Guixiang Zhu},
  doi          = {10.7717/peerj-cs.3023},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3023},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intent-aware knowledge graph-based model for electrical power material recommendation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new machine learning method for rainfall classification: Temporal random tree. <em>PEERJCS</em>, <em>11</em>, e3022. (<a href='https://doi.org/10.7717/peerj-cs.3022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional classification algorithms usually assume that all samples in a dataset contribute equally to the training of a machine learning model, which is not always the case. In fact, samples in temporal data, such as precipitation data, may not have equal importance; more recent samples contain more accurate and useful information than earlier ones. To address this issue, the article proposes a novel method, named temporal random tree (TRT), in which recent training samples have a greater impact on the model’s decision-making process. It divides the dataset into temporal segments, assigns higher weights to classifiers trained on more recent data, and employs a weighted majority voting strategy. The experiments demonstrated the effectiveness of TRT on the real-world WeatherAUS precipitation dataset, achieving an accuracy of 83.54%, which represents a 5% improvement over the traditional random tree method. Additionally, our method achieved an average improvement of 9.98% compared to state-of-the-art results in the recent literature. These findings highlight TRT’s potential as a valuable method for spatiotemporal rainfall classification.},
  archive      = {J_PEERJCS},
  author       = {Kokten Ulas Birant and Bita Ghasemkhani and Özlem Varlıklar and Derya Birant},
  doi          = {10.7717/peerj-cs.3022},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3022},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A new machine learning method for rainfall classification: Temporal random tree},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fishing operation type recognition based on multi-branch convolutional neural network using trajectory data. <em>PEERJCS</em>, <em>11</em>, e3020. (<a href='https://doi.org/10.7717/peerj-cs.3020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of fishing vessel operations is vital for sustainable fishery management. Existing methods inadequately exploit spatiotemporal contextual information in vessel trajectories and fail to effectively fuse multimodal data. To address this, this study proposes a novel framework integrating Geohash-based geocoding with embedding techniques inspired by natural language processing to extract spatiotemporal features from trajectory sequences. We develop a multi-branch 1D convolutional neural network (MB-1dCNN) to minimize feature engineering dependency while enhancing operational-type recognition. Comparative experiments evaluate Geohash encoding lengths and network architectures (single-branch vs. multi-branch, fully-connected vs. 1D-CNN). Results indicate optimal Geohash encoding at length 5. The multi-branch structure significantly outperforms single-branch counterparts, and MB-1dCNN demonstrates superior performance over multi-branch model with fully connected layers (MB-FCNN), achieving additional gains in accuracy and F1-score. Key findings reveal: (1) 1D-CNN processing surpasses fully-connected networks in sequential feature extraction, (2) Multi-branch architectures enhance information fusion capabilities. The proposed MB-1dCNN establishes state-of-the-art performance for trajectory-based fishing operation recognition, offering valuable insights for spatial computing applications in maritime surveillance.},
  archive      = {J_PEERJCS},
  author       = {Bohui Jiang and Weifeng Zhou},
  doi          = {10.7717/peerj-cs.3020},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3020},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fishing operation type recognition based on multi-branch convolutional neural network using trajectory data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced clustering and transfer learning based approach for rice leaf disease segmentation and classification. <em>PEERJCS</em>, <em>11</em>, e3018. (<a href='https://doi.org/10.7717/peerj-cs.3018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rice, the world’s most important food crop, requires an early and accurate identification of the diseases that infect rice panicles and leaves to increase production and reduce losses. Most conventional methods of diagnosing diseases involve the use of manual instruments, which are ineffective, imprecise, and time-consuming. In light of such drawbacks, this article introduces an improved deep learning and transfer learning method for diagnosing and categorizing rice leaf diseases proficiently. First, all input images are preprocessed; the images are resized to a fixed size before applying a sophisticated contrast enhanced adaptive histogram equalization procedure. Diseased regions are then segmented through the developed gravity weighted kernelised density clustering algorithm. In terms of feature extraction, EfficientNetB0 is fine-tuned by subtracting the last fully connected layers, and the classification is conducted with the new fully connected layers. Also, the tent chaotic particle snow ablation optimizer is added into the learning process in order to improve the learning process and shorten the time of convergence. The performance of the proposed framework was tested on two benchmark datasets and presented accuracy results of 98.87% and 97.54%, respectively. Comparisons of the proposed method with six fine-tuned models show the performance advantage and validity of the proposed method.},
  archive      = {J_PEERJCS},
  author       = {Samia Nawaz Yousafzai and Fahd N. Al-Wesabi and Hadeel Alsolai and Shouki A. Ebad and Inzamam Mashood Nasir and Emad Fadhal and Adel Thaljaoui},
  doi          = {10.7717/peerj-cs.3018},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3018},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Advanced clustering and transfer learning based approach for rice leaf disease segmentation and classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing BERT-based models for arabic and low-resource languages in crime text classification. <em>PEERJCS</em>, <em>11</em>, e3017. (<a href='https://doi.org/10.7717/peerj-cs.3017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bidirectional encoder representations from Transformers (BERT) has recently attracted considerable attention from researchers and practitioners, demonstrating notable effectiveness in various natural language processing (NLP) tasks, including text classification. This efficacy can be attributed to its unique architectural features, particularly its ability to process text using both left and right context, having been pre-trained on extensive datasets. In the context of the criminal domain, the classification of data is a crucial activity, and Transformers are increasingly recognized for their potential to support law enforcement efforts. BERT has been released in English and Chinese, as well as a multilingual version that accommodates over 100 languages. However, there is a pressing need to analyze the availability and performance of BERT in Arabic and other low-resource languages. This study primarily focuses on analyzing BERT-based models tailored for the Arabic language; however, due to the limited number of existing studies in this area, the research extends to include other low-resource languages. The study evaluates these models’ performance in comparison to machine learning (ML), deep learning (DL), and other Transformer models. Furthermore, it assesses the availability of relevant data and examines the effectiveness of BERT-based models in low-resource linguistic contexts. The study concludes with recommendations for future research directions, supported by empirical statistical evidence.},
  archive      = {J_PEERJCS},
  author       = {Njood K. Al-harbi and Manal Alghieth},
  doi          = {10.7717/peerj-cs.3017},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3017},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Assessing BERT-based models for arabic and low-resource languages in crime text classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Further observations on the security of speck32-like ciphers using machine learning. <em>PEERJCS</em>, <em>11</em>, e3015. (<a href='https://doi.org/10.7717/peerj-cs.3015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of Internet of Things across various industries, the security of communications between different devices is one of the critical concerns to consider. The lightweight cryptography emerges as a specialized solution to address security requirements for resource-constrained environments. Consequently, the comprehensive security evaluation of the lightweight cryptographic primitives—from the structure of ciphers and cryptographic components—has become imperative. In this article, we focus on the security evaluation of rotation parameters in the Speck32-like lightweight cipher family. We establish a machine learning-driven security evaluation framework for the rotational parameter selection principles—the core of Speck32’s design architecture. To assess different parameters security, we develop neural-differential distinguishers with considering of two distinct input difference models: (1) the low-Hamming-weight input differences and (2) the input differences from optimal differential characteristics. Our methodology achieves the security evaluation of 256 rotation parameters using the accuracy of neural distinguishers as the evaluation criteria. Our results illustrate the parameter (7,3) has stronger ability to resist machine learning-aided distinguishing attack compared to the standard (7,2) configuration. To our knowledge, this represents the first comprehensive study applying machine learning techniques for security assessment of Speck32-like ciphers. Furthermore, we investigate the reason for the difference in the accuracy of neural distinguishers with different rotation parameters. Our experimental results demonstrate that the bit bias in output differences and truncated differences is the important factor affecting the accuracy of distinguishers.},
  archive      = {J_PEERJCS},
  author       = {Zezhou Hou and Jiongjiong Ren and Shaozhen Chen},
  doi          = {10.7717/peerj-cs.3015},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3015},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Further observations on the security of speck32-like ciphers using machine learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-phase deep learning framework for advanced phishing detection using the novel OptSHQCNN approach. <em>PEERJCS</em>, <em>11</em>, e3014. (<a href='https://doi.org/10.7717/peerj-cs.3014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Phishing attacks are now regarded as one of the most prevalent cyberattacks that often compromise the security of different communication and internet networks. Phishing websites are created with the goal of generating cyber threats in order to ascertain the user’s financial information. Fake websites are frequently created and circulated online, which results in the loss of essential user assets. Phishing websites can result in monetary loss, intellectual property theft, damage to one’s reputation, and disruption of regular business activities. Over the past decade, a number of anti-phishing tactics have been proposed to detect and reduce these attempts. They are still imprecise and ineffective, though. Deep Learning (DL), which can precisely learn the intrinsic features of the websites and recognize phishing websites, is one of the innovative techniques utilized to solve this issue. Methods In this study, we proposed a novel OptSHQCNN phishing detection method. Pre-deployment and post-deployment are the two phases of the proposed methodology. The dataset undergoes preprocessing in the pre-deployment phase, which includes data balancing, and handling invalid features, irrelevant features, and missing values. The convolutional block attention module (CBAM) then extracts the main characteristics from web page code and linkages. The red kite optimization algorithm (RKOA) selects the significant key attributes in the third stage. The final phase involves classifying the data using the Shallow hybrid quantum-classical convolutional neural network (SHQCNN) model. To improve the effectiveness of the classification approach, the hyperparameters present in the SHQCNN model are fine-tuned using the shuffled shepherd optimization algorithm (SSOA). Results In the post-deployment phase, the URL is encoded using Optimized Bidirectional Encoder Representations from Transformers (OptBERT), after which the features are extracted. The retrieved properties are fed into a trained classifier. Next, a prediction of “phishing” or “Legitimate” is produced by the classifier. With a maximum of above 99% accuracy, precision, recall, and F1-score, respectively, the investigation’s findings showed that the suggested technique performed better than other popular phishing detection methods. The creation of a security plugin for clients, browsers, and other instant messaging applications that operate on network edges, PCs, smartphones, and other personal terminals can be aided by these findings.},
  archive      = {J_PEERJCS},
  author       = {Srikanth Meda and Vangipuram Sesha Srinivas and Killi Chandra Bhushana Rao and Repudi Ramesh and Narasimha Rao Yamarthi},
  doi          = {10.7717/peerj-cs.3014},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3014},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A dual-phase deep learning framework for advanced phishing detection using the novel OptSHQCNN approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting sport event outcomes using deep learning. <em>PEERJCS</em>, <em>11</em>, e3011. (<a href='https://doi.org/10.7717/peerj-cs.3011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the outcomes of sports events is inherently difficult due to the unpredictable nature of gameplay and the complex interplay of numerous influencing factors. In this study, we present a deep learning framework that combines a one-dimensional convolutional neural network (1D CNN) with a Transformer architecture to improve prediction accuracy. The 1D CNN effectively captures local spatial patterns in structured match data, while the Transformer leverages self-attention mechanisms to model long-range dependencies. This hybrid design enables the model to uncover nuanced feature interactions critical to outcome prediction. We evaluate our approach on a benchmark sports dataset, where it outperforms traditional machine learning methods and standard deep learning models in both accuracy and robustness. Our results demonstrate the promise of integrating convolutional and attention-based mechanisms for enhanced performance in sports analytics and predictive modeling.},
  archive      = {J_PEERJCS},
  author       = {Jianxiong Gao and Yi Cheng and Jianwei Gao},
  doi          = {10.7717/peerj-cs.3011},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3011},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting sport event outcomes using deep learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A social recommendation model based on adaptive residual graph convolution networks. <em>PEERJCS</em>, <em>11</em>, e3010. (<a href='https://doi.org/10.7717/peerj-cs.3010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating social information in the recommendation algorithm based on graph neural network (GNN) alleviates the data sparsity and cold-start problems to a certain extent, and effectively improves the recommendation performance of the model. However, there are still shortcomings in the existing studies: on the one hand, the potential effect of noise in the raw data is ignored; on the other hand, only relying on the single interaction information between the user and the item and failing to make full use of the rich multi-aided information. These factors lead to an unsatisfactory learning effect of the model. To address the above problems, we propose a social recommendation model based on adaptive residual graph convolutional networks (SocialGCNRI). Specifically, we use the idea of fast Fourier transform (FFT), a filtering algorithm in the field of signal processing, to attenuate the raw data noise in the frequency domain, followed by utilizing the user-social relations, item-association relations, and user-item-interaction relations to form a heterogeneous graph to supplement the model information, and finally using a graph convolution algorithm with an adaptive residual graph to improve the expressive power of the model. Extensive experiments on two real datasets show that SocialGCNRI outperforms state-of-the-art social recommendation methods on a variety of common evaluation metrics.},
  archive      = {J_PEERJCS},
  author       = {Rui Chen and Kangning Pang and Qingfang Liu and Lei Zhang and Hao Wu and Cundong Tang and Pu Li},
  doi          = {10.7717/peerj-cs.3010},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3010},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A social recommendation model based on adaptive residual graph convolution networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting danceability and song ratings using deep learning and auditory features. <em>PEERJCS</em>, <em>11</em>, e3009. (<a href='https://doi.org/10.7717/peerj-cs.3009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting a song’s danceability and overall rating poses a significant challenge due to the complex interplay between musical characteristics and listener preferences. In this study, we propose a deep learning framework that jointly addresses the tasks of danceability estimation and popularity prediction. Our model integrates a Bidirectional Long Short-Term Memory (BiLSTM) network to capture sequential and contextual patterns from categorical inputs, alongside a Residual Network (ResNet) that extracts hierarchical representations from numerical auditory features. These complementary feature streams are fused using a cross-attention mechanism, enabling the model to effectively learn intricate relationships across heterogeneous data modalities. Experimental evaluations demonstrate that our approach consistently outperforms traditional machine learning baselines and recent deep learning models. The results demonstrate the effectiveness of cross-attention in structured music data modelling and highlight the framework’s potential in advancing music recommendation and audio analysis systems.},
  archive      = {J_PEERJCS},
  author       = {Wei Wu},
  doi          = {10.7717/peerj-cs.3009},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3009},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting danceability and song ratings using deep learning and auditory features},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-learning type-2 fuzzy systems with adaptive rule reduction for time series forecasting. <em>PEERJCS</em>, <em>11</em>, e3004. (<a href='https://doi.org/10.7717/peerj-cs.3004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In rapidly changing scenarios, uncertainty and chaotic oscillations often obstruct time series prediction. However, Type-1 fuzzy systems face challenges in handling high uncertainty levels, therefore, Type-2 fuzzy systems become a better solution. Nonetheless, the complexity of Type-2 fuzzy models can produce overwhelming rules, compromising interpretability and computational efficiency. We present a Self-Learning Type-2 Fuzzy System with adaptive rule reduction that optimizes the rule base as forecast accuracy begins to deteriorate after adaptation. Our model combines participatory learning (PL) and Kernel Recursive Least Squares (KRLS) for online learning, an Adaptive reduced rule strategy to eliminate repeating rules and gain computational efficiency. Our approach incorporates a compatibility measure rooted in Type-2 fuzzy sets, paving the way for an improved consideration of uncertainty. Complex datasets, including Mackey-Glass chaotic time series and Taiwan Capitalization Weighted Stock Index (TAIEX), are used to evaluate the model, which demonstrates its superior forecasting performance compared to state-of-the-art models. Experiments show that our solution, through the development of a few rules, obtains lower error measures maintaining a small rule base, thus proving to be a scalable approach amenable to on-line deployment in fast paced environments such as those appearing in the financial markets, industrial processes and others that demand highly accurate time series forecasts in the presence of uncertainty.},
  archive      = {J_PEERJCS},
  author       = {Abdulwhab Alkharashi and Gaganjot Kaur and Hadeel Alsolai and Hatim Dafaalla and Somia Asklany and Othman Alrusaini and Ali Alqazzaz and Menwa Alshammeri},
  doi          = {10.7717/peerj-cs.3004},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3004},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Self-learning type-2 fuzzy systems with adaptive rule reduction for time series forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving gaussian naive bayes classification on imbalanced data through coordinate-based minority feature mining. <em>PEERJCS</em>, <em>11</em>, e3003. (<a href='https://doi.org/10.7717/peerj-cs.3003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a widely used classification model, the Gaussian Naive Bayes (GNB) classifier experiences a significant decline in performance when handling imbalanced data. Most traditional approaches rely on sampling techniques; however, these methods alter the quantity and distribution of the original data and are prone to issues such as class overlap and overfitting, thus presenting clear limitations. This article proposes a coordinate transformation algorithm based on radial local relative density changes (RLDC). A key feature of this algorithm is that it preserves the original dataset’s quantity and distribution. Instead of modifying the data, it enhances classification performance by generating new features that more prominently represent minority classes. The algorithm transforms the dataset from absolute coordinates to RLDC-relative coordinates, revealing latent local relative density change features. Due to the imbalanced distribution, sparse feature space, and class overlap, minority class samples can exhibit distinct patterns in these transformed features. Based on these new features, the GNB classifier can increase the conditional probability of the minority class, thereby improving its classification performance on imbalanced datasets. To validate the effectiveness of the proposed algorithm, this study conducts comprehensive comparative experiments using the GNB classifier on 20 imbalanced datasets of varying scales, dimensions, and characteristics. The evaluation includes 10 oversampling algorithms, two undersampling algorithms, and two hybrid sampling algorithms. Experimental results show that the RLDC-based coordinate transformation algorithm ranks first in the average performance across three classification evaluation metrics. Compared to the average values of the comparison algorithms, it achieves improvements of 21.84%, 33.45%, and 54.63% across the three metrics, respectively. This algorithm offers a novel approach to addressing the imbalanced data problem in GNB classification and holds significant theoretical and practical value.},
  archive      = {J_PEERJCS},
  author       = {Wei Wang and Li Yan and Fen Liu and Yanxi Li},
  doi          = {10.7717/peerj-cs.3003},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3003},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving gaussian naive bayes classification on imbalanced data through coordinate-based minority feature mining},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid deep learning framework for skin disease localization and classification using wearable sensors. <em>PEERJCS</em>, <em>11</em>, e3002. (<a href='https://doi.org/10.7717/peerj-cs.3002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of skin diseases is essential for timely intervention and treatment. This article proposes a patch-based, interpretable deep learning framework for skin disease detection using wearable sensors and clinical data. Specifically, a fully convolutional residual neural network (FCRN) is employed to extract local features from high-resolution skin images captured via wearable sensors, using a patch-level training approach. Pre-processing techniques—including image resampling, intensity normalization, and noise reduction—standardize the input data to ensure consistency across sensor variations. To enhance local feature learning, the FCRN incorporates residual modules, which mitigate gradient vanishing and improve model performance. The framework generates disease probability maps that visualize regions of high diagnostic risk, providing interpretable insights into skin anomalies. In the proposed methodology, a convolutional neural network (CNN) integrates image-derived features with clinical data such as patient demographics, symptoms, and medical history. This CNN-based multimodal fusion approach improves the model’s ability to capture spatial relationships and enhances classification performance. Experimental evaluations demonstrate that the proposed framework achieves state-of-the-art results across multiple evaluation metrics, including accuracy, sensitivity, and specificity. The interpretable disease probability maps highlight affected skin regions, enhancing model transparency and clinical usability. This approach demonstrates the potential of combining wearable sensor technology with deep learning for efficient, scalable, and explainable skin disease detection, laying the foundation for real-time clinical applications.},
  archive      = {J_PEERJCS},
  author       = {Xiaoling Zhao and Huixin Zhang and Qian Zheng and Caihong Jing},
  doi          = {10.7717/peerj-cs.3002},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3002},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid deep learning framework for skin disease localization and classification using wearable sensors},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning in time series forecasting with transformer models and RNNs. <em>PEERJCS</em>, <em>11</em>, e3001. (<a href='https://doi.org/10.7717/peerj-cs.3001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the increasing need for accurate weather forecasts, the use of neural networks, especially transformer and recurrent neural networks (RNNs), has been highlighted for their ability to capture complex patterns in time series. This study examined 14 neural network models applied to forecast weather variables, evaluated using metrics such as median absolute error (MedianAbsE), mean absolute error (MeanAbsE), maximum absolute error (MaxAbsE), root mean squared percent error (RMSPE), and root mean square error (RMSE). Transformer-based models such as Informer, iTransformer, Former, and patch time series transformer (PatchTST) stood out for their accuracy in capturing long-term patterns, with Informer showing the best performance. In contrast, RNN models such as auto-temporal convolutional networks (TCN) and bidirectional TCN (BiTCN) were better suited to short-term forecasting, despite being more prone to significant errors. Using iTransformer it was possible to achieve a MedianAbsE of 1.21, MeanAbsE of 1.24, MaxAbsE of 2.86, RMSPE de 0.66, and RMSE de 1.43. This study demonstrates the potential of neural networks, especially transformers, to improve accuracy, providing a practical and theoretical basis for selecting the most suitable models for predictive applications.},
  archive      = {J_PEERJCS},
  author       = {Rogerio Pereira dos Santos and João P. Matos-Carvalho and Valderi R. Q. Leithardt},
  doi          = {10.7717/peerj-cs.3001},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3001},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning in time series forecasting with transformer models and RNNs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective classification for neonatal brain injury using EEG feature selection based on elastic net regression and improved crow search algorithm. <em>PEERJCS</em>, <em>11</em>, e3000. (<a href='https://doi.org/10.7717/peerj-cs.3000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neonatal brain injury carries the risk of neurological sequelae such as epileptic seizures, cerebral palsy, intellectual disability, and even death. Classification methods based on electroencephalography (EEG) signals and machine learning algorithms are crucial for assessing neonatal brain injury. However, classification methods that utilise all features from the original EEG signals may result in lengthy training and classification times, thereby reducing the performance of the classification system. This article presents a novel classification system with a proposed feature selection method for assessing neonatal brain injury, in which the feature selection method is combined using elastic net (EN) regression and an improved crow search algorithm (ICSA), named EN-ICSA. In the EN-ICSA method, EN regression is used to conduct the pre-screening of features. The ICSA is utilised to select the essential figures further by introducing the dynamic perception probability for deciding whether to search locally or globally, a novel neighbor-following strategy for the local search and a global search strategy according to the crow’s search experience, resulting in accelerating the search efficiency while effectively avoiding falling into local optima. Experimental results demonstrate that the proposed system, based on support vector machine (SVM) with the EN-ICSA feature selection method, performs exceptionally well compared to other traditional machine learning and feature selection methods, achieving an accuracy of 91.94%, precision of 92.32%, recall of 89.85%, and F1-score of 90.82%.},
  archive      = {J_PEERJCS},
  author       = {Ling Li and Tao Yue and Hui Wu and Yanping Zhao and Qinmei Liu and Hairong Zhang and Wei Xu},
  doi          = {10.7717/peerj-cs.3000},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3000},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Effective classification for neonatal brain injury using EEG feature selection based on elastic net regression and improved crow search algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of predictive maintenance efficiency with the comparison of machine learning models in machining production process in brake industry. <em>PEERJCS</em>, <em>11</em>, e2999. (<a href='https://doi.org/10.7717/peerj-cs.2999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The utilization of technologies such as artificial intelligence (AI) and machine learning (ML) in industrial sectors has become a crucial requirement to enhance the efficiency and stability of production processes. Regular maintenance of machines and early detection of faults play a critical role in ensuring uninterrupted production and business continuity. Predictive maintenance practices, combined with sensors and data analysis methods, enable the collection, analysis, and transformation of machine-related data into meaningful insights. As a result, the anticipation of potential machine failures, the execution of planned maintenance activities, and the prevention of unexpected downtime become possible. These methods not only improve productivity in production processes but also contribute to reducing maintenance costs. Methods This study aims to predict machine faults using data analysis methods and enhance the accuracy performance of these predictions for an industrial company that produces braking components. Comprehensive examination and analysis of data were conducted to understand the symptoms and relationships of machine failures. ML classification methods were employed in the relevant study. Results Challenges such as the imbalance of class distributions in the dataset, the presence of missing and outlier values, and the high costs of necessary equipment and training pose significant barriers to implementation. Addressing these issues is critical for achieving effective predictive maintenance solutions. In order to achieve more accurate results, data splitting and k-fold cross-validation methods were applied during the learning and testing phases to overcome the imbalance problem in the dataset, undersampling techniques were applied, and outlier detection and normalization processes were used to improve data quality. The model performances, evaluated through accuracy, precision, recall, and F1-score, area under the curve (AUC), Cohen’s Matthew’s correlation coefficient (MCC) were compared. Hyperparameter optimization was also performed, resulting in significant improvements in model performance. This study contributes to the literature in terms of predictive maintenance application, classification, and data partitioning techniques. The findings highlight the importance of data preprocessing and advanced modeling techniques in predictive maintenance and emphasize how addressing data challenges can enhance the overall performance and reliability of ML models.},
  archive      = {J_PEERJCS},
  author       = {Can Aydın and Burak Evrentuğ},
  doi          = {10.7717/peerj-cs.2999},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2999},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evaluation of predictive maintenance efficiency with the comparison of machine learning models in machining production process in brake industry},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracing truth: Dynamic temporal networks for multi-modal fake news detection. <em>PEERJCS</em>, <em>11</em>, e2998. (<a href='https://doi.org/10.7717/peerj-cs.2998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the internet continues to evolve rapidly and social media becomes increasingly prevalent, the ways people access information has become increasingly diverse. However, the proliferation of fake news has emerged as a critical problem, presenting major challenges to the integrity of the information ecosystem. To address the complex propagation mechanisms of fake news, existing studies leverage multi-modal information and dynamic propagation social graphs for effective detection. Nonetheless, capturing the temporal relationships of propagation nodes in dynamic social networks accurately and dynamically integrating multi-modal information for improved detection accuracy remains a technical challenge. In response, This study proposes a multimodal approach to fake news detection—the dynamic temporal network (DTN) model. Firstly, this model designs a time similarity strength metric to measure the temporal similarity among nodes in propagation sequences and introduces a weighting mechanism to dynamically fuse multi-modal information. Secondly, it constructs a social propagation graph model, enhancing node representation through the dynamic variations of time similarity and graph structure, and utilizes the Transformer encoder to extract the overall semantic features of news propagation. Furthermore, the model views the news propagation process as a complex system, analyzing the temporal dynamics of news in real social networks, effectively revealing the abnormal propagation patterns of fake news. Further analysis demonstrates that the proposed DTN model exhibits high accuracy and effectiveness in multi-modal fake news detection.},
  archive      = {J_PEERJCS},
  author       = {Jiaen Hu and Juan Zhang and Zichen Li},
  doi          = {10.7717/peerj-cs.2998},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2998},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tracing truth: Dynamic temporal networks for multi-modal fake news detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA-FIS: A high-speed dynamic adaptive fault injection server framework for reliable FPGA-based embedded systems. <em>PEERJCS</em>, <em>11</em>, e2996. (<a href='https://doi.org/10.7717/peerj-cs.2996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault injection is a critical technique for assessing the reliability of field programmable gate array (FPGA)-based embedded systems, particularly in radiation-prone and safety-critical applications. Conventional fault injection methods, such as bit upset fault injection testing (BUFIT), single critical fault injection testing (SCFIT), and dynamic partial reconfiguration (DPR), suffer from high resource overhead, slow injection speeds, and limited adaptability, making them inadequate for real-time fault resilience evaluation. This article introduces the dynamic adaptive fault injection server (DA-FIS), a high-speed, scalable, and resource-efficient fault injection framework designed to overcome these limitations. Unlike traditional methods, DA-FIS employs a configurable LFSR-based fault generator that enables adaptive and real-time fault injection based on workload sensitivity and system conditions. The proposed framework integrates masking logic and dynamic propagation tracking, allowing precise injection of single-event upsets (SEUs) and multiple-bit upsets (MBUs) into FPGA configuration memory and logic without disturbing non-targeted regions. DA-FIS is implemented on the Xilinx Zynq-7000 FPGA and evaluated across multiple benchmark workloads, including the Bubble Sort algorithm, 4-bit adder, 4-bit multiplier, and counter-based logic circuits. Experimental results demonstrate that DA-FIS achieves a fault injection rate of 111.1 faults per second, outperforming BUFIT (53.4 faults/s), SCFIT (27 faults/s), and DPR (18.5 faults/s), with 30% lower FPGA resource overhead compared to SCFIT. The adaptive architecture ensures seamless scalability across different FPGA platforms, making it suitable for space electronics, automotive safety systems, and high-performance computing. Additionally, DA-FIS supports real-time error model adjustments, enabling researchers to analyze fault propagation, error correction strategies, and security vulnerabilities in FPGA-based architectures. This work establishes DA-FIS as a superior fault injection framework, offering high-speed, precision-controlled fault testing while maintaining minimal FPGA overhead and enhanced scalability. Future research will explore machine learning-assisted fault modeling and self-healing FPGA architectures to further enhance FPGA fault resilience in safety-critical and autonomous systems.},
  archive      = {J_PEERJCS},
  author       = {Fatimah Alhayan and Gaganjot Kaur and Sultan Alanazi and Mohammed Burhanur Rehman and Wahida Mansouri and Da’ad Albalawneh and Ali Alqazzaz and Hanadi Alkhudhayr},
  doi          = {10.7717/peerj-cs.2996},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2996},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DA-FIS: A high-speed dynamic adaptive fault injection server framework for reliable FPGA-based embedded systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formal modeling of a causal consistent distributed system and verification of its history via model checking using colored petri net. <em>PEERJCS</em>, <em>11</em>, e2995. (<a href='https://doi.org/10.7717/peerj-cs.2995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various consistency models for replicated distributed systems (DSs) have been developed and are usually implemented in the middleware layer. Causal consistency (CC) is a widely used consistency model appropriate for distributed applications like discussion groups and forums. One of the known distributed algorithms for CC is based on logical time synchronization with Fidge vector clocks that use the concepts of the hold-back and delivery queues for each replica. The basics of the algorithm and its assumptions are presented in the article. Then, a novel formal hierarchical colored Petri net model of a DS with CC support and three constituting replicas is presented. The proposed model operates based on the presented distributed algorithm for CC support with potential randomness for delays in message delivery. The article tries to answer the question: is a given distributed history (DH) a valid image of a causal-consistent distributed system (CCDS)? The proposed model validates a DH via model checking. The question is answered by the execution of the proposed model and the generation of its state space graph (SSG). Required model checking functions are developed for automatically analyzing SSG for (1) extracting the existence of the answer and (2) extraction of the shortest proof scenarios that can generate the given input DH. The model was used to analyze four case study examples. The article presents three effective techniques for decreasing the state space explosion problem. Results show that the colored Petri net model of a CCDS can automatically validate a DH using model checking.},
  archive      = {J_PEERJCS},
  author       = {Khalid Amjed Mohammed Alsaegg and Saeid Pashazadeh and Mina Zolfy Lighvan},
  doi          = {10.7717/peerj-cs.2995},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2995},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Formal modeling of a causal consistent distributed system and verification of its history via model checking using colored petri net},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early breast cancer detection in CT scans using convolutional neural bidirectional feature pyramid network. <em>PEERJCS</em>, <em>11</em>, e2994. (<a href='https://doi.org/10.7717/peerj-cs.2994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the leading causes of death among women worldwide. Early detection plays a crucial role in reducing mortality rates. While mammography is a widely used diagnostic tool, computed tomography (CT) scans are increasingly being explored for detecting breast cancer due to their high-resolution imaging and ability to visualize tissue in 3D. Despite the potential of CT scans in visualizing breast tissue in 3D with high resolution, extracting meaningful patterns from these scans is difficult due to the complex and nonlinear nature of the tissue features. The challenge lies in developing computational methods that can accurately detect and localize breast cancer lesions, especially when the tumors vary in size, shape, and density. In this article, we proposed a framework called convolutional neural bidirectional feature pyramid network, which integrates multi-scale feature extraction and bidirectional feature fusion for breast cancer detection in CT scans. The proposed framework classified the images into diseased and non-diseased and then identified the infected region on breast tissue. Using convolutional neural networks, we defined several layers to classify the diseased and normal CT scan images. We collected data on breast CT scans taken from the radiology department, Ayub Teaching Hospital Abbottabad, Pakistan. We evaluated the model using a variety of classification metrics such as precision, recall, F1-measure, and average precision to determine its effectiveness in finding breast cancer lesions, and we found 96.11% accuracy. Our findings show that compared with current state-of-the-art methods, the proposed framework has satisfactory results in identifying breast cancer areas, and the proposed framework over the baselines has achieved a 1.71% improvement.},
  archive      = {J_PEERJCS},
  author       = {Tahani Jaser Alahmadi and Adeel Ahmed and Amjad Rehman and Abeer Rashad Mirdad and Bayan Al Ghofaily and Shehryar Ali},
  doi          = {10.7717/peerj-cs.2994},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2994},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Early breast cancer detection in CT scans using convolutional neural bidirectional feature pyramid network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foundational models and federated learning: Survey, taxonomy, challenges and practical insights. <em>PEERJCS</em>, <em>11</em>, e2993. (<a href='https://doi.org/10.7717/peerj-cs.2993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning has the potential to unlock siloed data and distributed resources by enabling collaborative model training without sharing private data. As more complex foundational models gain widespread use, the need to expand training resources and integrate privately owned data grows as well. In this article, we explore the intersection of federated learning and foundational models, aiming to identify, categorize, and characterize technical methods that integrate the two paradigms. As a unified survey is currently unavailable, we present a literature survey structured around a novel taxonomy that follows the development life-cycle stages, along with a technical comparison of available methods. Additionally, we provide practical insights and guidelines for implementing and evolving these methods, with a specific focus on the healthcare domain as a case study, where the potential impact of federated learning and foundational models is considered significant. Our survey covers multiple intersecting topics, including but not limited to federated learning, self-supervised learning, fine-tuning, distillation, and transfer learning. Initially, we retrieved and reviewed a set of over 4,200 articles. This collection was narrowed to more than 250 thoroughly reviewed articles through inclusion criteria, featuring 42 unique methods. The methods were used to construct the taxonomy and enabled their comparison based on complexity, efficiency, and scalability. We present these results as a self-contained overview that not only summarizes the state of the field but also provides insights into the practical aspects of adopting, evolving, and integrating foundational models with federated learning.},
  archive      = {J_PEERJCS},
  author       = {Cosmin-Andrei Hatfaludi and Alex Serban},
  doi          = {10.7717/peerj-cs.2993},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2993},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Foundational models and federated learning: Survey, taxonomy, challenges and practical insights},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HTCNN-attn: A fine-grained hierarchical multi-label deep learning model for disaster emergency information intelligent extraction from social media. <em>PEERJCS</em>, <em>11</em>, e2992. (<a href='https://doi.org/10.7717/peerj-cs.2992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenge of extracting fine-grained emergency information from noisy social media during disasters, we propose HTCNN-Attn, a hierarchical multi-label deep learning model. It integrates a three-level tree-structured labeling architecture, Transformer-based global feature extraction, convolutional neural network (CNN) layers for local pattern capture, and a hierarchical attention mechanism. The model employs a hierarchical loss function to enforce label consistency across three levels: binary disaster filtering (Level 1), mid-level category classification (Level 2), and fine-grained subcategory extraction (Level 3). Experiments on the Appen and HumAID datasets demonstrate superior performance, achieving an accuracy of 0.9725, a Micro-F1 of 0.7402, and a hierarchical consistency (HC) score of 0.821, outperforming state-of-the-art baselines. Ablation experiments validate the importance of hierarchical modeling, Transformer encoding, CNN layers, pre-trained embeddings, and the hierarchical attention mechanism. Cross-event generalization tests on the CrisisBench dataset demonstrate robust generalization (HC = 0.678), while its lightweight design enables efficient real-time deployment (12.0 ms latency). A case study of the 2015 Nepal earthquake validates its practical utility, where the model accurately classified tweets into hierarchical labels and routed structured information to support emergency response coordination. This demonstrates the effectiveness of the proposed model in supporting rapid, efficient, and fine-grained emergency response after disasters, thereby enhancing disaster response capabilities.},
  archive      = {J_PEERJCS},
  author       = {Shanshan Li and Qingjie Liu and Xiaoling Sun},
  doi          = {10.7717/peerj-cs.2992},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2992},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HTCNN-attn: A fine-grained hierarchical multi-label deep learning model for disaster emergency information intelligent extraction from social media},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on license plate recognition based on graphically supervised signal-assisted training. <em>PEERJCS</em>, <em>11</em>, e2989. (<a href='https://doi.org/10.7717/peerj-cs.2989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background With the rapid growth of the number of cars and the increasing complexity of urban transportation, it is particularly important to achieve high-accuracy license plate recognition in complex scenarios. However, since license plate recognition models are mostly deployed on embedded devices with limited computational resources, designing a lightweight and accurate model has become an urgent problem in the field of license plate recognition. Methods This study proposes an improved license plate recognition algorithm. We use the License Plate Recognition Network (LPRNet) as the base model. To enhance its accuracy, we incorporate graphically supervised signals for assisted training. This approach refines the training process, yielding a model that is both lightweight and highly accurate. An auxiliary training branch is added, utilizing these graphical signals to guide the model in learning improved image features. Results Experiments show that compared with LPRNet, this study improves the accuracy in all test sets of the Chinese City Parking Dataset (CCPD) dataset, where the average accuracy is improved by 5.86%, the maximum accuracy by 10.9%, the average character precision by 2.1%, and the average recall by 6.9%, indicating that this study can achieve higher accuracy while keeping it lightweight. This study also provides new ideas for other deep learning image recognition tasks.},
  archive      = {J_PEERJCS},
  author       = {Dianwei Chi and Zehao Jia and Lizhen Liu},
  doi          = {10.7717/peerj-cs.2989},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2989},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on license plate recognition based on graphically supervised signal-assisted training},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TaGra: An open python package for easily generating graphs from data tables through manifold learning. <em>PEERJCS</em>, <em>11</em>, e2986. (<a href='https://doi.org/10.7717/peerj-cs.2986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of analyzing high-dimensional data affects many scientific disciplines, from pharmacology to chemistry and biology. Traditional dimensionality reduction methods often oversimplify data, making it difficult to interpret individual points. This distortion can complicate the visualization of mutual distances between data points in the reduced space. Graphs provide an effective framework for representing objects and their relationships. One of their possible use is visualizing similarity patterns in tabular datasets. Here we introduce TaGra, an off-the-shelf package designed to generate a graph of similarity relations from tabular data. TaGra enables the visualization of datasets in 2D space, identification of typical data points and outliers, and assessment of the separation between items with different target variables. We describe TaGra’s functionality, options and setup. The software including examples, instructions and a guide, is openly available on PyPI at https://pypi.org/project/TaGra/ and on GitHub at https://github.com/davidetorre92/TaGra.},
  archive      = {J_PEERJCS},
  author       = {Davide Torre and Davide Chicco},
  doi          = {10.7717/peerj-cs.2986},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2986},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TaGra: An open python package for easily generating graphs from data tables through manifold learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DLProv: A suite of provenance services for deep learning workflow analyses. <em>PEERJCS</em>, <em>11</em>, e2985. (<a href='https://doi.org/10.7717/peerj-cs.2985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) workflows consist of multiple interdependent and repetitive steps, including data preparation, model training, evaluation, and deployment. Each step involves decisions impacting the final model’s performance, interpretability, and applicability. These models rely on data, preprocessing operations, and configuration, underscoring the need for mechanisms to ease the analysis throughout the entire life cycle—from model generation and selection to deployment. Moreover, ensuring trust, reproducibility, and transparency becomes important as DL models transition into production environments. Traceability across the steps of the DL workflow is essential to address these challenges. However, existing traceability solutions often present limitations. Many fail to integrate the steps of the DL workflow, focusing on either data preparation or model training. Additionally, they frequently rely on proprietary formats to represent traceability data and rarely produce a provenance document that can accompany the model into production. To bridge these gaps, we present DLProv, a suite of provenance services designed to ensure end-to-end traceability across DL workflows. DLProv supports structured query language (SQL)-based querying during training and generates provenance graphs that capture data preparation steps, model training, and evaluation. These provenance graphs comply with the PROV de facto standard, ensuring interoperability across different environments. One of the key strengths of DLProv lies in its framework-agnostic architecture. The suite’s services can be invoked independently of the DL framework, enabling integration across several training and deployment workflows. Furthermore, DLProv includes specialized instances designed for specific DL frameworks, such as Keras and physics-informed neural networks (PINNs), offering adaptability to a wide range of applications. We evaluated DLProv using well-established datasets, including Modified National Institute of Standards and Technology (MNIST) and Canadian Institute for Advanced Research (CIFAR)-100. These datasets were chosen to illustrate the suite’s capability to capture and manage provenance data across tasks of varying complexity, from basic image classification to more complex DL workflows. Additionally, we evaluated DLProv within a handwritten transcription workflow, further showcasing its flexibility. Across all these use cases, DLProv showed its ability to ease SQL-based queries during model training while maintaining framework independence. An important aspect of our evaluation was measuring the overhead introduced by integrating DLProv into DL workflows. The results showed a maximum overhead of 1.4% in execution time, highlighting the suite’s minimal impact on DL workflow performance. For comparative analysis, we benchmarked this overhead against MLflow, further reinforcing DLProv’s suitability for real-world DL applications.},
  archive      = {J_PEERJCS},
  author       = {Débora Pina and Liliane Kunstmann and Adriane Chapman and Daniel de Oliveira and Marta Mattoso},
  doi          = {10.7717/peerj-cs.2985},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2985},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DLProv: A suite of provenance services for deep learning workflow analyses},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-based enhanced boosting algorithm for depression detection. <em>PEERJCS</em>, <em>11</em>, e2981. (<a href='https://doi.org/10.7717/peerj-cs.2981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a rapidly increasing mental disorder that can interfere with a person’s ability and negatively affect functions in various aspects of life. Fortunately, machine learning and deep learning techniques have demonstrated excellent results in the early detection of depression using social media data. Most recently, researchers have utilized boosting algorithms including pre-defined boosting algorithms or built their own boosting algorithm for the detection of depression. However, both types of boosting algorithms struggle with the analysis of complex feature sets, the enhancement of weak learners, and the handling of larger datasets. Thus, this study has developed a novel feature-based enhanced boosting algorithm (F-EBA). The proposed model covers two pipelines, the feature engineering pipeline which improves the quality of features by picking up the most relevant features while the classification pipeline uses an ensemble approach designed to boost/elevate the model’s performances. The experimental results highlighted that various parameter including WordVec and BERT embeddings, attention mechanisms, and feature elimination techniques, significantly contributed to the selection of the most relevant features. This approach resulted in generating an optimized feature set that augmented both the model’s accuracy and its interpretability. In addition, utilizing over 46 million records, the F-EBA model significantly enhanced the performance of weak learners through a weight maximization strategy, achieving an impressive accuracy rate of 95%. Moreover, the integration of an adversarial layer that employs defense mechanisms against synonymous text and sarcastic phrases within the datasets has further boosted the F-EBA model’s accuracy to approximately 97%, surpassing the results reported in prior studies. Moreover, the optimized feature sets derived from the F-EBA model make a substantial contribution to boosting the performance of baseline classifiers, marking a novel advancement in the field.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Sadiq Rohei and Kasturi Dewi Varathan and Shivakumara Palaiahnakote and Nor Badrul Anuar},
  doi          = {10.7717/peerj-cs.2981},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2981},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Feature-based enhanced boosting algorithm for depression detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data driven healthcare insurance system using machine learning and blockchain technologies. <em>PEERJCS</em>, <em>11</em>, e2980. (<a href='https://doi.org/10.7717/peerj-cs.2980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare recommendations and insurance have recently been one of the most emerging research areas in health informatics. The fraud in health insurance is becoming increasingly common day by day. To handle healthcare insurance fraud, there is an urgent need for an intelligent system that cannot only identify and monitor doctors’ and hospitals’ behavior regarding the health services they provide to patients but can also recommend doctors and hospitals to insured employees based on the quality of services they provided previously. This system creates patient and doctor profiles separately, based on their rating. The proposed system combines singular value decomposition (SVD), K-nearest neighbors based collaborative filtering (KNN-based CF), item-based collaborative filtering (Item-based CF), content-based filtering using term frequency-inverse document frequency (TF-IDF), and K-means clustering and probability distributions to recommend doctors and insurance plans. The system measures similarity scores between patients and doctors using cosine similarity, which helps to determine similarity scores and refine the recommendations. This study also uses blockchain technology to automate insurance claims reimbursement. The results are validated using real data from the employees of a local hospital. The system provides recommendations with a root mean square error (RMSE) value of 0.478 and a mean absolute error (MAE) value of 0.0422. The insurance plans developed using the proposed system have reduced the overall expenditure of the local hospital, with a reduction in total expenses. Blockchain technology further helps prevent healthcare fraud. In the proposed system, a healthcare insurance claims reimbursement system is built using smart contract technology on the Ethereum blockchain, ensuring security & transparency and lowering the number of healthcare frauds. The system includes roles for the insurance company, healthcare provider, and patients. It also provides a platform for claim submission, approval, or refusal. In Pakistan, no such system existed before recommending doctors from different hospitals based on their professional conduct or the good health services they provide.},
  archive      = {J_PEERJCS},
  author       = {Irum Matloob and Shoab Khan and Bushra Bashir and Rukaiya Rukaiya and Javed Ali Khan and Hessa Alfraihi},
  doi          = {10.7717/peerj-cs.2980},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2980},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data driven healthcare insurance system using machine learning and blockchain technologies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating, retrieving persona and generating responses for long-term open-domain dialogue. <em>PEERJCS</em>, <em>11</em>, e2979. (<a href='https://doi.org/10.7717/peerj-cs.2979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain dialogue systems have shown remarkable capabilities in generating natural and consistent responses in short-term conversations. However, in long-term conversations such as multi-session chat (MSC), where the dialogue history exceeds the model’s maximum input length (i.e., 1024 tokens), existing dialogue generation systems often overlook the information from earlier dialogues, leading to the loss of context. To prevent such loss and generate natural, consistent responses, we propose a GRGPerDialogue framework, consisting of three main stages: generating persona from past dialogues, retrieving persona relevant to the current utterance, and generating responses based on both persona and recent dialogues. In the first stage, we generate the persona of each speaker in real-time with diverse expressions, leveraging Llama 2 In-Context Learning (ICL). Subsequently, we propose a new dataset called Persona-Utterance Pair (PUP) and use it to train Facebook dense passage retrieval (DPR) model for retrieving persona sentences relevant to the current utterance. Finally, we train generative models such as Generative Pre-trained Transformer 2 (GPT-2) and Bidirectional and Auto-Regressive Transformers (BART) to generate responses based on retrieved persona sentences and the recent dialogues. Experimental results on a long-term dialogue dataset demonstrate that the GRGPerDialogue framework outperforms baseline models by approximately 0.6% to 1% in terms of the Rouge-1 metric. Furthermore, human evaluation results supported the effectiveness of GRGPerDialogue. These results indicate that GRGPerDialogue can generate responses that are not only more fluent and consistent, but also more relevant to the dialogue history than baseline models.},
  archive      = {J_PEERJCS},
  author       = {Dohyun Cha and Dawon Lee and Jihie Kim},
  doi          = {10.7717/peerj-cs.2979},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2979},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Generating, retrieving persona and generating responses for long-term open-domain dialogue},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classifying reservoir facies using attention-based residual neural networks. <em>PEERJCS</em>, <em>11</em>, e2977. (<a href='https://doi.org/10.7717/peerj-cs.2977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate classification of reservoir facies remains a fundamental challenge in petroleum geoscience, with significant implications for resource extraction efficiency and reservoir characterization. Traditional approaches relying on manual interpretation and conventional machine learning methods often struggle with the complexity and heterogeneity of well-log data. This architectural approach, in contrast to traditional single-stream or non-residual designs, significantly enhances the model’s ability to concentrate on key geological features while preserving hierarchical representations of the data. Consequently, it more effectively addresses data heterogeneity and contextual dependencies. The framework was trained and evaluated using measurements from eight wells that represent diverse geological settings. Comparative experiments against conventional machine learning models and state-of-the-art deep learning techniques demonstrated the superiority of our method, achieving an area under the receiver operating characteristic curve (AUROC) of 0.883 and an area under the precision-recall curve (AUPRC) of 0.502. These enhancements enable more accurate identification of subtle facies boundaries and lithological variations, particularly in complex geological formations, thereby facilitating improved reservoir delineation and reducing uncertainty in field development planning. Furthermore, reproducibility analyses confirmed consistent performance across independent trials, underscoring the model’s robustness and its viability for real-world reservoir characterization workflows.},
  archive      = {J_PEERJCS},
  author       = {An Hai Nguyen and Khang Nguyen and Nga Mai},
  doi          = {10.7717/peerj-cs.2977},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2977},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classifying reservoir facies using attention-based residual neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Web application firewall based on machine learning models. <em>PEERJCS</em>, <em>11</em>, e2975. (<a href='https://doi.org/10.7717/peerj-cs.2975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing reliance on web applications for storing sensitive data and financial transactions has elevated the importance of web application security. A machine learning-based web application firewall was designed to protect web applications against injection vulnerabilities. A hybrid dataset, including CISC 2010, HTTPParams 2015, and real-time Hypertext Transfer Protocol (HTTP) requests, was employed. The study evaluated five classification algorithms—K-nearest neighbors, logistic regression, naïve Bayes, support vector machine, and decision tree—for detecting cross site scripting (XSS), Structured Query Language (SQL) Injection, Operating System Command Injection, and Local File Inclusion attacks. Decision tree was identified as the algorithm with the highest precision, accuracy, recall, F1-score, receiver operating characteristic (ROC), and area under the curve (AUC) values. According to the confusion matrix analysis, the real-time tested web application firewalls (WAF) achieved a remarkably high F1 score of 93.13% and accuracy of 93.27%. The findings indicate that machine learning-based WAFs effectively protect web applications against injection threats. Future work includes expanding the WAF to cover other attack types and testing it on different datasets.},
  archive      = {J_PEERJCS},
  author       = {Muhammed Ersin Durmuşkaya and Selim Bayraklı},
  doi          = {10.7717/peerj-cs.2975},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2975},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Web application firewall based on machine learning models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of efficient generalized digital fractional order differentiators using an improved whale optimization algorithm. <em>PEERJCS</em>, <em>11</em>, e2971. (<a href='https://doi.org/10.7717/peerj-cs.2971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new design and realization method for generalized digital fractional-order differentiator (GFOD) based on a composite structure of infinite impulse response (IIR) subfilters. The proposed method utilizes an improved whale optimization algorithm (IWOA) to compute the optimal coefficients of IIR subfilters of the realization structure. IWOA is developed by incorporating a piecewise linear chaotic mapping (PWLCM) and an adaptive inertia weight based on the hyperbolic tangent function (AIWHT) into the framework of original whale optimization algorithm (WOA). Simulation experiments are conducted to compare the performance of our method with that of well-known techniques, real-coded genetic algorithm (RCGA), particle swarm optimization (PSO), and original WOA. The results show that the new metaheuristic is superior to the other metaheuristics in terms of attaining the most accurate GFOD approximation. Moreover, the proposed IIR-based GFOD is compared with state-of-the-art GFOD, and observed to save about 50% of implementation complexity. Therefore, our method can be utilized in real-world digital signal processing applications.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Ali Mohammed Moqbel and Talal Ahmed Ali Ali and Zhu Xiao and Amani Ali Ahmed Ali},
  doi          = {10.7717/peerj-cs.2971},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2971},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of efficient generalized digital fractional order differentiators using an improved whale optimization algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of EfficientNet CNN models for multi-label chest X-ray disease diagnosis. <em>PEERJCS</em>, <em>11</em>, e2968. (<a href='https://doi.org/10.7717/peerj-cs.2968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of chest X-ray images, which are critical for the early diagnosis of many diseases, is a difficult and time-consuming process due to the multiple labeling requirements and similar looking pathologies. In traditional methods, expert physicians analyze high-resolution chest X-ray images to diagnose these diseases using observational methods, a process that can lead to human error and hence misdiagnosis or underdiagnosis. In this study, we aim to autonomously detect 14 different diseases that significantly affect human health and some cases even lead to death using chest X-ray images in a multi-class manner using deep learning techniques. Previous studies on chest X-ray images focus on a single disease or have low success rates, and the architectures presented in previous studies have high computational costs. The novelty of this work is that it presents a hybrid lightweight, fast and attention-based architecture with high classification performance. In this study, we used the ChestX-Ray14 dataset consisting of 112,104 labeled chest X-ray images of 14 disease classes. Eight deep learning architectures (EfficientNetB0-B7) and coordinate attention mechanism are used in the training and testing processes. The proposed EfficientNetB7 architecture achieved an average overall classification performance with an AUC value of 0.8265. The EfficientNet enhanced with coordinate attention architecture achieved a classification success with an AUC value of 0.8309. Moreover, when the proposed architecture and the individual disease classes are considered separately, higher classification success is achieved for eight of the 14 diseases in the dataset. Finally, the results of this study outperformed the classification performance of other similar studies in the literature in terms of AUC score. The results obtained in our study show that the proposed deep learning based lightweight and fast architecture can support radiologists in decision making in disease diagnosis. The use of autonomous disease diagnosis systems can support the protection of human health by preventing incomplete or erroneous diagnoses.},
  archive      = {J_PEERJCS},
  author       = {Murat Ucan and Buket Kaya and Osman Aygun and Mehmet Kaya and Reda Alhajj},
  doi          = {10.7717/peerj-cs.2968},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2968},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparison of EfficientNet CNN models for multi-label chest X-ray disease diagnosis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Specx: A c++ task-based runtime system for heterogeneous distributed architectures. <em>PEERJCS</em>, <em>11</em>, e2966. (<a href='https://doi.org/10.7717/peerj-cs.2966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallelization is needed everywhere, from laptops and mobile phones to supercomputers. Among parallel programming models, task-based programming has demonstrated a powerful potential and is widely used in high-performance scientific computing. Not only does it allow efficient parallelization across distributed heterogeneous computing nodes, but it also allows for elegant source code structuring by describing hardware-independent algorithms. In this article, we present Specx, a task-based runtime system written in modern C++. Specx supports distributed heterogeneous computing by simultaneously exploiting central processing units (CPUs) and graphics processing units (GPUs) (CUDA/HIP) and incorporating communication into the task graph. We describe the specificities of Specx and demonstrate its potential by running parallel applications.},
  archive      = {J_PEERJCS},
  author       = {Paul Cardosi and Bérenger Bramas},
  doi          = {10.7717/peerj-cs.2966},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2966},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Specx: A c++ task-based runtime system for heterogeneous distributed architectures},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Result assessment tool (RAT): Empowering search engine data analysis. <em>PEERJCS</em>, <em>11</em>, e2962. (<a href='https://doi.org/10.7717/peerj-cs.2962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Result Assessment Tool (RAT) is a Python-based software toolkit that enables researchers to analyze results from commercial search engines, social media platforms, and library search systems. RAT provides an integrated environment for designing studies, collecting results, and performing automated analysis. The software consists of two main modules: RAT Frontend and RAT Backend. RAT Frontend uses Flask to provide a researcher view for designing studies and an evaluation view for collecting ratings from study participants. RAT Backend includes modules for collecting search results, extracting source code, and adding classifiers for automated analysis. The system has been used in various studies, including search engine effectiveness studies, interactive information retrieval studies, and classification studies.},
  archive      = {J_PEERJCS},
  author       = {Sebastian Sünkler and Dirk Lewandowski and Sebastian Schultheiß and Nurce Yagci},
  doi          = {10.7717/peerj-cs.2962},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2962},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Result assessment tool (RAT): Empowering search engine data analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid extraction model for semantic knowledge discovery of water conservancy big data. <em>PEERJCS</em>, <em>11</em>, e2960. (<a href='https://doi.org/10.7717/peerj-cs.2960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the growing demand for efficient public opinion analysis in water conservancy and related domains, as well as the inefficiencies and limited scalability of existing automated web data extraction algorithms for multi-source datasets, this research integrates advanced technologies including big data analytics, natural language processing, and deep learning. A novel, transferable web information extraction model based on deep learning (WIEM-DL) is proposed, leveraging knowledge graphs, machine learning, and ontology-based methods. This model is designed to adapt to varying website structures, enabling effective cross-website information extraction. By refining water conservancy-related online public opinion content and extracting key feature information from critical sentences, the WIEM-DL model excels in locating main content while filtering out noise. This approach not only reduces processing time but also significantly improves extraction accuracy and efficiency. Furthermore, the model establishes methods for micro-level public opinion information extraction and feature representation, creating a fusion space for data-level integration. This serves as a robust foundation for multi-granularity semantic knowledge integration in public opinion big data. Experimental results demonstrate that the WIEM-DL model substantially outperforms traditional information extraction methods, setting a new benchmark for extraction performance.},
  archive      = {J_PEERJCS},
  author       = {Yanna Feng and Feng Zhang and Yongheng Zhang and Jiangang Dong and PengJu Wang},
  doi          = {10.7717/peerj-cs.2960},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2960},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid extraction model for semantic knowledge discovery of water conservancy big data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sepsis detection using deep learning and residual convolutional networks. <em>PEERJCS</em>, <em>11</em>, e2958. (<a href='https://doi.org/10.7717/peerj-cs.2958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is a life-threatening complication caused by infection that leads to extensive tissue damage. If not treated promptly, it can become fatal. Early identification and diagnosis of sepsis are critical to improving patient outcomes. Although recent technological advancements have aided sepsis detection, challenges remain in timely diagnosis using standard clinical practices. In this article, we present a new deep learning model to detect the occurrence of sepsis and the African vulture optimization algorithm (AVOA) to enhance the model performance. The system comprises four crucial steps: First, the enhanced convolutional learning framework (ECLF) with atrous convolutional and multi-level strategies that aim to learn high-level features from the nonlinear mapping of the medical data. Second is the spatio-channel attention network (SCAN), which has a neural architecture designed to focus on significant regions, such as spatial and channel regions, but not restricted to them. Third is the hierarchical dilated convolutional block (HDCB), which utilises a stacked dilated deep convolutional architecture for spatial feature context retrieval. Last is the residual path convolutional chain (RPCC), which uses a multi-residual convolutional approach for feature propagation, preserving important information. The sepsis detection model we bring forth involves many components, as mentioned above, and thus achieves a higher accuracy for timely intervention during sepsis. The combination of AVOA into the model ensures that it is robust and easily transferable, delivering high performance for adaptation to complicated structures inside medical datasets. The proposed model was evaluated on a clinical dataset and achieved outstanding performance, with an accuracy of 99.4%, precision of 98%, recall of 99.2%, F1-score of 99.0%, and an area under the curve (AUC) of 0.998. These results demonstrate the model’s superior ability to detect sepsis accurately and reliably, outperforming traditional clinical scoring methods and conventional machine learning approaches.},
  archive      = {J_PEERJCS},
  author       = {Ahmed S. Almasoud and Ghada Moh Samir Elhessewi and Munya A. Arasi and Abdulsamad Ebrahim Yahya and Menwa Alshammeri and Donia Badawood and Faisal Mohammed Nafie and Mohammed Assiri},
  doi          = {10.7717/peerj-cs.2958},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2958},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Efficient sepsis detection using deep learning and residual convolutional networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot cross-lingual stance detection via adversarial language adaptation. <em>PEERJCS</em>, <em>11</em>, e2955. (<a href='https://doi.org/10.7717/peerj-cs.2955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stance detection has been widely studied as the task of determining if a social media post is positive, negative or neutral towards a specific issue, such as support towards vaccines. Research in stance detection has often been limited to a single language and, where more than one language has been studied, research has focused on few-shot settings, overlooking the challenges of developing a zero-shot cross-lingual stance detection model. This article makes the first such effort by introducing a novel approach to zero-shot cross-lingual stance detection, multilingual translation-augmented bidirectional encoder representations from Transformers (BERT) (MTAB), aiming to enhance the performance of a cross-lingual classifier in the absence of explicit training data for target languages. Our technique employs translation augmentation to improve zero-shot performance and pairs it with adversarial learning to further boost model efficacy. Through experiments on datasets labeled for stance towards vaccines in four languages—English, German, French, Italian, we demonstrate the effectiveness of our proposed approach, showcasing improved results in comparison to a strong baseline model as well as ablated versions of our model. Our experiments demonstrate the effectiveness of model components, not least the translation-augmented data as well as the adversarial learning component, to the improved performance of the model. We have made our source code accessible on GitHub: https://github.com/amcs18pd05/MTAB-cross-lingual-vaccine-stance-detection-2.},
  archive      = {J_PEERJCS},
  author       = {Bharathi A. and Arkaitz Zubiaga},
  doi          = {10.7717/peerj-cs.2955},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2955},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Zero-shot cross-lingual stance detection via adversarial language adaptation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing the knowledge of artificial intelligence chatbots in pharmacology: Examples of two groups of drugs. <em>PEERJCS</em>, <em>11</em>, e2954. (<a href='https://doi.org/10.7717/peerj-cs.2954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives The study aimed to evaluate eight artificial intelligence chatbots (ChatGPT-3.5, Microsoft Copilot, Gemini, You.com, Perplexity, Character.ai, Claude 3.5, and ChatRTX) in answering questions related to two pharmacological topics taught during the basic pharmacology curriculum for medical students: antifungal drugs and hypolipidemic drugs. Methods Chatbots’ performance was assessed by answering 60 single-choice questions on antifungal and hypolipidemic drugs topics. The questions were designed to have four answers (a, b, c, and d), and the artificial intelligence (AI) role was to choose the proper one. The assessment was performed twice with a 1-year hiatus to determine if artificial intelligence chatbots’ effectiveness changed over time. All the answers were checked for being right or wrong according to up-to-date pharmacology knowledge. To improve the clarity of results, to each score, a mark was assigned based on the grading system applied in our unit. Statistica software version 13.3 and Microsoft Excel 2010 were used for statistical analysis. Results In 2023, the best results on the subject of antifungal drugs were obtained by Gemini (formerly Bard) and on the topic of hypolipidemic drugs by You.com (formerly YouChat). In 2024 Microsoft Copilot answered correctly the highest number of questions in both topics. The total results of all artificial intelligence chatbots in 2023 and 2024 were compared using t-test for dependent samples. Statistical analysis revealed that artificial intelligence chatbots improved over time in both pharmacological topics, but this change was not statistically significant (p = 0.784 for antifungal drugs subject and p = 0.056 for hypolipidemic drugs). Conclusions The accuracy of AI chatbots’ responses regarding antifungal and hypolipidemic drugs improved over one year, though not significantly. None of the tested AI systems provided correct answers to all questions within these pharmacological fields.},
  archive      = {J_PEERJCS},
  author       = {Marcin Mateusz Granat and Aleksandra Paź and Dagmara Mirowska-Guzel},
  doi          = {10.7717/peerj-cs.2954},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2954},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Testing the knowledge of artificial intelligence chatbots in pharmacology: Examples of two groups of drugs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HybridFormer: A convolutional neural network-transformer architecture for low dose computed tomography image denoising. <em>PEERJCS</em>, <em>11</em>, e2952. (<a href='https://doi.org/10.7717/peerj-cs.2952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-dose computed tomography (CT) is a potent strategy to minimize X-ray radiation and its detrimental effects on patients. However, reducing radiation significantly boosts noise in reconstructed images, causing blur and obscuring critical tissue details. This obscurity poses significant challenges for doctors in making accurate diagnoses. Traditional techniques like sinogram domain filtration and iterative reconstruction algorithms require inaccessible raw data. Thus, this article introduces HybridFormer, a revolutionary image-denoising model utilizing the Residual Convolution-Swin Transformer Network, designed to enhance images while preserving vital details. Firstly, this algorithm constructs residual convolution for local feature extraction and Swin Transformer for global feature extraction, boosting denoising efficacy. Secondly, to address texture detail errors, we introduced a combined attention transformer unit (CATU) with a cross-channel attentive fusion layer (CCAFL), integrated with residual blocks to form a residual convolution and Swin Transformer Fusion Block (RSTB). Finally, using RSTB, we developed a deep feature refinement module (DFRM) to preserve image details. To avoid smoothing, we combined multi-scale perceptual loss from ResNet-50 with Charbonnier loss into a composite loss function. Validated on the AAPM2016 Mayo dataset, HybridFormer outperformed other state-of-the-art algorithms, achieving improvements of 0.02 dB, 0.16%, and 0.28% in PSNR, SSIM, and FSIM, respectively. Compared with other advanced algorithms, the proposed algorithm achieved the best performance indicators, confirming its superiority.},
  archive      = {J_PEERJCS},
  author       = {Shanaz Sharmin Jui and Zhitao Guo and Rending Jiang and Jiale Liu and Bohua Li},
  doi          = {10.7717/peerj-cs.2952},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2952},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HybridFormer: A convolutional neural network-transformer architecture for low dose computed tomography image denoising},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study on an efficient citrus huanglong disease detection algorithm based on three-channel aggregated attention. <em>PEERJCS</em>, <em>11</em>, e2943. (<a href='https://doi.org/10.7717/peerj-cs.2943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Aiming at the problems of complex and diverse field symptoms of citrus Huanglong disease (HLB), low efficiency and insufficient recognition accuracy of traditional detection methods, this study proposes an efficient detection algorithm based on improved You Only Look Once (YOLO)v8. Methods Firstly, a new character to float (C2f) Attention inverse residual moving block (IRMB) module is designed, which significantly enhances the model’s sensitivity to tiny disease features while reducing the number of parameters by fusing the lightweight IRMB with the adaptive attention gating mechanism, and solves the problem of losing key texture information due to downsampling in the traditional C2f module. Secondly, the three-channel aggregated attention module Powerneck is proposed in the Neck section, which realizes efficient cross-scale feature interactions, effectively suppresses background noise interference, and improves robustness in complex field scenes through SimFusion_4in feature alignment, information fusion module (IFM) global context fusion, and Power channel dynamic weighting strategy. In addition, the detection head design is optimized by structural reparameterization technique to further accelerate the inference process. Results The experimental results show that on the citrus dataset containing 12 diseases and two health states, the mAP50 of this model reaches 97% and the accuracy is 91.5%, which is 1.1% and 1.2% higher than that of the original YOLOv8, respectively, and the inference speed is improved by 14.6% to 370 frames per second (FPS). Comparison of the different models shows that the C2f Attention IRMB, through the mechanism of dual attention The comparison of different models shows that C2f Attention IRMB strengthens the feature expression ability through the dual-attention mechanism, and the Powerneck module reduces redundant computation through dynamic channel pruning, and the two synergistically optimize the model performance significantly. Compared with mainstream models such as YOLOv5m and YOLOv7x, this method is more advantageous in the balance of accuracy and speed, and can meet the demand of real-time detection in the field. Discussion The algorithm provides an efficient tool for early and accurate identification of citrus Huanglong disease, which is of great practical significance for reducing pesticide misuse and improving the efficiency of orchard management, and also provides new ideas for the design of lightweight target detection models in agricultural scenarios.},
  archive      = {J_PEERJCS},
  author       = {Yizong Wang and Zhengrong Xiao and Hong Wang and Fei Li and Jiya Tian},
  doi          = {10.7717/peerj-cs.2943},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2943},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A study on an efficient citrus huanglong disease detection algorithm based on three-channel aggregated attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cword2vec: A novel morphological rule-based word embedding approach for urdu text sentiment analysis. <em>PEERJCS</em>, <em>11</em>, e2937. (<a href='https://doi.org/10.7717/peerj-cs.2937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word embeddings are essential to natural language processing tasks because they contain a single word’s syntactic and semantic information. Word embeddings have been developed widely for numerous spoken languages across the globe like English. The research community needs to pay more attention to the Urdu language despite its significant number of speakers, which amounts to approximately 231.3 million individuals. Urdu is a complex language because word boundaries in Urdu are unspecified, as it does not employ delimiters between words. The compound word, a multiword expression, is a more complex word consisting of many strings or independent base words. Traditionally, compound words are identified during the word segmentation using bigram or trigram approaches. The challenge with these techniques is that they do not produce meaningful words. This study uses morphological rule-based compound words in Urdu text documents. For text representation, a self-trained morphological rule-based compound word embedding (Cword2vec) based on the word2vec model is proposed for Urdu text sentiment analysis. The performance of self-trained morphological rule-based compound word embedding was then evaluated using four well-known deep learning models, i.e., long short-term memory (LSTM), bidirectional LSTM (BiLSTM), convolutional neural networks (CNN), and convolutional LSTM (C-LSTM) for sentiment analysis. We also compare the performance of morphological rule-based compound words with traditional compound word identification techniques such as bigrams and trigrams. Regardless of the classification model, word embedding using our proposed morphological rule-based compound words outperformed in terms of precision, recall, F1 score, and accuracy than bigrams and trigrams.},
  archive      = {J_PEERJCS},
  author       = {Saquib Khushhal and Abdul Majid and Syed Ali Abass and Rabia Riaz and Mohammad Babar and Shafiq Ahmad},
  doi          = {10.7717/peerj-cs.2937},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2937},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Cword2vec: A novel morphological rule-based word embedding approach for urdu text sentiment analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness modeling for topics with different scales in short texts. <em>PEERJCS</em>, <em>11</em>, e2936. (<a href='https://doi.org/10.7717/peerj-cs.2936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of topic modeling to short texts is beset by challenges such as data sparsity and an absence of contextual information. Traditional research methods tend to prioritise high-attention and popular topics, frequently overlooking the identification of emerging topics. Consequently, subjects of a minor scale are prone to being overlooked during the topic identification process. Furthermore, in the context of topic modelling, information that varies in terms of the attention it receives is not treated equally. In order to address the aforementioned issues, a fairness-oriented topic discovery approach (MixTM-G) is proposed. This approach has been designed to facilitate the discovery of topics with different levels of attention. The proposed methodology involves the integration of normalized pointwise mutual information (NPMI) within a graph model to analyse text data. This approach leverages the correlation between data points to assess the semantic relationships between words, thus addressing the limitations posed by sparse data. The employment of graph algorithms facilitates the identification of semantically related clusters within the document graph, thereby enhancing the semantic associations between sparse data. Finally, a mixed topic modeling approach (MixTM), based on bi-grams and tri-grams combinations, is proposed to further improve topic discovery by strengthening the contextual relationships between words. The experimental results demonstrate the efficacy of the proposed method in topic modelling. In comparison to conventional methods, the proposed approach exhibits superior performance in detecting small-scale topics under equivalent conditions.},
  archive      = {J_PEERJCS},
  author       = {Chuangying Zhu and Yongyu Liang and Xinyuan Liang and Limiao Zhong and Fei Xie},
  doi          = {10.7717/peerj-cs.2936},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2936},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fairness modeling for topics with different scales in short texts},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opposition-based learning techniques in metaheuristics: Classification, comparison, and convergence analysis. <em>PEERJCS</em>, <em>11</em>, e2935. (<a href='https://doi.org/10.7717/peerj-cs.2935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, opposition-based learning (OBL) has emerged as a powerful enhancement strategy in metaheuristic algorithms (MAs), gaining significant attention for its potential to accelerate convergence and improve solution quality. Existing research lacks a structured analysis of how different OBL variants influence optimization performance when integrated into various MAs. This study categorizes and analyzes nine distinct OBL techniques: basic opposition-based learning, quasi-opposition-based learning, generalized opposition-based learning, current optimum opposition-based learning, quasi-reflection opposition-based learning, centroid opposition-based learning, random opposition-based learning, super opposition-based learning, and stochastic opposition-based learning. To systematically assess the effectiveness of these techniques, five widely used OBL variants—basic opposition-based learning, quasi-opposition-based learning, generalized opposition-based learning, current optimum opposition-based learning, quasi-reflection opposition-based learning—were selected for implementation within five well-established MAs: differential evolution, genetic algorithm, particle swarm optimization, artificial bee colony, and harmony search. These hybridized algorithms were evaluated across different integration phases, including the initialization passes and generation updates phase, and in both phases. To experimentally demonstrate the capability of OBL strategies to enhance MAs that face common issues such as slow convergence, limited exploration, and imbalanced exploration-exploitation, we have used 12 benchmark functions from CEC2022 suite. Key performance metrics—including maximum, minimum, mean, standard deviation, and convergence curves—were rigorously analyzed to quantify the improvements introduced by each OBL-enhanced MA. Additionally, a Friedman test was conducted to statistically validate the performance differences among the variants. The results indicate that quasi-reflection opposition-based learning consistently outperforms other OBL variants, demonstrating superior convergence speed and solution quality across most benchmark functions.},
  archive      = {J_PEERJCS},
  author       = {Rihab Lakbichi and Farouq Zitouni and Saad Harous and Aridj Ferhat and Abdelhadi Limane and Abdulaziz S. Almazyad and Guojiang Xiong and Ali Wagdy Mohamed},
  doi          = {10.7717/peerj-cs.2935},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2935},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Opposition-based learning techniques in metaheuristics: Classification, comparison, and convergence analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimising AI writing assessment using feedback and knowledge graph integration. <em>PEERJCS</em>, <em>11</em>, e2893. (<a href='https://doi.org/10.7717/peerj-cs.2893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the authors provide a novel framework for the effectiveness of AI writing assessment systems by embedding state-of-the-art deep learning networks, user feedback mechanisms, and knowledge graph frameworks. Most writing assessment tools cannot give personalized, detailed feedback. To tackle this problem, we employ writing assessment transformer models BERT and GPT-3, which allow exploring and scoring the writing on various features, including phrase structure, semantics, vocabulary usage, etc. In our system, we propose a dynamic relational knowledge graph that incorporates writing concepts and their relations, making it easier for the system to devise contextualized thesaurus-wise suggestions. The addition of graph neural networks (GNNs) empowers the model by boosting the GNN’s learning ability regarding the knowledge graph and improving comprehension of complex semantics. Additionally, we have included an iterative design whereby user feedback is collected, and the system adjusts the feedback given in light of historical feedback and changes in a user’s writing behavior over time. The system reconceptualizes the problem of user AI interaction by incorporating its dynamic nature and movement towards the known user and not vice-versa, achieving higher efficiency. To assess user satisfaction and improvements in the quality of the prepared texts, the authors conduct a series of user studies evaluating the efficiency of this integrated system. However, the preliminary data obtained from the task performance analysis show that the results of the proposed framework are far better than those of traditional methods, achieving a better level of engagement and feedback while performing the assessment. This study underscores the potential of deep learning, feedback, and knowledge graph integration in leveraging writing education. It can potentially reform learners’ capabilities, enabling them to write better and more effectively.},
  archive      = {J_PEERJCS},
  author       = {Ci Zhang},
  doi          = {10.7717/peerj-cs.2893},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2893},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimising AI writing assessment using feedback and knowledge graph integration},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Charting new territories: Fuzzy systems in english language teaching and learning. <em>PEERJCS</em>, <em>11</em>, e2887. (<a href='https://doi.org/10.7717/peerj-cs.2887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background In reality, the English language is a mystery; despite its inherent worth and the advantages of fluency, there is a pervasive impression that English instruction in secondary schools is of low quality, contributing to students’ lack of proficiency in the language in higher education and beyond. Pedagogical approaches persist in the classroom, topic after subject, including English. Analyzing texts in great depth via translation and emphasizing vocabulary are joint exercises in English classes. Students waste a lot of time copying things off the board in English classes despite the growing recognition of the significance of both listening and speaking effectively. Methods The Fuzzy Bayesian Intelligent Tutoring System (FB-ITS) is an artificial intelligence (AI) system that adaptively supports students in English teaching and learning settings. It is built in this experimental research employing AI methodologies based on fuzzy logic and the Bayesian network methodology. Using conventional approaches that rely primarily on numerical scores to evaluate academics’ teaching and research activities at various levels is becoming increasingly challenging. Expert systems based on fuzzy logic, suggested in this study, can handle teacher and student evaluations even when faced with imprecise information and uncertainty; this is necessary since academic performance is being indexed in multiple international databases using impact indices at different scales. Results The results showed that, on average, students using the FB-ITS took less time to complete the post-test than students using the conventional e-learning system. This research proposes an English teaching and learning approach that has been very successful based on experimental findings of related big data clustering algorithms. The assessment accuracy has risen by 4%, and the teaching resource utilization rate has been increased by 5%.},
  archive      = {J_PEERJCS},
  author       = {Xiaomei Wen and Deng Pan},
  doi          = {10.7717/peerj-cs.2887},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2887},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Charting new territories: Fuzzy systems in english language teaching and learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSLDV-hop: A robust localization algorithm for WSN using PSO and refinement process. <em>PEERJCS</em>, <em>11</em>, e2770. (<a href='https://doi.org/10.7717/peerj-cs.2770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various areas, wireless sensor networks (WSNs) are popular for achieving goals related to security in buildings when there is fire, in military areas to know the position of terrorists in moles and to observe the behavior of animals in forest areas. All these objectives can be achieved only when the position of the sensor is known to the base station, which helps to achieve the appropriate action in unwanted situations. The controlling point is the base station, which would be able to take action only in case the correct position of the unwanted event is known to the base station. Researches have designed various localization/positioning approaches but still have some challenges related to the accuracy of sensor nodes in localization. Distance vector hop is a popular localization algorithm. Its dependence on the estimated average size of a hop results in a significant localization error. This work suggests an improved algorithm combining a refinement procedure with particle swarm optimization, called DVHOP-PSO. This improved algorithm, called PSLDV-Hop, uses exact anchor sensor node coordinates and fractional hop count information to correct estimated distances. By utilizing an improved iterative evolution algorithm, the PSLDV-Hop algorithm reduces localization errors by achieving a higher degree of accuracy in node localization. Simulation results demonstrate their superiority over other classical improved algorithms and the original distance vector hop. The simulation of this approach is done using the MATLAB tool by considering different parameters such as the number of anchor nodes, number of sensor nodes, area, and range of sensor nodes. Integrating particle swarm optimization with distance vector hop, the proposed localization algorithm consistently outperforms conventional methods, showcasing significant percentage improvements . The suggested algorithm consistently performs better than all other approaches at ranges 20 and 40. Overall, the suggested method performs noticeably better than distance vector hop at range 40, especially when range grows by up to 65%. Additionally, across communication ranges of 20, 30, and 40 units, the proposed algorithm consistently outshines PSO-DV-Hop and GA-DV-Hop, exhibiting notable percentage improvements in localization accuracy.},
  archive      = {J_PEERJCS},
  author       = {Bhupinder Kaur and Deepak Prashar and Arfat Ahmad Khan and Seifedine Kadry and Jungeun Kim},
  doi          = {10.7717/peerj-cs.2770},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2770},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {PSLDV-hop: A robust localization algorithm for WSN using PSO and refinement process},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
