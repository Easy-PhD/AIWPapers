<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PEERJCS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="peerjcs">PEERJCS - 68</h2>
<ul>
<li><details>
<summary>
(2025). Comparing hand-based and controller-based interactions in virtual reality learning: Effects on presence and interaction performance. <em>PEERJCS</em>, <em>11</em>, e3168. (<a href='https://doi.org/10.7717/peerj-cs.3168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) holds significant promise for enhancing science education by providing immersive and interactive learning experiences. However, the optimal interaction modality within educational VR environments remains an open question. This study investigates the impact of hand-based vs. controller-based interaction on sixth-grade students’ sense of presence and interaction performance in a VR science laboratory simulation. Fifty-four sixth-grade students were randomly assigned to either a hand-based interaction group or a controller-based interaction group. Participants completed three interactive science experiments (solar system, electrical circuits, and force/energy) within a virtual laboratory environment designed to mimic their school’s physical lab. Presence was assessed using a validated Turkish adaptation of the Presence Questionnaire (PQ), while interaction performance was evaluated using a structured observation form completed by a school teacher. Independent samples t-tests and Mann-Whitney U tests were used to compare the presence and performance scores between the groups. Supplementary analyses explored the effects of gender and prior VR experience. Contrary to expectations, no significant differences were found in either presence (t(49.4) = −0.01, p = 0.992) or interaction performance (t(52) = −1.30, p = 0.199) between the hand-based and controller-based interaction groups. Both interaction modalities yielded comparable levels of self-reported presence and observed performance. However, an unexpected finding emerged regarding performance. A supplementary analysis revealed a significant main effect of gender on performance scores (F(1, 50) = 4.844, p = 0.032), independent of interaction type. Specifically, males demonstrated significantly higher performance than females. This study suggests that, for sixth-grade students engaging in these specific VR science simulations, hand-based and controller-based interactions are equally effective in terms of fostering presence and supporting interaction performance. These findings have practical implications for the design and implementation of VR learning environments, particularly in resource-constrained settings where the reduced maintenance and hygiene concerns associated with hand-based interaction may be advantageous.},
  archive      = {J_PEERJCS},
  author       = {Murat Saran},
  doi          = {10.7717/peerj-cs.3168},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3168},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparing hand-based and controller-based interactions in virtual reality learning: Effects on presence and interaction performance},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end autonomous driving model based on visual perception for temporary roads. <em>PEERJCS</em>, <em>11</em>, e3152. (<a href='https://doi.org/10.7717/peerj-cs.3152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The research on autonomous driving using deep learning has made significant progress on structured roads, but there has been limited research on temporary roads. The End-to-End autonomous driving model is highly integrated, allowing for the direct translation of input data into desired driving actions. This method eliminates inter-module coupling, thereby enhancing the safety and stability of autonomous vehicles. Methods Therefore, we propose a novel End-to-End model for autonomous driving on temporary roads specifically designed for mobile robots. The model takes three road images as input, extracts image features using the Global Context Vision Transformer (GCViT) network, plans local paths through a Transformer network and a gated recurrent unit (GRU) network, and finally outputs the steering angle through a control model to manage the automatic tracking of unmanned ground vehicles. To verify the model performance, both simulation tests and field tests were conducted. Results The experimental results demonstrate that our End-to-End model accurately identifies temporary roads. The trajectory planning time for a single frame is approximately 100 ms, while the average trajectory deviation is 0.689 m. This performance meets the real-time processing requirements for low-speed vehicles, enabling unmanned vehicles to execute tracking tasks in temporary road environments.},
  archive      = {J_PEERJCS},
  author       = {Qinghua Su and Min Xie and Liyong Wang and Yue Song and Ao Cui and Zhihao Xie},
  doi          = {10.7717/peerj-cs.3152},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3152},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An end-to-end autonomous driving model based on visual perception for temporary roads},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mam-incept-net: A novel inception model for precise interpretation of mammography images. <em>PEERJCS</em>, <em>11</em>, e3149. (<a href='https://doi.org/10.7717/peerj-cs.3149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis of breast cancer through periodic screening is a vital ally in the fight for survival. Mammography, recognized as one of the most widely used and cost-effective tools for detecting early signs of asymmetry, calcification, masses, and architectural distortion in breast tissue, plays a significant role in nearly all screening scenarios. However, the interpretation and scoring of mammograms is a complex multi-parameter process that frequently leads to false-positive and false-negative results. This article introduces a new deep-learning-based model that classifies mammograms according to the Breast Imaging Reporting and Data System (BI-RADS) assessment categories. The model is trained on a private dataset, intentionally excluding no BI-RADS categories. A novel deep neural network architecture is employed to more accurately classify breasts, including their boundaries, as regions of interest (ROIs). The ConvNeXt architecture serves as a feature extractor for lower-level features, which are then combined with the layers of a randomly initialized naive inception module to capture higher-level features. Diagnosis is achieved through three experimental tests, yielding accuracy rates ranging from 82.08% to 86.27%. These promising accuracy levels, in comparison to previous studies, can be attributed to a more comprehensive approach to addressing BI-RADS scoring challenges. In addition to pursuing further enhancements in accuracy, future research should consider integrating prior radiology reports to create a more realistic end-to-end computer-aided detection system.},
  archive      = {J_PEERJCS},
  author       = {Amira Tandirovic Gursel and Yasin Kaya},
  doi          = {10.7717/peerj-cs.3149},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3149},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Mam-incept-net: A novel inception model for precise interpretation of mammography images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delegated multi-party private set intersections from extendable output functions. <em>PEERJCS</em>, <em>11</em>, e3141. (<a href='https://doi.org/10.7717/peerj-cs.3141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operations on sensitive datasets from different parties are essential for various practical applications, such as verifying shopping lists or enforcing no-fly lists. Traditional methods often require one party to access both datasets, which poses privacy concerns. Private set operations provide a solution by enabling these functions without revealing the data involved. However, protocols involving three or more parties are generally much slower than unsecured methods. Outsourced private set operations, where computations are delegated to a non-colluding server, can significantly improve performance, though current protocols have not fully leveraged this assumption. We propose a new protocol that removes the need for public-key cryptography. Our non-interactive set intersection protocol relies solely on the security of an extendable output function, achieving high efficiency. Even in a ten-client setting with 16,384-element sets, the intersection can be computed in under 54 s without communication overhead. Our results indicate that substantial performance improvements can be made without sacrificing privacy, presenting a practical and efficient approach to private set operations.},
  archive      = {J_PEERJCS},
  author       = {Aslı Bay},
  doi          = {10.7717/peerj-cs.3141},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3141},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Delegated multi-party private set intersections from extendable output functions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight fabric defect detection with parallel dilated convolution and dual attention mechanism. <em>PEERJCS</em>, <em>11</em>, e3136. (<a href='https://doi.org/10.7717/peerj-cs.3136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting defects in fabrics is essential to quality control in the manufacturing process of textile productions. To increase detection efficiency, a variety of automatic fabric defect detections have been developed. However, most of these methods rely on complex model with heavy parameters, leading to high computational costs that hinder their adaptation to real-time detection environments. To overcome these obstacles, we proposed a lightweight fabric defect detection (Light-FDD), building upon the You Only Look Once v8 Nano (YOLOv8n) framework with further optimizations. Specifically, the backbone employed an improved FasterNet architecture for feature extraction. In order to capture multi-scale contextual information, we designed a parallel dilated convolution downsampling (PDCD) block to replace the conventional downsampling block in the backbone. In addition, a novel dual attention mechanism, called the global context and receptive-filed (GCRF) attention, was presented to help the model focus on key regions. Furthermore, a lightweight cross-stage partial (CSP) layer was deployed by dual convolution for feature fusion, reducing redundant parameters to further lighten the model. Results from extensive experiments on public fabric defect datasets showed that Light-FDD outperforms existing state-of-the-art lightweight models in terms of detection accuracy while requiring low computational cost. The present study suggests that the performance and effectiveness of detection models can be balanced through the adoption of reasonable strategies.},
  archive      = {J_PEERJCS},
  author       = {Zheqing Zhang and Kezhong Lu and Gaoming Yang},
  doi          = {10.7717/peerj-cs.3136},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3136},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A lightweight fabric defect detection with parallel dilated convolution and dual attention mechanism},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLPruner: Pruning convolutional neural networks with automatic mask learning. <em>PEERJCS</em>, <em>11</em>, e3132. (<a href='https://doi.org/10.7717/peerj-cs.3132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, filter pruning has been recognized as an indispensable technique for mitigating the significant computational complexity and parameter burden associated with deep convolutional neural networks (CNNs). To date, existing methods are based on heuristically designed pruning metrics or implementing weight regulations to penalize filter parameters during the training process. Nevertheless, human-crafted pruning criteria tend not to identify the most critical filters, and the introduction of weight constraints can inadvertently interfere with weight training. To rectify these obstacles, this article introduces a novel mask learning method for autonomous filter pruning, negating requirements for weight penalties. Specifically, we attribute a learnable mask to each filter. During forward propagation, the mask is transformed to a binary value of 1 or 0, serving as indicators for the necessity of corresponding filter pruning. In contrast, throughout backward propagation, we use straight-through estimator (STE) to estimate the gradient of masks, accommodating the non-differentiable characteristic of the rounding function. We verify that these learned masks aptly reflect the significance of corresponding filters. Concurrently, throughout the mask learning process, the training of neural network parameters remains uninfluenced, therefore protecting the normal training process of weights. The efficacy of our proposed filter pruning method based on mask learning, termed MLPruner, is substantiated through its application to prevalent CNNs across numerous representative benchmarks.},
  archive      = {J_PEERJCS},
  author       = {Sihan Chen and Ying Zhao},
  doi          = {10.7717/peerj-cs.3132},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3132},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MLPruner: Pruning convolutional neural networks with automatic mask learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing a 3D convolutional neural network to detect alzheimer’s disease based on MRI. <em>PEERJCS</em>, <em>11</em>, e3129. (<a href='https://doi.org/10.7717/peerj-cs.3129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a progressive neurological disorder that affects millions worldwide, leading to cognitive decline and memory impairment. Structural changes in the brain gradually impair cognitive functions, and by the time symptoms become evident, significant and often irreversible neuronal damage has already occurred. This makes early diagnosis critical, as timely intervention can help slow disease progression and improve patients’ quality of life. Recent advancements in machine learning and neuroimaging have enabled early detection of AD using imaging data and computer-aided diagnostic systems. Deep learning, particularly with magnetic resonance imaging (MRI), has gained widespread recognition for its ability to extract high-level features by leveraging localized connections, weight sharing, and three-dimensional invariance. In this study, we present a 3d convolutional neural network (3D-CNN) designed to enhance classification accuracy using data from the latest version of the OASIS database (OASIS-3). Unlike traditional 2D approaches, our model processes full 3D MRI scans to preserve spatial information and prevent information loss during dimensionality reduction. Additionally, we applied advanced preprocessing techniques, including intensity normalization and noise reduction, to enhance image quality and improve classification performance. Our proposed 3D-CNN achieved an impressive classification accuracy of 91%, outperforming several existing models. These results highlight the potential of deep learning in developing more reliable and efficient diagnostic tools for early Alzheimer’s detection, paving the way for improved clinical decision-making and patient outcomes.},
  archive      = {J_PEERJCS},
  author       = {Maitha Alarjani and Abdulmajeed Almuaibed},
  doi          = {10.7717/peerj-cs.3129},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3129},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimizing a 3D convolutional neural network to detect alzheimer’s disease based on MRI},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plagiarism detection across languages: A comprehensive study of arabic and english-to-arabic long documents. <em>PEERJCS</em>, <em>11</em>, e3128. (<a href='https://doi.org/10.7717/peerj-cs.3128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plagiarism detection in Arabic texts remains a significant challenge due to the complex morphological structure, rich linguistic diversity, and scarcity of high-quality labeled datasets. This study proposes a robust framework for Arabic plagiarism detection by integrating Siamese neural networks (SNN) with state-of-the-art transformer architectures, specifically AraT5 and Longformer. The system employs a hybrid workflow, combining transformer-based encoders and a classification objective to implicitly learn textual similarity. To address the inherent imbalance in Arabic plagiarism datasets, both weighted cross-entropy loss and Dice loss functions were utilized to optimize model training. Extensive experiments were conducted using the ExAraCorpusPAN2015 dataset, demonstrating the effectiveness of the proposed architecture. Results indicate that AraT5 with weighted cross-entropy loss outperformed other configurations, achieving an F1-score of 0.9058. Additionally, comparative analysis with existing methodologies highlights the superiority of our approach in handling nuanced semantic and structural variations within Arabic texts. This study underscores the importance of transformer-based architectures and class-specific loss functions in enhancing plagiarism detection accuracy in under-resourced languages like Arabic.},
  archive      = {J_PEERJCS},
  author       = {Ahmad Abdelaal and Abdallah Elsaadany and Abdelrhman Ahmed Medhat and Aysha Al Shamsi and Noha Gamal ElDin Saad Ali},
  doi          = {10.7717/peerj-cs.3128},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3128},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Plagiarism detection across languages: A comprehensive study of arabic and english-to-arabic long documents},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based tokenization for IoT traffic classification across diverse network environments. <em>PEERJCS</em>, <em>11</em>, e3126. (<a href='https://doi.org/10.7717/peerj-cs.3126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of the Internet of Things (IoT) has significantly increased the volume and diversity of network traffic, making accurate IoT traffic classification crucial for maintaining network security and efficiency. However, existing traffic classification methods, including traditional machine learning and deep learning approaches, often exhibit critical limitations, such as insufficient generalization across diverse IoT environments, dependency on extensive labelled datasets, and susceptibility to overfitting in dynamic scenarios. While recent transformer-based models show promise in capturing contextual information, they typically rely on standard tokenization, which is ill-suited for the irregular nature of IoT traffic and often remains confined to single-purpose tasks. To address these challenges, this study introduces MIND-IoT, a novel and scalable framework for classifying generalized IoT traffic. MIND-IoT employs a hybrid architecture that combines Transformer-based models for capturing long-range dependencies and convolutional neural networks (CNNs) for efficient local feature extraction. A key innovation is IoT-Tokenize, a custom tokenization pipeline designed to preserve the structural semantics of network flows by converting statistical traffic features into semantically meaningful feature-value pairs. The framework operates in two phases: a pre-training phase utilizing masked language modeling (MLM) on large-scale IoT data (UNSW IoT Traces and MonIoTr) to learn robust representations and a fine-tuning phase that adapts the model to specific classification tasks, including binary IoT vs. non-IoT classification, IoT category classification, and device identification. Comprehensive evaluation across multiple diverse datasets (IoT Sentinel, YourThings, and IoT-FCSIT, in addition to the pre-training datasets) demonstrates MIND-IoT’s superior performance, robustness, and adaptability compared to traditional methods. The model achieves an accuracy of up to 98.14% and a 97.85% F1-score, demonstrating its ability to classify new datasets and adapt to emerging tasks with minimal fine-tuning and remarkable efficiency. This research positions MIND-IoT as a highly effective and scalable solution for real-world IoT traffic classification challenges.},
  archive      = {J_PEERJCS},
  author       = {Firdaus Afifi and Faiz Zaki and Hazim Hanif and Nik Aqil and Nor Badrul Anuar},
  doi          = {10.7717/peerj-cs.3126},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3126},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Transformer-based tokenization for IoT traffic classification across diverse network environments},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging PSO-MLP for intelligent assessment of student learning in remote environments: A multimodal approach. <em>PEERJCS</em>, <em>11</em>, e3121. (<a href='https://doi.org/10.7717/peerj-cs.3121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of artificial intelligence (AI) has catalyzed transformative changes in education, particularly in mobile and online learning environments. While existing deep learning models struggle to efficiently integrate the complexity of remote education data and optimize model performance, this article proposes an intelligent evaluation method for students’ learning states based on multimodal data. First, the joint characteristics of the pre-class mental status survey information and the health big data of teachers and students in the online teaching process constitute input data. Then, the multilayer perceptron (MLP) is used to intelligently identify the students’ status and classify their enthusiasm for the class. Finally, the particle swarm optimization (PSO) model is used to optimize the model and improve the overall recognition rate. Compared to traditional methods, the PSO-MLP model with combined multimodal data performs well, achieving an accuracy of 0.891. It provides an operational, technical solution for the education system, provides a new AI foundation for personalized teaching and student health management by accurately assessing students’ learning status, and helps to improve the effectiveness and efficiency of remote education.},
  archive      = {J_PEERJCS},
  author       = {Jing Wang and Muhammad Asif},
  doi          = {10.7717/peerj-cs.3121},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3121},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging PSO-MLP for intelligent assessment of student learning in remote environments: A multimodal approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). International trade market forecasting and decision-making system: Multimodal data fusion under meta-learning. <em>PEERJCS</em>, <em>11</em>, e3120. (<a href='https://doi.org/10.7717/peerj-cs.3120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional market analysis tools primarily rely on unidimensional data, such as historical trading records and price trends. However, these data are often insufficient to reflect the actual state of the market fully. This study introduces a meta-learning-based (MLB) multimodal data fusion approach to optimize feature extraction and fusion strategies, addressing the complexity and heterogeneity inherent in international trade market data. Initially, the mel-frequency cepstral coefficients (MFCC) method is employed to transform the original audio signal into more discriminative spectral features. For image data, the convolutional block attention module (CBAM) is incorporated to capture both channel-wise and spatial attention, thereby improving the model’s ability to focus on market-relevant information. In the feature fusion stage, a meta-learning bidirectional feature pyramid network (ML-BiFPN) is proposed to refine the interaction of multi-scale information via a bidirectional feature pyramid structure. An adaptive weighting mechanism is employed to adjust the feature fusion ratio dynamically. Experimental results demonstrate that the proposed multimodal data fusion model, ML-BiFPN under meta-learning, significantly outperforms existing methods in prediction performance. When tested on the publicly available Trade Map dataset, the average accuracy improves by 9.37%, and the F1-score increases by 0.0473 compare to multilayer perceptron (MLP), achieving a prediction accuracy of 94.55% and an F1-score of 0.912. Notably, under small sample conditions, the model’s advantage becomes even more pronounced, with an average precision (AP) improvement of 2.79%. These findings have significant implications for international trade market forecasting and decision-making, providing enterprises with a more comprehensive understanding of market dynamics, enhancing forecasting accuracy, and supporting scientifically informed decision-making to gain a competitive edge in the marketplace},
  archive      = {J_PEERJCS},
  author       = {Yiming Bai and Muhammad Asif},
  doi          = {10.7717/peerj-cs.3120},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3120},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {International trade market forecasting and decision-making system: Multimodal data fusion under meta-learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An advanced error state kalman filter (ESKF)-based terrain contour matching (TERCOM) method for tracking an aerial vehicle using a low-cost digital elevation map. <em>PEERJCS</em>, <em>11</em>, e3118. (<a href='https://doi.org/10.7717/peerj-cs.3118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terrain Aided Navigation (TAN) systems hold significant potential for delivering accurate navigation for Uncrewed Aerial Vehicles (UAVs). However, a major limitation of conventional TAN systems lies in the time-consuming correlation technique used to search the a priori map, specifically the Digital Elevation Maps (DEM). This article presents a fuzzy heuristic method for the mean absolute deviation (MAD) correlation scheme (FH-MAD), aimed at reducing the computational complexity and execution time of the TAN algorithm. The fuzzy logic system uses heading and roll angle data from onboard sensors to determine the aircraft’s matching area. The output membership functions are designed based on parameters that depend on terrain features. Additionally, the proposed method incorporates an error state Kalman Filter (ESKF) as the navigation algorithm to estimate the UAV’s position under various maneuvering conditions. To evaluate the effectiveness of the proposed system, tests were conducted using two distinct DEMs with varying topographical characteristics and dimensions. The results demonstrate improved position accuracy and a significant reduction in computation time compared to traditional TAN methods, making the approach suitable for real-time UAV navigation applications.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Bilal Kadri and Sofia Yousuf},
  doi          = {10.7717/peerj-cs.3118},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3118},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An advanced error state kalman filter (ESKF)-based terrain contour matching (TERCOM) method for tracking an aerial vehicle using a low-cost digital elevation map},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced information cross-attention fusion for drug–target binding affinity prediction. <em>PEERJCS</em>, <em>11</em>, e3117. (<a href='https://doi.org/10.7717/peerj-cs.3117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The rapid development of artificial intelligence has permeated many fields, with its application in drug discovery becoming increasingly mature. Machine learning, particularly deep learning, has significantly improved the efficiency of drug discovery. In the core task of predicting drug–target affinity (DTA), deep learning enhances predictive performance by automatically extracting complex features from compounds and proteins. Methods Traditional approaches often rely heavily on sequence and two-dimensional structural information, overlooking critical three-dimensional and physicochemical properties. To address this, we propose a novel model—Cross Attention Fusion based on Information Enhancement for Drug–Target Affinity Prediction (CAFIE-DTA)—which incorporates protein 3D curvature and electrostatic potential information. The model approximates protein surface curvature using Delaunay triangulation, calculates total electrostatic potential via Adaptive Poisson-Boltzmann Solver (APBS) software, and employs cross multi-head attention to fuse physicochemical and sequence information of proteins. Simultaneously, it integrates graph-based and physicochemical features of compounds using the same attention mechanism. The resulting protein and compound vectors are concatenated for affinity prediction. Results Cross-validation and comparative evaluations on the benchmark Davis and KIBA datasets demonstrate that CAFIE-DTA outperforms existing methods. On the Davis dataset, it achieved improvements of 0.003 in confidence interval (CI) and 0.022 in R2. On the KIBA dataset, it improved MSE by 0.008, CI by 0.005, and R2 by 0.017. Compared to traditional models relying on 2D structures and sequence data, CAFIE-DTA shows superior performance in DTA prediction. The source code is available at: https://github.com/NTU-MedAI/CAFIE-DTA.},
  archive      = {J_PEERJCS},
  author       = {Ailu Fei and Yihan Wang and Tiantian Ruan and Yekang Zhang and Min Yao and Li Wang},
  doi          = {10.7717/peerj-cs.3117},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3117},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced information cross-attention fusion for drug–target binding affinity prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A literature survey of shapelet quality measures for time series classification. <em>PEERJCS</em>, <em>11</em>, e3115. (<a href='https://doi.org/10.7717/peerj-cs.3115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Things, time series classification (TSC) has gained significant attention from researchers due to its applications in various real-world fields, including electroencephalogram/electrocardiogram classification, emotion recognition, and error message detection. To improve classification performance, numerous TSC methods have been proposed in recent years. Among these, shapelet-based TSC methods are particularly notable for their intuitive interpretability. A critical task within these methods is evaluating the quality of candidate shapelets. This paper provides a comprehensive survey of the state-of-the-art measures for assessing shapelet quality. To present a structured overview, we begin by proposing a taxonomy of these measures, followed by a detailed description of each one. We then discuss these measures, highlighting the challenges faced by current research and offering suggestions for future directions. Finally, we summarize the findings of this survey. We hope that this work will serve as a valuable resource for researchers in the field.},
  archive      = {J_PEERJCS},
  author       = {Teng Li and Xiaodong Guo and Cun Ji},
  doi          = {10.7717/peerj-cs.3115},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3115},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A literature survey of shapelet quality measures for time series classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel deep learning approach for predicting stone-free rates post-ESWL on uncontrasted CT. <em>PEERJCS</em>, <em>11</em>, e3111. (<a href='https://doi.org/10.7717/peerj-cs.3111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracorporeal shock wave lithotripsy (ESWL) is one of the most often employed therapy methods for managing kidney stones. In our work, we sought to assess the efficacy of the artificial intelligence model developed using non-contrast computed tomography (CT) images in predicting stone-free rates for ESWL. The main difference between this study and other studies is that it proposes an artificial intelligence-based model that predicts the success of ESWL treatment using artificial intelligence methods. Data from 910 patients who underwent ESWL between January 2016 and June 2021 were analyzed retrospectively. Since the local binary pattern (LBP) and histogram of oriented gradients (HOG) feature extraction methods gave more successful results than other methods, a new feature map was obtained using the neighborhood component analysis (NCA) dimension reduction method after combining the features obtained using these methods. Then, the reduced feature map was classified into classifiers. In conclusion, we analyzed the effect of ESWL treatment using different artificial intelligence methods and found that the prediction accuracy was 94% on average. Results were obtained from seven different convolutional neural networks (CNNs) and two textural-based models in the study. Since textural-based models achieved the highest success among these models, these models were used as the base in the proposed model. The proposed model achieved better results than nine different models used in the study. When the results obtained from the proposed hybrid model for ESWL prediction are examined, this model will guide experts in the treatment of the disease.},
  archive      = {J_PEERJCS},
  author       = {Ozgur Efiloglu and Muhammed Yildirim and Kadir Yildirim and Harun Bingol and Mustafa Kaan Akalin and Meftun Culpan and Bilal Alatas and Asif Yildirim},
  doi          = {10.7717/peerj-cs.3111},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3111},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel deep learning approach for predicting stone-free rates post-ESWL on uncontrasted CT},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving course evaluation processes in higher education institutions: A modular system approach. <em>PEERJCS</em>, <em>11</em>, e3110. (<a href='https://doi.org/10.7717/peerj-cs.3110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Course and instructor evaluations (CIE) are essential tools for assessing educational quality in higher education. However, traditional CIE systems often suffer from inconsistencies between structured responses and open-ended feedback, leading to unreliable insights and increased administrative workload. This study suggests a modular system to address these challenges, leveraging sentiment analysis and inconsistency detection to enhance the reliability and efficiency of CIE processes. Background Improving the reliability of CIE data is crucial for informed decision-making in higher education. Existing methods fail to address discrepancies between numerical scores and textual feedback, resulting in misleading evaluations. This study proposes a system to identify and exclude inconsistent data, providing more reliable insights. Methods Using the Design Science Research Methodology (DSRM), a system architecture was developed with five modules: data collection, preprocessing, sentiment analysis, inconsistency detection, and reporting. A dataset of 13,651 anonymized Turkish CIE records was used to train and evaluate machine learning algorithms, including support vector machines, naive Bayes, random forest, decision trees, K-nearest neighbors, and OpenAI’s GPT-4 Turbo Preview model. Sentiment analysis results from open-ended responses were compared with structured responses to identify inconsistencies. Results The GPT-4 Turbo Preview model outperformed traditional algorithms, achieving 85% accuracy, 88% precision, and 95% recall. Analysis of a prototype system applied to 431 CIEs identified a 37% inconsistency rate. By excluding inconsistent data, the system generated reliable reports with actionable insights for course and instructor performance. The purpose of this study is to design and evaluate a new system using the Design Science Research (DSR) approach to enhance the accuracy and reliability of course evaluation processes employed in higher education institutions. The modular system effectively addresses inconsistencies in CIE processes, offering a scalable and adaptable solution for higher education institutions. By integrating advanced machine learning techniques, the system enhances the accuracy and reliability of evaluation reports, supporting data-driven decision-making. Future work will focus on refining sentiment analysis for neutral comments and broadening the system’s applicability to diverse educational contexts. This innovative approach represents a significant advancement in leveraging technology to improve educational quality.},
  archive      = {J_PEERJCS},
  author       = {İlker Kocaoğlu and Erinç Karataş},
  doi          = {10.7717/peerj-cs.3110},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3110},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving course evaluation processes in higher education institutions: A modular system approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Palmprint recognition based on principal line features. <em>PEERJCS</em>, <em>11</em>, e3109. (<a href='https://doi.org/10.7717/peerj-cs.3109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing prevalence and diversity of imaging devices, palmprint recognition has emerged as a technology that better meets the demands of the modern era. However, traditional manual methods have limitations in effectively extracting palmprint principal line features. To address this, we introduce a novel data augmentation method. First, the wide line extraction (WLE) filter is utilized to specifically target and extract the prominent principal lines of palmprints by leveraging their direction and width characteristics. Then, a Gabor filter is applied to the WLE-extracted results to purify the features and remove fine lines, as fine lines can introduce noise and redundancy that interfere with the accurate extraction of significant principal line features crucial for palmprint recognition. Evaluating this data augmentation across four common Vision Transformer (ViT) classification models, experimental results show that it improves the recognition rates of all databases to varying degrees, with a remarkable 32.9% increase on the high-resolution XINHUA database. With the successful removal of fine lines by WLE, we propose a new Layer Visual Transformer (LViT) design paradigm. For its input, distinct blocking strategies are adopted, carefully designed to partition the data to capture different levels of spatial and feature information, using larger blocks for global structure and smaller ones for local details. The output results of these different blocking strategies are fused by “sum fusion” and “maximum fusion”, and the local and global features are effectively utilized by combining complementary information to improve the recognition performance and get state-of-the-art results on multiple databases. Moreover, LViT requires fewer training iterations due to the synergistic effects of the blocking strategies, optimizing the learning process. Finally, by simulating real-world noise conditions, we comprehensively evaluate LViT and find that, compared with traditional methods, our approach exhibits excellent noise-resistant generalization ability, maintaining stable performance across the PolyU II, IIT Delhi, XINHUA, and NTU-CP-V1 databases.},
  archive      = {J_PEERJCS},
  author       = {Hongxia Wang and Teng Lv},
  doi          = {10.7717/peerj-cs.3109},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3109},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Palmprint recognition based on principal line features},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BEPCD: An ensemble learning-based intrusion detection framework for in-vehicle CAN bus. <em>PEERJCS</em>, <em>11</em>, e3108. (<a href='https://doi.org/10.7717/peerj-cs.3108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development and widespread adoption of intelligent vehicles and the Internet of Vehicles (IoV), vehicle security has become a growing concern. Modern vehicles manage key components via the controller area network (CAN) connected electronic control units (ECUs). CAN bus intrusion techniques are the primary methods of compromising the IoV, posing a significant threat to the normal operation of critical vehicle systems, such as the power systems. However, existing attack detection methods still have shortcomings in terms of feature extraction and the diversity of attack type detection. To address these challenges, we propose an intrusion detection framework named basic ensemble and pioneer class decision (BEPCD). The framework first constructs a 15-dimensional feature model to hierarchically characterize CAN bus messages. Subsequently, BEPCD incorporates multi-model ensemble learning enhanced by a Pioneer class selector and confidence-driven voting mechanisms, enabling precise classification of both conventional and emerging attack patterns. Additionally, we analyze the importance of different data features across four machine learning algorithms. Experimental results on public datasets demonstrate that the proposed detection framework effectively detects intrusions in-vehicle CAN bus. Compared to other intrusion detection frameworks, our framework improves the overall F1-score by 1% to 5%. Notably, it achieves an approximately 77.5% performance enhancement in detecting replay attacks.},
  archive      = {J_PEERJCS},
  author       = {Bocheng Xu and Fei Cao and Xilong Li and Song Tian and Wenbo Deng and Shudan Yue},
  doi          = {10.7717/peerj-cs.3108},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3108},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {BEPCD: An ensemble learning-based intrusion detection framework for in-vehicle CAN bus},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2LE-BO-DeepTrade: An integrated deep learning framework for stock price prediction. <em>PEERJCS</em>, <em>11</em>, e3107. (<a href='https://doi.org/10.7717/peerj-cs.3107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel, integrated deep-learning framework named 2LE-BO-DeepTrade for stock closing price prediction. This framework combines 2LE-ICEEMDAN denoising, deep learning models tuned with Bayesian optimization, and a piecewise linear representation (PLR)-based trading strategy. The framework utilizes the model that provides the highest accuracy among optimized long short-term memory (LSTM), long short term memory with batch normalization (LSTM-BN), and gated recurrent unit (GRU) models on data preprocessed with the 2LE-ICEEMDAN denoising method. The model’s performance is comprehensively evaluated using both statistical metrics and a PLR-based trading strategy specifically developed for this study. Experimental studies were conducted on AKBNK, MGROS, KCHOL, THYAO, and ULKER stocks, which are traded on Borsa Istanbul and represent different sectors. During the denoising phase, noise in the stock prices was successfully removed, and noiseless intrinsic mode functions (IMFs) were obtained. The optimal model and hyperparameters for each IMF component were determined using Bayesian optimization, significantly improving prediction accuracy. The model within this framework, characterized by its optimized yet simple structure, demonstrated superior predictive performance compared to the more complex ICE2DE-MDL model in the literature. When compared to ICE2DE-MDL, the 2LE-BO-DeepTrade model, across all tested stocks, reduced the average root mean square error (RMSE) value by 94.4%, the average mean absolute error (MAE) value by 93.6%, and the average mean absolute percentage error (MAPE) value by 37.4% while increasing the average R2 value by 1.1%. Furthermore, the PLR-based trading strategy, specifically developed for this study, generated “Buy” and “Sell” signals, exhibiting a remarkably superior financial performance to a passive investment strategy. Across all considered stocks, the PLR-based strategy yielded, on average, 66 times more profit than the passive approach. These findings substantiate that the proposed integrated deep learning-based stock forecasting framework can significantly enhance the accuracy of stock market predictions and the returns of trading strategies.},
  archive      = {J_PEERJCS},
  author       = {Zinnet Duygu Akşehir and Erdal Kılıç},
  doi          = {10.7717/peerj-cs.3107},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3107},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {2LE-BO-DeepTrade: An integrated deep learning framework for stock price prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data trace as the scientific foundation for trusted metrological data: A review for future metrology direction. <em>PEERJCS</em>, <em>11</em>, e3106. (<a href='https://doi.org/10.7717/peerj-cs.3106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the digital transformation of metrology, ensuring the trustworthiness and integrity of measurement data during its generation, transmission, and storage—i.e., trustworthy detection of measurement data—has become a critical challenge. Data traces are residual marks left during the data processing, which help identify malicious activities targeting measurement data. These traces are especially important when the trust and integrity of potential data evidence are under threat. To this end, this article systematically reviews relevant core techniques and analyzes various detection methods across the different stages of the data lifecycle, evaluating their applicability and limitations in identifying data tampering, unauthorized access, and anomalous operations. The findings suggest that trace detection technologies can enhance the traceability and transparency of metrological data, thereby providing technical support for building a trustworthy digital metrology system. This review lays the theoretical foundation for future research on developing automated anomaly detection models, improving forensic techniques for data tampering in measurement devices, and constructing multi-modal, full-lifecycle traceability frameworks for measurement data. Subsequent studies should focus on aligning these technologies with metrological standards and verifying their deployment in real-world measurement instruments.},
  archive      = {J_PEERJCS},
  author       = {Zhanshuo Cao and Boyong Gao and Zilong Liu and Xingchuang Xiong and Bin Wang and Chenbo Pei},
  doi          = {10.7717/peerj-cs.3106},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3106},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data trace as the scientific foundation for trusted metrological data: A review for future metrology direction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of deep learning methods in aquatic animal husbandry. <em>PEERJCS</em>, <em>11</em>, e3105. (<a href='https://doi.org/10.7717/peerj-cs.3105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aquatic animal husbandry is crucial for global food security and supports millions of livelihoods around the world. With the growing demand for seafood, this industry has become economically significant for many regions, contributing to local and global economies. However, as the industry grows, it faces various major challenges that are not encountered in small-scale setups. Traditional methods for classifying, detecting, and monitoring aquatic animals are often time-consuming, labor-intensive, and prone to inaccuracies. The labor-intensive nature of these operations has led many aquaculture operators to move towards automation systems. Yet, for an automation system to be effectively deployed, it needs an intelligent decision-making system, which is where deep learning techniques come into play. In this article, an extensive methodological review of machine learning methods, primarily the deep learning methods used in aquatic animal husbandry are concisely summarized. This article focuses on the use of deep learning in three key areas: classification, localization, and segmentation. Generally, classification techniques are vital in distinguishing between different species of aquatic organisms, while localization methods are used to identify the respective animal’s position within a video or an image. Segmentation techniques, on the other hand, enable the precise delineation of organism boundaries, which is essential information in accurate monitoring systems. Among these key areas, segmentation techniques, particularly through the U-Net model, have shown the best results, even achieving a high segmentation performance of 94.44%. This article also highlights the potential of deep learning to enhance the precision, productivity, and sustainability of automated operations in aquatic animal husbandry. Looking ahead, deep learning offers huge potential to transform the aquaculture industry in terms of cost and operations. Future research should focus on refining existing models to better address real-world challenges such as sensor input quality and multi-modal data across various environments for better automation in the aquaculture industry.},
  archive      = {J_PEERJCS},
  author       = {Marzuraikah Mohd Stofa and Fatimah Az Zahra Azizan and Mohd Asyraf Zulkifley},
  doi          = {10.7717/peerj-cs.3105},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3105},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A review of deep learning methods in aquatic animal husbandry},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced BERT model with improved local feature extraction and long-range dependency capture in promoter prediction for hearing loss. <em>PEERJCS</em>, <em>11</em>, e3104. (<a href='https://doi.org/10.7717/peerj-cs.3104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Promoter prediction has a key role in helping to understand gene regulation and in developing gene therapies for complex diseases such as hearing loss (HL). While traditional Bidirectional Encoder Representations from Transformers (BERT) models excel in capturing contextual information, they often have limitations in simultaneously extracting local sequence features and long-range dependencies inherent in genomic data. To address this challenge, we propose DNABERT-CBL (DNABERT-2_CNN_BiLSTM), an enhanced BERT-based architecture that fuses a convolutional neural network (CNN) and a bidirectional long and short-term memory (BiLSTM) layer. The CNN module is able to capture local regulatory features, while the BiLSTM module can effectively model long-distance dependencies, enabling efficient integration of global and local features of promoter sequences. The models are optimized using three strategies: individual learning, cross-disease training and global training, and the performance of each module is verified by constructing comparison models with different combinations. The experimental results show that DNABERT-CBL outperforms the baseline DNABERT-2_BASE model in hearing loss promoter prediction, with a 20% reduction in loss, a 3.3% improvement in the area under the working characteristic curve (AUC) of the subjects, and a 5.8% improvement in accuracy at a sequence length of 600 base pairs. In addition, DNABERT-CBL consistently outperforms other state-of-the-art BERT-based genome models on several evaluation metrics, highlighting its superior generalization ability. Overall, DNABERT-CBL provides an effective framework for accurate promoter prediction, offers valuable insights into gene regulatory mechanisms, and supports the development of gene therapies for hearing loss and related diseases.},
  archive      = {J_PEERJCS},
  author       = {Jing Sun and Yangfan Huang and Jiale Fu and Li Teng and Xiao Liu and Xiaohua Luo},
  doi          = {10.7717/peerj-cs.3104},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3104},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An enhanced BERT model with improved local feature extraction and long-range dependency capture in promoter prediction for hearing loss},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software defined network intrusion system to detect malicious attacks in computer internet of things security using deep extractor supervised random forest technique. <em>PEERJCS</em>, <em>11</em>, e3103. (<a href='https://doi.org/10.7717/peerj-cs.3103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The architecture of software-defined networking (SDN) involves the separation of the network control plane from the routing plane. If this initiative turns out well, it has the potential to reduce operating expenses and the duration required to provide new services in comparison to traditional networks. However, this architecture has additional security concerns, including a single point of failure that could potentially provide any user with unrestricted access to the entire network. Nevertheless, it is essential to reduce the probability of security breaches. The development of immediate intrusion detection systems (IDSs) that can quickly spot and stop malicious activities like distributed denial of service (DDoS), DoS, web-attacks, and Bot-NET is an important part of SDN architecture. Several researchers are using cutting-edge methods, such as machine learning, to investigate and elucidate the causes behind the sudden rise in attacks and abnormal behavior, but the majority of these methods are deficient in terms of flexibility and accuracy. This study proposed a lightweight method for detecting different SDN attacks from intrusion-defined networks. The lightweight long short-term memory (LSTM) network has the capability to capture temporal patterns and sequential interactions in the SDN data. It also learned important context that is efficient for feature extraction and then developed supervised random forest (SRF) for the attack prediction. The dataset consists of 207,146 rows and 84 features that were preprocessed, including separate features and target attacks. The experiments show that the proposed method achieved 99.93% accuracy for attack detection and 0.0090 loss, confirming its efficacy. We also tested the proposed method on another SDN dataset and achieved 99.43% accuracy for multi-class attack detection. Furthermore, the use of supervised random forest reduces the model’s complexity, resulting in increased overall efficiency.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Mujahid and Abeer Rashad Mirdad and Faten S. Alamri and Anees Ara and Amjad Khan},
  doi          = {10.7717/peerj-cs.3103},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3103},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Software defined network intrusion system to detect malicious attacks in computer internet of things security using deep extractor supervised random forest technique},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pattern transition recognition based on transfer learning for exoskeleton across different terrains. <em>PEERJCS</em>, <em>11</em>, e3099. (<a href='https://doi.org/10.7717/peerj-cs.3099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion intention detection is a growing trend in wearable robots. In the study, a novel transfer learning method based on temporal convolutional network spatial attention (TCN-SA) is applied for pattern transition recognition under triple physical loads on different terrains. The proposed approach is used to recognize eight locomotion modes transition among five dynamic locomotion modes in sequence, such as level ground walking, stair ascending, stair descending, ramp ascending, and ramp descending. To address the problem of pattern transition recognition, transfer learning adapts a model from the source domain to the target domain. Temporal convolutional network (TCN) relies on local relationships in time sequence and gains steady gradient propagation. Furthermore, spatial attention (SA) provides insight into significant components in multi-dimensional feature selection. Pattern transition recognition based on a transfer learning method achieves higher accuracy and earlier prediction time (Pre-T). The accuracy of pattern transition detection reaches 97.46%, 97.62%, and 98.21% in M0, M20, and M40, respectively. In the process of pattern transition recognition, Pre-T of next locomotion mode in M0, M20, and M40 are 240–600 ms, 200–410 ms, and 120–420 ms before the step into that locomotion mode. The proportion of prediction time in a gait cycle (Pre-T/GC) in M0, M20, and M40 is 14.2–36%, 14.82–28.22%, and 7.4–29.1%, respectively. Ultimately, the results indicate that the proposed approach fulfills the expected performance in Pre-T and comparisons with TCN-attention, TCN, residual network (ResNet), and long short-term memory (LSTM) in assessment criteria. Our study early detects pattern transition, allowing the exoskeleton to traverse between adjacent terrains smoothly.},
  archive      = {J_PEERJCS},
  author       = {Yifan Gao and Jianbin Zheng and Yang Gao and Ziyao Chen and Jing Tang and Liping Huang},
  doi          = {10.7717/peerj-cs.3099},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3099},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Pattern transition recognition based on transfer learning for exoskeleton across different terrains},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing transformer-based prediction of human microbe–disease associations through integrated loss strategies. <em>PEERJCS</em>, <em>11</em>, e3098. (<a href='https://doi.org/10.7717/peerj-cs.3098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microorganisms play an important role in many complex diseases, influencing their onset, progression, and potential treatment outcomes. Exploring the associations between microbes and human diseases can deepen our understanding of disease mechanisms and assist in improving diagnosis and therapy. However, traditional biological experiments used to uncover such relationships often demand substantial time and resources. In response to these limitations, computational methods have gained traction as more practical tools for predicting microbe-disease associations. Despite their growing use, many of these models still face challenges in terms of accuracy, stability, and adaptability to noisy or sparse data. To overcome the aforementioned limitations, we propose a novel predictive framework, HyperGraph Neural Network with Transformer for Microbe-Disease Associations (HGNNTMDA), designed to infer potential associations between human microbes and diseases. The framework begins by integrating microbe–disease association data with similarity-based features to construct node representations. Two graph construction strategies are employed: a K-nearest neighbor (KNN)-based adjacency matrix to build a standard graph, and a K-means clustering approach that groups similar nodes into clusters, which serve as hyperedges to define the incidence matrix of a hypergraph. Separate hypergraph neural networks (HGNNs) are then applied to microbe and disease graphs to extract structured node-level features. An attention mechanism (AM) is subsequently introduced to emphasize informative signals, followed by a Transformer module to capture contextual dependencies and enhance global feature representation. A fully connected layer then projects these features into a unified space, where association scores between microbes and diseases are computed. For model optimization, we propose a hybrid loss strategy combining contrastive loss and Huber loss. The contrastive loss aids in learning discriminative embeddings, while the Huber loss enhances robustness against outliers and improves predictive stability. The effectiveness of HGNNTMDA is validated on two benchmark datasets—HMDAD and Disbiome—using five-fold cross-validation (5CV). Our model achieves an AUC of 0.9976 on HMDAD and 0.9423 on Disbiome, outperforming six existing state-of-the-art methods. Further case studies confirm its practical value in discovering novel microbe–disease associations.},
  archive      = {J_PEERJCS},
  author       = {Rong Zhu and Yong Wang and Junliang Shang and Ling-Yun Dai and Feng Li},
  doi          = {10.7717/peerj-cs.3098},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3098},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimizing transformer-based prediction of human microbe–disease associations through integrated loss strategies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy evaluation and explainable machine learning for diagnosis of rheumatic and autoimmune diseases. <em>PEERJCS</em>, <em>11</em>, e3096. (<a href='https://doi.org/10.7717/peerj-cs.3096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new combination of an explainable machine learning approach with a fuzzy evaluation framework is proposed to improve the diagnostic performance and interpretation of rheumatic and autoimmune diseases. This work addresses three major challenges: (i) overlapping symptoms and complex clinical presentations, (ii) the lack of interpretability in traditional machine learning models, and (iii) the difficulty of selecting the best diagnosis model. To overcome these challenges, a new dataset was collected from Iraq’s hospitals and health centers between 2019 and 2024. The size of dataset is 12,085 patients and includes 14 features in seven classes (rheumatoid arthritis, reactive arthritis, ankylosing spondylitis, Sjogren syndrome, systemic lupus erythematosus, psoriatic arthritis, and normal). The dataset is subjected to extensive preprocessing with attribute imputation (mean and mode), encoding categorical features, and balancing the data to pass it to 12 different machine learning models. Performance is evaluated based on precision, recall, F-score, kappa, Hamming loss, Matthews correlation coefficient, and accuracy to identify the best model. To select the optimal model, we apply fuzzy decision by opinion score method (FDOSM). The FDOSM process involves assessments from three domain experts to ensure a robust and well-rounded evaluation. Furthermore, the explainable artificial intelligence (XAI) technique provides global and local explanations for model predictions. Local interpretable model explanations (LIME) were used as explanations and significantly increased the transparency and reliability of the clinical decision-making process. The results show that the FDOSM yields gradient boosting with a 0.1333 score and a rank of 1, is the best model with an accuracy of 86.89%, precision of 87.35%, and kappa of 84.51%. The best model using XAI to increase confidence and trustworthiness in clinical decision-making and healthcare applications.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Fadhil Mahdi and Arezoo Jahani and Dhafar Hamed Abd},
  doi          = {10.7717/peerj-cs.3096},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3096},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fuzzy evaluation and explainable machine learning for diagnosis of rheumatic and autoimmune diseases},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The DBCV index is more informative than DCSI, CDbw, and VIASCKDE indices for unsupervised clustering internal assessment of concave-shaped and density-based clusters. <em>PEERJCS</em>, <em>11</em>, e3095. (<a href='https://doi.org/10.7717/peerj-cs.3095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering methods are unsupervised machine learning techniques that aggregate data points into specific groups, called clusters, according to specific criteria defined by the clustering algorithm employed. Since clustering methods are unsupervised, no ground truth or gold standard information is available to assess its results, making it challenging to know the results obtained are good or not. In this context, several clustering internal rates are available, like Silhouette coefficient, Calinski-Harabasz index, Davies-Bouldin, Dunn index, Gap statistic, and Shannon entropy, just to mention a few. Even if popular, these clustering internal scores work well only when used to assess convex-shaped and well-separated clusters, but they fail when utilized to evaluate concave-shaped and nested clusters. In these concave-shaped and density-based cases, other coefficients can be informative: Density-Based Clustering Validation Index (DBCVI), Compose Density between and within clusters Index (CDbw), Density Cluster Separability Index (DCSI), Validity Index for Arbitrary-Shaped Clusters based on the kernel density estimation (VIASCKDE). In this study, we describe the DBCV index precisely, and compare its outcomes with the outcomes obtained by CDbw, DCSI, and VIASCKDE on several artificial datasets and on real-world medical datasets derived from electronic health records, produced by density-based clustering methods such as density-based spatial clustering of applications with noise (DBSCAN). To do so, we propose an innovative approach based on clustering result worsening or improving, rather than focusing on searching the “right” number of clusters like many studies do. Moreover, we also recommend open software packages in R and Python for its usage. Our results demonstrate the higher reliability of the DBCV index over CDbw, DCSI, and VIASCKDE when assessing concave-shaped, nested, clustering results.},
  archive      = {J_PEERJCS},
  author       = {Davide Chicco and Giuseppe Sabino and Luca Oneto and Giuseppe Jurman},
  doi          = {10.7717/peerj-cs.3095},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3095},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The DBCV index is more informative than DCSI, CDbw, and VIASCKDE indices for unsupervised clustering internal assessment of concave-shaped and density-based clusters},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing variable neighbourhood search algorithms for the direct aperture optimisation in radiotherapy. <em>PEERJCS</em>, <em>11</em>, e3094. (<a href='https://doi.org/10.7717/peerj-cs.3094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensity modulated radiation therapy (IMRT) is a prevalent approach for administering radiation therapy in cancer treatment. The primary objective of IMRT is to devise a treatment strategy that eradicates cancer cells from the tumour while minimising damage to the surrounding organs at risk. Conventional IMRT planning entails a sequential procedure: optimising beam intensity for a certain set of angles, followed by sequencing. Unfortunately, treatment plans obtained in the optimisation stage are severely impaired after the sequencing stage due to physical and delivery constraints that are not considered during the optimisation stage. One method that tackles the issues above is the direct aperture optimisation (DAO) technique. The DAO problem seeks to generate a set of deliverable aperture configurations and a corresponding set of radiation intensities. This method accounts for physical and delivery time limitations, facilitating the creation of clinically appropriate treatment programs. In this article, we propose and compare two variable neighbourhood search (VNS) based algorithms, called variable neighbourhood descent (VND) and reduced variable neighbourhood search (rVNS). The VND algorithm is a deterministic variant of VNS that systematically explores different neighbourhood structures. This approach allows for a more thorough solution for space exploration while maintaining computational efficiency. The rVNS, unlike traditional VNS algorithms, does not require any transition rule, as it integrates a set of predefined neighbourhood moves at each iteration. We apply our proposed algorithms to prostate cancer cases, achieving highly competitive results for both algorithms. In particular, the proposed rVNS requires 62.75% fewer apertures and achieved a 63.93% reduction in beam-on time compared to the sequential approach’s best case, which means treatment plans that can be delivered in less time. Additionally, we evaluate the clinical quality of the treatment plans using established dosimetric indicators, comparing our results against those produced by matRad’s tool for DAO to assess target coverage and organ-at-risk sparing.},
  archive      = {J_PEERJCS},
  author       = {Mauricio Moyano and Keiny Meza-Vasquez and Gonzalo Tello-Valenzuela and Nicolle Ojeda-Ortega and Carolina Lagos and Guillermo Cabrera-Guerrero},
  doi          = {10.7717/peerj-cs.3094},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3094},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparing variable neighbourhood search algorithms for the direct aperture optimisation in radiotherapy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multicriteria scheduling of two-subassembly products with batch availability and precedence constraints. <em>PEERJCS</em>, <em>11</em>, e3093. (<a href='https://doi.org/10.7717/peerj-cs.3093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the multicriteria problems of scheduling a set of n products on a fabrication facility, focusing on batch availability and precedence constraints. Each product is composed of two distinct subassemblies: a common subassembly, shared across all products, and a unique subassembly unique to each product. The common subassemblies are processed together in batches, with each batch requiring an initial setup, while unique subassemblies are handled individually. The availability of a common subassembly is contingent upon the completion of its entire batch (i.e., batch availability), whereas a unique subassembly becomes available immediately after its processing. The product completion time is determined by the availability of both subassemblies. Strict (weak) precedence means that if a product precedes another, then the latter can start only after the former is completed (the latter cannot start earlier than the former). We propose O(n4)-time algorithms to simultaneously optimize makespan and maximum cost, as well as to lexicographically optimize two maximum costs and makespan under strict or weak precedence constraints.},
  archive      = {J_PEERJCS},
  author       = {Zhenxin Wen and Shuguang Li},
  doi          = {10.7717/peerj-cs.3093},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3093},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multicriteria scheduling of two-subassembly products with batch availability and precedence constraints},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of artwork resource management system based on block classification coding and bit plane rearrangement. <em>PEERJCS</em>, <em>11</em>, e3092. (<a href='https://doi.org/10.7717/peerj-cs.3092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the vigorous development of the art market, the management of art resources is confronted with increasingly difficult challenges, such as copyright protection, authenticity verification, and efficient storage. Currently, the digital watermarking and compression schemes applied to artworks struggle to achieve an effective balance among robustness, image quality preservation, and watermark capacity. Moreover, they lack sufficient scalability when dealing with large-scale datasets. To address these issues, this article proposes an innovative algorithm that integrates watermarking and compression for artwork images, namely the Block Classification Coding—Bit Plane Rearrangement—Integrated Compression and Watermark Embedding (BCC-BPR-ICWE) algorithm. By employing refined block classification coding (RS-BCC) and optimized bit plane rearrangement (BPR) techniques, this algorithm significantly enhances the watermark embedding capacity and robustness while ensuring image quality. Experimental results demonstrate that, compared to existing classical algorithms, the proposed method excels in terms of watermarked image quality (PSNR > 57 dB, SSIM = 0.9993), watermark capacity (0.5 bpp), and tampering recovery performance (PSNR = 41.17 dB, SSIM = 0.9993). The research in this article provides strong support for its practical application in large-scale art resource management systems. The proposed technique not only promotes the application of digital watermarking and compression technologies in the field of art management but also offers new ideas and directions for the future development of related technologies.},
  archive      = {J_PEERJCS},
  author       = {Xiaomeng Xia},
  doi          = {10.7717/peerj-cs.3092},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3092},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of artwork resource management system based on block classification coding and bit plane rearrangement},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDSUD: Dynamically detecting subsequence uncertainty and diversity for active learning in imbalanced chinese sentiment analysis. <em>PEERJCS</em>, <em>11</em>, e3091. (<a href='https://doi.org/10.7717/peerj-cs.3091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment structure analysis in Chinese text typically relies on supervised deep-learning methods for sequence labeling. However, obtaining large-scale labeled datasets is both resource-intensive and time-consuming. To address these challenges, this study proposes Dynamically Detecting Subsequence Uncertainty and Diversity (DDSUD), a Bidirectional Encoder Representations from Transformers (BERT)-based active learning framework designed to tackle subsequence uncertainty and enhance the diversity of imbalanced datasets. DDSUD combines subsequence uncertainty detection, diversity-driven sample selection, and dynamic weighting, enabling an adaptive balance between these factors throughout the active learning iterations. Experimental results show that DDSUD achieves performance close to fully supervised training schemes with only 50% of the data labeled, and outperforms other state-of-the-art active learning methods with the same amount of labeled data. Moreover, by dynamically adjusting the trade-off between subsequence uncertainty and diversity, DDSUD demonstrates strong adaptability and generalization capability in low-resource environments, especially in handling imbalanced datasets, significantly improving the recognition of minority class samples.},
  archive      = {J_PEERJCS},
  author       = {Shufeng Xiong and Yibo Si and Guipei Zhang and Bingkun Wang and Guang Zheng and Haiping Si},
  doi          = {10.7717/peerj-cs.3091},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3091},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DDSUD: Dynamically detecting subsequence uncertainty and diversity for active learning in imbalanced chinese sentiment analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of performance evaluation method for higher education reform based on adaptive fuzzy algorithm. <em>PEERJCS</em>, <em>11</em>, e3090. (<a href='https://doi.org/10.7717/peerj-cs.3090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a performance evaluation framework for university teachers based on the adaptive neural fuzzy inference system (ANFIS), aiming to enhance teaching quality and institutional management through a scientific, objective, and comprehensive assessment mechanism. The proposed method begins by developing a robust evaluation index system that integrates key dimensions of academic activity, including teaching performance, research contributions, and fundamental faculty information. A total of 16 sub-indicators are incorporated into the evaluation framework. To optimize data processing and reduce redundancy, factor analysis is applied, simplifying the indicator set while maintaining the integrity and effectiveness of the evaluation process. The core of the system leverages the strengths of both fuzzy logic and neural networks, combining the capacity of fuzzy systems to handle imprecise and uncertain information with the adaptive learning capabilities of neural networks. This hybrid approach improves the accuracy, interpretability, and adaptability of the evaluation results. By continuously optimizing the model using training data, the system dynamically refines its rule base and parameters, eliminating the reliance on manually defined parameters common in traditional fuzzy systems. The effectiveness of the ANFIS-based evaluation model is validated through empirical experiments. The results demonstrate that the proposed model outperforms conventional methods, such as backpropagation (BP) neural networks and support vector machines (SVMs), in terms of accuracy, precision, and overall performance. This research offers a novel and practical approach for evaluating university teacher performance, enabling more accurate reflection of teaching and research outcomes, and providing valuable decision-making support for academic management.},
  archive      = {J_PEERJCS},
  author       = {Dakun Yang and Muhammad Sheraz Arshad Malik},
  doi          = {10.7717/peerj-cs.3090},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3090},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of performance evaluation method for higher education reform based on adaptive fuzzy algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMCFormer (hierarchical multi-scale convolutional transformer): A hybrid CNN+Transformer network for intelligent VIA screening. <em>PEERJCS</em>, <em>11</em>, e3088. (<a href='https://doi.org/10.7717/peerj-cs.3088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer ranks first in incidence among malignant tumors of the female reproductive system, and 80% of women who die from cervical cancer worldwide are from developing countries. Visual inspection with acetic acid (VIA) screening based on artificial intelligence-assisted diagnosis can provide a cheap and rapid screening method. This will attract more low-income women to volunteer for regular cervical cancer screening. However, current AI-based VIA screening studies either have low accuracy or require expensive equipment assistance. In this article, we propose the Hierarchical Multi-Scale Convolutional Transformer network, which combines the hierarchical feature extraction capability of Convolutional Neural Network (CNNs) and the global dependency modeling capability of Transformers to address the challenges of realizing intelligent VIA screening. Hierarchical multi-scale convolutional transformer (HMCFormer) can be divided into a Transformer branch and a CNN branch. The Transformer branch receives unenhanced lesion sample images, and the CNN branch receives lesion sample images enhanced by the proposed dual-color space-based image enhancement algorithm. The authors design a hierarchical multi-scale pixel excitation module for adaptive multi-scale and multi-level local feature extraction. The authors apply the structure of the Swin Transformer network with minor modifications in the global perception modeling process. In addition, the authors propose two feature fusion concepts: adaptive preprocessing and superiority-inferiority fusion, and design a feature fusion module based on these concepts, which significantly improves the collaborative ability of the Transformer branch and the CNN branch. The authors collected and summarized 5,000 samples suitable for VIA screening methods from public datasets provided by companies such as Intel and Google, forming the PCC5000 dataset. On this dataset, the proposed algorithm achieves a screening accuracy of 97.4% and a grading accuracy of 94.8%.},
  archive      = {J_PEERJCS},
  author       = {Bo Feng and Chao Xu and Zhengping Li and Chuanyi Zhang},
  doi          = {10.7717/peerj-cs.3088},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3088},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HMCFormer (hierarchical multi-scale convolutional transformer): A hybrid CNN+Transformer network for intelligent VIA screening},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise injection into freeman chain codes. <em>PEERJCS</em>, <em>11</em>, e3084. (<a href='https://doi.org/10.7717/peerj-cs.3084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel method for direct noise injection into geometric shapes described by eight-or four-directional Freeman chain codes. Noise is applied to randomly selected segments of a chain code sequence using a set of predefined actions. The design of alterations retains topological characteristics of shapes. The method is tested on various shapes, including open, self-intersecting, and simple shapes, among which the latest two may contain holes. Fractal dimension and mean distance from original are utilised in order to analyse the amount of injected noise in sequences of chain codes. The proposed method enables efficient noise injection directly into Freeman chain codes for use in data augmentation and regularization during neural network training.},
  archive      = {J_PEERJCS},
  author       = {Luka Lukač and Andrej Nerat and Damjan Strnad and Ivana Kolingerová and Borut Žalik},
  doi          = {10.7717/peerj-cs.3084},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3084},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Noise injection into freeman chain codes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A path aggregation network with deformable convolution for visual object detection. <em>PEERJCS</em>, <em>11</em>, e3083. (<a href='https://doi.org/10.7717/peerj-cs.3083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges encountered in visual object detection is the multi-scale issue. Many approaches have been proposed to tackle this issue. In this article, we propose a novel neck that can perform effective fusion of multi-scale features for a single-stage object detector. This neck, named the deformable convolution and path aggregation network (DePAN), is an integration of a path aggregation network with a deformable convolution block added to the feature fusion branch to improve the flexibility of feature point sampling. The deformable convolution block is implemented by repeated stacking of a deformable convolution cell. The DePAN neck can be plugged in and easily applied to various models for object detection. We apply the proposed neck to the baseline models of Yolov6-N and YOLOV6-T, and test the improved models on COCO2017 and PASCAL VOC2012 datasets, as well as a medical image dataset. The experimental results verify the effectiveness and applicability in real-world object detection.},
  archive      = {J_PEERJCS},
  author       = {Chengming Rao and Zunhao Hu and QiMing Zhao and Min Shan and Li Mao},
  doi          = {10.7717/peerj-cs.3083},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3083},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A path aggregation network with deformable convolution for visual object detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of ball detection techniques in sports. <em>PEERJCS</em>, <em>11</em>, e3079. (<a href='https://doi.org/10.7717/peerj-cs.3079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting balls in sports plays a pivotal role in enhancing game analysis, providing real-time data for spectators, and improving decision-making and strategic thinking for referees and coaches. This is a highly debated and researched topic, but most works focus on one sport. Effective generalization of a single method or algorithm to different sports is much harder to achieve. This article reviews methodologies and advancements in object detection tailored to ball detection across various sports. Traditional computer vision techniques and modern deep learning methods are visited, emphasizing their strengths, limitations, and adaptability to diverse game scenarios. The challenges of occlusion, dynamic backgrounds, varying ball sizes, and high-speed movements are identified and discussed. This review aims to consolidate existing knowledge, compare state-of-the-art detection models, highlight pivotal challenges and possible solutions, and propose future research directions. The article underscores the importance of optimizations for accurate and efficient ball detection, setting the foundation for next-generation sports analytics systems.},
  archive      = {J_PEERJCS},
  author       = {Cristiano Moreira and Lino Ferreira and Paulo Jorge Coelho},
  doi          = {10.7717/peerj-cs.3079},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3079},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A comprehensive review of ball detection techniques in sports},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware implementation of FPGA-based spiking attention neural network accelerator. <em>PEERJCS</em>, <em>11</em>, e3077. (<a href='https://doi.org/10.7717/peerj-cs.3077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are recognized as third-generation neural networks and have garnered significant attention due to their biological plausibility and energy efficiency. To address the resource constraints associated with using field programmable gate arrays (FPGAs) for numerical recognition in SNNs, we proposed a lightweight spiking efficient attention neural network (SeaSNN) accelerator. We designed a simple, four-layer structured network, achieving a recognition accuracy of 93.73% through software testing on the MNIST dataset. To further enhance the model’s accuracy, we developed a highly spiking efficient channel attention mechanism (SECA), resulting in a significant performance improvement and an increase in test accuracy to 94.28%. For higher recognition speed, we optimized circuit parallelism by applying techniques such as loop unrolling, loop pipelining, and array partitioning. Finally, SeaSNN was implemented and verified on an FPGA board, achieving an inference speed of 0.000401 seconds per frame and a power efficiency of 0.42 TOPS/W at a frequency of 200 MHz. These results demonstrate that the proposed low-power, high-precision, and fast handwritten digit recognition system is well-suited for handwritten digit recognition tasks.},
  archive      = {J_PEERJCS},
  author       = {Shiyong Geng and Zhida Wang and Zhipeng Liu and Mengzhao Zhang and Xuelong Zhu and Yongping Dan},
  doi          = {10.7717/peerj-cs.3077},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3077},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hardware implementation of FPGA-based spiking attention neural network accelerator},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-step partitioning combined with SOM neural network-based clustering technique effectively improves SAT solver performance. <em>PEERJCS</em>, <em>11</em>, e3076. (<a href='https://doi.org/10.7717/peerj-cs.3076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the core engine of electronic design automation (EDA) tools, the efficiency of Boolean Satisfiability Problem (SAT) solver largely determines the cycle of integrated circuit research and development. The effectiveness of SAT solvers has steadily turned into the key bottleneck of circuit design cycle due to the dramatically increased integrated circuit scale. The primary issue of SAT solver now is the divergence between SAT used in industry and research on pure solution algorithms. We propose a strategy for partitioning the SAT problem based on the structural information then solving it. By effectively extracting the structure information from the original SAT problem, the self-organizing map (SOM) neural network deployed in the division section can speed up the sub-thread solver’s processing while avoiding cumbersome parameter adjustments. The experimental results demonstrate the stability and scalability of our technique, which can drastically shorten the time required to solve industrial benchmarks from various sources.},
  archive      = {J_PEERJCS},
  author       = {Siyu Yun and Xinsheng Wang},
  doi          = {10.7717/peerj-cs.3076},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3076},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-step partitioning combined with SOM neural network-based clustering technique effectively improves SAT solver performance},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aligning to the teacher: Multilevel feature-aligned knowledge distillation. <em>PEERJCS</em>, <em>11</em>, e3075. (<a href='https://doi.org/10.7717/peerj-cs.3075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a technique for transferring knowledge from a teacher’s (large) model to a student’s (small) model. Usually, the features of the teacher model contain richer information, while the features of the student model carry less information. This leads to a poor distillation effect in the process of knowledge transfer due to insufficient feature information and poor generalisation ability of the student model. In order to effectively reduce the feature differences between teacher–student models, we propose a Multilevel Feature Alignment Knowledge Distillation method (MFAKD), which includes a spatial dimension alignment module and a multibranch channel alignment (MBCA) module. The upsampling operation enables the student feature map to align with the teacher feature map in spatial dimensions, allowing students to learn more comprehensive teacher features. Moreover, MBCA achieves the alignment of the student feature maps and teacher feature maps on channels, which can transform the student features into a form more similar to the teacher features. Subsequently, the student model uses the discriminative classifiers from the pretrained teacher model to perform the student model inference. In summary, the student model can utilize their strengths and surpass the teacher model. The method was validated on the CIFAR-100 dataset and obtains state-of-the-art results. In particular, the classification accuracy of the student model WRN-40-2 exceeds that of the teacher model ResNet-8×4 by almost 2%. The method also demonstrated excellent performance on the Tiny ImageNet dataset.},
  archive      = {J_PEERJCS},
  author       = {Yang Zhang and Pan He and Chuanyun Xu and Jingyan Pang and Xiao Wang and Xinghai Yuan and Pengfei Lv and Gang Li},
  doi          = {10.7717/peerj-cs.3075},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3075},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Aligning to the teacher: Multilevel feature-aligned knowledge distillation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An image quality assessment algorithm based on ‘global + local’ feature fusion. <em>PEERJCS</em>, <em>11</em>, e3074. (<a href='https://doi.org/10.7717/peerj-cs.3074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been increasing research on image quality assessment. Among the existing mainstream approaches, image feature extraction tends to be simplistic, leading to insufficient quality information extraction and underutilization of the extracted data. Additionally, the correlation between different regions of the image is often neglected. This study proposes an image quality assessment algorithm based on global-local feature fusion (IQA-GL). First, the global and local features of the image are extracted separately, and irrelevant information in the local features is filtered out. Then, a global-local feature fusion model is constructed to enhance the interaction of feature information and gather image quality data across all feature channels. Finally, the relationship between individual image patches and the global image is modeled, adjusting the weights of each image patch to aggregate a quality score for the global image. Experimental results show the IQA-GL performs excellently on public datasets. This study innovatively combines global and local features, offering a new perspective for image quality assessment.},
  archive      = {J_PEERJCS},
  author       = {Yang Yang and Norisma Binti Idris and Ainuddin Wahid Abdul Wahab and Dingguo Yu and Chang Liu},
  doi          = {10.7717/peerj-cs.3074},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3074},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An image quality assessment algorithm based on ‘global + local’ feature fusion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Morphological and structural complexity analysis of low-resource english-turkish language pair using neural machine translation models. <em>PEERJCS</em>, <em>11</em>, e3072. (<a href='https://doi.org/10.7717/peerj-cs.3072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural machine translation (NMT) has achieved remarkable success in high-resource language pairs; however, its effectiveness for morphologically rich and low-resource languages like Turkish remains underexplored. As a highly agglutinative and morphologically complex language with limited high-quality parallel data, Turkish serves as a representative case for evaluating NMT systems on low-resource and linguistically challenging settings. Its structural divergence from English makes it a critical testbed for assessing tokenization strategies, attention mechanisms, and model generalizability in neural translation. This study investigates the comparative performance of two prominent NMT paradigms—the Transformer architecture, and recurrent-based sequence-to-sequence (Seq2Seq) models with attention for both English-to-Turkish and Turkish-to-English translation. The models are evaluated under various configurations, including different tokenization strategies (Byte Pair Encoding (BPE) vs. Word Tokenization), attention mechanisms (Bahdanau and an exploratory hybrid mechanism combining Bahdanau and Scaled Dot-Product attention), and architectural depths (layer count and attention head number). Extensive experiments using automatic metrics such as BiLingual Evaluation Understudy (BLEU), Metric for Evaluation of Translation with Explicit ORdering (METEOR), and Translation Error Rate (TER) reveal that the Transformer model with three layers, eight attention heads, and BPE tokenization achieved the best performance, obtaining a BLEU score of 47.85 and METEOR score of 44.62 in the English-to-Turkish direction. Similar performance trends were observed in the reverse direction, indicating the model’s generalizability. These findings highlight the potential of carefully optimized Transformer-based NMT systems in handling the complexities of morphologically rich, low-resource languages like Turkish in both translation directions.},
  archive      = {J_PEERJCS},
  author       = {Mehmet Acı and Nisa Vuran Sarı and Çiğdem İnan Acı},
  doi          = {10.7717/peerj-cs.3072},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3072},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Morphological and structural complexity analysis of low-resource english-turkish language pair using neural machine translation models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-driven insights into arab media’s sustainable development goals coverage. <em>PEERJCS</em>, <em>11</em>, e3071. (<a href='https://doi.org/10.7717/peerj-cs.3071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines how Arab media have engaged with the United Nations Sustainable Development Goals (SDGs) over the past decade and evaluates the alignment between media coverage and official government priorities. The research addresses the lack of large-scale, Arabic-focused analyses in SDG discourse, which is often dominated by English-language studies. We collected and processed a unique dataset of over 1.2 million Arabic news articles from ten countries between 2010 and 2024. Using a combination of data augmentation, deep learning (specifically, Transformer-based models), and large language models (LLMs), we trained classifiers to detect references to the SDGs and categorize articles by specific SDGs. The results reveal regional patterns in SDG coverage, with North African countries focusing more on governance-related goals, while Gulf countries emphasize economic and environmental themes. Our findings reveal a general alignment between media discourse and official SDG priorities, with notable exceptions. This study is the first to combine artificial intelligence (AI) methods and Arabic media at this scale for SDG analysis, offering new tools and insights for policymakers, media professionals, and development stakeholders.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Alsuhaibani and Kamel Gaanoun and Ali Mustafa Qamar},
  doi          = {10.7717/peerj-cs.3071},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3071},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Artificial intelligence-driven insights into arab media’s sustainable development goals coverage},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multifeature fusion for claim scope-aware litigation risk prediction for patent drafts. <em>PEERJCS</em>, <em>11</em>, e3069. (<a href='https://doi.org/10.7717/peerj-cs.3069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ‘claim scope’, or the ‘legal boundaries’ defined by patent claims, has been considered crucial for determining a patent’s value and its associated litigation risk. However, no direct claim semantics-based indicators currently exist to quantify patent claim scope, and existing scope measures are primarily indirect, which limits their ability to capture the semantic nuances of claim text. Additionally, the reliance on post-grant features restricts the applicability of existing litigation prediction models to patent drafts. These limitations complicate the patent drafting process, during which claims are formulated without feedback on scope and litigation risk. This often leads to suboptimal claim articulation, resulting in inadequate protection, increased legal vulnerabilities, or reduced patent grant probability. To address this gap, the hyponym tree score (HTS) is proposed as a novel indicator for quantifying claim scope by analysing hyponym counts, sentence structure, and dependency relations within patent claims. Building on this, early-stage litigation risk prediction has been achieved using a new deep learning model, the Multifeature BERT-Powered Fusion for Author-level Patent Litigation Risk Analysis (MAPRA). The MAPRA model restricts its input features to those available at early stages, such as indicators derived from claim text, inventor information, assignee details, and HTS, ensuring applicability to both draft-stage and granted patents. Despite excluding all post-grant or acquired data, MAPRA achieves a superior area under the receiver operating characteristic curve (AUC) of 0.878, outperforming the most comparable prior study, which reports an AUC of 0.822 using both early-stage and immediate post-grant features. By quantifying claim scope and enabling early-stage litigation risk prediction, this research offers a valuable screening tool for patent drafters, examiners, attorneys, and innovators. It supports informed decision-making during drafting and helps mitigate potential litigation risks. Furthermore, it lays a foundation for future research on claim scope modeling and the development of predictive tools for intellectual property litigation management.},
  archive      = {J_PEERJCS},
  author       = {Chitrakala Sakthivel and Jinesh Jose},
  doi          = {10.7717/peerj-cs.3069},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3069},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multifeature fusion for claim scope-aware litigation risk prediction for patent drafts},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identity-based linear homomorphic signature for a restricted combiners’ group for e-commerce. <em>PEERJCS</em>, <em>11</em>, e3068. (<a href='https://doi.org/10.7717/peerj-cs.3068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the volume of electronic transaction data increases and the demand for real-time processing grows, network coding techniques have become popular for improving performance. However, early implementations often overlooked critical data security issues, such as forgery and data leakage. While existing homomorphic signature schemes effectively ensure data integrity, they can unintentionally allow malicious actors to exploit intermediate signatures. This misuse can lead to unnecessary bandwidth consumption and hinder the verification processes for legitimate users. To address the problem of malicious combinations, we apply the Chinese Remainder theorem (CRT) to establish a layer of secret-sharing that restricts access to authorized users in conjunction with homomorphic signatures. Furthermore, we introduce a formal definition for an identity-based linear homomorphic signature for a restricted combiners’ group (IBLHS-RCG). This framework integrates linear homomorphic signatures with the CRT within the context of e-commerce, enabling us to develop a specialized scheme for IBLHS-RCG. We demonstrate that our scheme is unforgeable against adaptive chosen-message attacks. Additionally, simulations conducted using the Python Pairing-based Cryptography Library (PYPBC) show that the signing and verification costs of our approach are low.},
  archive      = {J_PEERJCS},
  author       = {Yuan Tian and Weitao Song and Tanping Zhou and Bin Hu and Xuan Zhou and Yujie Ding and Weidong Zhong and Xiaoyuan Yang},
  doi          = {10.7717/peerj-cs.3068},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3068},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Identity-based linear homomorphic signature for a restricted combiners’ group for e-commerce},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A social information sensitive model for conversational recommender systems. <em>PEERJCS</em>, <em>11</em>, e3067. (<a href='https://doi.org/10.7717/peerj-cs.3067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommender systems (CRS) facilitate natural language interactions for more effective item suggestions. While these systems show promise, they face challenges in effectively utilizing and integrating informative data with conversation history through semantic fusion. In this study we present an innovative framework for extracting social information from conversational datasets by inferring ratings and constructing user-item interaction and user-user relationship graphs. We introduce a social information sensitive semantic fusion (SISSF) method that employs contrastive learning (CL) to bridge the semantic gap between generated social information and conversation history. We evaluated the framework on two public datasets (ReDial and INSPIRED) using both automatic and human evaluation metrics. Our SISSF framework demonstrated significant improvements over baseline models across all metrics. For the ReDial dataset, SISSF achieved superior performance in recommendation tasks (R@1: 0.062, R@50: 0.437) and conversational quality metrics (Distinct-2: 4.223, Distinct-3: 5.595, Distinct-4: 6.155). Human evaluation showed marked improvement in both fluency (1.81) and informativeness (1.63). We observed similar performance gains on the INSPIRED dataset, with notable improvements in recommendation accuracy (R@1: 0.046, R@10: 0.129, R@50: 0.269) and response diversity (Distinct-2: 2.061, Distinct-3: 4.293, Distinct-4: 6.242). The experimental results consistently validate the effectiveness of our approach in both recommendation and conversational tasks. These findings suggest that incorporating social context through CL can significantly improve the personalization and relevance of recommendations in conversational systems.},
  archive      = {J_PEERJCS},
  author       = {Abdulaziz Mohammed and Mingwei Zhang and Gehad Abdullah Amran and Husam M. Alawadh and Ruizhe Wang and Amerah Alabrah and Ali A. Al-Bakhrani},
  doi          = {10.7717/peerj-cs.3067},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3067},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A social information sensitive model for conversational recommender systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of streaming data anomaly detection in network security. <em>PEERJCS</em>, <em>11</em>, e3066. (<a href='https://doi.org/10.7717/peerj-cs.3066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity has always been a subject of great concern, and anomaly detection has gained increasing attention due to its ability to detect novel attacks. However, network anomaly detection faces significant challenges when dealing with massive traffic, logs, and other forms of streaming data. This article provides a comprehensive review and a multi-faceted analysis of recent algorithms for anomaly detection in network security. It systematically categorizes and elucidates the various types of datasets, measurement techniques, detection algorithms, and output results of streaming data. Furthermore, the review critically compares network security application scenarios and problem-solving capabilities of streaming data anomaly detection methods. Building on this analysis, the study identifies and delineates promising future research directions. This article endeavors to achieve rapid and efficient detection of streaming data, thereby providing better security for network operations. This research is highly significant in addressing the challenges and difficulties of analyzing anomalies in streaming data. It also serves as a valuable reference for further development in the field of network security. It is anticipated that this comprehensive review will serve as a valuable resource for security researchers in their future investigations within network security.},
  archive      = {J_PEERJCS},
  author       = {Pengju Zhou},
  doi          = {10.7717/peerj-cs.3066},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3066},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A survey of streaming data anomaly detection in network security},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving machine learning detection of alzheimer disease using enhanced manta ray gene selection of alzheimer gene expression datasets. <em>PEERJCS</em>, <em>11</em>, e3064. (<a href='https://doi.org/10.7717/peerj-cs.3064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most prominent neurodegenerative diseases globally is Alzheimer’s disease (AD). The early diagnosis of AD is a challenging task due to complex pathophysiology caused by the presence and accumulation of neurofibrillary tangles and amyloid plaques. However, the late enriched understanding of the genetic underpinnings of AD has been made possible due to recent advancements in data mining analysis methods, machine learning, and microarray technologies. However, the “curse of dimensionality” caused by the high-dimensional microarray datasets impacts the accurate prediction of the disease due to issues of overfitting, bias, and high computational demands. To alleviate such an effect, this study proposes a gene selection approach based on the parameter-free and large-scale manta ray foraging optimization algorithm. Given the dimensional disparities and statistical relationship distributions of the six investigated datasets, in addition to four evaluated machine learning classifiers; the proposed Sign Random Mutation and Best Rank enhancements that substantially improved MRFO’s exploration and exploitation contributed to efficient identification of relevant genes and to machine learning improved prediction accuracy.},
  archive      = {J_PEERJCS},
  author       = {Zahraa Ahmed and Mesut Çevik},
  doi          = {10.7717/peerj-cs.3064},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3064},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving machine learning detection of alzheimer disease using enhanced manta ray gene selection of alzheimer gene expression datasets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced text clustering and sentiment analysis framework for online education: A BIF-DCN approach in computer education. <em>PEERJCS</em>, <em>11</em>, e3062. (<a href='https://doi.org/10.7717/peerj-cs.3062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding students’ emotional responses to course content and assignments is crucial for developing effective teaching strategies and improving online learning resources. To address this need, we propose a novel deep learning-based framework called BERT and BTF-IDF Integrated Framework with Deep Clustering Network (BIF-DCN), designed to accurately analyze student sentiment on educational platforms. The framework combines three key components: Bidirectional Encoder Representations from Transformers (BERT) for initial text feature extraction, Bi-level Term Frequency–Inverse Document Frequency (BTF-IDF) for enhanced feature representation, and an Improved Deep Embedded Clustering (IDEC) model for sentiment classification. BERT captures rich semantic features from student comments, which are further refined using BTF-IDF to highlight informative terms. These features are then clustered using the IDEC model to identify underlying sentiment-based topics. Experimental results show that BIF-DCN achieves higher clustering accuracy than existing IDEC-based and traditional single-model approaches on both public and self-constructed datasets. In addition to performance improvements, our method enables in-depth sentiment analysis of clustered topics, offering practical insights for optimizing teaching materials. This framework provides educators with valuable tools to better understand student needs and deliver more personalized and effective instruction, ultimately enhancing teaching quality and learner satisfaction.},
  archive      = {J_PEERJCS},
  author       = {Qingyun Zhang and Yang Li and Muhammad Sheraz Arshad Malik},
  doi          = {10.7717/peerj-cs.3062},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3062},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced text clustering and sentiment analysis framework for online education: A BIF-DCN approach in computer education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GaitTriViT and GaitVViT: Transformer-based methods emphasizing spatial or temporal aspects in gait recognition. <em>PEERJCS</em>, <em>11</em>, e3061. (<a href='https://doi.org/10.7717/peerj-cs.3061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image recognition tasks, subjects with long distances and low resolution remain a challenge, whereas gait recognition, identifying subjects by walking patterns, is considered one of the most promising biometric technologies due to its stability and efficiency. Previous gait recognition methods mostly focused on constructing a sophisticated model structure for better model performance during evaluation. Moreover, these methods are primarily based on traditional convolutional neural networks (CNNs) due to the dominance of CNNs in computer vision. However, since the alternative form of Transformer, named Vision Transformers (ViTs), has been introduced into the computer vision field, the ViTs have gained strong attention for its outstanding performance in various tasks. Thus, unlike previous methods, this project introduces two Transformer-based methods: a completely ViTs-based method GaitTriViT, and a Video Vision Transformer (Video ViT) based method GaitVViT. The GaitTriViT leverages the ViTs to gain more fine-grained spatial features, while GaitVViT enhances the capacity of temporal extraction. This work evaluates their performances and the results show the still-existing gaps and several encouraging outperforms compared with current state-of-the-art (SOTA), demonstrating the difficulties and challenges these Transformer-based methods will encounter continuously. However, the future of Vision Transformers in gait recognition is still promising.},
  archive      = {J_PEERJCS},
  author       = {Hongyun Sheng},
  doi          = {10.7717/peerj-cs.3061},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3061},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GaitTriViT and GaitVViT: Transformer-based methods emphasizing spatial or temporal aspects in gait recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-guided RGB-P grasp generation. <em>PEERJCS</em>, <em>11</em>, e3060. (<a href='https://doi.org/10.7717/peerj-cs.3060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of robotics, object grasping is a complex and challenging task. Although state-of-the-art computer vision-based models have made significant progress in predicting grasps, the lack of semantic information from textual data makes them susceptible to ambiguities in object recognition. For example, when asked to grasp a specific object on a table with many objects, robots relying only on visual data can easily get confused and grasp the wrong object. To address this limitation, we propose a multimodal approach that seamlessly integrates 3D data (shape) and red-green-blue (RGB) images (color, texture) into a unified representation called red-green-blue and point cloud (RGB-P), while also incorporating semantic information from textual descriptions processed by a large language model (LLM) to enhance object disambiguation. This combination of data allows our model to accurately infer and capture target objects based on natural language descriptions, overcoming the limitations of vision-only approaches. Our approach achieves superior performance, with an average precision (AP) of 53.2% on the GraspNet-1Billion dataset, significantly outperforming state-of-the-art methods. Additionally, we introduce an automated dataset creation pipeline that addresses the challenges of data collection and annotation. This pipeline leverages cutting-edge models: LLMs for text generation, Stable Diffusion for image synthesis, Depth Anything for depth estimation, using standard intrinsic parameters from the Kinect depth sensor to ensure geometric consistency, and GraspNet for grasp estimation. This automated process generates high-quality datasets with paired RGB-P, images, textual descriptions and potential grasp poses, significantly reducing the manual effort and enabling large-scale data collection.},
  archive      = {J_PEERJCS},
  author       = {Van Duc Vu and Van Thiep Nguyen and Nam Hai Pham and Dinh-Cuong Hoang and Phan Xuan Tan},
  doi          = {10.7717/peerj-cs.3060},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3060},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Text-guided RGB-P grasp generation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDMU-net: 3D multi-dimensional decoupled multi-scale U-net for pancreatic cancer segmentation. <em>PEERJCS</em>, <em>11</em>, e3059. (<a href='https://doi.org/10.7717/peerj-cs.3059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pancreatic cancer, as a highly lethal malignant tumor, presents significant challenges for early diagnosis and treatment. Accurate segmentation of the pancreas and tumors is crucial for surgical planning and treatment strategy development. However, due to the variable morphology, blurred boundaries, and low contrast with surrounding tissues in CT images, traditional manual segmentation methods are inefficient and heavily reliant on expert experience. To address this challenge, this study proposes a lightweight automated 3D segmentation algorithm—Multi-Dimensional Decoupled Multi-Scale U-Net (MDMU-Net). First, depthwise separable convolution is employed to reduce model complexity. Second, a multi-dimensional decoupled multi-scale module is designed as the primary encoder module, which independently extracts features along depth, height, and width dimensions through parallel multi-scale convolutional kernels, achieving fine-grained modeling of complex anatomical structures. Finally, cross-dimensional channel and spatial attention mechanisms are introduced to enhance recognition capability for small tumors and blurred boundaries. Experimental results on the MSDPT and NIHP datasets demonstrate that MDMU-Net exhibits competitive advantages in both pancreatic segmentation DSC (0.7108/0.7709) and tumor segmentation DSC (showing an 11.8% improvement over AttentionUNet), while achieving a 15.3% enhancement in HD95 boundary accuracy compared to 3DUX-Net. While maintaining clinically viable precision, the model significantly improves computational efficiency, with parameter count (26.97M) and FLOPs (84.837G) reduced by 65.5% and 71%, respectively, compared to UNETR, providing reliable algorithmic support for precise diagnosis and treatment of pancreatic cancer.},
  archive      = {J_PEERJCS},
  author       = {Lian Lu and Miao Wu and Gan Sen and Fei Ren and Tao Hu},
  doi          = {10.7717/peerj-cs.3059},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3059},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MDMU-net: 3D multi-dimensional decoupled multi-scale U-net for pancreatic cancer segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A defensive model and implementation baseline for the metaverse and extended reality systems. <em>PEERJCS</em>, <em>11</em>, e3054. (<a href='https://doi.org/10.7717/peerj-cs.3054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metaverse and extended reality (XR) systems are vulnerable to emerging security threats, as developers have prioritized competitive business gains over security. The virtual entities, immersive experiences, and lack of centralized governance pose significant challenges in establishing standardized guidelines for XR systems and its stakeholders. In this research, a panoramic view is presented to identify mitigation strategies and defensive capabilities, including authenticity, privacy, integrity, interoperability, virtual forensics, and incident reporting to counter potential threats. To facilitate the implementation of a secure XR system, a novel baseline model is introduced, outlining key attributes and functions aligned with the available libraries. A statistical analysis is performed to assess the quality and effectiveness of development resources in embedding novel XR security features. Furthermore, this research assesses the security posture of prominent XR systems and examines the applicable regulatory frameworks in immersive environment. Finally, security recommendations are proposed to counter the threat landscape of XR and the metaverse.},
  archive      = {J_PEERJCS},
  author       = {Sara Qamar and Hasan Tahir and Zahid Anwar and Naveed Ahmed and Shahzaib Tahir and Muhammad Aleem},
  doi          = {10.7717/peerj-cs.3054},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3054},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A defensive model and implementation baseline for the metaverse and extended reality systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDoS attack detection in edge-IIoT digital twin environment using deep learning approach. <em>PEERJCS</em>, <em>11</em>, e3052. (<a href='https://doi.org/10.7717/peerj-cs.3052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The industrial Internet of Things (IIoT) and digital twins are redefining how digital models and physical systems interact. IIoT connects physical intelligence, and digital twins virtually represent their physical counterparts. With the rapid growth of Edge-IIoT, it is crucial to create security and privacy regulations to prevent vulnerabilities and threats (i.e., distributed denial of service (DDoS)). DDoS attacks use botnets to overload the target system with requests. In this study, we introduce a novel approach for detecting DDoS attacks in an Edge-IIoT digital twin-based generated dataset. The proposed approach is designed to retain already learned knowledge and easily adapt to new models in a continuous manner without retraining the deep learning model. The target dataset is publicly available and contains 157,600 samples. The proposed models M1, M2, and M3 obtained precision scores of 0.94, 0.93, and 0.93; recall scores of 0.91, 0.97, and 0.99; F1-scores of 0.93, 0.95, and 0.96; and accuracy scores of 0.93, 0.95, and 0.96, respectively. The results demonstrated that transferring previous model knowledge to the next model consistently outperformed baseline approaches.},
  archive      = {J_PEERJCS},
  author       = {Feras Al-Obeidat and Adnan Amin and Ahmed Shuhaiber and Inam ul Haq},
  doi          = {10.7717/peerj-cs.3052},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3052},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DDoS attack detection in edge-IIoT digital twin environment using deep learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic generation of explanations in autonomous systems: Enhancing human interaction in smart home environments. <em>PEERJCS</em>, <em>11</em>, e3041. (<a href='https://doi.org/10.7717/peerj-cs.3041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In smart environments, autonomous systems often adapt their behavior to the context, and although such adaptations are generally beneficial, they may cause users to struggle to understand or trust them. To address this, we propose an explanation generation system that produces natural language descriptions (explanations) to clarify the adaptive behavior of smart home systems in runtime. These explanations are customized based on user characteristics and the contextual information derived from the user interactions with the system. Our approach leverages a prompt-based strategy using a fine-tuned large language model, guided by a modular template that integrates key data such as the type of explanation to be generated, user profile, runtime system information, interaction history, and the specific nature of the system adaptation. As a preliminary step, we also present a conceptual model that characterize explanations in the domain of autonomous systems by defining their core concepts. Finally, we evaluate the user experience of the generated explanations through an experiment involving 118 participants. Results show that generated explanations are perceived positive and with high level of acceptance.},
  archive      = {J_PEERJCS},
  author       = {Oscar Peña-Cáceres and Antoni Mestre and Manoli Albert and Vicente Pelechano and Miriam Gil},
  doi          = {10.7717/peerj-cs.3041},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3041},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automatic generation of explanations in autonomous systems: Enhancing human interaction in smart home environments},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of methods and software for polygenic risk score analysis. <em>PEERJCS</em>, <em>11</em>, e3039. (<a href='https://doi.org/10.7717/peerj-cs.3039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polygenic risk scores (PRSs) are emerging as powerful tools for predicting individual susceptibility to various diseases and traits based on genetic variants. These scores integrate information from multiple genetic markers associated with the trait or disease of interest, offering personalized risk assessment and enhancing disease management strategies. PRS is an active area of research and is being studied in various fields, such as disease prediction. This review explores the advancement of PRS research, focusing on methodological approaches, software tools, and applications across diverse disciplines. A systematic literature review identified 40 relevant articles classified based on PRS methods and software. Key methods for PRS computation, including penalized regression and threshold-based approaches, Bayesian approaches, and machine learning approaches, are discussed, along with notable software and their features. Applications of PRS in disease prevention are highlighted. Challenges and future directions, such as increasing diversity in genetic data, integrating environmental factors, and evaluating clinical implications, are also discussed to guide future research and implementation efforts.},
  archive      = {J_PEERJCS},
  author       = {Sara Benoumhani and Areej Al-Wabil and Niddal Imam and Bashayer Alfawaz and Amaan Zubairi and Dalal Aldossary and Mariam AlEissa},
  doi          = {10.7717/peerj-cs.3039},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3039},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A review of methods and software for polygenic risk score analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated multi-task feature learning and interactive active optimization for scene retargeting in preschool educational applications. <em>PEERJCS</em>, <em>11</em>, e3035. (<a href='https://doi.org/10.7717/peerj-cs.3035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In artificial intelligence (AI), effective adaptation of educational imagery across diverse screen formats is essential, particularly in preschool education, where visual content must simultaneously engage and instruct young learners. This study introduces a novel scene retargeting model tailored to preserve pedagogically significant visual elements during image resizing. The proposed framework leverages the binarized normed gradients (BING) objectness metric to efficiently identify and prioritize key regions within educational images, such as objects and facial features. A core component of our approach is integrating a locality-preserved and interactive active optimization (LIAO) mechanism, which simulates human visual attention by generating gaze shift paths (GSPs) that guide feature prioritization. These GSPs are further transformed into hierarchical deep features using a multi-layer representation, followed by refinement through a Gaussian mixture model (GMM) to enhance scene understanding and retargeting fidelity. Experimental evaluations demonstrate that the proposed model not only surpasses five state-of-the-art methods in performance but also achieves a 3% improvement in accuracy compared to the next-best approach, all while reducing inference time by over 50%. The results confirm the model’s effectiveness and efficiency, offering a robust solution for educational content adaptation that aligns with cognitive and pedagogical requirements in early childhood learning environments.},
  archive      = {J_PEERJCS},
  author       = {Suhui Yao and Lan Lv},
  doi          = {10.7717/peerj-cs.3035},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3035},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Integrated multi-task feature learning and interactive active optimization for scene retargeting in preschool educational applications},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on surface reconstruction based on 3D gaussian splatting. <em>PEERJCS</em>, <em>11</em>, e3034. (<a href='https://doi.org/10.7717/peerj-cs.3034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface reconstruction is a foundational topic in computer graphics and has gained substantial research interest in recent years. With the emergence of advanced neural radiance fields (NeRFs) and 3D Gaussian splatting (3D GS), numerous innovative many novel algorithms for 3D model surface reconstruction have been developed. The rapid expansion of this field presents challenges in tracking ongoing advancements. This survey aims to present core methodologies for the surface reconstruction of 3D models and establish a structured roadmap that encompasses 3D representations, reconstruction methods, datasets, and related applications. Specifically, we introduce 3D representations using 3D Gaussians as the central framework. Additionally, we provide a comprehensive overview of the rapidly evolving surface reconstruction methods based on 3D Gaussian splatting. We categorize the primary phases of surface reconstruction algorithms for 3D models into scene representation, Gaussian optimization, and surface structure extraction. Finally, we review the available datasets, applications, and challenges and suggest potential future research directions in this domain. Through this survey, we aim to provide valuable resources that support and inspire researchers in the field, fostering advancements in 3D reconstruction technologies.},
  archive      = {J_PEERJCS},
  author       = {Zheng Xu and Gang Chen and Feng Li and Lingyu Chen and Yuanhang Cheng},
  doi          = {10.7717/peerj-cs.3034},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3034},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A survey on surface reconstruction based on 3D gaussian splatting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing reliable and energy-efficient UAV communications with RIS and deep reinforcement learning. <em>PEERJCS</em>, <em>11</em>, e3031. (<a href='https://doi.org/10.7717/peerj-cs.3031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth in wireless communication demands has led to a surge in research on technologies capable of enhancing communication reliability, coverage, and energy efficiency. Among these, uncrewed aerial vehicles (UAV) and reconfigurable intelligent surfaces (RIS) have emerged as promising solutions. Prior research on using deep reinforcement learning (DRL) to integrate RIS with UAV concentrated on enhancing signal quality and coverage, but it ignored the challenges caused by electromagnetic interference (EMI). This article introduces a novel framework addressing the challenges posed by EMI from Gallium nitride (GaN) power amplifiers in RIS-assisted UAV communication systems. By integrating DRL with quadrature phase shift keying (QPSK) modulation, the proposed system dynamically optimizes UAV deployment and RIS configurations in real-time, mitigating EMI effects, improving signal-to-interference-plus-noise ratio (SINR), and enhancing energy efficiency. The framework demonstrates superior performance, with an SINR improvement of up to 6.5 dB in interference-prone environments, while achieving a 38% increase in energy efficiency compared to baseline models. Additionally, the system significantly reduces EMI impact, with a mitigation rate of over 70%, and extends coverage area by 35%. The integration of QPSK and DRL allows for real-time decision-making that balances communication quality and energy consumption. These results show the system’s potential to outperform traditional methods, particularly in dynamic and challenging environments such as urban, disaster recovery, and remote settings.},
  archive      = {J_PEERJCS},
  author       = {Wasim Ahmad and Umar Islam and Abdulkadhem A. Abdulkadhem and Babar Shah and Fernando Moreira and Ali Abbas},
  doi          = {10.7717/peerj-cs.3031},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3031},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing reliable and energy-efficient UAV communications with RIS and deep reinforcement learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of offensive content in the kazakh language using machine learning and deep learning approaches. <em>PEERJCS</em>, <em>11</em>, e3027. (<a href='https://doi.org/10.7717/peerj-cs.3027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the urgent need to detect destructive content, including religious extremism, racism, cyberbullying, and nation oriented extremism messages, on social media platforms in the Kazakh language. Given the agglutinative structure and rich morphology of Kazakh, standard natural language processing (NLP) models require significant adaptation. The study employs a range of machine learning and deep learning techniques, such as logistic regression, support vector machines (SVM), and long short-term memory (LSTM) networks, to classify destructive content. This article demonstrates the effectiveness of combining n-gram and stemming methods with machine learning algorithms, achieving high accuracy in content classification. The findings underscore the importance of developing language-specific NLP tools tailored to Kazakh’s linguistic complexities. This research not only contributes to ensuring online safety by detecting destructive content in Kazakh digital spaces, but also provides a framework for applying similar techniques to other lesser-resourced languages.},
  archive      = {J_PEERJCS},
  author       = {Milana Bolatbek and Moldir Sagynay and Shynar Mussiraliyeva and Zhastay Yeltay},
  doi          = {10.7717/peerj-cs.3027},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3027},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Detection of offensive content in the kazakh language using machine learning and deep learning approaches},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative multi objective optimization based automatic fake news detection. <em>PEERJCS</em>, <em>11</em>, e3016. (<a href='https://doi.org/10.7717/peerj-cs.3016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the digital revolution, access to information is expanding day by day and individuals can access information quickly through the internet and social media platforms. However, in most cases, there is no mechanism in place to evaluate the accuracy of news that spreads rapidly on social media. This increases the potential for fake news to mislead both individuals and society. In order to minimize the negative effects of fake news, it has become a critical necessity to detect them quickly and effectively. Metaheuristic methods can provide more effective solutions in fake news detection compared to traditional methods. Especially in small datasets, metaheuristics are known to produce faster and more effective solutions than artificial intelligence and machine learning based methods. In the literature, the majority of fake news detection studies have focused on the optimization of a single criterion. In this study, unlike other studies, a method that enables simultaneous optimization of two criteria (precision and recall) in fake news detection is developed. In the proposed approach, an innovative solution is presented by using the Crowding Distance Level method instead of the Crowding Distance method used in the standard Non-dominated Sorting Genetic Algorithm 2 (NSGA-2) algorithm. The proposed method is tested on four different datasets such as Covid-19, Syrian war daily news and FakeNewsNet (Gossipcop). The results show that the proposed method achieves high success especially on small datasets.},
  archive      = {J_PEERJCS},
  author       = {Cebrail Barut and Suna Yildirim and Bilal Alatas and Gungor Yildirim},
  doi          = {10.7717/peerj-cs.3016},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3016},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Innovative multi objective optimization based automatic fake news detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting temperature and rainfall using deep learning for the challenging climates of northern india. <em>PEERJCS</em>, <em>11</em>, e3012. (<a href='https://doi.org/10.7717/peerj-cs.3012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate temperature and rainfall (T&R) forecasting is vital for the climate-sensitive regions of Northern India, particularly Jammu, Kashmir, and Ladakh, where volatile weather patterns significantly affect livelihoods, socio-economic development, and disaster management efforts. Despite their importance, traditional forecasting methods often fall short due to their high computational demands and inability to provide localized, real-time predictions, leaving a critical research gap in addressing these challenges. This study addresses the need for precise and efficient T&R forecasting using deep learning-based framework tailored to the unique climatic conditions of these regions. The major research focus is to develop and evaluate a model capable of capturing complex temporal dependencies in localized time-series weather data. Utilizing data from the Indian Meteorological Department (IMD) for Jammu, Srinagar, and Ladakh stations covering the period from January 1, 2000, to December 31, 2023, the proposed framework employs recurrent neural networks (RNN) and long short-term memory (LSTM) architectures, both optimized for time-series forecasting. Key findings reveal that while both RNN and LSTM models exhibit robust performance in single input single output (SISO) setups, RNN model consistently outperforms the LSTM in capturing intricate temporal relationships. The RNN model in MIMO configuration achieved significantly lower mean absolute error (MAE), root mean squared error (RMSE), and mean squared error (MSE) for Jammu, Srinagar, and Ladakh, with respective values of [0.0636, 0.1011, 0.0401] for Jammu, [0.1048, 0.1555, 0.0455] for Srinagar, and [0.0854, 0.1344, 0.0411] for Ladakh. These results underscore the RNN model’s precision, making it a practical tool for real-time weather forecasting. By enhancing the accuracy of T&R predictions in regions with challenging meteorological conditions, this study contributes to improved climate adaptation strategies, disaster preparedness, and sustainable development. Its findings hold broader implications for advancing localized forecasting technologies in other regions with similar climatic complexities.},
  archive      = {J_PEERJCS},
  author       = {Syed Nisar Hussain Bukhari and Kingsley A. Ogudo},
  doi          = {10.7717/peerj-cs.3012},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3012},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Forecasting temperature and rainfall using deep learning for the challenging climates of northern india},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Next generation sequencing under attack: Investigating insider threats and organizational behaviour. <em>PEERJCS</em>, <em>11</em>, e3008. (<a href='https://doi.org/10.7717/peerj-cs.3008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next generation sequencing (NGS) has become a cornerstone of modern genomics, enabling high-throughput analysis of DNA and RNA with wide applications across medicine, research, and biotechnology. However, the growing adoption of NGS technologies has introduced significant cyber-biosecurity risks, particularly those arising from insider threats and organizational shortcomings. While technical vulnerabilities have received attention, the human and behavioral dimensions of cybersecurity in NGS environments remain underexplored. This study investigates the role of human factors and organizational behavior in shaping cyber-biosecurity risks in NGS workflows. A mixed-method approach was employed, combining survey data from 120 participants across four countries with statistical analyses including chi-square tests, cross-tabulations, and cluster analysis. The study assessed cybersecurity training availability, employee engagement, training effectiveness, and awareness of insider threats. Findings reveal substantial gaps in training frequency and participation, with 36% of respondents reporting no access to NGS-specific cybersecurity training. Only a minority of participants felt confident in detecting cyber threats, and 32.5% had never applied cybersecurity knowledge in practice. Chi-square results indicate significant associations between training frequency and threat recognition, training relevance, and knowledge application. Cluster analysis further categorized organizations into “robust,” “moderate,” and “emergent” cybersecurity maturity profiles. The study offers an evidence-based framework to enhance cyber-biosecurity in NGS settings by addressing human-centric risks. It recommends role-specific training, frequent policy updates, and improved organizational communication to mitigate insider threats. These insights support the development of targeted interventions and policies to strengthen the cybersecurity culture in genomics organizations.},
  archive      = {J_PEERJCS},
  author       = {Nasreen Anjum and Hani Alshahrani and Darakhshan Syed and Asadullah Shaikh and Mahreen Ul Hassan},
  doi          = {10.7717/peerj-cs.3008},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3008},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Next generation sequencing under attack: Investigating insider threats and organizational behaviour},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acute lymphoblastic leukemia cancer diagnosis in children and adults using transforming blood fluorescence microscopy imaging. <em>PEERJCS</em>, <em>11</em>, e2997. (<a href='https://doi.org/10.7717/peerj-cs.2997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leukemia is a highly aggressive kind of cancer that may impact the bone marrow. The most fatal type acute lymphoblastic leukemia (ALL), is characterized by the excessive growth of immature white blood cells in the bone marrow. For diagnostic purposes, hematologists and experts use a state-of-the-art microscope fitted with a high-powered magnifying lens to analyze blood and bone marrow samples. Experts attribute the rapid progress to the presence of adolescent white blood cells, not fully developed ones. A good treatment for ALL, no matter where it comes from, includes chemotherapy, medication given through a transplant. Experts have difficulties in accurately evaluating explosive cell features due to the onerous and time-consuming nature of manual diagnosis for this disease. A total of 89 individuals suspected of having ALL underwent sample collection, resulting in the acquisition of 3,256 images. The dataset is classified into four different types of cancer: early-stage, benign cells, pre-cancerous cells, and pro-cancer cells. The proposed approach employs several preprocessing and augmentation techniques to improve the results. The studies demonstrate that the technique achieved a recall rate of 100% for the pro-cell cancer subtype, an overall accuracy of 98.67% using enhanced data, and an overall accuracy of 97.87% using the original data. The experiments have shown that the proposed equipment is superior in reliability and accuracy compared to existing approaches, and it facilitates early detection in medical imaging.},
  archive      = {J_PEERJCS},
  author       = {Amjad Rehman and Muhammad Mujahid and Tanzila Saba and Faten S. Alamri and Noor Ayesha},
  doi          = {10.7717/peerj-cs.2997},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2997},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Acute lymphoblastic leukemia cancer diagnosis in children and adults using transforming blood fluorescence microscopy imaging},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LPITutor: An LLM based personalized intelligent tutoring system using RAG and prompt engineering. <em>PEERJCS</em>, <em>11</em>, e2991. (<a href='https://doi.org/10.7717/peerj-cs.2991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Development of large language models (LLMs) has transformed the landscape of personalized education through intelligent tutoring systems (ITS) which responds to diverse learning requirements. This article proposed a model named LLM based Personalized Intelligent Tutoring System (LPITutor) that is based on LLM for personalized ITS that leverages retrieval-augmented generation (RAG) and advanced prompt engineering techniques to generate customized responses aligned with students’ requirements. The aim of LPITutor is to provide customized learning content that adapts to different levels of learners skills and question complexity. The performance of proposed model was evaluated on accuracy, completeness, clarity, difficulty alignment, coherence, and relevance. The finding of LPITutor indicates that it effectively balances the response accuracy and clarity with significant alignment to the difficulty level of student queries. The proposed work also emphasises the broader implications of artificial intelligence (AI)-driven ITS in education and presents future directions for improving the adaptation and optimization of LPITutor.},
  archive      = {J_PEERJCS},
  author       = {Zhensheng Liu and Prateek Agrawal and Saurabh Singhal and Vishu Madaan and Mohit Kumar and Pawan Kumar Verma},
  doi          = {10.7717/peerj-cs.2991},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2991},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {LPITutor: An LLM based personalized intelligent tutoring system using RAG and prompt engineering},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk assessment based on a new decision-making approach with fermatean fuzzy sets. <em>PEERJCS</em>, <em>11</em>, e2990. (<a href='https://doi.org/10.7717/peerj-cs.2990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background This study presents a new approach to decision-making based on the selection of decision-makers according to evaluated criteria in multi-criteria decision-making (MCDM) methods. Therefore, sub-decision-maker groups (SDMGs) are created for each evaluated criterion. The SDMG approach, which is created according to the criteria, offers a more flexible and dynamic structure than the existing approaches. This approach aims to use the expertise and knowledge of decision-makers more effectively. The decision-making approach presented in this study offers an innovative model and adds a new dimension to decision-making processes. This decision-making approach is applied to the plastic injection moulding machine risk assessment, as it involves different criteria. In addition to classical risk parameters such as probability, severity, frequency, and detectability, new parameters such as human error, machine error, and existing safety measures are also used in the risk assessment. Methods The integration of the analytic hierarchy process (AHP) and the technique for order preference by similarity to ideal solution (TOPSIS) methods into the interval valued fermatean fuzzy set (IVFFS) environment makes an important contribution to a more comprehensive consideration of risks and uncertainties in the risk assessment process. The IVFF-AHP method is used to weight the risk parameters and determine the hazard scores, and the TOPSIS method is used to rank the hazards. A holistic and systematic approach to risk assessment has been achieved by integrating these two methods. Modelling of these methods is carried out using MATLAB_R2024a software. Results According to the evaluated criteria, it was concluded that the determination of the decision makers separately is applicable to the decision-making process. Identifying the existing safety measures parameter as the most important risk parameter emphasizes the central role of this factor in risk assessment. In addition, machine error and human error parameters are also found to be important in risk assessment. These parameters, which are used for the first time in the literature, offer a broader perspective than traditional methods and provide significant advantages in risk assessment. According to the evaluations, electricity, asphyxiating and toxic gases, and hot water use are determined as the most risky hazards. The sensitivity and comparative analysis performed in the study confirm that the proposed methodology produces consistent and reasonable results.},
  archive      = {J_PEERJCS},
  author       = {Hilal Biderci and Ali F. Guneri},
  doi          = {10.7717/peerj-cs.2990},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2990},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Risk assessment based on a new decision-making approach with fermatean fuzzy sets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRDAGE: A prescription recommendation framework for traditional chinese medicine based on data augmentation and multi-graph embedding. <em>PEERJCS</em>, <em>11</em>, e2974. (<a href='https://doi.org/10.7717/peerj-cs.2974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The prescriptions of traditional chinese medicine (TCM) have made a great contribution to the treatment of disease and the maintenance of good health. Current research on prescription recommendations mainly focuses on the correlation between symptoms and herbs. However, the semantic information inherent in both symptoms and herbs has received limited attention. Furthermore, most datasets in the field of TCM suffer from limited data volumes, which can adversely impact model training. Methods To tackle these challenges, we present a prescription recommendation framework called PRDAGE, which is based on data augmentation and multi-graph embedding. We started by collecting medical records and creating a dataset of 3,052 classic medical cases, where we normalized the symptoms and herbs. Additionally, we developed a multi-layer embedding method for symptoms and herbs, using Sentence Bert (SBert) and graph convolutional networks. The aim of this multi-layer embedding method is to capture and represent the semantic information of symptoms and herbs, as well as the complex relationships between them. Additionally, a median-based random data augmentation method was introduced to enrich the medical case data, effectively enhancing the model’s accuracy. Results The model was evaluated against baseline models on an unenhanced dataset (Dataset-B), and the results showed that the proposed PRDAGE framework exhibited superior overall performance. Compared to the second-best model, PRDAGE achieved improvements in accuracy and recall rates of 1.69% and 3.80%, respectively, on the Top@10 metric. Ablation experiments further revealed that both the data augmentation and multi-layer embedding modules contributed to the improved model performance. Conclusion In conclusion, the experimental results suggest that PRDAGE is an effective prescription recommendation framework. The multi-layer embedding approach effectively represents the semantic information of symptoms and the complex relationships between symptoms and herbs. Additionally, the use of median-based data augmentation has a positive impact on the overall performance and generalization ability of the model.},
  archive      = {J_PEERJCS},
  author       = {Zhihua Wen and Yunchun Dong and Lihong Peng and Longxin Zhang and Junfeng Yan},
  doi          = {10.7717/peerj-cs.2974},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2974},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {PRDAGE: A prescription recommendation framework for traditional chinese medicine based on data augmentation and multi-graph embedding},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMAY-net: Adaptive multi-scale attention YOLO network for liver and gallbladder segmentation in laparoscopic cholecystectomy. <em>PEERJCS</em>, <em>11</em>, e2961. (<a href='https://doi.org/10.7717/peerj-cs.2961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel liver and gallbladder segmentation framework, named Adaptive Multi-Scale Attention YOLO Network (AMAY-Net), designed for semantic segmentation of laparoscopic cholecystectomy images. Building upon the powerful feature extraction capabilities of You Only Look Once (YOLO), AMAY-Net incorporates several advanced modules to enhance performance in medical image segmentation tasks. First, a multi-scale feature extraction module is employed to capture anatomical structures of various sizes, ensuring effective detection of large organs like the liver and smaller structures such as the gallbladder and surgical instruments. Second, an adaptive class-balancing loss function is implemented to dynamically adjust the weights of underrepresented classes, improving the segmentation accuracy of small structures. Additionally, the network integrates a spatial and channel attention mechanism, enhancing the focus on critical regions in the image. Finally, residual connections are introduced in the YOLO backbone to improve feature propagation and gradient flow efficiency. Experimental results demonstrate that AMAY-Net achieves superior performance on the CholecSeg8k dataset, with significant improvements in the segmentation accuracy of key anatomical structures such as the liver and gallbladder.},
  archive      = {J_PEERJCS},
  author       = {Yuyang Zhou and Yulai You and Xiaokai Tan and Juncheng Tang},
  doi          = {10.7717/peerj-cs.2961},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2961},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AMAY-net: Adaptive multi-scale attention YOLO network for liver and gallbladder segmentation in laparoscopic cholecystectomy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning for digital twin applications: A privacy-preserving and low-latency approach. <em>PEERJCS</em>, <em>11</em>, e2877. (<a href='https://doi.org/10.7717/peerj-cs.2877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital twin (DT) concept has recently gained widespread application for mapping the state of physical entities, enabling real-time analysis, prediction, and optimization, thereby enhancing the management and control of physical systems. However, when sensitive information is extracted from physical entities, it faces potential leakage risks, as DT service providers are typically honest yet curious. Federated learning (FL) offers a new distributed learning paradigm that protects privacy by transmitting model updates from edge servers to local devices, allowing training on local datasets. Nevertheless, the training parameters communicated between local mobile devices and edge servers may contain raw data that malicious adversaries could exploit. Furthermore, variations in mapping bias across local devices and the presence of malicious clients can degrade FL training accuracy. To address these security and privacy threats, this paper proposes the FL-FedDT scheme—a privacy-preserving and low-latency FL method that employs an enhanced Paillier homomorphic encryption algorithm to safeguard the privacy of local device parameters without transmitting data to the server. Our approach introduces an improved Paillier encryption method with a new hyperparameter and pre-calculates multiple random intermediate values during the key generation stage, significantly reducing encryption time and thereby expediting model training. Additionally, we implement a trusted FL global aggregation method that incorporates learning quality and interaction records to identify and mitigate malicious updates, dynamically adjusting weights to counteract the threat of malicious clients. To evaluate the efficiency of our proposed scheme, we conducted extensive experiments, with results validating that our approach achieves training accuracy and security on par with baseline methods, while substantially reducing FL iteration time. This enhancement contributes to improved DT mapping and service quality for physical entities. (The code for this study is publicly available on GitHub at: https://github.com/fujianU/federated-learning. The URL address of the MNIST dataset is: https://gitcode.com/Resource-Bundle-Collection/d47b0/overview?utm_source=pan_gitcode&index=top&type=href&;.)},
  archive      = {J_PEERJCS},
  author       = {Jie Li and Dong Wang},
  doi          = {10.7717/peerj-cs.2877},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2877},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Federated learning for digital twin applications: A privacy-preserving and low-latency approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
