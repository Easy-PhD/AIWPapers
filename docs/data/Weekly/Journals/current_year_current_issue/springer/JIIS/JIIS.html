<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JIIS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jiis">JIIS - 16</h2>
<ul>
<li><details>
<summary>
(2025). Correction to: Evaluating user story quality with LLMs: A comparative study. <em>JIIS</em>, <em>63</em>(4), 1453-1454. (<a href='https://doi.org/10.1007/s10844-025-00948-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JIIS},
  author       = {Sharma, Amol and Tripathi, Anil Kumar},
  doi          = {10.1007/s10844-025-00948-2},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1453-1454},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Correction to: Evaluating user story quality with LLMs: A comparative study},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating user story quality with LLMs: A comparative study. <em>JIIS</em>, <em>63</em>(4), 1423-1451. (<a href='https://doi.org/10.1007/s10844-025-00939-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the quality of user stories is crucial for the success of agile software development. This paper investigates the efficacy of Large Language Models (LLMs) in assessing the quality of individual user stories using the Quality User Story (QUS) framework, which categorizes quality criteria into syntactic, semantic, and pragmatic dimensions. Leveraging three state-of-the-art LLMs—GPT-4o, GPT-4-Turbo, and GPT-3.5-Turbo—this study employs two prompting strategies: context minimal and context rich, to gauge performance across eight user story quality criteria. To ensure robust validation, we generated 960 user stories using alternative LLMs (Gemini and Meta AI’s LLaMA3), which were then assessed for quality by 69 postgraduate students. The quality assessments were further verified by a team comprising a research scholar and a senior postgraduate student. The evaluation of these 960 user stories by the three LLMs under study reveal significant insights into their relative strengths and weaknesses. The results demonstrate that GPT-4o and GPT-4-Turbo exhibit superior performance in evaluating user stories, particularly excelling in syntactic and pragmatic criteria with minimal impact from additional contextual details. Conversely, GPT-3.5-Turbo reveals noticeable limitations, struggling to maintain effectiveness, particularly when handling richer contextual inputs. This research marks a pivotal step towards automated quality assessment in requirements engineering, highlighting both the potential and areas for improvement in leveraging LLMs for robust user story evaluation.},
  archive      = {J_JIIS},
  author       = {Sharma, Amol and Kumar Tripathi, Anil},
  doi          = {10.1007/s10844-025-00939-3},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1423-1451},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Evaluating user story quality with LLMs: A comparative study},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rumor detection for emergency events via few-shot ensembled prompt learning. <em>JIIS</em>, <em>63</em>(4), 1391-1422. (<a href='https://doi.org/10.1007/s10844-025-00944-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rumors on social media can cause panic during emergency events. Unlike conventional rumor detection, it is more challenging to detect rumors about emergency events that have not happened in history, due to the shortage of relevant corpus. Therefore, we treat emergency events rumor detection as a few-shot learning problem. Recently, large language models (LLMs) such as ChatGPT have been widely considered in various NLP tasks. However, LLMs may face limitations due to non-real-time knowledge and unwillingness to provide direct answers. Prompt learning effectively leverages pre-trained language models (PLMs), and prompt tuning can inject the latest knowledge into PLMs with very few instances. Therefore, we propose a template-based Ensembled Prompt Tuning (EPT) model. We contribute an approach leveraging the knowledge in PLMs to generate label words for verbalizer construction. Furthermore, we treat few-shot rumor detection as an MLM problem and design two types of prompt templates for online posts and comments. An ensemble strategy is introduced to make the final prediction. Experimental results on three datasets demonstrate the effectiveness of the proposed EPT model, outperforming current SOTA on accuracy and F1-score. Ablation study has shown the necessity and effectiveness of the ensemble strategy. Besides, we make comparisons with prevalent LLMs under both few-shot and zero-shot settings, the results show the competitiveness of our EPT model.},
  archive      = {J_JIIS},
  author       = {Su, Chen and Zhou, Junkang and Jiang, Zhentao and Zhu, Shuwei and Li, Chao and Fang, Wei and Lu, Heng-yang},
  doi          = {10.1007/s10844-025-00944-6},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1391-1422},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Rumor detection for emergency events via few-shot ensembled prompt learning},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying influential nodes using semi local isolating centrality based on average shortest path. <em>JIIS</em>, <em>63</em>(4), 1361-1390. (<a href='https://doi.org/10.1007/s10844-025-00943-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex networks, identifying influential nodes becomes critical as these networks emerge rapidly. Extensive studies have been carried out on intricate networks to comprehend diverse real-world networks, including transportation networks, facebook networks, animal social networks, etc. Centrality measures like degree, betweenness, closeness, and clustering centralities are used to find influential nodes, but these measures have limitations in implementation with large-scale networks. These centrality measures are classified into global and local centralities. Semi-local structures perform well compared to local and global centralities but efficient centrality for finding influential nodes remains a challenging issue in large-scale networks. To address this challenge, a Semi-Local Average Isolating Centrality (SAIC) metric is proposed that integrates semi-local and local information to identify important nodes in large networks, along with the relative change in average shortest path. Here, we consider extended neighborhood concept for selecting the nodes nearest neighbors along with the weighted edge policy to find the best influential nodes by using SAIC. Along with these, SAIC also consider isolated nodes which significantly impact the network connectedness by maximizing the number of connected components upon removal. As a result SAIC differentiates itself from other centrality metrics by employing a distributed approach to define semi-local structure and utilizing an efficient edge weighting policy. The analysis of SAIC has been performed on multiple real-time datasets using Kendall tau’s coefficient. Using the Susceptible-Infected-Recovered (SIR) and Independent Cascade(IC) models, the performance of SAIC has been examined to determine maximum information spread in comparison to the most recent metrics in some real-world datasets. Our proposed method SAIC performs better in terms of information spreading when compare with other exisiting methods, with an improvement ranging from 4.11% to 17.9%.},
  archive      = {J_JIIS},
  author       = {Madupuri, ReddyPriya and C.C, Sobin and Enduri, Murali Krishna and Anamalamudi, Satish},
  doi          = {10.1007/s10844-025-00943-7},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1361-1390},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Identifying influential nodes using semi local isolating centrality based on average shortest path},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid transformer based model for sarcasm detection from news headlines. <em>JIIS</em>, <em>63</em>(4), 1339-1359. (<a href='https://doi.org/10.1007/s10844-025-00941-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis becomes significantly challenging when there are traces of sarcastic words or phrases present in text. A common finding of the researchers working on sentiment analysis is that the presence of sarcastic comments escalates the complexity of sentiment analysis drastically. Consequently, it is a common practice to detect the sarcastic elements in a text prior to performing the sentiment analysis so as to increase the accuracy of the same. Sarcastic phrases often have an intrinsic linguistic pattern which the research community is working on during the last few years with an objective to identify them in a generically. But most of the cases the researchers used either rule based or machine learning-driven approach for the detection of sarcasm present in text. The scope of application of deep learning for identifying sarcastic comments is principally due to the scarcity of dataset. In order to bridge the gap, this article presents a novel hybrid transformer based approach that leverages the strengths of RoBERTa, Bidirectional Long Short-Term Memory (Bi-LSTM), and Multi-Head Attention in order to enhance the efficiency of sarcasm detection especially from News Headlines. Our approach combines contextual embeddings, sequential modelling and attention mechanisms to effectively capture the nuances of sarcastic expressions in text. Experimental results demonstrate that our model achieves competitive accuracy and efficiency, providing a robust solution for sarcasm detection tasks.},
  archive      = {J_JIIS},
  author       = {Khan, Amit and Majumdar, Dipankar and Mondal, Bikromadittya},
  doi          = {10.1007/s10844-025-00941-9},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1339-1359},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {A hybrid transformer based model for sarcasm detection from news headlines},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating textual explanations for scheduling systems leveraging the reasoning capabilities of large language models. <em>JIIS</em>, <em>63</em>(4), 1287-1337. (<a href='https://doi.org/10.1007/s10844-025-00940-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling systems are critical for planning projects, resources, and activities across many industries to achieve goals efficiently. As scheduling requirements grow in complexity, the use of Artificial Intelligence (AI) solutions has received more attention. However, providing comprehensible explanations of these decision-making processes remains a challenge and blocker to adoption. The emergent field of eXplainable Artificial Intelligence (XAI) aims to address this by establishing human-centric interpretation of influencing factors for machine decisions. The leading field of autonomous interpretation in Natural Language Processing (NLP) is Large Language Model (LLM)s, for their generalist knowledge and reasoning capabilities. To explore LLMs’ potential to generate explanations for scheduling queries, we selected a benchmark set of Job Shop scheduling problems. A novel framework that integrates the selected language models, GPT-4 and Large Language Model Meta AI (LLaMA), into scheduling systems is introduced, facilitating human-like explanations to queries from different categories through few-shot learning. The explanations were analysed for accuracy, consistency, completeness, conciseness, and language across different scheduling problem sizes and complexities. The approach achieved an overall accuracy of 59% with GPT-4 and 35% with LLaMA, with minimal impact from the varied schedule sizes observed, proving the approach can handle different datasets and is performance scalable. Several responses demonstrated high comprehension of complex queries; however, response quality fluctuated due to the few-shot learning approach. This study establishes a baseline for measuring generalist LLM capabilities in handling explanations for autonomous scheduling systems, with promising results for an LLM providing XAI interactions to explain scheduling decisions.},
  archive      = {J_JIIS},
  author       = {Powell, Cheyenne and Riccardi, Annalisa},
  doi          = {10.1007/s10844-025-00940-w},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1287-1337},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Generating textual explanations for scheduling systems leveraging the reasoning capabilities of large language models},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MF-EDNet: Predicting stock market sector indices based on multi-feature fusion under emergency events. <em>JIIS</em>, <em>63</em>(4), 1265-1286. (<a href='https://doi.org/10.1007/s10844-025-00938-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impact of emergency events on the stock market cannot be underestimated, as their unpredictability poses significant challenges to investors’ stock operations. This calls for researchers and investors to seek more effective features and reasonable methods to mitigate risks. In the context of multi-feature prediction methods, analyzing the correlation between multi-dimensional features or data has always been a challenging issue. This paper proposes a stock market index prediction framework based on an encoder-decoder architecture (MF-EDNet). The framework leverages the dynamic correlation between stock data and futures data as prior knowledge, integrating features of both internal sequences (industry indices) and external sequences (futures data) to capture the impact of emergency events on the stock market. The newly proposed Multi-Dimensional Convolutional Attention Module (MCAM) further enhances the feature extraction and attention capabilities of the attention mechanism. Experiments on multiple industry indices in the Chinese stock market demonstrate that MF-EDNet can effectively extract important features from stock and futures data, exhibiting good predictive performance under emergency events. The proposed MF-EDNet model achieved improvements of 35.8% and 22.9% in the Matthews correlation coefficient (MCC), a 3.3% increase in accuracy (ACC) and a 7.86% enhancement in profit compared to previous state-of-the-art methods.},
  archive      = {J_JIIS},
  author       = {Han, Tianjiao and Yuan, Chenxun and Wang, Pengcheng and Hao, Xingwei and Guo, Fenghua},
  doi          = {10.1007/s10844-025-00938-4},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1265-1286},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {MF-EDNet: Predicting stock market sector indices based on multi-feature fusion under emergency events},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid transformer-CNN architecture for multivariate time series forecasting: Integrating attention mechanisms with convolutional feature extraction. <em>JIIS</em>, <em>63</em>(4), 1233-1264. (<a href='https://doi.org/10.1007/s10844-025-00937-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately forecasting time series data remains a critical challenge, particularly in financial markets where volatility and noise obscure underlying patterns. Traditional deep learning approaches often struggle to simultaneously capture local and global dependencies, limiting their effectiveness in detecting directional changes. To address these challenges, we propose an innovative hybrid model that integrates Transformers with 1D Convolutional Neural Networks (1D-CNN), leveraging their complementary strengths. The self-attention mechanism of Transformers enhances the model’s ability to capture long-term dependencies, while 1D-CNN excels at extracting local patterns and refining feature representations. Unlike conventional models that aim to predict exact values, our approach is explicitly designed to learn and detect changes in trends rather than forecasting precise numerical values. The primary motivation of this work is to improve Directional Accuracy (DA) and Signal Directional Change Detection, two critical factors for robust time series forecasting in financial applications. Our model effectively mitigates the impact of market fluctuations by enhancing trend detection and reducing false signals. Performance evaluation is conducted using specialized metrics, including DA, Trend Consistency Index (TCI), Signal Shift Error (SSE), and Precision of Trend Change (PTC), ensuring a comprehensive assessment of the model’s predictive capabilities. Experimental results demonstrate that our hybrid model significantly outperforms both custom and state-of-the-art architectures, achieving superior accuracy in detecting trend reversals and signal shifts. This research contributes to advancing time series modeling by introducing a modular, scalable, and high-precision forecasting framework, applicable across various domains.},
  archive      = {J_JIIS},
  author       = {El Zaar, Abdellah and Mansouri, Amine and Benaya, Nabil and Bakir, Toufik and El Allati, Abderrahim},
  doi          = {10.1007/s10844-025-00937-5},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1233-1264},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Hybrid transformer-CNN architecture for multivariate time series forecasting: Integrating attention mechanisms with convolutional feature extraction},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective optimization approach for session-based recommendation systems. <em>JIIS</em>, <em>63</em>(4), 1203-1232. (<a href='https://doi.org/10.1007/s10844-025-00935-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems face the persistent challenge of balancing multiple conflicting objectives, such as relevance, diversity, and user engagement, while adapting to the complexities of session-based data. Traditional methods often struggle to address these challenges effectively, particularly when user interactions are diverse, sparse, or structured in short sessions. Moreover, the trade-offs between accuracy-focused metrics and diversity-oriented metrics pose additional hurdles in achieving well-rounded recommendations. This paper presents a novel session-based multi-objective recommendation approach designed to address these challenges. The method employs session clustering to group similar behavioral patterns, reducing complexity and enhancing focus. Within each cluster, focused item subsetting refines the recommendation space, enabling efficient identification of high-performing solutions. Additionally, Differential Evolution (DE)-based optimization facilitates a balance between competing objectives, while cross-cluster knowledge sharing accelerates convergence and enhances generalization by transferring effective strategies between session clusters. Extensive experiments conducted on real-world datasets demonstrate that the proposed approach consistently outperforms state-of-the-art session-based and multi-objective optimization-based recommender systems. These results highlight its ability to adapt to diverse interaction patterns and effectively balance conflicting objectives, making it a practical and scalable solution for modern recommendation systems.},
  archive      = {J_JIIS},
  author       = {Zaizi, Fatima Ezzahra and Qassimi, Sara and Rakrak, Said},
  doi          = {10.1007/s10844-025-00935-7},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1203-1232},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {A multi-objective optimization approach for session-based recommendation systems},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-enhanced representation learning for graph collaborative filtering recommendation models. <em>JIIS</em>, <em>63</em>(4), 1179-1202. (<a href='https://doi.org/10.1007/s10844-025-00933-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph collaborative filtering recommendation models have gained significant attention in recommender systems due to their ability to capture complex user-item relationships through interaction graphs. However, these models often overlook the rich information contained in textual and tabular data, which can provide valuable insights into user preferences and item characteristics. To address this limitation, we propose a model-agnostic multi-task representation learning framework, LLMGCF, which aims to enhance the performance of graph-based recommendation models by integrating textual data enriched by large language models (LLMs) and feature-engineered tabular data. Specifically, our framework leverage contrastive learning to align semantic signals derived from LLM-enhanced textual data, attribute-based tabular signals, and collaborative graph signals, enabling effective cross-modal knowledge fusion. Additionally, LLMGCF utilizes multi-task learning to jointly optimize the supervised recommendation retrieval task and the cross-modal knowledge alignment task. Experimental results on two public datasets demonstrate that LLMGCF outperforms state-of-the-art (SOTA) graph collaborative filtering models, achieving average improvements of approximately 4.17% in Recall and 3.68% in NDCG. Furthermore, our framework exhibits strong robustness against random noise, highlighting its practical applicability in real-world scenarios.},
  archive      = {J_JIIS},
  author       = {Mou, Daen and Wei, Zhihua and Ni, Lin and Song, Na and Sun, Yiwei and Chu, Weizhong and Jin, Benkai},
  doi          = {10.1007/s10844-025-00933-9},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1179-1202},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {LLM-enhanced representation learning for graph collaborative filtering recommendation models},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distant supervised relation extraction with label entailment and collaborative denoising. <em>JIIS</em>, <em>63</em>(4), 1153-1177. (<a href='https://doi.org/10.1007/s10844-025-00932-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distant supervision automatically generates large-scale annotated data for relation extraction by aligning texts with knowledge bases, reducing the dependence on human annotation. However, distant supervision relation extraction inevitably introduces label noise, including false positive (FP) noise caused by neglecting sentence meanings and false negative (FN) noise due to the incompleteness of knowledge bases. Previous sentence-level methods mainly focus on the FP noise and ignore the FN noise, which induces severe misleading in both training and testing procedures. To address this issue, we propose a novel two-stage sentence-level noise reduction framework that explicitly tackles both the FN and FP problems. At stage one, we perform noise-filtering with label entailment, which filters out the FN noise before training through semantic matching between the negative instance and every relation label. At stage two, we propose robust training with collaborative denoising, which dynamically removes FP noise during training by maintaining two relation classifiers simultaneously and enabling them to learn useful knowledge from each other. Experimental results show that our method achieves significant improvements over previous state-of-the-art methods on two widely-used benchmarks. For example, a 2.55% F1 score improvements on NYT-10 dataset with BiLSTM implementation is achieved. Moreover, we validate the effectiveness of our method in reducing both the FN and FP noise.},
  archive      = {J_JIIS},
  author       = {Xie, Tingyu and Li, Qi and Wang, Gaoang and Wang, Hongwei},
  doi          = {10.1007/s10844-025-00932-w},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1153-1177},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Distant supervised relation extraction with label entailment and collaborative denoising},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-training dual-network for denoising federated recommendation. <em>JIIS</em>, <em>63</em>(4), 1129-1151. (<a href='https://doi.org/10.1007/s10844-025-00930-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated recommendation (FR) offers users personalized recommendation services while safeguarding their privacy. Conventional FR considers all items that users interact with as items that users like. However, this assumption fails to capture genuine user preferences due to some noisy samples, in which users interact with items they do not like. Most research in FR disregards such noisy samples, consequently inducing local models to learn inaccurate user preferences. Since the global model is aggregated from local models, its performance is compromised, adversely affecting user experience. Furthermore, data heterogeneity, privacy protection, and communication burden requirements make denoising FR more complicated. In this paper, we propose a Self-Training Dual-Network (STDFed) for denoising FR. Specifically, each client’s global and local models inherently form a dual-network. On each client, STDFed considers samples with high predicted probabilities from the dual-network as clean samples to construct a self-training dataset, while the remaining are treated as unlabeled samples. The self-training dataset replaces the original dataset to support local training. In subsequent rounds, STDFed identifies the unlabeled samples with low predicted probabilities as noisy samples, which will be either discarded or labeled as 0 and then added to the self-training dataset. STDFed is model-agnostic and can be applied to most FR. Extensive experiments show that STDFed improves the performance of FR by an average of 6.96% across multiple datasets without increasing communication costs.},
  archive      = {J_JIIS},
  author       = {Liu, Pingshan and He, Haoning and Lu, Guoxin},
  doi          = {10.1007/s10844-025-00930-y},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1129-1151},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Self-training dual-network for denoising federated recommendation},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantification of part-of-speech relationships for aspect sentiment triplet extraction. <em>JIIS</em>, <em>63</em>(4), 1105-1127. (<a href='https://doi.org/10.1007/s10844-025-00929-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect Sentiment Triple Extraction (ASTE) is a fine-grained aspect-level sentiment analysis aimed at extracting aspect words, opinion words, and the sentiment polarity between them in text. Existing end-to-end ASTE models tend to ignore information such as the potential lexical relationships between words and their differences in importance, and fail to take full advantage of the interaction between lexical features and dependency features. To address the above problem, we propose a Quantification of Part-of-speech Relationships for Aspect Sentiment Triplet Extraction (QoPR-ASTE). The model first extracts contextual semantic features by combinatorial encoder; then analyzes the connection and dependency between lexical pairs of words to mine potential syntactic information, and assigns weights according to the importance, constructs lexical adjacency weight matrices and dependency type weight matrices, so as to remove the redundant information and strengthen the expression of syntactic features; then extracts the syntactic information by the two-group graphical convolutional network and the syntactic interaction module and fusion; finally, the semantic and syntactic information is fused through the multi-head attention graph transformation module, and the sentence is labeled using word pair relations and decoded to extract the triad. The F1 values of this model on the four public datasets demonstrate an improvement of 0.80%, 1.46%, 0.92%, and 0.78% compared to the PBLUN model, respectively. The experimental results indicate that the model is capable of efficiently mining potential semantic and syntactic information, thereby significantly enhancing the accuracy of triplet extraction.},
  archive      = {J_JIIS},
  author       = {Wang, Jiacan and Liu, Jianhua and Ke, Tianci and Chen, Kewei and Cai, Zijie and Xu, Ge},
  doi          = {10.1007/s10844-025-00929-5},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1105-1127},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Quantification of part-of-speech relationships for aspect sentiment triplet extraction},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble time series models for stock price prediction and portfolio optimization with sentiment analysis. <em>JIIS</em>, <em>63</em>(4), 1079-1103. (<a href='https://doi.org/10.1007/s10844-025-00928-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The work introduces a hybrid ensemble model that combines conventional stock market prediction models with sentiment analysis of news articles in order to improve the accuracy of predictions. By utilizing the advantages of Long Short-Term Memory(LSTM), Gated Recurrent Unit(GRU), Bidirectional LSTM (BiLSTM), and Recurrent Neural Network(RNN) models in an ensemble framework, we attain an impressive average prediction accuracy of 91.89% where our model was evaluated on ten stocks and surpassed the performance of current models. This outcome underscores the need to integrate news sentiment with technical indicators to get a thorough comprehension of market dynamics. Moreover, the proposed model-driven portfolio regularly outperforms the Nifty 50 benchmark at different risk tolerance levels (0.3, 0.5, and 0.7), generating a stable positive alpha. This indicates greater returns when adjusted for risk. The model’s ability to adapt to the varying needs of investors is demonstrated by the performance it achieved across risk profiles. The proposed model is also compared with the existing models to show the model’s efficiency.},
  archive      = {J_JIIS},
  author       = {Narayana, Malineni Lakshmi and Kartha, Arundhati J and Mandal, Ankur Kumar and P, Roshini and Suresh, Akshaya and Jose, Arun Cyril},
  doi          = {10.1007/s10844-025-00928-6},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1079-1103},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Ensemble time series models for stock price prediction and portfolio optimization with sentiment analysis},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multi-round retrieval with knowledge distillation for sequential recommendation. <em>JIIS</em>, <em>63</em>(4), 1055-1077. (<a href='https://doi.org/10.1007/s10844-025-00926-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential Recommendation (SR) involves predicting the next item that a user is likely to interact with based on their historical interactions. SR models examine the sequence of a user’s actions to analyze complex behavioral patterns and capture diverse user preferences. However, existing works primarily rely on a single-round inference paradigm, which limits their ability to capture the ever-changing diversity of user preferences, and overlooks the influence of user noisy interactions. In this work, we propose MRKD, an adaptive multi-round retrieval framework for sequential recommendation via past-future knowledge distillation. MRKD comprises three key modules: user-wise translator, item-wise translator and past-future knowledge distillation. User-wise and item-wise translator extract meaningful context information from multi-round retrieval processes for refining the representations of items and users in proximity to the target item. The past-future knowledge distillation is to supervise the contextual aggregation process and prevent information loss via distilling valuable knowledge from users’ future interactions. We conduct experiments on five datasets and compare MRKD with 10 competitive baselines to evaluate its performance. Experimental results demonstrate the superiority of our MRKD, equipped with the adaptive multi-round retrieval strategy, over existing state-of-the-art models.},
  archive      = {J_JIIS},
  author       = {Mo, Yuhua and Liu, Yang and Ye, Chaowen and Cheng, Zhangtao and Deng, Chao and Zhuo, Zhencheng and Chen, Kaidi and Zhou, Fan},
  doi          = {10.1007/s10844-025-00926-8},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1055-1077},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Adaptive multi-round retrieval with knowledge distillation for sequential recommendation},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network translations for building SentiWordNets. <em>JIIS</em>, <em>63</em>(4), 1033-1054. (<a href='https://doi.org/10.1007/s10844-024-00911-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A SentiWordNet (SWN) is a WordNet, in which the synsets are annotated with sentiment scores. The initial steps for building a new SWN in a target language are similar to those for building a new WordNet. In particular, the creator may translate existing SWNs or WordNets to the target language or expand a seed sentiment lexicon. The next step involves training classifiers to identify the sentiment of the synset members. A major issue in building a new SWN is the lack of language resources for translating existing SWNs or WordNets to the target language and creating a dataset for training sentiment classifiers. Bilingual dictionaries are reliable aids in translation, but are expensive, may not be available, and time-consuming to construct. With the rapid development of artificial neural networks, machine translation systems have become commonplace and effective for a significant number of language pairs from around the world. Creating datasets for training machine translation models is also arguably cheaper than constructing bilingual dictionaries from scratch. This paper proposes effective approaches for constructing new SWNs using neural network translation systems. We introduce strategies for selecting synset members from translation candidates and computing sense-orders for these synset members. Our approaches are able to construct a new SWN in any language if the language is supported by machine translators.},
  archive      = {J_JIIS},
  author       = {Lam, Khang Nhut and Le, Trung Phuong and Ngu, Khanh Cong and Le, Kien Trung and Le, Phuc Minh and Nguyen, Huy Hoang-Dang and Kalita, Jugal},
  doi          = {10.1007/s10844-024-00911-7},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1033-1054},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Neural network translations for building SentiWordNets},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
