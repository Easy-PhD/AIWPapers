<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva">MVA - 12</h2>
<ul>
<li><details>
<summary>
(2025). ConcatPhys: A dual-channel path data concatenation network for robust remote heart rate estimation. <em>MVA</em>, <em>36</em>(6), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01737-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial video-based Blood Volume Pulse (BVP) signal extraction technology has demonstrated significant potential in remote health monitoring. However, most current methods are susceptible to interference from lighting changes and have limited generalization ability in dynamic or complex environments. This paper proposes a dual-channel path data concatenation network called ConcatPhys to improve the accuracy and robustness of remote heart rate (HR) estimation. First, a region-focus (RF) block is introduced to concentrate on spatial attention mechanisms, focusing on physiologically relevant regions. This approach effectively uncovers subtle local feature changes and suppresses irrelevant features, reducing sensitivity to background noise and lighting variations. Second, a dual-path framework is constructed for remote photoplethysmography (rPPG) signal prediction. By incorporating dual-path frequency-domain consistency loss and adjacent-frame similarity loss, the network’s anti-interference capability against illumination variations such as lighting changes is strengthened. Finally, leveraging the temporal correlation between adjacent video frames over short intervals, three consecutive feature image segments are concatenated. By averaging the HR values of these three adjacent segments, the video-level HR is computed. This approach enables efficient reconstruction of rPPG signals and accurate HR estimation using only a 6-s facial video segment. Experimental results demonstrate that ConcatPhys achieves state-of-the-art performance across multiple public datasets (VIPL-HR, OBF, UBFC-rPPG), highlighting its significant potential for remote health monitoring applications.},
  archive      = {J_MVA},
  author       = {Ge, Xiaorui and Xing, Jiahe and Li, Bin},
  doi          = {10.1007/s00138-025-01737-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ConcatPhys: A dual-channel path data concatenation network for robust remote heart rate estimation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the potential of deep learning techniques for analyzing athlete movements in competitive athletics sports. <em>MVA</em>, <em>36</em>(6), 1-20. (<a href='https://doi.org/10.1007/s00138-025-01733-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research work, a Deep Learning (DL) approach utilizing Spatial Transformer, Temporal Transformer, and Collaborative Movement Centric (CMC) module is presented to classify sports event based on athlete movements in competitive sports. Primarily, the Spatial Transformer utilizing the Spatial Feature extraction module (SFEM) is introduced to extract detailed spatial information from video frames. Here, the SFEM employs Modulated Moving Average Graph Convolutional Network (MMA-GCN) to extract complex spatial relationships by learning the knowledge from offset and modulation parameters. Secondly, the Temporal Transformer is designed that employs Temporal Feature extraction module (TFEM) module to extract long-range temporal dependencies and model evolution across consecutive frames. Lastly, the CMC module combines spatial and temporal information into a unified representation which is used to perform sports events categorization based on athlete movements. The proposed model is evaluated on Olympic sports dataset and University of Central Florida’s (UCF) Sports dataset across distinct evaluation measures. The model achieved 98.36% accuracy, 99.42% precision, 98.42% recall, 98.91% F1-score on the Olympic Sports dataset and 98.64% accuracy, 98.45% precision, 98.91% recall, and 98.68% F1-score on the UCF Sports dataset. Moreover, this method showed supreme results compared with existing techniques in all metrics, demonstrating its effectiveness and potential for high- applications in sports analytics and athlete monitoring.},
  archive      = {J_MVA},
  author       = {Gao, Yilun and Zou, Jie and Zhang, Yuexin},
  doi          = {10.1007/s00138-025-01733-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Exploring the potential of deep learning techniques for analyzing athlete movements in competitive athletics sports},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained 3D vehicle shape manipulation via latent space editing. <em>MVA</em>, <em>36</em>(6), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01739-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant potential of 3D object editing to impact various industries, recent research in 3D generation and editing has primarily focused on converting text and images into 3D models, often paying limited attention to the need for fine-grained control over existing 3D objects. This paper introduces a framework that uses a pre-trained regressor to enable continuous and attribute-specific modifications of both the stylistic and geometric attributes of 3D vehicle models. Here, “fine-grained control” refers to the ability to adjust specific geometric or stylistic attributes (such as roof length or perceived luxury) in a continuous and independent manner. Our method aims to preserve the identity of vehicle 3D objects and support multi-attribute editing, allowing for extensive customization while maintaining the model’s structural integrity. The framework leverages DeepSDF to obtain latent representations suitable for continuous attribute editing. Experimental results demonstrate the effectiveness of our approach in achieving detailed, controlled edits on a variety of vehicle 3D models. The code is released at https://github.com/JiangDong-miao/Vehicle_LatentEdit .},
  archive      = {J_MVA},
  author       = {Miao, JiangDong and Ikeda, Tatsuya and Raytchev, Bisser and Mizoguchi, Ryota and Hiraoka, Takenori and Nakashima, Takuji and Shimizu, Keigo and Higaki, Toru and Kaneda, Kazufumi},
  doi          = {10.1007/s00138-025-01739-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fine-grained 3D vehicle shape manipulation via latent space editing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ED-ViTTL: Ensemble vision transformer and transfer learning approach for brain tumor classification. <em>MVA</em>, <em>36</em>(6), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01741-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of brain tumors from magnetic resonance imaging (MRI) remains a challenging task due to the inherent heterogeneity of tumor morphology, class imbalance within datasets, and the limitations of individual deep learning models. To address these challenges, we propose ED-ViTTL (Ensembled Deep Vision Transformer and Transfer Learning), a hybrid framework that leverages both local and global feature representations to enhance diagnostic performance. The model integrates five advanced variants of the Vision Transformer (R50-ViT-L/16, ViT-L/16, ViT-L/32, ViT-B/16, and ViT-B/32) alongside a transfer-learned VGG19 convolutional neural network. Feature embeddings extracted from the ViT and CNN branches are fused through fully connected layers, enabling robust classification into four categories: glioma, meningioma, pituitary tumor, and healthy brain. Experiments were conducted on a publicly available dataset comprising 3264 MRI scans, partitioned into training (70%), validation (15%), and testing (15%) sets using stratified sampling. To mitigate class imbalance and improve model generalization, we employed stratified 5-fold cross-validation, class-weighted categorical cross-entropy, and extensive data augmentation. The best-performing ensemble configuration (ViT-B/32 + VGG19) achieved a classification accuracy of 98.67%, with class-specific AUC values exceeding 0.99 and ROC curves demonstrating clear inter-class separability. Performance metrics, including precision, recall, and F1-scores, remained consistently high across folds, with the confusion matrix indicating minimal misclassifications. These findings demonstrate that ED-ViTTL delivers stable and reproducible results, underscoring its potential as a reliable computer-aided diagnostic tool for assessing brain tumors in clinical practice.},
  archive      = {J_MVA},
  author       = {Thakur, Amit and Patnaik, Pawan Kumar and Kumar, Manoj and Choudhary, Chaitali},
  doi          = {10.1007/s00138-025-01741-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ED-ViTTL: Ensemble vision transformer and transfer learning approach for brain tumor classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editing implicit and explicit representations of radiance fields: A survey. <em>MVA</em>, <em>36</em>(6), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01742-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent years by offering a new volumetric representation, which is compact and provides high-quality image rendering. However, the methods to edit those radiance fields developed slower than the many improvements to other aspects of NeRF. With the recent development of alternative radiance field-based representations inspired by NeRF as well as the worldwide rise in popularity of text-to-image models, many new opportunities and strategies have emerged to provide radiance field editing. In this paper, we deliver a comprehensive survey of the different editing methods present in the literature for NeRF and other similar radiance field representations. We propose a new taxonomy for classifying existing works based on their editing methodologies, review pioneering models, reflect on current and potential new applications of radiance field editing, and compare state-of-the-art approaches in terms of editing options and performance.},
  archive      = {J_MVA},
  author       = {Hubert, Arthur and Elghazaly, Gamal and Frank, Raphaël},
  doi          = {10.1007/s00138-025-01742-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Editing implicit and explicit representations of radiance fields: A survey},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loid: Lane occlusion inpainting and detection for enhanced autonomous driving systems. <em>MVA</em>, <em>36</em>(6), 1-11. (<a href='https://doi.org/10.1007/s00138-025-01744-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate lane segmentation is essential for effective path planning and lane following in autonomous driving, especially in scenarios with significant occlusion from vehicles and pedestrians. Existing models often struggle under such conditions, leading to unreliable navigation and safety risks. We propose two innovative approaches to enhance lane detection in these challenging environments, each showing notable improvements over conventional methods. The first approach aug-Segment improves conventional lane detection models by augmenting the training dataset (e.g. CULanes) with simulated occlusions and training a segmentation model. This method achieves a 12% improvement over a number of SOTA models on the CULanes dataset. Additionally, we present a second approach, LOID: Lane Occlusion Inpainting and Detection, designed to address the issue more comprehensively and with improved robustness. LOID introduces an advanced lane segmentation network that uses an image processing pipeline to identify and mask occluded regions. An inpainting model is then applied to reconstruct the road environment in the occluded areas. The enhanced image is then processed by a lane detection algorithm, resulting in a 20% and 24% improvement over several SOTA models on the BDDK100 and CULanes datasets respectively, highlighting the effectiveness of the proposed approach.},
  archive      = {J_MVA},
  author       = {Agrawal, Aayush and Sivakumar, Ashmitha Jaysi and Kaif, Ibrahim and Banerjee, Chayan},
  doi          = {10.1007/s00138-025-01744-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Loid: Lane occlusion inpainting and detection for enhanced autonomous driving systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-contact SpO2 monitoring via multi-channel pulse signals from facial videos using machine learning. <em>MVA</em>, <em>36</em>(6), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01736-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blood oxygen saturation (SpO2) is a critical indicator of lung function, and its convenient and rapid monitoring is vital for preventing various diseases. Recent advancements in non-contact SpO2 measurement using RGB cameras have demonstrated its potential in diverse application scenarios. This paper presents a method to extract three-channel pulse signals from designated facial video regions and estimates blood oxygen saturation signals using traditional regression machine learning models. By analyzing facial videos, this paper approach aims to achieve accurate and non-contact SpO2 monitoring, enhancing accessibility in remote and virtual healthcare environments. We conducted experiments on publicly available datasets, demonstrating that paper method can precisely estimate SpO2 from facial videos of different subjects. The proposed features exhibit excellent performance across multiple machine learning regression models, indicating good research value and applicability in fields such as health screening and telemedicine. The relevant code can be accessed at: https://github.com/Si-tong-416/my_first_code_to_pre_SpO2},
  archive      = {J_MVA},
  author       = {Si, Tong and Song, Yuanbo and Sun, Peng and Xiao, Jinyuan and Zou, Rui and Chen, Zhencheng},
  doi          = {10.1007/s00138-025-01736-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Non-contact SpO2 monitoring via multi-channel pulse signals from facial videos using machine learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated fixed-point iterations for image deblurring and defiltering. <em>MVA</em>, <em>36</em>(6), 1-17. (<a href='https://doi.org/10.1007/s00138-025-01740-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of image deblurring consists in restoring a clean image from a given blurry image, typically corrupted by noise. In this work, we propose accelerated schemes inspired by variants of the Landweber iterations. The main idea consists of interpreting these iterations as gradient descent steps, using an appropriate conditioning of the gradient directions by a correction term, and accelerating the iterations by using alternating step sizes. Numerical experiments show that the combination of these different ingredients leads to improvements in image deblurring tasks. In particular, we show that the results obtained improve over state-of-the-art methods. We also show that the proposed approach can be adapted and used for more general black-box defiltering tasks, and, in particular, show its performance for the problem of enhancing low-light and blurry images in the presence of noise.},
  archive      = {J_MVA},
  author       = {Fayolle, Pierre-Alain},
  doi          = {10.1007/s00138-025-01740-6},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Accelerated fixed-point iterations for image deblurring and defiltering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High accurate SMPL-X generation based on volumetric reconstruction. <em>MVA</em>, <em>36</em>(6), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01745-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research on accurately generating human bodies, such as SMPL-X, for human motion and expression has gained momentum. However, traditional 2D image-based methods for generating human body models inevitably face limitations, such as depth ambiguity. This paper proposes a method that accurately generates human bodies, like SMPL or SMPL-X, using 3D volumetric data. Since 3D volumetric data contains precise spatial information, we leverage it to estimate SMPL-X joints accurately and reconstruct the SMPL-X model from them. The proposed method extracts 2D key points from multi-view images projected from 3D volumetric data and generates an accurate human body model through confidence-based 3D reconstruction. By applying this method, we can generate body models with a mean per-joint position error (MPJPE) of about 50 mm, resolving the depth ambiguity issue. Additionally, by fitting 3D volumetric data to human mesh models, we effectively utilize the rich depth information to overcome challenges such as depth ambiguity.},
  archive      = {J_MVA},
  author       = {Kim, Jung-Woo and Lee, Hak-Bum and Yoon, Seung-Hwan and Yang, Seung-Jun and Um, Gi-Mun and Seo, Young-Ho},
  doi          = {10.1007/s00138-025-01745-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {High accurate SMPL-X generation based on volumetric reconstruction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating long-term training for remote sensing object detection. <em>MVA</em>, <em>36</em>(6), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01747-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, numerous methods have achieved impressive performance in remote sensing object detection, relying on convolution or transformer architectures. Such detectors typically have a feature backbone to extract useful features from raw input images. A common practice in current detectors is initializing the backbone with pre-trained weights available online. Fine-tuning the backbone is typically required to generate features suitable for remote-sensing images. While the prolonged training could lead to over-fitting, hindering the extraction of basic visual features, it can enable models to gradually extract deeper insights and richer representations from remote sensing data. Striking a balance between these competing factors is critical for achieving optimal performance. In this study, we aim to investigate the performance and characteristics of remote sensing object detection models under very long training schedules, and propose a novel method named Dynamic Backbone Freezing (DBF) for feature backbone fine-tuning on remote sensing object detection under long-term training. Our method addresses the dilemma of whether the backbone should extract low-level generic features or possess specific knowledge of the remote sensing domain, by introducing a module called ’Freezing Scheduler’ to manage the update of backbone features during long-term training dynamically. Extensive experiments on DOTA and DIOR-R show that our approach enables more accurate model learning while substantially reducing computational costs in long-term training. Besides, it can be seamlessly adopted without additional effort due to its straightforward design. The code is available at https://github.com/unique-chan/dbf .},
  archive      = {J_MVA},
  author       = {Park, JongHyun and Kim, Yechan and Jeon, Moongu},
  doi          = {10.1007/s00138-025-01747-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Investigating long-term training for remote sensing object detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised learning combined with regression pre-training of siamese network based on superpixel segmentation for hyperspectral image classification. <em>MVA</em>, <em>36</em>(6), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01753-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the success of deep neural networks (DNNs), insufficient labeled samples remain a key challenge for DNN based hyperspectral image (HSI) classification. To perform better with limited labeled samples, some HSI classification methods try to acquire spatial information through superpixel segmentation (SPS). However, the existing methods seldom use multi-band and multi-scale SPS simultaneously to extract features, and lack effective means to mine the superpixel-wise information fully. This paper proposes a semi-supervised learning method combined with regression pre-training of Siamese network based on superpixel segmentation (SLRSS). The method innovatively introduces two key components: (1) an average edge-weighted graph using SPS results that provides the regression labels for the regression pre-training, and (2) a model-selected labeling (MSL) mechanism that enhances the subsequent semi-supervised learning process. First, we design an average edge-weighted graph to generate the similarity labels of each sample pair by using the multi-band multi-scale SPS results obtained from the unlabeled HSI. Next, we randomly sample a proportion of sample pairs with their corresponding similarity labels and add them into the regression training (RT) set. To fully utilize the limited labeled samples in the training set for classification, these limited labeled samples are also used to construct the sample pairs with the similarity labels according to their true labels and added into the RT set. Then, the parameters of the well pre-trained Siamese network after the regression training are used to initialize the parameters of the feature extraction module in the classification training. Finally, we design a novel semi-supervised learning strategy named as MSL to augment the training set in the classification training. Experimental results on two HSI datasets show that the proposed SLRSS outperforms several state-of-the-art methods significantly, with only five labeled samples per class.},
  archive      = {J_MVA},
  author       = {Mu, Caihong and Liu, Yu and Dong, Zhidong and Feng, Jiajie and Liu, Yi},
  doi          = {10.1007/s00138-025-01753-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Semi-supervised learning combined with regression pre-training of siamese network based on superpixel segmentation for hyperspectral image classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RPFE-net: RoI-guided pseudo-LiDAR point cloud feature enhancement network for multi-modal 3D object detection. <em>MVA</em>, <em>36</em>(6), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01755-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous driving, 3D object detection gives the vehicle the perception that enables it to drive safely and accurately in complex traffic environments. Camera-LiDAR 3D object detection is currently widely studied. However, there is still a great challenge to deal with the inherent data differences between the two modalities and achieve accurate feature fusion. Therefore, this paper introduces RoI-Guided Pseudo-LiDAR Point cloud Feature Enhancement Network (RPFE-Net), which contains a pseudo-point cloud discard module and point voxel ensemble network based on anti-noise submanifold convolution. It generates pseudo-point clouds with high density and semantic information from depth completion and merges them with LiDAR data. RPFE-Net effectively improves 3D object detection accuracy by solving the problem of computational cost due to high density of pseudo-point clouds and the problem of information loss during the voxelization of pseudo-point clouds. Experiments on the KITTI dataset demonstrate that our RPFE-Net can achieve decent detection accuracy and fast inference speed.},
  archive      = {J_MVA},
  author       = {Lin, Ruifan and Feng, Xinxin and Chen, Yuren and Zheng, Haifeng},
  doi          = {10.1007/s00138-025-01755-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {RPFE-net: RoI-guided pseudo-LiDAR point cloud feature enhancement network for multi-modal 3D object detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
