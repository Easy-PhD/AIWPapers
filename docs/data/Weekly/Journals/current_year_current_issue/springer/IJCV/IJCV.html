<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJCV</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijcv">IJCV - 40</h2>
<ul>
<li><details>
<summary>
(2025). Correction: BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos. <em>IJCV</em>, <em>133</em>(10), 7513. (<a href='https://doi.org/10.1007/s11263-025-02532-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Duporge, Isla and Kholiavchenko, Maksim and Harel, Roi and Wolf, Scott and Rubenstein, Daniel I and Crofoot, Margaret C and Berger-Wolf, Tanya and Lee, Stephen J and Barreau, Julie and Kline, Jenna and Ramirez, Michelle and Stewart, Charles V},
  doi          = {10.1007/s11263-025-02532-1},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7513},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: A generalized contour vibration model for building extraction. <em>IJCV</em>, <em>133</em>(10), 7512. (<a href='https://doi.org/10.1007/s11263-025-02512-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Xu, Chunyan and Yao, Shuaizhen and Xu, Ziqiang and Cui, Zhen and Yang, Jian},
  doi          = {10.1007/s11263-025-02512-5},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7512},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: A generalized contour vibration model for building extraction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Local concept embeddings for analysis of concept distributions in vision DNN feature spaces. <em>IJCV</em>, <em>133</em>(10), 7511. (<a href='https://doi.org/10.1007/s11263-025-02501-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Mikriukov, Georgii and Schwalbe, Gesina and Bade, Korinna},
  doi          = {10.1007/s11263-025-02501-8},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7511},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Local concept embeddings for analysis of concept distributions in vision DNN feature spaces},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RINDNet++: Edge detection for discontinuity in reflectance, illumination, normal, and depth. <em>IJCV</em>, <em>133</em>(10), 7486-7510. (<a href='https://doi.org/10.1007/s11263-025-02541-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental building block in computer vision, edges, categorized by the discontinuity in surface-Reflectance, Illumination, surface-Normal or Depth, are critical for scene understanding. Despite significant progress in detecting generic or individual edge types, a holistic approach to simultaneously identifying all four edge types presents a unique challenge. In this paper, we propose RINDNet++, a novel neural network solution to address this challenge by detecting all four edge types. RINDNet++ is designed with a three-stage process that effectively leverages the distinct attributes of each edge type and their relationship. In Stage I, a common backbone extracts hierarchical features for all edges. Stage II then tailors these features for each edge type using specialized decoders. Stage III predicts the initial detection results with independent decision heads based on the enhanced features from the preceding stages. Additionally, RINDNet++ incorporates an attention module that refines edge detection by highlighting inter-type relationships, leading to enhanced edge maps. To enhance rigorous training and evaluation, we introduce the first benchmark, BSDS-RIND, incorporating annotations for all four edge types and supporting both single-scale and multi-scale testing. With the integration of state-of-the-art edge detection methods, BSDS-RIND establishes a robust framework for performance evaluation. Extensive experiments show that our proposed RINDNet++ yields promising results in comparison with the state-of-the-art approaches. Moreover, RINDNet++ excels in detecting generic edges and enhances performance in downstream applications such as shadow detection and depth estimation.},
  archive      = {J_IJCV},
  author       = {Pu, Mengyang and Huang, Yaping and Guan, Qingji and Liu, Zhihao and Ling, Haibin},
  doi          = {10.1007/s11263-025-02541-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7486-7510},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RINDNet++: Edge detection for discontinuity in reflectance, illumination, normal, and depth},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segment anything in context with vision foundation models. <em>IJCV</em>, <em>133</em>(10), 7460-7485. (<a href='https://doi.org/10.1007/s11263-025-02517-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of large-scale pre-training, vision foundation models have emerged as powerful tools for open-world image understanding, showcasing remarkable capabilities across a range of visual tasks. However, unlike large language models that excel at directly tackling various language tasks, vision foundation models often require the integration of task-specific architectural modifications and extensive fine-tuning to achieve satisfactory performance in specific domains. This limitation not only increases the complexity of deployment but also restricts their broader applicability in dynamic, real-world scenarios. In this work, we present Matcher, a novel perception paradigm that utilizes off-the-shelf vision foundation models to address various segmentation tasks. Matcher can segment anything by using in-context examples without training. Additionally, we design three effective components within the Matcher framework to collaborate with these foundation models and unleash their full potential in diverse tasks. Specifically, we develop a bidirectional matching strategy to ensure precise visual matching. Then, we design a robust prompt sampler that generates mask proposals with diverse semantic granularity. To further enhance accuracy, we propose an innovative instance-level matching strategy that effectively filters out false-positive mask fragments. In addition, we deploy another vision foundation model to retrieve in-context examples for better prompt engineering for Matcher. Our comprehensive experiments demonstrate that Matcher, without any additional training, has impressive generalization performance across a wide range of visual tasks, underscoring its substantial potential in advancing toward general perception.},
  archive      = {J_IJCV},
  author       = {Liu, Yang and Zhu, Muzhi and Chen, Hao and Wang, Xinlong and Feng, Bo and Wang, Hao and Li, Shiyu and Vemulapalli, Raviteja and Shen, Chunhua},
  doi          = {10.1007/s11263-025-02517-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7460-7485},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Segment anything in context with vision foundation models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FovEx: Human-inspired explanations for vision transformers and convolutional neural networks. <em>IJCV</em>, <em>133</em>(10), 7437-7459. (<a href='https://doi.org/10.1007/s11263-025-02543-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainability in artificial intelligence (XAI) remains a crucial aspect for fostering trust and understanding in machine learning models. Current visual explanation techniques, such as gradient-based or class-activation-based methods, often exhibit a strong dependence on specific model architectures. Conversely, perturbation-based methods, despite being model-agnostic, are computationally expensive as they require evaluating models on a large number of forward passes. We introduce Foveation-based Explanations (FovEx), a novel XAI method inspired by human vision, which combines biologically inspired foveation-based transformations with gradient-driven overt attention to iteratively select locations of interest. These locations are selected to maximize the performance of the model to be explained with respect to the downstream task and then combined to generate an attribution map. We provide a thorough evaluation with qualitative and quantitative assessments on established benchmarks. Our method achieves state-of-the-art performance on both transformers (on 4 out of 5 metrics) and convolutional models (on 3 out of 5 metrics), demonstrating its versatility among various architectures. Furthermore, we show the alignment between the explanation map produced by FovEx and human gaze patterns (+14% in NSS compared to RISE, +203% in NSS compared to GradCAM). This comparison enhances our confidence in FovEx’s ability to close the interpretation gap between humans and machines.},
  archive      = {J_IJCV},
  author       = {Panda, Mahadev Prasad and Tiezzi, Matteo and Vilas, Martina and Roig, Gemma and Eskofier, Bjoern M. and Zanca, Dario},
  doi          = {10.1007/s11263-025-02543-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7437-7459},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FovEx: Human-inspired explanations for vision transformers and convolutional neural networks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On efficient variants of segment anything model: A survey. <em>IJCV</em>, <em>133</em>(10), 7406-7436. (<a href='https://doi.org/10.1007/s11263-025-02539-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Segment Anything Model (SAM) is a foundational model for image segmentation tasks, known for its strong generalization across diverse applications. However, its impressive performance comes with significant computational and resource demands, making it challenging to deploy in resource-limited environments such as edge devices. To address this, a variety of SAM variants have been proposed to enhance efficiency while keeping accuracy. This survey provides the first comprehensive review of these efficient SAM variants. We begin by exploring the motivations driving this research. We then present core techniques used in SAM and model acceleration. This is followed by a detailed exploration of SAM acceleration strategies, categorized by approach, and a discussion of several future research directions. Finally, we offer a unified and extensive evaluation of these methods across various hardware, assessing their efficiency and accuracy on representative benchmarks, and providing a clear comparison of their overall performance. To complement this survey, we summarize the papers and codes related to efficient SAM variants at https://github.com/Image-and-Video-Computing-Group/On-Efficient-Variants-of-Segment-Anything-Model .},
  archive      = {J_IJCV},
  author       = {Sun, Xiaorui and Liu, Jun and Shen, Hengtao and Zhu, Xiaofeng and Hu, Ping},
  doi          = {10.1007/s11263-025-02539-8},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7406-7436},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {On efficient variants of segment anything model: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of data augmentation in visual reinforcement learning. <em>IJCV</em>, <em>133</em>(10), 7368-7405. (<a href='https://doi.org/10.1007/s11263-025-02472-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual reinforcement learning (RL), which makes decisions directly from high-dimensional visual inputs, has demonstrated significant potential in various domains. However, deploying visual RL techniques in the real world remains challenging due to their low sample efficiency and large generalization gaps. To tackle these obstacles, data augmentation (DA) has become a widely used technique in visual RL for acquiring sample-efficient and generalizable policies by diversifying the training data. This survey aims to provide a timely and essential review of DA techniques in visual RL in recognition of the thriving development in this field. In particular, we propose a unified framework for analyzing visual RL and understanding the role of DA in it. We then present a principled taxonomy of the existing augmentation techniques used in visual RL and conduct an in-depth discussion on how to better leverage augmented data in various scenarios. Moreover, we report the empirical evaluation of DA-based techniques in visual RL and conclude by highlighting the directions for future research. As the first comprehensive survey of DA in visual RL, this work is expected to offer valuable guidance to this emerging field.},
  archive      = {J_IJCV},
  author       = {Ma, Guozheng and Wang, Zhen and Yuan, Zhecheng and Wang, Xueqian and Yuan, Bo and Tao, Dacheng},
  doi          = {10.1007/s11263-025-02472-w},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7368-7405},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A comprehensive survey of data augmentation in visual reinforcement learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FineBio: A fine-grained video dataset of biological experiments with hierarchical annotation. <em>IJCV</em>, <em>133</em>(10), 7352-7367. (<a href='https://doi.org/10.1007/s11263-025-02523-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the development of science, accurate and reproducible documentation of the experimental process is crucial. Automatic recognition of the actions in experiments from videos would help experimenters by complementing the recording of experiments. Towards this goal, we propose FineBio, a new fine-grained video dataset of people performing biological experiments. The dataset consists of multi-view videos of 32 participants performing mock biological experiments with a total duration of 14.5 hours. One experiment forms a hierarchical structure, where a protocol consists of several steps, each further decomposed into a set of atomic operations. The uniqueness of biological experiments is that while they require strict adherence to steps described in each protocol, there is freedom in the order of atomic operations. We provide hierarchical annotation on protocols, steps, atomic operations, object locations, and their manipulation states, providing new challenges for structured activity understanding and hand-object interaction recognition. To find out challenges on activity understanding in biological experiments, we introduce baseline models and results on four different tasks, including (i) step segmentation, (ii) atomic operation detection (iii) object detection, and (iv) manipulated/affected object detection. Dataset and code are available from https://github.com/aistairc/FineBio .},
  archive      = {J_IJCV},
  author       = {Yagi, Takuma and Ohashi, Misaki and Huang, Yifei and Furuta, Ryosuke and Adachi, Shungo and Mitsuyama, Toutai and Sato, Yoichi},
  doi          = {10.1007/s11263-025-02523-2},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7352-7367},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FineBio: A fine-grained video dataset of biological experiments with hierarchical annotation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-rate monocular depth estimation via cross frame-rate collaboration of frames and events. <em>IJCV</em>, <em>133</em>(10), 7332-7351. (<a href='https://doi.org/10.1007/s11263-025-02488-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining the complementary benefits of frames and events has been widely used for monocular depth estimation in challenging scenarios. However, most existing methods utilize a synchronous fusion of two modalities, ignoring the advantages of high temporal resolution from event cameras, which results in low-rate depth maps constrained by the frame sampling rate. To this end, this paper proposes a novel cross frame-rate frame-event joint learning network, namely CFRNet, collaborating two heterogeneous streams for high-rate and fine-grained monocular depth estimation. Technically, a cross frame-rate multimodal fusion (CFMF) module is first designed for the joint representation of frames and events. By employing implicit spatial alignment and dynamic attention-based fusion, it addresses the misalignment between frames and events at different moments, robustly combining the strengths of both modalities in diverse challenging scenarios. Following the CFMF, a temporal consistent modeling (TCM) module adopting the recurrent structure is created to keep the temporal consistency of joint representations from CFMF. Experimental results demonstrate that the depth estimation accuracy of our approach outperforms existing five state-of-the-art methods and our three baselines involving single modality on two public datasets (i.e., DSEC and MVSEC) while achieving a high frame rate up to 100 Hz. Codes can be available at https://github.com/liuxu0303/CFRNet .},
  archive      = {J_IJCV},
  author       = {Liu, Xu and Fan, Xiaopeng and Li, Jianing and Li, Dianze and Zhang, Wei and Ma, Zhengyu and Tian, Yonghong},
  doi          = {10.1007/s11263-025-02488-2},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7332-7351},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {High-rate monocular depth estimation via cross frame-rate collaboration of frames and events},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning latent part-whole hierarchies for point clouds. <em>IJCV</em>, <em>133</em>(10), 7312-7331. (<a href='https://doi.org/10.1007/s11263-025-02533-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strong evidence suggests that humans perceive the 3D world by parsing visual scenes and objects into part-whole hierarchies. Although deep neural networks have the capability of learning powerful multi-level representations, they can not explicitly model part-whole hierarchies, which limits their expressiveness and interpretability in processing 3D vision data such as point clouds. To this end, we propose an encoder-decoder style latent variable model that explicitly learns the part-whole hierarchies for the multi-level point cloud segmentation. Specifically, the encoder takes a point cloud as input and predicts the per-point latent subpart distribution at the middle level. The decoder takes the latent variable and the feature from the encoder as an input and predicts the per-point part distribution at the top level. During training, only annotated part labels at the top level are provided, thus making the whole framework weakly supervised. We explore two kinds of approximated inference algorithms, i.e., most-probable-latent and Monte Carlo methods, and three stochastic gradient estimations for learning discrete latent variables, i.e., straight-through, REINFORCE, and pathwise estimators. Experimental results on the PartNet dataset show that the proposed method achieves state-of-the-art performance in not only top-level part segmentation but also middle-level latent subpart segmentation.},
  archive      = {J_IJCV},
  author       = {Gao, Xiang and Hu, Wei and Liao, Renjie},
  doi          = {10.1007/s11263-025-02533-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7312-7331},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning latent part-whole hierarchies for point clouds},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards balanced representation learning with semantic anchor regularization. <em>IJCV</em>, <em>133</em>(10), 7293-7311. (<a href='https://doi.org/10.1007/s11263-025-02519-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning refers to the process of learning meaningful and informative features from raw data, of which one good criterion is to attain intra-class compactness and inter-class separability in the semantic space. However,real-world data are always imbalanced and noisy. Existing methods such as prototype-based learning and contrastive learning are deeply bounded to the feature learning process and susceptible to imbalanced data distribution. In this paper, we disentangle the representation regularization from the feature learning process and propose a semantic anchor regularization (SAR) that is generated from predefined anchors. These anchors serve as an independent third-party measurement and are made semantic-aware by sharing the task head with feature learning. By controlling the separability between semantic anchors and pulling the learned representation to these semantic anchors, the intra-class compactness and inter-class separability can be intuitively achieved. In essence, SAR performs in the manner of visual-language alignment but is more flexible. Extensive results on classification, segmentation, long-tailed learning, and semi-supervised learning demonstrate the SAR’s effectiveness for different downstream tasks.},
  archive      = {J_IJCV},
  author       = {Wang, Chengjie and Nie, Qiang and Chen, Ying and Li, Jialin and Liu, Yong and Jiang, Xi and Ge, Yanqi and Wu, Yunsheng and Zheng, Feng and Ma, Lizhuang},
  doi          = {10.1007/s11263-025-02519-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7293-7311},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards balanced representation learning with semantic anchor regularization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Six-point method for multi-camera systems with reduced solution space. <em>IJCV</em>, <em>133</em>(10), 7270-7292. (<a href='https://doi.org/10.1007/s11263-025-02531-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relative pose estimation using point correspondences (PC) is a widely used technique. A minimal configuration of six PCs is required for two views of generalized cameras. In this paper, we present several minimal solvers that use six PCs to compute the 6DOF relative pose of multi-camera systems, including a minimal solver for the generalized camera and two minimal solvers for the practical configuration of two-camera rigs. The equation construction is based on the decoupling of rotation and translation. Rotation is represented by Cayley or quaternion parametrization, and translation can be eliminated by using the hidden variable technique. Ray bundle constraints are found and proven when a subset of PCs relate the same cameras across two views. This is the key to reducing the number of solutions and generating numerically stable solvers. Moreover, all configurations of six-point problems for multi-camera systems are enumerated by the Pólya enumeration theorem. Extensive experiments demonstrate the superior accuracy and efficiency of our solvers compared to state-of-the-art six-point methods. The code is available at https://github.com/jizhaox/relpose-6pt .},
  archive      = {J_IJCV},
  author       = {Guan, Banglei and Zhao, Ji and Mitra, Saibal and Kneip, Laurent},
  doi          = {10.1007/s11263-025-02531-2},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7270-7292},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Six-point method for multi-camera systems with reduced solution space},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active stereo in the wild through virtual pattern projection. <em>IJCV</em>, <em>133</em>(10), 7242-7269. (<a href='https://doi.org/10.1007/s11263-025-02511-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel general-purpose guided stereo paradigm that mimics the active stereo principle by replacing the unreliable physical pattern projector with a depth sensor. It works by projecting virtual patterns consistent with the scene geometry onto the left and right images acquired by a conventional stereo camera, using the sparse hints obtained from a depth sensor, to facilitate the visual correspondence. Purposely, any depth sensing device can be seamlessly plugged into our framework, enabling the deployment of a virtual active stereo setup in any possible environment and overcoming the severe limitations of physical pattern projection, such as the limited working range and environmental conditions. Exhaustive experiments on indoor and outdoor datasets featuring both long and close range, including those providing raw, unfiltered depth hints from off-the-shelf depth sensors, highlight the effectiveness of our approach in notably boosting the robustness and accuracy of algorithms and deep stereo without any code modification and even without re-training. Additionally, we assess the performance of our strategy on active stereo evaluation datasets with conventional pattern projection. Indeed, in all these scenarios, our virtual pattern projection paradigm achieves state-of-the-art performance. The source code is available at: https://github.com/bartn8/vppstereo .},
  archive      = {J_IJCV},
  author       = {Bartolomei, Luca and Poggi, Matteo and Tosi, Fabio and Conti, Andrea and Mattoccia, Stefano},
  doi          = {10.1007/s11263-025-02511-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7242-7269},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Active stereo in the wild through virtual pattern projection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CEDFlow++: Latent contour enhancement for dark optical flow estimation. <em>IJCV</em>, <em>133</em>(10), 7222-7241. (<a href='https://doi.org/10.1007/s11263-025-02528-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CEDFlow introduces a latent contour enhancement method into dark optical flow estimation and achieves advanced performance. Nevertheless, it largely focuses on addressing the motion boundary in a local manner. Unfortunately, it falls short in performance when addressing significant variations or large-scale degraded scenes. This paper introduces CEDFlow++, which features three innovative modules to address the key challenges of CEDFlow. Firstly, we introduce a decomposition-based feature encoder (DBFE), which captures both fine-grained and large-scale features through its local encoder and a uniquely designed sparse attention-based global encoder that suppresses noise and interference that only exist in the dark. Secondly, for reliable motion analysis, we propose a customized dual cost-volume reasoning (DCVR), which integrates important high-contrast feature correlations of the global cost volume into the local cost volume, effectively capturing salient yet holistic motion information while mitigating motion ambiguity caused by darkness. Importantly, we present a contour-guided attention (CGA) which enables context-adaptive extraction of contour features by modifying the sign properties of the Sobel kernel parameters in latent space, specifically targeting large-scale contours that are suitable for motion boundaries. Experimental results on the FCDN and VBOF datasets show that CEDFlow++ outperforms state-of-the-art methods in terms of the EPE index and produces more accurate and robust optical flow.},
  archive      = {J_IJCV},
  author       = {Zuo, Fengyuan and Jin, Haiyan and Xiao, Zhaolin and Su, Haonan and Zhang, Meng},
  doi          = {10.1007/s11263-025-02528-x},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7222-7241},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CEDFlow++: Latent contour enhancement for dark optical flow estimation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mamba capsule routing towards part-whole relational camouflaged object detection. <em>IJCV</em>, <em>133</em>(10), 7201-7221. (<a href='https://doi.org/10.1007/s11263-025-02530-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The part-whole relational property endowed by Capsule Networks (CapsNets) has been known successful for camouflaged object detection due to its segmentation integrity. However, the previous Expectation Maximization (EM) capsule routing algorithm with heavy computation and large parameters obstructs this trend. The primary attribution behind lies in the pixel-level capsule routing. Alternatively, in this paper, we propose a novel mamba capsule routing at the type level. Specifically, we first extract the implicit latent state in mamba as capsule vectors, which abstract type-level capsules from pixel-level versions. These type-level mamba capsules are fed into the EM routing algorithm to get the high-layer mamba capsules, which greatly reduce the computation and parameters caused by the pixel-level capsule routing for part-whole relationships exploration. On top of that, to retrieve the pixel-level capsule features for further camouflaged prediction, we achieve this on the basis of the low-layer pixel-level capsules with the guidance of the correlations from adjacent-layer type-level mamba capsules. Extensive experiments on three widely used COD benchmark datasets demonstrate that our method significantly outperforms state-of-the-arts. Code has been available on https://github.com/Liangbo-Cheng/mamba_capsule .},
  archive      = {J_IJCV},
  author       = {Zhang, Dingwen and Cheng, Liangbo and Liu, Yi and Wang, Xinggang and Han, Junwei},
  doi          = {10.1007/s11263-025-02530-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7201-7221},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mamba capsule routing towards part-whole relational camouflaged object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked image modeling: A survey. <em>IJCV</em>, <em>133</em>(10), 7154-7200. (<a href='https://doi.org/10.1007/s11263-025-02524-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we survey recent studies on masked image modeling (MIM), an approach that emerged as a powerful self-supervised learning technique in computer vision. The MIM task involves masking some information, e.g. pixels, patches, or even latent representations, and training a model, usually an autoencoder, to predicting the missing information by using the context available in the visible part of the input. We identify and formalize two categories of approaches on how to implement MIM as a pretext task, one based on reconstruction and one based on contrastive learning. Then, we construct a taxonomy and review the most prominent papers in recent years. We complement the manually constructed taxonomy with a dendrogram obtained by applying a hierarchical clustering algorithm. We further identify relevant clusters via manually inspecting the resulting dendrogram. Our review also includes datasets that are commonly used in MIM research. We aggregate the performance results of various masked image modeling methods on the most popular datasets, to facilitate the comparison of competing methods. Finally, we identify research gaps and propose several interesting directions of future work. We supplement our survey with the following public repository containing organized references: https://github.com/vladhondru25/MIM-Survey .},
  archive      = {J_IJCV},
  author       = {Hondru, Vlad and Croitoru, Florinel Alin and Minaee, Shervin and Ionescu, Radu Tudor and Sebe, Nicu},
  doi          = {10.1007/s11263-025-02524-1},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7154-7200},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Masked image modeling: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Insect-foundation: A foundation model and large multimodal dataset for vision-language insect understanding. <em>IJCV</em>, <em>133</em>(10), 7128-7153. (<a href='https://doi.org/10.1007/s11263-025-02521-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual insects since they are often trained on the general knowledge of vision-language data. Meanwhile, understanding insects is a fundamental problem in precision agriculture, helping to promote sustainable development in agriculture. Therefore, this paper proposes a novel multimodal conversational model, Insect-LLaVA, to promote visual understanding in insect-domain knowledge. In particular, we first introduce a new large-scale Multimodal Insect Dataset with Visual Insect Instruction Data that enables the capability of learning the multimodal foundation models. Our proposed dataset enables conversational models to comprehend the visual and semantic features of the insects. Second, we propose a new Insect-LLaVA model, a new general Large Language and Vision Assistant in Visual Insect Understanding. Then, to enhance the capability of learning insect features, we develop an Insect Foundation Model by introducing a new micro-feature self-supervised learning with a Patch-wise Relevant Attention mechanism to capture the subtle differences among insect images. We also present Description Consistency loss to improve micro-feature learning via text descriptions. The experimental results evaluated on our new Visual Insect Question Answering benchmarks illustrate the effective performance of our proposed approach in visual insect understanding and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks Project Page: https://uarkcviu.github.io/projects/insectfoundation .},
  archive      = {J_IJCV},
  author       = {Truong, Thanh-Dat and Nguyen, Hoang-Quan and Nguyen, Xuan-Bac and Dowling, Ashley and Li, Xin and Luu, Khoa},
  doi          = {10.1007/s11263-025-02521-4},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7128-7153},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Insect-foundation: A foundation model and large multimodal dataset for vision-language insect understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A causal intervention method for domain generalization with a self-supervised auxiliary task. <em>IJCV</em>, <em>133</em>(10), 7110-7127. (<a href='https://doi.org/10.1007/s11263-025-02529-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization tackles the challenge of domain shifts by learning a model from diverse source domains that can effectively generalize to unseen target domains. This paper explores domain generalization on image data with auxiliary learning task(s), which leverages auxiliary task(s) to extract transferable features and boost the performance of the primary domain generalization task on unseen domains. Causal intervention provides an attractive strategy to tackle domain shifts and learn causal dependencies in domain generalization tasks. However, most of the existing causal intervention methods are tailored for single-task learning. Causal intervention on multiple tasks (e.g., the primary and auxiliary tasks in domain generalization) remains under-explored. In this paper, we propose CI-DGA, a novel causal intervention method for domain generalization on image data with self-supervised auxiliary task(s). In CI-DGA, we employ a hidden confounder to model the data distribution of a specific domain, wherein this distribution changing with domain shifts. Theoretically, we show that the negative effects by this confounder can be eliminated by causal intervention, and the causal relations between the input images and the labels in both the primary and auxiliary tasks are identifiable. Additionally, we develop a deep architecture to implement the causal inference models, in which we provide an approximate strategy to reduce the computational cost and avoid simultaneous sampling operations on multiple variables. Comprehensive experimental results on three widely-used benchmark datasets show that the proposed CI-DGA has superior performance against state-of-the-art baselines for domain generalization on images.},
  archive      = {J_IJCV},
  author       = {Gong, Qinkang and Pan, Yan and Lai, Hanjiang and Yin, Jian},
  doi          = {10.1007/s11263-025-02529-w},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7110-7127},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A causal intervention method for domain generalization with a self-supervised auxiliary task},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayes-CAL: Robust cross-modal alignment by bayesian approach for few-shot OoD generalization. <em>IJCV</em>, <em>133</em>(10), 7076-7109. (<a href='https://doi.org/10.1007/s11263-025-02527-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, fine-tuning large pre-trained models has yielded promising results in few-shot learning regime. However, their ability to generalize on two-dimensional Out-of-Distribution (OoD) data, characterized by correlation shift and diversity shift, remains largely unexplored. Recent studies demonstrate that even with a vast amount of data, few methods can beat the empirical risk minimization method (ERM) simultaneously on the two-dimensional OoD generalization. Consequently, OoD generalization in the few-shot regime emerges as a challenging area in model generalization research, where the OoD test performance suffers from the synergies of both data distribution shifts and overfitting few-shot samples. In this paper, utilizing informative natural language supervision, we investigate a novel Bayesian cross-modal image-text alignment learning method (Bayes-CAL) to address this issue, where only the texts representations are fine-tuned via image-text alignment in a domain-invariant manner under the proposed regularization. The Bayesian approach is essentially introduced to avoid overfitting the base classes observed in the training process and improve generalization to unseen classes. Under mild assumptions, we have theoretically demonstrated our Bayes-CAL method can achieve lower generalization errors under distribution shifts, compared to previous methods. To validate the effectiveness of the proposed Bayes-CAL, in addition to the extensive experiments on the image classification task, we also verify the OoD generalization performance on the object detection and instance segmentation tasks, addressing the gap in previous works on the challenging few-shot OoD generalization. Compared with recent CLIP-based methods, Bayes-CAL achieved state-of-the-art OoD generalization performance on these tasks with a large margin. More stable generalization performances on unseen classes of the Bayes-CAL are further demonstrated in the experiments.},
  archive      = {J_IJCV},
  author       = {Zhu, Lin and Yin, Weihan and Wu, Fan and Gu, Qinying and Wang, Xinbing and Zhou, Chenghu and Ye, Nanyang},
  doi          = {10.1007/s11263-025-02527-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7076-7109},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bayes-CAL: Robust cross-modal alignment by bayesian approach for few-shot OoD generalization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot paragraph-level handwriting imitation with latent diffusion models. <em>IJCV</em>, <em>133</em>(10), 7054-7075. (<a href='https://doi.org/10.1007/s11263-025-02525-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The imitation of cursive handwriting is mainly limited to generating handwritten words or lines. Multiple synthetic outputs must be stitched together to create paragraphs or whole pages, whereby consistency and layout information are lost. To close this gap, we propose a method for imitating handwriting at the paragraph level that also works for unseen writing styles. Therefore, we introduce a modified latent diffusion model that enriches the encoder-decoder mechanism with specialized loss functions that explicitly preserve the style and content. We enhance the attention mechanism of the diffusion model with adaptive 2D positional encoding and the conditioning mechanism to work with two modalities simultaneously: a style image and the target text. This significantly improves the realism of the generated handwriting. We set a new benchmark in our comprehensive evaluation, achieving 61 % mAP and 56 % top-1 accuracy in style preservation, significantly outperforming the previous best method (37 % mAP, 30 % top-1). We are making our code publicly available for reproducibility, supporting research in this area and research into potential countermeasures: https://github.com/M4rt1nM4yr/paragraph_handwriting_imitation_ldm},
  archive      = {J_IJCV},
  author       = {Mayr, Martin and Dreier, Marcel and Kordon, Florian and Seuret, Mathias and Zöllner, Jochen and Wu, Fei and Maier, Andreas and Christlein, Vincent},
  doi          = {10.1007/s11263-025-02525-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7054-7075},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Zero-shot paragraph-level handwriting imitation with latent diffusion models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DreamArtist: Controllable one-shot text-to-image generation via positive-negative adapter. <em>IJCV</em>, <em>133</em>(10), 7037-7053. (<a href='https://doi.org/10.1007/s11263-025-02526-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-arts text-to-image generation models such as Imagen Saharia et al. (2022) and Stable Diffusion Model Rombach et al. (2022) have succeed remarkable progresses in synthesizing high-quality, feature-rich images with high resolution guided by human text prompts. Since certain characteristics of image content e.g., very specific object entities or styles, are very hard to be accurately described by text, some example-based image generation approaches have been proposed, i.e. generating new concepts based on absorbing the salient features of a few input references. Despite of acknowledged successes, these methods have struggled on accurately capturing the reference examples’ characteristics while keeping diverse and high-quality image generation, particularly in the one-shot scenario (i.e. given only one reference). To tackle this problem, we propose a simple yet effective framework, namely DreamArtist, which adopts a novel positive-negative prompt-tuning learning strategy on the pre-trained diffusion model, and it has shown to well handle the trade-off between the accurate controllability and fidelity of image generation with only one reference example. Specifically, our proposed framework incorporates both positive and negative embeddings or adapters and optimizes them in a joint manner. The positive part aggressively captures the salient characteristics of the reference image to drive diversified generation and the negative part rectifies inadequacies from the positive part. We have conducted extensive experiments and evaluated the proposed method from image similarity (fidelity) and diversity, generation controllability, and style cloning. And our DreamArtist has achieved a superior generation performance over existing methods. Besides, our additional evaluation on extended tasks, including concept compositions and prompt-guided image editing, demonstrates its effectiveness for more applications. DreamArtist project page: https://www.sysu-hcp.net/projects/dreamartist/index.html},
  archive      = {J_IJCV},
  author       = {Dong, Ziyi and Wei, Pengxu and Lin, Liang},
  doi          = {10.1007/s11263-025-02526-z},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7037-7053},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DreamArtist: Controllable one-shot text-to-image generation via positive-negative adapter},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DomainStudio: Fine-tuning diffusion models for domain-driven image generation using limited data. <em>IJCV</em>, <em>133</em>(10), 7012-7036. (<a href='https://doi.org/10.1007/s11263-025-02498-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising diffusion probabilistic models (DDPMs) have been proven capable of synthesizing high-quality images with remarkable diversity when trained on large amounts of data. Unfortunately, they are still vulnerable to overfitting when fine-tuned on limited data. Existing works have explored subject-driven generation with text-to-image (T2I) models using a few samples. However, there is still a lack of effective and stable data-efficient methods to synthesize images in specific domains (e.g. styles or properties), which remains challenging due to ambiguities inherent in natural language and out-of-distribution effects. This paper introduces a few-shot fine-tuning approach named DomainStudio as a domain-driven image generation paradigm, which is designed to retain the subjects from prior knowledge provided by pre-trained models and adapt them to the domain extracted from training data, pursuing high quality and great diversity. We propose to keep the image-level relative distances between adapted samples and enhance the learning of high-frequency details from both pre-trained models and training samples. DomainStudio is compatible with both unconditional and T2I DDPMs. The proposed method achieves better results than current state-of-the-art GAN-based approaches in unconditional few-shot image generation. It also outperforms existing few-shot fine-tuning methods for modern large-scale T2I diffusion models like Textual Inversion and DreamBooth on synthesizing samples in specific domains characterized by few-shot training data.},
  archive      = {J_IJCV},
  author       = {Zhu, Jingyuan and Ma, Huimin and Chen, Jiansheng and Yuan, Jian},
  doi          = {10.1007/s11263-025-02498-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7012-7036},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DomainStudio: Fine-tuning diffusion models for domain-driven image generation using limited data},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RealDTT: Towards a comprehensive real-world dataset for tampered text detection. <em>IJCV</em>, <em>133</em>(10), 6993-7011. (<a href='https://doi.org/10.1007/s11263-025-02515-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The swift advancement of text manipulation in AI-generated images and the rise of false document fabrication emphasize the need for effective detection methods applicable in real-world settings. While current forensics research primarily addresses tampered text in natural images, text manipulation in documents presents a more realistic struggle to handle. To address the robustness of current detection methods and datasets, we aim to develop a real-world, large-scale dataset containing manually tampered documents and diverse automatic tampering techniques. Our work distinguishes itself from existing benchmarks through three key features: Manual Tampering: encompassing the simulation of realism and cognition, where human edits are often subtle and contextually coherent. Diverse Generators: rich manipulating types for tampered images ensure the coverage of traditional and advanced tampering techniques. Multilingual and Multiscene Coverage: spanning English and Chinese text across natural scenes and documents, with varied resolutions. We have developed a comprehensive dataset, RealDTT, to evaluate the open-set generalization capabilities of text-tampered detection models. The RealDTT encompasses approximately 300,000 diverse synthetic samples originating from nine distinct generative models. To our knowledge, this represents the most extensive collection of Deepfake model types currently available. Complementing these synthetic samples are 4,012 meticulously manually tampered images. Moreover, leveraging the RealDTT dataset, we propose a robust tampered text detection model, TTDMamba, which fully harnesses the unique strengths of the Mamba architecture and integrates selective scanning, high-frequency feature aggregation, and disentangled semantic axial attention to process global information while maintaining linear complexity. Extensive experiments demonstrate that the proposed TTDMamba exhibits remarkable efficacy.},
  archive      = {J_IJCV},
  author       = {Duan, Junxian and Sun, Hao and Ji, Fan and Zhou, Kai and Wang, Zhiyong and Huang, Huaibo and Jin, Lianwen},
  doi          = {10.1007/s11263-025-02515-2},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6993-7011},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RealDTT: Towards a comprehensive real-world dataset for tampered text detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale texture fusion for reference-based image super-resolution: New dataset and solution. <em>IJCV</em>, <em>133</em>(10), 6971-6992. (<a href='https://doi.org/10.1007/s11263-025-02514-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR) methods often encounter great performance drops on severe degraded low-resolution (LR) images. Recently, reference-based super-resolution (RefSR) methods offer a promising solution by introducing high-quality reference (Ref) images as prior for reconstruction. However, existing RefSR methods often struggle to effectively explore informative textures from Ref images. Additionally, their performance is significantly restricted by the quality of the available training dataset. To address these challenges, we propose an innovative multi-scale texture fusion for reference-based super-resolution via a state-space model, which enables efficient multi-scale feature fusion and long-term dependency modeling for better texture restoration. Specifically, our method mainly consists of a series of texture matching fusion groups (TMFG), which include a multi-scale matching module (MSMM) and a state-space fusion module (SSFM). MSMM can match multi-scale features of Ref images at different stages of the image restoration process to accurately locate key multi-scale similar textures contained in Ref images. SSFM effectively fuses multi-scale textures obtained from Ref images by leveraging stronger modeling relationships based on long-term dependencies of linear complexity. Such a design encourages in-depth exploration of the multi-scale texture correspondence between LR and Ref images, thereby achieving the utilization of textures in Ref and better assisting in image restoration. Notably, we introduce a new large-scale dataset, dubbed DRefSR, designed explicitly for the RefSR task. Our DRefSR offers a wider variety of scenes, more accurately matched image pairs, and a larger volume of samples. With 47,653 image pairs, our dataset substantially exceeds existing datasets (13,761 pairs) in scale. Experiments verify our dataset’s superiority and demonstrate that our method outperforms SOTA methods both quantitatively and qualitatively. DRefSR dataset and code are available at: https://github.com/edbca/SSMTF .},
  archive      = {J_IJCV},
  author       = {Zhou, Hongyang and Zhu, Xiaobin and Qin, Jingyan and Xu, Yu and Cesar-Jr., Roberto M. and Yin, Xu-Cheng},
  doi          = {10.1007/s11263-025-02514-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6971-6992},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-scale texture fusion for reference-based image super-resolution: New dataset and solution},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reblurring-guided single image defocus deblurring: A learning framework with misaligned training pairs. <em>IJCV</em>, <em>133</em>(10), 6953-6970. (<a href='https://doi.org/10.1007/s11263-025-02522-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For single image defocus deblurring, acquiring well-aligned training pairs (or training triplets), i.e., a defocus blurry image, an all-in-focus sharp image (and a defocus blur map), is a challenging task for developing effective deblurring models. Existing image defocus deblurring methods typically rely on training data collected by specialized imaging equipment, with the assumption that these pairs or triplets are perfectly aligned. However, in practical scenarios involving the collection of real-world data, direct acquisition of training triplets is infeasible, and training pairs inevitably encounter spatial misalignment issues. In this work, we introduce a reblurring-guided learning framework for single image defocus deblurring, enabling the learning of a deblurring network even with misaligned training pairs. By reconstructing spatially variant isotropic blur kernels, our reblurring module ensures spatial consistency between the deblurred image, the reblurred image and the input blurry image, thereby addressing the misalignment issue while effectively extracting sharp textures from the all-in-focus sharp image. Moreover, spatially variant blur can be derived from the reblurring module, and serve as pseudo supervision for defocus blur map during training, interestingly transforming training pairs into training triplets. To leverage this pseudo supervision, we propose a lightweight defocus blur estimator coupled with a fusion block, which enhances deblurring performance through seamless integration with state-of-the-art deblurring networks. Additionally, we have collected a new dataset for single image defocus deblurring (SDD) with typical misalignments, which not only validates our proposed method but also serves as a benchmark for future research. The effectiveness of our method is validated by notable improvements in both quantitative metrics and visual quality across several datasets with real-world defocus blurry images, including DPDD, RealDOF, DED, and our SDD. The source code and dataset are available at https://github.com/ssscrystal/Reblurring-guided-JDRL .},
  archive      = {J_IJCV},
  author       = {Ren, Dongwei and Shu, Xinya and Li, Yu and Wu, Xiaohe and Li, Jin and Zuo, Wangmeng},
  doi          = {10.1007/s11263-025-02522-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6953-6970},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Reblurring-guided single image defocus deblurring: A learning framework with misaligned training pairs},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAT-TPT: Class-agnostic text-based test-time prompt tuning for vision-language models. <em>IJCV</em>, <em>133</em>(10), 6930-6952. (<a href='https://doi.org/10.1007/s11263-025-02508-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning has emerged as an effective method for adapting pre-trained vision-language models (VLMs) to diverse downstream tasks. However, it often struggles with generalization to unseen domains due to its dependence on labeled data. Unlike traditional approaches that rely on fixed prompts or parameters learned during training, Test-time Prompt Tuning (TPT) dynamically refines learnable prompts for individual samples at test time. Nevertheless, existing TPT methods frequently overlook alignment between visual and textual embeddings and lack mechanisms to ensure intra-modal diversity. In this work, we introduce CAT-TPT (Class-Agnostic Text-based Test-time Prompt Tuning), a novel approach that integrates attribute-guided augmentation, improved visual-textual alignment, and label-free adaptation for VLMs. By leveraging class-agnostic attributes generated by a large language model, CAT-TPT jointly optimizes both vision and language modalities, promoting enhanced intra-class diversity and seamless adaptation at test time. Extensive experiments demonstrate that CAT-TPT consistently outperforms state-of-the-art methods in zero-shot generalization, achieving an average improvement of 6.66% over existing TPT methods on out-of-distribution (OOD) data across five benchmarks, 3.17% in cross-dataset evaluations across ten fine-grained datasets, and 4.04% under fifteen diverse and challenging corruption types. Code is available at https://github.com/AIM-SKKU/CAT-TPT .},
  archive      = {J_IJCV},
  author       = {Zhang, Youjia and Liu, Huiling and Kim, Youngeun and Hong, Sungeun},
  doi          = {10.1007/s11263-025-02508-1},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6930-6952},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CAT-TPT: Class-agnostic text-based test-time prompt tuning for vision-language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guiding audio-visual question answering with collective question reasoning. <em>IJCV</em>, <em>133</em>(10), 6912-6929. (<a href='https://doi.org/10.1007/s11263-025-02510-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-Visual Question Answering (AVQA) requires the model to answer questions with complex dynamic audio-visual information. Prior works on this task mainly consider only using single question-answer pairs during training, overlooking the rich semantic associations between questions. In this work, we propose a novel Collective Question-Guided Network (CoQo), which accepts multiple question-answer pairs as input and leverages the reasoning over these questions to assist the model training process. The core module is the proposed Question Guided Transformer (QGT), which uses collective question reasoning to perform question-guided feature extraction. Since multiple question-answer pairs are not always available, especially during inference, our QGT uses a set of learnable tokens to learn the collective information from multiple questions during training. At inference time, these learnable tokens bring additional reasoning information even when only one question is used as input. We employ QGT in both spatial and temporal dimensions to extract question-related features effectively and efficiently. To better capture detailed audio-visual associations, we train the model in a finer level by distinguishing feature pairs of different questions within the same video. Extensive experiments demonstrate that our method can achieve state-of-the-art performance on three AVQA datasets while reducing training time significantly. We also observe strong performances of our method on three VQA benchmarks. Detailed ablation studies further confirm the effectiveness of our proposed collective question reasoning scheme, both quantitatively and qualitatively.},
  archive      = {J_IJCV},
  author       = {Pei, Baoqi and Huang, Yifei and Chen, Guo and Xu, Jilan and Wang, Yali and Wang, Limin and Lu, Tong and Qiao, Yu and Wu, Fei},
  doi          = {10.1007/s11263-025-02510-7},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6912-6929},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guiding audio-visual question answering with collective question reasoning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic masking with curriculum learning for robust HDR image reconstruction. <em>IJCV</em>, <em>133</em>(10), 6896-6911. (<a href='https://doi.org/10.1007/s11263-025-02504-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Dynamic Range (HDR) image reconstruction aims to reconstruct images with a larger dynamic range from multiple Low Dynamic Range (LDR) images with different exposures. Existing methods face two challenges: visual artifacts in the restored images and insufficient model generalization capabilities. This paper addresses these issues by leveraging the inherent potential of Masked Image Modeling (MIM). We propose a Segment Anything Model (SAM)-guided masking strategy, leveraging large-model priors to direct the HDR reconstruction network via curriculum learning. This strategy gradually increases the difficulty from simple to complex tasks, guiding the model to effectively learn semantic priors that prevent the model from overfitting to the training data. Our approach starts by training the model without any masks, then progressively increasing the masking ratio of input features guided by semantic segmentation maps, which compels the model to learn semantic information during restoration. Subsequently, we make an adaption to reduce the masking ratio to minimize the input discrepancy between the training and testing stage. Besides, we manipulate the computation of the loss based on the perceptual quality of reconstructed images, where challenging areas (e.g., over-/under-exposed regions) are given more weight to improve image restoration results. Furthermore, through specialized module design, our method can be fine-tuned to any number of inputs, achieving comparable performance to models trained from scratch with only 5.5% of parameter adjustments. Extensive qualitative and quantitative experiments demonstrate that our approach surpasses state-of-the-art methods in both effectiveness and generalization. Our code is available at: https://github.com/eezkni/SMHDR},
  archive      = {J_IJCV},
  author       = {Ni, Zhangkai and Zhang, Yang and Ren, Kerui and Yang, Wenhan and Wang, Hanli and Kwong, Sam},
  doi          = {10.1007/s11263-025-02504-5},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6896-6911},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic masking with curriculum learning for robust HDR image reconstruction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning extensible series-parallel lookup tables for efficient image super-resolution. <em>IJCV</em>, <em>133</em>(10), 6873-6895. (<a href='https://doi.org/10.1007/s11263-025-02516-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lookup table (LUT) has shown its efficacy in low-level vision tasks due to the valuable characteristics of low computational cost and hardware independence. However, recent attempts to address the problem of single image super-resolution (SISR) with lookup tables are highly constrained by the small receptive field size. Besides, their frameworks of finite-layer lookup tables limit the extension and generalization capacities of the model. In this paper, we propose a framework of series-parallel lookup tables (SPLUT) to alleviate the above issues and achieve efficient image super-resolution. On the one hand, we cascade multiple lookup tables to enlarge the receptive field of each extracted feature vector. On the other hand, we propose a parallel network which includes two branches of cascaded lookup tables which process different components of the input low-resolution images. By doing so, the two branches collaborate with each other and compensate for the precision loss of discretizing input pixels when establishing a lookup table. Since SPLUT uniformly discretizes the input density space for the series-parallel index, the input distortion by sub-optimal index quantizers results in inefficient utilization of the non-linearities capability for each LUT. With LUTs expansion, the performance becomes increasingly constrained due to the cumulative quantization errors. To address this, we propose SPLUT with learnable index quantizers, named LISPLUT, which automatically adjusts index distributions by non-uniformly allocating the quantization thresholds according to the input density distribution with minimal storage and computational overhead. Moreover, we introduce a lightweight bit-level spatial compensation to inherit the computational efficiency of the independent parallel topology and compensate for the quantization drop by enhancing the guidance of the complementary bit-level components. Compared to previous lookup table-based methods, our framework has stronger representation abilities with more flexible architectures. Furthermore, we no longer need interpolation methods which introduce redundant computations so that our method can achieve less energy cost. Extensive experimental results on five popular benchmark datasets show that our method obtains superior SISR performance in a more efficient way. The code is available at https://github.com/zhjy2016/SPLUT .},
  archive      = {J_IJCV},
  author       = {Zhang, Jingyi and Wang, Ziwei and Ma, Cheng and Zhou, Jie and Lu, Jiwen},
  doi          = {10.1007/s11263-025-02516-1},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6873-6895},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning extensible series-parallel lookup tables for efficient image super-resolution},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A likelihood ratio-based approach to segmenting unknown objects. <em>IJCV</em>, <em>133</em>(10), 6860-6872. (<a href='https://doi.org/10.1007/s11263-025-02509-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the Out-of-Distribution (OoD) segmentation task is a prerequisite for perception systems operating in an open-world environment. Large foundational models are frequently used in downstream tasks, however, their potential for OoD remains mostly unexplored. We seek to leverage a large foundational model to achieve robust representation. Outlier supervision is a widely used strategy for improving OoD detection of the existing segmentation networks. However, current approaches for outlier supervision involve retraining parts of the original network, which is typically disruptive to the model’s learned feature representation. Furthermore, retraining becomes infeasible in the case of large foundational models. Our goal is to retrain for outlier segmentation without compromising the strong representation space of the foundational model. To this end, we propose an adaptive, lightweight unknown estimation module (UEM) for outlier supervision that significantly enhances the OoD segmentation performance without affecting the learned feature representation of the original network. UEM learns a distribution for outliers and a generic distribution for known classes. Using the learned distributions, we propose a likelihood-ratio-based outlier scoring function that fuses the confidence of UEM with that of the pixel-wise segmentation inlier network to detect unknown objects. We also propose an objective to optimize this score directly. Our approach achieves a new state-of-the-art across multiple datasets, outperforming the previous best method by 5.74% average precision points while having a lower false-positive rate. Importantly, strong inlier performance remains unaffected. The code and pre-trained models are available at: https://github.com/NazirNayal8/UEM-likelihood-ratio .},
  archive      = {J_IJCV},
  author       = {Nayal, Nazir and Shoeb, Youssef and Güney, Fatma},
  doi          = {10.1007/s11263-025-02509-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6860-6872},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A likelihood ratio-based approach to segmenting unknown objects},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of few-shot action recognition. <em>IJCV</em>, <em>133</em>(10), 6832-6859. (<a href='https://doi.org/10.1007/s11263-025-02503-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot action recognition aims to address the high cost and impracticality of manually labeling complex and variable video data in action recognition. It requires accurately classifying human actions in videos using only a few labeled examples per class. Compared to few-shot learning in image scenarios, few-shot action recognition is more challenging due to the intrinsic complexity of video data. Numerous approaches have driven significant advancements in few-shot action recognition, which underscores the need for a comprehensive survey. Unlike early surveys that focus on few-shot image or text classification, we deeply consider the unique challenges of few-shot action recognition. In this survey, we provide a comprehensive review of recent methods and introduce a novel and systematic taxonomy of existing approaches, accompanied by a detailed analysis. We categorize the methods into generative-based and meta-learning frameworks, and further elaborate on the methods within the meta-learning framework, covering aspects: video instance representation, category prototype learning, and generalized video alignment. Additionally, the survey presents the commonly used benchmarks and discusses relevant advanced topics and promising future directions. We hope this survey can serve as a valuable resource for researchers, offering essential guidance to newcomers and stimulating seasoned researchers with fresh insights.},
  archive      = {J_IJCV},
  author       = {Wanyan, Yuyang and Yang, Xiaoshan and Dong, Weiming and Xu, Changsheng},
  doi          = {10.1007/s11263-025-02503-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6832-6859},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A comprehensive review of few-shot action recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generic scene graph generation model with hierarchical prompt learning. <em>IJCV</em>, <em>133</em>(10), 6813-6831. (<a href='https://doi.org/10.1007/s11263-025-02499-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene Graph Generation (SGG) delivers structured knowledge to represent complex scenes and has proven effective in many computer vision tasks. However, traditional SGG models suffer from two limitations that hinder their applicability for higher-level visual tasks: (1) a rigid structure that results in low efficiency and limited flexibility, and (2) biased optimization that results in biased predictions that favor uninformative predicates. To resolve these two issues, we propose GSGG (Generic Scene Graph Generation), a novel, efficient, and flexible SGG model that (1) combines generalized modules to construct top-performance and high-efficiency SGG model and (2) employs a prompt learning-based relation decoder with a novel Hierarchical Prompt (HP) learning method to mitigate biased optimization. HP utilizes the composition of basic prompts constrained to progressively narrowed class groups and encourages the corresponding prompts to focus on the learning of increasingly informative predicates. Extensive evaluations on three SGG benchmarks demonstrate the excellent efficiency and performance of GSGG with HP. We also introduce a novel predicate generalization task with a new benchmark, and experiments on it demonstrate the effectiveness of HP in base-to-novel predicate generalization.},
  archive      = {J_IJCV},
  author       = {Zhu, Xuhan and Xing, Yifei and Wang, Ruiping and Wang, Yaowei and Lan, Xiangyuan},
  doi          = {10.1007/s11263-025-02499-z},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6813-6831},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generic scene graph generation model with hierarchical prompt learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TokenPacker: Efficient visual projector for multimodal LLM. <em>IJCV</em>, <em>133</em>(10), 6794-6812. (<a href='https://doi.org/10.1007/s11263-025-02491-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multimodal large language models (MLLMs), the visual projector is a crucial component that connects the visual encoder with the large language model (LLM). Most current MLLMs adopt a simple multi-layer perceptron (MLP) to preserve visual contexts via direct transformation. However, this approach tends to generate redundant visual tokens, particularly when processing high-resolution images, ultimately reducing the efficiency of MLLMs. Recent efforts to address this issue have employed resamplers or abstractors to reduce token quantity. Unfortunately, these methods often fail to capture finer details, thereby limiting the model’s visual reasoning capabilities. In this work, we introduce TokenPacker, a novel visual projector designed to generate condensed visual tokens through a coarse-to-fine scheme. Initially, we interpolate the visual features into a low-resolution point query that provides an overall visual representation. We then integrate high-resolution, multi-level regional cues using a region-to-point injection module, which enriches the point query with local context. This enhancement effectively transforms the initial query into a more detailed representation suitable for LLM reasoning. Furthermore, we propose a dynamic image slicing scheme to efficiently handle high-resolution images with TokenPacker. Extensive experiments demonstrate that TokenPacker can compress the visual tokens by 75% $$\sim $$ 89%, while maintaining or even improving performance on various benchmarks, achieving significantly higher efficiency. The source codes and models can be found at https://github.com/CircleRadon/TokenPacker .},
  archive      = {J_IJCV},
  author       = {Li, Wentong and Yuan, Yuqian and Liu, Jian and Tang, Dongqi and Wang, Song and Qin, Jie and Zhu, Jianke and Zhang, Lei},
  doi          = {10.1007/s11263-025-02491-7},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6794-6812},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {TokenPacker: Efficient visual projector for multimodal LLM},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust object detection with domain-invariant training and continual test-time adaptation. <em>IJCV</em>, <em>133</em>(10), 6768-6793. (<a href='https://doi.org/10.1007/s11263-025-02465-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world environment can be highly dynamic causing substantial domain shifts. Such real-world domain shifts can span over time with domain changes across multiple domains, manifested into the pertinent content or style changes, or both, where content may refer to underlying image layout and styles are domain-specific such as color and texture. Performance of safety-critical applications, especially robust object detection system in autonomous driving, must adapt to such test-time domain shifts. However, our empirical analysis shows existing domain adaptation and generalization methods fail to fit the domain changes with substantial style or content shifts. In this paper, we first analyze and investigate effective solutions to overcome domain overfitting for robust object detection without the above shortcomings. To simultaneously address temporal and multiple domain shifts, we propose a continual test-time generalizable domain adaptation (CoTGA) method for robust object detection: 1) the domain-invariant training (DIT) module leverages the Normalization Perturbation (NP) method to initialize a style-invariant object detection model, by perturbing the channel statistics of source domain low-level features to synthesize various latent styles. The trained deep model can perceive diverse potential domains and generalizes well even without observations of target domain data in training; 2) the test-time adaptation (TTA) module updates the DIT-trained model online during inference, through the consistency regularization between predictions of the weakly and strongly augmented unlabeled images. TTA addresses the content discrepancies problem of the DIT-initialized generalizable model; 3) the generalizable weights preservation (GWP) module keeps the learned generalizable weights to avoid domain overfitting in generalization across multiple domains. Extensive experiments demonstrate these three modules collaboratively enable a deep model to generalize well under challenging real-world domain shifts.},
  archive      = {J_IJCV},
  author       = {Fan, Qi and Segu, Mattia and Schiele, Bernt and Dai, Dengxin and Tai, Yu-Wing and Tang, Chi-Keung},
  doi          = {10.1007/s11263-025-02465-9},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6768-6793},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust object detection with domain-invariant training and continual test-time adaptation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on training paradigms for deep supervised hashing. <em>IJCV</em>, <em>133</em>(10), 6729-6767. (<a href='https://doi.org/10.1007/s11263-025-02506-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep supervised hashing have made remarkable achievements in the large-scale image retrieval task. However, the main training paradigms (i.e., pairwise and pointwise) of existing deep supervised hashing methods, which will significantly impact the performance of deep supervised hashing methods under practical retrieval tasks, remain insufficiently explored. Motivated by the critical role of training paradigms in deep supervised hashing and the lack of comprehensive evaluations in this area, we systematically establish the evaluation protocols and conduct an extensive study through 1,833 experiments, yielding 7,332 results across 12 datasets. Our key findings include observations such as: 1) Pointwise hashing methods tend to exhibit higher retrieval accuracy in scenarios with seen-class queries but underperform significantly with unseen-class queries. 2) Pointwise hashing methods show greater robustness with seen-class queries, whereas pairwise hashing methods with soft constraints excel when queries are from unseen classes. 3) The impact of hash code dimensions is minimal on the retrieval performance of pointwise hashing methods but more pronounced for pairwise hashing, primarily due to suboptimal real-valued feature optimization. Code along with training logs for all experiments are open-source and available at https://github.com/aassxun/DSH_Analysis .},
  archive      = {J_IJCV},
  author       = {Shen, Yang and Wang, Peng and Wei, Xiu-Shen and Yao, Yazhou},
  doi          = {10.1007/s11263-025-02506-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6729-6767},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An empirical study on training paradigms for deep supervised hashing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implementation and validation of distributed bundle adjustment for super large scale datasets. <em>IJCV</em>, <em>133</em>(10), 6712-6728. (<a href='https://doi.org/10.1007/s11263-025-02505-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a distributed bundle adjustment (DBA) method using the Levenberg-Marquardt (LM) algorithm for super large-scale datasets. Most of the existing methods partition the global map to small ones and conduct bundle adjustment in the submaps. In order to fit the parallel framework, they use approximate solutions instead of the LM algorithm. However, those methods often give sub-optimal results. Different from them, we utilize the LM algorithm to conduct global bundle adjustment where the formation of the reduced camera system (RCS) is actually parallelized and executed in a distributed way. To store the large RCS, we compress it with a block-based sparse matrix compression format (BSMC), which fully exploits its block feature. The BSMC format also enables the distributed storage and updating of the global RCS. The proposed method is extensively evaluated and compared with the state-of-the-art pipelines using both synthetic and real datasets. Preliminary results demonstrate the efficient memory usage and vast scalability of the proposed method compared with the baselines. For the first time, we conducted parallel bundle adjustment using LM algorithm on a real datasets with 1.18 million images and a synthetic dataset with 10 million images (about 500 times that of the state-of-the-art LM-based BA) on a distributed computing system.},
  archive      = {J_IJCV},
  author       = {Zheng, Maoteng and Chen, Nengcheng and Zhu, Junfeng and Zeng, Xiaoru and Qiu, Huanbin and Jiang, Yuyao and Lu, Xingyue and Qu, Hao},
  doi          = {10.1007/s11263-025-02505-4},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6712-6728},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Implementation and validation of distributed bundle adjustment for super large scale datasets},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting lightweight hierarchical ViT and dynamic framework for efficient visual tracking. <em>IJCV</em>, <em>133</em>(10), 6689-6711. (<a href='https://doi.org/10.1007/s11263-025-02500-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based visual trackers have demonstrated significant advancements due to their powerful modeling capabilities. However, their practicality is limited on resource-constrained devices because of their slow processing speeds. To address this challenge, we present HiT, a novel family of efficient tracking models that achieve high performance while maintaining fast operation across various devices. The core innovation of HiT lies in its Bridge Module, which connects lightweight transformers to the tracking framework, enhancing feature representation quality. Additionally, we introduce a dual-image position encoding approach to effectively encode spatial information. HiT achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark, outperforming all previous efficient trackers. Building on HiT, we propose DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by selecting routes with varying computational requirements. DyHiT uses search area features extracted by the backbone network and inputs them into an efficient dynamic router to classify tracking scenarios. Based on the classification, DyHiT applies a divide-and-conquer strategy, selecting appropriate routes to achieve a superior trade-off between accuracy and speed. The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while maintaining an AUC of 62.4% on LaSOT. Furthermore, we introduce a training-free acceleration method based on the dynamic routing architecture of DyHiT. This method significantly improves the execution speed of various high-performance trackers without sacrificing accuracy. For instance, our acceleration method enables the state-of-the-art tracker SeqTrack-B256 to achieve a $$2.68\times $$ speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of 69.9% on the LaSOT. Codes, models, and results are available at https://github.com/kangben258/HiT .},
  archive      = {J_IJCV},
  author       = {Kang, Ben and Chen, Xin and Zhao, Jie and Bo, Chunjuan and Wang, Dong and Lu, Huchuan},
  doi          = {10.1007/s11263-025-02500-9},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6689-6711},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploiting lightweight hierarchical ViT and dynamic framework for efficient visual tracking},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From forest to zoo: Great ape behavior recognition with ChimpBehave. <em>IJCV</em>, <em>133</em>(10), 6668-6688. (<a href='https://doi.org/10.1007/s11263-025-02484-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the significant challenge of recognizing behaviors in non-human primates, specifically focusing on chimpanzees. Automated behavior recognition is crucial for both conservation efforts and the advancement of behavioral research. However, it is often hindered by the labor-intensive process of manual video annotation. Despite the availability of large-scale animal behavior datasets, effectively applying machine learning models across varied environmental settings remains a critical challenge due to the variability in data collection contexts and the specificity of annotations. In this paper, we introduce ChimpBehave, a novel dataset comprising over 2 h and 20 min of video (approximately 215,000 frames) of zoo-housed chimpanzees, annotated with bounding boxes and fine-grained locomotive behavior labels. Uniquely, ChimpBehave aligns its behavior classes with those in PanAf, an existing dataset collected in distinct visual environments, enabling the study of cross-dataset generalization - where models are trained on one dataset and tested on another with differing data distributions. We benchmark ChimpBehave using state-of-the-art video-based and skeleton-based action recognition models, establishing performance baselines for both within-dataset and cross-dataset evaluations. Our results highlight the strengths and limitations of different model architectures, providing insights into the application of automated behavior recognition across diverse visual settings. The dataset, models, and code can be accessed at: https://github.com/MitchFuchs/ChimpBehave},
  archive      = {J_IJCV},
  author       = {Fuchs, Michael and Genty, Emilie and Bangerter, Adrian and Zuberbühler, Klaus and Odobez, Jean-Marc and Cotofrei, Paul},
  doi          = {10.1007/s11263-025-02484-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6668-6688},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {From forest to zoo: Great ape behavior recognition with ChimpBehave},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision generalist model: A survey. <em>IJCV</em>, <em>133</em>(10), 6639-6667. (<a href='https://doi.org/10.1007/s11263-025-02502-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have witnessed the great success of the generalist model in natural language processing. The generalist model is a general framework trained with massive data and is able to process various downstream tasks simultaneously. Encouraged by their impressive performance, an increasing number of researchers are venturing into the realm of applying these models to computer vision tasks. However, the inputs and outputs of vision tasks are more diverse, and it is difficult to summarize them as a unified representation. In this paper, we provide a comprehensive overview of the vision generalist models, delving into their characteristics and capabilities within the field. First, we review the background, including the datasets, tasks, and benchmarks. Then, we dig into the design of frameworks that have been proposed in existing research, while also introducing the techniques employed to enhance their performance. To better help the researchers comprehend the area, we take a brief excursion into related domains, shedding light on their interconnections and potential synergies. To conclude, we provide some real-world application scenarios, undertake a thorough examination of the persistent challenges, and offer insights into possible directions for future research endeavors.},
  archive      = {J_IJCV},
  author       = {Wang, Ziyi and Rao, Yongming and Sun, Shuofeng and Liu, Xinrun and Wei, Yi and Yu, Xumin and Liu, Zuyan and Wang, Yanbo and Liu, Hongmin and Zhou, Jie and Lu, Jiwen},
  doi          = {10.1007/s11263-025-02502-7},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6639-6667},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Vision generalist model: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
