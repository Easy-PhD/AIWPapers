<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ML</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ml">ML - 9</h2>
<ul>
<li><details>
<summary>
(2025). Causality from bottom to top: A survey. <em>ML</em>, <em>114</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10994-025-06855-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality has become a fundamental approach for explaining the relationships between events, phenomena, and outcomes in various fields of study. It has invaded various fields and applications, such as medicine, healthcare, economics, finance, fraud detection, cybersecurity, education, public policy, recommender systems, anomaly detection, robotics, control, sociology, marketing, and advertising. In this paper, we survey its development over the past five decades, shedding light on the differences between causality and other approaches, as well as the preconditions for using it. Furthermore, the paper illustrates how causality interacts with new approaches such as Artificial Intelligence (AI), Generative AI (GAI), Machine and Deep Learning, Reinforcement Learning (RL), and Fuzzy Logic. We study the impact of causality on various fields, its contribution, and its interaction with state-of-the-art approaches. Additionally, the paper exemplifies the trustworthiness and explainability of causality models. We offer several ways to evaluate causality models and discuss future directions.},
  archive      = {J_ML},
  author       = {Weinberg, Abraham Itzhak and Premebida, Cristiano and Faria, Diego Resende},
  doi          = {10.1007/s10994-025-06855-5},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Causality from bottom to top: A survey},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Special issue on machine learning in soccer. <em>ML</em>, <em>114</em>(11), 1-8. (<a href='https://doi.org/10.1007/s10994-025-06783-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Berrar, Daniel and Davis, Jesse and Lopes, Philippe and Dubitzky, Werner},
  doi          = {10.1007/s10994-025-06783-4},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-8},
  shortjournal = {Mach. Learn.},
  title        = {Guest editorial: Special issue on machine learning in soccer},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with confidence: Training better classifiers from soft labels. <em>ML</em>, <em>114</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10994-025-06860-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised machine learning, models are typically trained using data with hard labels, i.e., definite assignments of class membership. This traditional approach, however, does not take the inherent uncertainty in these labels into account. We investigate whether incorporating label uncertainty, represented for each instance as a discrete probability distribution over the class labels, known as a soft label, improves the predictive performance of classification models, focusing on tabular data. We first demonstrate the potential value of soft label learning (SLL) for estimating model parameters in a simulation experiment, particularly for limited sample sizes and imbalanced data. Subsequently, we compare the performance of various wrapper methods for learning from both hard and soft labels using identical base classifiers. On real-world-inspired synthetic data with clean labels, the SLL methods consistently outperform the hard label methods. Since real-world data is often noisy and precise soft labels are challenging to obtain, we study the effect that noisy probability estimates have on model performance. Alongside conventional noise models, our study examines four types of miscalibration that are known to affect human annotators. The results show that SLL methods outperform the hard label methods in the majority of settings. Finally, we evaluate the methods on a real-world dataset with confidence scores, where the SLL methods are shown to match the traditional methods for predicting the (noisy) hard labels while providing more accurate confidence estimates.},
  archive      = {J_ML},
  author       = {de Vries, Sjoerd and Thierens, Dirk},
  doi          = {10.1007/s10994-025-06860-8},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Learning with confidence: Training better classifiers from soft labels},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-based feature generation from text for interpretable machine learning. <em>ML</em>, <em>114</em>(11), 1-30. (<a href='https://doi.org/10.1007/s10994-025-06867-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional text representations like embeddings and bag-of-words hinder rule learning and other interpretable machine learning methods due to high dimensionality and poor comprehensibility. This article investigates using Large Language Models (LLMs) to extract a small number of interpretable text features. We propose two workflows: one fully automated by the LLM (feature proposal and value calculation), and another where users define features and the LLM calculates values. This LLM-based feature extraction enables interpretable rule learning, overcoming issues like spurious interpretability seen with bag-of-words. We evaluated the proposed methods on five diverse datasets (including scientometrics, banking, hate speech, and food hazard). LLM-generated features yielded predictive performance similar to the SciBERT embedding model but used far fewer, interpretable features. Most generated features were considered relevant for the corresponding prediction tasks by human users. We illustrate practical utility on a case study focused on mining recommendation action rules for the improvement of research article quality and citation impact.},
  archive      = {J_ML},
  author       = {Balek, Vojtěch and Sýkora, Lukáš and Sklenák, Vilém and Kliegr, Tomáš},
  doi          = {10.1007/s10994-025-06867-1},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {LLM-based feature generation from text for interpretable machine learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISA3: A 3-dimensional expansion of instance space analysis. <em>ML</em>, <em>114</em>(11), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06871-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The experimental validation of algorithms depends strongly on the characteristics of the test set used. Ideally, such a set should exhibit diverse characteristics that challenge an algorithm and are present in real-world problems. An approach to examine the diversity and representativeness of a test set is Instance Space Analysis, which uses a 2D projection to visualise the test set while identifying the strengths and weaknesses of competing algorithms. However, this has the limitation of discarding potentially useful information while crowding out the space as more features and algorithms are considered. This paper describes an extension of Instance Space Analysis into 3D, which retains a higher degree of information while maintaining the explainability of the visualisation by finding the rotations that maximise the linear trends. In addition to the expansion to 3D, a new algorithm for identifying portfolio footprints is introduced, offering a more robust and reliable method for identifying footprints in both 2D and 3D instance spaces. As a case study, we present a performance analysis of unsupervised anomaly detection methods, subject to changes in the normalisation technique. The results demonstrate the advantages by identifying regions of strength of algorithms previously thought to be underpowered.},
  archive      = {J_ML},
  author       = {Simpson, Connor and Muñoz, Mario Andrés and Kandanaarachchi, Sevvandi and Campello, Ricardo J. G. B.},
  doi          = {10.1007/s10994-025-06871-5},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {ISA3: A 3-dimensional expansion of instance space analysis},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASS-CSP: Mining with answer set solving for contrast sequential pattern mining. <em>ML</em>, <em>114</em>(11), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06876-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present MASS-CSP (Mining with Answer Set Solving - Contrast Sequential Patterns), a declarative approach to the Contrast Sequential Pattern Mining (CSPM) task, which is based on the logic-based framework of Answer Set Programming (ASP). The CSPM task focuses on identifying significant differences in frequent sequences relative to specific classes, leading to the concept of a contrast sequential pattern. The article describes how MASS-CSP addresses the CSPM task and related extensions-mining closed, maximal and constrained patterns. Evaluation aims at comparing the basic version of MASS-CSP against the extended versions as regards the size of output and time-memory requirements.},
  archive      = {J_ML},
  author       = {Sterlicchio, Gioacchino and Lisi, Francesca Alessandra},
  doi          = {10.1007/s10994-025-06876-0},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {MASS-CSP: Mining with answer set solving for contrast sequential pattern mining},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evo-path: A two-stage temporal knowledge graph reasoning model and its application in human behavior prediction. <em>ML</em>, <em>114</em>(11), 1-28. (<a href='https://doi.org/10.1007/s10994-025-06886-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Knowledge Graphs (TKGs) contain a vast number of facts with timestamps in the real world and have more abundant semantic information compared with Knowledge Graphs (KGs). However, TKGs are usually incomplete. Reasoning potential facts in the future is a challenging task and has emerged as a hotspot in the research of TKGs. One key of this task is to dive deep into the evolutional patterns contained in historical facts, which are helpful to predict future facts. However, most of the existing models focus on modeling evolutional patterns based on the entire graph, which ignores the important role of the query-related paths. Furthermore, the existing graph neural networks (GNNs) cannot efficiently extract the graph structural information that is vital to model the evolutional pattern. In order to address these two problems, we propose a two-stage reasoning model Evo-Path in this paper. At the temporal path searching stage, Evo-Path learns a temporal-semantic policy network and employs beam search policy to obtain the clue paths and candidate answers based on Deep Reinforcement Learning (DRL). At the evolutional reasoning stage, we propose a Multi-Relational Graph Attention Network (MRGAT) to encode the structural information of clue subgraphs so as to model the evolutional patterns over clue paths and deduce final answers. We evaluate our model on four public TKG datasets: ICEWS14, ICEWS18-7000, ICEWS05-15-7000, and GDELT. Extensive experimental results show that Evo-Path not only outperforms other state-of-the-art baselines in MRR and Hits@1 but also has interpretability for reasoning results. Three application case studies highlight the significant value of our model for predicting human behavior.},
  archive      = {J_ML},
  author       = {He, Mingsheng and Zhu, Lin and Bai, Luyi},
  doi          = {10.1007/s10994-025-06886-y},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Evo-path: A two-stage temporal knowledge graph reasoning model and its application in human behavior prediction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DP-FedSecure: A secure and efficient federated learning scheme based on adaptive differential privacy. <em>ML</em>, <em>114</em>(11), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06888-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning, as a paradigm of distributed machine learning, allows participants to collaboratively model without sharing data, effectively addressing the data island problem. However, relying solely on model transmission still poses privacy leakage risks. Hao et al. proposed an efficient and lightweight federated learning scheme, HLX+19, based on differential privacy, which demonstrates significant advantages in privacy protection and performance. However, we found that HLX+19 has inherent security vulnerabilities and limited scalability. Therefore, this paper first designs a reconstruction attack experiment aimed at approximating the recovery of original data from the noise-added data of HLX+19 in a collusion scenario. Subsequently, we propose DP-FedSecure, a federated learning privacy protection scheme based on adaptive noise. By adopting adaptive noise, we enhance the randomness of the noise distribution, thereby improving the security of the scheme. We conducted a security analysis of DP-FedSecure and experimentally validated its effectiveness in resisting reconstruction attacks. Finally, we performed comparative experiments on encryption efficiency and accuracy. The results indicate that, DP-FedSecure achieves approximately 97.43% improvement in encryption efficiency compared to HLX+19, and the impact of security parameters on encryption efficiency has been reduced from exponential to linear. Therefore, DP-FedSecure demonstrates high efficiency and good scalability in terms of encryption. Experiments on real-world datasets further validate the high accuracy of DP-FedSecure, while we balanced the relationship between security and accuracy through adaptive noise.},
  archive      = {J_ML},
  author       = {Chen, Shuo and Zhou, Tanping and Xie, Huiyu and Du, Weidong and Yang, Xiaoyuan},
  doi          = {10.1007/s10994-025-06888-w},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {DP-FedSecure: A secure and efficient federated learning scheme based on adaptive differential privacy},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing RL components for wagner’s framework via brouwer’s conjecture. <em>ML</em>, <em>114</em>(11), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06890-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we continue the study and systematization started in our previous work (Angileri et al. in: Lecture notes in computer science 15243 LNAI, 2025, pp 325–338. https://doi.org/10.1007/978-3-031-78977-9_21 ) of Wagner’s Reinforcement Learning framework to investigate graph conjectures. After identifying three main directions that impact the framework’s performance (the environment dynamics, the RL algorithm and the neural network used as a function approximator), we conduct an ablation study to evaluate the effectiveness of each component, analyzing several variations of them. The experiments compare three environment dynamics, implemented as Gym spaces (Linear, Local and Global), two algorithms [PPO and the Cross-Entropy method (Wagner in Constructions in combinatorics via neural networks, 2021, https://arxiv.org/abs/2104.14516 )], different neural network structures (Multi-Layer Perceptron and Graph Neural Networks) and reward systems. This study was intended not only to test the framework’s capabilities, but also to identify a configuration of environment, algorithm, and neural network that can be effective when exploring graph spaces, even with a complex target. For this reason, all the experiments were executed on Brouwer’s Conjecture. We also present the data collected with the various trained models, as these interesting configurations can be used in the inference process on the problem. Our analysis shows that a proper calibration of the individual components of the framework can significantly improve its performance, suggesting effective settings for addressing complex problems and contributing to the study of Brouwer’s Conjecture. All the codes and data are open source and available at https://github.com/CuriosAI/graph_conjectures .},
  archive      = {J_ML},
  author       = {Angileri, Flora and Lombardi, Giulia and Fois, Andrea and Faraone, Renato and Metta, Carlo and Salvi, Michele and Bianchi, Luigi Amedeo and Fantozzi, Marco and Galfrè, Silvia Giulia and Pavesi, Daniele and Parton, Maurizio and Morandin, Francesco},
  doi          = {10.1007/s10994-025-06890-2},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Analyzing RL components for wagner’s framework via brouwer’s conjecture},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
