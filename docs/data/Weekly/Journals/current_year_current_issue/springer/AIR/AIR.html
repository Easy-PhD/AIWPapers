<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AIR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="air">AIR - 32</h2>
<ul>
<li><details>
<summary>
(2025). Oriented object detection in optical remote sensing images using deep learning: A survey. <em>AIR</em>, <em>58</em>(11), 1-61. (<a href='https://doi.org/10.1007/s10462-025-11256-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented object detection is a fundamental yet challenging task in remote sensing (RS), aiming to locate and classify objects with arbitrary orientations. Recent advancements in deep learning have significantly enhanced the capabilities of oriented object detection methods. Given the rapid development of this field, a comprehensive survey of the recent advances in oriented object detection is presented in this paper. Specifically, we begin by tracing the technical evolution from horizontal object detection to oriented object detection and highlighting the specific related challenges, including feature misalignment, spatial misalignment, oriented bounding box (OBB) regression problems, and common issues encountered in RS. Subsequently, we further categorize the existing methods into detection frameworks, OBB regression techniques, feature representation approaches, and solutions to common issues and provide an in-depth discussion of how these methods address the above challenges. In addition, we cover several publicly available datasets and evaluation protocols. Furthermore, we provide a comprehensive comparison and analysis involving the state-of-the-art methods. Toward the end of this paper, we identify several future directions for oriented object detection research.},
  archive      = {J_AIR},
  author       = {Wang, Kun and Wang, Zi and Li, Zhang and Su, Ang and Teng, Xichao and Pan, Erting and Liu, Minhao and Yu, Qifeng},
  doi          = {10.1007/s10462-025-11256-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Oriented object detection in optical remote sensing images using deep learning: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-mediated healthcare and trust. a trust-construct and trust-factor framework for empirical research. <em>AIR</em>, <em>58</em>(11), 1-17. (<a href='https://doi.org/10.1007/s10462-025-11306-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application of Artificial Intelligence (AI) in healthcare is growing exponentially, and its use is expected to continue expanding in the coming years. However, lack of trust in AI systems remains a significant barrier to their widespread adoption. This article analyzes the problem of trust, its various features and its application in AI-mediated healthcare. We first review the literature on trust and trust in technology to detect which theoretical constructs are essential to trust. We then identify the factors that we consider fundamental for a rich and complex comprehension of trust in AI-mediated healthcare. We finally propose a trust-factor framework that could be used for empirical research on AI-mediated healthcare and its practical implementation.},
  archive      = {J_AIR},
  author       = {Alonso, Marcos and Astobiza, Aníbal M. and Ortega Lozano, Ramón},
  doi          = {10.1007/s10462-025-11306-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-17},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-mediated healthcare and trust. a trust-construct and trust-factor framework for empirical research},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in multimodal differential evolution: A comprehensive review and future perspectives. <em>AIR</em>, <em>58</em>(11), 1-73. (<a href='https://doi.org/10.1007/s10462-025-11314-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal optimization involves identifying multiple global and local optima of a function, offering valuable insights into diverse optimal solutions within the search space. Evolutionary algorithms (EAs) excel at finding various solutions in a single run, providing a distinct advantage over classical optimization techniques that often require multiple restarts without guarantee of obtaining diverse solutions. Among these EAs, differential evolution (DE) stands out as a powerful and versatile optimizer for continuous parameter spaces. DE has shown significant success in multi-modal optimization by utilizing its population-based search to promote the formation of multiple stable subpopulations, each targeting different optima. Recent advancements in DE for multi-modal optimization have focused on niching methods, parameter adaptation, hybridization with other algorithms, including machine learning, and applications across various domains. Given these developments, it is an opportune moment to present a critical review of the latest literature and identify key future research directions. This paper offers a comprehensive overview of recent DE advancements in multimodal optimization, including methods for handling multiple optima, hybridization with EAs, and machine learning, and highlights a range of real-world applications. Additionally, the paper outlines a set of compelling open problems and future research issues from multiple perspectives.},
  archive      = {J_AIR},
  author       = {Chauhan, Dikshit and Shivani and Jung, Donghwi and Yadav, Anupam},
  doi          = {10.1007/s10462-025-11314-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-73},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advancements in multimodal differential evolution: A comprehensive review and future perspectives},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ai-enabled framework for anomaly detection in power distribution networks under false data injection attacks. <em>AIR</em>, <em>58</em>(11), 1-38. (<a href='https://doi.org/10.1007/s10462-025-11318-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern power distribution networks are becoming cyber physical systems due to the addition of more advanced metering infrastructure (AMI). This has introduced new vulnerabilities to cyber threats, particularly false data injection (FDI) attacks. These attacks compromise the integrity of power consumption data, leading to financial losses, operational inefficiencies, and grid instability. Rule-based techniques and traditional machine learning models are two examples of traditional anomaly detection methods that often have problems. Often, these methods generate an excessive number of false alarms, struggle to adapt to new attack patterns, and perform poorly in large-scale deployments. This research suggests a robust anomaly identification framework (AIF) that uses an autoencoder (AE) for feature transformation and a multi-layer perceptron (MLP) to identify anomalies in AMI integrated with smart grids. The proposed approach first applies synthetic features extraction inspired by real-world smart meter capabilities and transforms the dataset using a denoising AE. MLP assisted in the classification to detect multiple FDI attack types with improved accuracy and reliability. Numerous experiments have been performed, and the results indicate that the suggested method works better than popular methods like correlation analysis, techniques based on clustering, and standard outlier identification algorithms. Compared to baseline methods, the proposed technique improves detection accuracy by up to approximately 25%, reduces false positives, and enhances the system’s ability to generalize across different cyberattack strategies. The proposed work computes seven different types of criterion matrices to verify the effectiveness of finding anomalies. The overall average results include mean squared error (0.0793), accuracy (92%), F1-Score (92%), recall (91%), specificity (94%), area under the curve (97%), and mean average precision (96%). These findings accentuate the potential of the proposed AIF performance in fortifying smart grid cybersecurity.},
  archive      = {J_AIR},
  author       = {Ahmad, Hasnain and Mustafa, Ghulam and Gulzar, Muhammad Majid and Ahmed, Ijaz and Khalid, Muhammad},
  doi          = {10.1007/s10462-025-11318-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Ai-enabled framework for anomaly detection in power distribution networks under false data injection attacks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence for secure and sustainable industrial control systems - A survey of challenges and solutions. <em>AIR</em>, <em>58</em>(11), 1-86. (<a href='https://doi.org/10.1007/s10462-025-11320-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industrial environments, the security and sustainability of Industrial Control Systems (ICS) have become crucial. This comprehensive review examines the transformative potential of Artificial Intelligence (AI) in ICS, focusing on technologies like Machine Learning (ML), Deep Learning (DL), Large Language Models (LLMs), and cloud computing. Moreover, this research explores integrating existing and proposed sustainable practices within the ICS framework, with a particular emphasis on energy efficiency and carbon footprint reduction, to enhance the overall sustainability of ICS. This review employed a systematic approach to select relevant articles from multiple reputable databases, such as Scopus, IEEE Explore, Science Direct, ACM digital library, Web of Science, and IET digital library, including 250 articles that provide valuable insights into the intersection of AI, security, and sustainability in ICS. This review examines vulnerabilities in ICS, such as data breaches, insider threats, and malware, emphasizing the need for effective anomaly detection. It highlights how AI technologies like anomaly detection and predictive analytics can enhance threat detection and response in ICS by improving accuracy and efficiency. The review offers insights to researchers and professionals on the future of secure, sustainable ICS, supporting a resilient industrial landscape that meets cybersecurity, compliance, and sustainability goals.},
  archive      = {J_AIR},
  author       = {Aslam, Muhammad Muzamil and Tufail, Ali and Gul, Haji and Irshad, Muhammad Nauman and Namoun, Abdallah},
  doi          = {10.1007/s10462-025-11320-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-86},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence for secure and sustainable industrial control systems - A survey of challenges and solutions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Don’t push the button! exploring data leakage risks in machine learning and transfer learning. <em>AIR</em>, <em>58</em>(11), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11326-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, there is growing evidence in the literature that ML approaches are not always used appropriately, leading to incorrect and sometimes overly optimistic results. One reason for this inappropriate use of ML may be the increasing availability of machine learning tools, leading to what we call the “push the button” approach. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. In particular, this paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Indeed, crucial steps in ML pipeline can be inadvertently overlooked, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussing how certain conditions can propagate through the ML approach workflow. Furthermore, it explores the connection between data leakage and the specific task being addressed, investigates its occurrence in Transfer Learning framework, and compares standard inductive ML with transductive ML paradigms. The conclusion summarizes key findings, emphasizing the importance of addressing data leakage for robust and reliable ML applications considering tasks and generalization goals.},
  archive      = {J_AIR},
  author       = {Apicella, Andrea and Isgrò, Francesco and Prevete, Roberto},
  doi          = {10.1007/s10462-025-11326-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Don’t push the button! exploring data leakage risks in machine learning and transfer learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault-tolerant control strategies for industrial robots: State of the art and future perspective on AI-based fault management. <em>AIR</em>, <em>58</em>(11), 1-33. (<a href='https://doi.org/10.1007/s10462-025-11327-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault-tolerant control schemes are essential for affirming the safe and dependable operation of industrial robots. In this detailed review, we discuss the current developments in fault-tolerant control strategies for industrial robots. The main focus is given to highlight some major contributions in fault-tolerant control systems used in robotic manipulators with single or multiple joints, incorporating any linear or non-linear robust approach for industrial robots, design, and implementation. The paper also discusses adaptive fault-tolerant control of robots with sensor and/or actuator faults and unknown parameters, and fault-tolerant cooperative control of multiple robot teams for collaborative tasks. The present work provides a comprehensive overview of the recent advancements in fault-tolerant control strategies for industrial robots using both classical nonlinear methods as well as intelligent approaches using AI and machine learning, which will be useful for researchers and engineers working in this field.},
  archive      = {J_AIR},
  author       = {Khan, Zeashan and Nasir, Ali and Mekid, Samir},
  doi          = {10.1007/s10462-025-11327-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Fault-tolerant control strategies for industrial robots: State of the art and future perspective on AI-based fault management},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inclusive prompt engineering for large language models: A modular framework for ethical, structured, and adaptive AI. <em>AIR</em>, <em>58</em>(11), 1-51. (<a href='https://doi.org/10.1007/s10462-025-11330-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models have achieved impressive results across various tasks but remain limited in their ability to adapt ethically and structurally across diverse domains without retraining. This paper presents the Inclusive Prompt Engineering Model (IPEM), a modular framework designed to enhance LLM performance, adaptability, and ethical alignment through prompt-level strategies alone. IPEM integrates four components: Memory-of-Thought for multi-turn consistency, Enhanced Chain-of-Thought prompting for logical verification, Structured and Analogical Reasoning modules for tabular and cross-domain tasks, and Evaluation and Feedback Loops that incorporate uncertainty-aware selection and bias mitigation mechanisms. Evaluated across tasks in arithmetic reasoning, healthcare triage, financial forecasting, and inclusive question answering, IPEM consistently improves model outputs over a GPT-4 baseline. Notable outcomes include up to twenty percentage points in accuracy gains, a 25 percent reduction in logical errors, and nearly 20 percent reduction in social bias scores, all without modifying model weights. Moreover, IPEM reduces annotation demands by one-third while preserving performance, demonstrating its utility in low-resource environments. By unifying ethical safeguards and reasoning mechanisms in a prompt-based system, IPEM offers a reproducible and auditable pathway for deploying adaptable and fair AI systems. The framework contributes both practical solutions and theoretical insights to the evolving field of prompt engineering.},
  archive      = {J_AIR},
  author       = {Torkestani, Mohamad Saleh and Alameer, Ali and Palaiahnakote, Shivakumara and Manosuri, Taha},
  doi          = {10.1007/s10462-025-11330-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Inclusive prompt engineering for large language models: A modular framework for ethical, structured, and adaptive AI},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video diffusion generation: Comprehensive review and open problems. <em>AIR</em>, <em>58</em>(11), 1-55. (<a href='https://doi.org/10.1007/s10462-025-11331-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video generation has become an increasingly important component of AI-generated content (AIGC), owing to its rich semantic expressiveness and growing application potential. Among various generative paradigms, diffusion models have recently gained prominence due to their strong controllability, competitive visual quality, and compatibility with multimodal inputs. However, most existing surveys provide limited coverage of diffusion-based video generation, often lacking systematic analysis and comprehensive comparisons. To address this gap, this paper presents a thorough and structured review of diffusion models for video generation. We first outline the theoretical foundations and core architectures of diffusion models, and then the key design principles of representative methods for video generation were introduced. We propose a unified taxonomy that categorizes over two hundred methods, analyzing their key characteristics, strengths, and limitations. In addition, we compared the performance of classical methods and summarized commonly used datasets and evaluation metrics in this field for ease of model benchmarking and selection. Finally, we discuss open problems and future research directions, aiming to provide a valuable reference for both academic research and practical development.},
  archive      = {J_AIR},
  author       = {Ma, Wenping and Yang, Xiaoting and Jiao, Licheng and Li, Lingling and Liu, Xu and Liu, Fang and Chen, Puhua and Yang, Yuting and Ma, Mengru and Sun, Long and Zhang, Ruohan and Geng, Xueli and Guo, Yuwei and Yang, Shuyuan and Feng, Zhixi},
  doi          = {10.1007/s10462-025-11331-6},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Video diffusion generation: Comprehensive review and open problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy and security in recommenders: An analytical review. <em>AIR</em>, <em>58</em>(11), 1-41. (<a href='https://doi.org/10.1007/s10462-025-11333-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RSs) effectively curb information overload by providing personalized suggestions of items to users across different online domains. Their widespread use in e-commerce enhances user engagement, personalizes shopping experiences, and drives sales growth. However, despite the effectiveness of these systems at generating recommendations for users, they still raise major privacy and security concerns as their data could be exploited for malicious purposes, which can lead to data breaches and misuse. Therefore, this paper presents a comprehensive and systematic review of the underlying causes of privacy and security challenges in RS. It also provides a detailed taxonomy categorizing these concerns based on their targets and the risks they create. It further presents potential solutions that have been used in the literature while identifying challenges and possible research directions to pursue in a bid to address privacy and security concerns in RSs. This paper will be a useful resource for current and upcoming researchers in the domain of RSs. It will support knowledge advancement and steer appropriate research directions.},
  archive      = {J_AIR},
  author       = {Ojokoh, Bolanle Adefowoke and Isinkaye, Folasade Olubusola and Zhang, Ming and Tom, Joshua Joshua and Gabriel, Arome Junior and Afolabi, Olaitan and Afolabi, Bamidele},
  doi          = {10.1007/s10462-025-11333-4},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Privacy and security in recommenders: An analytical review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of deep learning in tea quality monitoring: A review. <em>AIR</em>, <em>58</em>(11), 1-45. (<a href='https://doi.org/10.1007/s10462-025-11335-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tea is a popular beverage which can offer numerous benefits to human health and support the local economy. There is an increasing demand for accurate and rapid tea quality evaluation methods to ensure that the quality and safety of tea products meet the customers’ expectations. Advanced sensing technologies in combination with deep learning (DL) offer significant opportunities to enhance the efficiency and accuracy for tea quality evaluation. This review aims to summarize the application of DL technologies for tea quality assessment in three stages: cultivation, tea processing, and product evaluation. Various state-of-the-art sensing technologies (e.g., computer vision, spectroscopy, electronic nose and tongue) have been used to collect key data (images, spectral signals, aroma profiles) from tea samples. By utilizing DL models, researchers are able to analyze a wide range of tea quality attributes, including tea variety, geographical origin, quality grade, fermentation stage, adulteration level, and chemical composition. The findings from this review indicate that DL, with its end-to-end analytical capability and strong generalization performance, can serve as a powerful tool to support various sensing technologies for accurate tea quality detection. However, several challenges remain, such as limited sample availability for data training, difficulties for fusing data from multiple sources, and lack of interpretability of DL models. To this end, this review proposes potential solutions and future studies to address these issues, providing practical considerations for tea industry to effectively uptake new technologies and to support the development of the tea industry.},
  archive      = {J_AIR},
  author       = {Wu, Tao and Zhou, Lei and Zhao, Yiying and Qi, Hengnian and Pu, Yuanyuan and Zhang, Chu and Liu, Yufei},
  doi          = {10.1007/s10462-025-11335-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of deep learning in tea quality monitoring: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence (AI) and machine learning (ML) in procurement and purchasing decision-support (DS): A taxonomic literature review and research opportunities. <em>AIR</em>, <em>58</em>(11), 1-36. (<a href='https://doi.org/10.1007/s10462-025-11336-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI), machine learning (ML) and decision-support (DS) are gaining increasing interest with widening adoption. This article investigates the enabler role of AI and ML for providing decision-support in procurement&purchasing domain. The study follows a systematic review approach via taxonomic analysis. Comprehensive analysis and discussions are provided for: (a) the relevance and applicability of AI and ML in procurement&purchasing decision-support; (b) functionalities/processes for which they are utilized; (c) related methodologies; and (d) implementation benefits as well as challenges. Findings reveal that procurement&purchasing area holds significant potential in terms of AI-ML applications for decision-support almost every related sub-process. This study is original by offering a process-oriented approach to the research domain; providing unique clustering and classification; and presenting detailed analyses via unique taxonomy tables with respect to approach, topic, focus, context and methodologies of the literature items reviewed. The study offers further research opportunities and has significant potential to provide managerial insights by the identified sectoral applications, benefits and challenges.},
  archive      = {J_AIR},
  author       = {Balkan, Dursun and Akyuz, Goknur Arzu},
  doi          = {10.1007/s10462-025-11336-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence (AI) and machine learning (ML) in procurement and purchasing decision-support (DS): A taxonomic literature review and research opportunities},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic radiology report generation with deep learning: A comprehensive review of methods and advances. <em>AIR</em>, <em>58</em>(11), 1-42. (<a href='https://doi.org/10.1007/s10462-025-11337-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic report generation refers to the process of generating medical reports from medical images without the need for manual intervention, enabling faster, more consistent, and objective analysis of radiological data. The rapid progress in deep learning, particularly in the fields of computer vision and natural language processing, has significantly improved the efficacy of this approach. By leveraging deep learning techniques, which seamlessly integrate image analysis with natural language generation, these methods have shown promise in interpreting complex medical images and producing highly accurate textual descriptions. In this paper, we provide a thorough review of various deep learning models and techniques employed for generating radiological reports, with a focus on chest X-ray images as a representative case. We propose a unified encoder-decoder framework that consists of an image encoder for extracting feature representations from medical images, a language decoder for generating textual reports, and enhancement components designed to refine model performance. Through a comprehensive comparison of existing state-of-the-art methods on the widely utilized MIMIC-CXR dataset, we highlight the innovative contributions made by recent advancements in the field. Furthermore, we discuss the current challenges and identify potential research directions for future advancements in this field.},
  archive      = {J_AIR},
  author       = {Li, Yilin and Kong, Chao and Zhao, Guosheng and Zhao, Zijian},
  doi          = {10.1007/s10462-025-11337-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automatic radiology report generation with deep learning: A comprehensive review of methods and advances},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI for cyber threat intelligence: Applications, challenges, and analysis of real-world case studies. <em>AIR</em>, <em>58</em>(11), 1-134. (<a href='https://doi.org/10.1007/s10462-025-11338-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive survey of the applications, challenges, and limitations of Generative AI (GenAI) in enhancing threat intelligence within cybersecurity, supported by real-world case studies. We examine a wide range of data sources in Cyber Threat Intelligence (CTI), including security reports, blogs, social media, network traffic, malware samples, dark web data, and threat intelligence platforms (TIPs). This survey provides a full reference for integrating GenAI into CTI. We discuss various GenAI models such as Large Language Models (LLMs) and Deep Generative Models (DGMs) like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models, explaining their roles in detecting and addressing complex cyber threats. The survey highlights key applications in areas such as malware detection, network traffic analysis, phishing detection, threat actor attribution, and social engineering defense. We also explore critical challenges in deploying GenAI, including data privacy, security concerns, and the need for interpretable and transparent models. As regulations like the European Commission’s AI Act emerge, ensuring trustworthy AI solutions is becoming more crucial. Real-world case studies, such as the impact of the WannaCry ransomware, the rise of deepfakes, and AI-driven social engineering, demonstrate both the potential and current limitations of GenAI in CTI. Our goal is to provide foundational insights and strategic direction for advancing GenAI’s role in future cybersecurity frameworks, emphasizing the importance of innovation, adaptability, and ongoing learning to enhance resilience against evolving cyber threats. Ultimately, this survey offers critical insights into how GenAI can shape the future of cybersecurity by addressing key challenges and providing actionable guidance for effective implementation.},
  archive      = {J_AIR},
  author       = {Balasubramanian, Prasasthy and Liyana, Sonali and Sankaran, Hamsini and Sivaramakrishnan, Shambavi and Pusuluri, Sruthi and Pirttikangas, Susanna and Peltonen, Ella},
  doi          = {10.1007/s10462-025-11338-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-134},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Generative AI for cyber threat intelligence: Applications, challenges, and analysis of real-world case studies},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent reinforcement learning for resources allocation optimization: A survey. <em>AIR</em>, <em>58</em>(11), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11340-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL’s ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing a pivotal role in industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, design steps and benchmarks. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL’s potential to advance resource allocation solutions.},
  archive      = {J_AIR},
  author       = {Hady, Mohamad A. and Hu, Siyi and Pratama, Mahardhika and Cao, Zehong and Kowalczyk, Ryszard},
  doi          = {10.1007/s10462-025-11340-5},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-agent reinforcement learning for resources allocation optimization: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). (Ir)rationality in AI: State of the art, research challenges and open questions. <em>AIR</em>, <em>58</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11341-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of rationality is central to the field of artificial intelligence (AI). Whether we are seeking to simulate human reasoning, or trying to achieve bounded optimality, our goal is generally to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in AI, and sets out the open questions in this area. We consider how the understanding of rationality in other fields has influenced its conception within AI, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we examine irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area remains limited. Methods that have up to now been developed for other purposes, namely adversarial scenarios, may be adapted to suit interactions with artificial agents. We further discuss the interplay between human and artificial agents, and the role that rationality plays within this interaction; many questions remain in this area, relating to potentially irrational behaviour of both humans and artificial agents.},
  archive      = {J_AIR},
  author       = {Macmillan-Scott, Olivia and Musolesi, Mirco},
  doi          = {10.1007/s10462-025-11341-4},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {(Ir)rationality in AI: State of the art, research challenges and open questions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review on key technologies toward smart healthcare systems based IoT: Technical aspects, challenges and future directions. <em>AIR</em>, <em>58</em>(11), 1-122. (<a href='https://doi.org/10.1007/s10462-025-11342-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unexpected death of humans due to a lack of medical care is a serious problem. Additionally, the number of elderly people requiring continuous care is increasing. A global aging population poses a challenge to the sustainability of conventional healthcare systems for the future. Simultaneously, recent years have seen remarkable progress in the Internet of Things (IoT) and communication technologies, alongside the growing importance of artificial intelligence (AI) explainability and information fusion. Therefore, developing smart healthcare systems based on IoT and advanced technologies is crucial. This would open up new possibilities for efficient and intelligent medical systems. Hence, it is imperative to present a prospective vision of smart healthcare systems and explore the key technologies that enable the development of these intelligent medical systems. With smart healthcare systems, the future of healthcare can be significantly enhanced, providing higher-quality care, improved treatment, and more efficient patient care. This paper aims to provide a comprehensive review of the key enabling and innovative technologies for smart healthcare systems. To this end, it will cover the primary goals of each technology, the current state of research, potential applications envisioned, associated challenges, and future research directions. This paper is intended to be a valuable resource for researchers and healthcare providers. Ultimately, this paper provides valuable insights for both industry professionals and academic researchers, while also identifying potential new research avenues.},
  archive      = {J_AIR},
  author       = {Alsabah, Muntadher and Naser, Marwah Abdulrazzaq and Albahri, A. S. and Albahri, O. S. and Alamoodi, A. H. and Abdulhussain, Sadiq H. and Alzubaidi, Laith},
  doi          = {10.1007/s10462-025-11342-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-122},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review on key technologies toward smart healthcare systems based IoT: Technical aspects, challenges and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel ensemble learning-based soft measurement method for rod-pumping system efficiency. <em>AIR</em>, <em>58</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11343-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of rod-pumping system efficiency is crucial for evaluating the performance of such systems. Currently, the efficiency of rod-pumping systems is primarily estimated using mechanistic models. With the continuous advancement of information technology and the improvement of oilfield databases, some researchers have employed single neural networks for prediction. However, single neural networks often suffer from low prediction accuracy and poor robustness to noise. To solve this problem, we propose a new integrated learning-based soft measurement of the efficiency of rod pumping systems. Firstly, we proposed five soft measurement methods for rod pumping system efficiency: BiGRU-BiLSTM-CrossAttention, BiRNN-BiGRU-KAN, CNN-BiGRU-KAN, BiLSTM-BiGRU-KAN, and BiLSTM-Transformer-KAN. Then, using these five methods as base learners and FNN as the meta-learner, we constructed a novel rod pumping system efficiency soft measurement method based on the Stacking ensemble learning framework. The hyperparameters were optimized using a multi-strategy integrated Crayfish optimization algorithm, and the model was validated using 5-fold cross-validation. To verify the accuracy of the proposed soft measurement method, we applied it to 10,250 real oil wells for calculation and conducted a comparative analysis with baseline models. The results demonstrate that the proposed soft measurement method can effectively predict the efficiency of rod pumping systems.},
  archive      = {J_AIR},
  author       = {Ma, Biao and Dong, Shimin},
  doi          = {10.1007/s10462-025-11343-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel ensemble learning-based soft measurement method for rod-pumping system efficiency},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFI-ensemble: Sugeno fuzzy integral-based ensemble of CNN models with meta-heuristic fuzzy measures for mouth and oral disease detection. <em>AIR</em>, <em>58</em>(11), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11345-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising prevalence of mouth and oral diseases (MOD), including gum disease and oral cancer, presents a major global health challenge, where timely detection is crucial for effective intervention. Recognizing the limitations of a single learning model in capturing intricate information for precise disease prediction from complex data, we introduce a robust deep learning ensemble framework named Sugeno Fuzzy Integral (SFI)-Ensemble in this paper. Our methodology involves a meticulous preprocessing of the dataset utilizing fuzzy contrast enhancement (FCE) to enhance data quality and contrast. Additionally, we propose a reconstruction approach that employs transfer learning (TL) and fine-tuning on four Convolutional Neural Network (CNN) models—DenseNet121, MobileNetV1, DenseNet169, and Resnet101V2—to optimize their architectures specifically for MOD classification. The focal point of this contribution lies in the introduction of a groundbreaking ensemble method. This ensemble method dynamically combines decision scores from the CNN models using the SFI-based technique, offering a resilient and adaptive approach that factors in the confidence of base learners’ predictions through fuzzy integrals. To overcome the prevalent challenge of experimentally defining fuzzy measures in ensemble methods based on fuzzy integrals, we surpass conventional manual tuning. Our approach involves the utilization of seven distinct meta-heuristic optimization algorithms for the optimal determination of fuzzy measures. This not only ensures stability but also highlights the effectiveness of the proposed SFI-Ensemble. A comprehensive assessment is carried out on publicly accessible datasets to detect MOD, complemented by Grad-CAM interpretability and meticulous statistical analyses. Additionally, we benchmark the results against baseline models and state-of-the-art methods, with our proposed framework consistently surpassing them, attaining an impressive accuracy of 99.70%. This underscores the superior performance and robustness of our proposed methodology in contrast to traditional ensemble methods. Our approach, integrating dataset preprocessing, model reconstruction, and ensemble innovation, provides doctors with an effective tool for accurate MOD diagnosis, enhancing adaptability and performance through fuzzy integral-based fusion.},
  archive      = {J_AIR},
  author       = {Asif, Sohaib and Chen, Shasha and Ying, Yajun and Zheng, Changfu and Wang, Vicky Yang and Wang, Enyu and Xu, Dong},
  doi          = {10.1007/s10462-025-11345-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {SFI-ensemble: Sugeno fuzzy integral-based ensemble of CNN models with meta-heuristic fuzzy measures for mouth and oral disease detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for intrusion detection in emerging technologies: A comprehensive survey and new perspectives. <em>AIR</em>, <em>58</em>(11), 1-63. (<a href='https://doi.org/10.1007/s10462-025-11346-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion Detection Systems (IDS) can help cybersecurity analysts detect malicious activities in computational environments. Recently, Deep Learning (DL) methods in IDS have demonstrated notable performance, revealing new underlying cybersecurity patterns in systems’ operations. Conversely, issues such as low performance in real systems, high false positive rates, and lack of explainability hinder its real-world deployment. In addition, the adoption of many new emerging technologies, such as cloud, edge computing, and the Internet of Things (IoT) introduces new forms of vulnerabilities. Therefore, the improvement of intrusion detection in emerging technologies depends on the clear definitions of challenging security problems and the limitations of existing solutions. The main goal of this research is to conduct a literature review of DL solutions for intrusion detection in emerging technologies to understand the state-of-the-art solutions and their limitations. Specifically, we conduct a comprehensive review of IDS-based automated threat defense methods, with the objective of identifying the landscape of, and opportunities for, incorporating DL methods into IDS. To accomplish this, a thorough review of IDS methods is conducted for multiple platforms and technologies, focusing on the use of common DL techniques. To expand on the study, several widely used IDS datasets are evaluated to assess their ability to train DL models and support researchers in understanding their characteristics and limitations. The analysis of attack vectors in emerging technologies is conducted, enabling an in-depth evaluation of security solutions in the future. Our findings show many clear opportunities for future research, including addressing the gap between solutions for controlled/simulated environments versus real systems, overcoming trustworthiness issues, including lack of explainability, and further exploring operationalization issues such as deployable solutions and continuous detection. Our analysis highlights that the operationalization of DL for intrusion detection in emerging technologies represents a key challenge to be addressed in the next few years.},
  archive      = {J_AIR},
  author       = {Neto, Euclides Carlos Pinto and Iqbal, Shahrear and Buffett, Scott and Sultana, Madeena and Taylor, Adrian},
  doi          = {10.1007/s10462-025-11346-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for intrusion detection in emerging technologies: A comprehensive survey and new perspectives},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of machine learning for computer-aided diagnosis of parkinson’s disease: Progress and benchmark case study. <em>AIR</em>, <em>58</em>(11), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11347-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has emerged as a vital tool for the diagnosis of Parkinson’s Disease (PD). This study presents a comprehensive review on the applications of ML for computer-aided diagnosis (CAD) of PD. We conducted a comprehensive review by searching articles published from 2010 till 2024. The risk of bias is assessed using the PROBAST checklist. Case studies are also provided. This review includes 117 articles with six categories: neuroimaging data (20.5%); voice data (40.2%); handwriting data (12.0%); gait data (14.5%); EEG data (8.5%); and other data (4.3%). According to the PROBAST checklist, only 28 articles (23.9%) have a low risk of bias. A benchmark case study is conducted for five different data modalities. We also discuss current limitations and future directions of applying ML to the diagnosis of PD. This review reduces the gap between Artificial Intelligence (AI) and PD medical professionals and provides helpful information for future research.},
  archive      = {J_AIR},
  author       = {Zhang, Juntao and Zhang, Yiming and Weng, Ying and Hosseini, Akram A. and Wang, Boding and Dening, Tom and Fan, Weinyu and Xiao, Weizhong},
  doi          = {10.1007/s10462-025-11347-y},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of machine learning for computer-aided diagnosis of parkinson’s disease: Progress and benchmark case study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV target tracking: A survey. <em>AIR</em>, <em>58</em>(11), 1-62. (<a href='https://doi.org/10.1007/s10462-025-11348-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) have become critical enablers of integrated air-space-ground Internet of Things (IoT) ecosystems, with target tracking serving as a foundational technology. This paper classifies UAV target tracking into two distinct paradigms: active tracking and passive tracking, differentiated by their operational scopes and technical objectives. Active tracking is defined as a closed-loop spatial pursuit system, whereby UAVs dynamically track targets through iterative cycles centered on three primary stages: online passive tracking, state fusion estimation, and tracking strategy generation, with subsequent execution phases implied in the loop. This workflow bridges perception and action, enabling spatial engagement through continuous sensor-to-control feedback. In contrast, passive tracking acts as a vision-centric analytical module that exclusively extracts target image-domain attributes from visual sensors—devoid of physical state inference or control mechanisms. As a preprocessing stage for active systems, it is constrained to the visual perception layer, lacking the spatial engagement capabilities inherent in closed-loop tracking systems. This paper conducts an in-depth analysis of the application, key challenges, and future trends in both active and passive UAV target tracking. By systematically discussing the relationships among relevant technologies, this work aims to establish a foundational reference framework and offer citation material for guiding the future development of UAV target tracking technologies.},
  archive      = {J_AIR},
  author       = {Wu, Pengnian and Li, Yixuan and Xue, Dong},
  doi          = {10.1007/s10462-025-11348-x},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-62},
  shortjournal = {Artif. Intell. Rev.},
  title        = {UAV target tracking: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on subspace clustering: Methods and applications. <em>AIR</em>, <em>58</em>(11), 1-50. (<a href='https://doi.org/10.1007/s10462-025-11349-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a pivotal strategy to deal with complicated and high-dimensional data, subspace clustering is to find a set of subspaces of a high-dimensional space and then partition each data point in dataset into the corresponding subspace. This field has witnessed remarkable progress over recent decades, with substantial theoretical advancements and successful applications spanning image processing, genomic analysis and text analysis. However, existing surveys predominantly focus on conventional shallow-structured methods, with few up-to-date reviews on deep-structured methods, i.e., deep neural network-based approaches. In fact, recent years has witnessed the overwhelming success of deep neural network in various fields, including computer vision, natural language processing, subspace clustering. To address this gap, this paper presents a comprehensive review on subspace clustering methods, including conventional shallow-structured and deep neural network based approaches, which systematically analyzes over 150 papers published in peer-reviewed journals and conferences, highlighting the latest research achievements, methods, algorithms and applications. Specifically, we first briefly introduce the basic principles and evolution of subspace clustering. Subsequently, we present an overview of research on subspace clustering, dividing the existing works into two categories: shallow subspace clustering and deep subspace clustering, based on the model architecture. Within each category, we introduce a refined taxonomy distinguishing linear and nonlinear approaches based on data characteristics and subspace structural assumptions. Finally, we discuss the challenges currently faced and future research direction for development in the field of subspace clustering.},
  archive      = {J_AIR},
  author       = {Miao, Jianyu and Zhang, Xiaochan and Yang, Tiejun and Fan, Chao and Tian, Yingjie and Shi, Yong and Xu, Mingliang},
  doi          = {10.1007/s10462-025-11349-w},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey on subspace clustering: Methods and applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending against attacks in deep learning with differential privacy: A survey. <em>AIR</em>, <em>58</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11350-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have witnessed the revolutionary development of deep learning. As the application domain of deep learning has expanded, its privacy risks have attracted attention since deep leaning methods often use private data for training. Some methods for attacking deep learning, such as membership inference attacks, increase the privacy risks of deep learning models. One risk-reducing defensive strategy with great potential is to apply some degree of random perturbation during the training (or other) phase. Therefore, differential privacy, as a privacy protection framework originally designed for publishing data, is widely used to protect the privacy of deep learning models due to its solid mathematical foundation. In this paper, we first introduce several attack methods that threaten deep learning. Then, we systematically review the cross-applications of differential privacy and deep learning to protect deep learning models. We encourage researchers to visually demonstrate the defense effects of their approaches in the literature rather than solely providing rigorous mathematical proofs. In addition to privacy, we also discuss and review the impact of differential privacy on the robustness, overfitting, and fairness of deep neural networks. Finally, we analyze some potential future research directions, highlighting the significant potential for differential privacy to make positive contributions to future deep learning systems.},
  archive      = {J_AIR},
  author       = {Xiangfei, Zhang and Qingchen, Zhang},
  doi          = {10.1007/s10462-025-11350-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Defending against attacks in deep learning with differential privacy: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swarm intelligence techniques and their applications in fog/edge computing: An in-depth review. <em>AIR</em>, <em>58</em>(11), 1-182. (<a href='https://doi.org/10.1007/s10462-025-11351-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in the Internet of Things (IoT) have connected diverse devices that often have limited resources and processing power. Artificial intelligence (AI) applications in fog and edge computing are greatly enhanced by Swarm Intelligence (SI) techniques. These SI methods improve resource allocation, task scheduling, and load balancing, making distributed systems more efficient and responsive to changing conditions. This paper systematically reviews 91 studies (2019–2023) on SI applications in fog/edge environments. We compare fog, edge, and cloud computing paradigms and analyze SI-based approaches using case studies, performance metrics, and evaluation tools. This review identifies key advantages and limitations of current SI-based approaches and highlights open issues and future research directions to enhance distributed computing systems. These insights aim to guide the development of more efficient and responsive AI-driven resource management strategies in fog/edge environments.},
  archive      = {J_AIR},
  author       = {Ghafari, Reyhane and Mansouri, Najme},
  doi          = {10.1007/s10462-025-11351-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-182},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Swarm intelligence techniques and their applications in fog/edge computing: An in-depth review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-induced deskilling in medicine: A mixed-method review and research agenda for healthcare and beyond. <em>AIR</em>, <em>58</em>(11), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11352-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Artificial Intelligence (AI) in healthcare is reshaping clinical practice, offering both opportunities for enhanced decision-making and risks of skill degradation among medical professionals. This growing impact calls for a comprehensive evaluation of its effects on medical expertise. This study presents a mixed-method literature review, combining systematic analysis with narrative synthesis to examine AI-induced deskilling and upskilling inhibition-the erosion of medical expertise and the reduction of opportunities for skill acquisition due to AI-driven decision support systems. Anchoring the discussion in the core medical competencies outlined by the Federation of Royal Colleges of Physicians of the UK-Practical Assessment of Clinical Examination Skills (PACES-MRCPUK), the systematic review identifies key vulnerabilities in physical examination, differential diagnosis, clinical judgment, and physician-patient communication. The narrative review explores broader themes related to Human–AI Interaction and the Impact of AI on Human Skills in Organizations. In response to concerns about the Second Singularity-a scenario in which decision-making autonomy is increasingly ceded to AI, weakening human oversight-this review advocates for a research agenda that prioritizes longitudinal studies, real-time monitoring of AI’s impact, and the development of frameworks to mitigate skill erosion, ensuring the preservation of professional autonomy and the safeguarding of the irreplaceable elements of human judgment in medicine and beyond.},
  archive      = {J_AIR},
  author       = {Natali, Chiara and Marconi, Luca and Dias Duran, Leslye Denisse and Cabitza, Federico},
  doi          = {10.1007/s10462-025-11352-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-induced deskilling in medicine: A mixed-method review and research agenda for healthcare and beyond},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved decisions for unknown behaviours in interactive dynamic influence diagrams. <em>AIR</em>, <em>58</em>(11), 1-28. (<a href='https://doi.org/10.1007/s10462-025-11355-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive dynamic influence diagrams (I-DIDs) are a general decision framework for a subject agent who interacts with other agents (of either collaborative or competitive) in a common environment with partial observability. The subject agent aims to optimize its decision-making (response strategy) while other agents concurrently adapt their behaviors over time. The I-DID model has faced a long-term challenge when other agents exhibit unknown behaviors that go beyond what the subject agent has planned for prior to their interactions. This is because the subject agent does not hold the capability of modeling unknown behaviours of other agents in traditional I-DID techniques. In this article, we adapt two different swarm intelligence (SI) techniques to develop new behaviours for other agents in I-DIDs. The SI-based algorithms have the strength of generating a collective set of behaviours that could potentially contain various types of agents’ behaviours. We theoretically analyze how the two algorithms impact the subject agent’s decision quality, and empirically demonstrate the algorithm performance in two commonly used problem domains.},
  archive      = {J_AIR},
  author       = {Pan, Yinghui and Zhou, Mengen and Ma, Biyang and Zeng, Yifeng and Ong, Yew-soon and Liu, Guoquan},
  doi          = {10.1007/s10462-025-11355-y},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improved decisions for unknown behaviours in interactive dynamic influence diagrams},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ameliorated elk herd optimizer for global optimization and engineering problems. <em>AIR</em>, <em>58</em>(11), 1-80. (<a href='https://doi.org/10.1007/s10462-025-11360-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization techniques have received significant attention for reliably addressing practical problems. A potential meta-heuristic called elk herd optimizer (EHO) was created, inspired by the social behavior and reproduction of elks. EHO has drawbacks, including poor convergence competency and a tendency to fall into local extrema in various optimization problems. Furthermore, this algorithm does not account for the memory of its search agents and has difficulty effectively balancing exploration and exploitation, which can lead to early convergence toward a local optimum. This study addresses the above issues by proposing an ameliorated EHO (AEHO) by incorporating several modifications into the basic EHO algorithm, which can be described as follows: A new hybrid memory-based EHO is developed that uses the particle swarm optimization (PSO) algorithm to guide EHO to search for reasonable candidate solutions. This hybrid approach was proposed to enhance EHO’s diversity and balance search capabilities to achieve strong search performance. Initially, a memory component was added to EHO using the idea of pbest from PSO to tap into promising search regions, which focuses on improving the best solutions and preventing the algorithm from getting stuck in a local optimum. In addition, the PSO concepts of (gbest) and (pbest) are used to enhance the best placements of the search agents in EHO. Finally, a greedy selection method was used to improve the efficiency of exhaustive exploration in AEHO, using the fitness values before and after updates as an indicator for efficacy of the best solutions. To evaluate the performance of the AEHO algorithm against a group of well-known competitors, we use ten complex test functions from the global CEC2022 test suite and thirty complex test functions from the global CEC2014 test suite. Based on the analysis of the experimental findings, AEHO performed optimally on 84% of the CEC2014 functions and 74% of the CEC2022 functions, ranking first in both suites with an average ranking of 3.11 and 1.62, respectively. The mean computation time of AEHO is about one-third of the average computation time for the first-ranked method, indicating that AEHO not only performs very well in global searches but also exhibits greater search efficiency when compared to newer optimization algorithms. The applicability and reliability of AEHO were thoroughly studied on four constrained engineering design problems and a real-world industrial process. The results demonstrate the superiority and promising potential of AEHO in addressing a wide range of challenging real-world problems.},
  archive      = {J_AIR},
  author       = {Al-Betar, Mohammed Azmi and Braik, Malik Sh. and Shambour, Qusai Yousef and Al-Naymat, Ghazi and Porntaveetus, Thantrira},
  doi          = {10.1007/s10462-025-11360-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-80},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Ameliorated elk herd optimizer for global optimization and engineering problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAoG: Decayed adaptation over gradients for parameter-free step size control. <em>AIR</em>, <em>58</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11362-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the scale of parameters in deep learning models continues to grow, the cost of training such models increases accordingly, posing increasingly significant challenges for stochastic optimization methods. A central issue in gradient-based optimization lies in the selection of the step size, whose appropriateness directly affects training efficiency and model performance. To address this issue, a series of parameter-free optimization methods that do not require manual tuning of the step size have been proposed in recent years. Among them, DoG and its improved variant DoWG are the most representative. Despite demonstrating strong performance across various tasks, DoG and DoWG still suffer from performance instability or slow convergence under certain model architectures or training conditions. This paper introduces Decayed Adaptation over Gradients (DAoG), a novel parameter-free optimization method that systematically addresses these limitations. Our key innovation lies in incorporating a principled step size decay mechanism for the first time within the parameter-free optimization framework, which substantially enhances both optimization stability and model generalization. Additionally, a parameter compression strategy is employed to reduce sensitivity to the initial step size. Theoretical analysis demonstrates that DAoG exhibits favorable convergence properties under L-smooth and G-Lipschitz conditions. Empirical studies across representative tasks in natural language processing and computer vision demonstrate that DAoG outperforms both DoG and DoWG in terms of convergence speed and generalization performance. Notably, it even rivals or surpasses Adam with cosine annealing in several challenging scenarios. These theoretical and experimental results suggest that DAoG effectively mitigates the overly conservative step size issue in DoG and the instability problem in DoWG, thereby advancing the development of parameter-free optimization methods in deep learning.},
  archive      = {J_AIR},
  author       = {Zhang, Yifan and Zhao, Di and Li, Hongyi and Pan, Chengwei},
  doi          = {10.1007/s10462-025-11362-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {DAoG: Decayed adaptation over gradients for parameter-free step size control},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Place recognition meet multiple modalities: A comprehensive review, current challenges and future development. <em>AIR</em>, <em>58</em>(11), 1-48. (<a href='https://doi.org/10.1007/s10462-025-11367-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Place recognition is a cornerstone of vehicle navigation and mapping, which is pivotal in enabling systems to determine whether a location has been previously visited. This capability is critical for tasks such as loop closure in Simultaneous Localization and Mapping (SLAM) and long-term navigation under varying environmental conditions. This survey comprehensively reviews recent advancements in place recognition, emphasizing three representative methodological paradigms: Convolutional Neural Network (CNN)-based approaches, Transformer-based frameworks, and cross-modal strategies. We begin by elucidating the significance of place recognition within the broader context of autonomous systems. Subsequently, we trace the evolution of CNN-based methods, highlighting their contributions to robust visual descriptor learning and scalability in large-scale environments. We then examine the emerging class of Transformer-based models, which leverage self-attention mechanisms to capture global dependencies and offer improved generalization across diverse scenes. Furthermore, we discuss cross-modal approaches that integrate heterogeneous data sources such as Lidar, vision, and text description, thereby enhancing resilience to viewpoint, illumination, and seasonal variations. We also summarize standard datasets and evaluation metrics widely adopted in the literature. To the best of our knowledge, no prior survey has systematically reviewed visual, LiDAR, and cross-modal place recognition concurrently. This work thus resolves a critical gap in existing literature dominated by single-modality studies. Finally, we identify current research challenges and outline prospective directions, including domain adaptation, real-time performance, and lifelong learning, to inspire future advancements in this domain. The unified framework of leading-edge place recognition methods, i.e., code library, and the results of their experimental evaluations are available at https://github.com/CV4RA/SOTA-Place-Recognitioner .},
  archive      = {J_AIR},
  author       = {Li, Zhenyu and Shang, Tianyi and Xu, Pengjie and Deng, Zhaojun},
  doi          = {10.1007/s10462-025-11367-8},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Place recognition meet multiple modalities: A comprehensive review, current challenges and future development},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic operation room scheduling DORS strategy based on explainable AI and fuzzy interface engine. <em>AIR</em>, <em>58</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11366-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poor surgical scheduling causes major problems in hospital operating rooms, such as long patient wait times, underutilized operating rooms, and high costs. Existing scheduling approaches, which are static or less adaptable, fail to handle real-time unpredictability. To overcome these constraints, this study presents Dynamic Operation Room Scheduling (DORS), a new intraday surgical scheduling system. DORS uses a two-layered architecture: (1) Explainable AI for feature selection that is based on critical scheduling criteria such as Round Robin, and (2) a dynamic scheduling system that includes a Receiving Module, a Checking Module for patient prioritization, and a Scheduling Module provided by a Fuzzy Interface Engine. This system allows for proactive schedule preparation and reactive modifications, making it possible to smoothly include unscheduled surgical operations. In comparison to traditional (FCFS, Round Robin) and optimization-based (genetic algorithm) methods. DORS dynamically modifies schedules to reduce average wait times (AWT), consistently outperforming other approaches by 120–560 min. DORS completes surgical operations more quickly (half of surgical operations in 255–725 min). In addition, DORS retains a modest runtime (45 ms) while increasing scheduling efficiency (98.6%). DORS also demonstrates strong stability, with low Relative Percentage Deviation (RPD) on high-demand days. Finally, DORS achieves the optimal blend of speed, efficiency, and responsiveness, making it the greatest choice for hospitals aiming to eliminate delays, optimize operating room usage, and effectively manage changing surgical needs.},
  archive      = {J_AIR},
  author       = {El-Balka, Rana Mohamed and Sakr, Noha and Rabie, Asmaa H. and Saleh, Ahmed I.},
  doi          = {10.1007/s10462-025-11366-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A dynamic operation room scheduling DORS strategy based on explainable AI and fuzzy interface engine},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting natural selection: Evolving dynamic neural networks using genetic algorithms for complex control tasks. <em>AIR</em>, <em>58</em>(11), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11382-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) and Genetic Algorithms (GAs) are widely used in decision-making and control tasks, but they often suffer from prolonged training times and inefficiencies. This paper addresses the need for a faster and more precise method to train neural networks in RL tasks, without sacrificing performance. The proposed approach enhances GAs by introducing mechanisms that optimize network architectures dynamically, minimizing unnecessary complexity while maintaining accuracy. The methodology includes a dynamic architecture adaptation technique that trims the neural network to its most compact and effective configuration. A Blending mechanism is introduced to improve the propagation of essential features across network layers, reducing the usage of non-linearity until necessary. An experience replay buffer is integrated to avoid redundant fitness evaluations, significantly reducing computational overhead. Additionally, a novel approach combines back-propagation with GAs for further refinement in supervised or RL tasks, using it as a mutation method to fine-tune the model. Experimental results demonstrate convergence speeds of around several seconds for simple tasks with well-defined rewards, and several minutes for more complex tasks. Training time is reduced by nearly 70%, and the approach provides faster inference speeds due to minimal architecture, making it applicable for mobile and edge devices. The method reduces computation, especially during inference, by over 90% due to the extremely low number of parameters. The performance metrics show comparable results to conventional approaches at the end of training. The proposed method is scalable and resource-efficient, outperforming existing neural network optimization techniques in both simulated environments and real-world applications. The developed framework is publicly available under the MIT license at https://github.com/AhmedBoin/atgen offering an open-source solution for the broader research community.},
  archive      = {J_AIR},
  author       = {Taha, Mohamed A. and Saafan, Mahmoud M. and Ayyad, Sarah M.},
  doi          = {10.1007/s10462-025-11382-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Revisiting natural selection: Evolving dynamic neural networks using genetic algorithms for complex control tasks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
