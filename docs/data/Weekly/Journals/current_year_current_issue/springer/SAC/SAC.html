<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac">SAC - 43</h2>
<ul>
<li><details>
<summary>
(2025). Improving the prediction accuracy of statistical models: A new hierarchical clustering approach. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10683-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statisticians and machine learning practitioners frequently encounter datasets originated from multiple populations but containing the same type of measurements. In such cases, predictive analytics is typically carried out by either fitting a separate model to each dataset independently or by merging the datasets and fitting a single model to the combined data. These approaches overlook the potential existence of multiple groups of datasets associated with different underlying models, and, therefore, fail to exploit the inherent similarity between datasets to improve predictions. A third alternative is to perform pairwise comparisons between the populations before fitting the models. However, this is not always feasible, can become a very challenging task with complex models, and often does not rely on predictive accuracy. To address these issues, we propose a clustering approach designed to improve predictions in general databases. The method is based on a novel type of objective function that represents the total by-group prediction error. The clustering problem is solved using a hierarchical-type algorithm of agglomerative nature that automatically obtains the resulting clustering partition in a fully data-driven manner. An additional advantage of this procedure is that the number of clusters is treated as a variable in the minimization problem, allowing it to be determined naturally in a way that optimizes the predictive accuracy of the underlying models. Furthermore, the technique is versatile and can be used with any type of model for both regression, and classification tasks. Several simulation experiments and two real-world applications involving housing prices demonstrate that the procedure outperforms benchmark approaches in terms of predictive accuracy.},
  archive      = {J_SAC},
  author       = {López-Oriona, Ángel and Sun, Ying and Vilar, José A.},
  doi          = {10.1007/s11222-025-10683-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Improving the prediction accuracy of statistical models: A new hierarchical clustering approach},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information-theoretic criteria for optimizing designs of individually randomized stepped-wedge clinical trials. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10690-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are essential for advancing medical knowledge and improving health care, with Randomized Clinical Trials (RCTs) considered the gold standard for minimizing bias and generating reliable evidence on treatment efficacy and safety. Stepped-wedge individual RCTs, which randomize participants into sequences transitioning from control to intervention at staggered time points, are increasingly adopted. To improve their design, we propose an information-theoretic framework based on D– and A–optimality criteria for participant allocation to sequences. Our approach leverages semidefinite programming for automated computation and is applicable across a range of settings, varying in: (i) number of sequences, (ii) attrition rates, (iii) optimality criteria, (iv) error correlation structures, and (v) multi-objective designs using the $$\epsilon $$ -constraint method.},
  archive      = {J_SAC},
  author       = {Duarte, Belmiro P. M. and Atkinson, Anthony C. and Moerbeek, Mirjam},
  doi          = {10.1007/s11222-025-10690-y},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Information-theoretic criteria for optimizing designs of individually randomized stepped-wedge clinical trials},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast post-process bayesian inference with variational sparse bayesian quadrature. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10695-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (vsbq), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. vsbq reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that vsbq builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.},
  archive      = {J_SAC},
  author       = {Li, Chengkun and Clarté, Grégoire and Jørgensen, Martin and Acerbi, Luigi},
  doi          = {10.1007/s11222-025-10695-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Fast post-process bayesian inference with variational sparse bayesian quadrature},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tests for simultaneous ordered alternatives in a two-way ANOVA with interaction. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10706-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many experimental designs, it is known a priori that the mean effects of the factors follow a monotone ordering. In this article, the problem of testing the homogeneity of the effects of both the factors against the alternative of their simultaneous monotone ordering is studied for a two-way crossed heteroscedastic ANOVA model. An intersection type test based on likelihood ratios of two sub-hypotheses and two multiple comparison tests are developed. Algorithms are proposed for the implementation of these tests using a parametric bootstrap approach and the asymptotic accuracy of this approach is also established. Extensive simulations are carried out to study the efficacy of these tests in controlling the type-I error rates and achieving a good power performance. It is shown that the proposed tests achieve the nominal size values regardless of the dimension of the design, number of replications in each cell and level of heterogeneity of error variances. Further, they are seen to have very good powers indicating consistency of tests. The proposed tests are further examined for their robustness under deviation from normality. An ‘R’ package is developed and shared on the open platform ‘GitHub’ for easy usage by practitioners. Finally, the applicability of our proposed test procedures is illustrated using two real data sets on mortality rates.},
  archive      = {J_SAC},
  author       = {Dey, Raju and Kumar, Somesh},
  doi          = {10.1007/s11222-025-10706-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Tests for simultaneous ordered alternatives in a two-way ANOVA with interaction},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile-based fitting for graph signals. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10689-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of monitoring tools has led to an emerging demand for analyzing data residing on graphs, referred to as graph signals. In this study, we propose a quantile-based fitting method for graph signals, which can be applicable to graph signals with a wide range of distributions. Unlike traditional data fitting methods, such as smoothing splines or quantile smoothing splines in Euclidean space, the proposed method is designed for the graph domain, considering the inherent structure of graphs. In contrast to prevalent graph signal fitting methods that rely on optimization problems with $$L_2$$ -norm fidelity, the proposed method provides robust fits for graph signals in the presence of outliers. More importantly, it identifies various distributional structures of graph signals beyond the mean feature. We further investigate the theoretical properties of the proposed solution, including its existence and uniqueness. Through a comprehensive simulation study and real data analysis, we demonstrate the promising performance of the proposed method.},
  archive      = {J_SAC},
  author       = {Kim, Kyusoon and Oh, Hee-Seok},
  doi          = {10.1007/s11222-025-10689-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Quantile-based fitting for graph signals},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical approaches to finite-horizon optimal stopping problems for the shiryaev process. <em>SAC</em>, <em>35</em>(6), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10696-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a finite-horizon optimal stopping problem the optimal stopping time is typically given by the first moment at which a sufficient statistic, namely a process containing all the relevant information on the problem, exceeds an unknown time-dependent boundary. This boundary often turns out to be the solution of a highly nonlinear integral equation involving the transition density of the sufficient statistic. When this density cannot be computed directly or easily, standard methods for solving the integral equation must be modified. This situation arises in sequential detection problems and in the pricing of certain derivative securities, where the corresponding sufficient statistics follow the so called Shiryaev process. In this context, we analyze and implement three distinct numerical methods for solving the integral equations characterizing the associated optimal stopping boundaries: two of them rely on solutions to partial differential equations, while the third is based on approximating the distribution of the sufficient statistic using a log-normal distribution. We demonstrate that these approaches return accurate results and are generally efficient.},
  archive      = {J_SAC},
  author       = {Buonaguidi, B.},
  doi          = {10.1007/s11222-025-10696-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Numerical approaches to finite-horizon optimal stopping problems for the shiryaev process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical mixtures of latent trait analyzers with concomitant variables for multivariate binary data. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10697-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing three-way data is challenging due to complex dependencies between observations, which must be accounted for to ensure reliable results. We focus on hierarchical, multivariate, binary data organized in a three-way data structure, where rows correspond to first-level units, columns to variables, and layers to second-level units within which the first-level units are nested. In this framework, model-based clustering methods can be effectively employed for dimensionality reduction purposes, facilitating a clear understanding of the phenomenon under investigation. In this work, we propose a novel modeling tool for a hierarchical clustering of first- and second-level units. We extend the Mixture of Latent Trait Analyzers (MLTA) with concomitant variables by letting prior component probabilities depend also on second-level-specific random effects. Parameter estimation is performed by means of a double EM algorithm based on a variational approximation of the model log-likelihood function, along with a nonparametric maximum likelihood estimation of the second-level-specific random effect distribution. This latter approach allows to estimate a discrete distribution which directly provides a clustering of second-level units. Within (conditional on) each of such clusters, first-level units are partitioned thanks to the MLTA specification. The proposal is applied to data from the European Social Survey to partition countries (second-level units) according to the baseline attitude of their residents (first-level units) toward digital technologies (variables). Within these clusters, residents are partitioned on the basis of their attitude toward specific digital skills. The influence of socio-economic factors on the identification of digitalization profiles is also taken into consideration via a concomitant variable approach.},
  archive      = {J_SAC},
  author       = {Failli, Dalila and Marino, Maria Francesca and Arpino, Bruno},
  doi          = {10.1007/s11222-025-10697-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Hierarchical mixtures of latent trait analyzers with concomitant variables for multivariate binary data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact computation of angular halfspace depth. <em>SAC</em>, <em>35</em>(6), 1-29. (<a href='https://doi.org/10.1007/s11222-025-10700-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The angular halfspace depth ( $$ahD$$ ) was, already in 1987, the first depth function proposed for the nonparametric analysis of directional data. Mainly due to its presumed high computational cost and lack of efficient computational algorithms, it was never widely used in directional data analysis. We address the problem of the exact computation of $$ahD$$ in any dimension d. We proceed in two steps: (i) We express $$ahD$$ as a generalized (Euclidean) halfspace depth in dimension $$d-1$$ , using a projection approach. That allows us to develop fast exact computational algorithms for $$ahD$$ in dimensions $$d=1, 2, 3$$ . (ii) In spaces of dimension 3]]d 3 we design an inductive procedure that reduces the dimensionality d in the computation of $$ahD$$ , until the algorithms for $$d \le 3$$ can be used. Using our advances we develop a family of powerful algorithms for the computation of $$ahD$$ in any dimension d. Our procedures are implemented efficiently in C++ with an interface in R. A detailed analysis of the complexity of the novel algorithms is performed. Surprisingly, we show that computing $$ahD$$ of multiple points with respect to the same dataset is substantially faster than the same task for the classical (Euclidean) halfspace depth.},
  archive      = {J_SAC},
  author       = {Dyckerhoff, Rainer and Nagy, Stanislav},
  doi          = {10.1007/s11222-025-10700-z},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {Exact computation of angular halfspace depth},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing mean independence with functional covariate. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10705-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new nonparametric conditional mean independence test for a scalar response and a functional covariate. The test statistics are built from continuous functionals over a residual marked empirical process indexed by a randomly projected functional covariate, which is less sensitive to tuning parameters and circumvents the curse of dimensionality. The asymptotic properties of the proposed test statistics under the null and the fixed alternative are established. We also show that our proposed test is able to detect a broad class of local alternatives converging to the null at the parametric rate. Due to the non-pivotal limiting null distribution, we use an easy-to-implement multiplier bootstrap procedure to estimate the critical values. Monte Carlo simulation studies demonstrate that our test outperforms other tests available in the literature due to its higher power and computational efficiency. The proposed test is further illustrated by analyzing the Tecator data set.},
  archive      = {J_SAC},
  author       = {Feng, Yongzhen and Li, Jie and Lu, Haokun and Song, Xiaojun},
  doi          = {10.1007/s11222-025-10705-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Testing mean independence with functional covariate},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random sampling of contingency tables and partitions: Two practical examples of the burnside process. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10708-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper gives new, efficient algorithms for approximate uniform sampling of contingency tables and integer partitions. The algorithms use the Burnside process, a general algorithm for sampling a uniform orbit of a finite group acting on a finite set. We show that a technique called ‘lumping’ can be used to derive efficient implementations of the Burnside process. For both contingency tables and partitions, the lumped processes have far lower per step complexity than the original Markov chains. We also define a second Markov chain for partitions called the reflected Burnside process. The reflected Burnside process maintains the computational advantages of the lumped process but empirically converges to the uniform distribution much more rapidly. By using the reflected Burnside process we can easily sample uniform partitions of size $$10^{10}$$ .},
  archive      = {J_SAC},
  author       = {Diaconis, Persi and Howes, Michael},
  doi          = {10.1007/s11222-025-10708-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Random sampling of contingency tables and partitions: Two practical examples of the burnside process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outcome-guided spike-and-slab lasso biclustering: A novel approach for enhancing biclustering techniques for gene expression analysis. <em>SAC</em>, <em>35</em>(6), 1-24. (<a href='https://doi.org/10.1007/s11222-025-10709-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering has gained interest in gene expression data analysis due to its ability to identify groups of samples that exhibit similar behaviour in specific subsets of genes (or vice versa), in contrast to traditional clustering methods that classify samples based on all genes. Despite advances, biclustering remains a challenging problem, even with cutting-edge methodologies. This paper introduces an extension of the recently proposed Spike-and-Slab Lasso Biclustering (SSLB) algorithm, termed Outcome-Guided SSLB (OG-SSLB), aimed at enhancing the identification of biclusters in gene expression analysis. Our proposed approach integrates disease outcomes into the biclustering framework through Bayesian profile regression. By leveraging additional clinical information, OG-SSLB improves the interpretability and relevance of the resulting biclusters. Comprehensive simulations and numerical experiments demonstrate that OG-SSLB achieves superior performance, with improved accuracy in estimating the number of clusters and higher consensus scores compared to the original SSLB method. Furthermore, OG-SSLB effectively identifies meaningful patterns and associations between gene expression profiles and disease states. These promising results demonstrate the effectiveness of OG-SSLB in advancing biclustering techniques, providing a powerful tool for uncovering biologically relevant insights. The OGSSLB software can be found as an R/C++ package at https://github.com/luisvargasmieles/OGSSLB .},
  archive      = {J_SAC},
  author       = {Vargas-Mieles, Luis A. and Kirk, Paul D. W. and Wallace, Chris},
  doi          = {10.1007/s11222-025-10709-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Outcome-guided spike-and-slab lasso biclustering: A novel approach for enhancing biclustering techniques for gene expression analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed estimation and inference for high-dimensional confounded models. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10710-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hidden confounded model has been widely applied in many fields. Without adjusting for the hidden confounders, the estimators from the standard methods of high-dimensional models could be biased, potentially leading to spurious scientific discoveries. Meanwhile, distributed computation has attracted wide attention in modern statistical learning. Based on high-dimensional confounded models, this paper proposes a deconfounding and debiasing approach for distributed computing, aiming to obtain accurate estimation by reducing the confounding effect and bias. Two different distributed methods are applied: one is the straightforward divide-and-conquer (DC) method and the other is the communication-efficient surrogate likelihood (CSL) method. The former is easy to use in practice, while the latter uses surrogate loss to achieve better performance than the former through multiple iterations. The estimation accuracy and asymptotic theories for both DC and CSL estimators are established. Extensive simulation experiments verify the good performance of the two estimators and two real data applications are also presented to illustrate their validity and feasibility.},
  archive      = {J_SAC},
  author       = {Liu, Jin and Fei, Yuxin and Ma, Wei and Wang, Lei},
  doi          = {10.1007/s11222-025-10710-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Distributed estimation and inference for high-dimensional confounded models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive tree ensembles for composite quantile regressions. <em>SAC</em>, <em>35</em>(6), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10711-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel approach that integrates Bayesian additive regression trees (BART) with the composite quantile regression (CQR) framework, creating a robust method for modeling complex relationships between predictors and outcomes under various error distributions. Unlike traditional quantile regression, which focuses on specific quantile levels, our proposed method, composite quantile BART, offers greater flexibility in capturing the entire conditional distribution of the response variable. By leveraging the strengths of BART and CQR, the proposed method provides enhanced predictive performance, especially in the presence of heavy-tailed errors and non-linear covariate effects. Numerical studies confirm that the proposed composite quantile BART method generally outperforms classical BART, quantile BART, and composite quantile linear regression models in terms of RMSE, especially under heavy-tailed or contaminated error distributions. Notably, under contaminated normal errors, it reduces RMSE by approximately 17% compared to composite quantile regression, and by 27% compared to classical BART.},
  archive      = {J_SAC},
  author       = {Lim, Yaeji and Lu, Ruijin and Ville, Madeleine St. and Chen, Zhen},
  doi          = {10.1007/s11222-025-10711-w},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian additive tree ensembles for composite quantile regressions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation based bayesian optimization. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10715-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. However, in complex scenarios involving optimization over categorical or mixed covariate spaces, GPs may not be ideal. This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires sampling-based access to posterior predictive distributions. SBBO allows the use of surrogate probabilistic models tailored for combinatorial spaces with discrete variables. Any Bayesian model in which posterior inference is carried out through Markov chain Monte Carlo can be selected as the surrogate model in SBBO. We demonstrate empirically the effectiveness of SBBO using various choices of surrogate models in applications involving combinatorial optimization.},
  archive      = {J_SAC},
  author       = {Naveiro, Roi and Tang, Becky},
  doi          = {10.1007/s11222-025-10715-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Simulation based bayesian optimization},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous analysis of longitudinal profiles using adaptive banded precision matrices via penalized fusion. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10716-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering profiles of longitudinal data is a prevalent method employed for analyzing heterogeneity patterns among individuals, aiming to identify clusters based on diverse patterns of the mean progression trajectories. The correlation structure plays a crucial role in longitudinal data analysis, as accurate modeling of this structure enhances the estimation efficiency of mean trajectories. In this paper, we assume that subjects are sampled from a Gaussian mixture distribution, and incorporate regularized bandable precision matrix structure information for each subgroup. In order to identify the latent group structure, we employ concave penalty functions to estimate the pairwise differences of the model parameters derived from finite mixture models. The model parameters and cluster labels are estimated simultaneously using the Expectation-Maximization (EM) algorithm in conjunction with the Alternating Direction Method of Multipliers (ADMM) algorithm. We establish computational convergence and provide a statistical guarantee through demonstrating the asymptotic rate. Numerical studies and real data analysis show improved clustering results and excellent accuracy performance.},
  archive      = {J_SAC},
  author       = {Liang, Chunhui and Ma, Wenqing},
  doi          = {10.1007/s11222-025-10716-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Heterogeneous analysis of longitudinal profiles using adaptive banded precision matrices via penalized fusion},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor train approximation of multivariate gaussian density by scaling and squaring. <em>SAC</em>, <em>35</em>(6), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10707-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor train decomposition is a promising tool for dealing with high dimensional arrays. Point mass filters utilise such arrays for representing probability density functions of the state. Proofs of concept of the application of the low rank decomposition have been provided in the literature. However, the application requires to design parameters, such as tensor train ranks. Since the parameters dictating the computational requirements are derived from the data according to more abstract hyper-parameters such as precision, an analysis of benchmark examples is needed for allocating resources. This paper studies the ranks in the case of Gaussian densities. The influence of correlation and the effect of rounding are discussed first. Efficiency of the density representation used by standard point mass filters is considered next. Aspects of series expansion of the Gaussian density evaluated over array are considered for the tensor train format. The growth of the ranks is illustrated on a four-dimensional example. An observation of the growth for a multi-dimensional case is made last. The lessons learned are valuable for designing efficient point mass filters. Namely, they show that at least the naive implementations of tensor decomposition methods do not break the curse of dimensionality.},
  archive      = {J_SAC},
  author       = {Ajgl, Jiří and Straka, Ondřej},
  doi          = {10.1007/s11222-025-10707-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Tensor train approximation of multivariate gaussian density by scaling and squaring},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new resampling method for meta gaussian distributions. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10717-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta Gaussian distributions, also known as multivariate Gaussian copula models, are a type of statistical distribution that is particularly useful in modeling dependencies among variables. The key advantage of meta Gaussian distributions is their flexibility - they can capture a wide range of dependency structures, making them a powerful tool for statistical modeling. Discrete approximations of continuous multivariate distributions, such as meta Gaussian distributions, which are of significant importance, are widely utilized across numerous disciplines. This paper introduces a new resampling method based on mean square error representative points (MSE-RPs) to construct accurate approximations for meta Gaussian distributions, thereby enhancing precision in statistical analysis. We carry out a systematic examination of the structural patterns and characteristics of MSE-RPs of these distributions. From a theoretical perspective, we analyze the invariance properties in copula-based association measures by leveraging group theory. This allows us to identify more stable invariants that are suitable for complex dependency structures. Through a simulation study, we demonstrate that MSE-RPs achieve significantly higher estimation accuracy for mean vectors and correlation matrices compared to Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods. Furthermore, MSE-RPs offer faster computation relative to the QMC method based on generalized good lattice points (GGLP) sets. Finally, we illustrate the practical advantages of our approach through empirical analysis on real-world datasets.},
  archive      = {J_SAC},
  author       = {He, Pingan and Fang, Kai-Tai and He, Ping and Ye, Huajun},
  doi          = {10.1007/s11222-025-10717-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {A new resampling method for meta gaussian distributions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive subgroup logistic regression for classification with unobserved heterogeneity. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10712-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unobserved heterogeneity refers to the variation among subjects that is not accounted for by the observed features used in a model. Its presence poses a substantial challenge to statistical modeling. This study introduces the Predictive Subgroup Logistic Regression (PSLR) model, which extends the conventional logistic regression and is specifically designed to address unobserved heterogeneity in classification problems. The PSLR model incorporates subject-specific intercepts in the log odds, fitted through a penalized likelihood approach with a concave pairwise fusion penalty. A novel two-step procedure is developed to facilitate the out-of-sample predictions for new subjects whose subgroup membership labels are unknown. This procedure allows the PSLR model to perform both inferential and predictive tasks. Through extensive simulation studies and an empirical application to a customer churn dataset in the telecommunications industry, the PSLR model not only demonstrates great performance in various aggregate accuracy metrics but also achieves a balanced effectiveness in sensitivity and specificity.},
  archive      = {J_SAC},
  author       = {Chen, Kun and Huang, Rui and Tong, Zhiwei},
  doi          = {10.1007/s11222-025-10712-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Predictive subgroup logistic regression for classification with unobserved heterogeneity},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient learning of symmetric positive-definite matrix regression. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10714-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Euclidean data are nowadays frequently encountered due to the advance in data-collection techniques. Under the Tikhonov regularization framework, this paper focuses on the gradient learning in a regression setting where the response is a symmetric positive-definite (SPD) matrix and the predictor is a Euclidean vector. We endow the SPD manifold with the Log-Euclidean metric to transform our model on the manifold to the Euclidean space and calculate the gradients by solving a linear system under the assumption that the gradient function resides in a reproducing kernel Hilbert space. We further simplify our algorithm and reduce the dimension of the linear system by singular value decomposition. Theoretical properties about the approximation error of the reducing-matrix-size algorithm and the error bound of gradient estimation are investigated as well. In numerical experiments, we show the validity of our SPD gradient learning algorithm in variable selection and sufficient dimension reduction. A real-world dataset about New York taxi networks is studied to illustrate the applicability of our algorithm.},
  archive      = {J_SAC},
  author       = {Chen, Baiyu and Fu, Xiaoyi and Li, Yunchen and Wang, Xiaozhou and Yu, Zhou},
  doi          = {10.1007/s11222-025-10714-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Gradient learning of symmetric positive-definite matrix regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing proportional hazards mixture cure models with transfer learning for interval-censored data. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10721-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a breast cancer study, we consider the analysis of interval-censored failure time data in the presence of a cure fraction. Although a great deal of literature has been established for the analysis of interval-censored data with cure fractions, there is no established method that adequately handles the limited sample size issue. Corresponding to this, we propose a transfer learning approach under the proportional hazards mixture cure models for interval-censored data with the aim to transfer the information from the informative auxiliary samples in a larger cohort to improve the performance of the target regression analysis. To assess the proposed method, an extensive simulation study is performed and suggests that it works well in practical situations. Furthermore, we apply the method to the breast cancer study with the focus on Black women by leveraging the data of other racial women and obtain the improved results.},
  archive      = {J_SAC},
  author       = {Lou, Yichen and Sun, Jianguo and Wang, Peijie and Zhao, Shishun},
  doi          = {10.1007/s11222-025-10721-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Enhancing proportional hazards mixture cure models with transfer learning for interval-censored data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust mean-shift clustering based on impartial trimming. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10718-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust version of the mean-shift algorithm is developed to cope with the presence of contaminated data when targeting a clustering problem by means of a modal approach. The goal is to protect nonparametric density-based clustering from the deleterious effect of outliers. Their occurrence affects the analysis mainly because outliers lead to the detection of spurious modes and groups. Therefore, the proposed methodology aims to recover the underlying clustered data configuration, while detecting and discarding outliers. A strategy to select the level of trimming is discussed. The finite sample behaviour of the proposed method is investigated by Monte Carlo numerical studies and empirical applications.},
  archive      = {J_SAC},
  author       = {Greco, Luca and Menardi, Giovanna},
  doi          = {10.1007/s11222-025-10718-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Robust mean-shift clustering based on impartial trimming},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-network assisted clustering using a grouped factor model. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10719-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the clustering task of large-scale panel data with assistance from multi-network information under the grouped factor model. In many real-world clustering tasks, multiple networks can be observed for the same set of cross-sectional units based on different types of interactions. Different networks are different portraits of latent group memberships, which inspired us to utilize multi-network information to improve the clustering accuracy and stability. Therefore, we propose a multi-network-assisted clustering method that encourages coherence between the clustering results and the weighted multi-network in a penalized manner. We also developed a flexible weight learning strategy to identify the clustering capacity differences of multiple networks. A computationally efficient algorithm with random initialization was developed to implement penalized estimation. Thorough simulation studies demonstrate that the proposed method is more promising than existing competitors, even with misleading network information. Finally, application to the daily returns of stocks traded on the Shanghai and Shenzhen stock exchanges demonstrates the effectiveness and efficiency of the new method. Supplementary materials for this article are available online.},
  archive      = {J_SAC},
  author       = {Liang, Wanwan and Wu, Ben and Fan, Xinyan and Zhang, Bo},
  doi          = {10.1007/s11222-025-10719-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Multi-network assisted clustering using a grouped factor model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating quantile regression for multi-source subgroup identification. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10713-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity and the multi-source nature of modern datasets present significant challenges for statistical methodologies. Motivated by the Stimulant Reduction Intervention using Dosed Exercise (STRIDE) study, we analyze primary outcomes from two complementary data sources to assess heterogeneity in treatment effects between experimental and control groups at medium and high quantiles. To address these challenges, we propose a novel approach that integrates quantile regression with weighted quantile loss and joint pairwise fusion penalties, enabling joint subgroup identification across data sources. Our method distinguishes between homogeneous and heterogeneous effects using a center regularization term and can detect sources lacking group structures. Theoretically, we establish weak Oracle properties, ensuring consistent estimation of group structures. Computationally, we employ the alternating direction method of multipliers (ADMM) and mitigate the burden of pairwise fusion through a k-nearest neighbors trimming method. The effectiveness of our approach is demonstrated through numerical simulations and an application to the STRIDE study.},
  archive      = {J_SAC},
  author       = {Wu, Jiaqi and Zhang, Weiping},
  doi          = {10.1007/s11222-025-10713-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Integrating quantile regression for multi-source subgroup identification},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite mixtures of multivariate poisson-log normal factor analyzers for clustering count data. <em>SAC</em>, <em>35</em>(6), 1-31. (<a href='https://doi.org/10.1007/s11222-025-10720-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mixture of multivariate Poisson-log normal factor analyzers is introduced by imposing constraints on the covariance matrix, which results in flexible models for clustering purposes. In particular, a class of eight parsimonious mixture models based on the mixtures of factor analyzers is introduced. The variational Gaussian approximation is used for parameter estimation, and information criteria are used for model selection. The proposed models are explored in the context of clustering discrete data arising from RNA sequencing studies. Using real and simulated data, the models are shown to give favourable clustering performance. The GitHub R package for this work is available at https://github.com/anjalisilva/mixMPLNFA and is released under the open-source MIT license.},
  archive      = {J_SAC},
  author       = {Payne, Andrea and Silva, Anjali and Rothstein, Steven J and McNicholas, Paul D. and Subedi, Sanjeena},
  doi          = {10.1007/s11222-025-10720-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Finite mixtures of multivariate poisson-log normal factor analyzers for clustering count data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized spectral clustering for large-scale multi-layer networks. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10723-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multi-layer networks with large numbers of nodes, edges, and layers arise across various domains, which poses a great computational challenge for downstream analysis. In this paper, we develop an efficient randomized spectral clustering algorithm for community detection in multi-layer networks. We first utilize the random sampling strategy to sparsify the adjacency matrix of each layer. Then we use the random projection strategy to accelerate the eigen-decomposition of the sum of squared sparsified adjacency matrices of all layers. The communities are finally obtained via the k-means of the eigenvectors. The algorithm not only has low time complexity but also saves storage space. Theoretically, we study the misclassification error rate of the proposed algorithm under the multi-layer stochastic block model, which shows that the randomization does not deteriorate the error bound under certain conditions. Numerical studies on multi-layer networks with millions of nodes show the superior efficiency of the proposed algorithm, which achieves clustering results rapidly. We develop a new R package, MLRclust, which makes the proposed methods available for both simulated and real multi-layer networks.},
  archive      = {J_SAC},
  author       = {Su, Wenqing and Guo, Xiao and Chang, Xiangyu and Yang, Ying},
  doi          = {10.1007/s11222-025-10723-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Randomized spectral clustering for large-scale multi-layer networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing quantile function estimation with beta-kernel smoothing. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10725-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a class of nonparametric quantile function estimators based on Beta kernel smoothing. We conduct a rigorous investigation into their large-sample properties, including asymptotic normality and mean squared equivalence to existing methods. Through extensive simulation studies, we demonstrate that the proposed Beta kernel estimators perform comparably to or outperform traditional empirical and symmetric location-scale kernel-based quantile estimators. Additionally, we provide two real-world applications to illustrate the practical effectiveness of our approach. The results suggest that Beta kernel smoothing offers a flexible and efficient alternative for quantile function estimation, particularly in cases where classical methods exhibit inefficiencies.},
  archive      = {J_SAC},
  author       = {Li, Juan and Yu, Ping and Shi, Jianhong and Song, Weixing},
  doi          = {10.1007/s11222-025-10725-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Enhancing quantile function estimation with beta-kernel smoothing},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive weighted composite quantile regression. <em>SAC</em>, <em>35</em>(6), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10726-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a nonparametric Bayesian estimation and model selection method for additive models based on weighted composite quantile regression. This method identifies the unknown smooth functions of the additive model as linear, nonlinear, or zero components by setting a multiplicative parameterized spike-slab prior distribution, which solves the problem of selecting predictive variable components in partially linear additive models when prior information is insufficient. In addition, it further generalizes the composite quantile regression to additive models by combining information from multiple quantiles. A Bayesian hierarchical model is established based on the mixed representation of the asymmetric Laplace distribution, and the posterior distributions of all unknown parameters are sampled by Markov chain Monte Carlo (MCMC). Finally, in the simulation and real data analysis, the variable selection results, root mean square error and other indicators are used to further prove that the method is more competitive than the existing methods.},
  archive      = {J_SAC},
  author       = {Ji, Yonggang and Wang, Mian and Zhou, Maoyuan},
  doi          = {10.1007/s11222-025-10726-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian additive weighted composite quantile regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive stratified monte carlo using decision trees. <em>SAC</em>, <em>35</em>(6), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10731-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been known for a long time that stratification is one possible strategy to obtain higher convergence rates for the Monte Carlo estimation of integrals over the hypercube $$[0, 1]^s$$ of dimension s. However, stratified estimators such as Haber’s are not practical as s grows, as they require $$\mathcal {O}(k^s)$$ evaluations for some $$k\ge 2$$ . We propose an adaptive stratification strategy, where the strata are derived from a decision tree applied to a preliminary sample. We show that this strategy leads to higher convergence rates, that is the corresponding estimators converge at rate $$\mathcal {O}(N^{-1/2-r})$$ for some $$r>0$$ for certain classes of functions. Empirically, we show through numerical experiments that the method may improve on standard Monte Carlo even when s is large.},
  archive      = {J_SAC},
  author       = {Chopin, Nicolas and Wang, Hejin and Gerber, Mathieu},
  doi          = {10.1007/s11222-025-10731-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive stratified monte carlo using decision trees},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid variational bayesian approach for spatial random effects structural equation modeling. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10730-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, structural equation modeling (SEM) has been widely applied in fields such as education, psychology, and environmental science. However, most studies overlook the spatial dependencies within the data, and there is limited research on SEM for spatial data. Spatial random effects (SRE) models, which flexibly capture spatial variation through a series of spatial basis functions (e.g., multiresolution wavelet basis functions), have become a powerful tool for spatial data analysis. This study extends the traditional SEM by incorporating SRE, resulting in a spatial random effects structural equation model (SRE-SEM) for modeling complex environmental spatial data. The model is fitted using a hybrid variational Bayes algorithm, with some model parameters estimated via the standard mean-field variational Bayes approach. To address the intractable posterior and the dimensionality of latent variables, the Metropolis-Hastings algorithm is employed to sample from the exact conditional posterior distribution of the latent variables. Additionally, a fixed-form variational Bayes approach is used to estimate the matrix on which the spatial covariance matrix depends. Simulation studies and case analyses demonstrate that the proposed model effectively captures the structure of spatial data, while the introduced estimation algorithms significantly improve computational efficiency. This study provides a robust and efficient framework for spatial data analysis, offering a promising solution for modeling complex environmental and socio-economic phenomena.},
  archive      = {J_SAC},
  author       = {Wu, Ying and Zhu, Hongyu and Zhang, Jiwei and Lu, Jing},
  doi          = {10.1007/s11222-025-10730-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A hybrid variational bayesian approach for spatial random effects structural equation modeling},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spilt conformal prediction with missing response. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10722-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction provides a distribution-free framework for constructing interval estimates of response variables with guaranteed coverage probabilities. This study introduces a missing weighted split conformalized quantile regression (M-WSCQR) method to address challenges associated with missing response variables, assuming a Missing at Random (MAR) mechanism. M-WSCQR employs covariate shift adjustment to account for the MAR mechanism, ensuring robust and reliable predictions with specified coverage guarantees. The proposed method exhibits double robustness and scalability, making it adaptable to a broad range of problem settings by modifying the weighting function or quantile regression model. Simulation studies conducted under varying missing rates, as well as both homoscedastic and heteroscedastic conditions, confirm the effectiveness and robustness of M-WSCQR. Additionally, its practical utility is demonstrated through analyses of two real-world datasets, highlighting its applicability in diverse inference contexts. In contrast to traditional methods that assume independent and identically distributed observations, M-WSCQR integrates information from both observed and missing groups. This approach accommodates covariate shifts arising from the MAR mechanism and differences in covariate distributions between observed and missing responses. By incorporating these considerations, M-WSCQR achieves enhanced robustness and versatility, offering a valuable solution for addressing missing data challenges in statistical modeling.},
  archive      = {J_SAC},
  author       = {Cao, Zhimiao and Zhang, Ce and Lian, Beibei and Jiang, Bei and Kong, Linglong and Yan, Xiaodong},
  doi          = {10.1007/s11222-025-10722-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Spilt conformal prediction with missing response},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring dependence between functional data via projection hilbert-schmidt covariance. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10727-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hilbert-Schmidt Independence Criteria is a well-known method for quantifying the dependence between two random vectors. However, it suffers from the curse of dimensionality. In this paper, we introduce a novel nonparametric independence test specifically designed for two functional random variables X and Y. The test is based on a new dependence metric, the so-called Projection Hilbert-Schmidt Covariance (PHSC), which efficiently characterizes the dependence of the random variables and improves upon the Hilbert-Schmidt Independence Criterion. The projection Hilbert-Schmidt covariance exhibits several favorable properties. It equals zero if and only if X and Y are independent, provided that the employed kernel is characteristic. It can be applied to random elements without finite moments when the employed kernel is bounded. It admits an U-statistic estimator, which facilitates the construction of our test. We construct a test based on the estimator and provide a theoretical critical value depending on the sample size and the bandwidths. Our analysis theoretically demonstrates the probabilities of the test committing two types of errors respectively, and proves its consistency under the local alternative hypothesis. Simulations and real data analysis show that the test based on the projection Hilbert-Schmidt covariance outperforms other competing tests for functional data.},
  archive      = {J_SAC},
  author       = {Tian, Zhentao and Wang, Darong and Zhang, Zhongzhan},
  doi          = {10.1007/s11222-025-10727-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Measuring dependence between functional data via projection hilbert-schmidt covariance},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Significativity indices for agreement values. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10728-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agreement measures, such as Cohen’s kappa or intraclass correlation, gauge the matching between two or more classifiers. They are used in a wide range of contexts from medicine, where they evaluate the effectiveness of medical treatments and clinical trials, to artificial intelligence, where they can quantify the approximation due to the reduction of a classifier. The consistency of different classifiers to a gold standard can be compared simply by using the order induced by their agreement measure with respect to the gold standard itself. Nevertheless, labelling an approach as good or bad exclusively by using the value of an agreement measure requires a scale or a significativity index. Some quality scales have been proposed in the literature for Cohen’s kappa, but they are mainly naïve, and their boundaries are arbitrary. We propose a general approach to evaluate the significance of any agreement value between two classifiers as the probability of randomly choosing a confusion matrix, built over the same data set, with a lower agreement value. This measure, named significativity, gauges the relevance of the observed agreement value rather than replacing the agreement measure used to calculate it. This work introduces two significativity indices: one dealing with finite data sets, the other one handling classification probability distributions. This manuscript also addresses the computational challenges of evaluating such indices and proposes some efficient algorithms for their evaluation.},
  archive      = {J_SAC},
  author       = {Casagrande, Alberto and Fabris, Francesco and Girometti, Rossano and Pagliarini, Roberto},
  doi          = {10.1007/s11222-025-10728-1},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Significativity indices for agreement values},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-inflation in the multivariate poisson lognormal family. <em>SAC</em>, <em>35</em>(6), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10729-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn’t account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to $$90\%$$ of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing $$90.6\%$$ of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.},
  archive      = {J_SAC},
  author       = {Batardière, Bastien and Chiquet, Julien and Gindraud, François and Mariadassou, Mahendra},
  doi          = {10.1007/s11222-025-10729-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Zero-inflation in the multivariate poisson lognormal family},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for matrix-vector linear regression without debiasing under kronecker covariance structure. <em>SAC</em>, <em>35</em>(6), 1-28. (<a href='https://doi.org/10.1007/s11222-025-10732-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider estimation and statistical inference for a mixed matrix-vector linear regression model by relaxing the independency entries restriction to a general Kronecker structure. A two-step estimation approach based on decorrelated score and rotation is proposed without additional debiasing procedure. In the first step, to deal with the high-dimensional nuisance matrix estimator, we construct an unbiased estimator for the vector parameter based on the decorrelated score function. In the second step, to achieve an unbiased and asymptotically normality estimator for the matrix estimator, we employ the sample-splitting and rotation-unbiasedness techniques. The Cramér-Rao lower bound of the proposed estimator for any linear function of matrix coefficient is attained. Simulation studies and an empirical analysis of Beijing air quality dataset demonstrate the superior performance of our proposed estimators.},
  archive      = {J_SAC},
  author       = {Ke, Baofang and Zhao, Weihua and Wang, Lei},
  doi          = {10.1007/s11222-025-10732-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Statistical inference for matrix-vector linear regression without debiasing under kronecker covariance structure},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiscale method for data collected from network edges via the line graph. <em>SAC</em>, <em>35</em>(6), 1-28. (<a href='https://doi.org/10.1007/s11222-025-10733-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected over networks can be modelled as noisy observations of an unknown function over the nodes of a graph or network structure, fully described by its nodes and their connections, the edges. In this context, function estimation has been proposed in the literature and typically makes use of the network topology such as relative node arrangement, often using given or artificially constructed node Euclidean coordinates. However, networks that arise in fields such as hydrology (for example, river networks) present features that challenge these established modelling setups since the target function may naturally live on edges (e.g., river flow) and/or the node-oriented modelling uses noisy edge data as weights. This work tackles these challenges and develops a novel lifting scheme along with its associated (second) generation wavelets that permit data decomposition across the network edges. The transform, which we refer to under the acronym LG-LOCAAT, makes use of a line graph construction that first maps the data in the line graph domain. We thoroughly investigate the proposed algorithm’s properties and illustrate its performance versus existing methodologies. We conclude with an application pertaining to hydrology that involves the denoising of a water quality index over the England river network, backed up by a simulation study for a river flow dataset.},
  archive      = {J_SAC},
  author       = {Cao, Dingjia and Knight, Marina I. and Nason, Guy P.},
  doi          = {10.1007/s11222-025-10733-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {A multiscale method for data collected from network edges via the line graph},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneity-aware debiased machine learning for high-dimensional partially linear models. <em>SAC</em>, <em>35</em>(6), 1-24. (<a href='https://doi.org/10.1007/s11222-025-10737-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, distributed data are often collected from different locations/times/populations /environments with non-negligible heterogeneity. In this paper, we consider inherently distributed data follow heterogeneous partially linear models with high-dimensional covariates, where each site involves a common parameter vector and site-specific nuisance functions. Three distributed estimators based on debiased machine learning are proposed to account for heterogeneity. The closed forms and asymptotic normal distributions of the proposed estimators have been explored and compared. Moreover, these estimators can be computed easily by transmitting some statistics from the local sites to the central site. The finite-sample performance is demonstrated through simulation studies and an application to Beijing multi-site air quality dataset is also provided.},
  archive      = {J_SAC},
  author       = {Wu, Yining and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-025-10737-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Heterogeneity-aware debiased machine learning for high-dimensional partially linear models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive generalized P-splines for functional data: A statistical framework via blockwise GSVD. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10734-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a novel approach for functional data approximation based on generalized P-splines with non-uniform and adaptively placed knots. The key innovation of our proposal is the integration of a conditioning-aware strategy for selecting both the number and positions of the knots, as well as the regularization parameters. By reformulating the Tikhonov regularization problem, we propose a computationally efficient criterion that controls model complexity while ensuring numerical stability. The resulting approximation framework not only improves the fit across the entire functional domain but also maintains compactness and robustness. Extensive numerical experiments conducted on both synthetic and real-world datasets demonstrate that our approach significantly outperforms traditional free knot and smoothing spline methods in terms of approximation error and conditioning.},
  archive      = {J_SAC},
  author       = {Magistris, Anna De and Romano, Elvira and Campagna, Rosanna},
  doi          = {10.1007/s11222-025-10734-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive generalized P-splines for functional data: A statistical framework via blockwise GSVD},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Post-transfer learning statistical inference in high-dimensional regression. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10738-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning (TL) for high-dimensional regression (HDR) is an important problem in machine learning, particularly when dealing with limited sample size in the target task. However, there currently lacks a method to quantify the statistical significance of the relationship between features and the response in TL-HDR settings. In this paper, we introduce a novel statistical inference framework for assessing the reliability of feature selection in TL-HDR, called PTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its ability to provide valid p-values to features selected in TL-HDR, thereby rigorously controlling the false positive rate (FPR) at desired significance level $$\alpha $$ (e.g., 0.05). Furthermore, we enhance statistical power by incorporating a strategic divide-and-conquer approach into our framework. We demonstrate the validity and effectiveness of the proposed PTL-SI through extensive experiments on both synthetic and real-world high-dimensional datasets, confirming its theoretical properties and utility in testing the reliability of feature selection in TL scenarios.},
  archive      = {J_SAC},
  author       = {Tam, Nguyen Vu Khai and My, Cao Huyen and Le Duy, Vo Nguyen},
  doi          = {10.1007/s11222-025-10738-z},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Post-transfer learning statistical inference in high-dimensional regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On semi-parametric progressive censoring. <em>SAC</em>, <em>35</em>(6), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10740-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns classical and Bayesian inference of the unknown parameters of a semi-parametric proportional hazard’s model when the data are progressively censored. Wang et al. (2010) considered this problem for specific parametric proportional hazard’s model with Weibull, Lomax and Gompertz baseline distributions. The aim of this paper is not to assume any specific parametric family of distributions. Instead, it is assumed that the baseline distribution has a piecewise constant hazard function with a given number of cut points. It becomes more flexible than any specific parametric class of distribution functions. The maximum likelihood estimators of the unknown parameters can be obtained in closed form. Order restricted maximum likelihood estimators have also been proposed. A very flexible Dirichlet gamma priors has been assumed on the unknown parameters. Based on these priors, the Bayes estimates, ordered restricted Bayes estimates, and the associated credible intervals are also provided. Simulation experiments have been performed to illustrate the effectiveness of the proposed method. In practice, the cut points may not be known. The choice of the cut points is an important issue. We have discussed the choice of the cut points based on a non-homogeneous Poisson process model. Finally, two data sets are analyzed for illustrative purposes.},
  archive      = {J_SAC},
  author       = {Prajapati, Deepak and Kundu, Debasis},
  doi          = {10.1007/s11222-025-10740-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {On semi-parametric progressive censoring},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-metric variable scaled splitting algorithm for nonsmooth nonconvex sparsity-penalized quantile regression. <em>SAC</em>, <em>35</em>(6), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10742-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent nonsmoothness of quantile regression problems creates significant computational difficulties. These challenges are further exacerbated when an additional nonconvex and nonsmooth sparsity penalty is incorporated into the model. By leveraging the Moreau envelope method to smooth out inherent nonsmooth components, we derive a stepwise smooth approximation scheme tailored for nonsmooth nonconvex sparsity-penalized quantile regression. However, this smoothing process introduces severe ill-conditioning into the problem formulation. To resolve this issue and nonconvexity due to the folded concave penalty, we propose an innovative two-metric variable scaled splitting algorithm that synergizes second-order optimization with first-order information from nonconvex penalties. The algorithm comprises a forward step that uses a quasi-Newton strategy to modify the descent direction and a backward step that employs an adaptive weight-metric matrix for the construction of the proximal operator associated with the nonconvex penalties. Theoretical analysis demonstrates the global convergence properties of the algorithm. Numerical experiments show that the proposed method outperforms widely used language packages R in solving penalized quantile regression problems, achieving superior computational efficiency and enhanced variable selection capabilities.},
  archive      = {J_SAC},
  author       = {Yang, Fan and Wang, Shangfei and Shen, Zhengwei and Bao, Wenwen},
  doi          = {10.1007/s11222-025-10742-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Two-metric variable scaled splitting algorithm for nonsmooth nonconvex sparsity-penalized quantile regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hausdorff correlation for interval-valued random objects. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10743-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the correlation between interval-valued data presents an essential yet challenging problem in modern statistical research due to the lack of basic geometric and algebraic structures. Existing methods are often limited by their reliance on algebraic formulations or assumptions about the underlying distribution of true values within intervals. Moreover, they primarily focus on simple midpoint-range interval representations, restricting their applicability to more complex interval structures, e.g., when the interval contains multiple segments. To address these limitations, we introduce the Fréchet framework into the interval metric space equipped with the Hausdorff distance, extending the notions of Fréchet mean and proposing a more general and straightforward interval dependency measure, called Hausdorff correlation. The proposed method offers a strong geometric interpretation, revealing the relationship between random intervals and their Hausdorff mean, while also accommodating a broader range of interval forms. From a theoretical perspective, we establish the foundational properties of the proposed framework, proving the existence and uniqueness of the Hausdorff mean. Empirical evaluations on both synthetic and real-world datasets demonstrate the distinctiveness and effectiveness of Hausdorff correlation and its superior performance in feature selection compared to existing methods. In particular, a real-world Wearable Watch Dataset analysis shows the Hausdorff correlation successfully captures the relationship between multi-segment sleep intervals and physiological indicators, where existing methods fail to provide meaningful estimates.},
  archive      = {J_SAC},
  author       = {Kang, Xinlai and Ouyang, Xiaxue and Liang, Haoxian and Meng, Cheng},
  doi          = {10.1007/s11222-025-10743-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Hausdorff correlation for interval-valued random objects},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An algorithm aiming at unimodal density-based clustering using gaussian mixture models. <em>SAC</em>, <em>35</em>(6), 1-27. (<a href='https://doi.org/10.1007/s11222-025-10659-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of unimodal density-based clustering based on Gaussian mixture models. In the proposed approach, clusters are thought of as regions of high probability density separated from other regions of low probability density, encouraging the creation of unimodal clusters. The problem of estimation of unimodal Gaussian mixtures has been solved only in the univariate case, while in this work we try to provide a solution for the multivariate setting. The unimodal density-based clustering works in two stages. First, a new merging algorithm based on the density definition of a cluster is used. This algorithm identifies which components should be merged in order to obtain a number of clusters less than or equal to the initial number of mixture components, on the basis of density similarities. Second, a penalized likelihood approach is adopted to induce unimodality in the merged set of components. We evaluate the performance of both methods on the basis of simulated samples and empirical applications.},
  archive      = {J_SAC},
  author       = {Tancini, Daniele and Scrucca, Luca and Bartolucci, Francesco},
  doi          = {10.1007/s11222-025-10659-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {An algorithm aiming at unimodal density-based clustering using gaussian mixture models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effective number of parameters in kernel density estimation. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10744-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise a new formula for measuring the effective degrees of freedom (EDoF) in kernel density estimation (KDE). Starting from the orthogonal polynomial sequence (OPS) expansion for the ratio of the empirical to the oracle density, we show how convolution with the kernel leads to a new OPS with respect to which one may express the resulting KDE. The expansion coefficients of the two OPS systems can then be related via a kernel sensitivity matrix, which leads to a natural oracle definition of EDoF through the trace operator. Asymptotic properties of the (empirical) plug-in EDoF are worked out through influence functions, and connections with other empirical EDoFs are established. Minimization of Kullback-Leibler divergence is investigated as an alternative to integrated squared error based bandwidth selection rules, yielding a new normal scale rule. The methodology, which arises from a proper oracle formulation and is not restricted to convolution kernels, suggests the possibility of a new bandwidth selection rule based on an information criterion such as AIC.},
  archive      = {J_SAC},
  author       = {Guglielmini, Sofia and Volobouev, Igor and Trindade, A. Alexandre},
  doi          = {10.1007/s11222-025-10744-1},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {The effective number of parameters in kernel density estimation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
