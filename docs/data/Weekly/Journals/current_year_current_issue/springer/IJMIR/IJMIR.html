<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMIR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmir">IJMIR - 4</h2>
<ul>
<li><details>
<summary>
(2025). Ultra fast-inference depth completion with linear attention-based cascaded hourglass network. <em>IJMIR</em>, <em>14</em>(4), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00381-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion is vital for various CV applications such as autonomous driving, robotics, and augmented reality. However, most of the existing approaches suffer from high computational overhead and slow inference speeds, limiting their real-time applicability. In this paper, we present the Linear Attention-based Cascade Hourglass Network (LA-CHN), a lightweight yet robust depth completion model designed for efficient and accurate dense depth prediction. The core of LA-CHN is our Lightweight Linear Attention (LLA) block, which substitutes quadratic self-attention with a ReLU-kernel linear mechanism and a spatial-reduction strategy to maintain a global receptive field at linear cost. These LLA blocks are embedded within a three-stage cascaded hourglass backbone, enabling multiscale feature aggregation and progressive refinement. Experimental results on the outdoor KITTI DC benchmark and indoor NYUv2 dataset show that our approach achieves superior performance compared to previous lightweight depth completion models.},
  archive      = {J_IJMIR},
  author       = {Wu, Zirui and Hao, Yongtao},
  doi          = {10.1007/s13735-025-00381-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Ultra fast-inference depth completion with linear attention-based cascaded hourglass network},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of multimodal visual representation learning: Tracing the evolution from CNNs to transformers and beyond. <em>IJMIR</em>, <em>14</em>(4), 1-30. (<a href='https://doi.org/10.1007/s13735-025-00382-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary goal of multimodal visual representation learning is to generate implicit information that effectively represents multimodal information by exploring the commonalities and characteristics between different modalities. This research report will discuss currently widely used advanced methods in the field of multimodal visual representation learning. This article will discuss these methods in the following order, culminating in multimodal visual learning: (1) pre-trained visual representation learning, (2) generative visual representation learning, (3) contrastive multimodal visual representation learning, and (4) image-text multimodal visual representation learning methods. Each element provides useful clues that ultimately lead to multimodal visual learning. Pre-trained visual representation learning refers to the application of supervised pre-training models in visual representation learning, while generative visual representation learning uses generative models to learn feature representations that can integrate multimodal information. Contrastive multimodal visual representation learning uses contrastive learning methods to compare similar and dissimilar sample pairs, learning feature representations in a self-supervised manner. Image-text multimodal visual representation learning methods, on the other hand, attempt to enhance the capabilities of visual representation learning by fusing visual information (such as images) with textual information. This review report will explain the above research background, the classification of different research methods, commonly used evaluation methods , and future development trends.},
  archive      = {J_IJMIR},
  author       = {Zhang, Dong and Wong, W. K. and Chew, I. M.},
  doi          = {10.1007/s13735-025-00382-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A comprehensive review of multimodal visual representation learning: Tracing the evolution from CNNs to transformers and beyond},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective reinforcement learning for recommender systems: A comprehensive survey of methods, challenges, and future directions. <em>IJMIR</em>, <em>14</em>(4), 1-35. (<a href='https://doi.org/10.1007/s13735-025-00383-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most traditional recommender systems (RSs) prioritize accuracy-based metrics, often favoring popular items while neglecting novelty, diversity, and user engagement. However, real-world recommendation scenarios involve multiple conflicting objectives, requiring advanced optimization techniques. Multi-Objective Optimization (MOO) has been explored in RSs using collaborative filtering and evolutionary algorithms, but these approaches suffer from cold-start issues, data sparsity, and high computational complexity. Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) offer promising alternatives by dynamically learning optimal policies. However, single-objective DRL struggles to balance accuracy with non-accuracy metrics, making it less effective for real-world trade-offs. This has led to the rise of Multi-Objective Reinforcement Learning (MORL), which optimizes conflicting objectives using Pareto-based optimization, scalarization, and policy gradient adaptations. Unlike prior surveys on RL-based RSs, this review specifically examines MORL techniques, providing a structured taxonomy, evaluation of optimization strategies, and analysis of their impact on personalization, diversity, fairness, and engagement. We also highlight unresolved challenges, including scalability, sample efficiency, computational complexity, and real-time applicability. Finally, we propose future research directions to enhance MORLâ€™s scalability, improve policy generalization, and integrate more efficient multi-objective learning techniques for large-scale recommendation systems.},
  archive      = {J_IJMIR},
  author       = {Fatima Ezzahra, Zaizi and Sana, Abakarim and Sara, Qassimi and Said, Rakrak},
  doi          = {10.1007/s13735-025-00383-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-35},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-objective reinforcement learning for recommender systems: A comprehensive survey of methods, challenges, and future directions},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An implicit layout-aware transformer for full-page end-to-end optical music recognition. <em>IJMIR</em>, <em>14</em>(4), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00385-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital preservation and accessibility of musical manuscripts present a significant challenge in musical heritage conservation. Optical Music Recognition (OMR) is a key solution for automating the transcription of sheet music into machine-readable formats, typically relying on multi-stage processing due to the complexity of musical notation. Although the trend in OMR is to reduce the number of processing steps, the latest single-step approaches lose layout information from the original image, which is crucial for accurately interpreting musical documents in practical applications. This paper presents the Layout-Aware Sheet Music Transformer, a novel method based on a single-step transcription strategy that leverages transformer attention mechanisms to extract layout information without explicit annotations. By dynamically analyzing internal model representations during direct image-to-sequence transcription, our approach extracts nuanced structural insights without requiring manual annotations, eliminating manual layout annotations while offering deeper insights into document structures that more faithfully reflect the underlying musical meaning. Experiments in four well-known OMR datasets show that our proposal is both superior in transcription and competitive in layout extraction, making it a solid candidate for a new state of the art in the field.},
  archive      = {J_IJMIR},
  author       = {Rios-Vila, Antonio and Fuentes-Martinez, Eliseo and Castellanos, Francisco J.},
  doi          = {10.1007/s13735-025-00385-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {An implicit layout-aware transformer for full-page end-to-end optical music recognition},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
