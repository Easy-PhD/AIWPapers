<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMIR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmir">IJMIR - 2</h2>
<ul>
<li><details>
<summary>
(2025). Ultra fast-inference depth completion with linear attention-based cascaded hourglass network. <em>IJMIR</em>, <em>14</em>(4), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00381-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion is vital for various CV applications such as autonomous driving, robotics, and augmented reality. However, most of the existing approaches suffer from high computational overhead and slow inference speeds, limiting their real-time applicability. In this paper, we present the Linear Attention-based Cascade Hourglass Network (LA-CHN), a lightweight yet robust depth completion model designed for efficient and accurate dense depth prediction. The core of LA-CHN is our Lightweight Linear Attention (LLA) block, which substitutes quadratic self-attention with a ReLU-kernel linear mechanism and a spatial-reduction strategy to maintain a global receptive field at linear cost. These LLA blocks are embedded within a three-stage cascaded hourglass backbone, enabling multiscale feature aggregation and progressive refinement. Experimental results on the outdoor KITTI DC benchmark and indoor NYUv2 dataset show that our approach achieves superior performance compared to previous lightweight depth completion models.},
  archive      = {J_IJMIR},
  author       = {Wu, Zirui and Hao, Yongtao},
  doi          = {10.1007/s13735-025-00381-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Ultra fast-inference depth completion with linear attention-based cascaded hourglass network},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of multimodal visual representation learning: Tracing the evolution from CNNs to transformers and beyond. <em>IJMIR</em>, <em>14</em>(4), 1-30. (<a href='https://doi.org/10.1007/s13735-025-00382-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary goal of multimodal visual representation learning is to generate implicit information that effectively represents multimodal information by exploring the commonalities and characteristics between different modalities. This research report will discuss currently widely used advanced methods in the field of multimodal visual representation learning. This article will discuss these methods in the following order, culminating in multimodal visual learning: (1) pre-trained visual representation learning, (2) generative visual representation learning, (3) contrastive multimodal visual representation learning, and (4) image-text multimodal visual representation learning methods. Each element provides useful clues that ultimately lead to multimodal visual learning. Pre-trained visual representation learning refers to the application of supervised pre-training models in visual representation learning, while generative visual representation learning uses generative models to learn feature representations that can integrate multimodal information. Contrastive multimodal visual representation learning uses contrastive learning methods to compare similar and dissimilar sample pairs, learning feature representations in a self-supervised manner. Image-text multimodal visual representation learning methods, on the other hand, attempt to enhance the capabilities of visual representation learning by fusing visual information (such as images) with textual information. This review report will explain the above research background, the classification of different research methods, commonly used evaluation methods , and future development trends.},
  archive      = {J_IJMIR},
  author       = {Zhang, Dong and Wong, W. K. and Chew, I. M.},
  doi          = {10.1007/s13735-025-00382-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A comprehensive review of multimodal visual representation learning: Tracing the evolution from CNNs to transformers and beyond},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
