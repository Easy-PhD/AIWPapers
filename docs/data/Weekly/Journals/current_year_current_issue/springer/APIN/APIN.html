<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>APIN</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="apin">APIN - 29</h2>
<ul>
<li><details>
<summary>
(2025). PretopoMD: Pretopology-based mixed data hierarchical clustering. <em>APIN</em>, <em>55</em>(15), 1-18. (<a href='https://doi.org/10.1007/s10489-025-06770-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm’s robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.},
  archive      = {J_APIN},
  author       = {Levy, Loup-Noé and Guerard, Guillaume and Djebali, Sonia and Amor, Soufian Ben},
  doi          = {10.1007/s10489-025-06770-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {PretopoMD: Pretopology-based mixed data hierarchical clustering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-discriminator generative adversarial networks with dynamic penalty to over-sample imbalanced credit datasets. <em>APIN</em>, <em>55</em>(15), 1-30. (<a href='https://doi.org/10.1007/s10489-025-06836-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of credit risk data imbalance reduces the effectiveness of assessment models. Existing oversampling methods focus only on a partial sample of a few classes, resulting in a lack of diversity in the types of data generated. This paper proposes an innovative GAN variant called Magnify-GAN. The originality of Magnify-GAN lies in the fact that it is equipped with a primary discriminator and multiple secondary discriminators, each of which employs a different loss function. This multi-discriminator approach not only improves the learning results, but also enriches the feedback received during the training process. In addition, we integrate an innovative dynamic coefficient mechanism to enable the model to dynamically adapt to changes in data distribution. To further improve stability and address the common modal collapse problem in GAN, a gradient penalty method is embedded in the training protocol. This integrated strategy ensures that Magnify-GAN can effectively generate samples representing various minority classes within the real data. Compared to ten classical imbalanced sampling methods, Magnify-GAN demonstrates superior performance in precision, F1-score, and AUC values across six synthetic and four real-world imbalanced datasets. Ablation studies, visualized through heatmaps, reveal the complementary synergy between the core modules. Furthermore, a complexity analysis shows that Magnify-GAN offers significant performance gains with moderate increases in computational cost compared to state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Dong, Xiaogang and Wang, Lifei and Qin, Xiwen and Shi, Hongyu},
  doi          = {10.1007/s10489-025-06836-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-30},
  shortjournal = {Appl. Intell.},
  title        = {Multi-discriminator generative adversarial networks with dynamic penalty to over-sample imbalanced credit datasets},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph convolutional network for time series classification using recurrence plots. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06841-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) is a crucial task across various domains, and its performance heavily depends on the quality of input representations. Among various representations, the recurrence plot (RP) effectively captures topological recurrence, the unique property of time series data. However, conventional convolutional neural networks (CNNs) cannot fully exploit this property since they treat the RP as grid-like data. In this study, we propose RP-GCN, a novel approach that uses a graph convolutional network (GCN) to exploit topological recurrence inherent in the RP, thereby improving TSC performance. Our method transforms a multivariate time series into graphs where state matrices act as node feature matrices and RPs serve as adjacency matrices, enabling graph convolution to utilize recurrence relationships. We evaluated RP-GCN on 35 benchmark multivariate time series classification datasets and demonstrated superior accuracy and efficient inference time compared to existing methods.},
  archive      = {J_APIN},
  author       = {Kang, Hyewon and Lee, Taek-Ho and Lee, Junghye},
  doi          = {10.1007/s10489-025-06841-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {A graph convolutional network for time series classification using recurrence plots},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preprocessing method for shield operational parameters adaptable to geological survey data characteristic for predicting disc cutter wear. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06846-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shield operational parameters are inherently noisy and, relative to concurrent geological exploration data, contain considerable redundancy, they must be pre-processed before the datasets input to artificial intelligence models. This paper presents a denoising and compression method for preprocessing shield operational parameters, integrating it with the stratal slicing method for predicting disc cutter wear. The operational parameter signals affecting cutter wear are first denoised using wavelet transform, Fourier transform, rolling average, and autoencoder techniques. The proposed Ring-based Summation Averaging (RSA) and Piecewise Aggregate Averaging (PAA) methods are then used to compress the denoised signals, resulting in compressed sequences composed of key points equal to the number of tunnel rings, effectively matching the geological parameters expanded by the stratal slicing method. Furthermore, the prepared data were tested using the long short-term memory (LSTM) + attention mechanism (AM) model to evaluate its application effectiveness in the Guangzhou Metro Line 18 railway. The results show that data compressed using PAA not only better tracks signal variations but also allows for flexible control of the output length of the compressed sequence. The combination of wavelet transforms denoising (WTD) with PAA exhibited the best wear prediction results, achieving R2 / MSE = 0.95 / 2.21 mm. By integrating WTD, PAA, stratal slicing method, and sequence models, a comprehensive and universal methodology is established that can predict disc cutter wear based on initial geological data and shield operational parameters.},
  archive      = {J_APIN},
  author       = {Mo, Deyun and Bai, Liping and Liao, Wenjiang and Tian, Xinyuan and Huang, Weiran},
  doi          = {10.1007/s10489-025-06846-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Preprocessing method for shield operational parameters adaptable to geological survey data characteristic for predicting disc cutter wear},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic gradient accelerated by negative momentum for training deep neural networks. <em>APIN</em>, <em>55</em>(15), 1-13. (<a href='https://doi.org/10.1007/s10489-025-06900-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast and robust stochastic optimization algorithms for training deep neural networks (DNNs) are still a topic of heated discussion. As a simple but effective way, the momentum technique, which utilizes historical gradient information, shows significant promise in training DNNs both theoretically and empirically. Nonetheless, the accumulation of error gradients in stochastic settings leads to the failure of momentum techniques, e.g., Nesterov’s accelerated gradient (NAG), in accelerating stochastic optimization algorithms. To address this problem, a novel type of stochastic optimization algorithm based on negative momentum (NM) is developed and analyzed. In this work, we applied NM to vanilla stochastic gradient descent (SGD), leading to SGD-NM. Although a convex combination of previous and current historical information is adopted in SGD-NM, fewer hyperparameters are introduced than those of the existing NM techniques. Meanwhile, we establish a theoretical guarantee for the resulting SGD-NM and show that SGD-NM enjoys the same low computational cost as vanilla SGD. To further show the superiority of NM in stochastic optimization algorithms, we propose a variant of stochastically controlled stochastic gradient (SCSG) based on the negative momentum technique, termed SCSG-NM, which achieves faster convergence compared to SCSG. Finally, we conduct experiments on various DNN architectures and benchmark datasets. The comparison results with state-of-the-art stochastic optimization algorithms show the great potential of NM in accelerating stochastic optimization, including more robust to large learning rates and better generalization.},
  archive      = {J_APIN},
  author       = {Li, Xiaotian and Yang, Zhuang and Wang, Yang},
  doi          = {10.1007/s10489-025-06900-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Stochastic gradient accelerated by negative momentum for training deep neural networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). External information-augmented contrastive learning framework for fake news detection. <em>APIN</em>, <em>55</em>(15), 1-21. (<a href='https://doi.org/10.1007/s10489-025-06807-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake news and information overload on social media has led to increased public confusion and poses a serious threat to social stability. Traditional fake news detection methods typically focus solely on the content of the news itself, making them vulnerable to manipulation by disinformation campaigns. This limitation highlights the need for a more comprehensive approach that incorporates external information to improve detection accuracy. In response to this challenge, we propose a novel framework for fake news detection, named External Information-Augmented Contrastive Learning (EACL). The EACL framework consists of three key modules: (1) the External Information Construction Module, which utilizes entity linking, embedding, and retrieval techniques to analyze news from both factual and public opinion perspectives, thus creating an analysis-friendly environment; (2) the Consistency Feature Extraction Module, which employs a distance-aware signed attention mechanism to model the consistency between news content and external information, while filtering out irrelevant data; and (3) the Comparative Learning Enhancement Module, which constructs positive and negative sample pairs to enhance the learning of semantic differences between fake and real news. Extensive qualitative and quantitative experiments conducted on two real-world datasets demonstrate that EACL achieves impressive accuracy rates of 85.2% and 82.9%, significantly outperforming existing baseline methods. The results further illustrate the effectiveness of integrating external information and contrastive learning in combating misinformation.},
  archive      = {J_APIN},
  author       = {Fang, Xiaochang and Zhang, Huaxiang and Wu, Hongchen and Liu, Li and Yu, Hongzhu and Li, Hongxuan and Jing, Zhaorong},
  doi          = {10.1007/s10489-025-06807-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {External information-augmented contrastive learning framework for fake news detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-stimulus generalized and corrected canonical correlation analysis for enhancing SSVEP detection. <em>APIN</em>, <em>55</em>(15), 1-24. (<a href='https://doi.org/10.1007/s10489-025-06859-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial filter-based calibration-training algorithms play a crucial role in improving the information transfer rate (ITR) of steady-state visual evoked potential based brain-computer interfaces (SSVEP-BCIs). These algorithms optimize spatial filters by suppressing the non-SSVEP related components, thereby enhancing the signal-to-noise ratio (SNR) of electroencephalogram (EEG) signals. However, conventional methods neglect the temporally-varying and spatially-coupled characteristics of EEG signals, leading to inherent ITR bottlenecks in BCIs. To this end, we propose a novel SSVEP detection algorithm, termed as multi-stimulus Generalized and Corrected Canonical Correlation Analysis (msGC3A), which is extended and corrected from the generalized canonical correlation analysis algorithm. Specifically, we develop corrected sine-cosine reference templates that enhance the spatial filters’ generalization capability across multiple stimuli. Moreover, we formulate a weighted correlation coefficient that synergistically integrates both generalized and corrected multi-stimulus templates for further enhancement. Empirical experiments have been conducted on two publicly available benchmark SSVEP datasets, and we compared the ensemble version of our msGC3A algorithm with four state-of-the-art algorithms. The results have shown that our algorithm significantly improves SSVEP detection performance while requiring less calibration data. Furthermore, we also conducted ablation experiments to show the adaptive capacity of employing our algorithm for SSVEP-BCIs.},
  archive      = {J_APIN},
  author       = {Lv, Yanhao and Luo, Tian-jian},
  doi          = {10.1007/s10489-025-06859-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Multi-stimulus generalized and corrected canonical correlation analysis for enhancing SSVEP detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards understanding the optimization mechanisms in deep learning. <em>APIN</em>, <em>55</em>(15), 1-22. (<a href='https://doi.org/10.1007/s10489-025-06875-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we adopt a probability distribution estimation perspective to explore the optimization mechanisms of supervised classification using deep neural networks. We demonstrate that, when employing the Fenchel-Young loss, despite the non-convex nature of the fitting error with respect to the model’s parameters, global optimal solutions can be approximated by simultaneously minimizing both the gradient norm and the structural error. The former can be controlled through gradient descent algorithms. For the latter, we prove that it can be managed by increasing the number of parameters and ensuring parameter independence, thereby providing theoretical insights into mechanisms such as overparameterization and random initialization. Ultimately, the paper validates the key conclusions of the proposed method through empirical results, illustrating its practical effectiveness.},
  archive      = {J_APIN},
  author       = {Qi, Binchuan and Gong, Wei and Li, Li},
  doi          = {10.1007/s10489-025-06875-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Towards understanding the optimization mechanisms in deep learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust low-rank representation with structured similarity learning for multi-label classification. <em>APIN</em>, <em>55</em>(15), 1-24. (<a href='https://doi.org/10.1007/s10489-025-06879-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling high-dimensional, noisy data in multi-label classification is challenging, as feature abundance and noise obscure actual data-label relationships. Traditional approaches often model labels and features independently, limiting dependency modeling and noise reduction. To address this, we propose a unified framework combining low-rank representation using nuclear norm regularization with structured similarity learning. This simultaneously projects features and labels into low-rank spaces while preserving key inter-sample and inter-label relationships through structural constraints, further capturing fine-grained correlations via a learned similarity Matrix. Extensive experiments on five benchmark datasets show our model outperforms state-of-the-art methods, achieving a 16% reduction in Hamming Lossl and a 14% improvement in Micro-F1 on high-dimensional, noisy datasets like CAL500 and Corel16k7, with consistent gains in Macro-F1 and Example-F1. These results demonstrate the model’s strong capability for noisy, high-dimensional multi-label classification.},
  archive      = {J_APIN},
  author       = {Ntaye, Emmanuel and Zhou, Conghua and Liu, Zhifeng and Song, Heping and Issahaku, Fadilul-lah Yassaanah and Shen, Xiang-Jun},
  doi          = {10.1007/s10489-025-06879-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Robust low-rank representation with structured similarity learning for multi-label classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced inverted transformer: Advancing variate token encoding and blending for time series forecasting. <em>APIN</em>, <em>55</em>(15), 1-19. (<a href='https://doi.org/10.1007/s10489-025-06886-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in channel-dependent Transformer-based forecasters highlight the efficacy of variate tokenization for time series forecasting. Despite this progress, challenges remain in handling complex time series. The vanilla Transformer, while effective in certain scenarios, faces limitations in addressing intricate cross-variate interactions and diverse temporal patterns. This paper presents the Enhanced Inverted Transformer (EiT for short), enhancing standard Transformer blocks for advanced modeling and blending of variate tokens. EiT incorporates three key innovations: First, a hybrid multi-patch attention mechanism that adaptively fuses global and local attention maps, capturing both stable and volatile correlations to mitigate overfitting and enrich inter-channel communication. Second, a multi-head feed-forward network with specialized heads for various temporal patterns, enhancing parameter efficiency and contributing to robust multivariate predictions. Third, paired channel normalization applied to each layer, preserving crucial channel-specific statistics and boosting forecasting performance. By integrating these innovations, EiT effectively overcomes limitations and unlocks the potential of variate tokens for accurate and robust multivariate time series forecasting. Extensive evaluations demonstrate that EiT achieves state-of-the-art (SOTA) performance, surpassing the previous method, the inverted Transformer, by an average of 4.4% in Mean Squared Error (MSE) and 3.4% in Mean Absolute Error (MAE) across five challenging long-term forecasting datasets.},
  archive      = {J_APIN},
  author       = {Li, Xin-Yi and Yang, Yu-Bin},
  doi          = {10.1007/s10489-025-06886-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced inverted transformer: Advancing variate token encoding and blending for time series forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid federated unlearning with tuning parameters based on fisher information matrix. <em>APIN</em>, <em>55</em>(15), 1-18. (<a href='https://doi.org/10.1007/s10489-025-06593-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed machine learning approach widely applied in privacy-sensitive scenarios. With the emergence of the “right to be forgotten” and the pursuit of data accuracy, there is an increasing demand to quickly and accurately delete targeted information from models while ensuring model performance. Therefore, federated unlearning has been introduced. Although current federated unlearning methods achieve effective unlearning, they often involve lengthy processes and require servers to store extensive historical update information. We propose a novel rapid federated unlearning method named FedTune. This method leverages the Fisher information matrix computed on the client side to assess the correlation between model parameters and the target data, identifying key parameters for adjustment. Based on the importance of these parameters and the frequency of client participation, FedTune determines appropriate adjustment ratios to increase the classification loss on the target data, thereby reducing the model’s accuracy and achieving effective data unlearning. Finally, the server collaborates with the remaining clients for a few rounds of retraining to restore the overall classification performance rapidly. We evaluated the FedTune method on the MNIST, CIFAR-10, and PURCHASE datasets, considering both fixed and dynamic client selection scenarios in privacy-sensitive and contamination settings. Experimental results show that FedTune reduces the time consumed by the unlearning process and server storage costs of the unlearning algorithm while ensuring model classification accuracy and effective unlearning compared to other unlearning algorithms.},
  archive      = {J_APIN},
  author       = {Zhao, Fengda and Xu, Qianyi and Wang, Hao and Guo, Dingding},
  doi          = {10.1007/s10489-025-06593-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Rapid federated unlearning with tuning parameters based on fisher information matrix},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large-scale group decision-making approach for quality function deployment based on dempster-shafer evidence theory and hierarchical clustering algorithm. <em>APIN</em>, <em>55</em>(15), 1-36. (<a href='https://doi.org/10.1007/s10489-025-06724-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality Function Deployment (QFD) is a classic customer requirements (CRs)-oriented quality management method. However, the increasing complexity and diversity of CRs in the modern society makes it impossible for the traditional QFD approach with a limited number of team members (TMs) to fully satisfy CRs. Therefore, in order to solve the QFD problem in complex environments, this paper proposes an improved QFD method based on Dempster–Shafer evidence theory (D-S theory) and hierarchical clustering algorithm in large-scale group environments. Firstly, utilizing the advantages of D-S theory in information processing and synthesis, the evaluation of quality characteristics (QCs) in the form of probabilistic linguistic term sets (PLTSs) is transformed into basic probability assignments (BPAs) to handle uncertainty more flexibly. Secondly, this paper designs a hierarchical clustering algorithm based on bounded confidence to divide TMs into subgroups, and fully considers the interaction willingness of TMs during the clustering process to ensure the efficiency and accuracy of decision-making. On this basis, the Stepwise Weight Assessment Ratio Analysis (SWARA) method based on distance degree is introduced to calculate the weight of CRs in a more objective way. Then, the Decision-making Trial and Evaluation Laboratory (DEMATEL) method based on D-S theory is used to deeply analyze the mutual influence relationship between QCs to reveal its internal logic. Besides, combined with the psychological expectations of TMs, the disappointment theory is used to prioritize QCs to ensure that products or services are more in line with customer expectations. Finally, this paper applies the proposed method to the development process of mobile health applications (mHealth apps) from the perspective of privacy security, verifying the practicability and superiority of the method. The effectiveness of the method in CRs transformation and product design optimization is further demonstrated through parametric and comparative analyses.},
  archive      = {J_APIN},
  author       = {Liu, Zhengmin and Feng, Xuan and Zhang, Jihao and Zhang, Bo and Wang, Wenxin and Liu, Peide},
  doi          = {10.1007/s10489-025-06724-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-36},
  shortjournal = {Appl. Intell.},
  title        = {A large-scale group decision-making approach for quality function deployment based on dempster-shafer evidence theory and hierarchical clustering algorithm},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way clustering ensemble based on shadowed sets with five approximation regions. <em>APIN</em>, <em>55</em>(15), 1-27. (<a href='https://doi.org/10.1007/s10489-025-06726-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering ensemble is a powerful technique for aggregating multiple clustering results. In order to address the challenge in clustering analysis which was brought by the uncertainty information in the datasets, this work presents a novel three-way clustering ensemble method based on shadowed sets with five approximation regions (3WCE-S5). Firstly, a set of clustering members are generated by fuzzy c-means clustering (FCM). A new shadowed sets is approximated by five regions, named as shadowed sets with five approximation regions (S5). Then, all objects are initially partitioned into five regions according to their membership degrees, which are provided by FCM. Secondly, according to multi-granularity rough sets, objects are further assigned into six approximated regions, namely a core region and five fringe regions. There is a partial order relationship between these six different approximate regions. Finally, the above six regions are processed by the new shadowed sets again to generate the output of three-way clustering. Ten University of California Irvine (UCI) data sets are employed to test the performance of this approach and five comparative methods. Accuracy (ACC), adjusted rand index (ARI), normalized mutual information (NMI) and time cost are utilized to quantify the clustering results.},
  archive      = {J_APIN},
  author       = {Yi, Huangjian and Guo, Dongkai and Zhang, Qinran and He, Xiaowei and Ren, Ruisi},
  doi          = {10.1007/s10489-025-06726-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {Three-way clustering ensemble based on shadowed sets with five approximation regions},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic interaction-enhanced encoding network for math word problem solving. <em>APIN</em>, <em>55</em>(15), 1-21. (<a href='https://doi.org/10.1007/s10489-025-06850-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving math word problems (MWPs) requires machines to understand not only the literal meaning of text but also the abstract logic and mathematical reasoning embedded within it. However, existing models often lack explicit reasoning capabilities for semantic information, particularly when dealing with complex math word problem texts. Additionally, these models tend to embed all kinds of information without fine-grained selection, which may introduce unexpected noise for mathematical expression generation. To address these challenges, we propose a Semantic Interaction-Enhanced Encoding Network (SIEN) for math expression generation is proposed in this paper. Firstly, SIEN constructs a semantic role interaction graph for each problem and employs a graph attention neural network to learn interaction and semantic information, offering a more structured and enriched view of the math word problem text. Secondly, SIEN introduces a multi-channel adapter module that simultaneously learns comprehensive contextual information from numeric information channel, hierarchical semantic information channel, and interaction information channel. Furthermore, SIEN introduces a dynamic weighting mechanism that adjusts the information weight from each channel to prioritize relevant information and reduce noise. Experimental results on three public benchmark datasets demonstrate that SIEN achieves significant performance improvement over other state-of-the-art baseline models.},
  archive      = {J_APIN},
  author       = {Xiao, Lingsheng and Chen, Yuzhong and Liu, Zhanghui and Zhong, Jiayuan and Dong, Yu},
  doi          = {10.1007/s10489-025-06850-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Semantic interaction-enhanced encoding network for math word problem solving},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KRMNet: Learning core representations for partial discharge pattern recognition via masked autoencoders and mixed position coding. <em>APIN</em>, <em>55</em>(15), 1-22. (<a href='https://doi.org/10.1007/s10489-025-06899-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial discharge pattern recognition (PDPR) is a crucial cornerstone for condition monitoring and safe operation of electrical devices. It has become an important hotspot in the field of energy systems. However, it faces several challenges, including noise interference, signal complexity, and difficulty in data labeling. This study proposes an efficient multi-scale masked autoencoder (MAE) network (KRMNet) to effectively address these challenges. KRMNet learns common and important multi-scale features and long-range semantic dependencies of partial discharge signals. Furthermore, by using the MAE structure and the transformer as the backbone, the model extracts key distinguishing features from phase-resolved partial discharge (PRPD) signals with background noise, interference, and labeling issues in an efficient and self-supervised manner. In addition, the efficient multi-scale module uses an efficient multi-scale attention mechanism to aggregate key information from multiple feature dimensions. The integration of the efficient multi-scale attention mechanism and contrastive learning methods improves the model’s ability to distinguish key information and resist interference. Experiments on two challenging PRPD datasets show that our proposed KRMNet achieves detection accuracies of 88.5% and 90.2% on noisy (ECPD) and clean (PUPD) datasets, respectively. This finding suggests that the method faces challenges in effectively managing noise interferences and missing labels.},
  archive      = {J_APIN},
  author       = {Deng, Yi and Chen, Jiawen and Xie, Quan and Tan, Dapeng and Liu, Hai},
  doi          = {10.1007/s10489-025-06899-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {KRMNet: Learning core representations for partial discharge pattern recognition via masked autoencoders and mixed position coding},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLDM-palm: A controllable latent diffusion model for high-fidelity palmprint generation based on bézier curves. <em>APIN</em>, <em>55</em>(15), 1-21. (<a href='https://doi.org/10.1007/s10489-025-06923-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using limited real data to synthesize realistic palmprints and expand training samples for recognition models has become a promising direction in palmprint recognition. However, the pseudo-palmprints generated by existing models still exhibit significant discrepancies from real ones, particularly in crease structures and fine-grained details. In this paper, we first introduce Latent Diffusion Models (LDM) as the backbone to improve the quality of palmprint generation. Secondly, to incorporate Bézier curves as control conditions into the model, we propose the Palm-to-Bézier Module (P2BM), which maps real palmprints to their corresponding Bézier-style pseudo-Bézier curves. These curves establish the connection between real palmprints and Bézier curves, which are used as conditional inputs during diffusion model training. At inference time, Bézier curves can be provided as conditions to generate high-resolution, fine-grained, and highly realistic synthetic palmprints. Thirdly, to enable Bézier curves to better model palmprint creases, we propose 12 Bézier curves templates based on real crease distribution priors. With only 10-step Denoising Diffusion Implicit Models (DDIM) sampling, our method achieves a significantly lower Fréchet Inception Distance (FID) compared to existing palmprint generation approaches. Moreover, the recognition models trained on the synthetic palmprints generated by our model achieve new state-of-the-art results in both Fisher Discriminant Ratio (FDR) and TAR@FAR metrics. Under a 1:1 train-test fine-tuning setting, our model improves average TAR@FAR= $$10^{-6}$$ performance by over $$10\%$$ compared to prior methods. We name our model CLDM-Palm (Controllable Latent Diffusion Model-Palm).},
  archive      = {J_APIN},
  author       = {Zhu, Yuanpan and Jia, Donghuai and Chu, Kevin and Zhi, Wenshuang and Li, Weide and Chen, Shukai},
  doi          = {10.1007/s10489-025-06923-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {CLDM-palm: A controllable latent diffusion model for high-fidelity palmprint generation based on bézier curves},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RXNet: Cross-modality person re-identification based on a dual-branch network. <em>APIN</em>, <em>55</em>(15), 1-24. (<a href='https://doi.org/10.1007/s10489-025-06501-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of text-based person re-identification (TI-ReID) is to match individuals using various methods by integrating information from both images and text. TI-ReID encounters significant challenges because of the clear differences in features between images and textual descriptions. Contemporary techniques commonly utilize a method that merges general and specific characteristics to obtain more detailed feature representations. However, these techniques depend on additional models for estimating or segmenting human poses to determine local characteristics, making it challenging to apply them in practice. To solve this problem, we propose a dual-path network based on RegNet and XLNet for TI-ReID (RXNet). In the image segment, RegNet is employed to acquire multitiered semantic image attributes and dynamically assimilate distinct local features through visual focus. In the text segment, XLNet is utilized, to extract significant semantic attributes from the text via a two-way encoding system based on an autoregressive model. Furthermore, to increase the efficacy of our model, we develop both residual triplet attention and dual attention to align features across different modalities. Additionally, we replace cross-entropy ID loss with smoothing ID loss to prevent overfitting while improving the efficiency of the model. Experimental results on the CUHK-PEDES dataset show that the proposed method achieves a rank-1/mAP accuracy of 85.49%/73.40%, outperforming the current state-of-the-art methods by a large margin.},
  archive      = {J_APIN},
  author       = {Zhang, Weiyang and Guo, Jiong and Liu, Qiang and Zou, Maoyang and Chen, Honggang and Peng, Jing},
  doi          = {10.1007/s10489-025-06501-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {RXNet: Cross-modality person re-identification based on a dual-branch network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prototypes guided model transformations between personalization and generalization in federated learning. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06566-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has gained popularity due to its ability to train a collaborative model while preserving privacy. However, it still faces limitations when dealing with heterogeneous data, primarily manifesting as the performance degradation of the global model and the inadaptability of the single global model to the divergence of client data distributions. Although the above issues are summarized by researchers as goals for generalization and personalization, few studies have simultaneously addressed both goals, with most prioritizing one over the other. In this paper, it is demonstrated that the FL iteration already incorporates model transformations between personalization and generalization, with a focus on ensuring the smooth functionality of these transformations under high data heterogeneity. Specifically, a novel Federated Prototype Transformation Framework (FedPT) is proposed, which is capable of generating a well-performing generalized model as well as personalized models simultaneously. FedPT constructs local prototype classifiers that explicitly guide personalized model optimization during local training, and these can be aggregated into a global prototype classifier suitable for generic tasks. The momentum update design retains the global knowledge in local training and aligns features between clients, which results in a smoother iteration. Moreover, an improved sample-level contrastive loss is presented to dig into deeper representations, achieving high-quality prototype generation even for missing or imbalanced classes. Experimental results demonstrate the exceptional performance of FedPT in both generalization and personalization tasks, outperforming latest methods.},
  archive      = {J_APIN},
  author       = {Xi, Yuan and Li, Qiong and Mao, HaoKun},
  doi          = {10.1007/s10489-025-06566-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Prototypes guided model transformations between personalization and generalization in federated learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DIMCAR: Dynamic intent modeling and context-aware recommendations in sparse data environment towards next basket prediction. <em>APIN</em>, <em>55</em>(15), 1-30. (<a href='https://doi.org/10.1007/s10489-025-06796-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fast-changing world of e-commerce, the success of recommender systems is crucial for boosting user engagement and increasing sales. Conventional models often struggle with evolving user preferences and data sparsity, hindering accurate predictions. Existing Graph-based regularization mechanisms and deep learning approaches address these challenges but remain sensitive to noise and computational complexity, limiting their effectiveness in large-scale, real-time settings. We propose a novel multi-layered Next Basket Recommender System called dynamic intent modelling and context-aware recommendation (DIMCAR) model to overcome these limitations. First, we resolve the data sparsity problem by constructing a novel optimized Graph Sparse Regularization framework for Non-negative Matrix Factorization (OGSR-NMF) framework integrating a time-varying graph structure, a novel hybrid sparsity norm, a modified Proximal Alternating Linearized Minimization (mPALM). Additionally, we dynamically model user intents and context using attention mechanisms and Gated Recurrent Units (GRUs). Finally, we integrate a novel Adaptive Reptile Basket Optimization Algorithm into a Deep Convolutional Neural Network, enhancing the model's adaptability to changing user behaviours in real time. Theoretical analysis and experiments on four benchmark datasets demonstrate that DIMCAR outperforms existing models in recommendation accuracy and user satisfaction.},
  archive      = {J_APIN},
  author       = {Arthur, John Kingsley and Zhou, Conghua and Shen, Xiang-Jun and Amber-Doh, Ronky Wrancis and Mantey, Eric Appiah and Osei-Kwakye, Jeremiah},
  doi          = {10.1007/s10489-025-06796-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-30},
  shortjournal = {Appl. Intell.},
  title        = {DIMCAR: Dynamic intent modeling and context-aware recommendations in sparse data environment towards next basket prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EGPT-SPE: Story point effort estimation using improved GPT-2 by removing inefficient attention heads. <em>APIN</em>, <em>55</em>(15), 1-16. (<a href='https://doi.org/10.1007/s10489-025-06824-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating story points from user requirements is crucial in the Software Development Life Cycle (SDLC) as it impacts resource allocation and timelines; inaccuracies can lead to missed deadlines and increased costs, harming a company’s reputation. While various techniques have emerged to automate this process, conventional machine learning methods often fail to understand the context of user requirements, and deep learning approaches face high computational costs. To address these issues, the Efficient GPT for Story Point Estimation (EGPT-SPE) algorithm optimizes the Multi-Head Attention module by removing inefficient heads, enhancing accuracy and reducing costs. Experiments on the Choetkiertikul dataset (23,313 issues across 16 open-source projects) and the TAWOS dataset (458,232 issues across 39 open-source projects from 12 public JIRA repositories) demonstrated a 5 to 15 percent accuracy improvement in both within-project and cross-project estimations, validating the algorithm’s effectiveness in agile story point estimation.},
  archive      = {J_APIN},
  author       = {Cheemaa, Amna Shahid and Azhar, Muhammad and Arif, Fahim and ul haq, Qazi Mazhar and Sohail, Muhammad and Iqbal, Asma},
  doi          = {10.1007/s10489-025-06824-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {EGPT-SPE: Story point effort estimation using improved GPT-2 by removing inefficient attention heads},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing vitiligo stage diagnosis through a reliable multimodal model with uncertainty calibration. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06839-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vitiligo is a common dermatological disease featuring hypopigmentation. Accurate staging of vitiligo is crucial for enhancing treatment efficacy. However, traditional diagnostic methods, which rely on physicians' subjective judgments, are time-consuming, labor-intensive, and prone to misdiagnosis. Recently, AI-powered multimodal dermatological classification models have demonstrated significant potential in this area. But the credibility of these models at the decision-making stage is an area that requires further refinement. This study proposes a multimodal disease staging diagnostic model with uncertainty calibration to analyze multimodal samples from three stages of vitiligo. The model innovatively extracts feature information from various modalities and transforms it into a Dirichlet distribution to assess sample uncertainty. Then, the Dempster—Shafer theory is used to fuse evidence from different modalities, generating a final diagnostic result and an uncertainty score. Additionally, an uncertainty—based loss function is designed. And by using an uncertainty threshold method, the model can detect high—uncertainty samples that require additional judgment, effectively reducing the risk of misdiagnosis and missed diagnosis. Experimental results show that this model outperforms existing methods in terms of accuracy, precision, recall, and F1—score. Anomaly detection and noise—resistance experiments verify the model's robustness in handling unknown and noisy data. This model offers a new approach for AI-assisted vitiligo diagnosis, which can assist doctors in making more accurate diagnostic decisions, contribute to improving treatment efficiency.},
  archive      = {J_APIN},
  author       = {Li, Zhiming and Jiang, Shuying and Xiang, Fan and Li, Chunying and Li, Shuli and Gao, Tianwen and He, Kaiqiao and Chen, Jianru and Zhang, Junpeng and Zhang, Junran},
  doi          = {10.1007/s10489-025-06839-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing vitiligo stage diagnosis through a reliable multimodal model with uncertainty calibration},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning techniques for point cloud tasks: A review. <em>APIN</em>, <em>55</em>(15), 1-52. (<a href='https://doi.org/10.1007/s10489-025-06854-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a significant means of representing 3D scenes, point clouds are extensively utilized in various fields Such as computer vision, autonomous driving, robotic interaction, and urban modeling. While deep learning has achieved remarkable Success in the realm of two-dimensional images, and its application to three-dimensional point clouds is also progressively gaining traction. However, the irregular and unstructured nature of point cloud data presents numerous challenges when applying deep learning algorithms to these 3D representations. To foster future research endeavors, this paper concentrates on three fundamental tasks associated with point clouds: classification, object detection, and semantic segmentation. It systematically reviews the current state of development regarding deep learning algorithms pertinent to these tasks. By organizing and analyzing existing literature alongside experimental results derived from publicly available datasets, this paper compares the strengths of different methodologies while also highlighting their limitations. Ultimately, it summarizes the technical challenges encountered in advancing deep learning algorithms for point clouds and outlines potential avenues for progress within this domain.},
  archive      = {J_APIN},
  author       = {Song, Xiaona and Zhang, Haozhe and Wang, Lijun and Niu, Jinxing and Zhu, Ying and Nian, Junjie and Cheng, Ruixue},
  doi          = {10.1007/s10489-025-06854-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-52},
  shortjournal = {Appl. Intell.},
  title        = {Deep learning techniques for point cloud tasks: A review},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing adversarial robustness in power quality classification systems: An attention-based defense framework. <em>APIN</em>, <em>55</em>(15), 1-27. (<a href='https://doi.org/10.1007/s10489-025-06865-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power quality monitoring is essential for ensuring the reliability, stability, and security of modern electrical networks. While deep learning models have demonstrated exceptional performance in classifying power quality disturbances, they remain critically vulnerable to adversarial perturbations—posing significant risks to smart grid cybersecurity. This paper introduces three novel contributions to the field of power quality cybersecurity: (1) Signal-Agnostic Adversarial (SAA) attacks—a perturbation method tailored specifically for power quality signals; (2) an attention-based convolutional neural network (CNN) architecture that consistently achieves 5–7% points higher robustness under attack compared to conventional models; and (3) comprehensive vulnerability fingerprinting, which exposes architecture-specific adversarial attack patterns and provides insights into structural weaknesses. We conduct a systematic analysis of CNN-based power quality classification models subjected to adversarial manipulations and propose effective defense strategies. Three attack methodologies are introduced and evaluated: the Fast Gradient Sign Method (FGSM), Signal-Specific Adversarial (SSA) attacks, and the proposed SAA attacks. Experimental results reveal catastrophic degradation in model performance, with accuracy reductions of up to 80–90% points under attack. To mitigate these vulnerabilities, our attention-based CNN model demonstrates significantly improved resilience, and adversarial training further enhances robustness—achieving up to 58.47% accuracy against SSA, the most potent attack vector. The findings underscore critical security implications of deep learning in power systems and offer practical mitigation strategies for enhancing robustness in real-world smart grid deployments.},
  archive      = {J_APIN},
  author       = {Alanazi, Mubarak and Alkhaldi, Nasser S.},
  doi          = {10.1007/s10489-025-06865-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing adversarial robustness in power quality classification systems: An attention-based defense framework},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual residual aggregation network for visual-language prompt tuning. <em>APIN</em>, <em>55</em>(15), 1-19. (<a href='https://doi.org/10.1007/s10489-025-06866-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning leverages a series of learnable prompts to effectively guide pre-trained visual language models (VLMs) to adapt to various downstream tasks. VLMs encode deep features from both visual and textual branches and learn the joint embedding space of the two modalities by optimizing the contrast loss. However, existing prompt tuning methods face two critical challenges: (1) One challenge is the forgetting of generalized knowledge. As features propagate through the visual encoder, generalizable knowledge captured in shallow layers is gradually lost, ultimately impairing the generalization ability of the joint embedding space for new classes. (2) The other challenge is that models trained on the base class suffer from semantic bias. To address these issues, we propose Visual Residual Aggregation Network for Visual-Language Prompt Tuning (VraPT). VraPT comprises two sequentially connected components: a residual aggregation module and a semantic consistency module. Firstly, in order to solve the problem of generalized knowledge forgetting, the residual aggregation module enables adaptive fusion of generalized features, which effectively preserves generalized knowledge. It also reveals the importance of shallow features in enhancing the generalization capability of text prompts. The fused representation is then fed into the semantic consistency module which is used to address the problem of semantic bias. By minimizing the divergence from the true semantic distribution, this module enhances the semantic representations in the visual space as well as the semantic coherence of the learnable prompts. Our method enables the learned prompts to retain both discriminative semantic information and generalized knowledge. Extensive experiments show that our proposed VraPT is an effective prompt tuning method, especially in recognizing new classes with great improvement. On average, VraPT improves the accuracy on base classes by 1.06% and on new classes by 2.63% across 11 datasets, along with a 1.91% gain in the harmonic mean (H) metric.},
  archive      = {J_APIN},
  author       = {Yu, Yunqian and Guo, Feng and Tian, Xianlong and Chen, Biao and Jing, Mengmeng and Zuo, Lin},
  doi          = {10.1007/s10489-025-06866-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Visual residual aggregation network for visual-language prompt tuning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection method for improving shape perception of small object defects on metal surfaces. <em>APIN</em>, <em>55</em>(15), 1-15. (<a href='https://doi.org/10.1007/s10489-025-06873-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defects on metal surfaces often exhibit complexity with diverse shapes, small sizes, and irregular patterns, leading to frequent missed and false detections during inspection and posing significant challenges to automated detection systems. Existing advanced object detectors, when applied directly to small defect detection on metal surfaces, fail to achieve satisfactory results. To mitigate these issues, we proposed a detection method to enhance the shape perception of small object defects on metal surfaces, namely MetalYOLO. Firstly, a novel location-aware attention mechanism is designed to integrate deformable convolutions to form a new feature selection module to enhance the focus on key defect features, optimizes the generation of offsets, and improve the model’s ability to adapt to complex shape objects. Secondly, the standard up-sampling module is replaced with a dynamic sampling module to dynamically adjust the sampling pattern of the input feature distribution to improve computational efficiency and retain complex or small-scale object features, thereby improving detection accuracy. Finally, a new detail-enhanced detection head is designed to further improve the network’s ability to capture fine-grained details by introducing a detail-enhanced attention-sharing module so as to utilize contextual information to selectively suppress irrelevant features, thereby reducing information redundancy. The proposed model is compared with baseline models on the ILS-MB and NEU-DET datasets. and the experimental results show significant improvements in false detection and missed detection rates with only a slight loss in inference speed. Meanwhile, the mAP reached 80.4% and 79.0%, respectively, which is 1.7% and 3.2% higher than the baseline algorithm.},
  archive      = {J_APIN},
  author       = {Zhu, Xingfei and Montagne, Christophe and Wang, Qimeng and Hu, Lingxiang and Yu, Jinghu and Tabia, Hedi and Hu, Qianqian},
  doi          = {10.1007/s10489-025-06873-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Detection method for improving shape perception of small object defects on metal surfaces},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing act: Engagement detection in online learning through master-assistant models with an enhanced hierarchical attention mechanism. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06893-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of online learning calls for the establishment of effective approaches to monitor and boost student engagement, which constitutes a key element influencing learning outcomes. The class imbalances within engagement datasets pose substantial challenges to precise detection and classification. Existing methods for detecting student engagement in online learning adopt weighted loss to address the issue of class imbalance in public datasets. However, due to the challenge of selecting appropriate weights and the risk of overfitting, the effectiveness of this approach often relies on extensive experiments for manual adjustments. To tackle this problem, we propose a Master-Assistant model to address the performance degradation caused by class imbalance to ensure effective detection of student engagement. The Assistant model is designed for coarse-grained classification according to different assistant strategies to assist the Master model for fine-grained classification. Furthermore, we extract multiple engagement-related handcrafted features and assigned different weights via an enhanced hierarchical attention mechanism. Finally, an accuracy of 70.69% and an F1-score of 68% are achieved on the Dataset for Affective States in E-Environments (DAiSEE), setting new state-of-the-art (SOTA) scores. Additionally, experiments on three other imbalanced datasets also validate the robustness of the Master-Assistant model in solving the class imbalance problem.},
  archive      = {J_APIN},
  author       = {Han, Tingting and Liu, Ruqian and Dou, Shuwei and Wang, Wei and Ding, Xiaoming and Zhang, Wenxia and Lang, Jihao and Li, Wenxuan and Han, Jixing},
  doi          = {10.1007/s10489-025-06893-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Balancing act: Engagement detection in online learning through master-assistant models with an enhanced hierarchical attention mechanism},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransMambaCC: Integrating transformer and pyramid mamba network for RGB-T crowd counting. <em>APIN</em>, <em>55</em>(15), 1-16. (<a href='https://doi.org/10.1007/s10489-025-06912-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T crowd counting is a challenging task that integrates RGB and thermal images to address the limitations of RGB-only approaches in scenes with poor illumination or occlusion. While transformer-based models have shown remarkable success in terms of capturing long-range dependencies, their high computational demands limit their practical applicability. To address this issue, a novel hybrid model named TransMambaCC, which fuses the analytical strength of transformer with the computational efficiency of Mamba, is proposed. This integration not only improves crowd analysis performance, but also significantly reduces computational overhead of the model. Additionally, a Pyramid Mamba module is innovatively designed to address the head-scale variations observed in congested scenes. Extensive experiments conducted on the RGBT-CC dataset demonstrate the superiority of TransMambaCC over the existing approaches in terms of both accuracy and efficiency. Furthermore, the model exhibits strong generalization capabilities, as evidenced by its performance on the ShanghaiTechRGBD dataset. The code is available at https://github.com/yjchen3250/TransMambaCC .},
  archive      = {J_APIN},
  author       = {Chen, Yangjian and Zhao, Huailin and Huang, Liangjun and Yang, Yubo and Kang, Wencan and Zhang, Jianwei},
  doi          = {10.1007/s10489-025-06912-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {TransMambaCC: Integrating transformer and pyramid mamba network for RGB-T crowd counting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning promotion policies with attention-based deep Q-networks. <em>APIN</em>, <em>55</em>(15), 1-15. (<a href='https://doi.org/10.1007/s10489-025-06914-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In financial services, personalized promotion strategies are critical for sustaining customer engagement and driving asset growth. We present FAT-DQN, a deep reinforcement learning framework for off-line environments that models sequential decision-making as a Markov Decision Process (MDP), where promotional actions influence future changes in customer assets under management (AUM). FAT-DQN extends the standard Deep Q-Network (DQN) architecture with a multi-head self-attention mechanism over promotion–reward histories augmented by learnable temporal encodings, and applies Feature-wise Linear Modulation (FiLM) to incorporate customer-segment embeddings. To improve robustness, we employ per-customer reward normalization and evaluate policies with both ranking-based metrics and counterfactual off-policy estimators. Empirical results on real promotion logs show that FAT-DQN consistently outperforms baseline methods, yielding a higher mean NDCG@3 (0.7744) compared to Batch-Constrained deep Q-learning (BCQ, 0.7325) and DQN (0.6852). It further improves alignment between predicted and realized outcomes, achieving a Spearman correlation of 0.2584, compared to 0.1619 for BCQ and 0.1522 for DQN. Counterfactual evaluations further show that FAT-DQN delivers consistently strong off-policy estimates, confirming its robustness across evaluation settings. These findings demonstrate that attention-based architectures with modulation offer a more effective and interpretable alternative to standard reinforcement learning approaches for personalized promotion planning in financial services.},
  archive      = {J_APIN},
  author       = {Xu, Yingnan and Wu, Xuchun and Li, Zhenjun and Liu, Congli and Zhang, Yansheng},
  doi          = {10.1007/s10489-025-06914-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Learning promotion policies with attention-based deep Q-networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal prompt learning with selective feature fusion: Towards robust cross-modal alignment. <em>APIN</em>, <em>55</em>(15), 1-28. (<a href='https://doi.org/10.1007/s10489-025-06919-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision–language models (VLMs) have shown impressive transferability but still struggle with robustness and generalization when applied to downstream tasks with limited supervision. To address these challenges, we propose a Selective Feature Fusion (SFF) framework that adaptively suppresses noisy visual regions and reinforces task-relevant cross-modal cues through lightweight, learnable gating. Our approach integrates text-guided visual masking and image-aware textual calibration into a unified pipeline, enabling more discriminative and semantically aligned multimodal representations. Comprehensive evaluations across nine widely used benchmarks demonstrate that our method consistently surpasses strong prompt-learning baselines under both few-shot and base-to-novel generalization settings. In particular, under the 8-shot scenario, our approach achieves the best overall accuracy, maintaining a clear margin over representative methods such as CoCoOp and MaPLe. These results highlight not only the robustness of our design but also its effectiveness in capturing cross-modal semantics under data-limited conditions. Further analyses, including ablation studies and qualitative visualizations, confirm that the proposed gating and calibration modules are complementary and play indispensable roles in improving performance. Taken together, this work provides a simple yet powerful strategy for enhancing the adaptability and generalization of VLMs in real-world scenarios.},
  archive      = {J_APIN},
  author       = {Han, Jiabao and Wang, Yahui and Zhong, Wei and Zhang, Ying and Yuan, Xichao},
  doi          = {10.1007/s10489-025-06919-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-28},
  shortjournal = {Appl. Intell.},
  title        = {Multimodal prompt learning with selective feature fusion: Towards robust cross-modal alignment},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
