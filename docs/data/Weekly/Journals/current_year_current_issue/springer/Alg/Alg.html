<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Alg</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alg">Alg - 5</h2>
<ul>
<li><details>
<summary>
(2025). On the parameterized complexity of eulerian strong component arc deletion. <em>Alg</em>, <em>87</em>(11), 1669-1709. (<a href='https://doi.org/10.1007/s00453-025-01336-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the Eulerian Strong Component Arc Deletion problem, where the input is a directed multigraph and the goal is to delete the minimum number of arcs to ensure every strongly connected component of the resulting digraph is Eulerian. This problem is a natural extension of the Directed Feedback Arc Set problem and is also known to be motivated by certain scenarios arising in the study of housing markets. The complexity of the problem, when parameterized by solution size (i.e., size of the deletion set), has remained unresolved and has been highlighted in several papers. In this work, we answer this question by ruling out (subject to the usual complexity assumptions) a fixed-parameter algorithm (FPT algorithm) for this parameter and conduct a broad analysis of the problem with respect to other natural parameterizations. We prove both positive and negative results. Among these, we demonstrate that the problem is also hard (W[1]-hard or even para-NP-hard) when parameterized by either treewidth or maximum degree alone. Complementing our lower bounds, we establish that the problem is in XP when parameterized by treewidth and FPT when parameterized either by both treewidth and maximum degree or by both treewidth and solution size. We show that on simple digraphs, these algorithms have near-optimal asymptotic dependence on the treewidth assuming the Exponential Time Hypothesis.},
  archive      = {J_Alg},
  author       = {Blažej, Václav and Jana, Satyabrata and Ramanujan, M. S. and Strulo, Peter},
  doi          = {10.1007/s00453-025-01336-6},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1669-1709},
  shortjournal = {Algorithmica},
  title        = {On the parameterized complexity of eulerian strong component arc deletion},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ShockHash: Near optimal-space minimal perfect hashing beyond brute-force. <em>Alg</em>, <em>87</em>(11), 1620-1668. (<a href='https://doi.org/10.1007/s00453-025-01321-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A minimal perfect hash function (MPHF) maps a set S of n keys to the first n integers without collisions. There is a lower bound of $$n\log _2e-\mathcal {O}(\log n) \approx 1.44n$$ bits needed to represent an MPHF. This can be reached by a brute-force algorithm that tries $$e^n$$ hash function seeds in expectation and stores the first seed that leads to an MPHF. The most space-efficient previous algorithms for constructing MPHFs all use such a brute-force approach as a basic building block. In this paper, we introduce ShockHash – Small, heavily overloaded cuckoo hash tables for minimal perfect hashing. ShockHash uses two hash functions $$h_0$$ and $$h_1$$ , hoping for the existence of a function $$f : S \rightarrow \{0,1\}$$ such that $$x \mapsto h_{f(x)}(x)$$ is an MPHF on S. It then uses a 1-bit retrieval data structure to store f using $$n + o(n)$$ bits. In graph terminology, ShockHash generates n-edge random graphs until stumbling on a pseudoforest – where each component contains as many edges as nodes. Using cuckoo hashing, ShockHash then derives an MPHF from the pseudoforest in linear time. We show that ShockHash needs to try only about $$(e/2)^n \approx 1.359^n$$ seeds in expectation. This reduces the space for storing the seed by roughly n bits (maintaining the asymptotically optimal space consumption) and speeds up construction by almost a factor of $$2^n$$ compared to brute-force. Bipartite ShockHash reduces the expected construction time again to about $$1.166^n$$ by maintaining a pool of candidate hash functions and checking all possible pairs. Using ShockHash as a building block within the RecSplit framework we obtain ShockHash-RS, which can be constructed up to 3 orders of magnitude faster than competing approaches. ShockHash-RS can build an MPHF for 10 million keys with 1.489 bits per key in about half an hour. When instead using ShockHash after an efficient k-perfect hash function, it achieves space usage similar to the best competitors, while being significantly faster to construct and query.},
  archive      = {J_Alg},
  author       = {Lehmann, Hans-Peter and Sanders, Peter and Walzer, Stefan},
  doi          = {10.1007/s00453-025-01321-z},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1620-1668},
  shortjournal = {Algorithmica},
  title        = {ShockHash: Near optimal-space minimal perfect hashing beyond brute-force},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving tight $$O(4^k)$$ runtime bounds on jumpk by proving that genetic algorithms evolve near-maximal population diversity. <em>Alg</em>, <em>87</em>(11), 1564-1619. (<a href='https://doi.org/10.1007/s00453-025-01323-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $$\textsc {Jump} _k$$ benchmark was the first problem for which crossover was proven to give a speed-up over mutation-only evolutionary algorithms. Jansen and Wegener (Algorithmica 2002) proved an upper bound of $$O(\textrm{poly}(n) + 4^k/p_c)$$ for the ( $$\mu $$ +1) Genetic Algorithm (( $$\mu $$ +1) GA), but only for unrealistically small crossover probabilities $$p_c$$ . To this date, it remains an open problem to prove similar upper bounds for realistic $$p_c$$ ; the best known runtime bound, in terms of function evaluations, for $$p_c = \Omega (1)$$ is $$O((n/\chi )^{k-1})$$ , $$\chi $$ a positive constant. We provide a novel approach and analyse the evolution of the population diversity, measured as sum of pairwise Hamming distances, for a variant of the ( $$\mu $$ +1) GA on $$\textsc {Jump} _k$$ . The ( $$\mu $$ +1)- $${\lambda _c}$$ -GA creates one offspring in each generation either by applying mutation to one parent or by applying crossover $${\lambda _c}$$ times to the same two parents (followed by mutation), to amplify the probability of creating an accepted offspring in generations with crossover. We show that population diversity in the ( $$\mu $$ +1)- $${\lambda _c}$$ -GA converges to an equilibrium of near-perfect diversity. This yields an improved time bound of $$O(\mu n \log (\mu ) + 4^k)$$ function evaluations for a range of k under the mild assumptions $$p_c = O(1/k)$$ and $$\mu \in \Omega (kn)$$ . For all constant k, the restriction is satisfied for some $$p_c = \Omega (1)$$ and it implies that the expected runtime for all constant k and an appropriate $$\mu = \Theta (kn)$$ is bounded by $$O(n^2 \log n)$$ , irrespective of k. For larger k, the expected time of the ( $$\mu $$ +1)- $${\lambda _c}$$ -GA is $$\Theta (4^k)$$ , which is tight for a large class of unbiased black-box algorithms and faster than the original ( $$\mu $$ +1) GA by a factor of $$\Omega (1/p_c)$$ . We also show that our analysis can be extended to other unitation functions such as $$\textsc {Jump} _{k, \delta }$$ and Hurdle.},
  archive      = {J_Alg},
  author       = {Opris, Andre and Lengler, Johannes and Sudholt, Dirk},
  doi          = {10.1007/s00453-025-01323-x},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1564-1619},
  shortjournal = {Algorithmica},
  title        = {Achieving tight $$O(4^k)$$ runtime bounds on jumpk by proving that genetic algorithms evolve near-maximal population diversity},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smoothed analysis of the 2-opt heuristic for the TSP under gaussian noise. <em>Alg</em>, <em>87</em>(11), 1518-1563. (<a href='https://doi.org/10.1007/s00453-025-01335-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2-opt heuristic is a very simple local search heuristic for the traveling salesperson problem. In practice it usually converges quickly to solutions within a few percentages of optimality. In contrast to this, its running-time is exponential and its approximation performance is poor in the worst case. Englert, Röglin, and Vöcking (Algorithmica, 2014) provided a smoothed analysis in the so-called one-step model in order to explain the performance of 2-opt on d-dimensional Euclidean instances, both in terms of running-time and in terms of approximation ratio. However, translating their results to the classical model of smoothed analysis, where points are perturbed by Gaussian distributions with standard deviation $$\sigma $$ , yields only weak bounds. We prove bounds that are polynomial in n and $$1/\sigma $$ for the smoothed running-time with Gaussian perturbations. In addition, our analysis for Euclidean distances is much simpler than the existing smoothed analysis. Furthermore, we prove a smoothed approximation ratio of $$O(\log (1/\sigma ))$$ . This bound is almost tight, as we also provide a lower bound of $$\Omega (\frac{\log n}{\log \log n})$$ for $$\sigma = O(1/\sqrt{n})$$ . Our main technical novelty here is that, different from existing smoothed analyses, we do not separately analyze objective values of the global and local optimum on all inputs (which only allows for a bound of $$O(1/\sigma )$$ ), but simultaneously bound them on the same input.},
  archive      = {J_Alg},
  author       = {Künnemann, Marvin and Manthey, Bodo and Veenstra, Rianne},
  doi          = {10.1007/s00453-025-01335-7},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1518-1563},
  shortjournal = {Algorithmica},
  title        = {Smoothed analysis of the 2-opt heuristic for the TSP under gaussian noise},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting double coverage for k-server via imperfect predictions. <em>Alg</em>, <em>87</em>(11), 1477-1517. (<a href='https://doi.org/10.1007/s00453-025-01333-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the online k-server problem in a learning-augmented setting. While in the traditional online model, an algorithm has no information about the request sequence, we assume that there is given some advice (for example, machine-learned predictions) on an algorithm’s decision. There is, however, no guarantee on the quality of the prediction, and it might be far from being correct. Our main result is a learning-augmented variation of the well-known Double Coverage algorithm for k-server on the line (Chrobak et al. in SIAM J Discret Math 4(2):172–181, 1991) in which we integrate predictions as well as our trust into their quality. We give an error-dependent worst-case performance guarantee, which is a function of a user-defined confidence parameter, and which interpolates smoothly between an optimal performance in case that all predictions are correct, and the best-possible performance regardless of the prediction quality. When given good predictions, we improve upon known lower bounds for online algorithms without advice. We further show that our algorithm achieves for any k almost optimal guarantees, within a class of deterministic learning-augmented algorithms respecting local and memoryless properties. Our algorithm outperforms a previously proposed (more general) learning-augmented algorithm. It is noteworthy that the previous algorithm crucially exploits memory, whereas our algorithm is memoryless. Finally, we demonstrate in experiments the practicability and the superior performance of our algorithm on real-world data.},
  archive      = {J_Alg},
  author       = {Lindermayr, Alexander and Megow, Nicole and Simon, Bertrand},
  doi          = {10.1007/s00453-025-01333-9},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1477-1517},
  shortjournal = {Algorithmica},
  title        = {Boosting double coverage for k-server via imperfect predictions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
