<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JMUI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jmui">JMUI - 4</h2>
<ul>
<li><details>
<summary>
(2025). How bare-hand clicking influences eye movement behavior in virtual button selection tasks: A comparative analysis with keyboard control. <em>JMUI</em>, <em>19</em>(3), 285-304. (<a href='https://doi.org/10.1007/s12193-025-00455-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The flexibility of virtual reality systems has fostered the development of various interaction techniques, yet our understanding of users’ eye movement behaviors with different interaction methods remains limited. This study compares eye movement details in goal-driven and stimulus-based virtual button selection tasks, using both direct bare-hand interaction and indirect keyboard control in virtual environments. Our findings show that both interface type and interaction mode significantly affect multiple eye movement metrics, including saccades, fixations, and blinks. These insights enhance our understanding of visual attention and action execution in virtual selection tasks, offering data-driven conclusions that can serve as a foundation for developing design guidelines for interaction techniques and interface designs in virtual reality. We believe these results will contribute to the creation of more natural, efficient, and user-friendly virtual reality systems, further advancing the technology.},
  archive      = {J_JMUI},
  author       = {Du, Xiaoxi and Jia, Lesong and Wu, Jinchun and Peng, Ningyue and Zhou, Xiaozhou and Xue, Chengqi},
  doi          = {10.1007/s12193-025-00455-2},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {285-304},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {How bare-hand clicking influences eye movement behavior in virtual button selection tasks: A comparative analysis with keyboard control},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Web-based multimodal learning system to develop social communication skills. <em>JMUI</em>, <em>19</em>(3), 271-284. (<a href='https://doi.org/10.1007/s12193-025-00460-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual agents offer a scalable and cost-effective alternative to traditional human-led social skills training, which is often limited by the availability of professional trainers. Our web-based learning system, developed following Bellack et al.’s training model, integrates speech recognition, response selection, speech synthesis, and nonverbal behavior generation to provide automated training. To evaluate its effectiveness, we conducted a four-week study with 60 Japanese participants from the general population, focusing on four key social skills. Participants completed questionnaires assessing autistic traits, social anxiety, and changes in social communication skills post-training. Results demonstrated significant improvements, with notable results in participants’ confidence in declining requests. These findings highlight the potential of web-based virtual agents for enhancing social communication skills and suggest promising applications for social communication research and intervention programs.},
  archive      = {J_JMUI},
  author       = {Tanaka, Hiroki and Lisitsyna, Alexandra},
  doi          = {10.1007/s12193-025-00460-5},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {271-284},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Web-based multimodal learning system to develop social communication skills},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring user interactions with commercial machines via real-world application logs in the lab. <em>JMUI</em>, <em>19</em>(3), 253-269. (<a href='https://doi.org/10.1007/s12193-025-00456-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log analysis is an effective user experience testing method for exploring subtle user interactions with commercial machines. The rapid iteration of commercial machines and their applications calls for a more cost-effective alternative to traditional in-the-wild approaches. However, lab-based log studies typically rely on customized testing programs, which do not represent real-world usage. Therefore, this study investigates the feasibility of log analysis using real-world applications in lab settings to explore user interaction. We present a case study exploring smartphone gesture interaction in more than 50 everyday operations across 10 real-world applications. We identified and analyzed detailed gesture parameters from log data on three common gesture sets, including tapping (single and double), zooming (pinching and spreading), and swiping (horizontal and vertical). Grounding in related works, we discuss observed behavior patterns, their potential causes, and implications for future design in commercial machine interfaces. We highlight the applicability of the presented approach by discussing its advantages, direction for improvements, and potential applications. We call for more studies to explore the norms and values of this approach.},
  archive      = {J_JMUI},
  author       = {Song, Fangli and Wang, Wei and Zhou, Dasen and Bryan-Kinns, Nick and Zhang, Jun and Chen, Qi and Du, Le},
  doi          = {10.1007/s12193-025-00456-1},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {253-269},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Exploring user interactions with commercial machines via real-world application logs in the lab},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of music tempo reflecting group activity on multi-participant exercise. <em>JMUI</em>, <em>19</em>(3), 235-251. (<a href='https://doi.org/10.1007/s12193-025-00457-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During training, it is common to perform certain movements repeatedly within a short period—such as squats, jumping jacks, and burpees—where the frequency and rhythm of the exercises significantly impact their effectiveness. In this study, we focused on group training sessions involving repetitive exercises—a frequent scenario in gyms, home workouts, and remote training environments. Specifically, we investigated the potential of using music tempo to enhance group training effectiveness, diverging from traditional methods that primarily rely on visual cues for guidance and synchronization. We conducted a user study in an in-person group exercise setting with 18 participants organized into six groups of three. They performed squats under conditions with and without music aligned to their average tempo. We then examined how adding group-synchronized musical feedback—on top of natural interactions such as observing each other’s movements and listening to footsteps—affected training outcomes. By analyzing skeletal data and questionnaire responses, we found that music tempo contributed to improvements in both movement synchronization and aspects of the overall training experience. We also gathered suggestions for future enhancements in music-based support, including applying the proposed method to groups with smaller physical ability differences and considering differences in users’ musical backgrounds.},
  archive      = {J_JMUI},
  author       = {Wang, Ruiyun and Jin, Yuchen and Huang, Jiayun and Takahashi, Shin},
  doi          = {10.1007/s12193-025-00457-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {235-251},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Effects of music tempo reflecting group activity on multi-participant exercise},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
