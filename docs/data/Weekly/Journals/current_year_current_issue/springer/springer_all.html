<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>springer</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aamas">AAMAS - 11</h2>
<ul>
<li><details>
<summary>
(2025). The cost and complexity of minimizing envy in house allocation. <em>AAMAS</em>, <em>39</em>(2), 1-54. (<a href='https://doi.org/10.1007/s10458-025-09710-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study almost envy-freeness in house allocation, where m houses are to be allocated among n agents so that every agent receives exactly one house. An envy-free allocation need not exist, and therefore we may have to settle for relaxations. We study different aggregate measures of envy as markers of fairness. In particular, we define the amount of envy experienced by an agent a w.r.t. an allocation to be the number of agents that agent a envies under that allocation. We quantify the envy generated by an allocation using three different metrics: 1) the number of agents who are envious; 2) the maximum amount of envy experienced by any agent; and 3) the total amount of envy experienced by all agents, and look for allocations that minimize one of the three metrics. We prove a host of algorithmic and hardness results. We also suggest practical approaches for these problems via integer linear program (ILP) formulations and report the findings of our experimental evaluation of ILPs. Finally, we study the price of fairness, which quantifies the loss of welfare we must suffer due to the fairness requirements, and present tight bounds as well as algorithms that simultaneously optimize both welfare and fairness.},
  archive      = {J_AAMAS},
  author       = {Madathil, Jayakrishnan and Misra, Neeldhara and Sethia, Aditi},
  doi          = {10.1007/s10458-025-09710-y},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-54},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {The cost and complexity of minimizing envy in house allocation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioral QLTL. <em>AAMAS</em>, <em>39</em>(2), 1-29. (<a href='https://doi.org/10.1007/s10458-025-09712-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Behavioral QLTL, a “behavioral” variant of Linear Temporal Logic (ltl) with second-order quantifiers. Behavioral qltl is characterized by the fact that the functions that assign the truth value of the quantified propositions along the trace can only depend on the past. In other words, such functions must be “processes” (Abadi et al., Realizable and Unrealizable Specifications of Reactive Systems, 1989) . This gives the logic a strategic flavor that we usually associate with planning. Indeed we show that temporally extended planning in nondeterministic domains and ltl synthesis are expressed in Behavioral qltl through formulas with a simple quantification alternation. As such alternation increases, we get to forms of planning/synthesis in which contingent and conformant planning aspects get mixed. We study this logic from the computational point of view and compare it to the original qltl (with non-behavioral semantics) and simpler forms of behavioral semantics.},
  archive      = {J_AAMAS},
  author       = {De Giacomo, Giuseppe and Perelli, Giuseppe},
  doi          = {10.1007/s10458-025-09712-w},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Behavioral QLTL},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-stage generalized deferred acceptance mechanism: Strategyproof mechanism for handling general hereditary constraints. <em>AAMAS</em>, <em>39</em>(2), 1-25. (<a href='https://doi.org/10.1007/s10458-025-09713-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The theory of two-sided matching has been extensively developed and applied to many real-life application domains. As the theory has been applied to increasingly diverse types of environments, researchers and practitioners have encountered various forms of distributional constraints. Arguably, the most general class of distributional constraints would be hereditary constraints; if a matching is feasible, then any matching that assigns weakly fewer students at each college is also feasible. However, under general hereditary constraints, it is shown that no strategyproof mechanism exists that simultaneously satisfies fairness and weak nonwastefulness, which is an efficiency (students’ welfare) requirement weaker than nonwastefulness. We propose a new strategyproof mechanism that works for hereditary constraints called the Multi-Stage Generalized Deferred Acceptance mechanism (MS-GDA). It uses the Generalized Deferred Acceptance mechanism (GDA) as a subroutine, which works when distributional constraints belong to a well-behaved class called hereditary M $$^{\natural }$$ -convex set. We show that GDA satisfies several desirable properties, most of which are also preserved in MS-GDA. We experimentally show that MS-GDA strikes a good balance between fairness and efficiency (students’ welfare) compared to existing strategyproof mechanisms when distributional constraints are close to an M $$^{\natural }$$ -convex set*.},
  archive      = {J_AAMAS},
  author       = {Kimura, Kei and Liu, Kweiguu and Sun, Zhaohong and Yahiro, Kentaro and Yokoo, Makoto},
  doi          = {10.1007/s10458-025-09713-9},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Multi-stage generalized deferred acceptance mechanism: Strategyproof mechanism for handling general hereditary constraints},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hedonic seat arrangement problems. <em>AAMAS</em>, <em>39</em>(2), 1-31. (<a href='https://doi.org/10.1007/s10458-025-09711-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a variant of hedonic games, called Seat Arrangement. The model is defined by a bijection from agents with preferences for each other to vertices in a graph G. The utility of an agent depends on the neighbors assigned in the graph. More precisely, it is the sum over all neighbors of the preferences that the agent has towards the agent assigned to the neighbor. We first consider the price of stability and fairness for different classes of preferences. In particular, we show that there is an instance such that the price of fairness (PoF) is unbounded in general. Moreover, we show an upper bound $$\tilde{d}(G)$$ and an almost tight lower bound $$\tilde{d}(G)-1/4$$ of PoF, where $$\tilde{d}(G)$$ is the average degree of an input graph. Then we investigate the computational complexity of problems to find certain “good” seat arrangements, say Utilitarian Arrangement, Egalitarian Arrangement, Stable Arrangement, and Envy-free Arrangement. We give dichotomies of computational complexity of four Seat Arrangement problems from the perspective of the maximum order of connected components in an input graph. For the parameterized complexity, Utilitarian Arrangement can be solved in time $$n^{O(\gamma )}$$ , while it cannot be solved in time $$f(\gamma )n^{o(\gamma )}$$ under ETH, where n is the number of agents and $$\gamma$$ is the vertex cover number of an input graph. Moreover, we show that Egalitarian Arrangement and Envy-free Arrangement are weakly NP-hard even on graphs of bounded vertex cover number. Finally, we prove that determining whether a stable arrangement can be obtained from a given arrangement by k swaps is W[1]-hard when parameterized by $$k+\gamma$$ , whereas it can be solved in time $$n^{O(k)}$$ .},
  archive      = {J_AAMAS},
  author       = {Bodlaender, Hans L. and Hanaka, Tesshu and Jaffke, Lars and Ono, Hirotaka and Otachi, Yota and van der Zanden, Tom C.},
  doi          = {10.1007/s10458-025-09711-x},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Hedonic seat arrangement problems},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity-seeking jump games in networks. <em>AAMAS</em>, <em>39</em>(2), 1-37. (<a href='https://doi.org/10.1007/s10458-025-09714-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, strategic games inspired by Schelling’s influential model of residential segregation have been studied in the TCS and AI literature. In these games, agents of k different types occupy the nodes of a network topology aiming to maximize their utility, which is a function of the fraction of same-type agents they are adjacent to in the network. As such, the agents exhibit similarity-seeking strategic behavior. In this paper, we introduce a class of strategic jump games in which the agents are diversity-seeking: The utility of an agent is defined as the fraction of its neighbors that are of different type than itself. We show that in general it is computationally hard to determine the existence of an equilibrium in such games. However, when the network is a tree, diversity-seeking jump games always admit an equilibrium assignment. For regular graphs and spider graphs with a single empty node, we prove a stronger result: The game is potential, that is, the improving response dynamics always converge to an equilibrium from any initial placement of the agents. We also show (nearly tight) bounds on the price of anarchy and price of stability in terms of the social welfare (the total utility of the agents).},
  archive      = {J_AAMAS},
  author       = {Narayanan, Lata and Sabbagh, Yasaman and Voudouris, Alexandros A.},
  doi          = {10.1007/s10458-025-09714-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-37},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Diversity-seeking jump games in networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinating monetary contributions in participatory budgeting. <em>AAMAS</em>, <em>39</em>(2), 1-30. (<a href='https://doi.org/10.1007/s10458-025-09715-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formalize a framework for coordinating funding and selecting projects, the costs of which are shared among agents with quasi-linear utility functions and individual budgets. Our model contains the discrete participatory budgeting model as a special case, while capturing other useful scenarios. We propose several important axioms and objectives and study how well they can be simultaneously satisfied. We show that whereas welfare maximization admits an FPTAS, welfare maximization subject to a natural and very weak participation requirement leads to a strong inapproximability. This result is bypassed if we consider some natural restricted valuations, namely laminar single-minded valuations and symmetric valuations. Our analysis for the former restriction leads to the discovery of a new class of tractable instances for the Set Union Knapsack problem, a classical problem in combinatorial optimization.},
  archive      = {J_AAMAS},
  author       = {Aziz, Haris and Gujar, Sujit and Padala, Manisha and Suzuki, Mashbat and Vollen, Jeremy},
  doi          = {10.1007/s10458-025-09715-7},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Coordinating monetary contributions in participatory budgeting},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). POSGGym: A library for decision-theoretic planning and learning in partially observable, multi-agent environments. <em>AAMAS</em>, <em>39</em>(2), 1-45. (<a href='https://doi.org/10.1007/s10458-025-09716-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seamless integration of Planning Under Uncertainty and Reinforcement Learning (RL) promises to bring the best of both model-driven and data-driven worlds to multi-agent decision-making, resulting in an approach with assurances on performance that scales well to more complex problems. Despite this potential, progress in developing such methods has been hindered by the lack of adequate evaluation and simulation platforms. Researchers have had to rely on creating custom environments, which reduces efficiency and makes comparing new methods difficult. In this paper, we introduce POSGGym : a library for facilitating planning and RL research in partially observable, multi-agent domains. It provides a diverse collection of discrete and continuous environments, complete with their dynamics models and a reference set of policies that can be used to evaluate generalization to novel co-players. Leveraging POSGGym, we empirically investigate existing state-of-the-art planning methods and a method that combines planning and RL in the type-based reasoning setting. Our experiments corroborate that combining planning and RL can yield superior performance compared to planning or RL alone, given the model of the environment and other agents is correct. However, our particular setup also reveals that this integrated approach could result in worse performance when the model of other agents is incorrect. Our findings indicate the benefit of integrating planning and RL in partially observable, multi-agent domains, while serving to highlight several important directions for future research. Code available at: https://github.com/RDLLab/posggym .},
  archive      = {J_AAMAS},
  author       = {Schwartz, Jonathon and Newbury, Rhys and Kulić, Dana and Kurniawati, Hanna},
  doi          = {10.1007/s10458-025-09716-6},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-45},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {POSGGym: A library for decision-theoretic planning and learning in partially observable, multi-agent environments},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing node selection in search based multi-agent path finding. <em>AAMAS</em>, <em>39</em>(2), 1-24. (<a href='https://doi.org/10.1007/s10458-025-09719-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multi-Agent Path Finding (MAPF) problem involves the task of finding paths for multiple agents that want to reach their destinations without obstructing other agents. Although MAPF is essential for numerous real-world applications, finding an optimal solution to this problem is NP-hard. Many approaches have been proposed in the literature, offering sub-optimal solutions to improve runtime efficiency. Lazy Constraints Addition search for MAPF (LaCAM) is a state-of-the-art sub-optimal MAPF algorithm that employs tree-based lazy successor generation to minimize planning effort. However, the success of the algorithm heavily relies on the effective selection of nodes for expansion. LaCAM employs a fixed heuristic throughout the entire search process, disregarding the agents’ preferences or characteristics of the underlying environment. Nevertheless, experiments with various heuristics indicate that no single heuristic consistently outperforms others across all scenarios. Consequently, in diverse environments, as the number of agents increases, reliance on a single, general heuristic leads to diminished runtime performance. Against this backdrop, with the intent to further speed up the runtime, we propose a novel approach, called eLaCAM, that adaptively selects nodes during the search process considering the current scenario of the environment and agents preferences. We introduce two distinct variants of eLaCAM. The first, eLaCAM-stat, statistically analyses previous results of using different heuristics and selects nodes accordingly. The second variant, eLaCAM-ML, analyze the environment by extracting necessary features to guide a machine learning framework in assisting adaptive node selection during the search process. Our extensive empirical results illustrate a notable improvement in runtime and a reduction in the search space compared to state-of-the-art MAPF algorithms.},
  archive      = {J_AAMAS},
  author       = {Alam, Md. Ahasanul and Mahmud, Shekhar and Mamun-or-Rashid, Md. and Khan, Md. Mosaddek},
  doi          = {10.1007/s10458-025-09719-3},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Optimizing node selection in search based multi-agent path finding},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information elicitation mechanisms for bayesian auctions. <em>AAMAS</em>, <em>39</em>(2), 1-45. (<a href='https://doi.org/10.1007/s10458-025-09718-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we design information elicitation mechanisms for Bayesian auctions. While in Bayesian mechanism design the distributions of the players’ private types are often assumed to be common knowledge, information elicitation considers the situation where the players know the distributions better than the decision maker. To weaken the information assumption in Bayesian auctions, we consider an information structure where the knowledge about the distributions is arbitrarily scattered among the players. In such an unstructured information setting, we design mechanisms for unit-demand auctions and additive auctions that aggregate the players’ knowledge, generating revenue that are constant approximations to the optimal Bayesian mechanisms with a common prior. Our mechanisms are 2-step dominant-strategy truthful and the approximation ratios improve gracefully with the amount of knowledge the players collectively have.},
  archive      = {J_AAMAS},
  author       = {Chen, Jing and Li, Bo and Li, Yingkai},
  doi          = {10.1007/s10458-025-09718-4},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-45},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Information elicitation mechanisms for bayesian auctions},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing policies for transition-independent multiagent systems that are robust to communication loss. <em>AAMAS</em>, <em>39</em>(2), 1-49. (<a href='https://doi.org/10.1007/s10458-025-09721-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a cooperative multiagent system, a collection of agents executes a joint policy in order to achieve some common objective. The successful deployment of such systems hinges on the availability of reliable inter-agent communication. However, many sources of potential disruption to communication exist in practice, such as radio interference, hardware failure, and adversarial attacks. In this work, we develop joint policies for cooperative multiagent systems that are robust to potential losses in communication. More specifically, we develop joint policies for cooperative Markov games with independent transitions and joint reach-avoid objectives. First, we propose an algorithm for the decentralized execution of joint policies during periods of communication loss. This algorithm is designed to work under arbitrary communication partitions between the agents. Next, we use the total correlation of the state-action process induced by a joint policy as a measure of the intrinsic dependencies between the agents. We then use this measure to lower-bound the performance of a joint policy under randomly intermittent or adversarial communication loss scenarios. We show the existence of a multiagent decision-making environment in which this bound is tight—the highest performance under intermittent communication loss, for any policy execution mechanism, is of the same order as the bound. We then present an algorithm that maximizes a proxy to this lower bound in order to synthesize minimum-dependency joint policies that remain performant under communication loss. Through two-agent and three-agent numerical experiments, we show that the proposed minimum-dependency policies require minimal coordination between the agents while incurring little to no loss in performance; the total correlation value of the synthesized policy is significantly lower than the total correlation value of the baseline policy which does not take potential communication losses into account. As a result, the performance of the minimum-dependency policies remains consistently high regardless of whether or not communication is available. By contrast, the performance of the baseline policy decreases drastically when communication is lost.},
  archive      = {J_AAMAS},
  author       = {Karabag, Mustafa O. and Neary, Cyrus and Topcu, Ufuk},
  doi          = {10.1007/s10458-025-09721-9},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-49},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Designing policies for transition-independent multiagent systems that are robust to communication loss},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the graph theory of majority illusions: Theoretical results and computational experiments. <em>AAMAS</em>, <em>39</em>(2), 1-55. (<a href='https://doi.org/10.1007/s10458-025-09720-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of an opinion in one’s direct circles is not necessarily a good indicator of its popularity in one’s entire community. Network structures make local information about global properties of the group potentially inaccurate, and the way a social network is wired constrains what kind of information distortion can actually occur. In this paper, we discuss which classes of networks allow for a large enough proportion of the population to get a wrong enough impression about the overall distribution of opinions. We start by focusing on the ‘majority illusion’, the case where one sees a majority opinion in one’s direct circles that differs from the global majority. We show that no network structure can guarantee that most agents see the correct majority. We then perform computational experiments to study the likelihood of majority illusions in different classes of networks. Finally, we generalize to other types of illusions.},
  archive      = {J_AAMAS},
  author       = {Venema-Los, Maaike and Christoff, Zoé and Grossi, Davide},
  doi          = {10.1007/s10458-025-09720-w},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {12},
  number       = {2},
  pages        = {1-55},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {On the graph theory of majority illusions: Theoretical results and computational experiments},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ail">AIL - 11</h2>
<ul>
<li><details>
<summary>
(2025). Correction to: Code is law: How COMPAS affects the way the judiciary handles the risk of recidivism. <em>AIL</em>, <em>33</em>(3), 873-874. (<a href='https://doi.org/10.1007/s10506-024-09400-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIL},
  author       = {Engel, Christopher and Linhardt, Lorenz and Schubert, Marcel},
  doi          = {10.1007/s10506-024-09400-2},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {873-874},
  shortjournal = {Artif. Intell. Law},
  title        = {Correction to: Code is law: How COMPAS affects the way the judiciary handles the risk of recidivism},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The digital transformation of jurisprudence: An evaluation of ChatGPT-4’s applicability to solve cases in business law. <em>AIL</em>, <em>33</em>(3), 847-871. (<a href='https://doi.org/10.1007/s10506-024-09406-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving landscape of legal information systems, ChatGPT-4 and other advanced conversational agents (CAs) offer the potential to disruptively transform the law industry. This study evaluates commercially available CAs within the German legal context, thereby assessing the generalizability of previous U.S.-based findings. Employing a unique corpus of 200 distinct legal tasks, ChatGPT-4 was benchmarked against Google Bard, Google Gemini, and its predecessor, ChatGPT-3.5. Human-expert and automated assessments of 4000 CA-generated responses reveal ChatGPT-4 to be the first CA to surpass the threshold of solving realistic legal tasks and passing a German business law exam. While ChatGPT-4 outperforms ChatGPT-3.5, Google Bard, and Google Gemini in both consistency and quality, the results demonstrate a considerable degree of variability, especially in complex cases with no predefined response options. Based on these findings, legal professionals should manually verify all texts produced by CAs before use. Novices must exercise caution with CA-generated legal advice, given the expertise needed for its assessment.},
  archive      = {J_AIL},
  author       = {Schweitzer, Sascha and Conrads, Markus},
  doi          = {10.1007/s10506-024-09406-w},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {847-871},
  shortjournal = {Artif. Intell. Law},
  title        = {The digital transformation of jurisprudence: An evaluation of ChatGPT-4’s applicability to solve cases in business law},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intermediate factors and precedential constraint. <em>AIL</em>, <em>33</em>(3), 827-846. (<a href='https://doi.org/10.1007/s10506-024-09405-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the extension of formal accounts of precedential constraint to make use of a factor hierarchy with intermediate factors. A problem arises, however, because constraints expressed in terms of intermediate factors may give different outcomes from those expressed only using base level factors. We argue that constraints that use only base level factors yield the correct outcomes, but that intermediate factors play an important role in the justification and explanation of those outcomes. The discussion is illustrated with a running example.},
  archive      = {J_AIL},
  author       = {Bench-Capon, Trevor},
  doi          = {10.1007/s10506-024-09405-x},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {827-846},
  shortjournal = {Artif. Intell. Law},
  title        = {Intermediate factors and precedential constraint},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-training improves few-shot learning in legal artificial intelligence tasks. <em>AIL</em>, <em>33</em>(3), 809-825. (<a href='https://doi.org/10.1007/s10506-024-09403-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the labeling costs in legal artificial intelligence tasks are expensive. Therefore, it becomes a challenge to utilize low cost to train a robust model. In this paper, we propose a LAIAugment approach, which aims to enhance the few-shot learning capability in legal artificial intelligence tasks. Specifically, we first use the self-training approach to label the amount of unlabelled data to enhance the feature learning capability of the model. Moreover, we also search for datasets that are similar to the training set by improving the text similarity function. We conducted experimental analyses for three legal artificial intelligence tasks, including evidence extraction, legal element extraction, and case multi-label prediction, which composed of 3500 judgement documents. The experimental results show that the proposed LAIAugment method has an average F1-score of 72.3% on the three legal AI tasks, which is 1.93% higher than the baseline model. At the same time, it shows a huge improvement in few-shot learning.},
  archive      = {J_AIL},
  author       = {Zhou, Yulin and Qin, Yongbin and Huang, Ruizhang and Chen, Yanping and Lin, Chuan and Zhou, Yuan},
  doi          = {10.1007/s10506-024-09403-z},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {809-825},
  shortjournal = {Artif. Intell. Law},
  title        = {Self-training improves few-shot learning in legal artificial intelligence tasks},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Japanese tort-case dataset for rationale-supported legal judgment prediction. <em>AIL</em>, <em>33</em>(3), 783-807. (<a href='https://doi.org/10.1007/s10506-024-09402-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the first dataset for Japanese Legal Judgment Prediction (LJP), the Japanese Tort-case Dataset (JTD), which features two tasks: tort prediction and its rationale extraction. The rationale extraction task identifies the court’s accepting arguments from alleged arguments by plaintiffs and defendants, which is a novel task in the field. JTD is constructed based on annotated 3477 Japanese Civil Code judgments by 41 legal experts, resulting in 7978 instances with 59,697 of their alleged arguments from the involved parties. Our baseline experiments show the feasibility of the proposed two tasks, and our error analysis by legal experts identifies sources of errors and suggests future directions of the LJP research.},
  archive      = {J_AIL},
  author       = {Yamada, Hiroaki and Tokunaga, Takenobu and Ohara, Ryutaro and Tokutsu, Akira and Takeshita, Keisuke and Sumida, Mihoko},
  doi          = {10.1007/s10506-024-09402-0},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {783-807},
  shortjournal = {Artif. Intell. Law},
  title        = {Japanese tort-case dataset for rationale-supported legal judgment prediction},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InstructPatentGPT: Training patent language models to follow instructions with human feedback. <em>AIL</em>, <em>33</em>(3), 739-782. (<a href='https://doi.org/10.1007/s10506-024-09401-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, patent prosecution is conceptualized as a system of reinforcement learning from human feedback. The objective of the system is to increase the likelihood for a language model to generate patent claims that have a higher chance of being granted. To showcase the controllability of the language model, the system learns from granted patents and pre-grant applications with different rewards. The status of “granted” and “pre-grant” are perceived as labeled human feedback implicitly. In addition, specific to patent drafting, the experiments in this research demonstrate the model’s capability to learn from adjusting claim length and inclusion of limiting terms for narrowing claim scope. As proof of concept, the experiments focus on claim ones only and the training data originates from a patent dataset tailored specifically for artificial intelligence. Although the available human feedback in patent prosecution are limited and the quality of generated patent text requires improvement, the experiments following the 3-stage reinforcement learning from human feedback have demonstrated that generative language models are capable of reflecting the human feedback or intent in patent prosecution. To enhance the usability of language models, the implementation in this research utilizes modern techniques that enable execution on a single consumer-grade GPU. The demonstrated proof of concept, which reduces hardware requirements, will prove valuable in the future as more human feedback in patent prosecution become available for broader use, either within patent offices or in the public domain.},
  archive      = {J_AIL},
  author       = {Lee, Jieh-Sheng},
  doi          = {10.1007/s10506-024-09401-1},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {739-782},
  shortjournal = {Artif. Intell. Law},
  title        = {InstructPatentGPT: Training patent language models to follow instructions with human feedback},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models in cryptocurrency securities cases: Can a GPT model meaningfully assist lawyers?. <em>AIL</em>, <em>33</em>(3), 691-737. (<a href='https://doi.org/10.1007/s10506-024-09399-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) could be a useful tool for lawyers. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying GPT-3.5’s legal reasoning and ChatGPT’s legal drafting capabilities. We examine whether a) GPT-3.5 can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to ChatGPT. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by ChatGPT and lawyers. GPT-3.5’s legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely missed additional, correct violations). ChatGPT performed better at legal drafting, and jurors’ decisions were not statistically significantly associated with the author of the document upon which they based their decisions. Because GPT-3.5 cannot satisfactorily conduct legal reasoning tasks, it would be unlikely to be able to help lawyers in a meaningful way at this stage. However, ChatGPT’s drafting skills (though, perhaps, still inferior to lawyers) could assist lawyers in providing legal services. Our research is the first to systematically study an LLM’s legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.},
  archive      = {J_AIL},
  author       = {Trozze, Arianna and Davies, Toby and Kleinberg, Bennett},
  doi          = {10.1007/s10506-024-09399-6},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {691-737},
  shortjournal = {Artif. Intell. Law},
  title        = {Large language models in cryptocurrency securities cases: Can a GPT model meaningfully assist lawyers?},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unfair clause detection in terms of service across multiple languages. <em>AIL</em>, <em>33</em>(3), 641-689. (<a href='https://doi.org/10.1007/s10506-024-09398-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing natural language processing systems for legal texts are developed for the English language. Nevertheless, there are several application domains where multiple versions of the same documents are provided in different languages, especially inside the European Union. One notable example is given by Terms of Service (ToS). In this paper, we compare different approaches to the task of detecting potential unfair clauses in ToS across multiple languages. In particular, after developing an annotated corpus and a machine learning classifier for English, we consider and compare several strategies to extend the system to other languages: building a novel corpus and training a novel machine learning system for each language, from scratch; projecting annotations across documents in different languages, to avoid the creation of novel corpora; translating training documents while keeping the original annotations; translating queries at prediction time and relying on the English system only. An extended experimental evaluation conducted on a large, original dataset indicates that the time-consuming task of re-building a novel annotated corpus for each language can often be avoided with no significant degradation in terms of performance.},
  archive      = {J_AIL},
  author       = {Galassi, Andrea and Lagioia, Francesca and Jabłonowska, Agnieszka and Lippi, Marco},
  doi          = {10.1007/s10506-024-09398-7},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {641-689},
  shortjournal = {Artif. Intell. Law},
  title        = {Unfair clause detection in terms of service across multiple languages},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting court judgment prediction and explanation using legal entities. <em>AIL</em>, <em>33</em>(3), 605-640. (<a href='https://doi.org/10.1007/s10506-024-09397-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic prediction of court case judgments using Deep Learning and Natural Language Processing is challenged by the variety of norms and regulations, the inherent complexity of the forensic language, and the length of legal judgments. Although state-of-the-art transformer-based architectures and Large Language Models (LLMs) are pre-trained on large-scale datasets, the underlying model reasoning is not transparent to the legal expert. This paper jointly addresses court judgment prediction and explanation by not only predicting the judgment but also providing legal experts with sentence-based explanations. To boost the performance of both tasks we leverage a legal named entity recognition step, which automatically annotates documents with meaningful domain-specific entity tags and masks the corresponding fine-grained descriptions. In such a way, transformer-based architectures and Large Language Models can attend to in-domain entity-related information in the inference process while neglecting irrelevant details. Furthermore, the explainer can boost the relevance of entity-enriched sentences while limiting the diffusion of potentially sensitive information. We also explore the use of in-context learning and lightweight fine-tuning to tailor LLMs to the legal language style and the downstream prediction and explanation tasks. The results obtained on a benchmark dataset from the Indian judicial system show the superior performance of entity-aware approaches to both judgment prediction and explanation.},
  archive      = {J_AIL},
  author       = {Benedetto, Irene and Koudounas, Alkis and Vaiani, Lorenzo and Pastor, Eliana and Cagliero, Luca and Tarasconi, Francesco and Baralis, Elena},
  doi          = {10.1007/s10506-024-09397-8},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {605-640},
  shortjournal = {Artif. Intell. Law},
  title        = {Boosting court judgment prediction and explanation using legal entities},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Re-evaluating GPT-4’s bar exam performance. <em>AIL</em>, <em>33</em>(3), 581-604. (<a href='https://doi.org/10.1007/s10506-024-09396-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perhaps the most widely touted of GPT-4’s at-launch, zero-shot capabilities has been its reported 90th-percentile performance on the Uniform Bar Exam. This paper begins by investigating the methodological challenges in documenting and verifying the 90th-percentile claim, presenting four sets of findings that indicate that OpenAI’s estimates of GPT-4’s UBE percentile are overinflated. First, although GPT-4’s UBE score nears the 90th percentile when examining approximate conversions from February administrations of the Illinois Bar Exam, these estimates are heavily skewed towards repeat test-takers who failed the July administration and score significantly lower than the general test-taking population. Second, data from a recent July administration of the same exam suggests GPT-4’s overall UBE percentile was below the 69th percentile, and $$\sim$$ 48th percentile on essays. Third, examining official NCBE data and using several conservative statistical assumptions, GPT-4’s performance against first-time test takers is estimated to be $$\sim$$ 62nd percentile, including $$\sim$$ 42nd percentile on essays. Fourth, when examining only those who passed the exam (i.e. licensed or license-pending attorneys), GPT-4’s performance is estimated to drop to $$\sim$$ 48th percentile overall, and $$\sim$$ 15th percentile on essays. In addition to investigating the validity of the percentile claim, the paper also investigates the validity of GPT-4’s reported scaled UBE score of 298. The paper successfully replicates the MBE score, but highlights several methodological issues in the grading of the MPT + MEE components of the exam, which call into question the validity of the reported essay score. Finally, the paper investigates the effect of different hyperparameter combinations on GPT-4’s MBE performance, finding no significant effect of adjusting temperature settings, and a significant effect of few-shot chain-of-thought prompting over basic zero-shot prompting. Taken together, these findings carry timely insights for the desirability and feasibility of outsourcing legally relevant tasks to AI models, as well as for the importance for AI developers to implement rigorous and transparent capabilities evaluations to help secure safe and trustworthy AI.},
  archive      = {J_AIL},
  author       = {Martínez, Eric},
  doi          = {10.1007/s10506-024-09396-9},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {581-604},
  shortjournal = {Artif. Intell. Law},
  title        = {Re-evaluating GPT-4’s bar exam performance},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring explainable AI in the tax domain. <em>AIL</em>, <em>33</em>(3), 551-579. (<a href='https://doi.org/10.1007/s10506-024-09395-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyses whether current explainable AI (XAI) techniques can help to address taxpayer concerns about the use of AI in taxation. As tax authorities around the world increase their use of AI-based techniques, taxpayers are increasingly at a loss about whether and how the ensuing decisions follow the procedures required by law and respect their substantive rights. The use of XAI has been proposed as a response to this issue, but it is still an open question whether current XAI techniques are enough to meet existing legal requirements. The paper approaches this question in the context of a case study: a prototype tax fraud detector trained on an anonymized dataset of real-world cases handled by the Buenos Aires (Argentina) tax authority. The decisions produced by this detector are explained through the use of various classification methods, and the outputs of these explanation models are evaluated on their explanatory power and on their compliance with the legal obligation that tax authorities provide the rationale behind their decision-making. We conclude the paper by suggesting technical and legal approaches for designing explanation mechanisms that meet the needs of legal explanation in the tax domain.},
  archive      = {J_AIL},
  author       = {Górski, Łukasz and Kuźniacki, Błażej and Almada, Marco and Tyliński, Kamil and Calvo, Madalena and Asnaghi, Pablo Matias and Almada, Luciano and Iñiguez, Hilario and Rubianes, Fernando and Pera, Octavio and Nigrelli, Juan Ignacio},
  doi          = {10.1007/s10506-024-09395-w},
  journal      = {Artificial Intelligence and Law},
  month        = {9},
  number       = {3},
  pages        = {551-579},
  shortjournal = {Artif. Intell. Law},
  title        = {Exploring explainable AI in the tax domain},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="air">AIR - 32</h2>
<ul>
<li><details>
<summary>
(2025). Oriented object detection in optical remote sensing images using deep learning: A survey. <em>AIR</em>, <em>58</em>(11), 1-61. (<a href='https://doi.org/10.1007/s10462-025-11256-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented object detection is a fundamental yet challenging task in remote sensing (RS), aiming to locate and classify objects with arbitrary orientations. Recent advancements in deep learning have significantly enhanced the capabilities of oriented object detection methods. Given the rapid development of this field, a comprehensive survey of the recent advances in oriented object detection is presented in this paper. Specifically, we begin by tracing the technical evolution from horizontal object detection to oriented object detection and highlighting the specific related challenges, including feature misalignment, spatial misalignment, oriented bounding box (OBB) regression problems, and common issues encountered in RS. Subsequently, we further categorize the existing methods into detection frameworks, OBB regression techniques, feature representation approaches, and solutions to common issues and provide an in-depth discussion of how these methods address the above challenges. In addition, we cover several publicly available datasets and evaluation protocols. Furthermore, we provide a comprehensive comparison and analysis involving the state-of-the-art methods. Toward the end of this paper, we identify several future directions for oriented object detection research.},
  archive      = {J_AIR},
  author       = {Wang, Kun and Wang, Zi and Li, Zhang and Su, Ang and Teng, Xichao and Pan, Erting and Liu, Minhao and Yu, Qifeng},
  doi          = {10.1007/s10462-025-11256-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Oriented object detection in optical remote sensing images using deep learning: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-mediated healthcare and trust. a trust-construct and trust-factor framework for empirical research. <em>AIR</em>, <em>58</em>(11), 1-17. (<a href='https://doi.org/10.1007/s10462-025-11306-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application of Artificial Intelligence (AI) in healthcare is growing exponentially, and its use is expected to continue expanding in the coming years. However, lack of trust in AI systems remains a significant barrier to their widespread adoption. This article analyzes the problem of trust, its various features and its application in AI-mediated healthcare. We first review the literature on trust and trust in technology to detect which theoretical constructs are essential to trust. We then identify the factors that we consider fundamental for a rich and complex comprehension of trust in AI-mediated healthcare. We finally propose a trust-factor framework that could be used for empirical research on AI-mediated healthcare and its practical implementation.},
  archive      = {J_AIR},
  author       = {Alonso, Marcos and Astobiza, Aníbal M. and Ortega Lozano, Ramón},
  doi          = {10.1007/s10462-025-11306-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-17},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-mediated healthcare and trust. a trust-construct and trust-factor framework for empirical research},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in multimodal differential evolution: A comprehensive review and future perspectives. <em>AIR</em>, <em>58</em>(11), 1-73. (<a href='https://doi.org/10.1007/s10462-025-11314-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal optimization involves identifying multiple global and local optima of a function, offering valuable insights into diverse optimal solutions within the search space. Evolutionary algorithms (EAs) excel at finding various solutions in a single run, providing a distinct advantage over classical optimization techniques that often require multiple restarts without guarantee of obtaining diverse solutions. Among these EAs, differential evolution (DE) stands out as a powerful and versatile optimizer for continuous parameter spaces. DE has shown significant success in multi-modal optimization by utilizing its population-based search to promote the formation of multiple stable subpopulations, each targeting different optima. Recent advancements in DE for multi-modal optimization have focused on niching methods, parameter adaptation, hybridization with other algorithms, including machine learning, and applications across various domains. Given these developments, it is an opportune moment to present a critical review of the latest literature and identify key future research directions. This paper offers a comprehensive overview of recent DE advancements in multimodal optimization, including methods for handling multiple optima, hybridization with EAs, and machine learning, and highlights a range of real-world applications. Additionally, the paper outlines a set of compelling open problems and future research issues from multiple perspectives.},
  archive      = {J_AIR},
  author       = {Chauhan, Dikshit and Shivani and Jung, Donghwi and Yadav, Anupam},
  doi          = {10.1007/s10462-025-11314-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-73},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advancements in multimodal differential evolution: A comprehensive review and future perspectives},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ai-enabled framework for anomaly detection in power distribution networks under false data injection attacks. <em>AIR</em>, <em>58</em>(11), 1-38. (<a href='https://doi.org/10.1007/s10462-025-11318-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern power distribution networks are becoming cyber physical systems due to the addition of more advanced metering infrastructure (AMI). This has introduced new vulnerabilities to cyber threats, particularly false data injection (FDI) attacks. These attacks compromise the integrity of power consumption data, leading to financial losses, operational inefficiencies, and grid instability. Rule-based techniques and traditional machine learning models are two examples of traditional anomaly detection methods that often have problems. Often, these methods generate an excessive number of false alarms, struggle to adapt to new attack patterns, and perform poorly in large-scale deployments. This research suggests a robust anomaly identification framework (AIF) that uses an autoencoder (AE) for feature transformation and a multi-layer perceptron (MLP) to identify anomalies in AMI integrated with smart grids. The proposed approach first applies synthetic features extraction inspired by real-world smart meter capabilities and transforms the dataset using a denoising AE. MLP assisted in the classification to detect multiple FDI attack types with improved accuracy and reliability. Numerous experiments have been performed, and the results indicate that the suggested method works better than popular methods like correlation analysis, techniques based on clustering, and standard outlier identification algorithms. Compared to baseline methods, the proposed technique improves detection accuracy by up to approximately 25%, reduces false positives, and enhances the system’s ability to generalize across different cyberattack strategies. The proposed work computes seven different types of criterion matrices to verify the effectiveness of finding anomalies. The overall average results include mean squared error (0.0793), accuracy (92%), F1-Score (92%), recall (91%), specificity (94%), area under the curve (97%), and mean average precision (96%). These findings accentuate the potential of the proposed AIF performance in fortifying smart grid cybersecurity.},
  archive      = {J_AIR},
  author       = {Ahmad, Hasnain and Mustafa, Ghulam and Gulzar, Muhammad Majid and Ahmed, Ijaz and Khalid, Muhammad},
  doi          = {10.1007/s10462-025-11318-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Ai-enabled framework for anomaly detection in power distribution networks under false data injection attacks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence for secure and sustainable industrial control systems - A survey of challenges and solutions. <em>AIR</em>, <em>58</em>(11), 1-86. (<a href='https://doi.org/10.1007/s10462-025-11320-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industrial environments, the security and sustainability of Industrial Control Systems (ICS) have become crucial. This comprehensive review examines the transformative potential of Artificial Intelligence (AI) in ICS, focusing on technologies like Machine Learning (ML), Deep Learning (DL), Large Language Models (LLMs), and cloud computing. Moreover, this research explores integrating existing and proposed sustainable practices within the ICS framework, with a particular emphasis on energy efficiency and carbon footprint reduction, to enhance the overall sustainability of ICS. This review employed a systematic approach to select relevant articles from multiple reputable databases, such as Scopus, IEEE Explore, Science Direct, ACM digital library, Web of Science, and IET digital library, including 250 articles that provide valuable insights into the intersection of AI, security, and sustainability in ICS. This review examines vulnerabilities in ICS, such as data breaches, insider threats, and malware, emphasizing the need for effective anomaly detection. It highlights how AI technologies like anomaly detection and predictive analytics can enhance threat detection and response in ICS by improving accuracy and efficiency. The review offers insights to researchers and professionals on the future of secure, sustainable ICS, supporting a resilient industrial landscape that meets cybersecurity, compliance, and sustainability goals.},
  archive      = {J_AIR},
  author       = {Aslam, Muhammad Muzamil and Tufail, Ali and Gul, Haji and Irshad, Muhammad Nauman and Namoun, Abdallah},
  doi          = {10.1007/s10462-025-11320-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-86},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence for secure and sustainable industrial control systems - A survey of challenges and solutions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Don’t push the button! exploring data leakage risks in machine learning and transfer learning. <em>AIR</em>, <em>58</em>(11), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11326-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, there is growing evidence in the literature that ML approaches are not always used appropriately, leading to incorrect and sometimes overly optimistic results. One reason for this inappropriate use of ML may be the increasing availability of machine learning tools, leading to what we call the “push the button” approach. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. In particular, this paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Indeed, crucial steps in ML pipeline can be inadvertently overlooked, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussing how certain conditions can propagate through the ML approach workflow. Furthermore, it explores the connection between data leakage and the specific task being addressed, investigates its occurrence in Transfer Learning framework, and compares standard inductive ML with transductive ML paradigms. The conclusion summarizes key findings, emphasizing the importance of addressing data leakage for robust and reliable ML applications considering tasks and generalization goals.},
  archive      = {J_AIR},
  author       = {Apicella, Andrea and Isgrò, Francesco and Prevete, Roberto},
  doi          = {10.1007/s10462-025-11326-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Don’t push the button! exploring data leakage risks in machine learning and transfer learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault-tolerant control strategies for industrial robots: State of the art and future perspective on AI-based fault management. <em>AIR</em>, <em>58</em>(11), 1-33. (<a href='https://doi.org/10.1007/s10462-025-11327-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault-tolerant control schemes are essential for affirming the safe and dependable operation of industrial robots. In this detailed review, we discuss the current developments in fault-tolerant control strategies for industrial robots. The main focus is given to highlight some major contributions in fault-tolerant control systems used in robotic manipulators with single or multiple joints, incorporating any linear or non-linear robust approach for industrial robots, design, and implementation. The paper also discusses adaptive fault-tolerant control of robots with sensor and/or actuator faults and unknown parameters, and fault-tolerant cooperative control of multiple robot teams for collaborative tasks. The present work provides a comprehensive overview of the recent advancements in fault-tolerant control strategies for industrial robots using both classical nonlinear methods as well as intelligent approaches using AI and machine learning, which will be useful for researchers and engineers working in this field.},
  archive      = {J_AIR},
  author       = {Khan, Zeashan and Nasir, Ali and Mekid, Samir},
  doi          = {10.1007/s10462-025-11327-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Fault-tolerant control strategies for industrial robots: State of the art and future perspective on AI-based fault management},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inclusive prompt engineering for large language models: A modular framework for ethical, structured, and adaptive AI. <em>AIR</em>, <em>58</em>(11), 1-51. (<a href='https://doi.org/10.1007/s10462-025-11330-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models have achieved impressive results across various tasks but remain limited in their ability to adapt ethically and structurally across diverse domains without retraining. This paper presents the Inclusive Prompt Engineering Model (IPEM), a modular framework designed to enhance LLM performance, adaptability, and ethical alignment through prompt-level strategies alone. IPEM integrates four components: Memory-of-Thought for multi-turn consistency, Enhanced Chain-of-Thought prompting for logical verification, Structured and Analogical Reasoning modules for tabular and cross-domain tasks, and Evaluation and Feedback Loops that incorporate uncertainty-aware selection and bias mitigation mechanisms. Evaluated across tasks in arithmetic reasoning, healthcare triage, financial forecasting, and inclusive question answering, IPEM consistently improves model outputs over a GPT-4 baseline. Notable outcomes include up to twenty percentage points in accuracy gains, a 25 percent reduction in logical errors, and nearly 20 percent reduction in social bias scores, all without modifying model weights. Moreover, IPEM reduces annotation demands by one-third while preserving performance, demonstrating its utility in low-resource environments. By unifying ethical safeguards and reasoning mechanisms in a prompt-based system, IPEM offers a reproducible and auditable pathway for deploying adaptable and fair AI systems. The framework contributes both practical solutions and theoretical insights to the evolving field of prompt engineering.},
  archive      = {J_AIR},
  author       = {Torkestani, Mohamad Saleh and Alameer, Ali and Palaiahnakote, Shivakumara and Manosuri, Taha},
  doi          = {10.1007/s10462-025-11330-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Inclusive prompt engineering for large language models: A modular framework for ethical, structured, and adaptive AI},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video diffusion generation: Comprehensive review and open problems. <em>AIR</em>, <em>58</em>(11), 1-55. (<a href='https://doi.org/10.1007/s10462-025-11331-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video generation has become an increasingly important component of AI-generated content (AIGC), owing to its rich semantic expressiveness and growing application potential. Among various generative paradigms, diffusion models have recently gained prominence due to their strong controllability, competitive visual quality, and compatibility with multimodal inputs. However, most existing surveys provide limited coverage of diffusion-based video generation, often lacking systematic analysis and comprehensive comparisons. To address this gap, this paper presents a thorough and structured review of diffusion models for video generation. We first outline the theoretical foundations and core architectures of diffusion models, and then the key design principles of representative methods for video generation were introduced. We propose a unified taxonomy that categorizes over two hundred methods, analyzing their key characteristics, strengths, and limitations. In addition, we compared the performance of classical methods and summarized commonly used datasets and evaluation metrics in this field for ease of model benchmarking and selection. Finally, we discuss open problems and future research directions, aiming to provide a valuable reference for both academic research and practical development.},
  archive      = {J_AIR},
  author       = {Ma, Wenping and Yang, Xiaoting and Jiao, Licheng and Li, Lingling and Liu, Xu and Liu, Fang and Chen, Puhua and Yang, Yuting and Ma, Mengru and Sun, Long and Zhang, Ruohan and Geng, Xueli and Guo, Yuwei and Yang, Shuyuan and Feng, Zhixi},
  doi          = {10.1007/s10462-025-11331-6},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Video diffusion generation: Comprehensive review and open problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy and security in recommenders: An analytical review. <em>AIR</em>, <em>58</em>(11), 1-41. (<a href='https://doi.org/10.1007/s10462-025-11333-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RSs) effectively curb information overload by providing personalized suggestions of items to users across different online domains. Their widespread use in e-commerce enhances user engagement, personalizes shopping experiences, and drives sales growth. However, despite the effectiveness of these systems at generating recommendations for users, they still raise major privacy and security concerns as their data could be exploited for malicious purposes, which can lead to data breaches and misuse. Therefore, this paper presents a comprehensive and systematic review of the underlying causes of privacy and security challenges in RS. It also provides a detailed taxonomy categorizing these concerns based on their targets and the risks they create. It further presents potential solutions that have been used in the literature while identifying challenges and possible research directions to pursue in a bid to address privacy and security concerns in RSs. This paper will be a useful resource for current and upcoming researchers in the domain of RSs. It will support knowledge advancement and steer appropriate research directions.},
  archive      = {J_AIR},
  author       = {Ojokoh, Bolanle Adefowoke and Isinkaye, Folasade Olubusola and Zhang, Ming and Tom, Joshua Joshua and Gabriel, Arome Junior and Afolabi, Olaitan and Afolabi, Bamidele},
  doi          = {10.1007/s10462-025-11333-4},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Privacy and security in recommenders: An analytical review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of deep learning in tea quality monitoring: A review. <em>AIR</em>, <em>58</em>(11), 1-45. (<a href='https://doi.org/10.1007/s10462-025-11335-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tea is a popular beverage which can offer numerous benefits to human health and support the local economy. There is an increasing demand for accurate and rapid tea quality evaluation methods to ensure that the quality and safety of tea products meet the customers’ expectations. Advanced sensing technologies in combination with deep learning (DL) offer significant opportunities to enhance the efficiency and accuracy for tea quality evaluation. This review aims to summarize the application of DL technologies for tea quality assessment in three stages: cultivation, tea processing, and product evaluation. Various state-of-the-art sensing technologies (e.g., computer vision, spectroscopy, electronic nose and tongue) have been used to collect key data (images, spectral signals, aroma profiles) from tea samples. By utilizing DL models, researchers are able to analyze a wide range of tea quality attributes, including tea variety, geographical origin, quality grade, fermentation stage, adulteration level, and chemical composition. The findings from this review indicate that DL, with its end-to-end analytical capability and strong generalization performance, can serve as a powerful tool to support various sensing technologies for accurate tea quality detection. However, several challenges remain, such as limited sample availability for data training, difficulties for fusing data from multiple sources, and lack of interpretability of DL models. To this end, this review proposes potential solutions and future studies to address these issues, providing practical considerations for tea industry to effectively uptake new technologies and to support the development of the tea industry.},
  archive      = {J_AIR},
  author       = {Wu, Tao and Zhou, Lei and Zhao, Yiying and Qi, Hengnian and Pu, Yuanyuan and Zhang, Chu and Liu, Yufei},
  doi          = {10.1007/s10462-025-11335-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of deep learning in tea quality monitoring: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence (AI) and machine learning (ML) in procurement and purchasing decision-support (DS): A taxonomic literature review and research opportunities. <em>AIR</em>, <em>58</em>(11), 1-36. (<a href='https://doi.org/10.1007/s10462-025-11336-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI), machine learning (ML) and decision-support (DS) are gaining increasing interest with widening adoption. This article investigates the enabler role of AI and ML for providing decision-support in procurement&purchasing domain. The study follows a systematic review approach via taxonomic analysis. Comprehensive analysis and discussions are provided for: (a) the relevance and applicability of AI and ML in procurement&purchasing decision-support; (b) functionalities/processes for which they are utilized; (c) related methodologies; and (d) implementation benefits as well as challenges. Findings reveal that procurement&purchasing area holds significant potential in terms of AI-ML applications for decision-support almost every related sub-process. This study is original by offering a process-oriented approach to the research domain; providing unique clustering and classification; and presenting detailed analyses via unique taxonomy tables with respect to approach, topic, focus, context and methodologies of the literature items reviewed. The study offers further research opportunities and has significant potential to provide managerial insights by the identified sectoral applications, benefits and challenges.},
  archive      = {J_AIR},
  author       = {Balkan, Dursun and Akyuz, Goknur Arzu},
  doi          = {10.1007/s10462-025-11336-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence (AI) and machine learning (ML) in procurement and purchasing decision-support (DS): A taxonomic literature review and research opportunities},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic radiology report generation with deep learning: A comprehensive review of methods and advances. <em>AIR</em>, <em>58</em>(11), 1-42. (<a href='https://doi.org/10.1007/s10462-025-11337-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic report generation refers to the process of generating medical reports from medical images without the need for manual intervention, enabling faster, more consistent, and objective analysis of radiological data. The rapid progress in deep learning, particularly in the fields of computer vision and natural language processing, has significantly improved the efficacy of this approach. By leveraging deep learning techniques, which seamlessly integrate image analysis with natural language generation, these methods have shown promise in interpreting complex medical images and producing highly accurate textual descriptions. In this paper, we provide a thorough review of various deep learning models and techniques employed for generating radiological reports, with a focus on chest X-ray images as a representative case. We propose a unified encoder-decoder framework that consists of an image encoder for extracting feature representations from medical images, a language decoder for generating textual reports, and enhancement components designed to refine model performance. Through a comprehensive comparison of existing state-of-the-art methods on the widely utilized MIMIC-CXR dataset, we highlight the innovative contributions made by recent advancements in the field. Furthermore, we discuss the current challenges and identify potential research directions for future advancements in this field.},
  archive      = {J_AIR},
  author       = {Li, Yilin and Kong, Chao and Zhao, Guosheng and Zhao, Zijian},
  doi          = {10.1007/s10462-025-11337-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automatic radiology report generation with deep learning: A comprehensive review of methods and advances},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI for cyber threat intelligence: Applications, challenges, and analysis of real-world case studies. <em>AIR</em>, <em>58</em>(11), 1-134. (<a href='https://doi.org/10.1007/s10462-025-11338-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive survey of the applications, challenges, and limitations of Generative AI (GenAI) in enhancing threat intelligence within cybersecurity, supported by real-world case studies. We examine a wide range of data sources in Cyber Threat Intelligence (CTI), including security reports, blogs, social media, network traffic, malware samples, dark web data, and threat intelligence platforms (TIPs). This survey provides a full reference for integrating GenAI into CTI. We discuss various GenAI models such as Large Language Models (LLMs) and Deep Generative Models (DGMs) like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models, explaining their roles in detecting and addressing complex cyber threats. The survey highlights key applications in areas such as malware detection, network traffic analysis, phishing detection, threat actor attribution, and social engineering defense. We also explore critical challenges in deploying GenAI, including data privacy, security concerns, and the need for interpretable and transparent models. As regulations like the European Commission’s AI Act emerge, ensuring trustworthy AI solutions is becoming more crucial. Real-world case studies, such as the impact of the WannaCry ransomware, the rise of deepfakes, and AI-driven social engineering, demonstrate both the potential and current limitations of GenAI in CTI. Our goal is to provide foundational insights and strategic direction for advancing GenAI’s role in future cybersecurity frameworks, emphasizing the importance of innovation, adaptability, and ongoing learning to enhance resilience against evolving cyber threats. Ultimately, this survey offers critical insights into how GenAI can shape the future of cybersecurity by addressing key challenges and providing actionable guidance for effective implementation.},
  archive      = {J_AIR},
  author       = {Balasubramanian, Prasasthy and Liyana, Sonali and Sankaran, Hamsini and Sivaramakrishnan, Shambavi and Pusuluri, Sruthi and Pirttikangas, Susanna and Peltonen, Ella},
  doi          = {10.1007/s10462-025-11338-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-134},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Generative AI for cyber threat intelligence: Applications, challenges, and analysis of real-world case studies},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent reinforcement learning for resources allocation optimization: A survey. <em>AIR</em>, <em>58</em>(11), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11340-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL’s ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing a pivotal role in industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, design steps and benchmarks. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL’s potential to advance resource allocation solutions.},
  archive      = {J_AIR},
  author       = {Hady, Mohamad A. and Hu, Siyi and Pratama, Mahardhika and Cao, Zehong and Kowalczyk, Ryszard},
  doi          = {10.1007/s10462-025-11340-5},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-agent reinforcement learning for resources allocation optimization: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). (Ir)rationality in AI: State of the art, research challenges and open questions. <em>AIR</em>, <em>58</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11341-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of rationality is central to the field of artificial intelligence (AI). Whether we are seeking to simulate human reasoning, or trying to achieve bounded optimality, our goal is generally to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in AI, and sets out the open questions in this area. We consider how the understanding of rationality in other fields has influenced its conception within AI, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we examine irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area remains limited. Methods that have up to now been developed for other purposes, namely adversarial scenarios, may be adapted to suit interactions with artificial agents. We further discuss the interplay between human and artificial agents, and the role that rationality plays within this interaction; many questions remain in this area, relating to potentially irrational behaviour of both humans and artificial agents.},
  archive      = {J_AIR},
  author       = {Macmillan-Scott, Olivia and Musolesi, Mirco},
  doi          = {10.1007/s10462-025-11341-4},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {(Ir)rationality in AI: State of the art, research challenges and open questions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review on key technologies toward smart healthcare systems based IoT: Technical aspects, challenges and future directions. <em>AIR</em>, <em>58</em>(11), 1-122. (<a href='https://doi.org/10.1007/s10462-025-11342-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unexpected death of humans due to a lack of medical care is a serious problem. Additionally, the number of elderly people requiring continuous care is increasing. A global aging population poses a challenge to the sustainability of conventional healthcare systems for the future. Simultaneously, recent years have seen remarkable progress in the Internet of Things (IoT) and communication technologies, alongside the growing importance of artificial intelligence (AI) explainability and information fusion. Therefore, developing smart healthcare systems based on IoT and advanced technologies is crucial. This would open up new possibilities for efficient and intelligent medical systems. Hence, it is imperative to present a prospective vision of smart healthcare systems and explore the key technologies that enable the development of these intelligent medical systems. With smart healthcare systems, the future of healthcare can be significantly enhanced, providing higher-quality care, improved treatment, and more efficient patient care. This paper aims to provide a comprehensive review of the key enabling and innovative technologies for smart healthcare systems. To this end, it will cover the primary goals of each technology, the current state of research, potential applications envisioned, associated challenges, and future research directions. This paper is intended to be a valuable resource for researchers and healthcare providers. Ultimately, this paper provides valuable insights for both industry professionals and academic researchers, while also identifying potential new research avenues.},
  archive      = {J_AIR},
  author       = {Alsabah, Muntadher and Naser, Marwah Abdulrazzaq and Albahri, A. S. and Albahri, O. S. and Alamoodi, A. H. and Abdulhussain, Sadiq H. and Alzubaidi, Laith},
  doi          = {10.1007/s10462-025-11342-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-122},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review on key technologies toward smart healthcare systems based IoT: Technical aspects, challenges and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel ensemble learning-based soft measurement method for rod-pumping system efficiency. <em>AIR</em>, <em>58</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11343-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of rod-pumping system efficiency is crucial for evaluating the performance of such systems. Currently, the efficiency of rod-pumping systems is primarily estimated using mechanistic models. With the continuous advancement of information technology and the improvement of oilfield databases, some researchers have employed single neural networks for prediction. However, single neural networks often suffer from low prediction accuracy and poor robustness to noise. To solve this problem, we propose a new integrated learning-based soft measurement of the efficiency of rod pumping systems. Firstly, we proposed five soft measurement methods for rod pumping system efficiency: BiGRU-BiLSTM-CrossAttention, BiRNN-BiGRU-KAN, CNN-BiGRU-KAN, BiLSTM-BiGRU-KAN, and BiLSTM-Transformer-KAN. Then, using these five methods as base learners and FNN as the meta-learner, we constructed a novel rod pumping system efficiency soft measurement method based on the Stacking ensemble learning framework. The hyperparameters were optimized using a multi-strategy integrated Crayfish optimization algorithm, and the model was validated using 5-fold cross-validation. To verify the accuracy of the proposed soft measurement method, we applied it to 10,250 real oil wells for calculation and conducted a comparative analysis with baseline models. The results demonstrate that the proposed soft measurement method can effectively predict the efficiency of rod pumping systems.},
  archive      = {J_AIR},
  author       = {Ma, Biao and Dong, Shimin},
  doi          = {10.1007/s10462-025-11343-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel ensemble learning-based soft measurement method for rod-pumping system efficiency},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFI-ensemble: Sugeno fuzzy integral-based ensemble of CNN models with meta-heuristic fuzzy measures for mouth and oral disease detection. <em>AIR</em>, <em>58</em>(11), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11345-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising prevalence of mouth and oral diseases (MOD), including gum disease and oral cancer, presents a major global health challenge, where timely detection is crucial for effective intervention. Recognizing the limitations of a single learning model in capturing intricate information for precise disease prediction from complex data, we introduce a robust deep learning ensemble framework named Sugeno Fuzzy Integral (SFI)-Ensemble in this paper. Our methodology involves a meticulous preprocessing of the dataset utilizing fuzzy contrast enhancement (FCE) to enhance data quality and contrast. Additionally, we propose a reconstruction approach that employs transfer learning (TL) and fine-tuning on four Convolutional Neural Network (CNN) models—DenseNet121, MobileNetV1, DenseNet169, and Resnet101V2—to optimize their architectures specifically for MOD classification. The focal point of this contribution lies in the introduction of a groundbreaking ensemble method. This ensemble method dynamically combines decision scores from the CNN models using the SFI-based technique, offering a resilient and adaptive approach that factors in the confidence of base learners’ predictions through fuzzy integrals. To overcome the prevalent challenge of experimentally defining fuzzy measures in ensemble methods based on fuzzy integrals, we surpass conventional manual tuning. Our approach involves the utilization of seven distinct meta-heuristic optimization algorithms for the optimal determination of fuzzy measures. This not only ensures stability but also highlights the effectiveness of the proposed SFI-Ensemble. A comprehensive assessment is carried out on publicly accessible datasets to detect MOD, complemented by Grad-CAM interpretability and meticulous statistical analyses. Additionally, we benchmark the results against baseline models and state-of-the-art methods, with our proposed framework consistently surpassing them, attaining an impressive accuracy of 99.70%. This underscores the superior performance and robustness of our proposed methodology in contrast to traditional ensemble methods. Our approach, integrating dataset preprocessing, model reconstruction, and ensemble innovation, provides doctors with an effective tool for accurate MOD diagnosis, enhancing adaptability and performance through fuzzy integral-based fusion.},
  archive      = {J_AIR},
  author       = {Asif, Sohaib and Chen, Shasha and Ying, Yajun and Zheng, Changfu and Wang, Vicky Yang and Wang, Enyu and Xu, Dong},
  doi          = {10.1007/s10462-025-11345-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {SFI-ensemble: Sugeno fuzzy integral-based ensemble of CNN models with meta-heuristic fuzzy measures for mouth and oral disease detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for intrusion detection in emerging technologies: A comprehensive survey and new perspectives. <em>AIR</em>, <em>58</em>(11), 1-63. (<a href='https://doi.org/10.1007/s10462-025-11346-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion Detection Systems (IDS) can help cybersecurity analysts detect malicious activities in computational environments. Recently, Deep Learning (DL) methods in IDS have demonstrated notable performance, revealing new underlying cybersecurity patterns in systems’ operations. Conversely, issues such as low performance in real systems, high false positive rates, and lack of explainability hinder its real-world deployment. In addition, the adoption of many new emerging technologies, such as cloud, edge computing, and the Internet of Things (IoT) introduces new forms of vulnerabilities. Therefore, the improvement of intrusion detection in emerging technologies depends on the clear definitions of challenging security problems and the limitations of existing solutions. The main goal of this research is to conduct a literature review of DL solutions for intrusion detection in emerging technologies to understand the state-of-the-art solutions and their limitations. Specifically, we conduct a comprehensive review of IDS-based automated threat defense methods, with the objective of identifying the landscape of, and opportunities for, incorporating DL methods into IDS. To accomplish this, a thorough review of IDS methods is conducted for multiple platforms and technologies, focusing on the use of common DL techniques. To expand on the study, several widely used IDS datasets are evaluated to assess their ability to train DL models and support researchers in understanding their characteristics and limitations. The analysis of attack vectors in emerging technologies is conducted, enabling an in-depth evaluation of security solutions in the future. Our findings show many clear opportunities for future research, including addressing the gap between solutions for controlled/simulated environments versus real systems, overcoming trustworthiness issues, including lack of explainability, and further exploring operationalization issues such as deployable solutions and continuous detection. Our analysis highlights that the operationalization of DL for intrusion detection in emerging technologies represents a key challenge to be addressed in the next few years.},
  archive      = {J_AIR},
  author       = {Neto, Euclides Carlos Pinto and Iqbal, Shahrear and Buffett, Scott and Sultana, Madeena and Taylor, Adrian},
  doi          = {10.1007/s10462-025-11346-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for intrusion detection in emerging technologies: A comprehensive survey and new perspectives},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of machine learning for computer-aided diagnosis of parkinson’s disease: Progress and benchmark case study. <em>AIR</em>, <em>58</em>(11), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11347-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has emerged as a vital tool for the diagnosis of Parkinson’s Disease (PD). This study presents a comprehensive review on the applications of ML for computer-aided diagnosis (CAD) of PD. We conducted a comprehensive review by searching articles published from 2010 till 2024. The risk of bias is assessed using the PROBAST checklist. Case studies are also provided. This review includes 117 articles with six categories: neuroimaging data (20.5%); voice data (40.2%); handwriting data (12.0%); gait data (14.5%); EEG data (8.5%); and other data (4.3%). According to the PROBAST checklist, only 28 articles (23.9%) have a low risk of bias. A benchmark case study is conducted for five different data modalities. We also discuss current limitations and future directions of applying ML to the diagnosis of PD. This review reduces the gap between Artificial Intelligence (AI) and PD medical professionals and provides helpful information for future research.},
  archive      = {J_AIR},
  author       = {Zhang, Juntao and Zhang, Yiming and Weng, Ying and Hosseini, Akram A. and Wang, Boding and Dening, Tom and Fan, Weinyu and Xiao, Weizhong},
  doi          = {10.1007/s10462-025-11347-y},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of machine learning for computer-aided diagnosis of parkinson’s disease: Progress and benchmark case study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV target tracking: A survey. <em>AIR</em>, <em>58</em>(11), 1-62. (<a href='https://doi.org/10.1007/s10462-025-11348-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) have become critical enablers of integrated air-space-ground Internet of Things (IoT) ecosystems, with target tracking serving as a foundational technology. This paper classifies UAV target tracking into two distinct paradigms: active tracking and passive tracking, differentiated by their operational scopes and technical objectives. Active tracking is defined as a closed-loop spatial pursuit system, whereby UAVs dynamically track targets through iterative cycles centered on three primary stages: online passive tracking, state fusion estimation, and tracking strategy generation, with subsequent execution phases implied in the loop. This workflow bridges perception and action, enabling spatial engagement through continuous sensor-to-control feedback. In contrast, passive tracking acts as a vision-centric analytical module that exclusively extracts target image-domain attributes from visual sensors—devoid of physical state inference or control mechanisms. As a preprocessing stage for active systems, it is constrained to the visual perception layer, lacking the spatial engagement capabilities inherent in closed-loop tracking systems. This paper conducts an in-depth analysis of the application, key challenges, and future trends in both active and passive UAV target tracking. By systematically discussing the relationships among relevant technologies, this work aims to establish a foundational reference framework and offer citation material for guiding the future development of UAV target tracking technologies.},
  archive      = {J_AIR},
  author       = {Wu, Pengnian and Li, Yixuan and Xue, Dong},
  doi          = {10.1007/s10462-025-11348-x},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-62},
  shortjournal = {Artif. Intell. Rev.},
  title        = {UAV target tracking: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on subspace clustering: Methods and applications. <em>AIR</em>, <em>58</em>(11), 1-50. (<a href='https://doi.org/10.1007/s10462-025-11349-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a pivotal strategy to deal with complicated and high-dimensional data, subspace clustering is to find a set of subspaces of a high-dimensional space and then partition each data point in dataset into the corresponding subspace. This field has witnessed remarkable progress over recent decades, with substantial theoretical advancements and successful applications spanning image processing, genomic analysis and text analysis. However, existing surveys predominantly focus on conventional shallow-structured methods, with few up-to-date reviews on deep-structured methods, i.e., deep neural network-based approaches. In fact, recent years has witnessed the overwhelming success of deep neural network in various fields, including computer vision, natural language processing, subspace clustering. To address this gap, this paper presents a comprehensive review on subspace clustering methods, including conventional shallow-structured and deep neural network based approaches, which systematically analyzes over 150 papers published in peer-reviewed journals and conferences, highlighting the latest research achievements, methods, algorithms and applications. Specifically, we first briefly introduce the basic principles and evolution of subspace clustering. Subsequently, we present an overview of research on subspace clustering, dividing the existing works into two categories: shallow subspace clustering and deep subspace clustering, based on the model architecture. Within each category, we introduce a refined taxonomy distinguishing linear and nonlinear approaches based on data characteristics and subspace structural assumptions. Finally, we discuss the challenges currently faced and future research direction for development in the field of subspace clustering.},
  archive      = {J_AIR},
  author       = {Miao, Jianyu and Zhang, Xiaochan and Yang, Tiejun and Fan, Chao and Tian, Yingjie and Shi, Yong and Xu, Mingliang},
  doi          = {10.1007/s10462-025-11349-w},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey on subspace clustering: Methods and applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending against attacks in deep learning with differential privacy: A survey. <em>AIR</em>, <em>58</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11350-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have witnessed the revolutionary development of deep learning. As the application domain of deep learning has expanded, its privacy risks have attracted attention since deep leaning methods often use private data for training. Some methods for attacking deep learning, such as membership inference attacks, increase the privacy risks of deep learning models. One risk-reducing defensive strategy with great potential is to apply some degree of random perturbation during the training (or other) phase. Therefore, differential privacy, as a privacy protection framework originally designed for publishing data, is widely used to protect the privacy of deep learning models due to its solid mathematical foundation. In this paper, we first introduce several attack methods that threaten deep learning. Then, we systematically review the cross-applications of differential privacy and deep learning to protect deep learning models. We encourage researchers to visually demonstrate the defense effects of their approaches in the literature rather than solely providing rigorous mathematical proofs. In addition to privacy, we also discuss and review the impact of differential privacy on the robustness, overfitting, and fairness of deep neural networks. Finally, we analyze some potential future research directions, highlighting the significant potential for differential privacy to make positive contributions to future deep learning systems.},
  archive      = {J_AIR},
  author       = {Xiangfei, Zhang and Qingchen, Zhang},
  doi          = {10.1007/s10462-025-11350-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Defending against attacks in deep learning with differential privacy: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swarm intelligence techniques and their applications in fog/edge computing: An in-depth review. <em>AIR</em>, <em>58</em>(11), 1-182. (<a href='https://doi.org/10.1007/s10462-025-11351-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in the Internet of Things (IoT) have connected diverse devices that often have limited resources and processing power. Artificial intelligence (AI) applications in fog and edge computing are greatly enhanced by Swarm Intelligence (SI) techniques. These SI methods improve resource allocation, task scheduling, and load balancing, making distributed systems more efficient and responsive to changing conditions. This paper systematically reviews 91 studies (2019–2023) on SI applications in fog/edge environments. We compare fog, edge, and cloud computing paradigms and analyze SI-based approaches using case studies, performance metrics, and evaluation tools. This review identifies key advantages and limitations of current SI-based approaches and highlights open issues and future research directions to enhance distributed computing systems. These insights aim to guide the development of more efficient and responsive AI-driven resource management strategies in fog/edge environments.},
  archive      = {J_AIR},
  author       = {Ghafari, Reyhane and Mansouri, Najme},
  doi          = {10.1007/s10462-025-11351-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-182},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Swarm intelligence techniques and their applications in fog/edge computing: An in-depth review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-induced deskilling in medicine: A mixed-method review and research agenda for healthcare and beyond. <em>AIR</em>, <em>58</em>(11), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11352-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Artificial Intelligence (AI) in healthcare is reshaping clinical practice, offering both opportunities for enhanced decision-making and risks of skill degradation among medical professionals. This growing impact calls for a comprehensive evaluation of its effects on medical expertise. This study presents a mixed-method literature review, combining systematic analysis with narrative synthesis to examine AI-induced deskilling and upskilling inhibition-the erosion of medical expertise and the reduction of opportunities for skill acquisition due to AI-driven decision support systems. Anchoring the discussion in the core medical competencies outlined by the Federation of Royal Colleges of Physicians of the UK-Practical Assessment of Clinical Examination Skills (PACES-MRCPUK), the systematic review identifies key vulnerabilities in physical examination, differential diagnosis, clinical judgment, and physician-patient communication. The narrative review explores broader themes related to Human–AI Interaction and the Impact of AI on Human Skills in Organizations. In response to concerns about the Second Singularity-a scenario in which decision-making autonomy is increasingly ceded to AI, weakening human oversight-this review advocates for a research agenda that prioritizes longitudinal studies, real-time monitoring of AI’s impact, and the development of frameworks to mitigate skill erosion, ensuring the preservation of professional autonomy and the safeguarding of the irreplaceable elements of human judgment in medicine and beyond.},
  archive      = {J_AIR},
  author       = {Natali, Chiara and Marconi, Luca and Dias Duran, Leslye Denisse and Cabitza, Federico},
  doi          = {10.1007/s10462-025-11352-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-induced deskilling in medicine: A mixed-method review and research agenda for healthcare and beyond},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved decisions for unknown behaviours in interactive dynamic influence diagrams. <em>AIR</em>, <em>58</em>(11), 1-28. (<a href='https://doi.org/10.1007/s10462-025-11355-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive dynamic influence diagrams (I-DIDs) are a general decision framework for a subject agent who interacts with other agents (of either collaborative or competitive) in a common environment with partial observability. The subject agent aims to optimize its decision-making (response strategy) while other agents concurrently adapt their behaviors over time. The I-DID model has faced a long-term challenge when other agents exhibit unknown behaviors that go beyond what the subject agent has planned for prior to their interactions. This is because the subject agent does not hold the capability of modeling unknown behaviours of other agents in traditional I-DID techniques. In this article, we adapt two different swarm intelligence (SI) techniques to develop new behaviours for other agents in I-DIDs. The SI-based algorithms have the strength of generating a collective set of behaviours that could potentially contain various types of agents’ behaviours. We theoretically analyze how the two algorithms impact the subject agent’s decision quality, and empirically demonstrate the algorithm performance in two commonly used problem domains.},
  archive      = {J_AIR},
  author       = {Pan, Yinghui and Zhou, Mengen and Ma, Biyang and Zeng, Yifeng and Ong, Yew-soon and Liu, Guoquan},
  doi          = {10.1007/s10462-025-11355-y},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improved decisions for unknown behaviours in interactive dynamic influence diagrams},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ameliorated elk herd optimizer for global optimization and engineering problems. <em>AIR</em>, <em>58</em>(11), 1-80. (<a href='https://doi.org/10.1007/s10462-025-11360-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization techniques have received significant attention for reliably addressing practical problems. A potential meta-heuristic called elk herd optimizer (EHO) was created, inspired by the social behavior and reproduction of elks. EHO has drawbacks, including poor convergence competency and a tendency to fall into local extrema in various optimization problems. Furthermore, this algorithm does not account for the memory of its search agents and has difficulty effectively balancing exploration and exploitation, which can lead to early convergence toward a local optimum. This study addresses the above issues by proposing an ameliorated EHO (AEHO) by incorporating several modifications into the basic EHO algorithm, which can be described as follows: A new hybrid memory-based EHO is developed that uses the particle swarm optimization (PSO) algorithm to guide EHO to search for reasonable candidate solutions. This hybrid approach was proposed to enhance EHO’s diversity and balance search capabilities to achieve strong search performance. Initially, a memory component was added to EHO using the idea of pbest from PSO to tap into promising search regions, which focuses on improving the best solutions and preventing the algorithm from getting stuck in a local optimum. In addition, the PSO concepts of (gbest) and (pbest) are used to enhance the best placements of the search agents in EHO. Finally, a greedy selection method was used to improve the efficiency of exhaustive exploration in AEHO, using the fitness values before and after updates as an indicator for efficacy of the best solutions. To evaluate the performance of the AEHO algorithm against a group of well-known competitors, we use ten complex test functions from the global CEC2022 test suite and thirty complex test functions from the global CEC2014 test suite. Based on the analysis of the experimental findings, AEHO performed optimally on 84% of the CEC2014 functions and 74% of the CEC2022 functions, ranking first in both suites with an average ranking of 3.11 and 1.62, respectively. The mean computation time of AEHO is about one-third of the average computation time for the first-ranked method, indicating that AEHO not only performs very well in global searches but also exhibits greater search efficiency when compared to newer optimization algorithms. The applicability and reliability of AEHO were thoroughly studied on four constrained engineering design problems and a real-world industrial process. The results demonstrate the superiority and promising potential of AEHO in addressing a wide range of challenging real-world problems.},
  archive      = {J_AIR},
  author       = {Al-Betar, Mohammed Azmi and Braik, Malik Sh. and Shambour, Qusai Yousef and Al-Naymat, Ghazi and Porntaveetus, Thantrira},
  doi          = {10.1007/s10462-025-11360-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-80},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Ameliorated elk herd optimizer for global optimization and engineering problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAoG: Decayed adaptation over gradients for parameter-free step size control. <em>AIR</em>, <em>58</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11362-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the scale of parameters in deep learning models continues to grow, the cost of training such models increases accordingly, posing increasingly significant challenges for stochastic optimization methods. A central issue in gradient-based optimization lies in the selection of the step size, whose appropriateness directly affects training efficiency and model performance. To address this issue, a series of parameter-free optimization methods that do not require manual tuning of the step size have been proposed in recent years. Among them, DoG and its improved variant DoWG are the most representative. Despite demonstrating strong performance across various tasks, DoG and DoWG still suffer from performance instability or slow convergence under certain model architectures or training conditions. This paper introduces Decayed Adaptation over Gradients (DAoG), a novel parameter-free optimization method that systematically addresses these limitations. Our key innovation lies in incorporating a principled step size decay mechanism for the first time within the parameter-free optimization framework, which substantially enhances both optimization stability and model generalization. Additionally, a parameter compression strategy is employed to reduce sensitivity to the initial step size. Theoretical analysis demonstrates that DAoG exhibits favorable convergence properties under L-smooth and G-Lipschitz conditions. Empirical studies across representative tasks in natural language processing and computer vision demonstrate that DAoG outperforms both DoG and DoWG in terms of convergence speed and generalization performance. Notably, it even rivals or surpasses Adam with cosine annealing in several challenging scenarios. These theoretical and experimental results suggest that DAoG effectively mitigates the overly conservative step size issue in DoG and the instability problem in DoWG, thereby advancing the development of parameter-free optimization methods in deep learning.},
  archive      = {J_AIR},
  author       = {Zhang, Yifan and Zhao, Di and Li, Hongyi and Pan, Chengwei},
  doi          = {10.1007/s10462-025-11362-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {DAoG: Decayed adaptation over gradients for parameter-free step size control},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Place recognition meet multiple modalities: A comprehensive review, current challenges and future development. <em>AIR</em>, <em>58</em>(11), 1-48. (<a href='https://doi.org/10.1007/s10462-025-11367-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Place recognition is a cornerstone of vehicle navigation and mapping, which is pivotal in enabling systems to determine whether a location has been previously visited. This capability is critical for tasks such as loop closure in Simultaneous Localization and Mapping (SLAM) and long-term navigation under varying environmental conditions. This survey comprehensively reviews recent advancements in place recognition, emphasizing three representative methodological paradigms: Convolutional Neural Network (CNN)-based approaches, Transformer-based frameworks, and cross-modal strategies. We begin by elucidating the significance of place recognition within the broader context of autonomous systems. Subsequently, we trace the evolution of CNN-based methods, highlighting their contributions to robust visual descriptor learning and scalability in large-scale environments. We then examine the emerging class of Transformer-based models, which leverage self-attention mechanisms to capture global dependencies and offer improved generalization across diverse scenes. Furthermore, we discuss cross-modal approaches that integrate heterogeneous data sources such as Lidar, vision, and text description, thereby enhancing resilience to viewpoint, illumination, and seasonal variations. We also summarize standard datasets and evaluation metrics widely adopted in the literature. To the best of our knowledge, no prior survey has systematically reviewed visual, LiDAR, and cross-modal place recognition concurrently. This work thus resolves a critical gap in existing literature dominated by single-modality studies. Finally, we identify current research challenges and outline prospective directions, including domain adaptation, real-time performance, and lifelong learning, to inspire future advancements in this domain. The unified framework of leading-edge place recognition methods, i.e., code library, and the results of their experimental evaluations are available at https://github.com/CV4RA/SOTA-Place-Recognitioner .},
  archive      = {J_AIR},
  author       = {Li, Zhenyu and Shang, Tianyi and Xu, Pengjie and Deng, Zhaojun},
  doi          = {10.1007/s10462-025-11367-8},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Place recognition meet multiple modalities: A comprehensive review, current challenges and future development},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic operation room scheduling DORS strategy based on explainable AI and fuzzy interface engine. <em>AIR</em>, <em>58</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11366-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poor surgical scheduling causes major problems in hospital operating rooms, such as long patient wait times, underutilized operating rooms, and high costs. Existing scheduling approaches, which are static or less adaptable, fail to handle real-time unpredictability. To overcome these constraints, this study presents Dynamic Operation Room Scheduling (DORS), a new intraday surgical scheduling system. DORS uses a two-layered architecture: (1) Explainable AI for feature selection that is based on critical scheduling criteria such as Round Robin, and (2) a dynamic scheduling system that includes a Receiving Module, a Checking Module for patient prioritization, and a Scheduling Module provided by a Fuzzy Interface Engine. This system allows for proactive schedule preparation and reactive modifications, making it possible to smoothly include unscheduled surgical operations. In comparison to traditional (FCFS, Round Robin) and optimization-based (genetic algorithm) methods. DORS dynamically modifies schedules to reduce average wait times (AWT), consistently outperforming other approaches by 120–560 min. DORS completes surgical operations more quickly (half of surgical operations in 255–725 min). In addition, DORS retains a modest runtime (45 ms) while increasing scheduling efficiency (98.6%). DORS also demonstrates strong stability, with low Relative Percentage Deviation (RPD) on high-demand days. Finally, DORS achieves the optimal blend of speed, efficiency, and responsiveness, making it the greatest choice for hospitals aiming to eliminate delays, optimize operating room usage, and effectively manage changing surgical needs.},
  archive      = {J_AIR},
  author       = {El-Balka, Rana Mohamed and Sakr, Noha and Rabie, Asmaa H. and Saleh, Ahmed I.},
  doi          = {10.1007/s10462-025-11366-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A dynamic operation room scheduling DORS strategy based on explainable AI and fuzzy interface engine},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting natural selection: Evolving dynamic neural networks using genetic algorithms for complex control tasks. <em>AIR</em>, <em>58</em>(11), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11382-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) and Genetic Algorithms (GAs) are widely used in decision-making and control tasks, but they often suffer from prolonged training times and inefficiencies. This paper addresses the need for a faster and more precise method to train neural networks in RL tasks, without sacrificing performance. The proposed approach enhances GAs by introducing mechanisms that optimize network architectures dynamically, minimizing unnecessary complexity while maintaining accuracy. The methodology includes a dynamic architecture adaptation technique that trims the neural network to its most compact and effective configuration. A Blending mechanism is introduced to improve the propagation of essential features across network layers, reducing the usage of non-linearity until necessary. An experience replay buffer is integrated to avoid redundant fitness evaluations, significantly reducing computational overhead. Additionally, a novel approach combines back-propagation with GAs for further refinement in supervised or RL tasks, using it as a mutation method to fine-tune the model. Experimental results demonstrate convergence speeds of around several seconds for simple tasks with well-defined rewards, and several minutes for more complex tasks. Training time is reduced by nearly 70%, and the approach provides faster inference speeds due to minimal architecture, making it applicable for mobile and edge devices. The method reduces computation, especially during inference, by over 90% due to the extremely low number of parameters. The performance metrics show comparable results to conventional approaches at the end of training. The proposed method is scalable and resource-efficient, outperforming existing neural network optimization techniques in both simulated environments and real-world applications. The developed framework is publicly available under the MIT license at https://github.com/AhmedBoin/atgen offering an open-source solution for the broader research community.},
  archive      = {J_AIR},
  author       = {Taha, Mohamed A. and Saafan, Mahmoud M. and Ayyad, Sarah M.},
  doi          = {10.1007/s10462-025-11382-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Revisiting natural selection: Evolving dynamic neural networks using genetic algorithms for complex control tasks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="aism">AISM - 8</h2>
<ul>
<li><details>
<summary>
(2025). Central limit theorems for vector-valued composite functionals with smoothing and applications. <em>AISM</em>, <em>77</em>(5), 821-852. (<a href='https://doi.org/10.1007/s10463-025-00934-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on vector-valued composite functionals, which may be nonlinear in probability. Our goal is establishing central limit theorems for these functionals when employed by mixed estimators. Our study is relevant to the evaluation and comparison of risk in decision-making contexts and extends to functionals that arise in machine learning. A generalized family of composite risk functionals is presented, which encompasses coherent risk measures, including systemic risk. The paper makes two main contributions. First, we analyze vector-valued functionals and provide a framework for evaluating high-dimensional risks. This enables comparison of multiple risk measures and supports estimation and asymptotic analysis of systemic risk and its optimal value in decision-making. Second, we derive new central limit theorems for optimized composite functionals using mixed estimators, including empirical and smoothed types. We give verifiable conditions for central limit formulae and demonstrate their applicability to several risk measures.},
  archive      = {J_AISM},
  author       = {Chen, Huihui and Dentcheva, Darinka and Lin, Yang and Stock, Gregory J.},
  doi          = {10.1007/s10463-025-00934-z},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {10},
  number       = {5},
  pages        = {821-852},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Central limit theorems for vector-valued composite functionals with smoothing and applications},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Posterior contraction rate and asymptotic bayes optimality for one group global–local shrinkage priors in sparse normal means problem. <em>AISM</em>, <em>77</em>(5), 787-819. (<a href='https://doi.org/10.1007/s10463-025-00932-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study inference on the mean vector of the normal means model in sparse asymptotic settings when it is modelled by broad classes of one-group global–local continuous shrinkage priors. We prove that the resulting posterior distributions contract around the truth at a near minimax rate with respect to squared $$L_2$$ loss when the global shrinkage parameter is estimated in empirical Bayesian ways or arbitrary priors supported on some appropriate interval are assigned to it. We then employ an intuitive multiple testing rule (using full Bayes treatment with global–local priors) in a problem of simultaneous testing (with additive misclassification loss) for the components of the mean assuming they are iid from a two-groups prior. In a first result of its kind, risk of our testing rule is shown to asymptotically match (up to a constant) that of the optimal rule in the two-groups setting.},
  archive      = {J_AISM},
  author       = {Paul, Sayantan and Chakrabarti, Arijit},
  doi          = {10.1007/s10463-025-00932-1},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {10},
  number       = {5},
  pages        = {787-819},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Posterior contraction rate and asymptotic bayes optimality for one group global–local shrinkage priors in sparse normal means problem},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The family of multivariate beta copulas revisited. <em>AISM</em>, <em>77</em>(5), 757-786. (<a href='https://doi.org/10.1007/s10463-025-00931-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article sheds new lights on the family of multivariate beta copulas that arises as the dependence structures of the multivariate generalized beta distribution of the second type. In particular, simple formulas for the computation of Kendall’s measure of association are derived and the asymmetry properties are investigated. Also, the multivariate extreme-value attractor of the beta copula is identified and it is shown that the beta family is closed under conditioning and belongs to the class of one-factor copulas. The sampling properties of the rank-based maximum-likelihood estimator are investigated with simulations and the usefulness of the beta copulas for the modeling of multivariate datasets is illustrated on triathlon data.},
  archive      = {J_AISM},
  author       = {Agbangla, Enagnon Narcisse and Quessy, Jean-François and Rivest, Louis-Paul},
  doi          = {10.1007/s10463-025-00931-2},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {10},
  number       = {5},
  pages        = {757-786},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {The family of multivariate beta copulas revisited},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distance covariance test of independence in high dimension, low sample size contexts. <em>AISM</em>, <em>77</em>(5), 731-755. (<a href='https://doi.org/10.1007/s10463-025-00928-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To check the mutual independence of a high-dimensional random vector without Gaussian assumption, Yao et al. (Journal of the Royal Statistical Society Series B, 80,455–480, 2018) recently introduced an important test by virtue of pairwise distance covariances. Despite its usefulness, the state-of-art test tends to have unsatisfactory size performance when the sample size is small. The present paper provides a theoretical explanation about this phenomenon, and accordingly proposes a new test in high dimension, low sample size contexts. The new test can be even justified as the dimension tends to infinity, regardless of whether the sample size is fixed or diverges. The power of the proposed distance covariance test is also investigated. To examine our theoretical findings and check the performance of the new test, simulation studies are applied. We further illustrate the proposed method by empirical analysis of a real dataset.},
  archive      = {J_AISM},
  author       = {Xu, Kai and Yang, Minghui},
  doi          = {10.1007/s10463-025-00928-x},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {10},
  number       = {5},
  pages        = {731-755},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {A distance covariance test of independence in high dimension, low sample size contexts},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rejoinder to the discussion of “Mode-based estimation of the center of symmetry”. <em>AISM</em>, <em>77</em>(5), 727-730. (<a href='https://doi.org/10.1007/s10463-025-00945-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AISM},
  author       = {Chacón, José E. and Fernández Serrano, Javier},
  doi          = {10.1007/s10463-025-00945-w},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {10},
  number       = {5},
  pages        = {727-730},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Rejoinder to the discussion of “Mode-based estimation of the center of symmetry”},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of “Mode-based estimation of the center of symmetry”. <em>AISM</em>, <em>77</em>(5), 723-725. (<a href='https://doi.org/10.1007/s10463-025-00944-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AISM},
  author       = {Pardo-Fernández, Juan Carlos},
  doi          = {10.1007/s10463-025-00944-x},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {10},
  number       = {5},
  pages        = {723-725},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Discussion of “Mode-based estimation of the center of symmetry”},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of “Mode-based estimation of the center of symmetry”. <em>AISM</em>, <em>77</em>(5), 719-721. (<a href='https://doi.org/10.1007/s10463-025-00943-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AISM},
  author       = {Hino, Hideitsu},
  doi          = {10.1007/s10463-025-00943-y},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {10},
  number       = {5},
  pages        = {719-721},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Discussion of “Mode-based estimation of the center of symmetry”},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mode-based estimation of the center of symmetry. <em>AISM</em>, <em>77</em>(5), 685-717. (<a href='https://doi.org/10.1007/s10463-025-00942-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the mean-median-mode triad of univariate centrality measures, the mode has been overlooked for estimating the center of symmetry in continuous and unimodal settings. This paper expands on the connection between kernel mode estimators and M-estimators for location, bridging the gap between the nonparametrics and robust statistics communities. The variance of modal estimators is studied in terms of a bandwidth parameter, establishing conditions for an optimal solution that outperforms the household sample mean. A purely nonparametric approach is adopted, modeling heavy-tailedness through regular variation. The results lead to an estimator proposal that includes a novel one-parameter family of kernels with compact support, offering extra robustness and efficiency. The effectiveness and versatility of the new method are demonstrated in a real-world case study and a thorough simulation study, comparing favorably to traditional and more competitive alternatives. Several myths about the mode are clarified along the way, reopening the quest for flexible and efficient nonparametric estimators.},
  archive      = {J_AISM},
  author       = {Chacón, José E. and Fernández Serrano, Javier},
  doi          = {10.1007/s10463-025-00942-z},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {10},
  number       = {5},
  pages        = {685-717},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Mode-based estimation of the center of symmetry},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="amai">AMAI - 6</h2>
<ul>
<li><details>
<summary>
(2025). Automatic error function learning with interpretable compositional networks. <em>AMAI</em>, <em>93</em>(3), 441-469. (<a href='https://doi.org/10.1007/s10472-022-09829-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Constraint Programming, constraints are usually represented as predicates allowing or forbidding combinations of values. However, some algorithms can exploit a finer representation: error functions. By associating a function to each constraint type to evaluate the quality of an assignment, it extends the expressiveness of regular Constraint Satisfaction Problem/Constrained Optimization Problem formalisms. Their usage comes with a price though: it makes problem modeling significantly harder, since users must provide a set of error functions that are not always easy to define. Here, we propose a method to automatically learn an error function corresponding to a constraint, given its predicate version only. This is, to the best of our knowledge, the first attempt to automatically learn error functions for hard constraints. In this paper, we also give for the first time a formal definition of combinatorial problems with hard constraints represented by error functions. Our method aims to learn error functions in a supervised fashion, trying to reproduce either the Hamming or the Manhattan distance, by using a graph model we named Interpretable Compositional Networks. This model allows us to get interpretable results. We run experiments on 7 different constraints to show its versatility. Experiments show that our system can learn functions that scale to high dimensions, and can learn fairly good functions over incomplete spaces. We also show that learned error functions can be used efficiently to represent constraints in different classic problems.},
  archive      = {J_AMAI},
  author       = {Richoux, Florian and Baffier, Jean-François},
  doi          = {10.1007/s10472-022-09829-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {441-469},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Automatic error function learning with interpretable compositional networks},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-driven approach to neural architecture search initialization. <em>AMAI</em>, <em>93</em>(3), 413-440. (<a href='https://doi.org/10.1007/s10472-022-09823-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic design in neural architecture search (NAS) has received a lot of attention, aiming to improve performance and reduce computational cost. Despite the great advances made, few authors have proposed to tailor initialization techniques for NAS. However, the literature shows that a good initial set of solutions facilitates finding the optima. Therefore, in this study, we propose a data-driven technique to initialize a population-based NAS algorithm. First, we perform a calibrated clustering analysis of the search space, and second, we extract the centroids and use them to initialize a NAS algorithm. We benchmark our proposed approach against random and Latin hypercube sampling initialization using three population-based algorithms, namely a genetic algorithm, an evolutionary algorithm, and aging evolution, on CIFAR-10. More specifically, we use NAS-Bench-101 to leverage the availability of NAS benchmarks. The results show that compared to random and Latin hypercube sampling, the proposed initialization technique enables achieving significant long-term improvements for two of the search baselines, and sometimes in various search scenarios (various training budget). Besides, we also investigate how an initial population gathered on the tabular benchmark can be used for improving search on another dataset, the So2Sat LCZ-42. Our results show similar improvements on the target dataset, despite a limited training budget. Moreover, we analyse the distributions of solutions obtained and find that that the population provided by the data-driven initialization technique enables retrieving local optima (maxima) of high fitness and similar configurations.},
  archive      = {J_AMAI},
  author       = {Traoré, Kalifou René and Camero, Andrés and Zhu, Xiao Xiang},
  doi          = {10.1007/s10472-022-09823-0},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {413-440},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {A data-driven approach to neural architecture search initialization},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chance constrained conic-segmentation support vector machine with uncertain data. <em>AMAI</em>, <em>93</em>(3), 389-411. (<a href='https://doi.org/10.1007/s10472-022-09822-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machines (SVM) is one of the well known supervised machine learning model. The standard SVM models are dealing with the situation where the exact values of the data points are known. This paper studies the SVM model when the data set contains uncertain or mislabelled data points. To ensure the small probability of misclassification for the uncertain data, a chance constrained conic-segmentation SVM model is proposed for multiclass classification. Based on the data set, a mixed integer programming formulation for the chance constrained conic-segmentation SVM is derived. Kernelization of chance constrained conic-segmentation SVM model is also exploited for nonlinear classification. The geometric interpretation is presented to show how the chance constrained conic-segmentation SVM works on uncertain data. Finally, experimental results are presented to demonstrate the effectiveness of the chance constrained conic-segmentation SVM for both artificial and real-world data.},
  archive      = {J_AMAI},
  author       = {Peng, Shen and Canessa, Gianpiero and Allen-Zhao, Zhihua},
  doi          = {10.1007/s10472-022-09822-1},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {389-411},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Chance constrained conic-segmentation support vector machine with uncertain data},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online learning of variable ordering heuristics for constraint optimisation problems. <em>AMAI</em>, <em>93</em>(3), 359-388. (<a href='https://doi.org/10.1007/s10472-022-09816-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solvers for constraint optimisation problems exploit variable and value ordering heuristics. Numerous expert-designed heuristics exist, while recent research learns novel, customised heuristics from past problem instances. This article addresses unseen problems for which no historical data is available. We propose one-shot learning of customised, problem instance-specific heuristics. To do so, we introduce the concept of deep heuristics, a data-driven approach to learn extended versions of a given variable ordering heuristic online. First, for a problem instance, an initial online probing phase collects data, from which a deep heuristic function is learned. The learned heuristics can look ahead arbitrarily-many levels in the search tree instead of a ‘shallow’ localised lookahead of classical heuristics. A restart-based search strategy allows for multiple learned models to be acquired and exploited in the solver’s optimisation. We demonstrate deep variable ordering heuristics based on the smallest, anti first-fail, and maximum regret heuristics. Results on instances from the MiniZinc benchmark suite show that deep heuristics solve 20% more problem instances while improving on overall runtime for the Open Stacks and Evilshop benchmark problems.},
  archive      = {J_AMAI},
  author       = {Doolaard, Floris and Yorke-Smith, Neil},
  doi          = {10.1007/s10472-022-09816-z},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {359-388},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Online learning of variable ordering heuristics for constraint optimisation problems},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from obstructions: An effective deep learning approach for minimum vertex cover. <em>AMAI</em>, <em>93</em>(3), 347-358. (<a href='https://doi.org/10.1007/s10472-022-09813-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational intractability has for decades motivated the development of a plethora of methodologies that mainly aim at a quality-time trade-off. The use of Machine Learning has finally emerged as one of the possible tools to obtain approximate solutions to $$\mathcal {N}\mathcal {P}$$ -hard optimization problems. Recently, Dai et al. introduced a method for computing such approximate solutions for instances of the Vertex Cover problem. In this paper we consider the effectiveness of selecting a proper training strategy by considering special problem instances called obstructions that we believe carry some intrinsic properties of the problem. Capitalizing on the recent work of Dai et al. on Vertex Cover, and using the same case study as well as 19 other problem instances, we show the utility of using obstructions for training neural networks. Experiments show that training with obstructions results in a surprisingly huge reduction in number of iterations needed for convergence, thus gaining a substantial reduction in the time needed for training the model.},
  archive      = {J_AMAI},
  author       = {Abu-Khzam, Faisal N. and Abd El-Wahab, Mohamed M. and Haidous, Moussa and Yosri, Noureldin},
  doi          = {10.1007/s10472-022-09813-2},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {347-358},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Learning from obstructions: An effective deep learning approach for minimum vertex cover},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data science meets optimization II. <em>AMAI</em>, <em>93</em>(3), 343-345. (<a href='https://doi.org/10.1007/s10472-025-09980-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Zhang, Yingqian and Guns, Tias and Lombardi, Michele and De Causmaecker, Patrick},
  doi          = {10.1007/s10472-025-09980-y},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {343-345},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Data science meets optimization II},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="apin">APIN - 15</h2>
<ul>
<li><details>
<summary>
(2025). PretopoMD: Pretopology-based mixed data hierarchical clustering. <em>APIN</em>, <em>55</em>(15), 1-18. (<a href='https://doi.org/10.1007/s10489-025-06770-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm’s robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.},
  archive      = {J_APIN},
  author       = {Levy, Loup-Noé and Guerard, Guillaume and Djebali, Sonia and Amor, Soufian Ben},
  doi          = {10.1007/s10489-025-06770-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {PretopoMD: Pretopology-based mixed data hierarchical clustering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-discriminator generative adversarial networks with dynamic penalty to over-sample imbalanced credit datasets. <em>APIN</em>, <em>55</em>(15), 1-30. (<a href='https://doi.org/10.1007/s10489-025-06836-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of credit risk data imbalance reduces the effectiveness of assessment models. Existing oversampling methods focus only on a partial sample of a few classes, resulting in a lack of diversity in the types of data generated. This paper proposes an innovative GAN variant called Magnify-GAN. The originality of Magnify-GAN lies in the fact that it is equipped with a primary discriminator and multiple secondary discriminators, each of which employs a different loss function. This multi-discriminator approach not only improves the learning results, but also enriches the feedback received during the training process. In addition, we integrate an innovative dynamic coefficient mechanism to enable the model to dynamically adapt to changes in data distribution. To further improve stability and address the common modal collapse problem in GAN, a gradient penalty method is embedded in the training protocol. This integrated strategy ensures that Magnify-GAN can effectively generate samples representing various minority classes within the real data. Compared to ten classical imbalanced sampling methods, Magnify-GAN demonstrates superior performance in precision, F1-score, and AUC values across six synthetic and four real-world imbalanced datasets. Ablation studies, visualized through heatmaps, reveal the complementary synergy between the core modules. Furthermore, a complexity analysis shows that Magnify-GAN offers significant performance gains with moderate increases in computational cost compared to state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Dong, Xiaogang and Wang, Lifei and Qin, Xiwen and Shi, Hongyu},
  doi          = {10.1007/s10489-025-06836-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-30},
  shortjournal = {Appl. Intell.},
  title        = {Multi-discriminator generative adversarial networks with dynamic penalty to over-sample imbalanced credit datasets},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph convolutional network for time series classification using recurrence plots. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06841-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) is a crucial task across various domains, and its performance heavily depends on the quality of input representations. Among various representations, the recurrence plot (RP) effectively captures topological recurrence, the unique property of time series data. However, conventional convolutional neural networks (CNNs) cannot fully exploit this property since they treat the RP as grid-like data. In this study, we propose RP-GCN, a novel approach that uses a graph convolutional network (GCN) to exploit topological recurrence inherent in the RP, thereby improving TSC performance. Our method transforms a multivariate time series into graphs where state matrices act as node feature matrices and RPs serve as adjacency matrices, enabling graph convolution to utilize recurrence relationships. We evaluated RP-GCN on 35 benchmark multivariate time series classification datasets and demonstrated superior accuracy and efficient inference time compared to existing methods.},
  archive      = {J_APIN},
  author       = {Kang, Hyewon and Lee, Taek-Ho and Lee, Junghye},
  doi          = {10.1007/s10489-025-06841-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {A graph convolutional network for time series classification using recurrence plots},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preprocessing method for shield operational parameters adaptable to geological survey data characteristic for predicting disc cutter wear. <em>APIN</em>, <em>55</em>(15), 1-20. (<a href='https://doi.org/10.1007/s10489-025-06846-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shield operational parameters are inherently noisy and, relative to concurrent geological exploration data, contain considerable redundancy, they must be pre-processed before the datasets input to artificial intelligence models. This paper presents a denoising and compression method for preprocessing shield operational parameters, integrating it with the stratal slicing method for predicting disc cutter wear. The operational parameter signals affecting cutter wear are first denoised using wavelet transform, Fourier transform, rolling average, and autoencoder techniques. The proposed Ring-based Summation Averaging (RSA) and Piecewise Aggregate Averaging (PAA) methods are then used to compress the denoised signals, resulting in compressed sequences composed of key points equal to the number of tunnel rings, effectively matching the geological parameters expanded by the stratal slicing method. Furthermore, the prepared data were tested using the long short-term memory (LSTM) + attention mechanism (AM) model to evaluate its application effectiveness in the Guangzhou Metro Line 18 railway. The results show that data compressed using PAA not only better tracks signal variations but also allows for flexible control of the output length of the compressed sequence. The combination of wavelet transforms denoising (WTD) with PAA exhibited the best wear prediction results, achieving R2 / MSE = 0.95 / 2.21 mm. By integrating WTD, PAA, stratal slicing method, and sequence models, a comprehensive and universal methodology is established that can predict disc cutter wear based on initial geological data and shield operational parameters.},
  archive      = {J_APIN},
  author       = {Mo, Deyun and Bai, Liping and Liao, Wenjiang and Tian, Xinyuan and Huang, Weiran},
  doi          = {10.1007/s10489-025-06846-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Preprocessing method for shield operational parameters adaptable to geological survey data characteristic for predicting disc cutter wear},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic gradient accelerated by negative momentum for training deep neural networks. <em>APIN</em>, <em>55</em>(15), 1-13. (<a href='https://doi.org/10.1007/s10489-025-06900-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fast and robust stochastic optimization algorithms for training deep neural networks (DNNs) are still a topic of heated discussion. As a simple but effective way, the momentum technique, which utilizes historical gradient information, shows significant promise in training DNNs both theoretically and empirically. Nonetheless, the accumulation of error gradients in stochastic settings leads to the failure of momentum techniques, e.g., Nesterov’s accelerated gradient (NAG), in accelerating stochastic optimization algorithms. To address this problem, a novel type of stochastic optimization algorithm based on negative momentum (NM) is developed and analyzed. In this work, we applied NM to vanilla stochastic gradient descent (SGD), leading to SGD-NM. Although a convex combination of previous and current historical information is adopted in SGD-NM, fewer hyperparameters are introduced than those of the existing NM techniques. Meanwhile, we establish a theoretical guarantee for the resulting SGD-NM and show that SGD-NM enjoys the same low computational cost as vanilla SGD. To further show the superiority of NM in stochastic optimization algorithms, we propose a variant of stochastically controlled stochastic gradient (SCSG) based on the negative momentum technique, termed SCSG-NM, which achieves faster convergence compared to SCSG. Finally, we conduct experiments on various DNN architectures and benchmark datasets. The comparison results with state-of-the-art stochastic optimization algorithms show the great potential of NM in accelerating stochastic optimization, including more robust to large learning rates and better generalization.},
  archive      = {J_APIN},
  author       = {Li, Xiaotian and Yang, Zhuang and Wang, Yang},
  doi          = {10.1007/s10489-025-06900-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Stochastic gradient accelerated by negative momentum for training deep neural networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). External information-augmented contrastive learning framework for fake news detection. <em>APIN</em>, <em>55</em>(15), 1-21. (<a href='https://doi.org/10.1007/s10489-025-06807-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake news and information overload on social media has led to increased public confusion and poses a serious threat to social stability. Traditional fake news detection methods typically focus solely on the content of the news itself, making them vulnerable to manipulation by disinformation campaigns. This limitation highlights the need for a more comprehensive approach that incorporates external information to improve detection accuracy. In response to this challenge, we propose a novel framework for fake news detection, named External Information-Augmented Contrastive Learning (EACL). The EACL framework consists of three key modules: (1) the External Information Construction Module, which utilizes entity linking, embedding, and retrieval techniques to analyze news from both factual and public opinion perspectives, thus creating an analysis-friendly environment; (2) the Consistency Feature Extraction Module, which employs a distance-aware signed attention mechanism to model the consistency between news content and external information, while filtering out irrelevant data; and (3) the Comparative Learning Enhancement Module, which constructs positive and negative sample pairs to enhance the learning of semantic differences between fake and real news. Extensive qualitative and quantitative experiments conducted on two real-world datasets demonstrate that EACL achieves impressive accuracy rates of 85.2% and 82.9%, significantly outperforming existing baseline methods. The results further illustrate the effectiveness of integrating external information and contrastive learning in combating misinformation.},
  archive      = {J_APIN},
  author       = {Fang, Xiaochang and Zhang, Huaxiang and Wu, Hongchen and Liu, Li and Yu, Hongzhu and Li, Hongxuan and Jing, Zhaorong},
  doi          = {10.1007/s10489-025-06807-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {External information-augmented contrastive learning framework for fake news detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-stimulus generalized and corrected canonical correlation analysis for enhancing SSVEP detection. <em>APIN</em>, <em>55</em>(15), 1-24. (<a href='https://doi.org/10.1007/s10489-025-06859-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial filter-based calibration-training algorithms play a crucial role in improving the information transfer rate (ITR) of steady-state visual evoked potential based brain-computer interfaces (SSVEP-BCIs). These algorithms optimize spatial filters by suppressing the non-SSVEP related components, thereby enhancing the signal-to-noise ratio (SNR) of electroencephalogram (EEG) signals. However, conventional methods neglect the temporally-varying and spatially-coupled characteristics of EEG signals, leading to inherent ITR bottlenecks in BCIs. To this end, we propose a novel SSVEP detection algorithm, termed as multi-stimulus Generalized and Corrected Canonical Correlation Analysis (msGC3A), which is extended and corrected from the generalized canonical correlation analysis algorithm. Specifically, we develop corrected sine-cosine reference templates that enhance the spatial filters’ generalization capability across multiple stimuli. Moreover, we formulate a weighted correlation coefficient that synergistically integrates both generalized and corrected multi-stimulus templates for further enhancement. Empirical experiments have been conducted on two publicly available benchmark SSVEP datasets, and we compared the ensemble version of our msGC3A algorithm with four state-of-the-art algorithms. The results have shown that our algorithm significantly improves SSVEP detection performance while requiring less calibration data. Furthermore, we also conducted ablation experiments to show the adaptive capacity of employing our algorithm for SSVEP-BCIs.},
  archive      = {J_APIN},
  author       = {Lv, Yanhao and Luo, Tian-jian},
  doi          = {10.1007/s10489-025-06859-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Multi-stimulus generalized and corrected canonical correlation analysis for enhancing SSVEP detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards understanding the optimization mechanisms in deep learning. <em>APIN</em>, <em>55</em>(15), 1-22. (<a href='https://doi.org/10.1007/s10489-025-06875-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we adopt a probability distribution estimation perspective to explore the optimization mechanisms of supervised classification using deep neural networks. We demonstrate that, when employing the Fenchel-Young loss, despite the non-convex nature of the fitting error with respect to the model’s parameters, global optimal solutions can be approximated by simultaneously minimizing both the gradient norm and the structural error. The former can be controlled through gradient descent algorithms. For the latter, we prove that it can be managed by increasing the number of parameters and ensuring parameter independence, thereby providing theoretical insights into mechanisms such as overparameterization and random initialization. Ultimately, the paper validates the key conclusions of the proposed method through empirical results, illustrating its practical effectiveness.},
  archive      = {J_APIN},
  author       = {Qi, Binchuan and Gong, Wei and Li, Li},
  doi          = {10.1007/s10489-025-06875-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Towards understanding the optimization mechanisms in deep learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust low-rank representation with structured similarity learning for multi-label classification. <em>APIN</em>, <em>55</em>(15), 1-24. (<a href='https://doi.org/10.1007/s10489-025-06879-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling high-dimensional, noisy data in multi-label classification is challenging, as feature abundance and noise obscure actual data-label relationships. Traditional approaches often model labels and features independently, limiting dependency modeling and noise reduction. To address this, we propose a unified framework combining low-rank representation using nuclear norm regularization with structured similarity learning. This simultaneously projects features and labels into low-rank spaces while preserving key inter-sample and inter-label relationships through structural constraints, further capturing fine-grained correlations via a learned similarity Matrix. Extensive experiments on five benchmark datasets show our model outperforms state-of-the-art methods, achieving a 16% reduction in Hamming Lossl and a 14% improvement in Micro-F1 on high-dimensional, noisy datasets like CAL500 and Corel16k7, with consistent gains in Macro-F1 and Example-F1. These results demonstrate the model’s strong capability for noisy, high-dimensional multi-label classification.},
  archive      = {J_APIN},
  author       = {Ntaye, Emmanuel and Zhou, Conghua and Liu, Zhifeng and Song, Heping and Issahaku, Fadilul-lah Yassaanah and Shen, Xiang-Jun},
  doi          = {10.1007/s10489-025-06879-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Robust low-rank representation with structured similarity learning for multi-label classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced inverted transformer: Advancing variate token encoding and blending for time series forecasting. <em>APIN</em>, <em>55</em>(15), 1-19. (<a href='https://doi.org/10.1007/s10489-025-06886-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in channel-dependent Transformer-based forecasters highlight the efficacy of variate tokenization for time series forecasting. Despite this progress, challenges remain in handling complex time series. The vanilla Transformer, while effective in certain scenarios, faces limitations in addressing intricate cross-variate interactions and diverse temporal patterns. This paper presents the Enhanced Inverted Transformer (EiT for short), enhancing standard Transformer blocks for advanced modeling and blending of variate tokens. EiT incorporates three key innovations: First, a hybrid multi-patch attention mechanism that adaptively fuses global and local attention maps, capturing both stable and volatile correlations to mitigate overfitting and enrich inter-channel communication. Second, a multi-head feed-forward network with specialized heads for various temporal patterns, enhancing parameter efficiency and contributing to robust multivariate predictions. Third, paired channel normalization applied to each layer, preserving crucial channel-specific statistics and boosting forecasting performance. By integrating these innovations, EiT effectively overcomes limitations and unlocks the potential of variate tokens for accurate and robust multivariate time series forecasting. Extensive evaluations demonstrate that EiT achieves state-of-the-art (SOTA) performance, surpassing the previous method, the inverted Transformer, by an average of 4.4% in Mean Squared Error (MSE) and 3.4% in Mean Absolute Error (MAE) across five challenging long-term forecasting datasets.},
  archive      = {J_APIN},
  author       = {Li, Xin-Yi and Yang, Yu-Bin},
  doi          = {10.1007/s10489-025-06886-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced inverted transformer: Advancing variate token encoding and blending for time series forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid federated unlearning with tuning parameters based on fisher information matrix. <em>APIN</em>, <em>55</em>(15), 1-18. (<a href='https://doi.org/10.1007/s10489-025-06593-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed machine learning approach widely applied in privacy-sensitive scenarios. With the emergence of the “right to be forgotten” and the pursuit of data accuracy, there is an increasing demand to quickly and accurately delete targeted information from models while ensuring model performance. Therefore, federated unlearning has been introduced. Although current federated unlearning methods achieve effective unlearning, they often involve lengthy processes and require servers to store extensive historical update information. We propose a novel rapid federated unlearning method named FedTune. This method leverages the Fisher information matrix computed on the client side to assess the correlation between model parameters and the target data, identifying key parameters for adjustment. Based on the importance of these parameters and the frequency of client participation, FedTune determines appropriate adjustment ratios to increase the classification loss on the target data, thereby reducing the model’s accuracy and achieving effective data unlearning. Finally, the server collaborates with the remaining clients for a few rounds of retraining to restore the overall classification performance rapidly. We evaluated the FedTune method on the MNIST, CIFAR-10, and PURCHASE datasets, considering both fixed and dynamic client selection scenarios in privacy-sensitive and contamination settings. Experimental results show that FedTune reduces the time consumed by the unlearning process and server storage costs of the unlearning algorithm while ensuring model classification accuracy and effective unlearning compared to other unlearning algorithms.},
  archive      = {J_APIN},
  author       = {Zhao, Fengda and Xu, Qianyi and Wang, Hao and Guo, Dingding},
  doi          = {10.1007/s10489-025-06593-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Rapid federated unlearning with tuning parameters based on fisher information matrix},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large-scale group decision-making approach for quality function deployment based on dempster-shafer evidence theory and hierarchical clustering algorithm. <em>APIN</em>, <em>55</em>(15), 1-36. (<a href='https://doi.org/10.1007/s10489-025-06724-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality Function Deployment (QFD) is a classic customer requirements (CRs)-oriented quality management method. However, the increasing complexity and diversity of CRs in the modern society makes it impossible for the traditional QFD approach with a limited number of team members (TMs) to fully satisfy CRs. Therefore, in order to solve the QFD problem in complex environments, this paper proposes an improved QFD method based on Dempster–Shafer evidence theory (D-S theory) and hierarchical clustering algorithm in large-scale group environments. Firstly, utilizing the advantages of D-S theory in information processing and synthesis, the evaluation of quality characteristics (QCs) in the form of probabilistic linguistic term sets (PLTSs) is transformed into basic probability assignments (BPAs) to handle uncertainty more flexibly. Secondly, this paper designs a hierarchical clustering algorithm based on bounded confidence to divide TMs into subgroups, and fully considers the interaction willingness of TMs during the clustering process to ensure the efficiency and accuracy of decision-making. On this basis, the Stepwise Weight Assessment Ratio Analysis (SWARA) method based on distance degree is introduced to calculate the weight of CRs in a more objective way. Then, the Decision-making Trial and Evaluation Laboratory (DEMATEL) method based on D-S theory is used to deeply analyze the mutual influence relationship between QCs to reveal its internal logic. Besides, combined with the psychological expectations of TMs, the disappointment theory is used to prioritize QCs to ensure that products or services are more in line with customer expectations. Finally, this paper applies the proposed method to the development process of mobile health applications (mHealth apps) from the perspective of privacy security, verifying the practicability and superiority of the method. The effectiveness of the method in CRs transformation and product design optimization is further demonstrated through parametric and comparative analyses.},
  archive      = {J_APIN},
  author       = {Liu, Zhengmin and Feng, Xuan and Zhang, Jihao and Zhang, Bo and Wang, Wenxin and Liu, Peide},
  doi          = {10.1007/s10489-025-06724-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-36},
  shortjournal = {Appl. Intell.},
  title        = {A large-scale group decision-making approach for quality function deployment based on dempster-shafer evidence theory and hierarchical clustering algorithm},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way clustering ensemble based on shadowed sets with five approximation regions. <em>APIN</em>, <em>55</em>(15), 1-27. (<a href='https://doi.org/10.1007/s10489-025-06726-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering ensemble is a powerful technique for aggregating multiple clustering results. In order to address the challenge in clustering analysis which was brought by the uncertainty information in the datasets, this work presents a novel three-way clustering ensemble method based on shadowed sets with five approximation regions (3WCE-S5). Firstly, a set of clustering members are generated by fuzzy c-means clustering (FCM). A new shadowed sets is approximated by five regions, named as shadowed sets with five approximation regions (S5). Then, all objects are initially partitioned into five regions according to their membership degrees, which are provided by FCM. Secondly, according to multi-granularity rough sets, objects are further assigned into six approximated regions, namely a core region and five fringe regions. There is a partial order relationship between these six different approximate regions. Finally, the above six regions are processed by the new shadowed sets again to generate the output of three-way clustering. Ten University of California Irvine (UCI) data sets are employed to test the performance of this approach and five comparative methods. Accuracy (ACC), adjusted rand index (ARI), normalized mutual information (NMI) and time cost are utilized to quantify the clustering results.},
  archive      = {J_APIN},
  author       = {Yi, Huangjian and Guo, Dongkai and Zhang, Qinran and He, Xiaowei and Ren, Ruisi},
  doi          = {10.1007/s10489-025-06726-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {Three-way clustering ensemble based on shadowed sets with five approximation regions},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic interaction-enhanced encoding network for math word problem solving. <em>APIN</em>, <em>55</em>(15), 1-21. (<a href='https://doi.org/10.1007/s10489-025-06850-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving math word problems (MWPs) requires machines to understand not only the literal meaning of text but also the abstract logic and mathematical reasoning embedded within it. However, existing models often lack explicit reasoning capabilities for semantic information, particularly when dealing with complex math word problem texts. Additionally, these models tend to embed all kinds of information without fine-grained selection, which may introduce unexpected noise for mathematical expression generation. To address these challenges, we propose a Semantic Interaction-Enhanced Encoding Network (SIEN) for math expression generation is proposed in this paper. Firstly, SIEN constructs a semantic role interaction graph for each problem and employs a graph attention neural network to learn interaction and semantic information, offering a more structured and enriched view of the math word problem text. Secondly, SIEN introduces a multi-channel adapter module that simultaneously learns comprehensive contextual information from numeric information channel, hierarchical semantic information channel, and interaction information channel. Furthermore, SIEN introduces a dynamic weighting mechanism that adjusts the information weight from each channel to prioritize relevant information and reduce noise. Experimental results on three public benchmark datasets demonstrate that SIEN achieves significant performance improvement over other state-of-the-art baseline models.},
  archive      = {J_APIN},
  author       = {Xiao, Lingsheng and Chen, Yuzhong and Liu, Zhanghui and Zhong, Jiayuan and Dong, Yu},
  doi          = {10.1007/s10489-025-06850-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Semantic interaction-enhanced encoding network for math word problem solving},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KRMNet: Learning core representations for partial discharge pattern recognition via masked autoencoders and mixed position coding. <em>APIN</em>, <em>55</em>(15), 1-22. (<a href='https://doi.org/10.1007/s10489-025-06899-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial discharge pattern recognition (PDPR) is a crucial cornerstone for condition monitoring and safe operation of electrical devices. It has become an important hotspot in the field of energy systems. However, it faces several challenges, including noise interference, signal complexity, and difficulty in data labeling. This study proposes an efficient multi-scale masked autoencoder (MAE) network (KRMNet) to effectively address these challenges. KRMNet learns common and important multi-scale features and long-range semantic dependencies of partial discharge signals. Furthermore, by using the MAE structure and the transformer as the backbone, the model extracts key distinguishing features from phase-resolved partial discharge (PRPD) signals with background noise, interference, and labeling issues in an efficient and self-supervised manner. In addition, the efficient multi-scale module uses an efficient multi-scale attention mechanism to aggregate key information from multiple feature dimensions. The integration of the efficient multi-scale attention mechanism and contrastive learning methods improves the model’s ability to distinguish key information and resist interference. Experiments on two challenging PRPD datasets show that our proposed KRMNet achieves detection accuracies of 88.5% and 90.2% on noisy (ECPD) and clean (PUPD) datasets, respectively. This finding suggests that the method faces challenges in effectively managing noise interferences and missing labels.},
  archive      = {J_APIN},
  author       = {Deng, Yi and Chen, Jiawen and Xie, Quan and Tan, Dapeng and Liu, Hai},
  doi          = {10.1007/s10489-025-06899-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {15},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {KRMNet: Learning core representations for partial discharge pattern recognition via masked autoencoders and mixed position coding},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ar">AR - 2</h2>
<ul>
<li><details>
<summary>
(2025). Persistent multi-resource coverage with heterogeneous multi-robot teams. <em>AR</em>, <em>49</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10514-025-10207-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-robot teams provide an effective solution for delivering multiple types of goods, such as food or medicine, to various locations of demand. This work presents a Voronoi-based coverage control approach to the multi-resource allocation problem, and considers a heterogeneous team comprising robots with different resource types and capacities. The team must supply resources to multiple demand locations. Demand of resources may change over time, and fluctuate in overall demand, which is represented over the environment as a time-varying density function. From the demand density, robots minimize their respective locational cost, adapting and moving to areas of higher demand. Robots must adhere to supply constraints and replenish resources over time to ensure persistent resource coverage. This paper therefore investigates how to enable persistent deployments, wherein robots must continually alternate between serving demand or replenishing resources. We explore four algorithms for resource replenishment, which vary in communication, forecasting, and information assumptions. Simulations and hardware experiments demonstrate a need-based auction algorithm, which aims to minimize service blackouts, produces the best performance for a heterogeneous team. We also present a discussion on acceptable alternatives for homogeneous teams without communication.},
  archive      = {J_AR},
  author       = {Coffey, Mela and Pierson, Alyssa},
  doi          = {10.1007/s10514-025-10207-6},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Auton. Robot.},
  title        = {Persistent multi-resource coverage with heterogeneous multi-robot teams},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive ergodic search with energy-aware scheduling for persistent multi-robot missions. <em>AR</em>, <em>49</em>(4), 1-24. (<a href='https://doi.org/10.1007/s10514-025-10215-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous robots are increasingly deployed for long-term information-gathering tasks, which pose two key challenges: planning informative trajectories in environments that evolve across space and time, and ensuring persistent operation under energy constraints. This paper presents a unified framework, mEclares, that addresses both challenges through adaptive ergodic search and energy-aware scheduling in multi-robot systems. Our contributions are two-fold: (1) we model real-world variability using stochastic spatiotemporal environments, where the underlying information evolves continuously over space and time under process noise. To guide exploration, we construct a target information spatial distribution (TISD) based on clarity, a metric that captures the decay of information in the absence of observations and highlights regions of high uncertainty; and (2) we introduce Robust-meSch ( RmeSch ), an online scheduling method that enables persistent operation by coordinating rechargeable robots sharing a single mobile charging station. Unlike prior work, our approach avoids reliance on preplanned schedules, static or dedicated charging stations, and simplified robot dynamics. Instead, the scheduler supports general nonlinear models, accounts for uncertainty in the estimated position of the charging station, and handles central node failures. The proposed framework is validated through real-world hardware experiments, and feasibility guarantees are provided under specific assumptions. [Code: https://github.com/kalebbennaveed/mEclares-main.git] [Experiment Video: https://www.youtube.com/watch?v=dmaZDvxJgF8]},
  archive      = {J_AR},
  author       = {Naveed, Kaleb Ben and Agrawal, Devansh R. and Kumar, Rahul and Panagou, Dimitra},
  doi          = {10.1007/s10514-025-10215-6},
  journal      = {Autonomous Robots},
  month        = {12},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Auton. Robot.},
  title        = {Adaptive ergodic search with energy-aware scheduling for persistent multi-robot missions},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="alg">Alg - 5</h2>
<ul>
<li><details>
<summary>
(2025). On the parameterized complexity of eulerian strong component arc deletion. <em>Alg</em>, <em>87</em>(11), 1669-1709. (<a href='https://doi.org/10.1007/s00453-025-01336-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the Eulerian Strong Component Arc Deletion problem, where the input is a directed multigraph and the goal is to delete the minimum number of arcs to ensure every strongly connected component of the resulting digraph is Eulerian. This problem is a natural extension of the Directed Feedback Arc Set problem and is also known to be motivated by certain scenarios arising in the study of housing markets. The complexity of the problem, when parameterized by solution size (i.e., size of the deletion set), has remained unresolved and has been highlighted in several papers. In this work, we answer this question by ruling out (subject to the usual complexity assumptions) a fixed-parameter algorithm (FPT algorithm) for this parameter and conduct a broad analysis of the problem with respect to other natural parameterizations. We prove both positive and negative results. Among these, we demonstrate that the problem is also hard (W[1]-hard or even para-NP-hard) when parameterized by either treewidth or maximum degree alone. Complementing our lower bounds, we establish that the problem is in XP when parameterized by treewidth and FPT when parameterized either by both treewidth and maximum degree or by both treewidth and solution size. We show that on simple digraphs, these algorithms have near-optimal asymptotic dependence on the treewidth assuming the Exponential Time Hypothesis.},
  archive      = {J_Alg},
  author       = {Blažej, Václav and Jana, Satyabrata and Ramanujan, M. S. and Strulo, Peter},
  doi          = {10.1007/s00453-025-01336-6},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1669-1709},
  shortjournal = {Algorithmica},
  title        = {On the parameterized complexity of eulerian strong component arc deletion},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ShockHash: Near optimal-space minimal perfect hashing beyond brute-force. <em>Alg</em>, <em>87</em>(11), 1620-1668. (<a href='https://doi.org/10.1007/s00453-025-01321-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A minimal perfect hash function (MPHF) maps a set S of n keys to the first n integers without collisions. There is a lower bound of $$n\log _2e-\mathcal {O}(\log n) \approx 1.44n$$ bits needed to represent an MPHF. This can be reached by a brute-force algorithm that tries $$e^n$$ hash function seeds in expectation and stores the first seed that leads to an MPHF. The most space-efficient previous algorithms for constructing MPHFs all use such a brute-force approach as a basic building block. In this paper, we introduce ShockHash – Small, heavily overloaded cuckoo hash tables for minimal perfect hashing. ShockHash uses two hash functions $$h_0$$ and $$h_1$$ , hoping for the existence of a function $$f : S \rightarrow \{0,1\}$$ such that $$x \mapsto h_{f(x)}(x)$$ is an MPHF on S. It then uses a 1-bit retrieval data structure to store f using $$n + o(n)$$ bits. In graph terminology, ShockHash generates n-edge random graphs until stumbling on a pseudoforest – where each component contains as many edges as nodes. Using cuckoo hashing, ShockHash then derives an MPHF from the pseudoforest in linear time. We show that ShockHash needs to try only about $$(e/2)^n \approx 1.359^n$$ seeds in expectation. This reduces the space for storing the seed by roughly n bits (maintaining the asymptotically optimal space consumption) and speeds up construction by almost a factor of $$2^n$$ compared to brute-force. Bipartite ShockHash reduces the expected construction time again to about $$1.166^n$$ by maintaining a pool of candidate hash functions and checking all possible pairs. Using ShockHash as a building block within the RecSplit framework we obtain ShockHash-RS, which can be constructed up to 3 orders of magnitude faster than competing approaches. ShockHash-RS can build an MPHF for 10 million keys with 1.489 bits per key in about half an hour. When instead using ShockHash after an efficient k-perfect hash function, it achieves space usage similar to the best competitors, while being significantly faster to construct and query.},
  archive      = {J_Alg},
  author       = {Lehmann, Hans-Peter and Sanders, Peter and Walzer, Stefan},
  doi          = {10.1007/s00453-025-01321-z},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1620-1668},
  shortjournal = {Algorithmica},
  title        = {ShockHash: Near optimal-space minimal perfect hashing beyond brute-force},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving tight $$O(4^k)$$ runtime bounds on jumpk by proving that genetic algorithms evolve near-maximal population diversity. <em>Alg</em>, <em>87</em>(11), 1564-1619. (<a href='https://doi.org/10.1007/s00453-025-01323-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $$\textsc {Jump} _k$$ benchmark was the first problem for which crossover was proven to give a speed-up over mutation-only evolutionary algorithms. Jansen and Wegener (Algorithmica 2002) proved an upper bound of $$O(\textrm{poly}(n) + 4^k/p_c)$$ for the ( $$\mu $$ +1) Genetic Algorithm (( $$\mu $$ +1) GA), but only for unrealistically small crossover probabilities $$p_c$$ . To this date, it remains an open problem to prove similar upper bounds for realistic $$p_c$$ ; the best known runtime bound, in terms of function evaluations, for $$p_c = \Omega (1)$$ is $$O((n/\chi )^{k-1})$$ , $$\chi $$ a positive constant. We provide a novel approach and analyse the evolution of the population diversity, measured as sum of pairwise Hamming distances, for a variant of the ( $$\mu $$ +1) GA on $$\textsc {Jump} _k$$ . The ( $$\mu $$ +1)- $${\lambda _c}$$ -GA creates one offspring in each generation either by applying mutation to one parent or by applying crossover $${\lambda _c}$$ times to the same two parents (followed by mutation), to amplify the probability of creating an accepted offspring in generations with crossover. We show that population diversity in the ( $$\mu $$ +1)- $${\lambda _c}$$ -GA converges to an equilibrium of near-perfect diversity. This yields an improved time bound of $$O(\mu n \log (\mu ) + 4^k)$$ function evaluations for a range of k under the mild assumptions $$p_c = O(1/k)$$ and $$\mu \in \Omega (kn)$$ . For all constant k, the restriction is satisfied for some $$p_c = \Omega (1)$$ and it implies that the expected runtime for all constant k and an appropriate $$\mu = \Theta (kn)$$ is bounded by $$O(n^2 \log n)$$ , irrespective of k. For larger k, the expected time of the ( $$\mu $$ +1)- $${\lambda _c}$$ -GA is $$\Theta (4^k)$$ , which is tight for a large class of unbiased black-box algorithms and faster than the original ( $$\mu $$ +1) GA by a factor of $$\Omega (1/p_c)$$ . We also show that our analysis can be extended to other unitation functions such as $$\textsc {Jump} _{k, \delta }$$ and Hurdle.},
  archive      = {J_Alg},
  author       = {Opris, Andre and Lengler, Johannes and Sudholt, Dirk},
  doi          = {10.1007/s00453-025-01323-x},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1564-1619},
  shortjournal = {Algorithmica},
  title        = {Achieving tight $$O(4^k)$$ runtime bounds on jumpk by proving that genetic algorithms evolve near-maximal population diversity},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smoothed analysis of the 2-opt heuristic for the TSP under gaussian noise. <em>Alg</em>, <em>87</em>(11), 1518-1563. (<a href='https://doi.org/10.1007/s00453-025-01335-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2-opt heuristic is a very simple local search heuristic for the traveling salesperson problem. In practice it usually converges quickly to solutions within a few percentages of optimality. In contrast to this, its running-time is exponential and its approximation performance is poor in the worst case. Englert, Röglin, and Vöcking (Algorithmica, 2014) provided a smoothed analysis in the so-called one-step model in order to explain the performance of 2-opt on d-dimensional Euclidean instances, both in terms of running-time and in terms of approximation ratio. However, translating their results to the classical model of smoothed analysis, where points are perturbed by Gaussian distributions with standard deviation $$\sigma $$ , yields only weak bounds. We prove bounds that are polynomial in n and $$1/\sigma $$ for the smoothed running-time with Gaussian perturbations. In addition, our analysis for Euclidean distances is much simpler than the existing smoothed analysis. Furthermore, we prove a smoothed approximation ratio of $$O(\log (1/\sigma ))$$ . This bound is almost tight, as we also provide a lower bound of $$\Omega (\frac{\log n}{\log \log n})$$ for $$\sigma = O(1/\sqrt{n})$$ . Our main technical novelty here is that, different from existing smoothed analyses, we do not separately analyze objective values of the global and local optimum on all inputs (which only allows for a bound of $$O(1/\sigma )$$ ), but simultaneously bound them on the same input.},
  archive      = {J_Alg},
  author       = {Künnemann, Marvin and Manthey, Bodo and Veenstra, Rianne},
  doi          = {10.1007/s00453-025-01335-7},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1518-1563},
  shortjournal = {Algorithmica},
  title        = {Smoothed analysis of the 2-opt heuristic for the TSP under gaussian noise},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting double coverage for k-server via imperfect predictions. <em>Alg</em>, <em>87</em>(11), 1477-1517. (<a href='https://doi.org/10.1007/s00453-025-01333-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the online k-server problem in a learning-augmented setting. While in the traditional online model, an algorithm has no information about the request sequence, we assume that there is given some advice (for example, machine-learned predictions) on an algorithm’s decision. There is, however, no guarantee on the quality of the prediction, and it might be far from being correct. Our main result is a learning-augmented variation of the well-known Double Coverage algorithm for k-server on the line (Chrobak et al. in SIAM J Discret Math 4(2):172–181, 1991) in which we integrate predictions as well as our trust into their quality. We give an error-dependent worst-case performance guarantee, which is a function of a user-defined confidence parameter, and which interpolates smoothly between an optimal performance in case that all predictions are correct, and the best-possible performance regardless of the prediction quality. When given good predictions, we improve upon known lower bounds for online algorithms without advice. We further show that our algorithm achieves for any k almost optimal guarantees, within a class of deterministic learning-augmented algorithms respecting local and memoryless properties. Our algorithm outperforms a previously proposed (more general) learning-augmented algorithm. It is noteworthy that the previous algorithm crucially exploits memory, whereas our algorithm is memoryless. Finally, we demonstrate in experiments the practicability and the superior performance of our algorithm on real-world data.},
  archive      = {J_Alg},
  author       = {Lindermayr, Alexander and Megow, Nicole and Simon, Bertrand},
  doi          = {10.1007/s00453-025-01333-9},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1477-1517},
  shortjournal = {Algorithmica},
  title        = {Boosting double coverage for k-server via imperfect predictions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="bcyb">BCYB - 13</h2>
<ul>
<li><details>
<summary>
(2025). Artificial intelligence meets brain theory (again). <em>BCYB</em>, <em>119</em>(4), 1-11. (<a href='https://doi.org/10.1007/s00422-025-01013-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After noting the cybernetic origins of Kybernetik/ Biological Cybernetics, we respond to the Editorial by Fellous et al. (2025) and then analyze talks from the NIH BRAIN NeuroAI 2024 Workshop to get one “snapshot” of the state of the conversation between Artificial intelligence (AI) and brain theory (BT). Key recommendations going beyond the earlier Editorial are that: (i) Successes in fitting ANNs to increasingly large neuroscience datasets must not distract us from the quixotic but demanding quest to understand “how the brain works” and discover underlying brain (and AI) operating principles. (ii) We must integrate functional and structural analyses in exploring systems of systems, integrating structures (e.g., brain regions, cortical modules) and functions (e.g., schemas for perception, action and cognition) that bridge between neural circuitry and patterns of behavior. (iii) We must study the diversity of intelligences exhibited by animals in their strategies for survival and not only the disembodied employment of language and reasoning. Finally and briefly, we note the urgency of assessing the societal implications of an age of increasingly pervasive human-machine symbiosis.},
  archive      = {J_BCYB},
  author       = {Arbib, Michael A.},
  doi          = {10.1007/s00422-025-01013-5},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Biol. Cybern.},
  title        = {Artificial intelligence meets brain theory (again)},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coding odor modality in piriform cortex efficiently with low-dimensional subspaces: A shared covariance decoding approach. <em>BCYB</em>, <em>119</em>(4), 1-20. (<a href='https://doi.org/10.1007/s00422-025-01015-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental question in neuroscience is how sensory signals are decoded from noisy cortical activity. We address this question in the olfactory system, decoding the route by which odorants arrive into the nasal cavity: through the nostrils (orthonasal), or through the back of the throat (retronasal). We recently showed with modeling and novel experiments on anesthetized rats that orthonasal versus retronasal modality information is encoded in the olfactory bulb (OB, a pre-cortical region). However, key questions remain: is modality information transmitted from OB to anterior piriform cortex (aPC)? How can this information be extracted from a much noisier cortical population with overall less firing? With simultaneous spike recordings of populations of neurons in OB and aPC, we show that an unsupervised and biologically plausible algorithm, Shared Covariance Decoding (SCD), can indeed linearly encode modality in low dimensional subspaces. Specifically, SCD improves encoding of ortho/retro in aPC compared to Fisher’s linear discriminant analysis (LDA). Consistent with our theoretical analysis, when noise correlations between OB and aPC are low and OB well-encodes modality, modality in aPC tends to be encoded optimally with SCD. We observe that with several algorithms (LDA, SCD, optimal) the decoding accuracy distributions are invariant when GABA $$_\text {A}$$ (ant-)agonists (bicuculline and muscimol) are applied to OB, which is consistent with invariance in population firing in aPC. Overall, we show modality information can be encoded efficiently in piriform cortex.},
  archive      = {J_BCYB},
  author       = {Selb, Delaney M. and Barreiro, Andrea K. and Gautam, Shree Hari and Shew, Woodrow L. and Ly, Cheng},
  doi          = {10.1007/s00422-025-01015-3},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Biol. Cybern.},
  title        = {Coding odor modality in piriform cortex efficiently with low-dimensional subspaces: A shared covariance decoding approach},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Basal ganglia: An amplifier for preparatory activity in motor control. <em>BCYB</em>, <em>119</em>(4), 1-14. (<a href='https://doi.org/10.1007/s00422-025-01016-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The basal ganglia make a significant contribution to the generation of motor behavior through their involvement in the descending motor pathways. Gaining insight into how the motor cortex produces motor patterns is also a key to understanding the function of the basal ganglia. The population dynamics approach is convenient for the reevaluation of the behavior of the motor cortex and also the roles of other components in the motor system. Here, it is proposed that the basal ganglia amplify preparatory activity in the motor cortex with the modulatory effect of phasic dopamine release. The influence of the basal ganglia is tested with a computational model on a center-out-reaching task. The results show that the basal ganglia facilitate movement initiation and increase the robustness of the behavior. These results, based on the perspective of population dynamics, may improve our understanding of the role of the basal ganglia in motor control and the symptoms of dopamine-related conditions in neurodegenerative diseases of motor control.},
  archive      = {J_BCYB},
  author       = {Çağdaş, Serhat and Şengör, Neslihan Serap},
  doi          = {10.1007/s00422-025-01016-2},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Biol. Cybern.},
  title        = {Basal ganglia: An amplifier for preparatory activity in motor control},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating buccal mass planar mechanics and anatomical features improves neuromechanical modeling of aplysia feeding behavior. <em>BCYB</em>, <em>119</em>(4), 1-29. (<a href='https://doi.org/10.1007/s00422-025-01017-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To understand how behaviors arise in animals, it is necessary to investigate both the neural circuits and the biomechanics of the periphery. A tractable model system for studying multifunctional control is the feeding apparatus of the marine mollusk Aplysia californica. Previous in silico and in roboto models have investigated how the nervous and muscular systems interact in this system. However, these models are still limited in their ability to match in vivo data both qualitatively and quantitatively. We introduce a new neuromechanical model of Aplysia feeding that combines a modified version of a previously developed neural model with a novel biomechanical model that better reflects the anatomy and kinematics of Aplysia feeding. The model was calibrated using a combination of previously measured biomechanical parameters and hand-tuning to behavioral data. Using this model, simulated feeding experiments were conducted, and the resulting behavioral metrics were compared to animal data. The model successfully produces three key behaviors seen in Aplysia and demonstrates a good quantitative agreement with biting and swallowing behaviors. Additional work is needed to match rejection behavior quantitatively and to reflect qualitative observations related to the relative contributions of two key muscles, the hinge and I3. Future improvements will focus on incorporating the effects of deformable 3D structures in the simulated buccal mass.},
  archive      = {J_BCYB},
  author       = {Bennington, Michael J. and Liao, Ashlee S. and Sukhnandan, Ravesh and Kundu, Bidisha and Rogers, Stephen M. and Gill, Jeffrey P. and McManus, Jeffrey M. and Sutton, Gregory P. and Chiel, Hillel J. and Webster-Wood, Victoria A.},
  doi          = {10.1007/s00422-025-01017-1},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Biol. Cybern.},
  title        = {Incorporating buccal mass planar mechanics and anatomical features improves neuromechanical modeling of aplysia feeding behavior},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding human visual foraging: A review. <em>BCYB</em>, <em>119</em>(4), 1-16. (<a href='https://doi.org/10.1007/s00422-025-01020-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual foraging tasks provide great insights into how organisms orient within their visual environment. These tasks are useful for simultaneously investigating concepts often addressed separately, such as attentional guidance, working memory, and strategy. Foraging tasks enable the study of continuous real-world visual exploration and how information about the environment is gathered. They yield rich and multifaceted datasets and can provide insights into the mechanisms of visual attention, visual search, visual memory, and other cognitive factors in a setting more closely resembling how we employ those factors in the real world. We provide a review of the literature and discuss the pros and cons of different ways of understanding and explaining human foraging. A popular approach has been to test whether foraging performance fits certain mathematical rules, such as the marginal value theorem, or so-called Lévy flights. We question the usefulness of such approaches, in particular in the context human foraging (or the foraging of any organism with a sizeable nervous system). The goals and rewards that determine foraging behavior are multifaceted, and understanding those will bring us closer to understanding how humans interact with the world. The usefulness of assessing whether performance falls in line with a particular mathematical rule is, in our opinion, questionable and resources may have been wasted on trying to answer such questions, instead of focusing on the rich insights that foraging data provides about vision, attentional selection, visual short-term memory and the gathering of information.},
  archive      = {J_BCYB},
  author       = {Kristjánsson, Tómas and Kristjánsson, Árni},
  doi          = {10.1007/s00422-025-01020-6},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Biol. Cybern.},
  title        = {Understanding human visual foraging: A review},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay suppression control of $$\beta$$ -oscillations: A proposal for dual-target adaptive deep brain stimulation on STN-GPe network model. <em>BCYB</em>, <em>119</em>(4), 1-18. (<a href='https://doi.org/10.1007/s00422-025-01021-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s Disease (PD) is a neurodegenerative disorder associated with Basal Ganglia (BG) dysfunction, where abnormal neuronal $$\beta$$ -oscillations ( $$12 \sim 30$$ Hz) have been shown to correlate with motor symptoms. Non-pharmacological therapies are based on Deep Brain Stimulation (DBS), delivering electric current waveform with constant frequency and amplitude to BG regions, commonly single targeting either the Subthalamic Nucleus (STN) or the Globus Palidus (GP). More recently, studies have also employed dual-target stimulation, which may synergistically increase therapeutic benefit. Additionally, novel designs of adaptive DBS (aDBS) with closed-loop feedback aim to further enhance efficiency when compared to open-loop procedures, while enabling it to deal with patient variability and disease progression. In this way, here we propose a dual-target aDBS controller, considering a computational model for STN-GPe circuit. Its goal is to suppress the mentioned oscillations at any stage of illness development and synaptic and connectivity parameters ranges, hence in principle adjustable to distinct patient conditions. The control method generally addresses the STN-GPe circuit as a nonlinear-delayed dynamical system, employing a robust technique of delay compensation. The controller architecture relies on recording and stimulating both STN and GPe, also using a straightforward predictor algorithm to select the external inputs for the STN-GPe circuit. The stimulation inputs consist of initial simple brief pulses that suppress or shift the onset of $$\beta$$ -oscillations. Then, weak amplitude signals are enough to sustain the achieved stabilization. The protocol has been fully simulated considering an in silico model. Within such theoretical framework, it was shown to be extremely efficient if the processing time is not too long. The dual-target aDBS put forward here is based on implementable technologies, thus potentially amenable to novel strategies for biomedical close-loop approaches. But concrete challenges for doing so are also discussed.},
  archive      = {J_BCYB},
  author       = {Polli, Jaderson G. and Kolbl, Florian and da Luz, M. G. E. and Lanusse, P.},
  doi          = {10.1007/s00422-025-01021-5},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Biol. Cybern.},
  title        = {Delay suppression control of $$\beta$$ -oscillations: A proposal for dual-target adaptive deep brain stimulation on STN-GPe network model},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Theoretical considerations on models of vestibular self-motion perception as inherent in computational frameworks of motion sickness. <em>BCYB</em>, <em>119</em>(4), 1-19. (<a href='https://doi.org/10.1007/s00422-025-01018-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines self-motion perception incorporated into motion sickness models. Research on modeling self-motion perception and motion sickness has advanced independently, though both are thought to share neural mechanisms, making the construction of a unified model opportune. Models based on the Subjective Vertical Conflict (SVC) theory, a refinement of the neural mismatch theory, have primarily focused on motion sickness, with limited validation for self-motion perception. Emerging studies have begun evaluating the perceptual validity of these models, suggesting that some models can reproduce perception in specific paradigms, while they often struggle to jointly capture motion perception and sickness. One prior study demonstrated that one of the SVC models could replicate illusory tilt during centrifugation, while others produced unrealistic responses, such as persistent tilt after motion cessation. In reality, under steady-state conditions such as being motionless, perceived motion is expected to settle to an appropriate state regardless of prior states. Based on the idea that this behavior is closely related to the equilibrium points and stability of the model dynamics, this study theoretically analyzed 6DoF-SVC models with a focus on them. Results confirmed that only one model ensures convergence from any state to a unique equilibrium point corresponding to plausible perception. In contrast, other SVC models and a conventional self-motion perception model converged to values dependent on earlier states. Further analysis showed that only this model captured both the somatogravic and Ferris wheel illusion. In conclusion, this 6DoF-SVC model unifies motion perception and sickness modeling, with theoretical convergence of the perceptual state.},
  archive      = {J_BCYB},
  author       = {Wada, Takahiro and Bos, Jelte E.},
  doi          = {10.1007/s00422-025-01018-0},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Biol. Cybern.},
  title        = {Theoretical considerations on models of vestibular self-motion perception as inherent in computational frameworks of motion sickness},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational model to reproduce fingertip trajectories and arm postures during human three-joint arm movements: Minimum muscle-stress-change model. <em>BCYB</em>, <em>119</em>(4), 1-16. (<a href='https://doi.org/10.1007/s00422-025-01022-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies on the computational principle for solving the movement selection problem for the human arm have primarily focused on hand trajectories associated with the two-joint movements of the shoulder and elbow joints. Further, only a few computational models, that consider the musculoskeletal system, have been investigated. From this perspective, a minimum muscle-stress-change model was evaluated for the fingertip trajectories and arm postures during three-joint movements in the horizontal plane, including wrist joint rotation. A musculoskeletal model of a three-joint arm with eight muscles was used to perform the optimization calculations that determine the optimal arm movements. Results show that the computational model can reproduce the measured fingertip trajectories and arm postures to an equal or greater extent compared with the minimum angular-jerk model and the minimum torque-change model. Furthermore, the errors of the minimum muscle-stress-change model remained small for different values of joint viscosity, physiological cross-sectional areas, and moment arms, resulting in a small dependency of these parameters. In contrast, the minimum torque-change model resulted in considerable errors under low-viscosity conditions. Consequently, the minimum muscle-stress-change model has emerged as a promising candidate for elucidating the computational principle.},
  archive      = {J_BCYB},
  author       = {Katayama, Masazumi},
  doi          = {10.1007/s00422-025-01022-4},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Biol. Cybern.},
  title        = {Computational model to reproduce fingertip trajectories and arm postures during human three-joint arm movements: Minimum muscle-stress-change model},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal position fuzzy control for coordinated movement of the ring and little fingers in an impaired human hand. <em>BCYB</em>, <em>119</em>(4), 1-16. (<a href='https://doi.org/10.1007/s00422-025-01023-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dexterity of the human hand is largely due to its multiple degrees of freedom. However, coordinating the movements of the ring and little fingers independently can be challenging because of the biomechanical and neurological interdependencies between them. This research presents a cascade control system based on fuzzy logic to manage the dynamic movements of these fingers within a simulated biomechanical model of a human hand. A mathematical model that incorporates transfer functions and state-space representations has been developed for the fingers. The fuzzy logic controller is designed to address the nonlinearity of the biomechanical model, optimizing both the transient and steady-state response parameters. The simulation results indicate that the system achieves a rise time of 0.6 s and a peak time of 0.3 s for the ring finger, with an overshoot of 5%. The little finger, on the other hand, exhibits an overshoot of less than 0.6% and a settling time ranging from 1 to 2.6 s across various joints. Overall, the proposed control system successfully coordinates finger movements, achieving a stable response within 3.5 s and minimal disturbances. These findings represent significant advancements in precision and robustness for prosthetic and robotic hand systems, providing a promising foundation for assistive technologies aimed at fine motor control rehabilitation.},
  archive      = {J_BCYB},
  author       = {Iqbal, Maryam and Rasool, Sabtain},
  doi          = {10.1007/s00422-025-01023-3},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Biol. Cybern.},
  title        = {Optimal position fuzzy control for coordinated movement of the ring and little fingers in an impaired human hand},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural models and algorithms for sensorimotor control of an octopus arm. <em>BCYB</em>, <em>119</em>(4), 1-31. (<a href='https://doi.org/10.1007/s00422-025-01019-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a biophysically realistic model of a soft octopus arm with internal musculature is presented. The modeling is motivated by experimental observations of sensorimotor control where an arm localizes and reaches a target. Major contributions of this article are: (i) development of models to capture the mechanical properties of arm musculature, the electrical properties of the arm peripheral nervous system (PNS), and the coupling of PNS with muscular contractions; (ii) modeling the arm sensory system, including chemosensing and proprioception; and (iii) algorithms for sensorimotor control, which include a novel feedback neural motor control law for mimicking target-oriented arm reaching motions, and a novel consensus algorithm for solving sensing problems such as locating a food source from local chemical sensory information (exogenous) and arm deformation information (endogenous). Several analytical results, including rest-state characterization and stability properties of the proposed sensing and motor control algorithms, are provided. Numerical simulations demonstrate the efficacy of our approach. Qualitative comparisons against observed arm rest shapes and target-oriented reaching motions are also reported.},
  archive      = {J_BCYB},
  author       = {Wang, Tixian and Halder, Udit and Gribkova, Ekaterina and Gillette, Rhanor and Gazzola, Mattia and Mehta, Prashant G.},
  doi          = {10.1007/s00422-025-01019-z},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-31},
  shortjournal = {Biol. Cybern.},
  title        = {Neural models and algorithms for sensorimotor control of an octopus arm},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of transcranial alternating current stimulation on spike train correlation in two-compartment model neurons. <em>BCYB</em>, <em>119</em>(4), 1-17. (<a href='https://doi.org/10.1007/s00422-025-01025-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated spiking has been widely found in large population of neurons and been linked to neural coding. Transcranial alternating current stimulation (tACS) is a promising non-invasive brain stimulation technique that can modulate the spiking activity of neurons. Despite its growing application, the tACS effects on the temporal correlation between spike trains are still not fully understood. In this study, we use a pair of unconnected two-compartment model neurons of the integrate-and-fire (IF) type to simulate the correlated spike trains driven by shared fluctuating dendritic inputs and exposed to weak alternating electric fields. Our results show that the output correlation increases with field intensity, but increases and then decreases with field frequency, displaying thus a frequency resonance. Through varying somatic and dendritic morphologies, we demonstrate that morphological differences between the soma and dendrites fundamentally shape the correlation-frequency resonance, with more pronounced differences yielding stronger resonance effects. Moreover, the anti-phase sinusoidal modulations induced by tACS at the soma and dendrite promote this correlation-frequency resonance, particularly when dendritic fluctuations exhibit a large mean value. We further examine the tACS effects on output correlation in biophysically and morphologically realistic pyramidal model neurons, revealing similar patterns to those observed in the two-compartment models. Our findings provide new insights into how tACS modulates the correlated spike trains and highlight the critical role of morphological differences between the soma and dendrites in determining the frequency-dependent output correlation. These predictions should be taken into consideration when understanding the tACS effects on population correlation and population coding.},
  archive      = {J_BCYB},
  author       = {Huang, Xuelin and Wei, Xile and Wang, Jiang and Yi, Guosheng},
  doi          = {10.1007/s00422-025-01025-1},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Biol. Cybern.},
  title        = {Effects of transcranial alternating current stimulation on spike train correlation in two-compartment model neurons},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Obituary for professor jack d. cowan. <em>BCYB</em>, <em>119</em>(4), 1-2. (<a href='https://doi.org/10.1007/s00422-025-01024-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BCYB},
  author       = {Ermentrout, Bard and Sejnowski, Terrence J. and Thomas, Peter J. and Wilson, Hugh R.},
  doi          = {10.1007/s00422-025-01024-2},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-2},
  shortjournal = {Biol. Cybern.},
  title        = {Obituary for professor jack d. cowan},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Molecular dynamics simulations of proteins: An in-depth review of computational strategies, structural insights, and their role in medicinal chemistry and drug development. <em>BCYB</em>, <em>119</em>(4), 1-24. (<a href='https://doi.org/10.1007/s00422-025-01026-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular dynamics (MD) simulations have emerged as a powerful and extensively employed tool in biomedical research, offering insights into intricate biomolecular processes such as structural flexibility and molecular interactions, and playing a pivotal role in the development of therapeutic approaches. Although MD techniques are applied to a variety of biomolecules including DNA, RNA, proteins, and their assemblies, this review focuses specifically on the role of MD in elucidating protein behavior and their interactions with inhibitors across different disease contexts. The selection of an appropriate force field is essential, as it greatly influences the reliability of simulation outcomes. Widely adopted MD software packages such as GROMACS, DESMOND, and AMBER leverage rigorously tested force fields and have shown consistent performance across diverse biological applications. Despite current successes, challenges remain in narrowing the gap between computational models and actual cellular conditions. The integration of machine learning and deep learning technologies is expected to accelerate progress in this evolving field.},
  archive      = {J_BCYB},
  author       = {Farhadi, Bita and Beygisangchin, Mahnoush and Ghamari, Nakisa and Jakmunee, Jaroon and Tang, Tang},
  doi          = {10.1007/s00422-025-01026-0},
  journal      = {Biological Cybernetics},
  month        = {12},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Biol. Cybern.},
  title        = {Molecular dynamics simulations of proteins: An in-depth review of computational strategies, structural insights, and their role in medicinal chemistry and drug development},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="cc">CC - 13</h2>
<ul>
<li><details>
<summary>
(2025). Smartphone selection with picture fuzzy modified combined compromise solutions. <em>CC</em>, <em>17</em>(5), 1-32. (<a href='https://doi.org/10.1007/s12559-025-10492-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of smartphones is a complex decision-making process influenced by multiple criteria such as technical specifications, brand reputation, price, battery capacity, internal storage, and user preferences. The ambiguity and uncertainty around phenomena are handled by fuzzy sets. Picture fuzzy (PF) sets are an extension of fuzzy sets used for managing uncertainty in more complex situations when fuzzy sets are unable to produce reliable results. This paper outlines a framework in PF environment to solve the problem of selection of smartphone based on different factors. A new PF knowledge measure is proposed to measure the amount of knowledge linked to PF sets, and its reliability is tested using some numerical examples. A new PF accuracy measure is proposed based on the suggested knowledge measure and is used to find the pattern similarity of unknown patterns with given pattern. A new score function is proposed to compare the PF numbers, which can get around the drawbacks of existing score functions. A modified combined compromise solution (CoCoSo) technique is provided to select the best smartphone by using suggested scoring function and accuracy measure. Lastly, to prove the effectiveness of the suggested method, a comparative analysis carries out with the other existing methods.},
  archive      = {J_CC},
  author       = {Garg, Manish and Kumar, Satish},
  doi          = {10.1007/s12559-025-10492-4},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-32},
  shortjournal = {Cogn. Comput.},
  title        = {Smartphone selection with picture fuzzy modified combined compromise solutions},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling task difficulty in a visual pattern memory task: A comprehensive analysis of factors influencing performance. <em>CC</em>, <em>17</em>(5), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10494-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional assessments of visual pattern memory difficulty rely predominantly on the number of items, overlooking other potential influential factors. This study aimed to identify and analyze several variables affecting performance in a visual pattern memory task. We conducted a computer-based visual pattern memory experiment with 20 participants. The paradigm systematically manipulated both the quantity and spatial arrangement of target stimuli while controlling for eye movements. Performance data were analyzed using generalized linear mixed models (GLMMs) and linear mixed models (LMMs) to identify significant predictors of task difficulty. Performance was influenced by the number and spatial relationships of target and non-target stimuli, their distance from the display center, and participants’ selection strategies. Dense stimulus patterns facilitated better recall than distributed configurations, suggesting an advantage of chunking strategies. Additionally, a left-side selection bias emerged, which was affected by target distribution. This study highlights the complexity of difficulty scaling in a visual pattern memory task. It demonstrates that incorporating multiple quantified aspects of visual stimuli, including spatial configuration, target distribution, and behavioral strategies, is essential for developing adaptive cognitive training programs that rely on spatial information processing.},
  archive      = {J_CC},
  author       = {Mortazavi, Fatemeh and Vahabie, Abdol-Hossein and Ahmadabadi, Majid Nili and Moradi, Hadi},
  doi          = {10.1007/s12559-025-10494-2},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Unveiling task difficulty in a visual pattern memory task: A comprehensive analysis of factors influencing performance},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge enhanced and incongruity perceiving network for multimodal sarcasm detection. <em>CC</em>, <em>17</em>(5), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10499-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm is a complex linguistic act in which the literal meaning is opposite to the true attitude. Multimodal sarcasm detection (MSD) aims to identify whether a given multimodal data sample is sarcastic. Although existing methods have achieved impressive success, they ignored sarcastic information in text and potential clues in images and failed to fully explore cross-modal feature representation and inconsistency between modalities. To solve the above problems, we propose a knowledge enhanced and incongruity perceiving network (KEIPN) for MSD. We design a text knowledge pretraining module that includes RoBERTa models enhanced with biased sentiment knowledge and sarcasm knowledge. Next, we develop an image knowledge amalgamation module to integrate different types of knowledge into visual model, enhancing its visual understanding capability. Pretrained language and visual models can extract better feature representations. Then, we propose a cross-modal knowledge distillation module to model the relationship between text and images and achieve adaptive weighted fusion. Finally, we design an incongruity perception module to capture the inconsistency between images and text and weight the loss using keyless attention mechanisms. Extensive experiments on widely used datasets MMSD and MMSD2.0 demonstrate the superiority of our model over state-of-the-art (SOTA) methods.},
  archive      = {J_CC},
  author       = {Liu, Mingqi and Li, Zhixin},
  doi          = {10.1007/s12559-025-10499-x},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Knowledge enhanced and incongruity perceiving network for multimodal sarcasm detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TDGAT: Triple-dimensional graph attention networks for exploring the optimal perspective for aspect-based sentiment analysis. <em>CC</em>, <em>17</em>(5), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10497-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) is an attractive and challenging fine-grained subtask in the natural language processing (NLP) community. In prior works, the relevant research has achieved significant progress in leveraging external knowledge to strengthen the sentiment representation in ABSA. However, the prior simple utilization of external knowledge hurts the model’s comprehension of the diversity of sentiment. Therefore, this paper proposed a novel ABSA approach to achieve multi-dimensional understanding of sentiment, namely triple-dimensional graph attention networks (TDGAT). Preliminarily, a task-oriented prompt template is designed to evoke the pre-trained language model (PLM) generation ability, which would provide the essential semantic features for sentiment understanding. To avoid the errors caused by the single-dimension external knowledge, TDGAT constructs triple-dimensional sentiment graphs depending on valence, arousal, and dominance, respectively, which enhance the model’s sentiment comprehension ability greatly. Besides, this work also leverages GATs to evacuate the sentiment relations through exploiting the constructed graphs, which would largely assist to learn an optimal ABSA model. Eventually, to uncover the optimal sentiment exploration perspective, extensive experiments are conducted on five public and available benchmark datasets, and the related results show the proposed TDGAT outperforms the-state-of-art baselines with different sentiment perspectives.},
  archive      = {J_CC},
  author       = {Shi, Xuefeng and Ding, Weiping and Hu, Min and Kang, Xin and Ren, Fuji},
  doi          = {10.1007/s12559-025-10497-z},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {TDGAT: Triple-dimensional graph attention networks for exploring the optimal perspective for aspect-based sentiment analysis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimized feature selection approach for multi-view ensemble learning in sentiment analysis of user reviews. <em>CC</em>, <em>17</em>(5), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10496-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment classification is a discipline of artificial intelligence that classifies evaluations as positive, neutral, or negative based on the emotional opinion detected through natural language processing. A key challenge in sentiment classification is high dimensionality, as it often leads to reduced model performance due to redundant or irrelevant features. Removing such features, a common practice in machine learning and data processing, helps create an optimal feature space by reducing the number of input variables. This study proposes a novel multi-view ensemble learning model that integrates ElasticNetCV-based feature selection across multiple text representations—namely, Term Frequency-Inverse Document Frequency (TF-IDF), DistillBERT, and Global Vectors for Word Representation (GloVe). To our knowledge, this is the first systematic evaluation of ElasticNetCV applied to both traditional and contextual embeddings within a unified model. The selected features are used to train a multi-view soft voting ensemble model incorporating Random Forest (RF), Logistic Regression (LR), and Artificial Neural Networks (ANN) as base classifiers. Experiments on Yelp, Amazon, and IMDB datasets demonstrate notable F1-scores of 97.1%, 91.2%, and 95.4%, respectively, highlighting the efficiency of the proposed approach.},
  archive      = {J_CC},
  author       = {Demirci, Fatih and Garip, Zeynep and Ekinci, Ekin},
  doi          = {10.1007/s12559-025-10496-0},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {An optimized feature selection approach for multi-view ensemble learning in sentiment analysis of user reviews},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customer-perceived value and social media analytics: How supplier evaluation can benefit from aspect-based sentiment analysis and fuzzy inference. <em>CC</em>, <em>17</em>(5), 1-28. (<a href='https://doi.org/10.1007/s12559-025-10495-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding customer-perceived value is essential for driving strategic supplier development and operational excellence in modern supply chains. As customer sentiments increasingly manifest in social media, natural language processing (NLP) techniques carry the potential of extracting actionable insights from unstructured data. This study proposes a novel decision-making model that combines aspect-based sentiment analysis (ABSA) with fuzzy inference systems (FIS) to support supplier evaluation based on customer value perception. Bridging symbolic and sub-symbolic AI, the model quantifies sentiment polarity, subjectivity, and aspect relevance from social media content, integrating this information into a multi-stage fuzzy logic framework for large-scale group decision-making (LSGDM). Unlike conventional supplier evaluation methods, this approach operationalizes concept-level affective information by fusing customer sentiment information with supply chain operational data. An illustrative application in the smartphone industry demonstrates the model’s ability to analyze social media data from the X platform and generate quantitative indicators reflecting customer value perception. The model effectively incorporates these insights into supplier evaluation, highlighting suppliers’ strengths and areas for improvement. The results show that sentiment-informed supplier assessment enables more responsive and customer-aligned development strategies. The proposed approach highlights the benefits of integrating sentic computing principles into supply chain analytics, showing the model’s capability to capture customer perceptions and make them a driver for continuous improvement initiatives in supplier development.},
  archive      = {J_CC},
  author       = {Zanon, Lucas Gabriel and Arantes, Rafael Ferro Munhoz and Calache, Lucas Daniel Del Rosso and Martins, Roberto Antonio and Carpinetti, Luiz Cesar Ribeiro},
  doi          = {10.1007/s12559-025-10495-1},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Cogn. Comput.},
  title        = {Customer-perceived value and social media analytics: How supplier evaluation can benefit from aspect-based sentiment analysis and fuzzy inference},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring artificial intelligence tutor teammate adaptability to harness discovery curiosity and promote learning in the context of interactive molecular dynamics. <em>CC</em>, <em>17</em>(5), 1-24. (<a href='https://doi.org/10.1007/s12559-025-10498-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the impact of an artificial intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during interactive molecular dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI’s curiosity triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate’s behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60 min. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity, and AI’s curiosity-triggering and response behaviors. Cross recurrence quantification analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI’s dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement, and epistemic curiosity. Future research will refine AI strategies by integrating multimodal data to enhance curiosity-driven learning.},
  archive      = {J_CC},
  author       = {Demir, Mustafa and Miratsky, Jacob and Mishra, Punya and Nguyen, Jonathan and Chan, Chun Kit and Singharoy, Abhishek},
  doi          = {10.1007/s12559-025-10498-y},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {Exploring artificial intelligence tutor teammate adaptability to harness discovery curiosity and promote learning in the context of interactive molecular dynamics},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-triggered optimized control for nonlinear multiagent systems via reinforcement learning strategy. <em>CC</em>, <em>17</em>(5), 1-9. (<a href='https://doi.org/10.1007/s12559-025-10502-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the event-triggered optimal control issue for a class of nonlinear strict-feedback multiagent system, which embraces optimization as a fundamental design principle for high-order system control. The reinforcement learning (RL) strategy based on the identifier-actor-critic structure, in the process of optimized backstepping design, is adopted to optimize control performance. Besides that, a novel event-triggered mechanism, which utilizes both on the latest sampled state and a non-negative threshold, is implemented to enhance the efficiency of resource utilization. The computing cost is reduced, and the communication resources are saved by reducing the number of sampling times and the amount of data transmission, thus improving the system efficiency. It is proved that the closed-loop system is semi-globally uniformly ultimately bounded (SGUUB) in probability and the Zeno behavior is prevented through the analysis of Lyapunov stability theory. The conduction of two simulation examples is as the final step to substantiate the efficacy of the designed method.},
  archive      = {J_CC},
  author       = {Meng, Luyu and Wang, Xin and Wang, Ziming},
  doi          = {10.1007/s12559-025-10502-5},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-9},
  shortjournal = {Cogn. Comput.},
  title        = {Event-triggered optimized control for nonlinear multiagent systems via reinforcement learning strategy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning for human-AI collaboration: Challenges, mechanisms, and methods. <em>CC</em>, <em>17</em>(5), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10500-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) has significantly advanced the research topic of human-Artificial-Intelligence (AI) collaboration, offering powerful methods for improving the interaction between humans and AI systems across a range of applications. Despite the rapid growth of research on this topic, a review systematizing the challenges, mechanisms, and methods specifically tailored for human-AI collaboration is still lacking in the literature. This review focuses on RL-based human-AI collaboration, which can be structured into the following four parts: (1) comprehensively summarizing and analyzing the main challenges in human-AI collaboration, including aligning AI systems with human behaviors, generalizing to new human collaborators, ensuring robustness in dynamic environments, and performing coordination among multiple agents; (2) discussing the core mechanisms to address these challenges, including behavior characterization, intention understanding, and multi-agent coordination, and presenting a human-centric viewpoint on these mechanisms; (3) exploring the methods containing each mechanism, and investigating their effectiveness in enhancing human-AI collaboration; (4) comparing the representative methods from the perspectives of scalability, adaptability, and interpretability, and identifying the current challenges haunting this topic and proposing the potential directions for the prospective research on it. We expect this work will provide valuable insights into the academic advancements and inspire the technological breakthroughs in RL-based human-AI collaboration in the near future.},
  archive      = {J_CC},
  author       = {Li, Wei and Liu, Hongming and Huang, Kaizhu and Hussain, Amir},
  doi          = {10.1007/s12559-025-10500-7},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Reinforcement learning for human-AI collaboration: Challenges, mechanisms, and methods},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple local-global correlation-based deep neural network with selective kernel attention for bearing fault diagnosis. <em>CC</em>, <em>17</em>(5), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10508-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning-based deep neural networks have emerged as a promising solution for bearing fault diagnosis, particularly under limited labeled data conditions. However, existing approaches often rely on a single linear comparison module at the end of the feature extraction pipeline and commonly utilize pre-trained models from other domains, which might not generalize well to specific datasets such as those involving bearing faults. In this study, we propose a novel few-shot learning model, namely the Multiple Local-Global Correlation-based Deep Neural Network (MLGCN), for bearing fault diagnosis. The model is designed by simultaneously learning nonlinear data distributions and embedding representations in an end-to-end fashion. Inspired by human perceptual mechanisms that focus attention on task-relevant information, we introduce a Selective Kernel Attention Feature Extractor module to dynamically capture diverse and critical signal features, which is especially effective in limited data conditions. In addition, we present the Multiple Dual Local-Global Cross Attention (Dual LGCA) module to leverage the full feature hierarchy for enhanced similarity learning, enabling hierarchical similarity reasoning—akin to human comparison strategies that integrate multi-scale contextual cues. The proposed model is assessed using two popular bearing fault benchmarks, including the Case Western Reserve University (CWRU) and Paderborn University (PU) datasets, in distinct experiments to assess its accuracy under limited data and its generalization to real bearing damages. Our code will be released at https://github.com/ZQuang2202/MLGCN-FewshotBearingFault .},
  archive      = {J_CC},
  author       = {Nguyen, Van-Quang and Tran, Thi-Thao and Vu, Manh-Hung and Pham, Van-Truong and Lo, Men-Tzung},
  doi          = {10.1007/s12559-025-10508-z},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Multiple local-global correlation-based deep neural network with selective kernel attention for bearing fault diagnosis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An EfficientNet-b0 framework for nitrogen stress identification in sustainable precision farming plants. <em>CC</em>, <em>17</em>(5), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10504-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nitrogen deficiency is a critical stress factor in plants, significantly impacting crop yields in precision agriculture. Early detection of nutrient imbalances, such as inadequate nitrogen levels, is essential for optimizing plant growth and achieving high agricultural productivity. Nitrogen plays a vital role in various plant processes, including the synthesis of chlorophyll, proteins, amino acids, and nucleic acids, all of which are crucial for proper plant development. Deficiency in nitrogen can lead to observable changes in plant morphology, such as reduced leaf number, discoloration, and stunted growth. Recent advancements in imaging technologies have facilitated the development of computer vision-based plant phenomics, enabling rapid, non-invasive, and automated detection of plant stress. In this study, we propose an automated deep learning (DL)-based phenotyping approach to detect and classify nitrogen deficiency in plant leaf images. The method employs EfficientNet-B0, a convolutional neural network (CNN) architecture, to identify and categorize stress symptoms associated with nitrogen insufficiency. The performance of the proposed model was evaluated on two datasets, using key metrics including accuracy, precision, recall, and F1 score. The results show that the EfficientNet-B0 model achieves an accuracy of 97.9%, outperforming the baseline model, which reached an accuracy of 93.2%. These results demonstrate the effectiveness of the proposed deep learning model in accurately detecting nitrogen deficiency in plants, offering a promising tool for precision agriculture applications.},
  archive      = {J_CC},
  author       = {Babu, Surya and Thawait, Kartikey and Gopi, Arepalli Peda and Srinivasarao, Ulligaddala and Naik, K. Jairam},
  doi          = {10.1007/s12559-025-10504-3},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {An EfficientNet-b0 framework for nitrogen stress identification in sustainable precision farming plants},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable deep learning system for custom report generation in breast cancer histology. <em>CC</em>, <em>17</em>(5), 1-13. (<a href='https://doi.org/10.1007/s12559-025-10510-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the most lethal type of cancer among women, one of the causes can be due to the lack of professionals to evaluate the results of medical images in time (Sharafaddini et al. Multimed Tools Appl, 1–112 2024). This problem is even greater in developing countries. In recent years, diagnostic tools based on artificial intelligence techniques have been developed to improve diagnosis time and results. In this work, we present a system that analyzes histopathological images obtained from breast tissue biopsies to design a classification system that distinguishes between benign and malignant tissue. To demonstrate that the proposed work is robust, multiple alternatives and combinations are studied to obtain the best cases. Finally, we compare the proposed approach with previous works. Furthermore, the developed system integrates explainable artificial intelligence techniques to produce a report to the physician, including a heat map with the areas the system has determined to be essential for classification.},
  archive      = {J_CC},
  author       = {Anguita-Molina, Miguel Ángel and Civit-Masot, Javier and Muñoz-Saavedra, Luis and Polo-Rodríguez, Aurora and Domínguez-Morales, Manuel},
  doi          = {10.1007/s12559-025-10510-5},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Cogn. Comput.},
  title        = {Explainable deep learning system for custom report generation in breast cancer histology},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Product color emotional design based on interdependent networks and cascading failure. <em>CC</em>, <em>17</em>(5), 1-23. (<a href='https://doi.org/10.1007/s12559-025-10507-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product color emotional design (PCED) systems are characterized by a power-law distribution; this is manifested as a small number of colors constituting a large number of emotional connections of users, and the majority of colors constituting a small number of emotional connections of users. This will lead to the neglect of the potential connections between colors, thus causing the inability to generate a product color design scheme based on color interactions. Consequently, the reliability of the design scheme ultimately undermines the reliability of the design scheme. For this reason, this paper proposes a PCED method based on interdependent networks (INs) and cascading failure (CF). First, a web crawler, word processing, clustering, and complex network analysis are utilized to construct the product color emotion dataset. Then, based on this dataset, the INs of color emotion are constructed, and the initial failure node in the color emotion network is determined based on a variety of important node judgment methods. Finally, the color scheme that meets the user’s emotional needs is generated via the CF-based product color scheme generation rules. The PCED of a high-speed rail business seat is provided as an example to verify the validity of the proposed method.},
  archive      = {J_CC},
  author       = {Ding, Man and Ju, Yixian and Liu, Zhengwen and Zhao, Fanghua and Cho, Jounghyung},
  doi          = {10.1007/s12559-025-10507-0},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Product color emotional design based on interdependent networks and cascading failure},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="cis">CIS - 3</h2>
<ul>
<li><details>
<summary>
(2025). SMALLM: A local small model augmented a cloud-based large language model for chinese named entity recognition in low-resource industries. <em>CIS</em>, <em>11</em>(11), 1-18. (<a href='https://doi.org/10.1007/s40747-025-02074-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the cloud-based commercial Large Language Models (LLMs) have achieved state-of-the-art (SOTA) performances and cutting-edge zero-shot learning abilities on a variety of Natural Language Processing (NLP) tasks, their performance on Named Entity Recognition (NER) is still significantly below supervised baselines. While NER in the universal domain has achieved remarkable success, NER in specialized industries continues to face challenges due to the scarcity of data and computational resource. To address these issues, this study proposes "SMALLM", a novel NER framework that integrates a local BERT-based model to augment the performance of the state-of-the-art LLM, ChatGPT, in an end-to-end manner. By leveraging both the strengths of the small model and the LLM, SMALLM acts as a domain expert through few-shot learning. SMALLM updates only hundreds of thousands of parameters in the local BERT-based model with minimal training data. This significantly reduces computational demands, enables fast training and minimizes reliance on data. Additionally, our framework provides a solution to the constraints posed by closed-source LLMs like ChatGPT converting their responses into one-hot vectors for subsequent optimization and integration. Through experiments on three industrial Chinese NER (CNER) datasets—Construction, CCKS2019- Military, and CCKS2020-Clinic—SMALLM enhances the performance of ChatGPT, achieving improvements of 28.80%, 13.48%, and 48.37%, respectively. Additionally, SMALLM consistently surpasses mainstream models in low-resource settings.},
  archive      = {J_CIS},
  author       = {Yang, Jinning and Yang, Zhile and Wu, Chengke and Guo, Yuanjun and Li, Xiao and Lin, Jiarui},
  doi          = {10.1007/s40747-025-02074-6},
  journal      = {Complex & Intelligent Systems},
  month        = {11},
  number       = {11},
  pages        = {1-18},
  shortjournal = {Complex Intell. Syst.},
  title        = {SMALLM: A local small model augmented a cloud-based large language model for chinese named entity recognition in low-resource industries},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-stage cross-project defect prediction framework based on feature representation and knowledge transfer. <em>CIS</em>, <em>11</em>(11), 1-21. (<a href='https://doi.org/10.1007/s40747-025-02098-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction is a critical process for enhancing software quality by identifying potential defects early in the development lifecycle. Cross-Project Defect Prediction (CPDP) broadens traditional defect prediction methods by leveraging data from source projects to predict defects in target projects. However, existing CPDP methods face two key issues: insufficient feature representation and inadequate alignment of data distributions between source and target projects. To address these challenges, this study introduces TriStage-CPDP, a novel three-stage framework. In the first stage, TriStage-CPDP harnesses the capabilities of CodeT5+ to extract nuanced semantic and syntactic features, combines GraphSAGE for software dependency representation, and incorporates traditional metrics to construct a comprehensive feature set, enabling a more nuanced representation of software features. The second stage employs an integrated feature selection strategy that combines Last Absolute Shrinkage and Selection Operator (LASSO) regression with Recursive Feature Elimination (RFE) to refine feature relevance and eliminate redundancy effectively. Finally, a domain adaptation mechanism based on Locality Preserving Projection (LPP) aligns data distributions between source and target projects in a shared subspace, ensuring practical knowledge transfer. Experiments on the PROMISE dataset demonstrate that TriStage-CPDP significantly outperforms state-of-the-art methods, improving F-measure, AUC, and MCC. These results highlight its effectiveness in enhancing defect prediction effectiveness for data-scarce or new projects, offering a robust solution for CPDP and valuable insights for future research.},
  archive      = {J_CIS},
  author       = {Zou, Yifan and Wang, Huiqiang},
  doi          = {10.1007/s40747-025-02098-y},
  journal      = {Complex & Intelligent Systems},
  month        = {11},
  number       = {11},
  pages        = {1-21},
  shortjournal = {Complex Intell. Syst.},
  title        = {A three-stage cross-project defect prediction framework based on feature representation and knowledge transfer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online social platform competition and rumor control in disaster scenarios: A zero-sum differential game approach with approximate dynamic programming. <em>CIS</em>, <em>11</em>(11), 1-21. (<a href='https://doi.org/10.1007/s40747-025-02103-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital era, the widespread dissemination of rumors on online social networks (OSNs) has emerged as a formidable challenge. Driven by the pursuit of user engagement, competitive social platforms often overlook effective rumor control mechanisms, exacerbating information disorders. A framework for modeling platform-wide competition during short-term but explosive information transmission scenarios is proposed in this paper. We first introduce a novel dynamic susceptible, exposed, infected, labeled, and recovered (SEILR) model, which is meticulously designed to capture the intricate dynamics of information transmission across multilayer networks. Second, we formulate a zero-sum differential game framework to analyze the strategic interactions among competing platforms. By integrating critical cost factors, including user losses, potential government sanctions, and social management expenses, our framework provides a comprehensive understanding of the economic implications of rumor control. Third, to address the complexity of deriving optimal strategies within the proposed framework, we innovatively combine iterative approximate dynamic programming (ADP) with neural networks. Leveraging the approximation capabilities of the neural networks, we enhance the efficiency and accuracy of the ADP process. Through extensive numerical simulations conducted in a duopoly market setting, we validate the efficacy of our proposed strategies. The results demonstrate their remarkable cost efficiency and superior rumor management capabilities relative to those of the existing approaches.},
  archive      = {J_CIS},
  author       = {Liu, Wenjia and Liu, Jida and Niu, Zhipeng},
  doi          = {10.1007/s40747-025-02103-4},
  journal      = {Complex & Intelligent Systems},
  month        = {11},
  number       = {11},
  pages        = {1-21},
  shortjournal = {Complex Intell. Syst.},
  title        = {Online social platform competition and rumor control in disaster scenarios: A zero-sum differential game approach with approximate dynamic programming},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="cms">CMS - 8</h2>
<ul>
<li><details>
<summary>
(2025). Consumption–Investment and reinsurance problem under markovian regime switching: Time-consistent solution. <em>CMS</em>, <em>13</em>(4), 1037-1073. (<a href='https://doi.org/10.1007/s40304-024-00418-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a characterization of equilibrium in a game theoretic description of discounting stochastic consumption, investment and reinsurance problem, in which the controlled state process evolves according to a multi-dimensional linear stochastic differential equation, when the noise is driven by a Brownian motion under the effect of a Markovian regime switching. The running and the terminal costs in the objective functional, are explicitly depended on some general discount functions, which create the time inconsistency of the considered model. Open-loop Nash equilibrium controls are described through some necessary and sufficient equilibrium conditions as well as a verification result. A state feedback equilibrium strategy is achieved via certain partial differential-difference equation. As an application, we study an investment–consumption and equilibrium reinsurance/new business strategies for some particular cases of power and logarithmic utility functions. A numerical example is provided to demonstrate the efficacy of theoretical results.},
  archive      = {J_CMS},
  author       = {Bouaicha, Nour El Houda and Chighoub, Farid and Pal Majumder, Abhishek},
  doi          = {10.1007/s40304-024-00418-1},
  journal      = {Communications in Mathematics and Statistics},
  month        = {8},
  number       = {4},
  pages        = {1037-1073},
  shortjournal = {Commun. Math. Stat.},
  title        = {Consumption–Investment and reinsurance problem under markovian regime switching: Time-consistent solution},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unadjusted langevin algorithm for non-convex weakly smooth potentials. <em>CMS</em>, <em>13</em>(4), 979-1036. (<a href='https://doi.org/10.1007/s40304-023-00350-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discretization of continuous-time diffusion processes is a widely recognized method for sampling. However, the canonical Euler Maruyama discretization of the Langevin diffusion process, referred as unadjusted Langevin algorithm (ULA), studied mostly in the context of smooth (gradient Lipschitz) and strongly log-concave densities, is a considerable hindrance for its deployment in many sciences, including statistics and machine learning. In this paper, we establish several theoretical contributions to the literature on such sampling methods for non-convex distributions. Particularly, we introduce a new mixture weakly smooth condition, under which we prove that ULA will converge with additional log-Sobolev inequality. We also show that ULA for smoothing potential will converge in $$L_{2}$$ -Wasserstein distance. Moreover, using convexification of nonconvex domain (Ma et al. in Proc Natl Acad Sci 116(42):20881–20885, 2019) in combination with regularization, we establish the convergence in Kullback–Leibler divergence with the number of iterations to reach $$\epsilon $$ -neighborhood of a target distribution in only polynomial dependence on the dimension. We relax the conditions of Vempala and Wibisono (Advances in Neural Information Processing Systems, 2019) and prove convergence guarantees under isoperimetry, and non-strongly convex at infinity.},
  archive      = {J_CMS},
  author       = {Nguyen, Dao and Dang, Xin and Chen, Yixin},
  doi          = {10.1007/s40304-023-00350-w},
  journal      = {Communications in Mathematics and Statistics},
  month        = {8},
  number       = {4},
  pages        = {979-1036},
  shortjournal = {Commun. Math. Stat.},
  title        = {Unadjusted langevin algorithm for non-convex weakly smooth potentials},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Backward linear-quadratic mean field social optima with partial information. <em>CMS</em>, <em>13</em>(4), 949-978. (<a href='https://doi.org/10.1007/s40304-023-00348-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the linear-quadratic social optima for a class of N weakly coupled backward system with partial information structure. The system dynamics are governed by linear backward stochastic differential equations, and the objective is to minimize a social cost. The stochastic filtering Hamiltonian system is obtained from variational analysis. By virtue of the stochastic filtering technique and backward decoupling method, the feedback form of optimal control is derived. Aiming to overcome the curse of dimensionality and reduce the information requirements, we design a set of decentralized control laws, which is further shown to be asymptotic. Finally, an example of the scalar-valued case is studied.},
  archive      = {J_CMS},
  author       = {Li, Mengzhen and Wu, Zhen},
  doi          = {10.1007/s40304-023-00348-4},
  journal      = {Communications in Mathematics and Statistics},
  month        = {8},
  number       = {4},
  pages        = {949-978},
  shortjournal = {Commun. Math. Stat.},
  title        = {Backward linear-quadratic mean field social optima with partial information},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous cluster expansion for field theories. <em>CMS</em>, <em>13</em>(4), 931-948. (<a href='https://doi.org/10.1007/s40304-023-00346-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new version of the cluster expansion is proposed without breaking the translation and rotation invariance. As an application of this technique, we construct the connected Schwinger functions of the regularized $$\phi ^4$$ theory in a continuous way.},
  archive      = {J_CMS},
  author       = {Zhao, Fang-Jie},
  doi          = {10.1007/s40304-023-00346-6},
  journal      = {Communications in Mathematics and Statistics},
  month        = {8},
  number       = {4},
  pages        = {931-948},
  shortjournal = {Commun. Math. Stat.},
  title        = {Continuous cluster expansion for field theories},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fischer–Marsden conjecture and equation in the complex hyperbolic quadric. <em>CMS</em>, <em>13</em>(4), 891-929. (<a href='https://doi.org/10.1007/s40304-023-00345-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By using the notion of Fischer–Marsden equation on real hypersurfaces in the complex hyperbolic quadric $${{Q^m}^*} = {{\text {SO}}^o_{2,m}/{\text {SO}}_2 {\text {SO}}_m}$$ , we can assert that there does not exist a non-trivial solution $$(g,{\nu })$$ of Fischer–Marsden equation on real hypersurfaces with isometric Reeb flow in the complex hyperbolic quadric $${Q^m}^*$$ . Next, as an application we also show that there does not exist a non-trivial solution $$(g,{\nu })$$ of the Fischer–Marsden equation on contact real hypersurfaces in the complex hyperbolic quadric $${Q^m}^*$$ . Consequently, the Fischer–Marsden conjecture is true on these two kinds of real hypersurfaces in the complex hyperbolic quadric $${Q^m}^*$$ .},
  archive      = {J_CMS},
  author       = {Suh, Young Jin},
  doi          = {10.1007/s40304-023-00345-7},
  journal      = {Communications in Mathematics and Statistics},
  month        = {8},
  number       = {4},
  pages        = {891-929},
  shortjournal = {Commun. Math. Stat.},
  title        = {Fischer–Marsden conjecture and equation in the complex hyperbolic quadric},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strong convergence theorems under sub-linear expectations and its applications in nonparametric regression models. <em>CMS</em>, <em>13</em>(4), 863-889. (<a href='https://doi.org/10.1007/s40304-023-00344-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we first study the complete convergence for arrays of rowwise widely orthant dependent random variables under sub-linear expectations. The complete convergence theorems are established in sense of sub-additive capacities under some mild conditions. As an application of the main results, we investigate the strong consistency for the weighted estimator in a nonparametric regression model based on widely orthant dependent errors under sub-linear expectations. In addition, we also obtain the rate of strong consistency for the estimator in a nonparametric regression model based on widely orthant dependent errors under sub-linear expectations.},
  archive      = {J_CMS},
  author       = {Wu, Yi and Deng, Xin and Xi, Mengmei and Wang, Xuejun},
  doi          = {10.1007/s40304-023-00344-8},
  journal      = {Communications in Mathematics and Statistics},
  month        = {8},
  number       = {4},
  pages        = {863-889},
  shortjournal = {Commun. Math. Stat.},
  title        = {Strong convergence theorems under sub-linear expectations and its applications in nonparametric regression models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneity exploration for multi-response regression. <em>CMS</em>, <em>13</em>(4), 845-861. (<a href='https://doi.org/10.1007/s40304-023-00342-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding treatment heterogeneity plays a key role in many contemporary applications arising from different areas. Although there is a growing literature on subgroup analysis based on the heterogeneous univariate regression model, little work has been done for the heterogeneous multi-response regression model. In contrast to the existing methods which are based on subject-specific intercepts, we introduce a heterogeneous multi-response regression method which allows the coefficients for treatment variables to be subject-dependent with unknown grouping information, thus applicable to a wider range of situations. Moreover, we provide an efficient algorithm based on concave pairwise fusion penalization and establish the oracle property of the proposed estimator. The effectiveness of the suggested method is demonstrated through simulation examples and an empirical study.},
  archive      = {J_CMS},
  author       = {Wu, Jie and Gao, Wenjie and Dong, Ruipeng and Zheng, Zemin},
  doi          = {10.1007/s40304-023-00342-w},
  journal      = {Communications in Mathematics and Statistics},
  month        = {8},
  number       = {4},
  pages        = {845-861},
  shortjournal = {Commun. Math. Stat.},
  title        = {Heterogeneity exploration for multi-response regression},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The large deviation of semi-linear stochastic partial differential equation driven by brownian sheet. <em>CMS</em>, <em>13</em>(4), 813-843. (<a href='https://doi.org/10.1007/s40304-023-00340-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove the large deviation principle for the law of the one-dimensional semi-linear stochastic partial differential equations driven by a nonlinear multiplicative noise. Firstly, combining the energy estimate and approximation procedure, we obtain the existence of the global solution. Secondly, the large deviation principle is obtained via the weak convergence method.},
  archive      = {J_CMS},
  author       = {Cao, Qiyong and Gao, Hongjun},
  doi          = {10.1007/s40304-023-00340-y},
  journal      = {Communications in Mathematics and Statistics},
  month        = {8},
  number       = {4},
  pages        = {813-843},
  shortjournal = {Commun. Math. Stat.},
  title        = {The large deviation of semi-linear stochastic partial differential equation driven by brownian sheet},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="coap">COAP - 11</h2>
<ul>
<li><details>
<summary>
(2025). A general preconditioner for a class of vertical tensor complementarity problems. <em>COAP</em>, <em>92</em>(1), 345-373. (<a href='https://doi.org/10.1007/s10589-025-00700-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, based on the methodology of the preconditioning technique, our objective is devoted to solving a class of vertical tensor complementarity problems (VTCP) by the preconditioned fixed point method based on tensor splitting. We firstly propose a general preconditioner based on the amount of negative components of special vector involved. Secondly, some convergence and comparison theorems of the proposed method are given. Thirdly, we analyze the influence of the parameter on the convergence rate of the proposed method. Finally, numerical examples are given to illustrate the effectiveness of the proposed method.},
  archive      = {J_COAP},
  author       = {Wu, Shi-Liang and Long, Mei and Li, Cui-Xia},
  doi          = {10.1007/s10589-025-00700-0},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {345-373},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A general preconditioner for a class of vertical tensor complementarity problems},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified approach for smoothing approximations to the exact $$\ell _1$$ -penalty for inequality-constrained optimization. <em>COAP</em>, <em>92</em>(1), 327-344. (<a href='https://doi.org/10.1007/s10589-025-00694-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In penalty methods for inequality-constrained optimization problems, the nondifferentiability of the exact $$\ell _1$$ -penalty function limits the use of efficient smooth algorithms for solving the subproblems. In light of this, a great number of smoothing techniques has been proposed in the literature. In this paper we present, in a unified manner, results and methods based on functions that smooth and approximate the exact penalty function. We show that these functions define a class of algorithms that converges to global and local minimizers. This unified approach allows us to derive sufficient conditions that guarantee the existence of local minimizers for the subproblems and to establish a linear convergence rate for this class of methods, using an error bound-type condition. Finally, numerical experiments with problems of the CUTEst collection are presented to illustrate the computational performance of some methods from the literature which can be recovered as particular cases of our unified approach.},
  archive      = {J_COAP},
  author       = {Rosa, Mariana da and Ribeiro, Ademir Alves and Karas, Elizabeth Wegner},
  doi          = {10.1007/s10589-025-00694-9},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {327-344},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A unified approach for smoothing approximations to the exact $$\ell _1$$ -penalty for inequality-constrained optimization},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive cyclic gradient methods with interpolation. <em>COAP</em>, <em>92</em>(1), 301-325. (<a href='https://doi.org/10.1007/s10589-025-00691-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient method is an important method for solving large scale problems. In this paper, a new gradient method framework for unconstrained optimization problem is proposed, where the stepsize is updated in a cyclic way. The Cauchy step is approximated by the quadratic interpolation. And the cycle for stepsize update is adjusted adaptively. Combining with the adaptive nonmonotone line search technique, we prove the global convergence of the proposed method. Furthermore, its sublinear convergence rate for convex problems and R-linear convergence rate for problems with quadratic functional growth property are analyzed. Numerical results show that our proposed algorithm enjoys good performances in terms of both computational cost and obtained function values.},
  archive      = {J_COAP},
  author       = {Xie, Yixin and Sun, Cong and Yuan, Ya-Xiang},
  doi          = {10.1007/s10589-025-00691-y},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {301-325},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Adaptive cyclic gradient methods with interpolation},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sequence-form differentiable path-following method to compute nash equilibria. <em>COAP</em>, <em>92</em>(1), 265-300. (<a href='https://doi.org/10.1007/s10589-025-00702-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sequence-form representation has shown remarkable efficacy in computing Nash equilibria for two-player extensive-form games with perfect recall. Nonetheless, devising an efficient algorithm for n-player games using the sequence form remains a substantial challenge. To bridge this gap, we establish a necessary and sufficient condition, characterized by a polynomial system, for Nash equilibrium within the sequence-form framework. Building upon this, we develop a sequence-form differentiable path-following method for computing a Nash equilibrium. This method involves constructing an artificial logarithmic-barrier game in sequence form, where two functions of an auxiliary variable are introduced to incorporate logarithmic-barrier terms into the payoff functions and construct the strategy space. Afterward, we prove the existence of a smooth path determined by the artificial game, originating from an arbitrary totally mixed behavioral-strategy profile and converging to a Nash equilibrium of the original game as the auxiliary variable approaches zero. In addition, a convex-quadratic-penalty method and a variant of linear tracing procedure in sequence form are presented as two alternative techniques for computing a Nash equilibrium. Numerical comparisons further illuminate the effectiveness and efficiency of these methods.},
  archive      = {J_COAP},
  author       = {Hou, Yuqing and Cao, Yiyin and Dang, Chuangyin and Wang, Yong},
  doi          = {10.1007/s10589-025-00702-y},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {265-300},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A sequence-form differentiable path-following method to compute nash equilibria},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preprocessing and valid inequalities for exact detection of critical nodes via integer programming. <em>COAP</em>, <em>92</em>(1), 215-263. (<a href='https://doi.org/10.1007/s10589-025-00698-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The critical nodes detection problem (CNDP) involves identifying a limited number of nodes for removal from an undirected graph, to maximize the disconnections between remaining node pairs. In this paper, we shall provide a high-efficiency algorithm for precisely solving the integer programming (IP) formulations for the CNDP. Firstly, a preprocessing procedure is introduced, which can not only reduce the size of the exponential-size IP formulation of the problem but also strengthen the linear programming relaxation. Secondly, the polyhedral properties of the polytope associated with the exponential-size IP formulation are explored, providing a flexible way to derive facet-defining inequalities for the polytope from certain projected ones. Thirdly, a family of strong valid inequalities based on clique subgraphs is developed for the polytope, with both necessary and sufficient conditions for them to be facet-defining. The complexity and algorithm of the separation problem for these inequalities are also investigated. Finally, we extend our research findings from the exponential-size IP formulation to two polynomial-size IP reformulations for the CNDP. Computational results demonstrate the efficacy of incorporating our proposed preprocessing and valid inequalities into an IP solver for solving all three CNDP formulations.},
  archive      = {J_COAP},
  author       = {Chen, Sheng-Jie and Chen, Liang and Li, Guang-Ming and Dai, Yu-Hong},
  doi          = {10.1007/s10589-025-00698-5},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {215-263},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Preprocessing and valid inequalities for exact detection of critical nodes via integer programming},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cardinality objective nonlinear programs for facility capacity expansion. <em>COAP</em>, <em>92</em>(1), 179-214. (<a href='https://doi.org/10.1007/s10589-025-00697-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a class of mathematical programs with inequality and equality constraints where the objective involves a cardinality penalty. The introduction of cardinality penalty can make the model automatically generate sparse solutions, but solving the cardinality objective nonlinear program is highly challenging since the objective function is discontinuous. We first give a continuous approximation and discuss its relationship with the original problem. Second, we propose a proximal augmented Lagrangian method for finding a weak directional(d)-stationary point of the continuous approximation. The proposed algorithm is a novel combination of the classical augmented Lagrangian method and proximal gradient algorithm. We prove that the proposed method globally converges to a weak d-stationary point of the continuous approximation, which is stronger than Clarke stationary point. Third, we demonstrate that the cardinality objective nonlinear program is a better model for the facility capacity expansion problem, which can generate key capacity expansion locations to avoid the high operating costs caused by expanding a large number of facilities. Finally, a systematic computational study on two capacity expansion problems is presented. The numerical results demonstrate the benefit of the cardinality objective nonlinear program and the effectiveness of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Li, Gao-Xi and Yang, Xin-Min},
  doi          = {10.1007/s10589-025-00697-6},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {179-214},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Cardinality objective nonlinear programs for facility capacity expansion},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A subspace minimization barzilai-borwein method for multiobjective optimization problems. <em>COAP</em>, <em>92</em>(1), 155-178. (<a href='https://doi.org/10.1007/s10589-025-00695-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear conjugate gradient methods have recently garnered significant attention within the multiobjective optimization community. These methods maintain consistency in conjugate parameters with their single-objective optimization counterparts. However, the desirable conjugate property of search directions remains uncertain, even for quadratic cases, in multiobjective conjugate gradient methods. This loss of the conjugate property significantly limits the applicability of these methods. To elucidate the role of the last search direction, we develop a subspace minimization Barzilai-Borwein method for multiobjective optimization problems (SMBBMO). In SMBBMO, each search direction is derived by optimizing a preconditioned Barzilai-Borwein subproblem within a two-dimensional subspace generated by the last search direction and the current Barzilai-Borwein descent direction. Furthermore, to ensure the global convergence of SMBBMO, we employ a modified Cholesky factorization on a transformed scale matrix, capturing the local curvature information of the problem within the two-dimensional subspace. Under mild assumptions, we establish both global and Q-linear convergence of the proposed method. Finally, comparative numerical experiments confirm the efficiency of SMBBMO, even when tackling large-scale and ill-conditioned problems.},
  archive      = {J_COAP},
  author       = {Chen, Jian and Tang, Liping and Yang, Xinmin},
  doi          = {10.1007/s10589-025-00695-8},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {155-178},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A subspace minimization barzilai-borwein method for multiobjective optimization problems},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The cosine measure relative to a subspace. <em>COAP</em>, <em>92</em>(1), 125-153. (<a href='https://doi.org/10.1007/s10589-025-00701-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cosine measure was introduced in 2003 to quantify the richness of finite positive spanning sets of directions in the context of derivative-free directional methods. A positive spanning set is a set of vectors whose nonnegative linear combinations span the whole space. The present work extends the definition of cosine measure. In particular, the paper studies cosine measures relative to a subspace, and proposes a deterministic algorithm to compute it. The paper also studies the situation in which the set of vectors is infinite. The extended definition of the cosine measure might be useful for subspace decomposition methods.},
  archive      = {J_COAP},
  author       = {Audet, Charles and Hare, Warren and Jarry-Bolduc, Gabriel},
  doi          = {10.1007/s10589-025-00701-z},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {125-153},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The cosine measure relative to a subspace},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bi-objective optimization based acquisition strategy for batch bayesian global optimization. <em>COAP</em>, <em>92</em>(1), 81-123. (<a href='https://doi.org/10.1007/s10589-025-00696-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we deal with batch Bayesian Optimization (Bayes-Opt) problems over a box. Bayes-Opt approaches find their main applications when the objective function is very expensive to evaluate. Sometimes, given the availability of multi-processor computing architectures, function evaluation might be performed in parallel in order to lower the clock-time of the overall computation. This paper fits this situation and is devoted to the development of a novel bi-objective optimization (BOO) acquisition strategy to sample batches of points where to evaluate the objective function. The BOO problem involves the Gaussian Process posterior mean and variance functions, which, in most of the acquisition strategies from the literature, are generally used in combination, frequently through scalarization. However, such scalarization could compromise the Bayes-Opt process performance, as getting the desired trade-off between exploration and exploitation is not trivial in most cases. We instead aim to reconstruct the Pareto front of the BOO problem exploiting first order information of the posterior mean and variance, thus generating multiple trade-offs of the two functions without any a priori knowledge. The algorithm used for the reconstruction is the Non-dominated Sorting Memetic Algorithm (NSMA), recently proposed in the literature and proved to be effective in solving hard MOO problems. Finally, we present two clustering approaches, each of them operating on a different space, to select potentially optimal points from the Pareto front. We compare our methodology with well-known acquisition strategies from the literature, showing its effectiveness on a wide set of experiments.},
  archive      = {J_COAP},
  author       = {Carciaghi, Francesco and Magistri, Simone and Mansueto, Pierluigi and Schoen, Fabio},
  doi          = {10.1007/s10589-025-00696-7},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {81-123},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A bi-objective optimization based acquisition strategy for batch bayesian global optimization},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Riemannian adaptive regularized newton methods with hölder continuous hessians. <em>COAP</em>, <em>92</em>(1), 29-79. (<a href='https://doi.org/10.1007/s10589-025-00692-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents strong worst-case iteration and operation complexity guarantees for Riemannian adaptive regularized Newton methods, a unified framework encompassing both Riemannian adaptive regularization (RAR) methods and Riemannian trust region (RTR) methods. We comprehensively characterize the sources of approximation in second-order manifold optimization methods: the objective function’s smoothness, retraction’s smoothness, and subproblem solver’s inexactness. Specifically, for a function with a $$\mu $$ -Hölder continuous Hessian, when equipped with a retraction featuring a $$\nu $$ -Hölder continuous differential and a $$\theta $$ -inexact subproblem solver, both RTR and RAR with $$2\!+\!\alpha $$ regularization (where $$\alpha =\min \{\mu ,\nu ,\theta \}$$ ) locate an $$(\epsilon ,\epsilon ^{\alpha /(1+\alpha )})$$ -approximate second-order stationary point within at most $$O(\epsilon ^{-(2+\alpha )/(1+\alpha )})$$ iterations and at most $${\widetilde{O}}(\epsilon ^{- (4+3\alpha ) /(2(1+\alpha ))})$$ Hessian-vector products with high probability. These complexity results are novel and sharp, and reduce to an iteration complexity of $$O(\epsilon ^{-3 /2})$$ and an operation complexity of $${\widetilde{O}}(\epsilon ^{-7 /4})$$ when $$\alpha =1$$ .},
  archive      = {J_COAP},
  author       = {Zhang, Chenyu and Jiang, Rujun},
  doi          = {10.1007/s10589-025-00692-x},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {29-79},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Riemannian adaptive regularized newton methods with hölder continuous hessians},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A randomized feasible algorithm for optimization with orthogonal constraints. <em>COAP</em>, <em>92</em>(1), 1-27. (<a href='https://doi.org/10.1007/s10589-025-00693-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a randomized feasible algorithm for optimization over the Stiefel manifold, where only some randomly chosen columns of the variable matrix are updated at each iteration. It is proved that the sequence of Riemannian gradients generated by the algorithm converges to zero with probability one. Numerical results show that the algorithm is efficient, especially for the problems when the matrices involved are sparse.},
  archive      = {J_COAP},
  author       = {Fei, Fan and Feng, Yuchen and Fan, Jinyan},
  doi          = {10.1007/s10589-025-00693-w},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A randomized feasible algorithm for optimization with orthogonal constraints},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="cstat">CSTAT - 24</h2>
<ul>
<li><details>
<summary>
(2025). A bootstrap-based bandwidth selection rule for kernel quantile estimators. <em>CSTAT</em>, <em>40</em>(7), 4037-4058. (<a href='https://doi.org/10.1007/s00180-024-01582-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantile has been widely used to quantify the uncertainty in many fields. In this paper, we study the estimation of quantiles via kernels, especially for extreme quantiles, and propose a bootstrap-based bandwidth selection (BBS) method for it. This method employs bootstrap sampling of data and least-squares regression to estimate the unknown bandwidth parameter in the kernel, which plays a crucial role in kernel smoothing. From a theoretical perspective, we establish a data-driven and bootstrap-based kernel quantile estimator and provide its asymptotic bias and variance, based on which the proposed method is shown to lead to the asymptotically optimal bandwidth selection in terms of minimizing the mean squared error. Numerical experiments demonstrate that the BBS method works well in both bandwidth selection and extreme quantile estimation.},
  archive      = {J_CSTAT},
  author       = {Liu, Xiaoyu and Song, Yan and Cheng, Hong-Fa and Zhang, Kun},
  doi          = {10.1007/s00180-024-01582-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {4037-4058},
  shortjournal = {Comput. Stat.},
  title        = {A bootstrap-based bandwidth selection rule for kernel quantile estimators},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic data generation method providing enhanced covariance matrix estimation. <em>CSTAT</em>, <em>40</em>(7), 4007-4035. (<a href='https://doi.org/10.1007/s00180-025-01643-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic data generation is an important tool to ensure data confidentiality. Various synthetic data generators have been developed in the literature. The methods in the literature are mostly for general purposes. They aim to generate data whose distributions are the same as the original data set, and the synthesized data are used for every purpose depending on who uses them. However, it could not be good for all purposes. In this paper, we study the synthetic data generation tailored for a specific purpose. We are particularly interested incovariance matrix estimation, which is a key part of many multivariate statistical analyses. To do it, we first see the connection between the sequential regression model and the modified Cholesky decomposition. We then devise a new synthetic data generator, named SynCov, that controls the error variances of the sequential regression model. We show that the sample covariance matrix of the synthetic data generated by SynCov is equivalent to a shrinkage covariance matrix estimator, which reduces estimation error in Frobenius norm. Our comprehensive numerical study shows that SynCov performs better than other synthetic data generation methods in covariance matrix estimation. Finally, we apply our SynCov to two real data examples, (i) the estimation of the covariance matrix of the (selected) variables of the Los Angeles City Employee Payroll data and (ii) the classification of the Taiwanese Bankruptcy Data.},
  archive      = {J_CSTAT},
  author       = {Kim, Seungkyu and Lim, Johan and Yu, Donghyeon},
  doi          = {10.1007/s00180-025-01643-0},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {4007-4035},
  shortjournal = {Comput. Stat.},
  title        = {Synthetic data generation method providing enhanced covariance matrix estimation},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially functional linear expectile regression model with missing observations. <em>CSTAT</em>, <em>40</em>(7), 3981-4005. (<a href='https://doi.org/10.1007/s00180-025-01652-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate estimation for the partially functional linear expectile regression model where observations are missing at random (MAR). First, we construct expectile regression (ER) estimators for both the slope functions and scalar parameters. Second, to obtain confidence intervals for the scalar parameters, we propose both the multiplier bootstrap method and the empirical likelihood (EL) method. Meanwhile, the maximum empirical likelihood (MEL) estimators for the scalar parameters are derived using the empirical log-likelihood ratio function. Furthermore, under mild conditions, we establish several asymptotic properties, including the convergence rates of the ER estimators for the scalar parameters and the slope function, the asymptotic normality of the ER estimators and the MEL estimators for the scalar parameters, and the convergence of the empirical log-likelihood ratio function to the standard chi-squared distribution. Finally, simulation studies and a real data analysis are conducted to evaluate the performance of the proposed methods.},
  archive      = {J_CSTAT},
  author       = {Wu, Chengxin and Ling, Nengxiang},
  doi          = {10.1007/s00180-025-01652-z},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3981-4005},
  shortjournal = {Comput. Stat.},
  title        = {Partially functional linear expectile regression model with missing observations},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning causal graphs using variable grouping according to ancestral relationship. <em>CSTAT</em>, <em>40</em>(7), 3947-3979. (<a href='https://doi.org/10.1007/s00180-025-01633-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the sample size is small relative to the number of variables, the accuracy of the conventional causal learning algorithm decreases. Some causal discovery methods are not feasible when the sample size is smaller than the number of variables. To circumvent these problems, some researchers proposed causal discovery algorithms using divide-and-conquer approaches (e.g., Cai et al. in Sada: a general framework to support robust causation discovery. In: International Conference on machine learning, PMLR, pp 208–216, 2013; Zhang et al. in IEEE Trans Cybern 52:3232–3243, 2020). For learning an entire causal graph, divide-and-conquer approaches first split variables into several subsets according to the conditional independence relationships among the variables, then apply a conventional causal discovery algorithm to each subset and merge the estimated results. Since the divide-and-conquer approach reduces the number of variables to which a causal discovery algorithm is applied, it is expected to improve the estimation accuracy, especially when the sample size is small relative to the number of variables and the model is sparse. However, existing methods are computationally expensive or do not provide sufficient accuracy when the sample size is small. This paper proposes a new algorithm for grouping variables according to the causal ancestral relationships, assuming that the causal model is LiNGAM (Shimizu et al. J Mach Learn Res 7:2003–2030, 2006). We call the proposed algorithm the causal ancestral-relationship-based grouping (CAG). The time complexity of the ancestor finding in the CAG is shown to be cubic in the number of variables. Extensive computer experiments confirm that the proposed method outperforms the original DirectLiNGAM (Shimizu et al. in J Mach Learn Res-JMLR 12:1225–1248, 2011) and other divide-and-conquer approaches not only in estimation accuracy but also in computation time when the sample size is small relative to the number of variables and the causal model is sparse or moderately dense. We also apply the proposed method to two real datasets to confirm its usefulness.},
  archive      = {J_CSTAT},
  author       = {Cai, Ming and Hara, Hisayuki},
  doi          = {10.1007/s00180-025-01633-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3947-3979},
  shortjournal = {Comput. Stat.},
  title        = {Learning causal graphs using variable grouping according to ancestral relationship},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical relations among principal component and factor analysis procedures elucidated from a comprehensive model. <em>CSTAT</em>, <em>40</em>(7), 3911-3946. (<a href='https://doi.org/10.1007/s00180-025-01611-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this review article, the term “hierarchy” is related to constrained-ness, but not to superiority. Procedures A and B forming a hierarchy means that A is a constrained variant of B or vice versa. A goal of this article is to present a hierarchy of principal component analysis (PCA) and factor analysis (FA) procedures, which follows from a comprehensive FA (CompFA) model. This model can be regarded as a hybrid of PCA and prevalent FA models. First, we show how a non-random version of the CompFA model leads to the following hierarchy: PCA is a constrained variant of completely decomposed FA, which itself is a constrained variant of matrix decomposition FA. Then, we prove that a random version of the CompFA model leads to minimum rank FA (MRFA) and constraining MRFA leads to random PCA (RPCA), so as to present the following hierarchy: Probabilistic PCA is a constrained variant of prevalent FA, and the latter is a constrained variant of RPCA, which is itself a constrained variant of MRFA. Finally, this hierarchy and the above hierarchy following from the non-random version are unified into one. We further utilize the unified hierarchy to present a strategy for selecting a procedure suitable to a data set.},
  archive      = {J_CSTAT},
  author       = {Adachi, Kohei},
  doi          = {10.1007/s00180-025-01611-8},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3911-3946},
  shortjournal = {Comput. Stat.},
  title        = {Hierarchical relations among principal component and factor analysis procedures elucidated from a comprehensive model},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-expanded ECME algorithms for logistic and penalized logistic regression. <em>CSTAT</em>, <em>40</em>(7), 3883-3909. (<a href='https://doi.org/10.1007/s00180-025-01619-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter estimation in logistic regression is a well-studied problem with the Newton–Raphson method being one of the most prominent optimization techniques used in practice. A number of monotone optimization methods including minorization-maximization (MM) algorithms, expectation-maximization (EM) algorithms and related variational Bayes approaches offer useful alternatives guaranteed to increase the logistic regression likelihood at every iteration. In this article, we propose and evaluate an optimization procedure that is based on a straightforward modification of an EM algorithm for logistic regression. Our method can substantially improve the computational efficiency of the EM algorithm while preserving the monotonicity of EM and the simplicity of the EM parameter updates. By introducing an additional latent parameter and selecting this parameter to maximize the penalized observed-data log-likelihood at every iteration, our iterative algorithm can be interpreted as a parameter-expanded expectation-conditional maximization either (ECME) algorithm, and we demonstrate how to use the parameter-expanded ECME with an arbitrary choice of weights and penalty function. In addition, we describe a generalized version of our parameter-expanded ECME algorithm that can be tailored to the challenges encountered in specific high-dimensional problems, and we study several interesting connections between this generalized algorithm and other well-known methods. Performance comparisons between our method, the EM algorithm, Newton–Raphson, and several other optimization methods are presented using an extensive series of simulation studies based upon both real and synthetic datasets.},
  archive      = {J_CSTAT},
  author       = {Henderson, Nicholas C. and Ouyang, Zhongzhe},
  doi          = {10.1007/s00180-025-01619-0},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3883-3909},
  shortjournal = {Comput. Stat.},
  title        = {Parameter-expanded ECME algorithms for logistic and penalized logistic regression},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian nonparametric hypothesis testing methods on multiple comparisons. <em>CSTAT</em>, <em>40</em>(7), 3867-3882. (<a href='https://doi.org/10.1007/s00180-025-01615-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce Bayesian testing procedures based on the Bayes factor to compare the means across multiple populations in classical nonparametric contexts. The proposed Bayesian methods are designed to maximize the probability of rejecting the null hypothesis when the Bayes factor exceeds a specified evidence threshold. It is shown that these procedures have straightforward closed-form expressions based on classical nonparametric test statistics and their corresponding critical values, allowing for easy computation. We also demonstrate that they effectively control Type I error and enable researchers to make consistent decisions aligned with both frequentist and Bayesian approaches, provided that the evidence threshold for the Bayesian methods is set according to the significance level of the frequentist tests. Importantly, the proposed approaches allow for the quantification of evidence from empirical data in favor of the null hypothesis, an advantage that frequentist methods lack, as they cannot quantify support for the null when the null hypothesis is not rejected. We also present simulation studies and real-world applications to illustrate the performance of the proposed testing procedures.},
  archive      = {J_CSTAT},
  author       = {Hai, Qiuchen and Ma, Zhuanzhuan},
  doi          = {10.1007/s00180-025-01615-4},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3867-3882},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian nonparametric hypothesis testing methods on multiple comparisons},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Derandomized truncated D-vine copula knockoffs with e-values to control the false discovery rate. <em>CSTAT</em>, <em>40</em>(7), 3843-3866. (<a href='https://doi.org/10.1007/s00180-024-01587-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Model-X knockoffs is a practical methodology for variable selection, which stands out from other selection strategies since it allows for the control of the false discovery rate, relying on finite-sample guarantees. In this article, we propose a Truncated D-vine Copula Knockoffs (TDCK) algorithm for sampling approximate knockoffs from complex multivariate distributions. Our algorithm enhances and improves features of previous attempts to sample knockoffs under the multivariate setting, with the three main contributions being: (1) the truncation of the D-vine copula, which reduces the dependence between the original variables and their corresponding knockoffs, thus improving the statistical power; (2) the employment of a straightforward non-parametric formulation for marginal transformations, eliminating the need for a specific parametric family or a kernel density estimator; (3) the use of the “rvinecopulib” R package offers better flexibility than the existing fitting vine copula knockoff methods. To eliminate the randomness from the different sets of selected variables in distinct realizations, we wrap the TDCK method with an existing derandomizing procedure for knockoffs, leading to a Derandomized Truncated D-vine Copula Knockoffs with e-values (DTDCKe) procedure. We demonstrate the robustness of the DTDCKe procedure under various scenarios with extensive simulation studies. We further illustrate its efficacy using a gene expression dataset, showing it achieves a more reliable gene selection than other competing methods when the findings are compared with those of a meta-analysis. The results indicate that our Truncated D-vine copula approach is robust and has superior power, representing an appealing approach for variable selection in different multivariate applications, particularly in gene expression analysis.},
  archive      = {J_CSTAT},
  author       = {Vásquez, Alejandro Román and Márquez Urbina, José Ulises and González Farías, Graciela and Escarela, Gabriel},
  doi          = {10.1007/s00180-024-01587-x},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3843-3866},
  shortjournal = {Comput. Stat.},
  title        = {Derandomized truncated D-vine copula knockoffs with e-values to control the false discovery rate},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated fitting of joint models of survival and longitudinal data with cumulative variations. <em>CSTAT</em>, <em>40</em>(7), 3819-3842. (<a href='https://doi.org/10.1007/s00180-025-01639-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been well recognized that not only biomarkers but also their variability are important for predicting biomarker-related diseases. Understanding and adequately modeling the variability of biomarkers is crucial for detecting and predicting health risks, leading to improved health outcomes and patient care. However, biomarker variability modeling comes with a high computational cost, as statistical models incorporating biomarkers’ variability rely on double integrals with two nested integrations, which must be repeatedly calculated during modeling. To reduce the computational burden, we propose a novel approach aligned with arc length in mathematics to approximate and model biomarker fluctuations. Furthermore, we propose an algorithm that aligns with fast arc length evaluations for the joint modeling of survival and longitudinal data. We synthesize multiple efficient computing methods into a unified framework to accelerate the entire computational process. The core component of the acceleration is the computational efficiency of the double integrals, even when the iterated integral representation of the double integral is not possible. Finally, we illustrate the usage and benefit of our algorithm in joint models in numerical examples and the primary biliary cholangitis clinical study.},
  archive      = {J_CSTAT},
  author       = {Gao, Yan and Sparapani, Rodney A. and Tarima, Sergey},
  doi          = {10.1007/s00180-025-01639-w},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3819-3842},
  shortjournal = {Comput. Stat.},
  title        = {Accelerated fitting of joint models of survival and longitudinal data with cumulative variations},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of stress–strength reliability for the generalized inverted exponential distribution based on improved adaptive type-II progressive censoring. <em>CSTAT</em>, <em>40</em>(7), 3781-3817. (<a href='https://doi.org/10.1007/s00180-025-01612-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to estimate the reliability of a stress–strength system using the generalized inverted exponential distribution (GIED). We achieve this by employing an improved adaptive Type-II progressive censoring scheme and utilizing various estimation techniques. The techniques used include maximum likelihood estimation through the EM algorithm and Bayesian inference. We use Markov chain Monte Carlo (MCMC) methods and TK approximation in the Bayesian framework. We compute various intervals, such as asymptotic confidence, arcsin transformed, Bayesian credible, and higher posterior density confidence intervals. To guide the estimation process, we use a generalized entropy loss function. Additionally, we conduct a comprehensive simulation analysis to validate the method’s performance and rigorously assess its applicability through real-life data analysis.},
  archive      = {J_CSTAT},
  author       = {Swaroop, Chatany and Dutta, Subhankar and Saini, Shubham and Tiwari, Neeraj},
  doi          = {10.1007/s00180-025-01612-7},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3781-3817},
  shortjournal = {Comput. Stat.},
  title        = {Estimation of stress–strength reliability for the generalized inverted exponential distribution based on improved adaptive type-II progressive censoring},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive importance sampling for locally stable point processes. <em>CSTAT</em>, <em>40</em>(7), 3745-3779. (<a href='https://doi.org/10.1007/s00180-025-01609-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding the expected value of a statistic of a locally stable point process in a bounded region is addressed. We propose an adaptive importance sampling for solving the problem. In our proposal, we restrict the importance point process to the family of homogeneous Poisson point processes, which enables us to generate quickly independent samples of the importance point process. The optimal intensity of the importance point process is found by applying the cross-entropy minimization method. In the proposed scheme, the expected value of the statistic and the optimal intensity are iteratively estimated in an adaptive manner. We show that the proposed estimator converges to the target value almost surely, and prove the asymptotic normality of it. We explain how to apply the proposed scheme to the estimation of the intensity of a stationary pairwise interaction point process. The performance of the proposed scheme is compared numerically with Markov chain Monte Carlo simulation and perfect sampling.},
  archive      = {J_CSTAT},
  author       = {Kang, Hee-Geon and Kim, Sunggon},
  doi          = {10.1007/s00180-025-01609-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3745-3779},
  shortjournal = {Comput. Stat.},
  title        = {An adaptive importance sampling for locally stable point processes},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On practical implementation of the fully robust one-sided cross-validation method in the nonparametric regression and density estimation contexts. <em>CSTAT</em>, <em>40</em>(7), 3715-3743. (<a href='https://doi.org/10.1007/s00180-025-01602-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fully robust one-sided cross-validation (OSCV) method has versions in the nonparametric regression and density estimation settings. It selects the consistent bandwidths for estimating the continuous regression and density functions that might have finitely many discontinuities in their first derivatives. The theoretical results underlying the method were thoroughly elaborated in the preceding publications, while its practical implementations needed improvement. In particular, until this publication, no appropriate implementation of the method existed in the density estimation context. In the regression setting, the previously proposed implementation has a serious disadvantage of occasionally producing the irregular OSCV functions that complicates the bandwidth selection procedure. In this article, we make a substantial progress towards resolving the aforementioned issues by proposing a suitable implementation of fully robust OSCV for density estimation and providing specific recommendations for the further improvement of the method in the regression setting.},
  archive      = {J_CSTAT},
  author       = {Savchuk, Olga},
  doi          = {10.1007/s00180-025-01602-9},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3715-3743},
  shortjournal = {Comput. Stat.},
  title        = {On practical implementation of the fully robust one-sided cross-validation method in the nonparametric regression and density estimation contexts},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the economic-statistical performance of variable acceptance sampling plans based on loss function. <em>CSTAT</em>, <em>40</em>(7), 3665-3713. (<a href='https://doi.org/10.1007/s00180-024-01581-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acceptance sampling plans (ASPs) for attributes are sometimes misapplied to normal quality characteristics. When inspection costs and quality levels are high, using variable ASPs (VASPs) can be preferable. Among developed approaches to design ASPs, few studies have incorporated losses into the cost objective function. Their limited attention, such as focusing on limited random scenarios, considering only the activation of one specification limit, failing to compare VASPs with military standards still in use, and relying on time-consuming solution procedures, motivated us to utilize the advantages of loss-based economic-statistical design, evaluate four VASPs and two military standards, and presenting detailed results. Additionally, we develop the first Particle swarm optimization (PSO)-based solution procedure for designing VASPs. Numerical and real case studies, which consider the activation of lower and upper specification limits, demonstrate the superior performance of (1) the repetitive group sampling plan, (2) MIL-STD-414 over MIL-STD-105E, and (3) PSO compared to other approaches.},
  archive      = {J_CSTAT},
  author       = {Jafarian-Namin, Samrad and Fattahi, Parviz and Salmasnia, Ali},
  doi          = {10.1007/s00180-024-01581-3},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3665-3713},
  shortjournal = {Comput. Stat.},
  title        = {Assessing the economic-statistical performance of variable acceptance sampling plans based on loss function},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient-based smoothing parameter estimation for neural P-splines. <em>CSTAT</em>, <em>40</em>(7), 3645-3663. (<a href='https://doi.org/10.1007/s00180-024-01593-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the popularity of deep learning models there have recently been many attempts to translate generalized additive models to neural nets. Generalized additive models are usually regularized by a penalty in the loss function and the magnitude of penalization is controlled by one or more smoothing parameters. In the statistical literature these smoothing parameters are estimated by criteria such as generalized cross-validation or restricted maximum likelihood. While the estimation of the primary regression coefficients is well calibrated and investigated for neural net based additive models, the estimation of smoothing parameters is often either based on testing data (and grid search), implicitly estimated or completely neglected. In this paper, we address the issue of explicit smoothing parameter estimation in neural net-based additive models fitted via gradient-based methods, such as the well-known Adam algorithm. We therefore investigate the data-driven smoothing parameter selection via gradient-based optimization of generalized cross-validation and restricted maximum likelihood. Thus we do not need to calculate Hessian information of the smoothing parameters. As an additive model structure, we use a translation of P-splines to neural nets, so-called neural P-splines. The fitting process of neural P-splines as well as the gradient-based smoothing parameter selection are investigated in a simulation study and an application.},
  archive      = {J_CSTAT},
  author       = {Dammann, Lea M. and Freitag, Marei and Thielmann, Anton and Säfken, Benjamin},
  doi          = {10.1007/s00180-024-01593-z},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3645-3663},
  shortjournal = {Comput. Stat.},
  title        = {Gradient-based smoothing parameter estimation for neural P-splines},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel-diffeomorphism bayesian bootstrap filter to reduce speckle noise on SAR images. <em>CSTAT</em>, <em>40</em>(7), 3613-3643. (<a href='https://doi.org/10.1007/s00180-025-01650-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite imagery is frequently subject to degradation by noise during both image acquisition and transmission processes. The primary goal of noise reduction techniques is to remove Speckle noise while retaining critical features of the images. In remote sensing applications, Synthetic Aperture Radar (SAR) imagery plays a vital role. Speckle, a granular disturbance typically modelled as multiplicative noise, impacts SAR images as well as all coherent images, resulting in a reduction in image quality. Over the past three decades, numerous techniques have been proposed to mitigate Speckle noise in SAR imagery. This study proposes the Kernel-Diffeomorphism Bayesian Bootstrap Filter (KDBBF) as a novel method for satellite image restoration. The method relies on the multivariate Kernel Diffeomorphism estimator and the Bayesian Bootstrap Filter (BBF). Comparative analyses of the results produced by the new method with those of other image restoration techniques reveal superior performance in Speckle noise reduction in SAR imagery, both quantitatively and qualitatively.},
  archive      = {J_CSTAT},
  author       = {Zribi, Mourad and Sadok, Ibrahim and Marhaba, Bassel},
  doi          = {10.1007/s00180-025-01650-1},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3613-3643},
  shortjournal = {Comput. Stat.},
  title        = {Kernel-diffeomorphism bayesian bootstrap filter to reduce speckle noise on SAR images},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable selection method based on BIC with consistency for non-zero partial correlations under a large-dimensional setting. <em>CSTAT</em>, <em>40</em>(7), 3585-3611. (<a href='https://doi.org/10.1007/s00180-025-01628-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of selecting non-zero partial correlations under the assumption of normality. It is cumbersome to compute variable selection criteria for all subsets of variable pairs when the number of variables is large, even if it is smaller than the sample size. To tackle this problem, we propose a fast and consistent variable selection method based on Bayesian information criterion (BIC). The consistency of the method is provided in a high-dimensional asymptotic framework such that the sample size and the number of variables both tend toward infinity under a certain rule. Through numerical simulations, it is shown that the proposed method has a high probability of selecting the true subset of pairs of non-zero partial correlation.},
  archive      = {J_CSTAT},
  author       = {Yamada, Takayuki and Sakurai, Tetsuro and Fujikoshi, Yasunori},
  doi          = {10.1007/s00180-025-01628-z},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3585-3611},
  shortjournal = {Comput. Stat.},
  title        = {Variable selection method based on BIC with consistency for non-zero partial correlations under a large-dimensional setting},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning based regulatory risk index for cryptocurrencies. <em>CSTAT</em>, <em>40</em>(7), 3563-3583. (<a href='https://doi.org/10.1007/s00180-025-01629-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrency markets are highly sensitive to regulatory changes, often experiencing sharp price fluctuations in response to new policies and government interventions. Despite this, existing market indices fail to adequately capture the risks associated with regulatory uncertainty. In this paper, we introduce the Cryptocurrency Regulatory Risk Index (CRRIX), a machine learning-based index designed to quantify the impact of regulatory developments on cryptocurrency markets. Our methodology employs Latent Dirichlet Allocation (LDA) to classify policy-related news articles from major cryptocurrency news platforms, providing an objective measure of regulatory risk. We find that the CRRIX exhibits strong synchronicity with VCRIX, a cryptocurrency volatility index, suggesting that regulatory uncertainty plays a significant role in driving market fluctuations. Our results indicate that regulatory risk is a leading factor in market volatility, with major policy shifts triggering significant market movements. The proposed regulatory risk index provides a novel approach to quantifying policy uncertainty in the cryptocurrency sector, offering valuable insights for market participants navigating this rapidly changing environment.},
  archive      = {J_CSTAT},
  author       = {Ni, Xinwen and Xie, Taojun and Härdle, Wolfgang Karl and Zuo, Xiaorui},
  doi          = {10.1007/s00180-025-01629-y},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3563-3583},
  shortjournal = {Comput. Stat.},
  title        = {A machine learning based regulatory risk index for cryptocurrencies},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinate gradient descent algorithm in adaptive LASSO for pure ARCH and pure GARCH models. <em>CSTAT</em>, <em>40</em>(7), 3527-3561. (<a href='https://doi.org/10.1007/s00180-025-01642-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a coordinate gradient descent (CGD) algorithm, based on the work of Tseng and Yun (Math Program 117:387–423; 2009a; J Optim Theory Appl 140(3):513–535, 2009b), to optimize the constrained negative quasi maximum likelihood with adaptive LASSO penalization for pure autoregressive conditional heteroscedasticity (ARCH) model and its generalized form (GARCH). The strategy for choosing the appropriate values of the shrinkage parameter through information criteria (IC) is also discussed. We evaluate the numerical efficiency of the proposed algorithm through simulated data. Results of simulation studies show that for moderate sample sizes, the adaptive LASSO with the Bayesian variant of IC correctly estimates the ARCH structure at a high rate, even when model orders are over-specified. On the other hand, the adaptive LASSO has a low rate of correctly estimating true GARCH structure, especially when the model orders are over-specified regardless of the choice of IC. In our case study using daily ASX Ordinary log returns, the adaptive LASSO yields sparser ARCH and GARCH models while maintaining adequate fit for the volatility.},
  archive      = {J_CSTAT},
  author       = {Nasir, Muhammad Jaffri Mohd and Khan, Ramzan Nazim and Nair, Gopalan and Nur, Darfiana},
  doi          = {10.1007/s00180-025-01642-1},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3527-3561},
  shortjournal = {Comput. Stat.},
  title        = {Coordinate gradient descent algorithm in adaptive LASSO for pure ARCH and pure GARCH models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A newton-based variant of exclusive lasso for improved sparse solutions. <em>CSTAT</em>, <em>40</em>(7), 3505-3525. (<a href='https://doi.org/10.1007/s00180-025-01630-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exclusive Lasso offers significant advantages in scenarios that require sparse solutions within groups, such as multi-omics or gene expression analysis. These applications involve inherent grouping structures where selecting only a subset of variables from each group is crucial due to high correlations among variables within groups. However, a key challenge in optimizing Exclusive Lasso stems from the non-differentiability of the $$L_{1}$$ -norm within each group. To tackle this issue, we propose a method to transform this norm into a differentiable form using quadratic and sigmoid function approximations. This transformation facilitates the use of a straightforward Newton-based approach to solve the intricate optimization problem. Importantly, our proposed variant of Exclusive Lasso relaxes the strict requirement of selecting at least one variable per group, in contrast to the conventional Exclusive Lasso, and hence enables sparser solutions. Extensive simulation studies underscore the superior performance of our approach compared to both traditional Lasso methods and conventional Exclusive Lasso formulations.},
  archive      = {J_CSTAT},
  author       = {Ravi, Dayasri and Groll, Andreas},
  doi          = {10.1007/s00180-025-01630-5},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3505-3525},
  shortjournal = {Comput. Stat.},
  title        = {A newton-based variant of exclusive lasso for improved sparse solutions},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing non-inferiority for three-arm trials under the PH model. <em>CSTAT</em>, <em>40</em>(7), 3477-3503. (<a href='https://doi.org/10.1007/s00180-025-01624-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of non-inferiority (NI) trials is to show that a new treatment is not worse than a reference treatment by more than a pre-specified margin. For ethical reasons, NI trials usually do not include a placebo arm such that neither the assay sensitivity nor the constancy can be validated. On the other hand, three-arm NI trials consisting of the new treatment, reference treatment, and placebo, can simultaneously test the superiority of the new treatment over placebo and the NI of the new treatment compared with the reference treatment. In this article, we consider assessing NI of a new treatment in three-arm trials with time to event outcomes subject to right censoring. Under the proportional hazards model, we develop a testing procedure for assessing NI based on the infimum of ratio of survival difference between the new treatment and the placebo to that between the reference treatment and the placebo within a specific time period. The proposed test statistics involves the estimates of treatment parameters and survival function evaluated at a specific time point and their corresponding standard error estimates. Simulation study indicates that the proposed test controls the type I error well and has decent power to detect the NI under moderate to large sample settings.},
  archive      = {J_CSTAT},
  author       = {Shen, Pao-sheng},
  doi          = {10.1007/s00180-025-01624-3},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3477-3503},
  shortjournal = {Comput. Stat.},
  title        = {Testing non-inferiority for three-arm trials under the PH model},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian modeling and forecasting of seasonal autoregressive models with scale-mixtures of normal errors. <em>CSTAT</em>, <em>40</em>(7), 3453-3475. (<a href='https://doi.org/10.1007/s00180-025-01617-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing Bayesian analysis methods of time series with seasonal pattern are based on the normality assumption; however, most of the real time series violate this assumption. With assuming the scale-mixtures of normal (SMN) distribution for the model errors, we introduce the Bayesian estimation and prediction of seasonal autoregressive (SAR) models, using the Gibbs sampler and Metropolis-Hastings algorithms. The SMN distribution is a general class that includes different symmetric heavy-tailed distributions as special cases, such as the Student’s t, slash and contaminated normal distributions. With employing different priors for the SAR parameters, we derive the full conditional posterior distributions of the SAR coefficients and scale parameter to be the multivariate normal and inverse gamma, respectively, and the conditional predictive distribution of the future observations to be the multivariate normal. For the other parameters related to the SMN distribution, we derive their conditional posteriors to be in a closed form but some of them are not standard distributions. Using the derived closed-form conditional posterior and predictive distributions, we propose the Gibbs sampler with the Metropolis-Hastings algorithm to approximate empirically the marginal posterior and predictive distributions. We introduce an extensive simulation study and a real application in order to evaluate the accuracy of the proposed MCMC algorithm.},
  archive      = {J_CSTAT},
  author       = {Amin, Ayman A.},
  doi          = {10.1007/s00180-025-01617-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3453-3475},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian modeling and forecasting of seasonal autoregressive models with scale-mixtures of normal errors},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate bayesian inference in a model for self-generated gradient collective cell movement. <em>CSTAT</em>, <em>40</em>(7), 3399-3452. (<a href='https://doi.org/10.1007/s00180-025-01606-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we explore parameter inference in a novel hybrid discrete-continuum model describing the movement of a population of cells in response to a self-generated chemotactic gradient. The model employs a drift-diffusion stochastic process, rendering likelihood-based inference methods impractical. Consequently, we consider approximate Bayesian computation (ABC) methods, which have gained popularity for models with intractable or computationally expensive likelihoods. ABC involves simulating from the generative model, using parameters from generated observations that are “close enough” to the true data to approximate the posterior distribution. Given the plethora of existing ABC methods, selecting the most suitable one for a specific problem can be challenging. To address this, we employ a simple drift-diffusion stochastic differential equation (SDE) as a benchmark problem. This allows us to assess the accuracy of popular ABC algorithms under known configurations. We also evaluate the bias between ABC-posteriors and the exact posterior for the basic SDE model, where the posterior distribution is tractable. The top-performing ABC algorithms are subsequently applied to the proposed cell movement model to infer its key parameters. This study not only contributes to understanding cell movement but also sheds light on the comparative efficiency of different ABC algorithms in a well-defined context.},
  archive      = {J_CSTAT},
  author       = {Devlin, Jon and Borowska, Agnieszka and Husmeier, Dirk and Mackenzie, John},
  doi          = {10.1007/s00180-025-01606-5},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3399-3452},
  shortjournal = {Comput. Stat.},
  title        = {Approximate bayesian inference in a model for self-generated gradient collective cell movement},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Informative right censoring in nonparametric survival models. <em>CSTAT</em>, <em>40</em>(7), 3385-3397. (<a href='https://doi.org/10.1007/s00180-025-01610-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis models allow us to analyze and predict the time until a certain event occurs. Existing nonparametric models assume that the censoring of observations is random and unrelated to the study conditions. The estimators of the survival and hazard functions assume a constant survival probability between modes, have poor interpretability for datasets with multimodal time distributions, and lead to poor-quality data descriptions. In this paper, we investigate the quality of nonparametric models on four medical datasets with informative censoring and multimodal time distribution and propose a modification to improve the description quality. Proved properties of IBS and AUPRC metrics show that the best quality is achieved at survival function with unimodal time distribution. We propose modifying the nonparametric model based on virtual events from a truncated normal distribution that allows for the suppression of informative censoring. We compared the quality of the nonparametric models on multiple random subsets of datasets of different sizes using the AUPRC and IBS metrics. According to the comparison of the quality using Welch’s test, the proposed model with virtual events significantly outperformed the existing Kaplan–Meier model for all datasets (p-value $$<10^{-6}$$ ). The quality increase of IBS is from 6.91 to 21.92%, and the quality increase of AUPRC is from 12.92 to 18.4%. The nonparametric models with virtual events provide a better interpretation, allow a better description of the observed data, and are stable in terms of the informativeness of censoring. The proposed method is embedded in an open-source survivors Python library.},
  archive      = {J_CSTAT},
  author       = {Vasilev, Iulii and Petrovskiy, Mikhail and Mashechkin, Igor},
  doi          = {10.1007/s00180-025-01610-9},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3385-3397},
  shortjournal = {Comput. Stat.},
  title        = {Informative right censoring in nonparametric survival models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KNN estimators for time series prediction: A functional partial linear single index model with missing responses and error-prone covariates. <em>CSTAT</em>, <em>40</em>(7), 3359-3384. (<a href='https://doi.org/10.1007/s00180-024-01573-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a functional partial linear single index model for strong $$\alpha$$ -mixing functional time series data when the responses are missing not at random and the real covariates are observed with measurement errors. We first extend three insertion methods developed for regression models with missing responses in finite dimensions, namely imputation, semiparametric regression surrogate and inverse marginal probability weighted approaches, to functional scenarios when the responses are scalar. Then the attenuation correction method is employed to eliminate the impact of measurement error on model estimation of unknown parameter in linear component, after we completing the missingness of responses by using above insertion methods. Meanwhile, we combine the kNN approach with insertion and attenuation correction approaches to capture the local structure of functional time series data and provide the estimations of unknown operators in the estimation process. The asymptotic properties of unknown parameters in the model are established under some mild assumptions. Furthermore, we make a comparison of three insertion methods, the oracle method and the ignoring method in simulation study and electricity consumption data analysis. All results indicate that our methodology has good performance.},
  archive      = {J_CSTAT},
  author       = {Meng, Shuyu and Huang, Zhensheng and Ling, Nengxiang},
  doi          = {10.1007/s00180-024-01573-3},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3359-3384},
  shortjournal = {Comput. Stat.},
  title        = {KNN estimators for time series prediction: A functional partial linear single index model with missing responses and error-prone covariates},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="dmkd">DMKD - 18</h2>
<ul>
<li><details>
<summary>
(2025). TSelect: Selecting relevant and non-redundant channels for multivariate time series classification. <em>DMKD</em>, <em>39</em>(6), 1-62. (<a href='https://doi.org/10.1007/s10618-025-01132-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many time series classification tasks, each instance is described by multiple channels (i.e., signals). This introduces an additional computational burden as the time and resources to train a classifier increase as more channels become available. This is problematic as some tasks can be described by a huge number of channels. However, not all channels may be necessary as some could be irrelevant or redundant, and including these can (dramatically) increase run time while yielding no benefit in terms of predictive performance. Therefore, it can be useful to automatically select a subset of the channels to include in the analysis. We propose TSelect, a novel scalable and classifier-agnostic approach that automatically selects a relevant and non-redundant subset of the channels for multivariate time series classification (MTSC). Experimentally, we show on a large benchmark suite that TSelect (1) eliminates on average 62% of the channels, (2) significantly improves a classifier’s run time without sacrificing predictive performance, and (3) outperforms the two state-of-the-art channel selectors (ECS and ECP) on the majority of the experiments.},
  archive      = {J_DMKD},
  author       = {Nuyts, Loren and Perini, Lorenzo and Davis, Jesse},
  doi          = {10.1007/s10618-025-01132-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-62},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TSelect: Selecting relevant and non-redundant channels for multivariate time series classification},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topic-aware influence maximization with deep reinforcement learning and graph attention networks. <em>DMKD</em>, <em>39</em>(6), 1-35. (<a href='https://doi.org/10.1007/s10618-025-01133-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization is a fundamental problem in network analysis, focusing on identifying a subset of nodes in a social network to maximize the spread of influence. In this paper, we present an approach for tackling the Influence Maximization (IM) problem, integrating Deep Reinforcement Learning (DRL) techniques with attentive Graph Neural Networks (GATs). Our study builds upon a prior algorithm (S2V-DQN-IM) and progressively refines it towards IM-GNN, ultimately achieving competitive performance against state-of-the-art methods on classic IM. Through experiments on benchmark datasets, we empirically validate the effectiveness of graph attention mechanisms and positional encoding, using the graph magnetic Laplacian, to reach state-of-the-art performance in terms of influence spread. Building on this success, we extend our IM-GNN framework to incorporate topic-awareness in TIM-GNN, recognizing the inherent topical nature of real-world diffusions. By harnessing probabilistic techniques, we construct topic-aware social graphs using real cascades and assess the effectivenesss of TIM-GNN on them. Our extensive experimental results validate the utility of our topic-aware approach, demonstrating significant advances over existing topic-aware IM methods. Finally, in order to improve upon performance (latency) at query time, we develop a variant of TIM-GNN, called TIM-GNN $$^x$$ , by using cross-attention mechanisms. We show it maintains comparable overall spread performance as its predecessor, while achieving a 10x-20x speed-up.},
  archive      = {J_DMKD},
  author       = {Halal, Taha and Cautis, Bogdan and Groz, Benoît and Gao, Ruize},
  doi          = {10.1007/s10618-025-01133-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Topic-aware influence maximization with deep reinforcement learning and graph attention networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multimodal graph learning for knowledge graph completion. <em>DMKD</em>, <em>39</em>(6), 1-27. (<a href='https://doi.org/10.1007/s10618-025-01139-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal knowledge graphs, which integrate visual and textual data, have been widely utilized in various applications. Despite their potential, they often suffer from incompleteness due to the large amount of undiscovered valuable triple knowledge within the graph. This has led to a surge in research on multimodal knowledge graph completion. However, existing methods often face challenges such as irrelevant noise in multimodal data and limitations of straightforward multimodal fusion, which can lead to suboptimal model performance. In this paper, we propose a novel adaptive multimodal graph learning approach for efficient knowledge graph completion. We first introduce an adaptive multimodal knowledge capture module designed to integrate entity-related multimodal knowledge. This module includes a relation-aware modal view construction process to achieve semantic meaning consistency, along with link-aware point-to-face interactions for coarse-grained multimodal capture and adaptive point-to-point interactions for fine-grained multimodal extraction. We then propose a two-stage multimodal graph fusion module, which includes a cross-modal augmentation module to perform first-stage multimodal fusion between the entity graph and visual/textual graph, as well as a dynamic selection fusion module to conduct the second-stage fusion between entity-visual and entity-textual graphs. Our method demonstrates superior effectiveness through empirical evaluations on three common datasets.},
  archive      = {J_DMKD},
  author       = {Peng, Jie and Shan, Yongxue and Zha, Yongfu and Wang, Xiaodong},
  doi          = {10.1007/s10618-025-01139-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Adaptive multimodal graph learning for knowledge graph completion},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection and evaluation of clusters within sequential data. <em>DMKD</em>, <em>39</em>(6), 1-30. (<a href='https://doi.org/10.1007/s10618-025-01140-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential data is ubiquitous—it is routinely gathered to gain insights into complex processes such as behavioral, biological, or physical processes. Challengingly, such data not only has dependencies within the observed sequences, but the observations are also often high-dimensional, sparse, and noisy. These are all difficulties that obscure the inner workings of the complex process under study. One solution is to calculate a low-dimensional representation that describes (characteristics of) the complex process. This representation can then serve as a proxy to gain insight into the original process. However, uncovering such low-dimensional representation within sequential data is nontrivial due to the dependencies, and an algorithm specifically made for sequences is needed to guarantee estimator consistency. Fortunately, recent theoretical advancements on Block Markov Chains have resulted in new clustering algorithms that can provably do just this in synthetic sequential data. This paper presents a first field study of these new algorithms in real-world sequential data; a wide empirical study of clustering within a range of data sequences. We investigate broadly whether, when given sparse high-dimensional sequential data of real-life complex processes, useful low-dimensional representations can in fact be extracted using these algorithms. Concretely, we examine data sequences containing GPS coordinates describing animal movement, strands of human DNA, texts from English writing, and daily yields in a financial market. The low-dimensional representations we uncover are shown to not only successfully encode the sequential structure of the data, but also to enable gaining new insights into the underlying complex processes.},
  archive      = {J_DMKD},
  author       = {Van Werde, Alexander and Senen–Cerda, Albert and Kosmella, Gianluca and Sanders, Jaron},
  doi          = {10.1007/s10618-025-01140-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Detection and evaluation of clusters within sequential data},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fastere: A fast framework for entity relation extractions. <em>DMKD</em>, <em>39</em>(6), 1-27. (<a href='https://doi.org/10.1007/s10618-025-01146-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity Relation Extraction (ERE) aims to identify semantic relations between entities from unstructured texts. Despite achieving promising results, existing methods face significant efficiency challenges due to the overwhelming presence of irrelevant candidate entity pairs–with over 96% of candidate pairs lacking meaningful relations in typical datasets. This computational burden severely limits the practical applicability of ERE systems. To address these limitations, we propose FastERE, a fast framework that introduces three key innovations: (1) a pruner-based Entity Pairs Selection Module that dynamically filters irrelevant entity pairs before relation extraction, reducing computational overhead while improving sample quality; (2) an efficient task reformulation that frames NER as sequence tagging with adjacent attention mechanisms and RE as grouped triplet prediction with masked parallel packing; and (3) a multi-task learning framework with prefix-enhanced shared encoders that enables effective feature interaction while maintaining task independence. Experimental results on benchmark datasets demonstrate that FastERE achieves 6-20 $$\times $$ speedup over state-of-the-art methods while maintaining competitive accuracy, with F1 scores of 69.9% and 67.0% on ACE05 and ACE04 respectively. Our code is available at https://github.com/xerrors/FastERE .},
  archive      = {J_DMKD},
  author       = {Zhang, Wenjie and Xu, Tianyang and Hua, Yang and Feng, Zhenhua and Song, Xiaoning},
  doi          = {10.1007/s10618-025-01146-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fastere: A fast framework for entity relation extractions},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metadata supported scale space attention networks for multivariate timeseries prediction. <em>DMKD</em>, <em>39</em>(6), 1-46. (<a href='https://doi.org/10.1007/s10618-025-01151-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate two problems faced in detecting/predicting intracranial hypertension (ICH) onsets. The first challenge is that intracranial pressure (ICP) can be measured only invasively, drilling a hole in patients skull. To tackle this challenge, we propose a novel metadata supported scale space attention (SSA) network to accurately and non-invasively predict intracranial hypertension (ICH) onsets, relying on supporting multi-variate data streams, including electroencephalogram (EEG), arterial blood pressure (ABP) and electrocardiogram (ECG). In particular, we propose a coupled network, integrating a convolutional neural network with scale space attention (CNN-SSA) and a long short-term memory (LSTM) network. This, however, opens up a new challenge—EEG readings at different health centers may rely on hardware with different numbers of sensors, which would render models not shareable across centers. To tackle this second challenge, we propose two alternative techniques: the first alternative relies on a multi-variate multi-scale neural network (M2NN) to impute missing EEG channels; the second alternative, on the other hand, identifies a core subset of EEG channels, shared by different centers, that can still provide accurate ICH predictions. We conduct rigorous experiments to substantiate our proposed method’s effectiveness—the results show that ICH can be predicted accurataly by relying on supporting data modalities, if we can properly attend the data at multiple scales and across modalities. Additionally, an ablation study demonstrates that the various components of our approach are essential in providing accurate ICH predictions.},
  archive      = {J_DMKD},
  author       = {Ravindranath, Manjusha and Candan, K. Selçuk and Appavu, Brian},
  doi          = {10.1007/s10618-025-01151-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-46},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Metadata supported scale space attention networks for multivariate timeseries prediction},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overlapping community detection with a new modularity measure in directed weighted networks. <em>DMKD</em>, <em>39</em>(6), 1-37. (<a href='https://doi.org/10.1007/s10618-025-01153-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying overlapping communities in complex networks is an important aspect of network analysis. This task, however, becomes extremely difficult when working with large-scale directed weighted networks, such as social media networks like Facebook, X, LinkedIn, biological systems, collaboration networks, neural networks, etc., which may consist of millions of entities. There are very few algorithms available for this task, and most are merely extensions of well-known algorithms designed for undirected networks. Besides, these algorithms often produce disjoint and low-quality community structures. On the other hand, evaluating overlapping community structures in directed weighted networks is also a major challenge due to the lack of specific quality measure designed for such networks. To address these challenges, we developed an algorithm called Community Detection in Directed And Weighted Networks (CD-DAWN). It is designed to identify overlapping communities in large, complex networks by selecting and expanding seeds, without needing any prior knowledge about the communities. We also developed a directed weighted overlapping modularity ( $$Q_\text {dwo}$$ ) to evaluate overlapping community structures in directed weighted networks, which is the first such measure to the best of our knowledge. To evaluate the effectiveness of CD-DAWN, we performed extensive experiments on both real-world and artificial networks. The results demonstrate that the proposed approach outperforms the baseline algorithms, accurately detecting high-quality and stable overlapping communities in complex networks.},
  archive      = {J_DMKD},
  author       = {Kumar, Abhinav and Kumari, Anjali and Kumar, Pawan and Dohare, Ravins},
  doi          = {10.1007/s10618-025-01153-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Overlapping community detection with a new modularity measure in directed weighted networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stable graph based decision route explanation in siamese neural networks. <em>DMKD</em>, <em>39</em>(6), 1-32. (<a href='https://doi.org/10.1007/s10618-025-01154-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese Neural Networks (SNNs) have shown promise in addressing a variety of tasks, even with limited data availability. However, their adoption is hindered by the lack of transparency in their decision-making processes. A key challenge in explaining SNNs lies in the absence of an inverse mapping between high-dimensional input feature vectors and the low-dimensional embedding space. Therefore, computing direct distances between input features becomes meaningless. Existing autoencoder-based explanation methods face several limitations. These include poor image reconstruction quality due to insufficient data and the omission of final distance layer of the SNN during the explanation process. While the Siamese Network Explainer (SINEX) can explain audio and grayscale images, it does not support RGB images. To overcome these challenges, we propose a method called Features Distance-based eXplanation (FDbX). This approach identifies salient features using ridge regression, trained on perturbed SLIC-segmented images. To enhance the selection of important features, we incorporate Bayesian analysis, which assigns importance scores to features. To provide a comprehensive explanation of the decision route, we construct a mathematical model that represents important features and their Hamming distances as a bipartite graph. In this graph, nodes represent features and edges denote distances between feature pairs. The resulting explanation heatmaps highlight critical image segments, offering more intuitive and visually informative explanations than existing methods. We evaluate stability and faithfulness of our method using stability indices such as $$R^2$$ and mean squared error. To the best of our knowledge, this is the first work to introduce Variable and Coefficient Stability Indices for image datasets.},
  archive      = {J_DMKD},
  author       = {Saleem, Rabia and Anjum, Ashiq and Yuan, Bo and Liu, Lu},
  doi          = {10.1007/s10618-025-01154-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-32},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Stable graph based decision route explanation in siamese neural networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distilcyphergpt: Enhancing large language models for knowledge graph question answering in cypher through knowledge distillation. <em>DMKD</em>, <em>39</em>(6), 1-36. (<a href='https://doi.org/10.1007/s10618-025-01157-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Question Answering (KGQA) systems allow users to interact with knowledge graphs using natural language queries, which are translated into structured database queries like Cypher. Existing KGQA approaches often rely on large language models, leading to high computational costs and slower inference times that impede real-time applications. To address these challenges, DistilCypherGPT is introduced as an efficient KGQA framework employing knowledge distillation in a teacher-student architecture, optimized for Cypher query generation on academic knowledge graphs. DistilCypherGPT significantly reduces computational demands, enabling deployment in resource-constrained environments while retaining high accuracy. Experimental results show that DistilCypherGPT maintains 99.51% accuracy, achieving a 23% reduction in model size and a 30% improvement in inference speed compared to the baseline. These findings corroborate DistilCypherGPT’s potential as a scalable, high-performance solution for KGQA, advancing efficient, real-time query translation with minimal computational overhead.},
  archive      = {J_DMKD},
  author       = {Chong, You Li and Lee, Chin Poo and Lim, Kian Ming},
  doi          = {10.1007/s10618-025-01157-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-36},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Distilcyphergpt: Enhancing large language models for knowledge graph question answering in cypher through knowledge distillation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HRHE: Hybrid relations-guided multi-modal knowledge graph completion using hierarchical embeddings. <em>DMKD</em>, <em>39</em>(6), 1-31. (<a href='https://doi.org/10.1007/s10618-025-01150-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal knowledge graph completion (MKGC) has garnered substantial research interest, particularly in applying multi-modal knowledge graphs (MMKGs). Previous studies have proposed various approaches to address the MKGC problem, including a variety of knowledge graph embedding techniques and approaches based on large language models. However, these methods face two limitations. First, many models embed entities and relations in a regular embedding space, failing to capture multi-modal components’ hierarchical semantics. Second, existing methods struggle to learn hybrid relations between entities of different modalities, which constrains the performance of MKGC models. In this paper, we propose a novel model, Hybrid Relations-guided MKGC with Hierarchical Embeddings (HRHE), to address the aforementioned limitations. Specifically, HRHE projects MMKGs into a polar coordinate system to capture hierarchical semantics and facilitate intra-inter-class interactions. Subsequently, HRHE introduces a general hybrid relations learning approach to discover implicit relations among entities of different modalities. Noting that our work can flexibly employ various neural network architectures, including convolutional neural networks, multi-layer perceptrons, and linear graph networks, to infer the implicit associations of MMKGs. Finally, an objective function is defined to leverage the inferred hybrid relations in guiding the modeling process of MMKGs. We investigate HRHE using two widely evaluated benchmarks and show that HRHE achieves better results than state-of-the-art MKGC baselines. Extensive evaluations demonstrate that constructing hierarchical semantic space and utilizing implicit associations among entities of different modalities can advance MKGC.},
  archive      = {J_DMKD},
  author       = {Lu, Xinyu and Li, Hao and Lu, Wei and Chang, Yanshuo and Xue, Feng},
  doi          = {10.1007/s10618-025-01150-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {HRHE: Hybrid relations-guided multi-modal knowledge graph completion using hierarchical embeddings},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models are zero-shot point-of-interest recommenders. <em>DMKD</em>, <em>39</em>(6), 1-46. (<a href='https://doi.org/10.1007/s10618-025-01148-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-of-interest (POI) recommendation systems play an important role in various location-based services by improving the user experience. Previous research has leveraged large-scale visit records to predict a user’s next visit POI based on the behavior of similar users. However, with the increasing emphasis on privacy preservation, there is a shift towards zero-shot recommendation that does not require training and only uses individual visit history data. As a better alternative to traditional zero-shot recommender systems, this paper proposes a novel zero-shot recommender system leveraging the ability of pre-trained large language models (LLMs) to understand human behavior called ZeroPOIRec. ZeroPOIRec involves a profiler module that enables LLMs to extract individual user preferences from multiple aspects, including spatio-temporal patterns and individual characteristics, and a recommender module that enhances the zero-shot POI recommendation performance via candidate refinement and prioritization. Through experiments using a benchmark dataset and a newly introduced real-world dataset with semantic variables, we demonstrate that, despite ZeroPOIRec being a zero-shot approach, it outperforms state-of-the-art methods in terms of recommendation performance.},
  archive      = {J_DMKD},
  author       = {Kim, Joeun and Seo, Youngjin and Kim, Yeonsoo and Kang, Junhyeok and Shin, Jeeho and Trirat, Patara and Lee, Jae-Gil},
  doi          = {10.1007/s10618-025-01148-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-46},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Large language models are zero-shot point-of-interest recommenders},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JammyTS: Joint attention and memory network for temporal scoping of facts. <em>DMKD</em>, <em>39</em>(6), 1-25. (<a href='https://doi.org/10.1007/s10618-025-01156-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Scoping of Facts is crucial for completing the temporal dimension of knowledge graphs. Current mainstream methods rely heavily on external resources for mining temporal information. However, the presence of noise in external resources, coupled with limitations in adaptively inferring non-continuous temporal dimensions with multiple temporal ranges, leads to low accuracy in predicting temporal ranges. To address these challenges, a model named JammyTS is proposed, which Joins an attention mechanism and a memory network for Temporal Scoping of facts. Specifically, JammyTS leverages attention to adjust the distribution of weights dynamically in memory networks and builds attention capsule-based networks to reduce the impact of noise in external resources. Furthermore, two linear classifiers are separately trained to infer the end and beginning timestamps of facts for inference of non-continuous temporal ranges. Extensive experiments on three datasets show that JammyTS improves the accuracy by up to 12.29% compared to the state-of-the-art.},
  archive      = {J_DMKD},
  author       = {Hu, Chenxi and Wu, Tao and Liu, Chunsheng and Chang, Chao},
  doi          = {10.1007/s10618-025-01156-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {JammyTS: Joint attention and memory network for temporal scoping of facts},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting and reacting to smart home novelties. <em>DMKD</em>, <em>39</em>(6), 1-32. (<a href='https://doi.org/10.1007/s10618-025-01158-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To be robust, AI systems need to quickly detect and effectively react to novelties in their environments. Novelty is characterized by a sudden change in the environment where this is little time to collect new data and retrain. This is particularly true for smart home systems, where novelties abound and accurate handling is required for reliable health monitoring and home automation that can react quickly and effectively to the novelties. In this paper, we introduce a Bayesian nonparametric method, called OTACON, for novelty handling. OTACON finds surprising situations using change point detection and adapts accordingly. To evaluate this proposed approach, we design a smart home novelty generator that embeds novelties of varying type and difficulty into CASAS real-world smart home datasets. We observe that OTACON outperforms a state-of-the-art method for eight types of novel scenarios, in some cases even outperforming its own pre-novelty baseline. The results provide evidence that this method can boost AI systems in their ability to handle a variety of unexpected situations.},
  archive      = {J_DMKD},
  author       = {Holder, Lawrence B. and Eaves, Baxter and Shafto, Patrick and Pereyda, Christopher and Thomas, Brian and Cook, Diane J.},
  doi          = {10.1007/s10618-025-01158-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-32},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Detecting and reacting to smart home novelties},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optirefine: Densest subgraphs and maximum cuts with k refinements. <em>DMKD</em>, <em>39</em>(6), 1-68. (<a href='https://doi.org/10.1007/s10618-025-01142-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-analysis tasks often involve an iterative process, which requires refining previous solutions. For instance, when analyzing social networks, we may obtain initial communities based on noisy metadata, and we want to improve them by adding influential nodes and removing non-important ones, without making too many changes. However, classic optimization algorithms, which typically find solutions from scratch, potentially return communities that are very dissimilar to the initial one. To mitigate these issues, we introduce the OptiRefine framework. The framework optimizes initial solutions by making a small number of refinements, thereby ensuring that the new solution remains close to the initial solution and simultaneously achieving a near-optimal solution for the optimization problem. We apply the OptiRefine framework to two classic graph-optimization problems: densest subgraph and maximum cut. For the densest-subgraph problem, we optimize a given subgraph’s density by adding or removing k nodes. We show that this novel problem is a generalization of k-densest subgraph, and provide constant-factor approximation algorithms for $$k=\Omega (n)$$ refinements. We also study a version of maximum cut in which the goal is to improve a given cut. We provide connections to the maximum cut with cardinality constraints and provide an optimal approximation algorithm in most parameter regimes under the Unique Games Conjecture for $$k=\Omega (n)$$ refinements. We evaluate our theoretical methods and scalable heuristics on synthetic and real-world data and show that they are highly effective in practice.},
  archive      = {J_DMKD},
  author       = {Tu, Sijing and Stankovic, Aleksa and Neumann, Stefan and Gionis, Aristides},
  doi          = {10.1007/s10618-025-01142-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-68},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Optirefine: Densest subgraphs and maximum cuts with k refinements},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPARAFAC2: Tracking evolving patterns in (incomplete) temporal data. <em>DMKD</em>, <em>39</em>(6), 1-33. (<a href='https://doi.org/10.1007/s10618-025-01122-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor factorizations have been widely used for the task of uncovering patterns in various domains. Often, the input is time-evolving, shifting the goal to tracking the evolution of the underlying patterns instead. To adapt to this more complex setting, existing methods incorporate temporal regularization but they either have overly constrained structural requirements or lack uniqueness which is crucial for interpretation. In this paper, in order to capture the underlying evolving patterns, we introduce t(emporal)PARAFAC2, which utilizes temporal smoothness regularization on the evolving factors. Previously, Alternating Optimization and Alternating Direction Method of Multipliers-based algorithmic approach has been introduced to fit the PARAFAC2 model to fully observed data. In this paper, we extend this algorithmic framework to the case of partially observed data and use it to fit the tPARAFAC2 model to complete and incomplete datasets with the goal of revealing evolving patterns. Our numerical experiments on simulated datasets demonstrate that tPARAFAC2 can extract the underlying evolving patterns more accurately compared to the state-of-the-art in the presence of high amounts of noise and missing data. Using two real datasets, we also demonstrate the effectiveness of the algorithmic approach in terms of handling missing data and tPARAFAC2 model in terms of revealing evolving patterns. The paper provides an extensive comparison of different approaches for handling missing data within the proposed framework, and discusses both the advantages and limitations of tPARAFAC2 model.},
  archive      = {J_DMKD},
  author       = {Chatzis, Christos and Schenker, Carla and Pfeffer, Max and Acar, Evrim},
  doi          = {10.1007/s10618-025-01122-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-33},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TPARAFAC2: Tracking evolving patterns in (incomplete) temporal data},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fed-FUEL: Fairness and utility enhancing agnostic federated learning framework. <em>DMKD</em>, <em>39</em>(6), 1-30. (<a href='https://doi.org/10.1007/s10618-025-01152-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging communication-efficient and collaborative learning paradigm of machine learning with privacy guarantees. As these advancements unfold, adapting FL for fairness-aware learning becomes crucial. In this context, we propose a pre-processing fairness and utility (balanced accuracy) enhancing agnostic federated learning framework (Fed-FUEL) that mitigates discrimination embedded in the non-independent identically distributed data. We contribute a novel adaptive data manipulation method that mitigates discrimination embedded in the data at client side during optimization, resulting in an optimized and fair centralized server. This pre-processing approach abstracts the model architecture from the equation, offering a significant advantage in a federated environment. This abstraction not only facilitates a broader application across diverse model architectures without necessitating modifications but also sidesteps the potential complexities and inefficiencies associated with model-specific in-processing methods. Extensive experiments with a range of publicly available datasets demonstrate that our method outperforms the competing baselines in terms of both discrimination mitigation and predictive performance. Our model effectively adapts to both statistical and causal fairness notions, as shown through our experiments.},
  archive      = {J_DMKD},
  author       = {Badar, Maryam and Younis, Raneen and Sikdar, Sandipan and Nejdl, Wolfgang and Fisichella, Marco},
  doi          = {10.1007/s10618-025-01152-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fed-FUEL: Fairness and utility enhancing agnostic federated learning framework},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RMIDDM: An unsupervised and interpretable concept drift detection method for data streams. <em>DMKD</em>, <em>39</em>(6), 1-28. (<a href='https://doi.org/10.1007/s10618-025-01155-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional machine learning techniques assume that data is drawn from a stationary source. This assumption is challenged in contexts with data streams for presenting constant and potentially infinite sequences whose distribution is prone to change over time. Based on these settings, detecting changes (a.k.a. concept drifts) is necessary to keep learning models up-to-date. Although state-of-the-art detection methods were designed to monitor the loss of predictive models, such monitoring falls short in many real-world scenarios where the true labels are not readily available. Therefore, there is increasing attention to unsupervised concept drift detection methods as approached in this paper. In this work, we present an unsupervised and interpretable method based on Radial Basis Function Networks (RBFN) and Markov Chains (MC), referred to as RMIDDM (Radial Markov Interpretable Drift Detection Method). In our method, RBF performs, in the intermediate layer, an activation process that implicitly produces groups of observations collected over time. Simultaneously, MC models the transitions between groups to support the detection of concept drifts, which happens when the active group changes and its probability exceeds a given threshold. A set of experiments with synthetic datasets and comparisons with state-of-the-art algorithms demonstrated that the proposed method can detect drifts at runtime in an efficient, interpretable, and independent way of labels, presenting competitive results and behavior. Additionally, to show its applicability in a real-world scenario, we analyzed new COVID-19 cases, deaths, and vaccinations to identify new waves as concept drifts and generate Markov models that allow understanding of their interaction.},
  archive      = {J_DMKD},
  author       = {Neto, Ruivaldo and Alencar, Brenno and Gomes, Heitor Murilo and Bifet, Albert and Gama, João and Cassales, Guilherme and Rios, Ricardo},
  doi          = {10.1007/s10618-025-01155-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {RMIDDM: An unsupervised and interpretable concept drift detection method for data streams},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Steering the LoCoMotif: Using domain knowledge in time series motif discovery. <em>DMKD</em>, <em>39</em>(6), 1-31. (<a href='https://doi.org/10.1007/s10618-025-01143-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Motif Discovery (TSMD) identifies repeating patterns in time series data, but its unsupervised nature might result in motifs that are not interesting to the user. To address this, we propose a framework that allows the user to impose constraints on the motifs to be discovered, where constraints can easily be defined according to the properties of the desired motifs in the application domain. We also propose an efficient implementation of the framework, the LoCoMotif-DoK algorithm. We demonstrate that LoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic data, outperforming other TSMD techniques which only support a limited form of domain knowledge.},
  archive      = {J_DMKD},
  author       = {Yurtman, Aras and Van Wesenbeeck, Daan and Meert, Wannes and Blockeel, Hendrik},
  doi          = {10.1007/s10618-025-01143-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Steering the LoCoMotif: Using domain knowledge in time series motif discovery},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ei">EI - 18</h2>
<ul>
<li><details>
<summary>
(2025). Group decision making approach based on linguistic q-rung orthopair fuzzy yager power weighted geometric aggregation operator for selection of nanoparticle in drug delivery system. <em>EI</em>, <em>18</em>(5), 1-21. (<a href='https://doi.org/10.1007/s12065-025-01064-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer remains one of the most prevalent and deadly forms of cancer worldwide, with a high mortality rate and limited treatment options available. Traditional chemotherapy and radiation therapy have been the mainstays of lung cancer treatment, but their efficacy is often limited by systemic toxicity and drug resistance. Recently, many nanoparticles have been identified and created for drug delivery with the ability to specifically target tumor cells while sparing healthy cells or organs. The nanoparticle-based drug delivery system offers distinct benefits over conventional medication delivery, including increased permeability and retention effect, precision targeting, and improved stability and biocompatibility. Therefore, this paper proposes the group decision making approach under linguistic q-rung orthopair fuzzy environment to select the best nanoparticle in drug delivery system for lung cancer treatment. Firstly, this paper proposes linguistic q-rung orthopair fuzzy Yager power weighted geometric aggregation operator for aggregating the linguistic q-rung orthopair fuzzy numbers based on the Yager’s norm. This paper also demonstrates several features of the proposed operator. Moreover, by using the proposed operator, this paper presents the group decision making approach. Furthermore, a case study is presented for the selection of optimal nanoparticle in drug delivery for the treatment of lung cancer to demonstrate the proposed decision making approach’s applicability. Finally, this paper presents comparison study, sensitivity analysis and superiority of proposed decision making approach with the other existing decision making methods to validate the proposed decision making approach.},
  archive      = {J_EI},
  author       = {Neelam and Bhardwaj, Reeta and Bhalla, Sonia and Thakur, Atul and Kumar, Kamal},
  doi          = {10.1007/s12065-025-01064-4},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Evol. Intell.},
  title        = {Group decision making approach based on linguistic q-rung orthopair fuzzy yager power weighted geometric aggregation operator for selection of nanoparticle in drug delivery system},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pythagorean fuzzy similarity measure with significance in MCDM and transportation challenges. <em>EI</em>, <em>18</em>(5), 1-29. (<a href='https://doi.org/10.1007/s12065-025-01075-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A potent extension of intuitionistic fuzzy sets that tackles ambiguity and uncertainty at a higher level is the Pythagorean fuzzy set ( $$\mathbb {P_YFS}$$ ). When assessing the degree of similarity between two fuzzy sets, the similarity measure is a helpful tool. It is used in typical decision-making scenarios. In this study, a new way to determining the degree of similarity is presented and contrasted with previous methods. All of the shortcomings of the existing measures can be mitigated by the newly suggested measure. The usefulness of this measure in using the modified Vlekriterijumsko KOmpromisno Rangiranje (VIKOR) approach to handle decision-making problems is illustrated with a few instances. Additionally, relevant examples are provided to illustrate how it could be used to challenges relating to transportation. The suggested method accomplishes the primary objective of the transportation problem, which is reduced transportation costs, when compared to other options.},
  archive      = {J_EI},
  author       = {Bajaj, Jyoti and Kumar, Satish},
  doi          = {10.1007/s12065-025-01075-1},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-29},
  shortjournal = {Evol. Intell.},
  title        = {Pythagorean fuzzy similarity measure with significance in MCDM and transportation challenges},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dendritic neuron model optimized with mountain gazelle optimizer for time-series forecasting. <em>EI</em>, <em>18</em>(5), 1-11. (<a href='https://doi.org/10.1007/s12065-025-01078-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series forecasting, prediction, and estimation have been active research topics in recent years due to their critical role in decision-making and future planning. Artificial neural networks (ANNs) are widely employed for forecasting applications and have demonstrated significant performance improvements. In this paper, we utilize a specialized type of ANN, the dendritic neuron model (DNM), for time-series forecasting. We apply an optimized DNM model by utilizing the power of metaheuristic (MH) optimization algorithms. Specifically, we employ an MH algorithm, the mountain gazelle optimizer (MGO), to optimize the parameters of the DNM, improve its configuration process, and ultimately improve the accuracy of the forecast. Furthermore, we compare MGO performance with several other optimization algorithms used to optimize the DNM model. Comprehensive evaluations are conducted on four publicly available time series datasets using multiple evaluation indicators. The results demonstrate the superiority of the MGO-DNM model over competing methods, with optimized DNM configurations consistently outperforming the traditional DNM in forecasting tasks.},
  archive      = {J_EI},
  author       = {Al-qaness, Mohammed A. A. and Ewees, Ahmed A. and Abd Elaziz, Mohamed and Damaševičius, Robertas and Samak, Ahmed H.},
  doi          = {10.1007/s12065-025-01078-y},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Evol. Intell.},
  title        = {Dendritic neuron model optimized with mountain gazelle optimizer for time-series forecasting},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the analytical solution of the nonlinear strain wave equation through the bilinear neural network method. <em>EI</em>, <em>18</em>(5), 1-17. (<a href='https://doi.org/10.1007/s12065-025-01068-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the strain wave equation, which governs the behavior of elastic wave propagation in different media of continuum mechanics. Due to the nonlinearity of the equation, it is often difficult to achieve exact solutions using certain traditional analytical and numerical approaches. In this work, we use the bilinear neural network method to provide exact solutions to the strain wave equation. This method effectively builds solutions by utilizing deep learning algorithms and bilinear transformations. The bilinear neural network method provides a potent substitute for traditional methods by methodically approximating nonlinear wave patterns and mapping them into a solvable framework. The strain wave equation provides an insight into the nonlinear long wave propagation behavior in shallow water by taking wave phase into account, and its versatility extends beyond fluid dynamics to various physical phenomena. By employing structured neural network models, the generalized lump, periodic, and other exact analytical solutions are obtained using the bilinear neural network approach. With the help of Mathematica, we extract these solutions and illustrate them in 3D, 2D, and contour plots. These visualizations helped illustrate the dynamics of the solutions obtained. The computational efficiency of the method was illustrated through its capacity to symbolically derive closed-form solutions without the need for iterative numerical methods and, in turn, diminish processing time and complexity significantly. The novelty of this work is the combination of bilinear transformation methods and neural network architectures to symbolically obtain exact analytical solutions as highlighted above for the nonlinear strain wave equation, representing a remarkable breakthrough compared to the conventional numerical and analytical methods. The validity of the solutions acquired was established by direct substitution in the bilinear equation as well as through checking their compliance with the equation of strain wave with the help of Mathematica.},
  archive      = {J_EI},
  author       = {Muslim, Humaira and Ahmed, Nauman and Macías, Siegfried and Ahmed, Muhammad Ozair and Baber, Muhammad Z. and Ceesay, Baboucarr and Macías-Díaz, Jorge E.},
  doi          = {10.1007/s12065-025-01068-0},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Evol. Intell.},
  title        = {On the analytical solution of the nonlinear strain wave equation through the bilinear neural network method},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Datastreams and beyond, from traditional approaches to quantum: A comprehensive survey. <em>EI</em>, <em>18</em>(5), 1-40. (<a href='https://doi.org/10.1007/s12065-025-01079-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datastreams serve as the fundamental conduits of information in contemporary society, playing a pivotal role in various aspects of our daily lives. This paper delves into the utilization of datastreams, elucidates the challenges encountered during their application, and presents potential solutions to overcome these challenges. Different approaches to handle datastreams are discussed, encompassing diverse methodologies. Some methods emphasize the employment of mathematical and statistical techniques for comprehending and analyzing data, while others capitalize on machine, deep, and continual learning to extract meaningful patterns. Furthermore, the paper explores the emergence of quantum machines that leverage the principles of quantum physics to tackle datastream related tasks. The advent of quantum machines holds promising prospects for revolutionizing data handling mechanisms in the future. In addition to data analysis, the paper delves into the platforms and systems employed to effectively process and organize datastreams. These platforms act as indispensable tools, facilitating efficient management and manipulation of the copious volumes of data within datastreams. Measurement of datastream performance is another key aspect addressed in the paper, wherein several metrics are examined. These metrics provide valuable insights into the efficacy of datastream methodologies. An intriguing subject discussed is eXplainable AI (XAI) datastreams, which offers a means to comprehend the decision-making processes of complex algorithms, such as neural networks. Lastly, the paper highlights the persisting challenges encountered in working with datastreams and outlines potential future directions in the field. These insights shed light on the evolving landscape of datastream utilization and anticipate the exciting possibilities that lie ahead.},
  archive      = {J_EI},
  author       = {Weinberg, Abraham Itzhak},
  doi          = {10.1007/s12065-025-01079-x},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-40},
  shortjournal = {Evol. Intell.},
  title        = {Datastreams and beyond, from traditional approaches to quantum: A comprehensive survey},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Copula extended power aggregation operators and MEREC method under linguistic q-rung orthopair fuzzy information: An application to plastic waste management solution selection. <em>EI</em>, <em>18</em>(5), 1-36. (<a href='https://doi.org/10.1007/s12065-025-01080-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing global consumption of plastics has led to an alarming surge in non-biodegradable waste, posing severe threats to ecosystems and wildlife. Addressing these challenges requires advanced decision-making frameworks to navigate the complexities of competing factors and uncertainties in plastic waste management (PWM). This study presents a pioneering decision-making framework tailored to the linguistic q-rung orthopair fuzzy ( $$L^{q}ROF$$ ) environment, offering enhanced capabilities for handling imprecision and uncertainty. A key innovation of this work is the introduction of new operational laws for $$L^{q}ROF$$ values, grounded in Archimedean copula and co-copula (ACC) theory. These laws exhibit essential properties such as idempotence, boundedness, and monotonicity, forming the foundation for novel power aggregation operators (AOs), including the $$L^{q}ROF$$ copula extended power average ( $$L^{q}ROF$$ CEPA) and geometric ( $$L^{q}ROF$$ CEPG) operators, along with their weighted variants. By integrating these AOs with the method based on the removal effects of criteria (MEREC) weighting technique, we propose a comprehensive decision-making approach for identifying optimal PWM strategies. The robustness and applicability of the framework are demonstrated through a detailed case study, sensitivity analysis, and comparative evaluation, highlighting its effectiveness in delivering reliable and insightful decisions for PWM challenges.},
  archive      = {J_EI},
  author       = {Ali, Jawad},
  doi          = {10.1007/s12065-025-01080-4},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-36},
  shortjournal = {Evol. Intell.},
  title        = {Copula extended power aggregation operators and MEREC method under linguistic q-rung orthopair fuzzy information: An application to plastic waste management solution selection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative multi-population social group optimization for many-objective optimization. <em>EI</em>, <em>18</em>(5), 1-37. (<a href='https://doi.org/10.1007/s12065-025-01081-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study provides an effective cooperative multi-populations swarm intelligence approach, called CMPMO-SGO, which is based on the Social Group Optimization (SGO) algorithm to address multi-/many-objective optimization problems. In particular, this study first puts forth a unique cooperative multi-population framework- namely CMPMO/des, with dual elite selection. CMPMO/des achieves significantly high performance on solutions convergence and diversity with four excellent strategies: the ability to independently optimize one objective within each subpopulation, collaboration among subpopulations via a global archive, dual elite selection mechanisms, and the inclusion of logistic chaotic single-dimensional perturbation. Afterward, due to its very high performance, SGO is employed as the single objective optimizer in each subpopulation. Sixteen state-of-the-art multi/many-objective optimization algorithms, two CMPMO/des based many-objective genetic algorithms named CMPMO-GA and many-objective Harris Hawks optimization named CMPMO-HHO, were extensively compared with the performance of CMPMO-SGO on 34 multi-objective and 19 many-objective benchmark problems. The results demonstrate that CMPMO-SGO provides promising performance in addressing multi/many-objective optimization problems by utilizing the CMPMO/des framework.},
  archive      = {J_EI},
  author       = {Naik, Anima},
  doi          = {10.1007/s12065-025-01081-3},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-37},
  shortjournal = {Evol. Intell.},
  title        = {Cooperative multi-population social group optimization for many-objective optimization},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment of uncertainties in stock exchange market using pythagorean hesitant fuzzy Sugeno–Weber aggregation operators. <em>EI</em>, <em>18</em>(5), 1-21. (<a href='https://doi.org/10.1007/s12065-025-01074-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty and fuzziness are involved in almost all aspects of life. Multi-attribute decision-making is the most suitable technique for identifying the optimal alternative from uncertain and fuzzy information, thereby reducing ambiguity in practical applications. n the modern age, business has shifted from a physical to an online mode, exemplified by Stack Exchange. However, it is too technical for new businesspeople to find the right investment opportunity in the stock exchange market. For handling such types of problems, the Pythagorean hesitant fuzzy set framework is an advanced and flexible assessment tool. The Sugeno–Weber t-norm and Sugeno–Weber t-conorm operations provide a flexible environment for data aggregation. The thought is that the Pythagorean hesitant fuzzy set is a more flexible and advanced structure than the simple one due to the hesitant degree. By motivating the concept of Pythagorean hesitant fuzzy set and Sugeno–Weber operation, we developed a family of aggregation operators called Pythagorean hesitant fuzzy Sugeno–Weber weighted averaging and Pythagorean hesitant fuzzy Sugeno–Weber weighted geometric operators. Additionally, we examine some fundamental axioms of aggregation operators, including boundedness, monotonicity, and idempotency, to verify the authenticity and accuracy of the proposed theory. To construct a multi-attribute decision-making algorithm based on the developed theory. We offer a solution to real-life multi-attribute decision-making (MADM) problems involving money investments in the stock exchange market through our diagnostic approach. Compared with existing methodologies, we investigate the applicability of Pythagorean hesitant fuzzy Sugeno Weber weighted averaging and Pythagorean hesitant fuzzy Sugeno Weber weighted geometric operators. Solid conclusions are discussed in the last section.},
  archive      = {J_EI},
  author       = {Khan, Muhammad Rizwan and Ullah, Kifayat and Khan, Qaisar and Senapati, Tapan},
  doi          = {10.1007/s12065-025-01074-2},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Evol. Intell.},
  title        = {Assessment of uncertainties in stock exchange market using pythagorean hesitant fuzzy Sugeno–Weber aggregation operators},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skewed medical data-oriented parallel modular diagnosis via scalable belief-ruled hierarchy. <em>EI</em>, <em>18</em>(5), 1-19. (<a href='https://doi.org/10.1007/s12065-025-01084-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosing gastric diseases must address skewed medical data, where positive cases of any specific gastric disease constitute only a small proportion of all cases due to the variety of gastric diseases. Therefore, a new approach is called for to effectively diagnose multiple gastric diseases with high scalability. In this study, a skewed medical data-oriented parallel modular diagnosis approach is proposed for gastric diseases via scalable belief-ruled hierarchy. First, analytical lower-to-middle factors integration functions are constructed. Then, a middle-to-top diagnosis model is constructed using a belief-ruled hierarchy. Third, a skewed data-oriented optimization model is constructed followed by an optimization algorithm. Finally, a parallel modular diagnosis mechanism is proposed for multiple gastric diseases. A practical case with 172 cases and nine types of gastric diseases gathered from a tertiary hospital in Hangzhou, China, during a three-month period is studied for validation. Case study results demonstrate that the proposed approach diagnoses positive cases of nine gastric diseases with an average accuracy of 86.93%. Compared with five other conditions with varied parameter configurations and four machine learning approaches, the proposed approach produces optimal results (e.g., 53.61%, 15.28%, 42.73%, and 40.30% by BPNN, SVM, RF, and GPR, respectively) owing to its penalty coefficient customized to the skewed gastric data to balance all-case diagnosis accuracy and positive-case accuracy.},
  archive      = {J_EI},
  author       = {Zeng, Xingwu and Chang, Leilei and Wang, Qianqian and Xu, Xiaobin and Zhang, Shuo and Hou, Bingbing},
  doi          = {10.1007/s12065-025-01084-0},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Evol. Intell.},
  title        = {Skewed medical data-oriented parallel modular diagnosis via scalable belief-ruled hierarchy},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault detection in photovoltaic modules using I–V curves and machine learning. <em>EI</em>, <em>18</em>(5), 1-14. (<a href='https://doi.org/10.1007/s12065-025-01086-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering growing global demand for electricity, it is essential to invest in clean and renewable energy production. Investing in renewable energy production is crucial to creating a sustainable matrix that can efficiently meet consumption needs. Reducing costs for solar energy operation and maintenance (O&M) is critical to ensure the economic viability of photovoltaic (PV) projects. Efficient management of these costs can make the difference between a successful project and an unprofitable installation. This work proposes the structuring of a dataset using real measurements of current–voltage (I–V) curves from a solar plant. Based on this, two classifications were performed to detect shading and soiling in PV modules. Two different methods were used for feature processing. One dataset was created using complete I–V curves, while the other relied solely on key parameters from a characteristic curve. The dataset used incorporates data on current, voltage, irradiance, and module temperature at the time of measurements. Different machine learning models were compared, achieving 100% accuracy in soiling classification and 93.2% in shading classification.},
  archive      = {J_EI},
  author       = {da Silva, Eduardo Goulart and Franchi, Claiton Moro and Treter, Marcos Eduardo and Gamarra, Daniel Fernando Tello},
  doi          = {10.1007/s12065-025-01086-y},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Evol. Intell.},
  title        = {Fault detection in photovoltaic modules using I–V curves and machine learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implementation of a novel enhanced hybrid multi-objective osprey optimization algorithm for off-grid hybrid system sizing. <em>EI</em>, <em>18</em>(5), 1-39. (<a href='https://doi.org/10.1007/s12065-025-01083-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel enhanced hybrid multi-objective osprey optimization algorithm (EHMOOOA) is proposed in this research work. It is used here for the optimum sizing of an off-grid composite renewable energy sources-based framework comprising a photovoltaic (PV) system, wind turbine generators (WTs), and battery energy storage system (BESS). The system is modelled to supply the administrative block of Kalyani Government Engineering College, Kalyani, India. The novelties include the integration of the quasi-oppositional-based learning mechanism, composite solution-oriented differential evolution algorithm (CSODEA), and Brownian motion concept into the osprey optimization algorithm (OOA). The contribution lies in the development of two novel mutation mechanisms, three new scaling coefficients and advanced extraction strategies for the core vector and the primary predecessor solutions of the difference vectors. The final stage of the exploitation process presents a novel scheme for the location upgrade of the solutions employing Brownian motion. A mathematical configuration has been built to minimize three primary fitness functions: loss of power supply probability (LPSP), levelized cost of electrical energy (LCOE), and net present cost (NPC). For validation purposes, the performances of the proposed hybrid meta-heuristic algorithm are compared with five other powerful optimization algorithms. Eight standard CEC benchmark functions and three CEC2020 actual scenario-constrained optimization problem functions are employed here for the evaluation of the proposed algorithm. The suggested composite methodology yielded the lowest and optimum values of LCOE (0.3029 $/kWh), NPC (0.9184e+05 $), and LPSP (0.000541). The optimum contributions made by PV, WT, and BESS are 80.9%, 10%, and 9.1%, respectively. Validation of the suggested technique has been further carried out by conducting statistical performance analysis through standard deviations. The proposed technique yielded the lowest standard deviation values: 7.4330e $$-$$ 04 (LCOE), 376.8249 (NPC), and 1.9901e $$-$$ 06 (LPSP), respectively. Further, to validate the outcomes, the Shapiro-Wilk test is conducted, followed by the Welch-like ANOVA and Games-Howell post-hoc tests. Moreover, a sensitivity test is performed to evaluate the robustness of EHMOOOA.},
  archive      = {J_EI},
  author       = {Bandopadhyay, Joy and Roy, Provas Kumar},
  doi          = {10.1007/s12065-025-01083-1},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-39},
  shortjournal = {Evol. Intell.},
  title        = {Implementation of a novel enhanced hybrid multi-objective osprey optimization algorithm for off-grid hybrid system sizing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SELFVarBL: A stacking-based ensemble learning framework with variable numbers of base-learners. <em>EI</em>, <em>18</em>(5), 1-18. (<a href='https://doi.org/10.1007/s12065-025-01085-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stacking is one of the most popular ensemble learning technique that combines the predictions of multiple base-learners (BLs) using a meta-learner to improve the overall performance of the model. To build a stacking-based ensemble, most of the existing works select top-k-performing models as the base-learners without considering the computational resource requirement of the system. This work presents a Stacking-based Ensemble Learning Framework with a Variable number of Base-Learners (SELFVarBL), which is capable of finding the trade-off between computational resource requirements and system performance by automatically selecting a suitable number and combinations of base-learners. Additionally, the proposed framework does not require excessive computational resources to achieve the optimal solution. The proposed framework is validated using three case studies with diversify datasets: the agriculture domain, the audio data analysis domain, and the self-generated synthetic datasets. The experimental results show the effectiveness of the proposed framework.},
  archive      = {J_EI},
  author       = {Singh, Mrityunjay and Modi, Shatrughan and Jakhar, Amit Kumar},
  doi          = {10.1007/s12065-025-01085-z},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Evol. Intell.},
  title        = {SELFVarBL: A stacking-based ensemble learning framework with variable numbers of base-learners},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new ant stridulation-based local search for combinatorial optimization problems. <em>EI</em>, <em>18</em>(5), 1-19. (<a href='https://doi.org/10.1007/s12065-025-01088-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stridulating insects, like ants, use acoustic communication as a crucial part of their social behavior, allowing them to coordinate their activities and respond to changing environmental conditions. This paper introduces a novel local search-based approach inspired by the acoustic communication in real ant colonies. Despite the large number of existing bio-inspired metaheuristics, no single algorithm is universally optimal, and a new approach inspired by ant sound communication could offer advantages in specific problems. Sound-based mechanisms could provide better ways to balance exploration and exploitation potentially improve search efficiency. In the proposed approach, artificial ants employ a 2-Opt local search guided by acoustic signals to enhance the quality of a population of initial solutions. To assess the efficacy of the proposed approach, experiments are conducted on various benchmark instances of the traveling salesman problem. Experimental results demonstrate promising performance, outperforming or matching state-of-the-art algorithms on several benchmark instances.},
  archive      = {J_EI},
  author       = {Sammoud, Samia and Alaya, Ines and Tagina, Moncef},
  doi          = {10.1007/s12065-025-01088-w},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Evol. Intell.},
  title        = {A new ant stridulation-based local search for combinatorial optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid meta-heuristic strategy based on firefly and coyote optimization for area coverage enhancement in directional sensor network. <em>EI</em>, <em>18</em>(5), 1-14. (<a href='https://doi.org/10.1007/s12065-025-01087-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directional sensor network(DSN) utilize directional sensors with a limited field of view for focused monitoring. This directionality enhances energy efficiency by reducing unnecessary sensing and data transmission. These sensor enables precise monitoring of specific areas while minimizing data redundancy. Optimizing sensor orientation is crucial for maximizing area coverage in randomly deployed networks, particularly in inaccessible terrains. Effective orientation ensures maximum coverage, enhances monitoring efficiency, and reduces the need for costly redeployments in challenging environments. This paper proposes a novel Hybrid Firefly-Coyote Optimization Algorithm (HF-COA) to address this problem. A grid-based mathematical framework is proposed to evaluate area coverage. The proposed HF-COA achieves superior performance in maximizing area coverage due to its effective balance of exploration and exploitation. Experimental results demonstrate that HF-COA achieves superior coverage performance compared to standalone FA and COA, under varying sensor parameters. The proposed method offers a reliable solution for enhancing monitoring efficiency in complex and dynamic environments.},
  archive      = {J_EI},
  author       = {Bharathi, P. Surya and Pavithra, R.},
  doi          = {10.1007/s12065-025-01087-x},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Evol. Intell.},
  title        = {Hybrid meta-heuristic strategy based on firefly and coyote optimization for area coverage enhancement in directional sensor network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matrix-based particle swarm optimization with hybrid strategy for multi-traveling salesman problem. <em>EI</em>, <em>18</em>(5), 1-21. (<a href='https://doi.org/10.1007/s12065-025-01082-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiple traveling salesman problem (mTSP) is an extension of the traveling salesman problem, which involves multiple salespeople visiting a set of cities, with each city being visited only once. It is widely used in logistics distribution, traffic scheduling, and multi robot collaboration, but faces challenges such as high computational complexity, difficult task allocation, and susceptibility to local optimal solutions. Most existing methods focus on finding solutions from scratch, primarily aiming to minimize the total travel distance, with insufficient attention paid to balancing travel costs among salespeople. This article proposes a matrix based multi strategy integrated particle swarm optimization algorithm, which improves efficiency through matrix representation and parallel computing, and introduces a dynamic diversity mechanism to balance local and global search capabilities. In addition, a new fitness evaluation function has been designed to effectively address significant route differences among salespeople. Among the five typical TSP problems, the performance of this algorithm is superior to MPSO, HJSPSO and standard PSO algorithms. It significantly improves solving efficiency and enhances result balance. This study provides an efficient and robust solution for the mTSP problem, which has important theoretical significance and practical application value.},
  archive      = {J_EI},
  author       = {Huang, Zhaoquan and Zhu, Donglin and Zhou, Changjun and Cheng, Shi},
  doi          = {10.1007/s12065-025-01082-2},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Evol. Intell.},
  title        = {Matrix-based particle swarm optimization with hybrid strategy for multi-traveling salesman problem},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emerging deep learning approaches for urban satellite image analysis: A survey on classification, segmentation, and change detection. <em>EI</em>, <em>18</em>(5), 1-45. (<a href='https://doi.org/10.1007/s12065-025-01090-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of urban satellite imagery has undergone a significant transformation due to the emergence of deep learning techniques. This survey thoroughly examines current progress in deep learning models designed for three primary tasks: classification, segmentation, and change detection in urban environments. A total of 114 reviewed studies were analyzed, including convolutional neural networks (CNNs), hybrid models, and transformer-based models such as ResNet, UNet++, HRNet, BR-Net, Mask2Former, TransCFCCNN, and STCD-EffV2T UNet. Prominent datasets such as EuroSAT, UC-Merced, SpaceNet, DeepGlobe, LEVIR-CD, and xBD were examined alongside performance metrics like accuracy, F1 score, IoU, precision, and recall, with results visualized through comparative bar plots. Our comparison analysis indicates that models such as GoogleNet, EfficientNet-B7, and MaxViT achieve classification accuracies over 99%, while segmentation models like Mask2Former and BR-Net exhibit IoU scores over 92%. STCD-EffV2T UNet achieves exceptional F1 scores of up to 98.79% for change detection. These models are crucial for urban development and environmental change monitoring, facilitating the classification of land cover types, segmentation of buildings, roads, and urban features, as well as the detection of urban and environmental changes. This study defines growing trends and challenges, incorporating model generalization, dataset diversity, and computing complexity. It additionally suggests future directions, including the use of Mamba Networks for scalable temporal modeling, the incorporation of Vision Transformers for enhanced feature extraction, and the utilization of GANs for dataset augmentation. This paper offers a comprehensive and systematic synthesis of deep learning methodologies, establishing a fundamental reference for subsequent research in urban satellite image analysis.},
  archive      = {J_EI},
  author       = {Soni, Tannu Kumar and Pujari, Pushpalata},
  doi          = {10.1007/s12065-025-01090-2},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-45},
  shortjournal = {Evol. Intell.},
  title        = {Emerging deep learning approaches for urban satellite image analysis: A survey on classification, segmentation, and change detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent parameters adaptation in fuzzy neural networks. <em>EI</em>, <em>18</em>(5), 1-17. (<a href='https://doi.org/10.1007/s12065-025-01095-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the offline handwritten signature recognition task, a challenging classification problem due to high intra-class variability and inter-class similarity. The objective is to accurately classify signature images into their corresponding identity classes, despite inherent uncertainty and imprecision in signature data. To address this, a convolutional neural network is employed for feature extraction, generating a robust representation of each signature-a discriminative and noise-tolerant feature vector that captures essential structural elements of the signature. These features are then classified using a Fuzzy Min-Max Neural Network, which excels in handling overlapping and ambiguous patterns. However, Fuzzy Min-Max Neural Network’s performance is highly sensitive to its parameters (vigilance threshold and sensitivity), often requiring time-consuming manual tuning. To overcome this limitation, an Evolution Strategy is proposed for automatic parameter optimization. This method uses an adaptive fitness function and a dynamic mutation strategy to efficiently explore the parameter space and accelerate convergence. The approach is validated on a proprietary offline signature dataset, and the results demonstrate that the proposed Evolution Strategy-Fuzzy Min-Max Neural Network system achieves superior accuracy, precision, and robustness compared to baseline and state-of-the-art methods.},
  archive      = {J_EI},
  author       = {Melhaoui, Ouafae El and Hdid, Jalal},
  doi          = {10.1007/s12065-025-01095-x},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Evol. Intell.},
  title        = {Intelligent parameters adaptation in fuzzy neural networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight mechanical fault diagnosis network driven by cooperative neuroevolution algorithm. <em>EI</em>, <em>18</em>(5), 1-13. (<a href='https://doi.org/10.1007/s12065-025-01096-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale mechanical fault detection plays a critical role in industrial production, and neural networks have been widely adopted for this task. However, the performance of neural networks heavily depends on the optimization of their architecture and parameters. Traditional optimization methods such as gradient descent and particle swarm optimization can improve model performance to some extent, but they typically require adjusting all weights, which not only increases computational complexity but also often leads to redundant network structures. To address these challenges, this paper proposes a cooperative neuroevolution algorithm for constructing lightweight neural networks tailored for fault diagnosis tasks. The algorithm introduces two co-evolving populations-one dedicated to learning the critical weights of the neural network, and the other focused on optimizing the precise weight values. This approach enables the discovery of a sparsely connected, lightweight neural network. By decoupling and cooperatively optimizing network structure and weights, the algorithm can more efficiently explore the search space to identify models that are both compact and high-performing. To validate the effectiveness of the proposed algorithm, extensive experiments are conducted on multiple industrial datasets involving large-scale mechanical fault detection. The results demonstrate that the algorithm maintains high diagnostic accuracy while significantly reducing the number of model parameters. Compared to conventional evolutionary algorithms, it exhibits superior model compression capability and generalization performance.},
  archive      = {J_EI},
  author       = {Yue, Haifeng and Zhang, Jue and Wang, Yize and Tian, Xiaojian and Zhang, Bairong and Zhang, Zhennan and Shao, Shuai},
  doi          = {10.1007/s12065-025-01096-w},
  journal      = {Evolutionary Intelligence},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Evol. Intell.},
  title        = {A lightweight mechanical fault diagnosis network driven by cooperative neuroevolution algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="evols">EVOLS - 4</h2>
<ul>
<li><details>
<summary>
(2025). Continual learning approaches for anomaly detection. <em>EVOLS</em>, <em>16</em>(4), 1-20. (<a href='https://doi.org/10.1007/s12530-025-09732-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection (AD) is a relevant problem in numerous real-world applications, especially when dealing with images. However, in real-world applications, it is common that the input data distribution can change over time, decreasing performance significantly. Therefore, in this study, we investigate the problem of Visual Anomaly Detection at Pixel-Level in the Continual Learning setting, where the model adapts to the new data while maintaining the knowledge of old data. We implement and test several AD techniques and adapt them to work in the CL setting using the Replay approach. We evaluate them using the well-known MVTec AD Dataset, where each object corresponds to a new learning task. Moreover, a significant challenge when dealing with the Replay approach is the memory occupied to store a portion of past images, which could be too heavy for many resource-constrained systems. Therefore, we propose a novel approach called SCALE, which performs high compression levels while preserving image quality through Super-Resolution techniques. Using the SCALE method to compress replay memory, in conjunction with the AD technique Inpaint, allows for obtaining the best AD results while significantly reducing memory consumption.},
  archive      = {J_EVOLS},
  author       = {Pezze, Davide Dalle and Anello, Eugenia and Masiero, Chiara and Susto, Gian Antonio},
  doi          = {10.1007/s12530-025-09732-7},
  journal      = {Evolving Systems},
  month        = {12},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Evol. Syst.},
  title        = {Continual learning approaches for anomaly detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixelated disparity network for hepatocellular carcinoma recognition from ultrasound images. <em>EVOLS</em>, <em>16</em>(4), 1-16. (<a href='https://doi.org/10.1007/s12530-025-09737-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The overgrowth of tumor cells in the liver results in Hepatocellular Carcinoma with unfamiliar symptoms in its earlier stages. Though the treatments facilitate transplantation, freezing, etc. the recognition of such tumors is to be made early using ultrasound images. This article proposes a Disparity Learning Network for Pixel Differentiation to recognize Hepatocellular Carcinoma from ultrasound image inputs. First, the conventional textural features are extracted from the input image from which the disparity for differential pixel distribution is estimated. This disparity is computed based on pixel absence in a well-distributed region and the corresponding error occurrence. In this case, the disparity network is constructed using one conditional and one training layer for disparity classification and new distribution training. In the classification process, the absence and high-density factors are differentiated for region-wise disparity estimation. Such estimation is used for training further classifications, for differentiating non-disparity regions. The maximum disparity pixel distributed region is recognized as the infected region from the given input.},
  archive      = {J_EVOLS},
  author       = {Usha, S. and Bala, Saroj and Saranya, M. D. and Suganyadevi, S.},
  doi          = {10.1007/s12530-025-09737-2},
  journal      = {Evolving Systems},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Evol. Syst.},
  title        = {Pixelated disparity network for hepatocellular carcinoma recognition from ultrasound images},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved ear recognition system based on efficient feature extraction and fusion techniques. <em>EVOLS</em>, <em>16</em>(4), 1-11. (<a href='https://doi.org/10.1007/s12530-025-09739-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric identification through ear image analysis and recognition has emerged as a promising biometric system due ear uniqueness and stability in varying environmental conditions. In this study, a novel approach for ear recognition using a fusion of two powerful feature extraction techniques such as Histogram of Oriented Gradients (HOG) and an improved Local Optimal Oriented Pattern (LOOP) based on Spatial Pyramid Decomposition (SPD) is proposed. An efficient scheme based on Discriminant Correlation Analysis (DCA) is adopted as a feature level fusion and dimensionality reduction technique where the performance of the recognition system is evaluated using a K-Nearest Neighbors (K-NN) classifier. Extensive experiments on six well known benchmarks ear datasets are conducted to assess the effectiveness of the proposed approach. Experimental results clearly indicate the superiority of the proposed method in terms of performance and complexity in comparison with the state-of-the-art ear recognition techniques.},
  archive      = {J_EVOLS},
  author       = {Lebed, Toufik and Boukharouba, Abdelhak},
  doi          = {10.1007/s12530-025-09739-0},
  journal      = {Evolving Systems},
  month        = {12},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Evol. Syst.},
  title        = {An improved ear recognition system based on efficient feature extraction and fusion techniques},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Healthcare fraud detection using adaptive learning and deep learning techniques. <em>EVOLS</em>, <em>16</em>(4), 1. (<a href='https://doi.org/10.1007/s12530-025-09741-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EVOLS},
  author       = {Matloob, Irum and Khan, Shoab and Rukaiya, Rukaiya and Alfraihi, Hessa and Khan, Javed Ali},
  doi          = {10.1007/s12530-025-09741-6},
  journal      = {Evolving Systems},
  month        = {12},
  number       = {4},
  pages        = {1},
  shortjournal = {Evol. Syst.},
  title        = {Correction: Healthcare fraud detection using adaptive learning and deep learning techniques},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="fodm">FODM - 9</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Supply chain coordination with buyback contract under uncertain demand and salvage value differentiation environments. <em>FODM</em>, <em>24</em>(3), 589. (<a href='https://doi.org/10.1007/s10700-025-09460-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FODM},
  author       = {Wang, Xiaobin and Yin, Fanghao},
  doi          = {10.1007/s10700-025-09460-2},
  journal      = {Fuzzy Optimization and Decision Making},
  month        = {9},
  number       = {3},
  pages        = {589},
  shortjournal = {Fuzzy Optim. Decis. Mak.},
  title        = {Correction: Supply chain coordination with buyback contract under uncertain demand and salvage value differentiation environments},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supply chain coordination with buyback contract under uncertain demand and salvage value differentiation environments. <em>FODM</em>, <em>24</em>(3), 563-588. (<a href='https://doi.org/10.1007/s10700-025-09457-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a buyback contract, the supplier is responsible for dealing with the remaining items at the end of the season. However, the salvage value of the remaining items will affect the order quantity and profit of the retailer, and the parameters of the buyback contract for coordinating the supply chain will also be influenced. In order to explore these effects, a model with uncertain demand and perfect quality replenishment for the centralized supply chain is presented, and taken as a benchmark. Then we examine the optimal order quantity for the retailer in both centralized and decentralized supply chains, where demand is uncertain, replenishment has imperfect quality, and the remaining perfect and imperfect items have different salvage values, respectively. We investigate these problems using uncertainty theory, formulate relevant models, solve them, and then present the main results. For the sake of coordinating the supply chain, we find that the buyback contract can achieve supply chain coordination under appropriate conditions. Moreover, the optimal order quantity for the retailer is determined by the inverse distribution of the uncertain demand. The salvage values of both perfect-quality items and imperfect-quality items have positive impacts on the optimal order quantity. Further, the optimal parameters of the buyback contract are determined based on the bargaining power of the supplier during the contract signing process. It is beneficial for the supplier with strong bargaining power to offer a buyback price close to the wholesale price.},
  archive      = {J_FODM},
  author       = {Wang, Xiaobin and Yin, Fanghao},
  doi          = {10.1007/s10700-025-09457-x},
  journal      = {Fuzzy Optimization and Decision Making},
  month        = {9},
  number       = {3},
  pages        = {563-588},
  shortjournal = {Fuzzy Optim. Decis. Mak.},
  title        = {Supply chain coordination with buyback contract under uncertain demand and salvage value differentiation environments},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertain finance: A systematic review of recent advances. <em>FODM</em>, <em>24</em>(3), 531-561. (<a href='https://doi.org/10.1007/s10700-025-09455-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty theory, established on the foundational axioms of normality, duality, subadditivity, and product, is a new branch of mathematics and it is particularly applicable in scenarios where the distribution function is not close enough to observed frequencies, a common occurrence in financial markets. Consequently, the field of uncertain finance has been actively explored and advanced by numerous scholars. This paper presents a comprehensive state-of-the-art review of recent advances in uncertain finance. Through a systematic literature review methodology, we conducted an extensive search in the Web of Science database, covering publications from January 2009 to April 2025. Our search strategy yielded a total of 162 relevant articles focusing on uncertain finance, which form the basis of this review. The study systematically summarizes the key theoretical models and findings in the field of uncertain finance. Furthermore, it provides an analysis of potential future research directions and emerging trends in this domain.},
  archive      = {J_FODM},
  author       = {Yang, Xiangfeng and Li, Haoxuan},
  doi          = {10.1007/s10700-025-09455-z},
  journal      = {Fuzzy Optimization and Decision Making},
  month        = {9},
  number       = {3},
  pages        = {531-561},
  shortjournal = {Fuzzy Optim. Decis. Mak.},
  title        = {Uncertain finance: A systematic review of recent advances},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On convergences of uncertain random sequences under U-S chance spaces. <em>FODM</em>, <em>24</em>(3), 485-529. (<a href='https://doi.org/10.1007/s10700-025-09454-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convergence has been a topic of considerable interest. This study further develops U-S chance theory to investigate convergences for uncertain random sequences in complex systems where human uncertainty and randomness with sub-linear characteristics coexist. Building upon two existing chance measures, this paper defines two new chance measures, presents their properties and proves the relationship among the four chance measures. Six expectations of uncertain random variables under U-S chance spaces are suggested based on Choquet integrals and sub-linear expectations. Meanwhile, their relationship and Markov’s inequality are proven. Furthermore, this paper systematically presents multiple definitions of the continuities for chance measures under U-S chance spaces and investigate the relationships among them. Based on the continuity assumption of uncertain measure, a new version of Borel-Cantelli lemma under U-S chance spaces is proven. Finally, several definitions of convergences for uncertain random sequences under U-S chance spaces are presented. By rigorous mathematical proofs and systematic construction of counterexamples, the relationships among different types of convergences are illustrated.},
  archive      = {J_FODM},
  author       = {Yang, Deguo and Zong, Zhaojun and Hu, Feng},
  doi          = {10.1007/s10700-025-09454-0},
  journal      = {Fuzzy Optimization and Decision Making},
  month        = {9},
  number       = {3},
  pages        = {485-529},
  shortjournal = {Fuzzy Optim. Decis. Mak.},
  title        = {On convergences of uncertain random sequences under U-S chance spaces},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Fuzzy demand electric vehicle routing problem with soft time windows. <em>FODM</em>, <em>24</em>(3), 483-484. (<a href='https://doi.org/10.1007/s10700-025-09459-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FODM},
  author       = {Abdulatif, Nour and Shalaby, Mohamed A.W. and Kassem, Sally S. and Khalil, Tarek},
  doi          = {10.1007/s10700-025-09459-9},
  journal      = {Fuzzy Optimization and Decision Making},
  month        = {9},
  number       = {3},
  pages        = {483-484},
  shortjournal = {Fuzzy Optim. Decis. Mak.},
  title        = {Correction: Fuzzy demand electric vehicle routing problem with soft time windows},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy demand electric vehicle routing problem with soft time windows. <em>FODM</em>, <em>24</em>(3), 457-481. (<a href='https://doi.org/10.1007/s10700-025-09453-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces a novel contribution to the Electric Vehicle Routing Problem (EVRP) field by addressing fuzzy demands, soft time windows, and recharging at demand points. The problem is formulated as a mixed-integer linear programming model that incorporates uncertainties in demand levels and allows flexibility in time windows with a penalty for its violation. LINGO software is utilized to solve the proposed model. To assess the effect of fuzzy demand, a parametric analysis is conducted by varying the fuzzy demand parameters using lexicographic fuzzy method implemented by LINGO. The model’s validity and effectiveness are verified using Solomon’s benchmark dataset, and further applied to a case study from the Egyptian local market. The solutions obtained are evaluated based on total costs incurred and total CO2 emissions. By analyzing the resulting solutions, managerial implications are deduced, providing a framework for decision-makers in electric vehicle fleet management. Recommendations are made to decision makers on fuzzy demand modeling techniques, charging infrastructure, pricing and incentive strategies, and management systems. This study contributes to the advancement of EVRP research, offering practical solutions for real-world transportation planning and logistics management.},
  archive      = {J_FODM},
  author       = {Abdulatif, Nour and Shalaby, Mohamed A.W. and Kassem, Sally S. and Khalil, Tarek},
  doi          = {10.1007/s10700-025-09453-1},
  journal      = {Fuzzy Optimization and Decision Making},
  month        = {9},
  number       = {3},
  pages        = {457-481},
  shortjournal = {Fuzzy Optim. Decis. Mak.},
  title        = {Fuzzy demand electric vehicle routing problem with soft time windows},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized mean semi-absolute deviation model of portfolio selection based on uncertainty theory. <em>FODM</em>, <em>24</em>(3), 431-456. (<a href='https://doi.org/10.1007/s10700-025-09452-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio selection problems, considering returns of the securities as uncertain variables, are an important area of contemporary research. In this line, linear and zigzag uncertainty distributions are popularly being used. These distributions contain two and three parameters, a, b and a, b, c, respectively. In this paper, two families of uncertainty distributions containing one arbitrary constant each have been introduced, and the properties are studied. Linear and zigzag uncertainty distributions then become a particular member of the respective family. This is achieved by introducing the arbitrary constants h and k, $$0 \le h< 1,~0<k<1$$ , representing the values of the cumulative uncertainty level at a (for linear) and b (for zigzag). Then, by varying the values of h and k, that is, by changing the slope(s) of the line segment(s), one can fit different distributions having one or two line segments. The newly introduced families have been successfully applied to model and solve portfolio selection problems in an uncertain environment. The solution that fulfills the investor’s requirements may then be chosen. The proposed method of solution has been illustrated by a numerical example. In constructing portfolio selection problems, we considered the adjustment of securities and the transaction costs involved. The solutions obtained for optimal returns and risks for different values of the arbitrary constants are compared.},
  archive      = {J_FODM},
  author       = {Chhatri, Sanjoy and Bhattacharya, Debasish and Das, Birojit},
  doi          = {10.1007/s10700-025-09452-2},
  journal      = {Fuzzy Optimization and Decision Making},
  month        = {9},
  number       = {3},
  pages        = {431-456},
  shortjournal = {Fuzzy Optim. Decis. Mak.},
  title        = {Generalized mean semi-absolute deviation model of portfolio selection based on uncertainty theory},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing RNN based model and bi-objective programming to a new mean-conditional value at risk-entropy for uncertain portfolio optimization with liquidity and diversification. <em>FODM</em>, <em>24</em>(3), 397-429. (<a href='https://doi.org/10.1007/s10700-025-09451-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a portfolio optimization problem characterized by uncertain returns. In this context, the returns of risky assets are viewed as uncertain variables, estimated by experienced experts. Initially, a mean-Conditional Value at Risk-entropy model is proposed for the uncertain portfolio optimization problem, considering four criteria: return, risk, liquidity, and the diversification degree of the portfolio. In this model, investment return is determined by the uncertain expected value, investment risk is represented by uncertain Conditional Value at Risk, and entropy is used to measure the diversification degree of the portfolio. Furthermore, our model differs from previous bi-objective optimization models by integrating both maximum return and minimum risk into a single objective form through the introduction of a risk aversion factor and the removal of dimensional influences caused by different units via a normalization method. Subsequently, several auxiliary portfolio selection models are converted into different equivalent deterministic models. Utilizing a neural network strategy with reducing dimension and complexity, the resulting single-objective optimization problem is solved. Based on Lyapunov theory, the proposed model is proven to be stable in the sense of Lyapunov and globally convergent to an exact optimal solution of the achieved mathematical programming problem for different weight values. The efficient frontier (Pareto Optimal Solution) is also provided using different weight values. Additionally, these algorithms determine an approximation of the set of efficient elements and diversify the solutions along the Pareto front. Computer simulations demonstrate the effectiveness and practicality of the proposed model.},
  archive      = {J_FODM},
  author       = {Andabil, Zahra Faraji and Nazemi, Alireza and Mirlohi, Seyyed Mojtaba},
  doi          = {10.1007/s10700-025-09451-3},
  journal      = {Fuzzy Optimization and Decision Making},
  month        = {9},
  number       = {3},
  pages        = {397-429},
  shortjournal = {Fuzzy Optim. Decis. Mak.},
  title        = {Utilizing RNN based model and bi-objective programming to a new mean-conditional value at risk-entropy for uncertain portfolio optimization with liquidity and diversification},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A visual interaction consensus approach for fuzzy social network with trust propagation. <em>FODM</em>, <em>24</em>(3), 367-396. (<a href='https://doi.org/10.1007/s10700-025-09450-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet and the increase in online communication, complex social relationship produces new challenges to group decision making. To deal with the group consensus problems consisting of fuzzy preference information and incomplete trust relationship, considering trust propagation, we exploit the theory and methods of network analysis, clustering, Pythagorean fuzzy set, and group decision making to construct a fuzzy information social network group consensus approach, and discuss four different scenarios of consensus (high–high, high–low, low–high and low–low) between the interest subgroups and whole social network by setting different compromise parameter values for different situations to reduce the supervision cost and keep initial opinions as much as possible. Finally, we use a vaccine selection case to verify the rationality and applicability of the proposed model.},
  archive      = {J_FODM},
  author       = {Diao, Wei-xue and Liu, Yong and Yi, Jin-hong and Guo, Xue-ge and Yang, Jun},
  doi          = {10.1007/s10700-025-09450-4},
  journal      = {Fuzzy Optimization and Decision Making},
  month        = {9},
  number       = {3},
  pages        = {367-396},
  shortjournal = {Fuzzy Optim. Decis. Mak.},
  title        = {A visual interaction consensus approach for fuzzy social network with trust propagation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="focm">FoCM - 8</h2>
<ul>
<li><details>
<summary>
(2025). Stable liftings of polynomial traces on tetrahedra. <em>FoCM</em>, <em>25</em>(4), 1397-1461. (<a href='https://doi.org/10.1007/s10208-024-09670-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the reference tetrahedron $$K$$ , we construct, for each $$k \in {\mathbb {N}}_0$$ , a right inverse for the trace operator $$u \mapsto (u, \partial _{\textbf{n}} u, \ldots , \partial _{\textbf{n}}^k u)|_{\partial K}$$ . The operator is stable as a mapping from the trace space of $$W^{s, p}(K)$$ to $$W^{s, p}(K)$$ for all $$p \in (1, \infty )$$ and $$s \in (k+1/p, \infty )$$ . Moreover, if the data is the trace of a polynomial of degree $$N \in {\mathbb {N}}_0$$ , then the resulting lifting is a polynomial of degree N. One consequence of the analysis is a novel characterization for the range of the trace operator.},
  archive      = {J_FoCM},
  author       = {Parker, Charles and Süli, Endre},
  doi          = {10.1007/s10208-024-09670-x},
  journal      = {Foundations of Computational Mathematics},
  month        = {8},
  number       = {4},
  pages        = {1397-1461},
  shortjournal = {Found. Comput. Math.},
  title        = {Stable liftings of polynomial traces on tetrahedra},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of langevin monte carlo from poincaré to log-sobolev. <em>FoCM</em>, <em>25</em>(4), 1345-1395. (<a href='https://doi.org/10.1007/s10208-024-09667-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution $$\pi $$ under the sole assumption that $$\pi $$ satisfies a Poincaré inequality. Using this fact to provide guarantees for the discrete-time Langevin Monte Carlo (LMC) algorithm, however, is considerably more challenging due to the need for working with chi-squared or Rényi divergences, and prior works have largely focused on strongly log-concave targets. In this work, we provide the first convergence guarantees for LMC assuming that $$\pi $$ satisfies either a Latała–Oleszkiewicz or modified log-Sobolev inequality, which interpolates between the Poincaré and log-Sobolev settings. Unlike prior works, our results allow for weak smoothness and do not require convexity or dissipativity conditions.},
  archive      = {J_FoCM},
  author       = {Chewi, Sinho and Erdogdu, Murat A. and Li, Mufan and Shen, Ruoqi and Zhang, Matthew S.},
  doi          = {10.1007/s10208-024-09667-6},
  journal      = {Foundations of Computational Mathematics},
  month        = {8},
  number       = {4},
  pages        = {1345-1395},
  shortjournal = {Found. Comput. Math.},
  title        = {Analysis of langevin monte carlo from poincaré to log-sobolev},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A polynomial time iterative algorithm for matching gaussian matrices with non-vanishing correlation. <em>FoCM</em>, <em>25</em>(4), 1287-1344. (<a href='https://doi.org/10.1007/s10208-024-09662-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the problem of matching vertices in two correlated Erdős-Rényi graphs, we study the problem of matching two correlated Gaussian Wigner matrices. We propose an iterative matching algorithm, which succeeds in polynomial time as long as the correlation between the two Gaussian matrices does not vanish. Our result is the first polynomial time algorithm that solves a graph matching type of problem when the correlation is an arbitrarily small constant.},
  archive      = {J_FoCM},
  author       = {Ding, Jian and Li, Zhangsong},
  doi          = {10.1007/s10208-024-09662-x},
  journal      = {Foundations of Computational Mathematics},
  month        = {8},
  number       = {4},
  pages        = {1287-1344},
  shortjournal = {Found. Comput. Math.},
  title        = {A polynomial time iterative algorithm for matching gaussian matrices with non-vanishing correlation},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative stability of the pushforward operation by an optimal transport map. <em>FoCM</em>, <em>25</em>(4), 1259-1286. (<a href='https://doi.org/10.1007/s10208-024-09669-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the quantitative stability of the mapping that to a measure associates its pushforward measure by a fixed (non-smooth) optimal transport map. We exhibit a tight Hölder-behavior for this operation under minimal assumptions. Our proof essentially relies on a new bound that quantifies the size of the singular sets of a convex and Lipschitz continuous function on a bounded domain.},
  archive      = {J_FoCM},
  author       = {Carlier, Guillaume and Delalande, Alex and Mérigot, Quentin},
  doi          = {10.1007/s10208-024-09669-4},
  journal      = {Foundations of Computational Mathematics},
  month        = {8},
  number       = {4},
  pages        = {1259-1286},
  shortjournal = {Found. Comput. Math.},
  title        = {Quantitative stability of the pushforward operation by an optimal transport map},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularized stein variational gradient flow. <em>FoCM</em>, <em>25</em>(4), 1199-1257. (<a href='https://doi.org/10.1007/s10208-024-09663-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stein variational gradient descent (SVGD) algorithm is a deterministic particle method for sampling. However, a mean-field analysis reveals that the gradient flow corresponding to the SVGD algorithm (i.e., the Stein Variational Gradient Flow) only provides a constant-order approximation to the Wasserstein gradient flow corresponding to the KL-divergence minimization. In this work, we propose the Regularized Stein Variational Gradient Flow, which interpolates between the Stein Variational Gradient Flow and the Wasserstein gradient flow. We establish various theoretical properties of the Regularized Stein Variational Gradient Flow (and its time-discretization) including convergence to equilibrium, existence and uniqueness of weak solutions, and stability of the solutions. We provide preliminary numerical evidence of the improved performance offered by the regularization.},
  archive      = {J_FoCM},
  author       = {He, Ye and Balasubramanian, Krishnakumar and Sriperumbudur, Bharath K. and Lu, Jianfeng},
  doi          = {10.1007/s10208-024-09663-w},
  journal      = {Foundations of Computational Mathematics},
  month        = {8},
  number       = {4},
  pages        = {1199-1257},
  shortjournal = {Found. Comput. Math.},
  title        = {Regularized stein variational gradient flow},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Algebraic varieties in quantum chemistry. <em>FoCM</em>, <em>25</em>(4), 1167-1198. (<a href='https://doi.org/10.1007/s10208-024-09657-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop algebraic geometry for coupled cluster (CC) theory of quantum many-body systems. The high-dimensional eigenvalue problems that encode the electronic Schrödinger equation are approximated by a hierarchy of polynomial systems at various levels of truncation. The exponential parametrization of the eigenstates gives rise to truncation varieties. These generalize Grassmannians in their Plücker embedding. We explain how to derive Hamiltonians, we offer a detailed study of truncation varieties and their CC degrees, and we present the state of the art in solving the CC equations.},
  archive      = {J_FoCM},
  author       = {Faulstich, Fabian M. and Sturmfels, Bernd and Sverrisdóttir, Svala},
  doi          = {10.1007/s10208-024-09657-8},
  journal      = {Foundations of Computational Mathematics},
  month        = {8},
  number       = {4},
  pages        = {1167-1198},
  shortjournal = {Found. Comput. Math.},
  title        = {Algebraic varieties in quantum chemistry},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Koszul complexes and relative homological algebra of functors over posets. <em>FoCM</em>, <em>25</em>(4), 1121-1165. (<a href='https://doi.org/10.1007/s10208-024-09660-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under certain conditions, Koszul complexes can be used to calculate relative Betti diagrams of vector space-valued functors indexed by a poset, without the explicit computation of global minimal relative resolutions. In relative homological algebra of such functors, free functors are replaced by an arbitrary family of functors. Relative Betti diagrams encode the multiplicities of these functors in minimal relative resolutions. In this article we provide conditions under which grading the chosen family of functors leads to explicit Koszul complexes whose homology dimensions are the relative Betti diagrams, thus giving a scheme for the computation of these numerical descriptors.},
  archive      = {J_FoCM},
  author       = {Chachólski, Wojciech and Guidolin, Andrea and Ren, Isaac and Scolamiero, Martina and Tombari, Francesca},
  doi          = {10.1007/s10208-024-09660-z},
  journal      = {Foundations of Computational Mathematics},
  month        = {8},
  number       = {4},
  pages        = {1121-1165},
  shortjournal = {Found. Comput. Math.},
  title        = {Koszul complexes and relative homological algebra of functors over posets},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergent regularization in inverse problems and linear plug-and-play denoisers. <em>FoCM</em>, <em>25</em>(4), 1087-1120. (<a href='https://doi.org/10.1007/s10208-024-09654-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regularization is necessary when solving inverse problems to ensure the well-posedness of the solution map. Additionally, it is desired that the chosen regularization strategy is convergent in the sense that the solution map converges to a solution of the noise-free operator equation. This provides an important guarantee that stable solutions can be computed for all noise levels and that solutions satisfy the operator equation in the limit of vanishing noise. In recent years, reconstructions in inverse problems are increasingly approached from a data-driven perspective. Despite empirical success, the majority of data-driven approaches do not provide a convergent regularization strategy. One such popular example is given by iterative plug-and-play (PnP) denoising using off-the-shelf image denoisers. These usually provide only convergence of the PnP iterates to a fixed point, under suitable regularity assumptions on the denoiser, rather than convergence of the method as a regularization technique, thatis under vanishing noise and regularization strength. This paper serves two purposes: first, we provide an overview of the classical regularization theory in inverse problems and survey a few notable recent data-driven methods that are provably convergent regularization schemes. We then continue to discuss PnP algorithms and their established convergence guarantees. Subsequently, we consider PnP algorithms with learned linear denoisers and propose a novel spectral filtering technique of the denoiser to control the strength of regularization. Further, by relating the implicit regularization of the denoiser to an explicit regularization functional, we are the first to rigorously show that PnP with a learned linear denoiser leads to a convergent regularization scheme. The theoretical analysis is corroborated by numerical experiments for the classical inverse problem of tomographic image reconstruction.},
  archive      = {J_FoCM},
  author       = {Hauptmann, Andreas and Mukherjee, Subhadip and Schönlieb, Carola-Bibiane and Sherry, Ferdia},
  doi          = {10.1007/s10208-024-09654-x},
  journal      = {Foundations of Computational Mathematics},
  month        = {8},
  number       = {4},
  pages        = {1087-1120},
  shortjournal = {Found. Comput. Math.},
  title        = {Convergent regularization in inverse problems and linear plug-and-play denoisers},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="gpem">GPEM - 8</h2>
<ul>
<li><details>
<summary>
(2025). Chaotic map-coded metaheuristics for metameric variable-length problems. <em>GPEM</em>, <em>26</em>(2), 1-54. (<a href='https://doi.org/10.1007/s10710-025-09517-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing systems with an unknown number of critical homogenous components presents a significant challenge in many real-world applications. Traditional metaheuristic approaches often require predefined component counts, leading to suboptimal solutions when this number is uncertain. To address this, we propose a chaotic map-encoded metaheuristic framework that dynamically adjusts the number of components during optimization. This approach is applied to two complex problems: dendritic neuron model (DNM) optimization, where the goal is to determine the optimal number of dendritic branches for improved learning performance, and wind farm layout optimization (WFLOP), which seeks to optimize the placement and number of wind turbines to maximize energy output while minimizing wake effects. Experimental results show that this approach outperforms variable-length genetic algorithms in DNM optimization and demonstrates competitive performance in WFLOP. These findings highlight the potential of chaotic maps to improve metaheuristic efficiency in variable-length optimization problems.},
  archive      = {J_GPEM},
  author       = {Yang, Haichuan and Li, Haotian and Yang, Yifei and Chiba, Naoya and Kagami, Shingo and Hashimoto, Koichi and Nagata, Yuichi},
  doi          = {10.1007/s10710-025-09517-6},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {12},
  number       = {2},
  pages        = {1-54},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Chaotic map-coded metaheuristics for metameric variable-length problems},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oversampling method based on adaptive artificial immune network and SMOTE. <em>GPEM</em>, <em>26</em>(2), 1-71. (<a href='https://doi.org/10.1007/s10710-025-09516-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of data imbalance often causes classification algorithms to overlook the minority classes, which are usually more valuable in practical applications. Consequently, this impacts the classification performance. Sampling strategies balance class distribution at the data level, making them an effective means of improving classifier performance. However, most existing methods focus on achieving a balance in sample quantity between classes while neglecting the impact of the original data’s spatial distribution in the feature space on classification outcomes. A new oversampling algorithm based on Adaptive Artificial Immune Network and SMOTE (ADAIN-SMOTE) is proposed in this paper. ADAIN incorporates a global mutation operator and an adaptive network suppression operator into Artificial Immune Network, enhancing the evolutionary network’s learning capability for the original data and achieving adaptive network compression. Ultimately, it evolves a network structure capable of mapping the distribution of the original data, which is then used to augment the minority class. SMOTE is subsequently applied to oversample the new minority class. The resulting synthetic minority samples avoid excessive sampling of noisy or irrelevant data and better preserve the true distribution of the original data. This method demonstrates general applicability across various classification models and significantly enhances classification performance. Comparative experiments on 26 datasets, 5 classifiers, and 8 oversampling algorithms show that the proposed algorithm ranks first in terms of average F1, G-mean, PR_AUC, and MCC metrics.},
  archive      = {J_GPEM},
  author       = {Bai, Lin and Sun, Mengchen and Jiang, Xianlin and Liu, Jingxuan and Liu, Jialu and Pan, Xiaoying},
  doi          = {10.1007/s10710-025-09516-7},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {12},
  number       = {2},
  pages        = {1-71},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {An oversampling method based on adaptive artificial immune network and SMOTE},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acknowledgment to reviewers (2024). <em>GPEM</em>, <em>26</em>(2), 1-3. (<a href='https://doi.org/10.1007/s10710-025-09518-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_GPEM},
  author       = {Trujillo, Leonardo},
  doi          = {10.1007/s10710-025-09518-5},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {12},
  number       = {2},
  pages        = {1-3},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Acknowledgment to reviewers (2024)},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial introduction to the special issue on evolutionary computation in art, music and design. <em>GPEM</em>, <em>26</em>(2), 1-4. (<a href='https://doi.org/10.1007/s10710-025-09519-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_GPEM},
  author       = {Machado, Penousal and Romero, Juan},
  doi          = {10.1007/s10710-025-09519-4},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {12},
  number       = {2},
  pages        = {1-4},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Editorial introduction to the special issue on evolutionary computation in art, music and design},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality-diversity in problems with composite solutions: A case study on body–brain robot optimization. <em>GPEM</em>, <em>26</em>(2), 1-36. (<a href='https://doi.org/10.1007/s10710-025-09520-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When considering those optimization problems where the solution is a combination of two parts, as, e.g., the concurrent optimization of the body and the brain of a robotic agent, one might want to solve them “in a quality-diversity (QD) way”, i.e., obtaining not just one very good solution, but a set of good and diverse solutions. We call them QD composite problems, and we propose a general formulation for them, as well as a set of indexes useful for comprehensively assessing solutions by measuring both quality and diversity. We experimentally compare a few QD evolutionary algorithms (EAs) on a case study of body–brain optimization of simulated robots, including several variants of MAP-elites (ME), a popular and effective EA for QD. We also propose a novel ME variant, called coevolutionary MAP-elites (CoME), that internally employs two populations, one for each part of the solution, and enforces diversity on them through user-provided descriptors, as the underlying ME does. CoME, instead of blindly combining all the respective parts to obtain full solutions, adopts a specific mapping strategy that is based on the location of each solution part in the respective descriptors space. The results of our comparative analysis show that ME works well in QD composite problems, but only if two archives, instead of just one, are employed, one for each part of the solution. Moreover, we show that the use of multi-archive variants of ME, e.g., CoME, can provide insights on the interplay between the two parts of the solution for the problem at hand, shedding light on key dynamics in co-evolution.},
  archive      = {J_GPEM},
  author       = {Medvet, Eric and Lippolis, Samuele and Nadizar, Giorgia},
  doi          = {10.1007/s10710-025-09520-x},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {12},
  number       = {2},
  pages        = {1-36},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Quality-diversity in problems with composite solutions: A case study on body–brain robot optimization},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kenichi morita: Reversible world of cellular automata. <em>GPEM</em>, <em>26</em>(2), 1-5. (<a href='https://doi.org/10.1007/s10710-025-09521-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_GPEM},
  author       = {Rokicki, Tomas},
  doi          = {10.1007/s10710-025-09521-w},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {12},
  number       = {2},
  pages        = {1-5},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Kenichi morita: Reversible world of cellular automata},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On fitting numerical features into probabilistic distributions to represent data for fuzzy pattern trees. <em>GPEM</em>, <em>26</em>(2), 1-26. (<a href='https://doi.org/10.1007/s10710-025-09522-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Pattern Trees (FPTs) are symbolic tree-based structures whose internal nodes are fuzzy operators, and the leaves are fuzzy features, which enhance interpretability by representing data with meaningful fuzzy terms. However, conventional FPT approaches typically employ uniformly distributed membership functions, which often fail to accurately represent features in real-world datasets. In this work, we propose an automatic method to adapt the bounds of fuzzy features based on their data distributions, with a focus on a simple triangular membership scheme. We evaluate our approach across 11 benchmark classification problems, incorporating six parsimony pressure methods to promote more compact solutions. Our results demonstrate that the adapted fuzzification scheme, beyond improving interpretability, consistently yields models that better balance accuracy and size when compared to uniform representations, appearing on the Pareto front 20 times, while the second-best scheme appeared only 15 times.},
  archive      = {J_GPEM},
  author       = {de Lima, Allan and Albarracín, Juan F. H. and Dias, Douglas Mota and Amaral, Jorge and Ryan, Conor},
  doi          = {10.1007/s10710-025-09522-9},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {12},
  number       = {2},
  pages        = {1-26},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {On fitting numerical features into probabilistic distributions to represent data for fuzzy pattern trees},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introducing look-ahead into relocation rules generated with genetic programming for the container relocation problem. <em>GPEM</em>, <em>26</em>(2), 1-39. (<a href='https://doi.org/10.1007/s10710-025-09523-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The container relocation problem is a critical combinatorial optimisation problem in warehouses and container ports. The goal is to retrieve all containers while minimising unnecessary relocations. As this problem is NP-hard, various heuristics have been proposed, including relocation rules (RRs), simple constructive heuristics that iteratively build solutions by determining how containers should be relocated within the yard for efficient retrieval. However, manually designing effective RRs is challenging, leading to the use of genetic programming to generate them automatically. A key limitation of both manually and automatically designed RRs is their restricted problem view and limited decision-making scope. This often results in suboptimal relocations, negatively impacting future operations and overall efficiency. A crucial aspect of RR design is defining effective relocation schemes that enhance decision-making by considering the long-term impact of relocations. This study investigates several relocation schemes that provide RRs with lookahead capabilities, enabling them to anticipate future consequences and make more informed moves. In addition to two standard schemes, four novel relocation schemes are introduced and evaluated using an established problem set. The results demonstrate that properly adapting relocation schemes can significantly enhance the performance of automatically designed RRs, leading to significantly better results.},
  archive      = {J_GPEM},
  author       = {Ɖurasević, Marko and Ɖumić, Mateja and Gil Gala, Francisco Javier and Jakobović, Domagoj},
  doi          = {10.1007/s10710-025-09523-8},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {12},
  number       = {2},
  pages        = {1-39},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Introducing look-ahead into relocation rules generated with genetic programming for the container relocation problem},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijcis">IJCIS - 233</h2>
<ul>
<li><details>
<summary>
(2025). Image registration using the arithmetic optimization algorithm for robotic visual servoing. <em>IJCIS</em>, <em>18</em>(1), 1-12. (<a href='https://doi.org/10.1007/s44196-024-00612-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual servoing using image registration is a method employed in robotics to control the movement of a system using visual information. In this context, we propose a new intensity-based image registration algorithm (IBIR) that uses information derived from images acquired at different times or from different views to determine the parameters of the geometric transformations needed to align these images. The Arithmetic Optimization Algorithm (AOA) is used to optimize these parameters, minimizing the difference between the images to be aligned. The proposed algorithm, Intensity-Based Image Registration via Arithmetic Optimisation Algorithm (IBIRAOA), is robust to image data fluctuations and perturbations and can avoid local optima. Simulation results prove the importance and efficiency of the proposed algorithm in terms of computation time and similarity of aligned images compared to other methods based on various metaheuristics. In addition, our results confirm a significant improvement in the trajectory of the wheeled mobile robot, thus reinforcing the overall effectiveness of our method in practical navigation and robotic control applications.},
  archive      = {J_IJCIS},
  author       = {Kmich, Mohamed and Harrade, Inssaf and Karmouni, Hicham and Sayyouri, Mhamed and Askar, S. S. and Abouhawwash, Mohamed},
  doi          = {10.1007/s44196-024-00612-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Image registration using the arithmetic optimization algorithm for robotic visual servoing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MORKO: A multi-objective Runge–Kutta optimizer for multi-domain optimization problems. <em>IJCIS</em>, <em>18</em>(1), 1-34. (<a href='https://doi.org/10.1007/s44196-024-00714-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current landscape, there is a rapid increase in the creation of new algorithms designed for specialized problem scenarios. The performance of these algorithms in unfamiliar or practical settings often remains untested. This paper presents a new development, the multi-objective Runge–Kutta optimizer (MORKO), which is built upon the principles of elitist non-dominated sorting and crowding distance. The goal is to achieve superior efficiency, diversity, and robustness in solutions. MORKO effectiveness is further enhanced by incorporating various strategies that maintain a balance between diversity and execution efficiency. This approach not only directs the search toward optimal regions but also ensures that the process does not become stagnant. The efficiency of MORKO is compared against renowned algorithms like the multi-objective marine predicator algorithm (MOMPA), multi-objective gradient-based optimizer (MOGBO), multi-objective evolutionary algorithm based on decomposition (MOEA/D), and non-dominated sorting genetic algorithm (NSGA-II) on several test benchmarks such as ZDT, DTLZ, constraint (CONSTR, TNK, SRN, BNH, OSY and KITA) and real-world engineering design (brushless DC wheel motor, safety isolating transformer, helical spring, two-bar truss, welded beam, disk brake, tool spindle and cantilever beam) problems. We used unique, non-overlapping performance metrics for this comparison and suggested a fresh correlation analysis technique for exploration. The MORKO algorithm outcomes were rigorously tested and confirmed using the non-parametric statistical evaluations. The MORKO algorithm proves to excel in deriving comprehensive and varied solutions for many tests and practical challenges, owing to its multifaceted features. Looking ahead, MORKO has potential applications in complex engineering and management tasks.},
  archive      = {J_IJCIS},
  author       = {Kalita, Kanak and Jangir, Pradeep and Pandya, Sundaram B. and Alzahrani, Ahmed Ibrahim and Alblehai, Fahad and Abualigah, Laith and Ezugwu, Absalom E.},
  doi          = {10.1007/s44196-024-00714-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {MORKO: A multi-objective Runge–Kutta optimizer for multi-domain optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective particle swarm optimization with integrated fireworks algorithm and size double archiving. <em>IJCIS</em>, <em>18</em>(1), 1-37. (<a href='https://doi.org/10.1007/s44196-024-00722-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-objective particle swarm optimization (MOPSO) is an optimization technique that mimics the foraging behavior of birds to solve difficult optimization problems. MOPSO is well known for its strong global search capability, which efficiently locates solutions that are close to the global optimum across a wide search domain. However, similar to many other optimization algorithms, the fast convergence property of MOPSO can occasionally lead to the population entering the local optimum too soon, obstructing researchers from investigating more efficient solutions. To address this challenge, the study proposes a novel framework that integrates the fireworks algorithm (FA) into MOPSO and establishes a size-double archiving mechanism to maintain population diversity. By preventing population homogenization, this mechanism promotes the retention of better solutions. Additionally, by fusing evolutionary data analysis with particle information, the study offers new individual optimal choices and adaptive parameter tuning to improve the algorithm’s robustness and adaptability and better manage the complexity of multi-objective optimization problems (MOPs). The suggested algorithm is compared with several existing MOPSOs and multi-objective evolutionary algorithms (MOEAs) in simulation experiments. Standard test problems like ZDT, UF, and DTLZ are used in the experiments. The new algorithm performs exceptionally well in terms of improving convergence and population diversity, as well as demonstrating significant competitiveness for solving MOPs.},
  archive      = {J_IJCIS},
  author       = {Zhang, Yansong and Liu, Yanmin and Zhang, Xiaoyan and Song, Qian and Ouyang, Aijia and Yang, Jie},
  doi          = {10.1007/s44196-024-00722-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Multi-objective particle swarm optimization with integrated fireworks algorithm and size double archiving},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stock market prediction based multi-attribute decision making model using picture fuzzy $${\hat{Z}}$$ -information. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-024-00664-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As demonstrated in the section above, the stock market place is a dynamic factor, which makes it possible for traders and investors to make good decisions based on the information acquired through accurate prediction. This research aims at improving the prediction of stock market by applying a new method to Multi-attribute Group Decision making (MAGDM). MAGDM goes through a cycle of evaluating and ranking several criteria hence enhancing the decision-making aspects further. To overcome the shortcomings of prior models, some EU and FU is incorporated by combining Zadeh’s $${\hat{Z}}$$ -numbers with Picture Fuzzy Sets (PFSs). This integration is to enhance the ability of the model to address completely unclear decisions utilizing the peculiarities of $${\hat{Z}}$$ -numbers. To compare decisions between decision-makers, we proposed picture fuzzy $${\hat{Z}}$$ -numbers (PF $${\hat{Z}}$$ N) and for their aggregation, introduced picture fuzzy weighted averaging, picture fuzzy ordered weighted averaging, picture fuzzy hybrid averaging, picture fuzzy weighted geometric, picture fuzzy ordered weighted geometric and picture fuzzy hybrid geometric operators based algebraic $${\mathfrak {T}}$$ -norm ( $${\mathfrak {T}}-N$$ ) and $${\mathfrak {T}}$$ -conorm ( $${\mathfrak {T}}-CNs$$ ) To verify the efficiency of our suggested technique, we compare these operators with the Combined Compromised Solution (CoCoSo) model focusing on the stock market analysis. Our results, therefore, show how these operators are important in improving decision making accuracy and precision in conditions of risk. This research laid down the basis for enhancing decision-making and dealing with uncertainty in different fields especially in the application of stock market prediction. The proposed methodology can be attributed to providing a systematic and a more efficient way of dealing with uncertainty which in one way or the other has an outcome of enhancing the credibility of the decision making process in the financial sector.},
  archive      = {J_IJCIS},
  author       = {Ashraf, Shahzaib and Khalid, Amna and Batool, Bushra and Tlija, Mehdi and Jana, Chiranjibe and Pamucar, Dragan},
  doi          = {10.1007/s44196-024-00664-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Stock market prediction based multi-attribute decision making model using picture fuzzy $${\hat{Z}}$$ -information},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid dynamic harris hawks optimized gated recurrent unit approach for breast cancer prediction. <em>IJCIS</em>, <em>18</em>(1), 1-16. (<a href='https://doi.org/10.1007/s44196-024-00712-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The breast cancer (BC) prediction is improved through the machine learning (ML) techniques. In this study, we develop an innovative forecasting framework called the Dynamic Harris Hawks Optimized Gated Recurrent Unit (DHH-GRU) for the prediction of BC. It combines the Gated Recurrent Unit (GRU) and Harris Hawks Optimization (HHO) methods. We gathered data and a training set that included the Wisconsin diagnostic BC (WDBC) dataset, which contains 569 patients with malignant and beginning cases. The collected data were pre-processed using min–max normalization, and important features were extracted by Fast Fourier transform (FFT) and the process of reducing the dimensionality with principal component analysis (PCA). Decimal scaling is employed to equalize the various feature effects. The proposed DHH-GRU technique incorporated the GRU for capturing sequential connections on temporal medical information, and the optimization process, DHH optimization, is utilized. The proposed method's effectiveness is compared and estimated with various existing techniques in terms of log-loss (0.06%), accuracy (98.05%), precision (98.09%), F1-score (98.28%), and recall (98.15%). The proposed DHH-GRU method has a more predictive ability with the sequential dependency in capturing GRU and DHH optimization’s combined behaviour of hunting. This method significantly improved the accuracy of BC prediction.},
  archive      = {J_IJCIS},
  author       = {Natarajan, Rajesh and Krishna, Sujatha and Gururaj, H. L. and Flammini, Francesco and Alfurhood, Badria Sulaiman and Kumar, C. M. Naveen},
  doi          = {10.1007/s44196-024-00712-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel hybrid dynamic harris hawks optimized gated recurrent unit approach for breast cancer prediction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive ensemble learning model-based binary white shark optimizer for software defect classification. <em>IJCIS</em>, <em>18</em>(1), 1-51. (<a href='https://doi.org/10.1007/s44196-024-00716-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software dominates modern enterprises, affecting numerous functions. Software firms constantly experiment with new methodologies to define and assess software quality to stay competitive and ensure excellence. Software engineering uses fundamentals and cutting-edge technology to develop great software. In recent decades, Data-mining techniques and machine learning for classifying problematic software projects have emerged to improve software quality. ML approaches, especially ensemble learning models, are becoming fundamental to software engineers’ daily jobs. This work created a binary white shark optimizer (WSO) to optimize standard ensemble learning models. The objective is to identify the most suitable ensemble number for weak learners to maximize accuracy on benchmark datasets. The EM model uses 14 weak learners. Twenty-one experimental runs are performed on 15 software-defective module datasets. The optimized ensemble model outperforms the standard Ensemble learning model in AUC-ROC, Accuracy, Precision, Recall, F1-Score, and Specificity. The enhanced model has an average accuracy of 86%, compared to 76% for the standard ensemble model across all datasets. The optimized model outperformed the conventional ensemble for the same datasets, with an average AUC of 72% compared to 61% for the standard ensemble. The optimized model was more stable than the standard model, with an STD of 5.53E−03 vs 7.24E−02 for the ensemble model. The WSO optimization process strengthens and generalizes optimizeels. The study suggests that evolutionary metaheuristic approaches can enhance EM models’ accuracy, trustworthiness, and adaptability.},
  archive      = {J_IJCIS},
  author       = {Saraireh, Jameel and Agoyi, Mary and Kassaymeh, Sofian},
  doi          = {10.1007/s44196-024-00716-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-51},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Adaptive ensemble learning model-based binary white shark optimizer for software defect classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Properties and applications of neutrosophic burr XII distribution. <em>IJCIS</em>, <em>18</em>(1), 1-11. (<a href='https://doi.org/10.1007/s44196-024-00721-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Primarily, when the hazards function has intricate structures, the BrXII distribution is an established framework for lifetime data analysis. However, classical probability models are limited in the sense that they cannot measure or record the data in exactness fueling the notion of indeterminacy in data collection process. This study addresses this issue by proposing the idea of neutrosophic BrXII (NeS-BrXII) distribution. The primary objective is to study the statistical properties of the proposed model by providing explicit expressions of reliability properties, the expression of moments and generating function, expression of order statistics, mean residual life, mean inactivity time, stochastic ordering, income inequality measures, and entropy in neutrosophic realm. In addition, the neutrosophic model parameters are estimated using the principle of maximum-likelihood estimation. Further, the precision of these model estimates is verified via a simulation study of the proposed model. Applying the model on two real-world material sciences data sets reinforces its efficacy with the NeS-BrXII distribution proving to be more suitable for managing anomalies in neutrosophic surface analysis among other models.},
  archive      = {J_IJCIS},
  author       = {Al-Essa, Laila A. and Jamal, Farrukh and Shafiq, Shakaiba and Khan, Sadaf and Abbas, Qamer and Khan Sherwani, Rehan Ahmad and Aslam, Muhammad},
  doi          = {10.1007/s44196-024-00721-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Properties and applications of neutrosophic burr XII distribution},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-way crossed effects fuzzy panel linear regression model. <em>IJCIS</em>, <em>18</em>(1), 1-10. (<a href='https://doi.org/10.1007/s44196-024-00723-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last two decades, the panel data model has become a focus of applied research. While there are numerous proposals for soft regression models in the literature, only a few linear regression models have been proposed based on fuzzy panel data. However, these models have serious limitations. This study is an attempt to propose a kind of two-way fuzzy panel regression model with crossed effects, fuzzy responses and crisp predictors to overcome the shortcomings of these models in real applications. The corresponding parameter estimation is provided based on a three-step procedure. For this purpose, the conventional least absolute error technique is employed. Two real data sets are analyzed to investigate the fitting and predictive capabilities of the proposed fuzzy panel regression model. These real data applications demonstrate that our proposed model has good fitting accuracy and predictive performance.},
  archive      = {J_IJCIS},
  author       = {Hesamian, Gholamreza and Johannssen, Arne},
  doi          = {10.1007/s44196-024-00723-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A two-way crossed effects fuzzy panel linear regression model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: A model for estimating resiliency of AI-based classifiers defending against cyber attacks. <em>IJCIS</em>, <em>18</em>(1), 1. (<a href='https://doi.org/10.1007/s44196-024-00725-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Barik, Kousik and Misra, Sanjay and Fernandez-Sanz, Luis},
  doi          = {10.1007/s44196-024-00725-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Correction: A model for estimating resiliency of AI-based classifiers defending against cyber attacks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel conflict deduction algorithm based on contradiction separation inference rule. <em>IJCIS</em>, <em>18</em>(1), 1-15. (<a href='https://doi.org/10.1007/s44196-024-00726-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated reasoning, a significant field within artificial intelligence, has attracted increased attention in recent years due to the rising demand for trustworthy AI. Binary resolution, among other inference rules, is crucial in automated reasoning of first-order logic, including the new conflict resolution method. Conflict resolution processes only two clauses in each deduction step and eliminates a complementary pairs of literals from input clauses. This paper proposes a contradiction separation conflict deduction (CSCD) method based on the contradiction separation rule to address these limitations. This novel resolution methodology, together with its automated reasoning theory and method, handles several clauses in each deduction step to seek for conflicts and generates learnt clauses through synergized deduction. Thus, the approach improves deduction by detecting conflicts more effectively, especially with lengthier input clauses. CSCD and conflict resolution are analyzed in detail, then how to create a practical CSCD algorithm and its implementation is summarized. We tested the CSCD algorithm to solve the CASC-26 problems and also applied it to the current leading ATP system (Eprover). Experimental results show that the CSCD deduction approach improves reasoning capability of conflict deduction method. Additionally, the Eprover with the proposed CSCD algorithm improves its performance and has solved various problems with a rating of 1 from the benchmark database TPTP.},
  archive      = {J_IJCIS},
  author       = {Guo, Hailin and Cao, Feng and Yi, Jianbing and Wu, Guanfeng and Li, Weicai},
  doi          = {10.1007/s44196-024-00726-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel conflict deduction algorithm based on contradiction separation inference rule},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing FACTS device placement using the fata morgana algorithm: A cost and power loss minimization approach in uncertain load scenario-based systems. <em>IJCIS</em>, <em>18</em>(1), 1-38. (<a href='https://doi.org/10.1007/s44196-024-00727-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, reliable power delivery and increasing demand are important issues in modern power systems. Flexible Alternating Current Transmission Systems (FACTS) devices are used to control transmission line parameters to increase power transfer and stability. Nevertheless, the problem of determining the optimal placement and sizing of these devices is still challenging, as the placement and sizing of the devices affects generation costs, power losses, voltage stability, and system reliability. This study proposes the Fata Morgana Algorithm (FATA), an optimization algorithm inspired by the natural process of mirage formation to optimize placement and sizing of FACTS devices in an IEEE 30 bus system with wind turbine integration. The FATA algorithm is evaluated against recently developed and improved optimization techniques, such as rime-ice formation phenomenon based Improved RIME (IRIME) Algorithm, Newton–Raphson-Based Optimization (NRBO), Resistance Capacitance Algorithm (RCA), Krill Optimization Algorithm (KOA), and Grey Wolf Optimizer (GWO), across multiple optimization objectives: reduction in generation cost, reduction in power loss and combined generation cost plus power loss, termed as Gross cost function. Results obtained show that FATA consistently outperforms the other algorithms in terms of convergence and solution quality, offering a robust approach to solving single objective optimization problems. FATA theoretically provides a good balance between exploration and exploitation, and produces better global solutions. It practically increases power system efficiency by lowering operational costs and losses and improving stability. Results indicate that the FATA algorithm produced minimum generation cost of 807.0405 $/h, which is 0.088–0.426% less than the competing algorithms. It also reduced power losses to 5.5917 MW, which is 1.095–6.781% less than other methods. For gross cost minimization, the FATA algorithm achieved a minimum gross cost of 1366.3727 $/h, which is 0.4799% better than the next best algorithm and 3.2261% better than the worst. The results also show that FATA is robust in solving complex optimization problems in power systems, and it provides significant improvements in run time and convergence efficiency. The main advantage for readers is that FATA provides a reliable and efficient way to optimize power systems. Future work could also investigate the application of FATA in real time, as well as in larger power networks with more renewable energy sources.},
  archive      = {J_IJCIS},
  author       = {Aljaidi, Mohammad and Jangir, Pradeep and Agrawal, Sunilkumar P. and Pandya, Sundaram B. and Parmar, Anil and Alkoradees, Ali Fayez and Arpita and Smerat, Aseel},
  doi          = {10.1007/s44196-024-00727-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimizing FACTS device placement using the fata morgana algorithm: A cost and power loss minimization approach in uncertain load scenario-based systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time facial expression recognition based on image processing in virtual reality. <em>IJCIS</em>, <em>18</em>(1), 1-16. (<a href='https://doi.org/10.1007/s44196-024-00729-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More virtual reality (VR) scenarios have become more prevalent in recent years. More and more people are getting into VR, meaning that objective physiological measures to assess a user's emotional state automatically are becoming more critical. Individuals’ emotional states impact their behaviour, opinions, emotions, and decisions. They may be used to analyze VR experiences and make systems react to and engage with the user’s emotions. VR environments require users to wear head-mounted displays (HMDs), blocking off their upper faces. That makes traditional Facial Expression Recognition (FER) approaches very limited in their usefulness. Thus, a Deep Learning (DL) solution combined with image processing is utilized to classify universal emotions: sadness, happiness, disgust, anger, fear and surprise. Hence, this paper suggests the Deep Automatic Facial Expression Recognition Model (DAFERM) for interactive virtual reality (VR) applications such as intelligent education, social networks, and virtual training. Two main parts comprise the system: one that uses deep neural networks (DNNs) for facial emotion identification and another that automatically tracks and segments faces. The system begins by following a marker on the front of the head-mounted display (HMD). With the help of the spatial data that has been retrieved, the positions and rotations of the face are estimated to segment the mouth. Finally, the system interacts with DNN using the pixels processed by the lips. It obtains the facial expression results in real time using an adaptive method for histogram-based mouth segmentation.},
  archive      = {J_IJCIS},
  author       = {Gong, Qingzhen and Liu, Xuefang and Ma, Yongqiang},
  doi          = {10.1007/s44196-024-00729-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Real-time facial expression recognition based on image processing in virtual reality},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized novel text embedding approach for fake news detection on twitter x: Integrating social context, temporal dynamics, and enhanced interpretability. <em>IJCIS</em>, <em>18</em>(1), 1-36. (<a href='https://doi.org/10.1007/s44196-024-00730-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of widespread misinformation, detecting fake news has become a crucial challenge, particularly on social media platforms. This paper introduces an optimized approach for Fake News Detection, combining BERT and GloVe embeddings with Principal Component Analysis (PCA) and attention mechanisms, enriched by social and temporal features for more effective text representation. Leveraging the CIC Truth Seeker Dataset 2023, we applied SHAP for feature selection and interpretability, ensuring transparency in the model’s predictions. Our methodology achieved a remarkable accuracy of 99.9% using a Random Forest classifier, showcasing the efficacy of this optimized hybrid approach. The integration of interpretability techniques such as LIME and SHAP provides deeper insights into the model’s decisions, making it a reliable tool for combating misinformation. This novel approach offers a robust and transparent solution to the growing threat of fake news, contributing significantly to the integrity of online information and public discourse on platforms like Twitter X.},
  archive      = {J_IJCIS},
  author       = {AlJamal, Mahmoud and Alquran, Rabee and Alsarhan, Ayoub and Aljaidi, Mohammad and Al-Jamal, Wafa’ Q. and Alkoradees, Ali Fayez},
  doi          = {10.1007/s44196-024-00730-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimized novel text embedding approach for fake news detection on twitter x: Integrating social context, temporal dynamics, and enhanced interpretability},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-driven optimized chaotic encryption scheme for medical image transmission in IoT-edge environment. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-024-00731-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) is adopted in a wide spectrum of applications in which a vast amount of data are produced and distributed to centralized cloud platforms to deliver various services. It involves smart devices that collect thousands of terabytes of heterogeneous data and deployed this to make instant decision that aids for the better performance and most comfort life. Traditional IoT architecture is heavily centralized, where it stores the most sensitive information that creates the multiple threats and security breaches as the attackers target towards these centralized cloud systems. To improve the security chain in IoT environment, edge computing (EC) was introduced to distribute the applications of IoT at the edge of the communication networks. However, these edge-based IoT are also vulnerable to many threats due to their decentralized and in secured management. Block chain (BC) technology offers a most trusted solution to resolve the security issues in the IoT-Edge computing environment. This research study presents the block chain driven medical image encryption technique using modified honey badger optimization with the ensemble chaotic systems. The proposed block chain framework uses the divergent methods that integrates differential scroll, Hénon chaotic maps and modified honey badger optimization to generate the optimum keys and high secured image data. These high secured data are stored in the block chain, ensuring the image security to be stored in edge nodes. The complete framework was experimented using Ethereum using Ganache API and Python3.19 are utilized as the major programs for designing the varied interfaces of the recommended model. The comprehensive experimentation is undertaken to assess the security strength of the recommended encryption scheme. The evaluation metrics like as NACI, UACI, Entropy and standard verification methods such as NIST standard tests are deployed and analyzed. To prove it security strength, proposed secured BC framework is compared with the wide-variety of secured frameworks. The experimental findings reveal that the suggested framework establishes a more robust and secure environment for image exchange, surpassing the performance of other blockchain-based systems in terms of integrity, robustness and security. Finally, the paper spreads the bright light of advantages in deploying the proposed framework to formulate the most secured environment in the IoT-Edge environment for medical image transmission.},
  archive      = {J_IJCIS},
  author       = {Archana, Goli and Goyal, Rajeev and Kumar, K. M. V. Madan},
  doi          = {10.1007/s44196-024-00731-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Blockchain-driven optimized chaotic encryption scheme for medical image transmission in IoT-edge environment},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing stock portfolio selection with trapezoidal bipolar fuzzy VIKOR technique with boruta-GA hybrid optimization model: A multicriteria decision-making approach. <em>IJCIS</em>, <em>18</em>(1), 1-30. (<a href='https://doi.org/10.1007/s44196-025-00733-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The investors’ main objective is to minimize the risks and to get the maximum returns in the random stock market requires them to choose the correct mix of the stocks. The traditional portfolio selection methods often struggle with market volatility, leading to less-than-the targeted profits. This research attempts to apply trapezoidal bipolar fuzzy Vise Kriterijumska Optimizacija I Kompromisno Resenje (VIKOR) method to help in decision-making. It also combines the fuzzy set theory with the VIKOR method, the trapezoidal bipolar fuzzy with VIKOR (TrBFV) approach offers a comprehensive and flexible system for evaluating investment options. The proposed approach has been validated through real-world illustrations. The analysis has been made with the VIKOR method and its integration with trapezoidal bipolar fuzzy sets. Financial decision-making can be very hard for investors who have access to big data due to the overwhelming amount of information they are required to interpret. The Boruta-GA approach combines the advantages of the Genetic Algorithm (GA) and Boruta Optimization Algorithm (BOA) methods, implementing the comprehensive feature selection capability of Boruta to detect all relevant features and harnessing the strength of GA to bring about improvement within a wide range of datasets. The result shows that this novel approach is effective and can help to investors to take decisions in the unpredictable financial market. This study is an attempt to provides investors with an appropriate approach to navigate the challenges of stock market investment. Abbreviations serve as a nomenclature table, detailing all acronyms.},
  archive      = {J_IJCIS},
  author       = {Sharma, Sunil Kumar},
  doi          = {10.1007/s44196-025-00733-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhancing stock portfolio selection with trapezoidal bipolar fuzzy VIKOR technique with boruta-GA hybrid optimization model: A multicriteria decision-making approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Publisher correction: Image registration using the arithmetic optimization algorithm for robotic visual servoing. <em>IJCIS</em>, <em>18</em>(1), 1-2. (<a href='https://doi.org/10.1007/s44196-025-00735-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Kmich, Mohamed and Harrade, Inssaf and Karmouni, Hicham and Sayyouri, Mhamed and Askar, S. S. and Abouhawwash, Mohamed},
  doi          = {10.1007/s44196-025-00735-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Publisher correction: Image registration using the arithmetic optimization algorithm for robotic visual servoing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal sizing and techno-economic analysis of combined solar wind power system, fuel cell and tidal turbines using meta-heuristic algorithms: A case study of lavan island. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00737-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combined renewable energy sources (RESs) are emerging as a competitive alternative to conventional energy production facilities due to their sustainability and zero-emission characteristics. However, determining the optimal system size is complicated by two major challenges: the cost of energy (COE) and the intermittent nature of RESs. This study introduces a novel mathematical approach to optimize the sizing of photovoltaic (PV), wind, hydrogen, battery, and fuel cell systems with electrolyzers, specifically tailored for the remote area of Lavan Island. The proposed method aims to deliver electricity without reliance on the traditional electricity distribution grid, while offering a scalable solution applicable to other geographical regions. The primary objective is to achieve cost-effective electricity generation while ensuring a reliable energy supply through the evaluation of system reliability indices. A fuzzy logic system is employed to minimize the costs of a hybrid system incorporating hydroelectric, wind, solar, and battery technologies, while simultaneously calculating two key reliability metrics: the Loss of Power Supply Probability (LPSP) and the Dump Energy Probability (DEP). To optimize the objective function, this study applies three advanced algorithms: the Shuffled Frog Leaping Algorithm (SFLA), the Grasshopper Optimization Algorithm (GOA), and the Honey Badger Algorithm (HBA). These algorithms are used to determine the global optimum, with comparative analyses conducted to highlight the performance of the proposed approach. The results are evaluated based on statistical metrics, including consistency, execution time, convergence speed, and the minimization of the objective function. The findings demonstrate the superiority and the reliability of the proposed method over alternative approaches, paving the way for cost-efficient and sustainable energy solutions in isolated regions.},
  archive      = {J_IJCIS},
  author       = {Talebi, Hessameddin and Nikoukar, Javad and Gandomkar, Majid},
  doi          = {10.1007/s44196-025-00737-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimal sizing and techno-economic analysis of combined solar wind power system, fuel cell and tidal turbines using meta-heuristic algorithms: A case study of lavan island},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperbolic graph convolutional network relation extraction model combining dependency syntax and contrastive learning. <em>IJCIS</em>, <em>18</em>(1), 1-25. (<a href='https://doi.org/10.1007/s44196-025-00738-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current relation extraction tasks, when the input sentence structure is complex, the performance of in-context learning methods based on large language model is still lower than that of traditional pre-train fine-tune models. For complex sentence structures, dependency syntax information can provide effective prior text structure information for relation extraction. However, most studies are affected by the noise in the syntactic information automatically extracted by natural language processing toolkits. Additionally, traditional pre-training encoders have issues such as an overly centralized representation of word embedding for high-frequency words, which adversely affects the model to learn contextual semantic information. To address proposed problem, the paper proposes a Hyperbolic Graph Convolutional Network Relation Extraction Model Combine Dependency Syntax and Contrastive Learning. Based on the hyperbolic graph neural network, dependent syntactic information and information optimization strategies are introduced to solve the problem of word embedding concentration. Simultaneously, to mitigate the impact of noise in dependency syntax information on the relation extraction task, a contrastive learning approach is employed. After the model learns context semantics separately in the original dependency syntax information and dependency syntax information with added random noise, it maximizes the mutual information between entity words to assist the model in distinguishing noise in dependency syntax. The experiments indicate that the proposed model in this paper can effectively enhance the performance of relation extraction on public datasets, especially achieving significantly higher precision on datasets with complex sentence structures compared to in-context learning.},
  archive      = {J_IJCIS},
  author       = {Li, Jinzhe},
  doi          = {10.1007/s44196-025-00738-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hyperbolic graph convolutional network relation extraction model combining dependency syntax and contrastive learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced LSTM approach for detecting IoT-based DDoS attacks using honeypot data. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00741-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the widening perils in network security is the Distributed Denial of Service (DDoS) attacks on the Internet of Things (IoT) ecosystem. This paper presents an enhanced Intrusion Detection System (IDS) through the proposal of an enhanced version of the long short-term memory (LSTM) model to detect DDoS attacks using honeypot-generated data. The proposed model aggregates the Conv1D, Bidirectional Long Short-Term Memory (Bi-LSTM), Bidirectional Gated Recurrent Unit (Bi-GRU), and dropout layers to extract temporal and spatial features from IoT traffic effectively. We tested the efficacy of the proposed system on a real-world IoT-DH dataset, which showed a remarkable accuracy of 99.41%, with an AUC score of 0.9999. A comparative analysis with other baseline models, such as LSTM, Bidirectional LSTM (Bi-LSTM), Gated Recurrent Unit (GRU), Recurrent Neural Network (RNN), Feedforward Neural Network (FNN), and Temporal Convolutional Network (TCN), proved that enhanced LSTM outperformed the other models. This indicates the robustness of the proposed model in correctly detecting DDoS attacks with high generalization capability for unseen traffic data. The contribution of this paper will be an addition to the deep learning techniques applied for the solution of intrusion detection systems (IDS), which will also allow the building and implementation of more efficient security mechanisms in IoT environments.},
  archive      = {J_IJCIS},
  author       = {Arnob, Arjun Kumar Bose and Mridha, M. F. and Safran, Mejdl and Amiruzzaman, Md and Islam, Md. Rajibul},
  doi          = {10.1007/s44196-025-00741-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An enhanced LSTM approach for detecting IoT-based DDoS attacks using honeypot data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ExAIRFC-GSDC: An advanced machine learning-based interpretable framework for accurate gas leakage detection and classification. <em>IJCIS</em>, <em>18</em>(1), 1-33. (<a href='https://doi.org/10.1007/s44196-025-00742-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gas leakage detection is imperative in various sectors, including chemical industries, coal mines, and household applications. The escalating number of accidents in coal mines, chemical industries, and homes underscores the urgency of swift and accurate gas detection methods. This research focuses on developing advanced systems that promptly identify gas types to prevent harm to human lives and the environment. This paper addresses the challenges of gas leakage detection and classification in diverse environments, such as industrial, residential, and mining scenarios. The proposed ExAIRFC-GSDC model integrates machine learning algorithms, particularly a Random Forest Classifier, with explainable artificial intelligence (XAI) techniques to enhance interpretability. This study employs a dataset comprising gas sensor measurements that encompassing gasses, such as Liquid Petroleum Gas (LPG), Compressed Natural Gas (CNG), Methane, Propane, and others. Various machine learning classifiers, including K-Nearest Neighbors, Decision Tree, Support Vector Machines, XGBoost, and others, are compared with ExAIRFC-GSDC for gas detection. The model demonstrates superior performance, achieving an accuracy rate of 98.67%. Incorporating SHAP and LIME explanations enhances the model's interpretability, providing insights into the contributions of individual sensors. Statistical analysis confirms the significant differences in sensor readings across different gas types. ExAIRFC-GSDC is a robust and explainable solution for accurate gas detection and classification in complex environments.},
  archive      = {J_IJCIS},
  author       = {Lalithadevi, B. and Krishnaveni, S.},
  doi          = {10.1007/s44196-025-00742-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {ExAIRFC-GSDC: An advanced machine learning-based interpretable framework for accurate gas leakage detection and classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BERT-BiGRU-senti-GCN: An advanced NLP framework for analyzing customer sentiments in E-commerce. <em>IJCIS</em>, <em>18</em>(1), 1-18. (<a href='https://doi.org/10.1007/s44196-025-00747-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis plays an important role in understanding employee feedback and improving workplace culture. By leveraging NLP techniques to analyze this feedback accurately, organizations can pinpoint specific areas that need improvement, address employee concerns, and foster a positive work environment. These NLP-driven deep learning models offer valuable tools for E-Commerce HR and sales departments, enabling monitoring employee and users’ sentiment trends over time and assisting in implementing targeted interventions. Focusing on the e-commerce industry, this work utilizes NLP-driven deep learning methodologies to analyze employee and user feedback, aiming to identify sentiments. The proposed NLP-driven, deep learning-based framework is designed to classify user feedback into positive, negative, or neutral sentiments. The key steps in this framework include data collection, NLP-enhanced feature extraction using BERT-BiGRU, and final classification using a Graph Neural Network-based finite-state automata. The effectiveness of this NLP-centric approach was tested on diverse datasets of customer feedback from the e-commerce industry. The results demonstrate the framework’s efficacy, achieving an impressive 93.35% accuracy rate, surpassing existing benchmark methods. The research significantly benefits e-commerce by refining product portfolios and enhancing workplace culture.},
  archive      = {J_IJCIS},
  author       = {Rana, Muhammad Rizwan Rashid and Nawaz, Asif and Rehman, Saif Ur and Abid, Muhammad Ali and Garayevi, Mubariz and Kajanová, Jana},
  doi          = {10.1007/s44196-025-00747-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {BERT-BiGRU-senti-GCN: An advanced NLP framework for analyzing customer sentiments in E-commerce},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of hybrid intrusion detection system leveraging ensemble stacked feature selectors and learning classifiers to mitigate the DoS attacks. <em>IJCIS</em>, <em>18</em>(1), 1-18. (<a href='https://doi.org/10.1007/s44196-025-00750-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denial of service (DoS) attacks occur more frequently with the progressive development of the Internet of things (IoT) and other Internet-based communication technologies. Since these technologies are deeply rooted in the individual’s comfort life, protecting the user’s privacy and security against the growing DoS attack has become a major challenge among researchers. In recent times, intrusion detection systems (IDS) have developed a vital part in ensuring security against these growing attacks. IDS is still unable to attain the optimum categorization performance due to a few bottlenecks. The speed and performance of the existing IDS are challenged by the intricacy of high-dimensional data and the efficacy of the conventional base classifiers. To tackle this aforementioned problem, this research article presents the hybrid IDS based on the combination of stacked feature selection methods such as Random Boruta Selector (RFS), Relief, Pearson coefficient (PCE) and Stacked learning classifiers (SLF). To reduce the dimension of the data features and to select the optimal feature sets, novel integration of RFS, Relief, PCE are deployed. As the final step, stacked classifiers are used for the classification of DoS attacks. All the trials in this framework were accompanied utilizing CICDDoS-2019 datasets and contrasted with the other similar models. The validation boundaries such as accuracy, precision, recall, specificity, and F1-score are used to evaluate the proposed framework. With an F1-score of 96%, accuracy of 96.5%, precision of 96.0%, and recall of 95.8%, the suggested model obtained a CICDDoS-2019 score of 96%. Compared with the other traditional classifiers, the suggested framework has produced the best classification performance in detecting the DoS attacks.},
  archive      = {J_IJCIS},
  author       = {Mamatha, P. and Balaji, S. and Anuraghav, S. Sai},
  doi          = {10.1007/s44196-025-00750-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Development of hybrid intrusion detection system leveraging ensemble stacked feature selectors and learning classifiers to mitigate the DoS attacks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessments of student’s adaptability using convoluted geyser bidirectional long short-term memory in online education. <em>IJCIS</em>, <em>18</em>(1), 1-17. (<a href='https://doi.org/10.1007/s44196-024-00724-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid transition from traditional in-person education to online classrooms has highlighted the need to effectively assess student engagement in virtual learning environments supported by learning management systems. Despite this shift, there remains a significant gap in predictive models that generalize across diverse blended courses, disciplines, and student demographics. Addressing this gap is essential for improving the accuracy and efficiency of predicting student adaptability in online education. To address these issues, this research introduces a novel student adaptability learning model named Convoluted Geyser Bidirectional Long Short-Term Memory (CGBiLSTM) model, designed to predict student adaptability in online entrepreneurship education. This technique has its ability to capture complicated patterns and dependencies in sequential data, which is critical for accurately assessing student adaptability. Furthermore, incorporating the Geyser Optimization Algorithm (GOA) into CGBiLSTM improves the performance by optimizing the learning process and training capabilities, resulting in more accurate and dependable predictions. The CGBiLSTM technique achieves an accuracy of 98.94%, a precision of 99.03%, a recall of 98.71%, and an F1-score of 98.15% proving its efficacy in assessing student adaptability. The CGBiLSTM model, enhanced by the GOA, provides a highly accurate and reliable solution for predicting student adaptability in online education, making it a vital tool for educators in the evolving virtual learning landscape.},
  archive      = {J_IJCIS},
  author       = {Baskar, B. S. Vijaya and Kesavan, Ramesh},
  doi          = {10.1007/s44196-024-00724-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Assessments of student’s adaptability using convoluted geyser bidirectional long short-term memory in online education},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coverage-based variable precision (I, PSO)-fuzzy rough sets with applications to emergency decision-making. <em>IJCIS</em>, <em>18</em>(1), 1-33. (<a href='https://doi.org/10.1007/s44196-024-00728-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the characteristics of imprecise, incomplete and fuzzy data in emergency environment, a novel emergency decision-making method based on coverage-based variable precision (I, PSO)-fuzzy rough set model is proposed. First, an improved (I, PSO)-fuzzy rough set model is proposed, which combines the covering-based fuzzy rough set (CFRS) and the variable precision fuzzy rough set (VPFRS). Second, inspired by the idea of attribute reduction, a novel method for determining attribute weights is introduced to optimize weight assignment in emergency decision-making. Last but not least, to illustrate the feasibility and effectiveness of the proposed method, an example of post-flood rescue force allocation in urban areas is demonstrated. Finally, the stability and superiority of the method are verified through sensitivity analysis and comparative evaluation.},
  archive      = {J_IJCIS},
  author       = {Yin, Ran and Chen, Minge and Wu, Jian and Liu, Yu},
  doi          = {10.1007/s44196-024-00728-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Coverage-based variable precision (I, PSO)-fuzzy rough sets with applications to emergency decision-making},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time athlete fatigue monitoring using fuzzy decision support systems. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00732-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports scientists worry about fatigue, because it affects performance, increases injury risk, and harms health. Traditional fatigue measurements may miss this complicated and ever-changing state, placing athletes at risk of injury while training. This study tests the premise that cutting-edge, real-time monitoring devices improve athlete health and performance. This research aims to reduce tiredness by developing a Fuzzy Decision Support System for Real-Time Athlete Weariness Monitoring. Fuzzy logic can handle unclear performance data, making it a more flexible and advanced alternative to standard methods. Sports athlete fatigue is complicated and dynamic, requiring improved, more precise, and real-time monitoring approaches. The FDSS-RAFM model uses fuzzy logic to account for human performance and physiology. The FDSS-RAFM model assesses athlete fatigue in a comprehensive and context-aware manner. This study’s findings can help coaches, players, and sports scientists improve training programs, reduce injury risk, and improve performance in ever-changing athletic contexts. Fuzzy decision-support systems and other cutting-edge technology can improve athletes’ health and performance, adding to sports science literature. Experimental results show that the proposed FDSS-RAFM model outperforms competing models in sensitivity (97%), specificity (89%), accuracy (96%), and dynamic adaptation error analysis (2.41%).},
  archive      = {J_IJCIS},
  author       = {Li, Aiqin},
  doi          = {10.1007/s44196-025-00732-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Real-time athlete fatigue monitoring using fuzzy decision support systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced heart disease prediction through spatial and temporal feature learning with SCN-deep BiLSTM. <em>IJCIS</em>, <em>18</em>(1), 1-34. (<a href='https://doi.org/10.1007/s44196-025-00734-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart disease prediction using machine learning methods faces various challenges, such as low data quality, missing irrelevant values, and underfit and overfit problems, which increase the time complexity and degrade the model's prediction performance. Moreover, the hybrid models for heart disease prediction showed poor accuracy due to the irrelevancy in the dataset. Therefore, a search optimizer with a deep convolutional neural network coupled with a Deep Bidirectional long short-term memory classifier (SCN-Deep BiLSTM) is proposed to handle the abovementioned issue. The importance of SCN-Deep BiLSTM relies upon establishing the spatial information and temporal features from the ECG signals that support learning while minimizing the computational complexity associated with learning from raw signals.The SCN-Deep BiLSTM model achieves the accuracy, F-score, precision, recall, and critical success index of 0.97, 0.97, 0.98, 0.99, and 0.97, respectively for 80% of model training, whereas the SCN-Deep BiLSTM model attained 0.97, 0.98, 0.96, 0.94, and 0.96 for accuracy, F-score, precision, recall and critical success index, respectively when K-Fold is 10. The performance outcome emphasizes the model’s efficacy and accurate prediction and classification of heart disease.},
  archive      = {J_IJCIS},
  author       = {Pandey, Vivek and Lilhore, Umesh Kumar and Walia, Ranjan},
  doi          = {10.1007/s44196-025-00734-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Advanced heart disease prediction through spatial and temporal feature learning with SCN-deep BiLSTM},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of physical education teaching quality based on hierarchical fuzzy set theory. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00736-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of physical education often faces challenges due to inadequate evaluation methods that fail to provide accurate, real-time teaching evaluation. These challenges influence student performance and overall teaching quality. This article introduces a quality assessment method using fuzzy set theory (QAM-FST) to evaluate physical education teaching. The proposed method extracts and classifies all the available teaching data to compute the students' performance over sessions through fuzzy rough set differentiations over partial and complete derivatives. The complete derivatives identify the factors contributing the maximum to the teaching quality, while the partial derivatives acquire the minimal influence factors. These derivatives are clubbed together through the hierarchical process to identify the precise quality impacting factors and least impacting factors to replace or recommend alternate suggestions. The QAM-FST framework offers a comprehensive, data-driven assessment ensuring the enhancement of PE teaching quality. The QAM-FST outperforms three current models in terms of suggestion accuracy (96.8%), assessment time reduction (22.66%), and total performance evaluation efficiency (16.78%). This data-driven platform guarantees improved physical education instruction quality through actionable insights obtained from real-time feedback.},
  archive      = {J_IJCIS},
  author       = {Tang, Chunlong},
  doi          = {10.1007/s44196-025-00736-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Analysis of physical education teaching quality based on hierarchical fuzzy set theory},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An algorithm for extracting features of basketball players' foul actions based on an attention mechanism. <em>IJCIS</em>, <em>18</em>(1), 1-15. (<a href='https://doi.org/10.1007/s44196-025-00743-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basketball foul action usually involves multiple features, and it is difficult to extract effective features from complex and diverse features. Therefore, a feature extraction algorithm for basketball players' foul action based on attention mechanism is proposed. On the basis of ResNet50 network model, the attention mechanism is integrated to build a basketball player foul action feature extraction model. The machine vision system is used to obtain basketball player action video as the input of the model. The space attention module and time attention module are, respectively, introduced into the video slow frame rate branch and video fast frame rate branch of ResNet50 network. By making the network give greater weight to the key areas of a single frame of video, and improving the network's attention to important video frames, the best spatio-temporal characteristics of foul actions are obtained. After fusion, the feature fusion results are input into the classifier with the cross-entropy loss function as the loss function, and the probability value of each possible foul action and the tag target probability are output, complete basketball player foul action feature extraction. The experimental results show that the algorithm can effectively identify the identification of basketball players, the cumulative matching feature value of foul action recognition can reach more than 95%, and the average accuracy is more than 70%; the F1 value and stability are high, which can reduce the error caused by data fluctuation and noise; the error rate of real-time detection is less than 4.5%, the omission rate is less than 4.7%, the detection time is lower than 14 ms, and the application effect is good.},
  archive      = {J_IJCIS},
  author       = {Wang, Peng},
  doi          = {10.1007/s44196-025-00743-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An algorithm for extracting features of basketball players' foul actions based on an attention mechanism},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E-commerce live-streaming platform and decision support system based on fuzzy association rule mining. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00744-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live streaming of e-commerce platforms attracts consumers for their products/purchases and hence the familiarity is retained high amid different competitors. Fuzzy decision systems are incorporated to filter the streaming content of the platforms to improve consumer augmentations at different promotions. Therefore to support such augmentation and consumer building process, this article proposes a filtered sale streaming model to improve the circulation of new launches and to project the existing products through sustainable promotions. In this process, the comprehensive transition rule for product promotions and sale improvements is defined using fuzzy mining. The fuzzy process introduces the different performance members based on consumer access rate and sale count. The rule modifications are defined using the above factors’ decrease over filtered promotions to boost the augmentation. Using the highest possible member weights over a product, sale, and consumers, the linear improvements between the three factors are estimated over the closure observed at each sale interval. Thus, the streaming modifications and the product exposures are modeled using different mining rules adaptable for e-commerce platforms.},
  archive      = {J_IJCIS},
  author       = {Liao, Hua},
  doi          = {10.1007/s44196-025-00744-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {E-commerce live-streaming platform and decision support system based on fuzzy association rule mining},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal AC power flow with energy storage and renewable energy: An efficient RL algorithm capable of handling equality constraints. <em>IJCIS</em>, <em>18</em>(1), 1-38. (<a href='https://doi.org/10.1007/s44196-025-00745-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using energy storage to solve the multiperiod OPF problem for renewable energy fluctuation is an effective way to increase operation safety and reduce the cost of power systems. However, in solving this OPF problem, model-based methods cannot accurately model uncertain scenarios, while traditional RL methods cannot satisfy the constraints well, and both methods have limitations. Therefore, we propose an RL method, ERL-HC, that does not require scene modelling and can handle general forms of physical constraints. First, a constraint policy network (CPN) is proposed that corrects the output of a neural network on the basis of the inequality generalized reduced gradient (GRG) method; the outputs of this network satisfy all constraints, and it can be trained in an end-to-end manner. Second, the critic network is improved based on the IM method to increase the sample learning efficiency by improving the agent's understanding of state interdependencies. Finally, the adaptive-tuning Lagrange multiplier method is applied in the AC framework to reduce the number of iterations of the inequality GRG in the CPN and efficiently train ERL-HC. ERL-HC was tested on two systems of different sizes. The results show that ERL-HC has a better learning ability than general safe RL algorithms, overcomes the limitations of mainstream safe RL methods in handling equality constraints, and addresses the poor generalization issues of RL methods that can handle equality constraints.},
  archive      = {J_IJCIS},
  author       = {Liu, Mingde and Zhu, Jianquan and Liu, Mingbo},
  doi          = {10.1007/s44196-025-00745-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimal AC power flow with energy storage and renewable energy: An efficient RL algorithm capable of handling equality constraints},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entropy-weighted TOPSIS-based bird strike risk assessment for an airport. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00746-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To identify the factors with higher bird strike risk at a certain airport, and to provide a theoretical basis for the airport to develop targeted and dynamic bird strike prevention strategies, this study has established a bird strike risk assessment index system for the airport based on the “3M1E” theory and relevant content of the “Management Measures for the Prevention and Control of Bird Strikes and Animal Intrusions at Transport Airports”. Then, under the premise of reasonably selecting the parameters of generalized intuitionistic fuzzy entropy, the entropy method is used to simultaneously determine the expert weights and indicator weights. Finally, a weighted TOPSIS model was utilized to assess and rank the high-risk factors contributing to bird strike incidents. Then, taking the bird strike event occurrence at a certain airport as an example for analysis, the second-level indicators are ranked. The results show that the top three indicators are Bird Prevention Funding ( $$D_3$$ ), Habitat Distribution ( $$C_1$$ ), and Dispersal Activities ( $$D_5$$ ). The assessment results provide the necessary basic data for the bird strike prevention work of the airport.},
  archive      = {J_IJCIS},
  author       = {Yu, Changyang and Zhao, Fan and Yin, Yu and Wu, Yi},
  doi          = {10.1007/s44196-025-00746-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Entropy-weighted TOPSIS-based bird strike risk assessment for an airport},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and implementation evaluation of personalized and differentiated teaching strategies for preschool children based on fuzzy decision support systems. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00748-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {School operations have changed due to information technology. Personalized teaching for preschool children requires innovative and adaptable strategies for providing the best-afford experience and learning environment. Conventional teaching approaches may not always accommodate students’ learning styles. This can lower performance and involvement. The expanding variety of pre-schoolers necessitates creative evaluation and optimization methods that promote inclusion and success for all children. The article introduces a preschool teaching evaluation model to improve the students’ learning experience. In particular, the evaluation model is designed for various assessment strategies irrespective of personalization. The proposed model inputs the students’ learning feasibility and the teaching mode to evaluate the personalization adaptability. The fuzzy decision support system improves the evaluation based on the above factors. It evaluates student-specific indicators, including attentiveness, body language, and engagement, to determine instructional flexibility. The model uses real-time classroom observations and student assessments to find realistic approaches with low-to-high fuzzy derivatives. The proposed system is designed to compute the adaptability from both factors under low-to-high fuzzy derivatives. By defining the maximum feasibility range, the teaching strategy is optimized to meet the adaptability. Thus, the low-level strategies are discarded using adaptability measures to reduce personalization failures. The proposed model is verified using adaptability, feasibility, and mode improvements. Research shows that the fuzzy decision support system makes courses more adaptive and feasible in many contexts, particularly those that involve games, audiovisual approaches, and crafts. Preschoolers, parents, and teachers indicated increased enjoyment, fewer customization failures, and greater involvement. Fuzzy-based evaluation increased feasibility ratings and approach versatility. The research provides a valuable foundation for solving classroom customization difficulties in preschool settings, emphasizing data-driven, adaptive teaching techniques. This method enables scalable applications in early childhood education through fuzzy decision-making and real-time evaluations.},
  archive      = {J_IJCIS},
  author       = {Hou, Yali},
  doi          = {10.1007/s44196-025-00748-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Design and implementation evaluation of personalized and differentiated teaching strategies for preschool children based on fuzzy decision support systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-source fusion positioning system based on MDAW-PF algorithm and PDR. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00749-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to multiple occlusions and strong interference in an indoor environment, the traditional single signal source location method is difficult to meet the requirements of high precision and high robustness. Therefore, this paper proposes a multi-source fusion location system based on an adaptive vector particle filter, which combines fingerprint location, pedestrian dead reckoning and map information. The received signal intensity is optimized by offline fingerprint calibration and Kalman filter. The adaptive vector particle filter adopts multi-direction sampling and weight adjustment, which effectively improves the diversity of particles and reduces errors. Compared with the single-source method and other multi-source systems, the positioning accuracy and trajectory fitting degree of the proposed system were significantly improved. The positioning error probability was 97%, the average error was 0.67 m, and the positioning accuracy reached 90.1%. In summary, the proposed multi-source fusion system provides an effective solution for indoor high-precision and reliable positioning.},
  archive      = {J_IJCIS},
  author       = {Wu, Wennan and Xu, Yigang and Li, Zhimin and Lai, Jizhou},
  doi          = {10.1007/s44196-025-00749-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A multi-source fusion positioning system based on MDAW-PF algorithm and PDR},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter adaptive manta ray foraging optimization for global continuous optimization problems and parameter estimation of solar photovoltaic models. <em>IJCIS</em>, <em>18</em>(1), 1-36. (<a href='https://doi.org/10.1007/s44196-025-00753-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The manta ray foraging optimization (MRFO) algorithm suffers from a fixed parameter $$ S $$ , limiting its adaptability in balancing search capability and convergence speed during different optimization stages. To address this limitation, a success-history-based parameter adaptation strategy is proposed to dynamically adjust $$ S $$ . Furthermore, to enhance population diversity and avoid premature convergence, a randomly selected individual from the top $$ G $$ high-quality solutions replaces the current best individual in the somersault foraging behavior. Based on these improvements, a parameter adaptive manta ray foraging optimization (PAMRFO) algorithm is developed. The experimental results demonstrate the effectiveness of PAMRFO. On the IEEE CEC2017 benchmark function set, PAMRFO achieved an average win rate of 82.39% across 29 functions compared to seven state-of-the-art algorithms. On 22 IEEE CEC2011 real-world optimization problems, PAMRFO achieved an average win rate of 55.91% compared to ten advanced algorithms. Sensitivity analysis identified optimal parameter settings, and further stability analysis revealed that PAMRFO exhibits higher success rates and computational efficiency among the four MRFO variants. Population diversity and exploration-exploitation analysis demonstrated the effectiveness of the proposed update mechanism in maintaining diversity and balancing exploration and exploitation. In solving parameter estimation problems for six multimodal solar photovoltaic models, PAMRFO outperformed other competing methods with a 100% success rate, highlighting its superior performance in the photovoltaic field. These findings validate the robustness, efficiency, and wide applicability of PAMRFO, providing advanced solutions for optimization problems in the new energy domain.},
  archive      = {J_IJCIS},
  author       = {Tang, Zhentao and Wang, Kaiyu and Yao, Yongxuan and Zhu, Mingxin and Zhuang, Lan and Chen, Huiqin and Li, Jing and Yan, Li and Gao, Shangce},
  doi          = {10.1007/s44196-025-00753-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Parameter adaptive manta ray foraging optimization for global continuous optimization problems and parameter estimation of solar photovoltaic models},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A secure authentication and key exchange protocol for vehicles to infrastructure network. <em>IJCIS</em>, <em>18</em>(1), 1-16. (<a href='https://doi.org/10.1007/s44196-025-00754-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles technology has been widely applied in various communication scenarios, including vehicles to vehicles, vehicles to roadside facilities, vehicles to pedestrians, and vehicles to cloud. Owing to transmitting data through public channels, various security issues like identity leakage, man in the middle attack, key leakage and etc., are also introduced simultaneously and still challenging to be solved. Researches and practices have shown that authentication and key exchange protocols are effective methods to solve such security issues. However, most existing security protocols for Internet of Vehicles are established on the premise that the registration process is with a secure channel, which is usually not satisfied and deviates from practical applications. Accordingly, an authentication and key exchange protocol with an insecure channel has been proposed, in which the operations of symmetric encryption and XOR encryption are adopted for all interactive processes to improve protocol security. The theoretical analysis and formal verification demonstrate that the proposed protocol satisfies security properties including authentication and confidentiality, and reduces the costs of computation and communication compared with the method with public key encryption.},
  archive      = {J_IJCIS},
  author       = {Xu, Peng and Wang, Xiuzhen and Chen, Meirong},
  doi          = {10.1007/s44196-025-00754-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A secure authentication and key exchange protocol for vehicles to infrastructure network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Publisher correction: A two-way crossed effects fuzzy panel linear regression model. <em>IJCIS</em>, <em>18</em>(1), 1. (<a href='https://doi.org/10.1007/s44196-025-00756-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Hesamian, Gholamreza and Johannssen, Arne},
  doi          = {10.1007/s44196-025-00756-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Publisher correction: A two-way crossed effects fuzzy panel linear regression model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: ExAIRFC-GSDC: An advanced machine learning-based interpretable framework for accurate gas leakage detection and classification. <em>IJCIS</em>, <em>18</em>(1), 1-2. (<a href='https://doi.org/10.1007/s44196-025-00758-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Lalithadevi, B. and Krishnaveni, S.},
  doi          = {10.1007/s44196-025-00758-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Correction: ExAIRFC-GSDC: An advanced machine learning-based interpretable framework for accurate gas leakage detection and classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing solid oxide fuel cell efficiency through advanced model identification using differential evolutionary mutation fennec fox algorithm. <em>IJCIS</em>, <em>18</em>(1), 1-31. (<a href='https://doi.org/10.1007/s44196-025-00759-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuel cells (FCs) are increasingly attracting attention for their efficient conversion of chemical energy into electricity without the need for combustion. Their high efficiency and versatility make them a promising technology across various applications. Researchers are actively exploring ways to optimize FC systems to meet specific energy needs. Among the different types of fuel cells, solid oxide fuel cells (SOFCs) stand out as a promising clean energy technology that generates electricity through electrochemical reactions. However, accurately modeling SOFCs, which is essential for reducing design costs, presents a challenge due to their complex and nonlinear characteristics. An ideal model should be adaptable to varying operating pressures and temperatures. This research introduces a novel approach for optimal SOFC model identification using a differential evolutionary mutation Fennec fox algorithm (DEMFFA). A real-world case study demonstrates the superior effectiveness of DEMFFA compared to existing methods. Additionally, a sensitivity analysis evaluates the influence of temperature and pressure on the model, with results indicating that the proposed method achieves higher efficiency than other approaches. The sum of the square error of the proposed algorithm is 1.18E-11 followed by the parent algorithm, Fennec fox algorithm (FFA) (1.24E-09), and some of the compared algorithms. The computational time of the proposed algorithm is 1.001 s, followed by the parent algorithm FFA (1.199 s) and some of the compared algorithms. DEMFFA offers significant potential, enhancing renewable energy, minimizing SOFC's environmental impact, and improving real-world applications like distributed power generation and hydrogen integration.},
  archive      = {J_IJCIS},
  author       = {Singla, Manish Kumar and Gupta, Jyoti and Kumar, Ramesh and Jangir, Pradeep and Louzazni, Mohamed and Giri, Nimay Chandra and Al-Gburi, Ahmed Jamal Abdullah and EI-Kenawy, E. I.-Sayed M. and Alharbi, Amal H.},
  doi          = {10.1007/s44196-025-00759-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhancing solid oxide fuel cell efficiency through advanced model identification using differential evolutionary mutation fennec fox algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid LECNN architecture: A computer-assisted early diagnosis system for lung cancer using CT images. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00761-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is one of the most common causes of cancer-related death. Therefore, early diagnosis of this cancer is crucial for planning patient treatment. This paper proposes a hybrid Lung Ensemble Convolutional Neural Network (LECNN) architecture for the computer-aided early diagnosis of lung cancer via CT images. The proposed hybrid approach integrates a transfer learning (TL) mechanism with ensemble learning (EL) on the basis of majority voting. Initially, CNN architectures (GoogLeNet, EfficientNet, DarkNet19, and ResNet18) are trained via TL, and the resulting CNN models are used as inputs in EL. The outputs from all the CNN architectures are evaluated via majority voting to identify the top-performing triple CNN combination, which is then utilized in the hybrid approach. The performance of the proposed method was assessed via the widely used IQ-OTH/NCCD dataset. Additionally, the impact of the elastic transformation method, a data augmentation technique, on performance improvement was investigated in the proposed method. The triple combination of the GoogLeNet, EfficientNet, and DarkNet19 CNN architectures, as part of the EL method in the hybrid approach, achieved superior performance on both the raw and augmented datasets according to the obtained performance results. The performance evaluations revealed that the proposed approach achieved more than a 5% improvement with the augmented dataset compared with the raw IQ-OTH/NCCD dataset, resulting in the highest performance. The proposed hybrid approach achieved 99% accuracy, 98.82% sensitivity, 99.48% specificity, 99.06% precision, and 98.94% F1 score on the augmented IQ-OTH/NCCD dataset. When compared with findings from previous studies using the same dataset, the proposed hybrid approach outperformed state-of-the-art methods. In conclusion, it demonstrates significant potential as a robust tool for computer-aided early lung cancer diagnosis systems and may also contribute to the development of future hybrid approaches in this field.},
  archive      = {J_IJCIS},
  author       = {Güraksın, Gür Emre and Kayadibi, Ismail},
  doi          = {10.1007/s44196-025-00761-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A hybrid LECNN architecture: A computer-assisted early diagnosis system for lung cancer using CT images},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued intuitionistic fuzzy multi-attribute decision-making based on entropy and bidirectional projection. <em>IJCIS</em>, <em>18</em>(1), 1-13. (<a href='https://doi.org/10.1007/s44196-025-00763-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel arctangent-based interval-valued intuitionistic fuzzy entropy function designed to address multi-attribute group decision-making problems, particularly in situations where attribute weights are either completely unknown or only partially known. The proposed entropy function computes the objective weights of the attributes and, by integrating these with subjective weights, derives a comprehensive weight. This methodology effectively addresses uncertainties and the incomplete nature of weight information in decision-making processes. Furthermore, the paper extends the bidirectional projection method to the interval intuitionistic fuzzy context, developing a multi-attribute decision-making model that integrates the arctangent-based interval-valued intuitionistic fuzzy entropy and the bidirectional projection method. Finally, a series of comparative experiments are conducted to validate the effectiveness and robustness of the proposed entropy function and bidirectional projection method in multi-attribute decision-making.},
  archive      = {J_IJCIS},
  author       = {Zheng, Jian and Dong, Minggao},
  doi          = {10.1007/s44196-025-00763-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Interval-valued intuitionistic fuzzy multi-attribute decision-making based on entropy and bidirectional projection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of communication transmission frequency linear algebraic model under aerial computing architecture. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00764-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of the demand for wireless communication systems, when the existing algebraic model processes real-time data, the frequency adjustment is inflexible, it is difficult to quickly optimize the transmission parameters to cope with network load changes, and the long-term operation fails to effectively control the CPU frequency, resulting in increased energy consumption. To better promote the development of wireless communication systems, this article aimed to use aerial computing architecture to optimize the linear algebraic model of communication transmission frequency, to better meet the needs of today's wireless communication systems. The article first designed a communication frequency stabilization device structure to ensure high stability of the output spectrum. Then it introduced a dynamic frequency adjustment module to achieve real-time adjustment of communication transmission frequency. It then improved the data transmission rate through the design of the aerial computing perception module. This article used a frequency optimization algorithm based on linear algebra to adjust its linear relationship, optimize transmission energy consumption, and improve transmission efficiency. Finally, to verify the application effect of aerial computing architecture in optimizing the linear algebraic model of communication transmission frequency, this paper compared it with traditional dynamic adjustment models and parallel computational models. The research results showed that for packet 13, the round-trip time required to transmit the model in this article was 1.21 ms; the response time was 0.009 ms, and the total energy consumption was 89.6-W hours. The traditional dynamic adjustment model required a round-trip time of 4.92 ms, a response time of 0.093 ms, and a total energy consumption of 119.1-W hours for packet 13 transmissions. The parallel computational model required a round-trip time of 6.33 ms, a response time of 0.063 ms, and a total energy consumption of 131.4-W hours for packet 13 transmissions. The results showed that the optimized communication transmission frequency linear algebraic model using aerial computing architecture had shorter communication delay and response time, lower energy consumption, and better frequency control performance. This article highlighted the important impact of aerial computing architecture on the stability, real-time performance, and transmission rate of linear algebraic models of communication transmission frequencies, providing more ideas for the design and planning of wireless communication systems.},
  archive      = {J_IJCIS},
  author       = {Gao, Yufeng},
  doi          = {10.1007/s44196-025-00764-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimization of communication transmission frequency linear algebraic model under aerial computing architecture},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DenseNet-ABiLSTM: Revolutionizing multiclass arrhythmia detection and classification using hybrid deep learning approach leveraging PPG signals. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00765-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arrhythmias (AM) are heart conditions that can lead to fatal cardiac arrest. Automated identification of arrhythmias is crucial for detecting cardiac diseases. Previous studies have used photoplethysmography (PPG) signals to identify arrhythmias, but there is limited research on their application for multiclass arrhythmia classification. This study introduces a Hybrid Deep Learning (HDL) model called DenseNet-ABiLSTM, which uses densely connected convolutional networks and Attention-based Bidirectional Long Short-Term Memory (ABiLSTM) to categorize various types of arrhythmias. The model uses 1D convolutional kernels to acquire multiscale conceptual features, followed by BiLSTM to understand temporal relationships among features. The Attention Mechanism layer is presented to improve detection performance. The model categorizes arrhythmia rhythms into six types: Sinus Rhythm (SR), Early Ventricular Contraction (EVC), Early Atrial Contraction (EAC), Ventricular Tachycardia (VT), Supraventricular Tachycardia (ST), and AF. Various metrics were assessed and compared with Electrocardiogram (ECG) results to determine AM rhythms. The mean performance measures showed strong overall performance, with a mean F1 score and accuracy of 87.74% and 89.14%, respectively.},
  archive      = {J_IJCIS},
  author       = {Saranya, K. and Karthikeyan, U. and Kumar, A. Saran and Salau, Ayodeji Olalekan and Tin Tin, Ting},
  doi          = {10.1007/s44196-025-00765-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {DenseNet-ABiLSTM: Revolutionizing multiclass arrhythmia detection and classification using hybrid deep learning approach leveraging PPG signals},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrusion detection system for network security using novel adaptive recurrent neural network-based fox optimizer concept. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00767-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of daily networks and communications rely heavily on network security. Researchers in cybersecurity emphasize the necessity of developing effective intrusion detection systems (IDS) to safeguard networks. The importance of efficient IDS escalates as attackers devise new types of attacks and network volumes expand. Furthermore, IDS aims to ensure the integrity, confidentiality, and availability of data transmitted across networked systems by preventing unauthorized access. Following numerous studies utilizing machine learning (ML) to develop effective IDS, the focus has shifted towards deep learning (DL) techniques as artificial neural networks (ANNs) and DL systems have become prevalent. ANNs are capable of generating features autonomously, eliminating the need for manual intervention. This paper introduces an innovative adaptive recurrent neural network-based fox optimizer (ARNN-FOX) method. The primary objective of the ARNN-FOX system is to efficiently detect and classify network intrusions, thereby enhancing network security. Data normalization is conducted to scale the incoming data into a usable format. The gray level co-occurrence matrix (GLCM) method is proposed for selecting the optimal subset of features for the ARNN-FOX method. In the proposed approach, the fox algorithm (FOX) is utilized for the adjustment of hyperparameters in the ARNN model. The efficacy of the ARNN-FOX approach is assessed using benchmark datasets. Based on comparative results, the ARNN-FOX method demonstrates superior performance in parameters such as accuracy, specificity, sensitivity, F1 Score, recall value, and precision values over existing models. The proposed ARNN-FOX-based IDS model for the network security in terms of accuracy is 15.12%, 8.79%, 6.45%, and 4.21% better than RNN, CNN-LSTM, DASO-RNN, and ChCSO-LSTM, respectively. Similarly, with respect to specificity, the suggested ARNN-FOX-based IDS model for network security outperforms RNN, CNN-LSTM, DASO-RNN, and ChCSO-LSTM by 32.43%, 8.89%, 3.16%, and 2.08%, respectively.},
  archive      = {J_IJCIS},
  author       = {Manivannan, R. and Senthilkumar, S.},
  doi          = {10.1007/s44196-025-00767-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Intrusion detection system for network security using novel adaptive recurrent neural network-based fox optimizer concept},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-perspective learning based on transformer for stock price trend. <em>IJCIS</em>, <em>18</em>(1), 1-18. (<a href='https://doi.org/10.1007/s44196-025-00768-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock constitutes a crucial element of the financial market, and accurately forecasting stock trends remains a significant and unresolved issue. Nonetheless, the stock’s considerable complexity renders accurate prediction of stock trends more challenging. This paper proposes a novel multi-perspective approach that converts the time series prediction challenge into an image classification problem, referred to as the Multi-perspective Denoise Transformer (MPDTransformer). We initially multi-factor features into two-dimensional images employing a multi-perspective approach to more comprehensively explain the actual market conditions and enhance the model’s practicality and adaptability; secondly, we utilize a Convolutional Autoencoder (CAE) to extract features, which effectively eliminates noise and enhances data purity; finally, to comprehensively capture the temporal relationships within the data and gain a deeper understanding of the overall time series, we employ a Transformer for prediction. Experimental results demonstrate that our method outperforms other prevalent stock trend prediction techniques.},
  archive      = {J_IJCIS},
  author       = {Li, Xiliang and Chen, Shuoru and Qiao, Xiaoyan and Zhang, Mingli and Zhang, Caiming and Zhao, Feng},
  doi          = {10.1007/s44196-025-00768-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Multi-perspective learning based on transformer for stock price trend},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid-CID: Securing IoT with mongoose optimization. <em>IJCIS</em>, <em>18</em>(1), 1-18. (<a href='https://doi.org/10.1007/s44196-025-00751-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) technology has evolved beyond personal devices to power global deployments across a wide range of networks which has a significant impact on global commerce. However, security challenges arise due to the wide range of protocols and computational capabilities in IoT devices. To combat these issues, a novel hybrid optimization-enabled neural network for classification of intrusion data against IoT system (Hybrid-CID) is proposed particularly to identify intrusions in resource-constrained IoT devices. Initially, the incoming data are standardized by removing the irrelevant information through preprocessing to ensure the performance of detection models. After preprocessing, the Hybrid-CID framework develops a hybrid optimization algorithm to identify the intrusions from the traffic data which ensures data privacy by maintaining the reliability and integrity of IoT deployments. Finally, the combined deep learning (DL) network classifies the identified intrusions to contribute proactive threat mitigation by ensuring the confidentiality of IoT system data. The Hybrid-CID system is validated through the benchmark CSE-CIC-IDS 2018 and CICIDS 2017 datasets using accuracy, specificity, precision, F1-score, recall, execution time, communication cost, detection rate, detection time, and computational cost. The Hybrid-CID framework achieves an overall accuracy of 97.82%, whereas the WDLSTM, TLBO-IDS, and DIS-IoT techniques achieve 87.42%, 89.58%, and 94.72%, respectively, for efficiently detecting intrusions in IoT networks.},
  archive      = {J_IJCIS},
  author       = {Sheeba, S. Merlin and Shaji, R. S.},
  doi          = {10.1007/s44196-025-00751-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hybrid-CID: Securing IoT with mongoose optimization},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lattice-based decision models for green urban development: Insights from $$L_{q}*$$ q-rung orthopair multi-fuzzy soft set. <em>IJCIS</em>, <em>18</em>(1), 1-27. (<a href='https://doi.org/10.1007/s44196-025-00755-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location selection is a critical process in decision-making for projects that involve multiple criteria, such as urban planning, industrial site development, or green building projects. Multiple criteria decision making (MCDM) is a systematic approach that evaluates and ranks potential alternatives based on a set of often conflicting criteria. This study focuses on selecting the optimal urban location for a green building project by employing the $$L_{q}*$$ q-rung orthopair multi-fuzzy soft-MCDM( $$L_{q}*$$ q-ROMFS) techniques. The $$L_{q}*$$ q-ROMFS set combines elements from two distinct theories with lattice ordering parameters: q-rung orthopair fuzzy set and multi-fuzzy soft set. It provides a mathematical framework with multiple parameters that effectively represents problems involving multi-dimensional data within a dataset. We expand this concept by establishing the algebraic structures of $$L_{q}*$$ q-ROMFS sets, including properties like modularity and distributivity, while also analyzing their homomorphism under lattice mappings. Finally, leveraging the $$L_{q}*$$ q-ROMFS matrix, we propose both a choice matrix and a weighted choice matrix to effectively address the selection of the optimal urban location for a green building project.},
  archive      = {J_IJCIS},
  author       = {Jayakumar, Vimala and Pethaperumal, Mahalakshmi and Kausar, Nasreen and Pamucar, Dragan and Simic, Vladimir and Salman, Mohammed Abdullah},
  doi          = {10.1007/s44196-025-00755-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Lattice-based decision models for green urban development: Insights from $$L_{q}*$$ q-rung orthopair multi-fuzzy soft set},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced model for gestational diabetes mellitus prediction using a fusion technique of multiple algorithms with explainability. <em>IJCIS</em>, <em>18</em>(1), 1-33. (<a href='https://doi.org/10.1007/s44196-025-00760-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High glucose levels during pregnancy cause Gestational Diabetes Mellitus (GDM). The risks include cesarean deliveries, long-term type 2 diabetes, fetal macrosomia, and infant respiratory distress syndrome. These risks highlight the need for accurate GDM prediction. This research proposes a novel fusion model for early GDM prediction. It uses conventional Machine Learning (ML) and advanced Deep Learning (DL) algorithms. Subsequently, it combines the strengths of both ML and DL algorithms using various ensemble techniques. It incorporates a meta-classifier that further reinforces its robust prediction performance. The dataset is split into training and testing sets in a 70/30 ratio. The initial steps involve exploratory analysis and data preprocessing techniques such as iterative imputation and feature engineering. Subsequently, oversampling is applied to the training set to address class imbalance which ensures the model learns effectively. The testing set remains imbalanced to maintain the credibility of the model’s performance evaluation. The fusion model achieves an accuracy of 98.21%, precision of 97.72%, specificity of 98.64%, recall of 97.47%, F1 score of 97.59%, and an Accuracy Under the Curve (AUC) of 99.91%. The model exhibits efficiency with an average processing time of 0.06 s to predict GDM. These results outperform the previous studies using the same GDM prediction dataset and demonstrate the model's superior performance. Additionally, Explainable Artificial Intelligence (XAI) techniques are utilized to interpret the model’s decisions. They highlight the most influential features in GDM prediction and ensures transparency. The proposed fusion model can facilitate proactive GDM prediction to elevate GDM management and maternal–fetal health outcomes.},
  archive      = {J_IJCIS},
  author       = {Hassan, Ahmad and Ahmad, Saima Gulzar and Iqbal, Tassawar and Munir, Ehsan Ullah and Ayyub, Kashif and Ramzan, Naeem},
  doi          = {10.1007/s44196-025-00760-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhanced model for gestational diabetes mellitus prediction using a fusion technique of multiple algorithms with explainability},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of distributionally robust optimization markov decision-making under uncertainty in scheduling of multi-category emergency medical materials. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00762-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the preliminary stages of public health emergencies, many regional public health systems do not have enough medical resources to address the needs that arise from the emergencies. Additionally, due to the rapid development of emergency situations, it is difficult to accurately understand all medical material demands. Thus, we develop a multi-category emergency medical materials robust scheduling method for multi-period, which continuously schedules the availability of multi-category emergency medical materials during times of uncertain demand. First, we developed a single-period Distributionally Robust Optimization (DRO) model to provide a powerful strategy for scheduling multi-category emergency medical materials with little information on material demand. In the DRO model, we prioritize medical material requirements into different categories, assuming only that the means and variances of information on the demand are available to seek an optimal implementation strategy in a single period. We then combine the DRO scheduling model with the Markov Decision Process (MDP) and extend it to the multi-period Distributionally Robust Optimization Markov Decision Process scheduling model (DRO-MDP). Our DRO-MDP model provides encouraging guidelines to solve the multi-period scheduling problem of multi-category emergency medical materials in uncertain situations. A simulated experiment is used to demonstrate the effectiveness of the proposed model. The simulation uses COVID-19 data from New Delhi, India in the spring of 2021. It is important to note that the model we propose can be easily generalized as a framework for any multi-category resource allocation problem with uncertain needs.},
  archive      = {J_IJCIS},
  author       = {Liang, Zhizhen and Wang, Xiaojia and Xu, Sheng and Chen, Wei},
  doi          = {10.1007/s44196-025-00762-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Application of distributionally robust optimization markov decision-making under uncertainty in scheduling of multi-category emergency medical materials},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SED-NET: Real-time suspicious event detection via deep learning-based di-stream neural network. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00766-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suspicious event detection (SED) identifies anomalous activities in surveillance data using computer vision and machine learning techniques. However, existing approaches have high false positive rates difficulty distinguishing suspicious from normal behaviors, and limited adaptability to dynamic environments. This research introduces a novel deep learning-based SED-NET model for detecting suspicious events in public places. Initially, input images are collected from two data for detecting Suspicious events. Surveillance camera videos are converted into frames and the suspicious images are pre-processed using a Gaussian adaptive bilateral filter (GABF) to reduce noise while preserving edges. A Sobel edge detector is used to detect the fine edges in the pre-processed frames for enhancing the structural details. Di-Stream Neural Network (DSNN) is introduced with the dual-branch EfficientNet-based feature extractor that retrieves both motion and pose features. The weighted Average Fusion method is used to combine pose and motion features to classify suspicious activities using the Simplified Spiking Neural Network (SSNN). The effectiveness of the proposed SED-NET method was evaluated using specificity, accuracy, sensitivity, and F1 score. The proposed SED-NET model attains an accuracy of 98.97% for UCSD Pedestrian and 98.84% for CUHK Avenue datasets. Moreover, the proposed SED-NET improved the overall accuracy by 7.44%, 4.18%, and 2.73% better than DenseNet121, SegAD, and 3DCNN, respectively.},
  archive      = {J_IJCIS},
  author       = {Siva Senthil, D. and Sivarani, T. S.},
  doi          = {10.1007/s44196-025-00766-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {SED-NET: Real-time suspicious event detection via deep learning-based di-stream neural network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hesitant fuzzy $$\beta $$ -covering $$({\mathcal {I}},$$ $${\mathcal {O}})$$ rough set models and applications to multi-attribute decision-making. <em>IJCIS</em>, <em>18</em>(1), 1-29. (<a href='https://doi.org/10.1007/s44196-025-00769-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hesitant fuzzy $$\beta $$ -covering rough set offers stronger representational capabilities than earlier hesitant fuzzy rough sets. Its flexibility makes it more suitable for hesitant fuzzy multi-attribute decision-making (MADM). As a result, it has become a popular research focus in decision analysis and has drawn significant attention from scholars. However, the existing hesitant fuzzy $$\beta $$ -covering rough set based on t-norms cannot handle the overlap and correlation between hesitant information well. Addressing this problem, we propose the hesitant fuzzy overlap function and hesitant fuzzy $$\beta $$ -covering $$({\mathcal {I}},$$ $${\mathcal {O}})$$ rough set (HF $$\beta $$ CIORS) models based on the hesitant fuzzy overlap function. First, we establish the definition of the hesitant overlap function and representable hesitant fuzzy overlap function on a partial order relation. Based on proposed definitions, we provide examples of representable and unrepresentable hesitant fuzzy overlap functions and offer a detailed proof to explain the unrepresentable function. Second, we construct four types of HF $$\beta $$ CIORS models and prove some of its important properties. Thirdly, we integrate the HF $$\beta $$ CIORS models with the TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) method and apply them to solve MADM problems. The validity of the proposed method is demonstrated through a practical application, and its stability and effectiveness are confirmed via sensitivity and comparative analyses. Based on these validations, our method proves effective in addressing MADM problems, offering reliable decision-making support.},
  archive      = {J_IJCIS},
  author       = {Wang, Jingyi and Shao, Songtao and Mao, Xiaoyan and Zhang, Xiaohong},
  doi          = {10.1007/s44196-025-00769-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hesitant fuzzy $$\beta $$ -covering $$({\mathcal {I}},$$ $${\mathcal {O}})$$ rough set models and applications to multi-attribute decision-making},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced hybrid machine learning model for accurate detection of cardiovascular disease. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00771-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease (CVD) is one of the foremost reasons behind the death of people worldwide. Prevention and early diagnosis are the only ways to control its progression and onset. Thus, there is an urgent need for a detection model comprising intelligent technologies, including Machine Learning (ML) and deep learning, to predict the future state of an individual suffering from cardiovascular disease by effectively analyzing patient data. This study aims to propose a hybrid model that provides a deep insight into the data under consideration to enhance model accuracy for effectively detecting cardiovascular disease. This current research proposes a hybrid model comprising four stages. In the first stage of the proposed hybrid model, the data imbalance problem is solved using a hybrid sampling technique named Synthetic Minority Oversampling Technique-Edited Nearest Neighbors Rule. In the second stage, the Chi-square is applied as a feature selection method to select the highly relevant features from the records of 1190 with 11 clinical features, curated by combining the 5 most popular datasets, including Long Beach VA, Hungarian, Switzerland, and Statlog (Heart). In the third stage, the preprocessed dataset is passed to a stacking ensemble model comprising three base learners: Random Forest Tree (RFT), K-Nearest Neighbor (K-NN), and AdaBoost classifier and one meta-learner: Logistic Regression (LR), optimized with Grid Search Cross-Validation (GSCV) optimization approach, whose performance is evaluated against individual classifier. In the fourth stage, the performance is evaluated in terms of accuracy, sensitivity, specificity, F1 score, and ROC_AUC score.. The comparative results prove that the proposed hybrid model scored the highest accuracy of 97.8%, 96.15% sensitivity, and 96.75% specificity and 98.6% ROC_AUC score when compared with the existing techniques and models after applying the SMOTE–ENN (for data balancing) and Chi-square (for feature selection) methods for the efficient detection of cardiovascular disease. The implementation results demonstrate that the suggested hybrid model may accurately identify cardiovascular disease among patients. It facilitates the application of robust clinical treatment strategies.},
  archive      = {J_IJCIS},
  author       = {Navita and Mittal, Pooja and Sharma, Yogesh Kumar and Lilhore, Umesh Kumar and Simaiya, Sarita and Saleem, Kashif and Ghith, Ehab Seif},
  doi          = {10.1007/s44196-025-00771-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Advanced hybrid machine learning model for accurate detection of cardiovascular disease},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted hybrid random forest model for significant feature prediction in alzheimer’s disease stages. <em>IJCIS</em>, <em>18</em>(1), 1-30. (<a href='https://doi.org/10.1007/s44196-025-00780-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent studies, several machine learning and deep learning prediction models have been proposed for the early detection and classification of various stages of Alzheimer’s Disease (AD). Many years before the actual onset of AD, there occur several structural changes in the brain. These structural brain features can be utilized in learning the disease progression from an early stage of the disease. The various stages of pathology cause mild cognitive impairment (MCI) from normal cognition and AD from normal cognition. This work intends to develop a weighted and hybrid random forest learning model that utilizes a relevant subset of predictors to diagnose the progression of the disease. The conversion from normal cognition to MCI is identified at an early stage of the onset of structural brain changes. The importance of proposed research works lies in more early identification of significant feature that increase disease progression and appropriate interventions greatly improve subjects’ recovery. The Alzheimer’s Disease Neuro Imaging Initiative (ADNI) cross-sectional MRI data were analyzed in this study that utilized brain curvature, grey matter density, white matter density, the volume of cortical and sub-cortical structures, shape of hippocampus, hippocampal subfield volume, Mini-mental state exam (MMSE), Clinical Dementia Rating (CDR), Estimated Total Intracranial Volume, Normalize Whole Brain Volume, and Atlas Scaling Factor for constructing randomized trees and thus predicting the features that cause the progression of disease stages from MCI to Alzheimer’s disease that causes dementia. Based on previous studies, there is a significant shortfall in understanding Alzheimer’s disease progression from pre-MCI stages and the classification of progressive and stable MCI groups. As a consequence of this challenge discussed, whether all the mild cognitively impaired people change to AD cohorts or remain in normal cognition and identification of the structural and functional features remains underexplored. Thus, the proposed Weighted Hybrid Random Forest algorithm (WHBM) utilized the 63 features that comprise the whole brain volume. The most significant and weighted features are derived which segregate 39% of subjects with cognitively progressive MCI and 51% of subjects with normal age-related cognitive decline. This implementation model proved to give robust AD conversion probability and identify significant features with 93% accuracy and 88% sensitivity that are sufficient for future clinical inferences. The optimized model thus resulted in the prediction of disease conversion probability from Mild Cognitive Impairment to AD because of significant structural features that are key-requisite for affected geriatric cohorts.},
  archive      = {J_IJCIS},
  author       = {Rohini, M. and Surendran, D.},
  doi          = {10.1007/s44196-025-00780-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Weighted hybrid random forest model for significant feature prediction in alzheimer’s disease stages},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid android malware detection and classification using deep neural networks. <em>IJCIS</em>, <em>18</em>(1), 1-26. (<a href='https://doi.org/10.1007/s44196-025-00783-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a deep learning-based framework for Android malware detection that addresses critical limitations in existing methods, particularly in handling obfuscation and scalability under rapid mobile app development cycles. Unlike prior approaches, the proposed system integrates a multi-dimensional analysis of Android permissions, intents, and API calls, enabling robust feature extraction even under reverse engineering constraints. Experimental results demonstrate state-of-the-art performance, achieving 98.2% accuracy (a 7.5% improvement over DeepAMD) on a cross-dataset evaluation spanning 15 malware families and 45,000 apps. The framework’s novel architecture enhances explainability by mapping detection outcomes to specific behavioral patterns while rigorous benchmarking across five public datasets (including Drebin, AndroZoo, and VirusShare) mitigates dataset bias and validates generalization. By outperforming existing techniques in accuracy, adaptability, and interpretability, this work advances the practicality of deep learning for real-world Android malware defense in evolving threat landscapes.},
  archive      = {J_IJCIS},
  author       = {Rashid, Muhammad Umar and Qureshi, Shahnawaz and Abid, Abdullah and Alqahtany, Saad Said and Alqazzaz, Ali and ul Hassan, Mahmood and Al Reshan, Mana Saleh and Shaikh, Asadullah},
  doi          = {10.1007/s44196-025-00783-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hybrid android malware detection and classification using deep neural networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Multicriteria decision-making under conditions of uncertainty: A fuzzy set perspective. john wiley & sons. ISBN: 978–1-119–53,492-1.. <em>IJCIS</em>, <em>18</em>(1), 1-5. (<a href='https://doi.org/10.1007/s44196-025-00784-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This overview is focused on the book reflecting research results on the fundamentals of the theory of multicriteria (multiobjective and multiattribute) decision-making under conditions of uncertainty. The facet of uncertainty is formalized based on a possibilistic (not probabilistic) approach. These results are based on the fuzzy set theory and its fusion with other branches of mathematics of uncertainty. The overview identifies the crucial arguments behind the ultimate need for this theory, reflects the book’s primary objectives, identifies the key possibilities delivered by the presented book's results, and elaborates on real-world problems solved by applying the findings reported in the book. The thorough critical analysis summarizes the advantages and limitations of the main results covered by the book.},
  archive      = {J_IJCIS},
  author       = {Ekel, Petr Iakovlevitch and Libório, Matheus Pereira and Pedrycz, Witold},
  doi          = {10.1007/s44196-025-00784-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-5},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Book review: Multicriteria decision-making under conditions of uncertainty: A fuzzy set perspective. john wiley & sons. ISBN: 978–1-119–53,492-1.},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MicrobeNet: An automated approach for microbe organisms prediction using feature fusion and weighted CNN model. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00777-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microbial organisms are everywhere, millions residing within the human body and also cover 60% of the living earth. These microbes can pose significant health risks, causing diseases such as malaria and toxoplasmosis. Toxoplasmosis is notably prevalent, with seroprevalence rates ranging from 3.6 to 84% in an African region, underscoring the necessity for automated microorganism detection techniques. This research work aims to predict the presence of microorganisms in the human body. We propose a novel approach that combines and integrates principal-component analysis, Chi-square, and analysis-of-variance features using a weighted convolutional-neural-network model called MicrobeNet. The results highlight the efficacy of the proposed method, achieving a remarkable 99.97% in accuracy, recall, precision, and F1-score. The experiments use multiple deep and machine learning models to detect ten distinct microbial forms. The results of the proposed model are compared with those of previously published research. Additionally, k-fold cross validation confirms the robustness of these findings. This research significantly advances the field of microbiology by providing a highly accurate method for microorganism identification, facilitating early disease detection and prevention.},
  archive      = {J_IJCIS},
  author       = {Alnowaiser, Khaled},
  doi          = {10.1007/s44196-025-00777-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {MicrobeNet: An automated approach for microbe organisms prediction using feature fusion and weighted CNN model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging multi-source data for local government financing vehicles debt risk assessment via random forests. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00778-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local government financing vehicles (LGFVs), in China, are pivotal indirect financing channels for urban development projects. However, the significant debt accumulated by these vehicles presents considerable long-term risks, including challenges to fiscal sustainability, threats to financial system stability, and disruptions to regional economic development. Timely monitoring of LGFV debt risks is essential for enabling effective interventions. This study develops a robust risk evaluation system for LGFVs by leveraging multi-source data and employing the Random Forest (RF) machine learning algorithm. We collected and analyzed a sample of 1584 Chinese LGFVs from a major state-owned bank. Through an examination of the mechanisms underlying LGFV debt risk and a review of relevant literature, we identified seven primary categories and 20 key risk indicators to construct our risk indicator system. After comparing several machine learning algorithms, we selected the RF algorithm to build the LGFV debt risk prediction model due to its superior performance. Our findings emphasize the External Guarantee Ratio, GDP growth rate, and proportion of the tertiary industry as critical risk indicators. The model evaluation demonstrates high accuracy, underscoring its significant potential for practical application. This study contributes to the management of local government debt risks and introduces a novel methodology with potential applicability in other areas of risk management.},
  archive      = {J_IJCIS},
  author       = {Li, Kejia and Chen, Zhen-Song},
  doi          = {10.1007/s44196-025-00778-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Leveraging multi-source data for local government financing vehicles debt risk assessment via random forests},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy decision support system for medical service quality management in hospitals. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00773-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical service quality in hospitals and clinical centres is expected to satisfy the patient’s satisfaction and the high precision diagnosis. Periodic assessment of the medical services increases the quality and the staff efficiency. Such quality assessments are analyzed using sophisticated computing techniques such as fuzzy, genetic algorithms, etc. Therefore, this article introduces a Quality Improvement Method using Fuzzy Decision Support (QIM-FDS) for periodic service enhancements in hospitals. This method acquires the diagnosis, care-taking, and environmental factors validated using the two levels of FDS. The first FDS handles the quality improvement and the second deals with recommendations. From the previous patient suggestions/complaints, the quality improvements are performed such that the varying inputs result in high service recommendations. The first FDS and the overall recommendations (from patients) towards the above considerations are accounted for in the second fuzzification process. This consideration assimilates the service demands and is the highest patient response for retaining the same level. Therefore, the fuzzification performs the different condition verification in the second decision-making. The joint FDS processes focus on delivering high-level improvements towards the considered factors.},
  archive      = {J_IJCIS},
  author       = {Cui, Hongrui and Tan, Qingli},
  doi          = {10.1007/s44196-025-00773-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A fuzzy decision support system for medical service quality management in hospitals},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction and optimization of student grades based on genetic algorithm and graph convolutional neural networks. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00775-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to limited support in registered courses, students frequently struggle to complete their courses in higher education institutions. To combat this, educational systems are incorporating intelligent prediction tools to help students improve their academic performance by predicting their grades. Students' demographic information, past performance in the subject, and course characteristics are some of the factors used by the grade prediction system to foretell how they will do in future. Complexity and non-linearity in the analysis of inter-variable connections pose problems for traditional prediction models. Our solution to these problems is a GGCNN, or Genetic Algorithm with Graph Convolutional Neural Networks. In order to improve the accuracy of predictions, GGCNN examines educational data and finds intricate linkages. Using a graph structure, the graph convolution model emphasizes the interdependencies among academic metrics, course features, and student performance. Relationships and correlations can be better predicted with the use of this dependency metric. Use of the genetic algorithm improves the grade prediction system by optimizing the network and making better use of features. Administrators and teachers alike can find ways to boost their kids' grades through the optimization process. To test how well the system performs on different measures, we utilize the Student Performance Kaggle dataset. This continues until the convergence requirements are satisfied. With Python as its implementation, the system was able to get an accuracy of 0.98% after 100 epochs and 0.97% after 1000 epochs.},
  archive      = {J_IJCIS},
  author       = {Li, Ting},
  doi          = {10.1007/s44196-025-00775-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Prediction and optimization of student grades based on genetic algorithm and graph convolutional neural networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning model leveraging time-series system call data to detect malware attacks in virtual machines. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00781-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Tenant Virtual Machine (TVM) user in the cloud may misuse its computing power to launch malware attack against other tenant VMs, Host OS, Hypervisor, or any other computing devices/resources inside the cloud environment of a Cloud Service Provider. The security solutions deployed within the TVM may not be reliable, as malware can disable them or remain undetected due to its hidden nature. Therefore, security solutions deployed outside the virtual machine are necessary. This research proposes deploying an Intrusion Detection System (IDS) at the Hypervisor layer, utilizing time series system call data and employing a Convolutional Neural Network (CNN) model to accurately detect the presence of malicious (malware) computer programs within virtual machines. The raw VMM system call traces are transformed into novel Time Series System Call patterns and utilized by a deep learning algorithm for training and building the classifier model. A deep learning model, CNN, is used to build the classifier model for detecting intrusions with high accuracy. It is capable of detecting both known and unknown malware. The CNN model is compared with machine learning algorithms for the results and discussions, and it outperforms ML algorithms in terms of intrusion detection accuracy when utilizing novel time series system call data..},
  archive      = {J_IJCIS},
  author       = {Melvin, A. Alfred Raja and Kathrine, Jaspher W. and Jeyabose, Andrew and Cenitta, D.},
  doi          = {10.1007/s44196-025-00781-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A deep learning model leveraging time-series system call data to detect malware attacks in virtual machines},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning for cancer diagnosis in medical images: A compendious study. <em>IJCIS</em>, <em>18</em>(1), 1-46. (<a href='https://doi.org/10.1007/s44196-025-00772-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s world, cancer stands out as one of the most perilous diseases, caused by the uncontrolled proliferation of cells within the human body. Early detection is paramount to ensuring that patients receive the necessary medical intervention in a timely manner. Recently, deep learning techniques, particularly convolutional neural networks, have proven to be incredibly effective in developing computer-aided diagnosis systems due to their remarkable accuracy in analyzing medical images. However, the process of training these neural networks from scratch is often complex and requires significant computational resources. Transfer learning has emerged as a powerful solution to overcome this challenge. This study examines the fundamental concepts of machine learning and deep learning-based computer-aided diagnostic systems. It underscores the significant role of transfer learning in enhancing diagnostic accuracy. It also illustrates the various transfer learning models employed to diagnose various cancer forms, including skin cancer, brain tumors, breast cancer, lung cancer, leukemia, prostate cancer, bladder cancer, and cervical cancer. This paper summarizes 151 studies conducted in recent years. In the end, the article offers a thorough discussion of the research findings, overall conclusions, and directions for future work.},
  archive      = {J_IJCIS},
  author       = {Kaur, Navreet and Hans, Rahul},
  doi          = {10.1007/s44196-025-00772-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-46},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Transfer learning for cancer diagnosis in medical images: A compendious study},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid method using grey wolf algorithm and genetic algorithm for IoT botnet DDoS attacks detection. <em>IJCIS</em>, <em>18</em>(1), 1-61. (<a href='https://doi.org/10.1007/s44196-025-00774-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is a vast network of interconnected physical objects that has improved the conditions for a computer-based physical world and improved efficiency. With the increase in communication in an IoT system, Internet security has decreased, and the most dangerous and sophisticated attacks in the IoT have emerged, i.e., DDoS and Botnet attacks. DDoS attacks are a serious threat to the availability of Internet services, especially since botnets can now be launched by almost anyone. In this situation, the use of an intrusion detection system (IDS) is essential to detect intruders and maintain the security of IoT networks. In this paper, a new IDS is proposed to detect IoT-Botnet DDoS attacks. This IDS is a new three-phase system, the first phase is related to preprocessing on the dataset and the second phase includes a new hybrid method for feature selection using filter and wrapper methods based on the Grey Wolf (GW) algorithm and genetics called GW-GA. In this method, the initial population is randomly selected and then at each stage, feature selection is done by both algorithms simultaneously and the final answer is compared and the best solutions are given as a new population to both algorithms and the third phase includes the use of machine learning and metaheuristic algorithms as classifiers. In the proposed method and to verify the performance, it is evaluated using the large BOT-IoT dataset. The results show that the proposed method significantly reduces the feature and also increases the classification accuracy compared to other methods, and the RF and Bagging algorithms have achieved a maximum recognition accuracy of 0.999. The dimensions of BOT-IoT have been reduced from 46 features to 12.},
  archive      = {J_IJCIS},
  author       = {Maazalahi, Mahdieh and Hosseini, Soodeh},
  doi          = {10.1007/s44196-025-00774-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-61},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel hybrid method using grey wolf algorithm and genetic algorithm for IoT botnet DDoS attacks detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioral patterns in micro-lending: Enhancing credit risk assessment with collaborative filtering and federated learning. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00776-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk assessment uses finance-based behavioural patterns for micro-lending purposes and organizations. The repayment behaviour and credit stability patterns are analyzed across varying repayment tenures and financed amounts. Due to limited borrower data and fluctuating financial patterns, micro-lending platforms have substantial hurdles when it comes to effectively evaluating credit risk. This article introduces a Collaborative Filtering Method using Lending Pattern Analysis (CFM-LPA). The proposed method is enhanced through collaborative federated learning, enabling the analysis of these patterns. This approach evaluates the return rate, credit limit, and consumer response behaviours. Federated learning processes one or more of these factors to assess diverse lending patterns. Based on these evaluations, the behavioural factor is updated for each return period, influencing the credit risk for subsequent return periods and supporting the financial stability of micro-lending operations. The model is trained individually on the identified factors, allowing the behavioural factor to be filtered. New credit risks are identified using this filtered factor from the previous return period. These insights help define new behavioural patterns for the specified credit limit. The proposed method enhances risk detection accuracy by 14.03% and improves return rate analysis by 13.28% across financed amounts. The above abstract is also graphically presented.},
  archive      = {J_IJCIS},
  author       = {Aldrees, Asma and Shahab, Sana and Dutta, Ashit Kumar and Ahmad, Waseem and Anjum, Mohd},
  doi          = {10.1007/s44196-025-00776-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Behavioral patterns in micro-lending: Enhancing credit risk assessment with collaborative filtering and federated learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSegNet: A multi-view coupled cross-modal attention model for enhanced MRI brain tumor segmentation. <em>IJCIS</em>, <em>18</em>(1), 1-32. (<a href='https://doi.org/10.1007/s44196-025-00787-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor incidence and mortality rates are increasing due to unique location and treatment challenges. Early detection, robust diagnosis, and prompt treatment are crucial for better clinical evaluations. However, traditional neural network-based diagnostic methods often overlook issues such as variation in multimodality information, loss of spatial information, and under-utilization of boundary information. This study presents the Multi-View Coupled Cross-Modal Attention Network (MSegNet), a novel Transformer-based segmentation framework that integrates cross-modal attention mechanisms and a multi-view architecture. MSegNet is designed to exploit multimodal MRI data’s spatial and depth dimensions, effectively capturing nuanced intermodal relationships and modeling long-range dependencies. The proposed framework also employs three data augmentation methods, which help prevent overfitting and improve the performance of segmentation network training, enhancing the model’s robustness and generalizability. The proposed model is validated using BraTS2019, BraTS2020 and Figshate brain datasets and is compared against three state-of-the-art 3D segmentation networks. Extensive experiments, including ablation studies and hyperparameter sensitivity analyses, highlight MSegNet’s robust performance. The dice scores for the whole tumor (WT), tumor core (TC) and enhancing tumor (ET) regions improved by 13. 96%, 12. 39%, and 11. 83%, respectively, while the Hausdorff distances were reduced by 3.64 mm, 2.98 mm, and 14.72 mm. These results demonstrate the model’s efficacy in enhancing segmentation precision, making it a valuable tool for clinical diagnosis and treatment planning.},
  archive      = {J_IJCIS},
  author       = {Wang, Yu and Xu, Juan and Guan, Yucheng and Ahmad, Faizan and Mahmood, Tariq and Rehman, Amjad},
  doi          = {10.1007/s44196-025-00787-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {MSegNet: A multi-view coupled cross-modal attention model for enhanced MRI brain tumor segmentation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual feature-based intrusion detection system for IoT network security. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00790-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has enabled widespread connectivity of smart devices but remains susceptible to cyber intrusions. In this research, a novel dual feature optimization using deep learning network for intrusion Detection (FOUND) technique has been proposed for enhancing security in IoT environments. The proposed method utilizes the bald eagle search (BES) algorithm and butterfly optimization algorithm (BOA) to capture both flow and packet level features to enhance the accuracy of the intrusion detection process. Moreover, a multi-head attention-based bidirectional gated recurrent unit (MHA-BiGRU) is utilized to classify Attack and Non-Attack classes with high precision. The efficacy of the suggested approach is measured utilizing metrics including recall (RC), accuracy, precision (PR), and f1score (F1S). Experimental outcomes utilizing BoT-IoT and UNSW-NB15 datasets demonstrate greater accuracy over existing models. In BoT-IoT, the accuracy of the FOUND approach is 1.5%, 1.1%, and 2.5% increase compared to existing GRU, RNN, and GCN methods, respectively.},
  archive      = {J_IJCIS},
  author       = {Biju, A. and Franklin, S. Wilfred},
  doi          = {10.1007/s44196-025-00790-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Dual feature-based intrusion detection system for IoT network security},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved transformer-based model for urban pedestrian detection. <em>IJCIS</em>, <em>18</em>(1), 1-16. (<a href='https://doi.org/10.1007/s44196-025-00791-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection is a crucial task in computer vision, applicable in object tracking, video surveillance, and autonomous driving. Recent years have witnessed substantial advancements in pedestrian detection due to the fast evolution of deep learning in object detection. Nonetheless, obstacles such as inadequate detection accuracy persist, mostly because of varied pedestrian postures and intricate environments. This study proposes the RT-DETR-improved model to overcome these issues based on the real-time detection transformer (RT-DETR). First, we incorporate the high-low frequency (HiLo) attention into the encoder, therefore enhancing the model’s detection performance. Furthermore, we present a nonlinear feature fusion module that fuses information from various feature scales and contexts more successfully. We also introduce a novel loss function, InnerMPDIoU, to enhance detection efficacy in congested environments. To evaluate our model’s performance, extensive experiments are conducted on the CityPersons dataset. Compared to the baseline model, the RT-DETR-improved model attains a 4.2% enhancement in mAP50, a 2.0% improvement in mAP, a 2.2% rise in accuracy, and a 3.1% gain in recall. The results demonstrate that the proposed method exhibits superior detection accuracy and robustness.},
  archive      = {J_IJCIS},
  author       = {Wu, Tianyong and Li, Xiang and Dong, Qiuxuan},
  doi          = {10.1007/s44196-025-00791-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An improved transformer-based model for urban pedestrian detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TraitBertGCN: Personality trait prediction using BertGCN with data fusion technique. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00792-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personality prediction via different techniques is an established and trending topic in psychology. The advancement of machine learning algorithms in multiple fields also attracted the attention of Automatic Personality Prediction (APP). This research proposes a novel TraitBertGCN method with a data fusion technique for predicting personality traits. Initially, this work integrates a pre-trained language model, Bidirectional Encoder Representations from Transformers (BERT), with a three-layer Graph Convolutional Network (GCN) to leverage large-scale language understanding and graph-based learning for personality prediction. This study fuses the two datasets (essays and myPersonality) to overcome the bias and generalize the model across different domains. We fine-tuned our TraitBertGCN model on the fused dataset and then evaluated it on both datasets individually to assess its adaptability and accuracy in varied contexts. We compared the proposed model’s results with previous studies; our model achieved better performance in personality trait prediction across multiple datasets, with an average accuracy of 77.42% on the essays dataset and 87.59% on the myPersonality dataset.},
  archive      = {J_IJCIS},
  author       = {Waqas, Muhammad and Zhang, Fengli and Laghari, Asif Ali and Almadhor, Ahmad and Petrinec, Filip and Iqbal, Asif and Khalil, Mian Muhammad Yasir},
  doi          = {10.1007/s44196-025-00792-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {TraitBertGCN: Personality trait prediction using BertGCN with data fusion technique},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid differential evolutionary algorithm for solving multi-objective distributed permutation flow-shop scheduling problem. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00793-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Distributed Permutation Flow-Shop Scheduling Problem (DPFSP) is a classic issue in distributed scheduling that involves job allocation and processing order within a factory, and it is known to be NP-hard. Numerous researchers have proposed various intelligent optimization algorithms to address the DPFSP; however, there are fewer studies related to the multi-objective DPFSP problem, and the algorithms for solving this problem also suffer from poor solution quality and tend to fall into local optimization and so on. To tackle the multi-objective DPFSP, this paper proposes a novel hybrid differential evolutionary algorithm aimed at minimizing both the maximum completion time and total delay time. In this algorithm, Bernoulli chaotic mapping is applied during the population initialization process to enhance the diversity of the initial population. Additionally, an adaptive mutation factor and crossover rate are designed to balance the global and local search capabilities of the algorithm. Furthermore, a novel selection strategy is constructed based on the NEH algorithm, specular reflection learning, and Pareto dominance relation to improve the quality of the solution set when solving instances of varying sizes. This strategy enhances the algorithm's optimization ability and helps it escape local optima. The effectiveness and superiority of the proposed algorithm are verified through 24 instances of different sizes. The results demonstrate that the proposed algorithm outperforms other improved algorithms in terms of convergence, and the uniformity and diversity of the solution set, making it an effective solution for the multi-objective distributed permutation flow-shop scheduling problem.},
  archive      = {J_IJCIS},
  author       = {Du, Xinzhe and Zhou, Yanping},
  doi          = {10.1007/s44196-025-00793-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel hybrid differential evolutionary algorithm for solving multi-objective distributed permutation flow-shop scheduling problem},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on cascade propagation of collaborative innovation risks in industrial clusters considering entities heterogeneity. <em>IJCIS</em>, <em>18</em>(1), 1-17. (<a href='https://doi.org/10.1007/s44196-025-00795-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this study is to effectively mitigate the cascading propagation of collaborative innovation risks within industrial clusters and bolster the stability of their innovation networks. Drawing upon the cascade failure theory in complex networks, we employ the BA scale-free network model to construct an industrial cluster innovation network. We develop a cascade propagation model for collaborative innovation risk, addressing three dimensions: risk load, risk capacity, and load redistribution following node failures. To enhance the model, we propose a risk load distribution strategy that considers the heterogeneity among innovation entities, focusing on the similarity degree, importance degree, and cooperation degree of neighboring innovative entities. Through simulation experiments, we demonstrate that the integrated allocation strategy significantly improves the resistance to destruction of industrial cluster innovation networks. However, under intentional attacks, the resilience of these networks remains relatively weak. Further investigation reveals that the risk load capacity enhanced by the integrated allocation strategy can somewhat fortify the resistance of industrial cluster innovation networks to such attacks. The findings offer valuable insights for risk management and stability enhancement in industrial cluster innovation networks.},
  archive      = {J_IJCIS},
  author       = {Shi, Xiaowei and Wang, Jifa and Wang, Yang},
  doi          = {10.1007/s44196-025-00795-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Research on cascade propagation of collaborative innovation risks in industrial clusters considering entities heterogeneity},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Better pseudo-labeling for semi-supervised domain generalization in medical magnetic resonance image segmentation. <em>IJCIS</em>, <em>18</em>(1), 1-30. (<a href='https://doi.org/10.1007/s44196-025-00786-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance image (MRI) is the primary diagnostic test used clinically for the diagnosis and assessment of a wide range of diseases. In recent years, many studies have employed artificial intelligence techniques for MRI segmentation. Deep learning methods have demonstrated potential to enhance segmentation performance. However, they still face two challenges: annotation scarcity and domain shift. The annotation of MRI is both challenging and costly, and well-annotated datasets are scarce and valuable. Moreover, due to variations in MRI machines, ensuring the independence and identical distribution between model training data and real-world data is difficult, which may lead to noisy model predictions and weak generalization ability. We aim to address the challenges through a multi-pronged approach. First, we propose a method that integrates confidence and uncertainty for generating reliable pseudo-labels. Second, we introduce a consistency learning method that employs self-perturbation at both the image and feature levels to encourage the learning of more generalized feature representations. Finally, we optimize pseudo-labels end-to-end with the teacher–student framework. To evaluate the effectiveness of our method, we conduct experiments on six different MRI segmentation datasets. The results showed that our method was superior to the existing methods in DSC, ASD and HD95 metrics. In addition, we evaluated the quality and quantity of the generated pseudo-labels, and the results showed that our method generated better pseudo-labels than other methods. Overall, our proposed method shows promising potential in assisting clinicians in practical applications.},
  archive      = {J_IJCIS},
  author       = {Hu, Liangqing and Meng, Zuqiang and Tan, Chaohong and Zhou, Yumin},
  doi          = {10.1007/s44196-025-00786-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Better pseudo-labeling for semi-supervised domain generalization in medical magnetic resonance image segmentation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A greedy constructive heuristic for solving the team orienteering problem with variable time windows. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00797-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orienteering problems are a subclass of routing problems, in which a selection of the set of nodes should be made for visiting, due to route length restrictions. These nodes can also impose time-window constraints, which can be variable if they are defined by a spread process, which behavior can be modified. The problem including all these features is the Team Orienteering Problem with Variable Time Windows (TOPVTW). In this paper, deterministic and randomized greedy constructive heuristic schemes are developed for solving the problem, along with the definition of some metrics that guide the constructive processes. One of the heuristics is combined with an existing exact mixed integer programming model to improve the outputs. All the solving strategies proposed are tested with instances representing the spread of a wildfire in a landscape, demonstrating improvements in performance when compared with existing exact solving methodologies.},
  archive      = {J_IJCIS},
  author       = {Granda, Bibiana and Vitoriano, Begoña},
  doi          = {10.1007/s44196-025-00797-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A greedy constructive heuristic for solving the team orienteering problem with variable time windows},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instance assignment coverage feature for operation control of SAT solver. <em>IJCIS</em>, <em>18</em>(1), 1-18. (<a href='https://doi.org/10.1007/s44196-025-00798-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Conflict-Driven Clause Learning (CDCL) framework integrates multiple heuristic components to solve Boolean satisfiability (SAT) problems through synergistic cooperation. Understanding the characteristics of these components in the underlying architecture provides crucial insights for designing corresponding methods to enhance the performance of CDCL solvers. Although numerous studies from diverse perspectives have been conducted, there remains a need to develop efficient methods and algorithms to meet the requirements for enhancing the performance efficiency of SAT solving. In this paper, we introduced two fundamental innovations: deep restart, a strategic reset mechanism that clears variable activity states while preserving learned clauses and making phase randomization, and assignment coverage time (CoverT), a novel metric quantifying the minimum—conflict count required to assign all variables at least once during search exploration. The CoverT metric provided unique insight into the characteristics of the instance structure, allowing dynamic adaptation of branching heuristics in our proposed Deep Restart-Enhanced Conflict-Driven Clause Learning algorithm framework (DR-CDCL). Experimental validation in 2021–2023 SAT Competition benchmarks demonstrated statistically significant improvements: Notably, the performance trade-off analysis revealed that while deep restart enhances solution diversity for satisfiable instances, it introduced a 2.1% overhead on unsatisfiable proofs due to clause learning pattern disruption, a phenomenon requiring further investigation. This work advances solver architecture design by establishing formal connections between exploratory search patterns and instance structural complexity. The implemented solution prototype and benchmark data are publicly available to facilitate reproducibility.},
  archive      = {J_IJCIS},
  author       = {Li, Zhihui and Chen, Shuwei and Wu, Guanfeng and Xu, Yang},
  doi          = {10.1007/s44196-025-00798-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Instance assignment coverage feature for operation control of SAT solver},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of machine learning algorithms for real-time gallbladder stone identification from ultrasound images in clinical decision support systems. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00740-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global epidemiology of gallstones in the twenty-first century affects millions of individuals, and ultrasound diagnostics effectively assess gallbladder size and function and detect abnormalities. This study collected datasets from local hospitals and reliable online sources for analysis using advanced CV/IP tools and WEKA. Image preprocessing techniques, including cropping, resizing, and grayscale conversion, were applied to 90 ultrasound images, extracting 600 ROIs with 21 features spanning binary, histogram, and texture attributes. The dataset was divided into balanced training and validation subsets, and supervised learning algorithms were optimized via cross-validation and grid search. Circular patterns were processed iteratively, with specific dimensions (512 × 512 for width/height, 32 × 32 for radius/blur, 128 × 128 for columns/rows). The performance of various machine learning classifiers was evaluated using accuracy, precision, recall, F1 score, AUC-ROC, MCC, Kappa GDR, and Dice Index, ensuring strong classification of normal and abnormal samples. The random forest (RF) classifier achieved the highest performance with an accuracy of 96.33%, followed by the MLP and Logit Boost classifiers with 95.67% and 95.40% accuracy rates, respectively. The RF model also exhibited the highest precision (0.9542), recall (0.9732), F1 score (0.9636), and a Dice Index (0.9649) with an MCC of 0.925, ROC area of 0.988, Kappa (0.921), and specificity of 95.34%-indicating its strong ability to balance true positives and negatives while minimizing misclassifications. The MLP classifier also performed well with a precision of 0.9477, a recall of 0.9665, and an F1 score of 0.957, while Logit Boost had similar results with a precision of 0.9411 and a recall of 0.9665. Other classifiers, such as the Bayes Net and J48 classifiers, showed slightly lower performance with accuracy rates of 94.67% but still exhibited good precision and recall, making them viable alternatives. This study highlights that the RF classifier achieved the highest superiority among other models in detecting gallbladder stones.},
  archive      = {J_IJCIS},
  author       = {Hong, Chen and Zafar, Imran and Ayaz, Muhammad Mazhar and Kanwal, Rimsha and Kanwal, Faheem and Dauelbait, Musaab and Bourhia, Mohammed and Jardan, Yousef A. Bin},
  doi          = {10.1007/s44196-025-00740-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Analysis of machine learning algorithms for real-time gallbladder stone identification from ultrasound images in clinical decision support systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: A two-way crossed effects fuzzy panel linear regression model. <em>IJCIS</em>, <em>18</em>(1), 1-2. (<a href='https://doi.org/10.1007/s44196-025-00808-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Hesamian, Gholamreza and Johannssen, Arne},
  doi          = {10.1007/s44196-025-00808-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Correction: A two-way crossed effects fuzzy panel linear regression model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced YOLOv8 for efficient parcel identification in disordered logistics environments. <em>IJCIS</em>, <em>18</em>(1), 1-16. (<a href='https://doi.org/10.1007/s44196-025-00794-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate parcel identification in disordered logistics environments poses significant challenges due to varying package sizes, materials, and orientations. This study presents an improved YOLOv8-Efficiency algorithm tailored for such complex scenarios. The proposed algorithm introduces the C2f-OR module to reduce parameters and computation, the Conv-Ghost module for efficient feature extraction, and the HIoU loss function to enhance identification accuracy. By constructing a dataset of 4689 photos, experiments demonstrate the algorithm's effectiveness, achieving a 93.2% mAP, a 1.6% recall rate improvement, and a significant reduction in computational complexity (9.9% decrease in FLOPs). This work provides a robust solution for real-time parcel identification in disordered logistics, facilitating automation and efficiency in logistics operations.},
  archive      = {J_IJCIS},
  author       = {Yu, Han and Fengshou, Zhang and Gaoshuai, Zhuang and Yuanhao, Qu and Aohui, He and Qingyang, Duan},
  doi          = {10.1007/s44196-025-00794-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhanced YOLOv8 for efficient parcel identification in disordered logistics environments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-scale spatial refinement graph convolutional network for skeleton-based action recognition. <em>IJCIS</em>, <em>18</em>(1), 1-17. (<a href='https://doi.org/10.1007/s44196-025-00802-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In skeleton-based action recognition, abstracting the human body to skeletal representations often results in the loss of crucial information, which may result in misclassification of similar actions. To address this issue, we propose a Cross-scale Spatial Refinement Graph Convolutional Network (CSR-GCN), which aims to improve action recognition accuracy by effectively capturing fine-grained features of skeleton sequences. In detail, we introduce an Attention-based Graph Pooling (AGP) module and a Cross-scale Feature Aggregation (CFA) module. The AGP module uses graph pooling to construct multi-scale skeletal sub-graphs, capturing implicit joint relationships and preserving crucial motion details. It retains global motion information while emphasizing local joint interactions, which enables a better understanding of dynamic changes in complex actions. Furthermore, the CFA module selectively integrates features from different spatial scales, enhancing feature distinctiveness while balancing global motion and local details. This multi-scale refinement of skeletal sequence representations, thereby capturing subtle dynamic changes in actions more precisely and enhancing the ability of the model to recognize and classify complex movement patterns. Finally, we validate the effectiveness of our method on three large-scale datasets, achieving superior accuracy compared to other state-of-the-art methods.},
  archive      = {J_IJCIS},
  author       = {Ke, Chengyuan and Liu, Sheng and Ke, Zhenghao and Feng, Yuan and Chen, Shengyong},
  doi          = {10.1007/s44196-025-00802-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Cross-scale spatial refinement graph convolutional network for skeleton-based action recognition},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GINSER: Geographic information system based optimal route recommendation via optimized faster R-CNN. <em>IJCIS</em>, <em>18</em>(1), 1-27. (<a href='https://doi.org/10.1007/s44196-025-00805-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic congestion and accident-prone zones present significant challenges to urban transportation by causing delays, pollution, and safety hazards. However, the existing techniques do not provide real-time recommendations for minimizing congestion leaving travelers and planners with suboptimal solutions. These systems often fail to integrate accident-prone zone detection with traffic congestion management and cannot provide a real time congestion-free route. This research aims to address these challenges by developing a geographic information system (GIS)-based optimal route recommendation model GINSER using advanced deep learning techniques. The primary objectives are to detect accident-prone zones, classify traffic congestion levels, and recommend efficient routes using GIS. The proposed GINSER model utilized CCTV camera as an input image are preprocessed using an adaptive Gaussian bilateral filter (AGBF) to remove noise and enhance image quality. Faster R-CNN is used for identifying and localizing objects in accident-prone areas. Particle swarm optimization (PSO) is used to hyperparameter tuning for improving an accuracy. A CNN-BiGRU model is utilized to classify traffic congestion levels into low, moderate, high, and congestion-free categories. GIS analyzes spatial data and traffic patterns to recommend the most efficient and congestion-free routes. The effectiveness of the proposed GINSER approach was assessed utilizing F1 score, accuracy, precision, recall, and specificity. The noise-free images using AGBF effectively enhances image quality by reducing noise leading to improved classification accuracy. PSO is utilized for hyperparameter tuning achieving a high accuracy of 95.24%. The GINSER model achieved a classification accuracy of 99.16%. The GINSER improved overall accuracy by 3.90%, 6.71%, 4.13%, and 0.70% better than TSANet, TCEVis, Ising-traffic, and AID, respectively. The proposed GINSER model offers a novel solution to urban transportation challenges by integrating deep learning and GIS technologies. Its ability to detect accident-prone zones classify congestion levels and recommend optimal routes ensures safer and more efficient mobility.},
  archive      = {J_IJCIS},
  author       = {Anitha Selvasofia, S. D. and SivaSankari, B. and Dinesh, R. and Muthukumaran, N.},
  doi          = {10.1007/s44196-025-00805-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {GINSER: Geographic information system based optimal route recommendation via optimized faster R-CNN},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: MSegNet: A multi-view coupled cross-modal attention model for enhanced MRI brain tumor segmentation. <em>IJCIS</em>, <em>18</em>(1), 1-2. (<a href='https://doi.org/10.1007/s44196-025-00807-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Wang, Yu and Xu, Juan and Guan, Yucheng and Ahmad, Faizan and Mahmood, Tariq and Rehman, Amjad},
  doi          = {10.1007/s44196-025-00807-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Correction: MSegNet: A multi-view coupled cross-modal attention model for enhanced MRI brain tumor segmentation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent decision support system for selecting optimal AI-powered assistive technology for individuals with disabilities. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00770-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel extension of conventional fuzzy sets in this paper: called trimorphic fuzzy sets. As per our research, trimorphic fuzzy sets which exhibit greater capability than intuitionistic fuzzy sets, picture fuzzy sets and bipolar fuzzy sets, present a viable approach to address ambiguity and uncertainty in decision-making scenarios. We present a complete characterization of trimorphic fuzzy sets, discuss their properties, and consider applications to real-world decision-making scenarios. We also present a case study to further highlight the practical applications of trimorphic fuzzy sets. We look into a few aggregation strategies for trimorphic fuzzy data in this work. We create the MCDM method using trimorphic fuzzy aggregation operators to help people with disabilities choose AI-Powered Assistive Technologies. We have also presented the extended TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) method with trimorphic fuzzy numbers. A numerical example for selection of AI-Powered Assistive Technologies using TOPSIS method is also provided.},
  archive      = {J_IJCIS},
  author       = {Minhaj and Muneeza and Khan, Asghar and Khishe, Mohammad and Gumaei, Abdu H. and Alzanin, Samah M. and Alkhamees, Bader Fahad and Ashraf, Shahzaib},
  doi          = {10.1007/s44196-025-00770-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An intelligent decision support system for selecting optimal AI-powered assistive technology for individuals with disabilities},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrioConvTomatoNet-BiLSTM: An efficient framework for the classification of tomato leaf diseases in real time complex background images. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00788-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tomatoes are the most valuable vegetable worldwide that suffer from leaf diseases, which affect long-term tomato protection. So, to protect the tomato plants from the leaf diseases, it is essential to perform appropriate control measures through early and accurate categorization of leaf diseases. Recently, automated deep learning-based methods, including convolutional neural networks (CNNs), guaranteed accurate and timely classification of tomato leaf diseases. However, CNNs primarily capture local context features within a limited receptive field, making them effective for uniform background images. To handle complex background images, utilizing local and global context features is essential for accurate classification. To do so, it is essential to hybrid CNN architecture with other deep learning modules. This work suggests the TrioConvTomatoNet-BiLSTM framework, a hybridization of CNN architecture named TrioConvTomatoNet with a sequence module named bidirectional long short-term memory (BiLSTM). The proposed framework integrated both local and global context features for the precise classification of images with complex backgrounds. As a result, the proposed framework achieves remarkable accuracy of 99.65%, 98.83%, and 99.20% in classifying tomato leaf disease images with non-uniform, synthetic, and real-time complex backgrounds against the TrioConvTomatoNet and TrioConvTomatoNet-LSTM frameworks. Despite the fact that it requires a lesser number of training parameters and attained maximum accuracy over other existing hybrid approaches, expresses its superiority, robustness, and practical applicability. These features highlight the potential of the proposed framework in the emerging field of smart agriculture by enabling smartphone-based classification of tomato leaf diseases with real-life scenarios.},
  archive      = {J_IJCIS},
  author       = {Ledbin Vini, S. and Rathika, P.},
  doi          = {10.1007/s44196-025-00788-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {TrioConvTomatoNet-BiLSTM: An efficient framework for the classification of tomato leaf diseases in real time complex background images},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of renewable energies based on circular bipolar complex intuitionistic fuzzy linguistic information with frank power aggregation operators and MABAC model. <em>IJCIS</em>, <em>18</em>(1), 1-40. (<a href='https://doi.org/10.1007/s44196-025-00800-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intuitionistic fuzzy sets and bipolar fuzzy sets are two different ideas, used for the assessment of opinions because the intuitionistic fuzzy sets contain the membership function and non-membership function, but the bipolar fuzzy set describes opinions under two oppositional directions with the positive membership function and negative membership function. This paper develops the circular bipolar complex intuitionistic fuzzy linguistic (Cir-BCIFL) set theory, which is an extension of the numerous existing models, such as fuzzy sets, intuitionistic fuzzy sets, bipolar fuzzy sets, linguistic sets, complex fuzzy sets, and circular intuitionistic fuzzy sets to cope with vague and complex information. In a decision-making scenario, Cir-BCIFL set theory is considered a massive prominent and efficient tool than other existing techniques. Therefore, for the construction of the power aggregation operators, the operational laws based on algebraic and Frank norms for the Cir-BCIFL set are proposed. In this paper, we also derive the theory of Cir-BCIFL Frank power averaging operator, Cir-BCIFL Frank power weighted averaging operator, Cir-BCIFL Frank power geometric operator, and Cir-BCIFL Frank power weighted geometric operator with their basic properties, such as idempotency, monotonicity, and boundedness. Then a multi-attribute border approximation area comparison (MABAC) approach is proposed based on the developed theory. Renewable energy is the form of energy, which is formed or produced from natural and genuine resources that are continuously inexhaustible or replenished. This application goals to assess renewable energy in the European Union to evaluate the most prominent types for their usage in the best and worst conditions. Thus, to deliberate the rationality and efficiency of the designed model, we explain and discuss how to solve numerical problems related to renewable energy for choosing the best way of industrialization with the help of a multi-attribute decision-making model. Next, we indicate how the contribution of the parameters in our designed technique affects the decision-making results. Finally, to exhibit the worth of the initiated theory, the obtained results are compared with the existing techniques.},
  archive      = {J_IJCIS},
  author       = {Ali, Zeeshan and Yang, Miin-Shen},
  doi          = {10.1007/s44196-025-00800-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-40},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Analysis of renewable energies based on circular bipolar complex intuitionistic fuzzy linguistic information with frank power aggregation operators and MABAC model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEANE: Context-aware dual-craft graph contrastive learning for enhanced extractive question answering. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00801-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extractive Question Answering (EQA) involves extracting accurate answer spans from a background passage in response to a given question. In recent years, there has been significant interest in leveraging Pre-trained Language Models (PLMs) and Graph Convolutional Networks (GCNs) to address EQA tasks. PLMs usually function as context encoders, while GCNs are employed to capture latent semantic relationships between answer spans and the passage/question. This combined approach has shown promise, yielding notable outcomes in EQA performance. However, current graph-based methods encounter a challenge where the graph structure is predefined without sufficient justification. This graph ambiguity can potentially lead to error propagation within the subsequent graph encoder. To alleviate this issue, this paper introduces Dual-craft basEd grAph coNtrastive lEarning (DEANE) for EQA, where the graph structure and node features are context-aware and data-driven. Initially, the passage and question are represented as a connected graph. Subsequently, the adaptive augmentation strategy is introduced to generate two distinct views of the original graph via reparameterization networks, where important graph edges and node features are prioritized. Finally, a multi-view contrastive loss is leveraged to learn latent representations from augmented graphs. Empirically, our method outperforms existing graph-based approaches on six well-established EQA benchmarks. Ablation studies further demonstrate the effectiveness of the proposed approach in mitigating structural ambiguity, enhancing encoder flexibility, and improving model performance through multi-view data integration.},
  archive      = {J_IJCIS},
  author       = {Ye, Dongfen and Zhou, Jianqiang and Huang, Gang},
  doi          = {10.1007/s44196-025-00801-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {DEANE: Context-aware dual-craft graph contrastive learning for enhanced extractive question answering},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced deep learning-based optimization model for the coverage optimization in wireless sensor networks. <em>IJCIS</em>, <em>18</em>(1), 1-25. (<a href='https://doi.org/10.1007/s44196-025-00803-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key component of wireless sensor networks (WSN) is optimal coverage. For WSN to have network lifetime and optimal resource utilization, maximum coverage must be guaranteed. The unequal distribution of sensor nodes in densely populated regions contributes to the build-up of network coverage. This research suggests a revolutionary intelligent deep learning-based optimization approach for WSN coverage optimization to tackle this problem. Here, the optimal network coverage of WSN is performed by the novel enhanced deep Q-network (EDQN) algorithm, where the parameters of DQN are tuned by nature inspired optimization algorithm called hippopotamus optimization (HO) with the intention of attaining the fitness function. In order to maximize coverage, the node position is changed in the developed methodology to represent the spatial properties of the network. Along with lowering latency, the increased coverage also increases throughput and network longevity. The outcome of several network coverage optimization trials is computed, and the EDQN-HO’s impact on network coverage optimization is also determined by adjusting the parameters. The network coverage optimization studies’ simulated findings demonstrate that the suggested EDQN-HO may be effectively utilized in a variety of settings. The proposed EDQN-HO for the WSN coverage model returns superior outcomes with 18.31%, 78.95%, 10.65%, 87.5%, 83.33%, and 38.20% than the existing methods in terms of coverage rate, energy consumption, computing time, positioning error, network lifetime, and average moving distance respectively.},
  archive      = {J_IJCIS},
  author       = {Kumar, S. Praveen and Nagendranath, M. V. S. S. and Alsamri, Jamal and Ebad, Shouki A.},
  doi          = {10.1007/s44196-025-00803-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhanced deep learning-based optimization model for the coverage optimization in wireless sensor networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedMEM: Adaptive personalized federated learning framework for heterogeneous mobile edge environments. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00814-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of the Internet of Things (IoT) and communication technologies, edge devices have become more diverse. This diversity has increased the computational load on these systems and led to differences between devices. In mobile edge computing, variations in communication and computing resources can prevent some devices from updating models quickly. This delay affects overall performance. In addition, in federated learning, data that is not independently and identically distributed (non-IID) makes it hard for clients to maintain personalized models.To address these issues, this paper introduces a personalized federated learning framework. This framework enhances the resource allocation optimization algorithm by dynamically adjusting the depth of model inference and the bandwidth allocation strategy, which assists devices with limited computational capabilities in completing inference tasks promptly. Furthermore, it divide the client models into global and personalized layers. Only the global layers are combined, which helps manage the diversity in data distributions. Simulation results show that the proposed FedMEM method is superior to other state-of-the-art methods, and can drastically reduce system latency.},
  archive      = {J_IJCIS},
  author       = {Ximing, Chen and Xilong, He and Du, Cheng and Tiejun, Wu and Qingyu, Tian and Rongrong, Chen and Jing, Qiu},
  doi          = {10.1007/s44196-025-00814-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {FedMEM: Adaptive personalized federated learning framework for heterogeneous mobile edge environments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic dataset generation method for object detection. <em>IJCIS</em>, <em>18</em>(1), 1-15. (<a href='https://doi.org/10.1007/s44196-025-00817-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the high construction cost of datasets for object detection, particularly in industrial application scenarios where sufficient sample images cannot be obtained from the Internet due to the specialized nature and diversity of objects and their working environments, this paper proposes a method to automatically generate synthetic datasets and train object detection models on them. First, 3D models of the target devices are created and rendered to ensure that the synthetic images exhibit realistic texture and detail. Next, a simulation environment is constructed and the 3D models are integrated into this environment using global domain randomization techniques. Finally, computer graphics methods are applied to automatically annotate target objects in the synthetic images. This approach effectively reduces the cost of data acquisition while maintaining the detection accuracy of the models. Several mainstream object detection models, including Faster R-CNN, SSD, and YOLO, are trained on synthetic datasets of anti-vibration dampers. Experimental results on real-world images demonstrate that models trained on synthetic data achieve relatively high accuracy. Furthermore, fine-tuning these models with a very small number of real images significantly enhances their performance. In addition, the models exhibit robustness against interference and occlusion.},
  archive      = {J_IJCIS},
  author       = {Zhou, Ningning and Li, Tong},
  doi          = {10.1007/s44196-025-00817-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Synthetic dataset generation method for object detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Author correction: Analysis of machine learning algorithms for real-time gallbladder stone identification from ultrasound images in clinical decision support systems. <em>IJCIS</em>, <em>18</em>(1), 1-2. (<a href='https://doi.org/10.1007/s44196-025-00826-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Chen, Hong and Zafar, Imran and Ayaz, Muhammad Mazhar and Kanwal, Rimsha and Kanwal, Faheem and Dauelbait, Musaab and Bourhia, Mohammed and Jardan, Yousef A. Bin},
  doi          = {10.1007/s44196-025-00826-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Author correction: Analysis of machine learning algorithms for real-time gallbladder stone identification from ultrasound images in clinical decision support systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-dataset analysis of language models for generalised multi-label review note distribution in animated productions. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00785-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the production of an animated film, supervisors and directors hold daily meetings to evaluate in-progress material. Over the course of the several years it takes to complete a film, thousands of text notes outlining required fixes are generated. These notes are manually allocated to various departments for resolution. However, as with any manual process, a significant number of notes are either delayed, miss-assigned or overlooked entirely, which can negatively impact the final quality of the film. This paper investigates the performance of various methods for automating the distribution of review notes across relevant departments using datasets from multiple films produced by an animation studio in Madrid, Spain. Since each note can belong to multiple departments, the task is posed as a multi-label classification problem. The analysis and comparison of the results obtained with datasets from three different films, focusing on generalisation, provides critical insights for any Animation Studio evaluating the use of these methods in their process. The methods leverage Large Language Models (LLMs), including encoder-only models such as BERT and decoder-only models like Llama 2. Fine-tuning with QLoRA and in-context learning techniques were applied and evaluated across all datasets, and a cross-dataset analysis is presented. The fine-tuned encoder-only model achieved an F1-score of 0.98 for notes directed to the Animation department. Training was carried out locally on an RTX-3090 GPU, completing it in less than 30 min.},
  archive      = {J_IJCIS},
  author       = {Garcés, Diego and Santos, Matilde and Fernández-Llorca, David},
  doi          = {10.1007/s44196-025-00785-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Cross-dataset analysis of language models for generalised multi-label review note distribution in animated productions},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid meta-heuristic algorithm for optimum micro-robotic position control with PID controller. <em>IJCIS</em>, <em>18</em>(1), 1-27. (<a href='https://doi.org/10.1007/s44196-025-00799-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present paper aims to propose a novel hybrid algorithm, where the Arithmetic Optimization Algorithm (AOA) and Rat Swarm Optimization (RSO) are employed for the proportional-integral-derivative (PID) controller to control the position of a micro-robotics system. In the algorithm proposed, we combine the exploratory mechanisms of AOA with RSO's exploitative behaviors. The proposed algorithm is employed for identifying the PID controller optimal parameters considering six different objective functions. Using CEC 2017 benchmark functions, the proposed hybrid is evaluated, and these functions’ performance is compared with the existing multiple algorithms. The statistical results are compared with the AOA, Jellyfish Search Optimization, and Harries Hawk Optimization algorithm for identifying the optimal PID controller settings considering multiple fitness functions. We consider performance indicators like PID controller parameters, rise time, settling time, and fitness values. The fetched simulation results revealed that, among all investigated fitness functions, the developed controller based on HAOARSO is the most effective algorithm for delivering global optimal solutions with less settling time and rise time, enabling the implementation on such optimization issues. Finally, the validation via MATLAB/Simulink simulations underscores the efficacy of the proposed algorithm.},
  archive      = {J_IJCIS},
  author       = {Baihan, Abdullah and Ghith, Ehab and Garg, Harish and Mirjalili, Seyedali and Izci, Davut and Rashdan, Mostafa and Salman, Mohammad and Saleem, Kashif},
  doi          = {10.1007/s44196-025-00799-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A hybrid meta-heuristic algorithm for optimum micro-robotic position control with PID controller},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social network analysis: A novel paradigm for improving community detection. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00812-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social network analysis has become increasingly important across a wide range of fields, offering valuable insights into complex systems of interconnected entities. One of the fundamental challenges in this field is the community detection problem, which involves identifying groups within networks. Multiple algorithms have been proposed, exploring new approaches to finding solutions for cohesive partitions of the graph. One of the most considered philosophies when defining this type of technique is the use of the graph’s adjacency matrix as input and the consideration of modularity as the function to be optimized. We propose an enhancement to this approach to community detection by incorporating high-order relationships between nodes, allowing for a more comprehensive capture of network structure. By modifying the algorithm’s input, our method improves community detection accuracy. Moreover, our proposed approach is universal, applicable to any algorithm that utilizes a matrix as input. Its value is further validated through a comprehensive set of results, comparing the original problem with the enhanced method we present. We also present a tourism case study.},
  archive      = {J_IJCIS},
  author       = {Hernández, Rodrigo and Gutiérrez, Inmaculada and Castro, Javier},
  doi          = {10.1007/s44196-025-00812-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Social network analysis: A novel paradigm for improving community detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new double weighted fuzzy hypergeometric naive bayes network and its application for user’s assessment in virtual reality simulators. <em>IJCIS</em>, <em>18</em>(1), 1-17. (<a href='https://doi.org/10.1007/s44196-025-00816-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessment methods have been used for identify the quality of procedures performed by human beings. In virtual reality simulators, user’s interaction data can be collected and utilized by Single User Assessment Systems (SUAS) to assess the user’s performance. In particular, some procedures can be considered well performed when the same objective is achieved a number of times with success within a limited universe of attempts. In this case, the data can be modeled by a Hypergeometric distribution. This paper presents the proposal of a new Double Weighted Fuzzy Hypergeometric Naive Bayes (DW-FHyperNB) Network as basis for a SUAS, which can be used in virtual reality simulators. The results showed that this new SUAS was able to achieve better results when compared to other SUAS based on different Naive Bayes Networks. This comparison was performed using SUAS based on Fuzzy Hypergeometric Naive Bayes Network, Classical Hypergeometric Naive Bayes Network, Double Weighted Classical Hypergeometric Naive Bayes Network, Multinomial Naive Bayes Network, Bayes Net, Support Vector Machine, Multilayer Perceptron Neural Network, Radial Basis Network with Gaussian Functions, C4.5 Decision Tree, Decision Table-Naive Bayes, Multinomial Logistic Regression, Simple Cart, and Random Forest was performed. The results obtained showed that the DW-FHyperNB Network produced the best performance, according to the Overall Accuracy Index, Kappa and Tau Coefficients, and diagnostic tests. The new DW-FHyperNB Network can also be utilized for data and image classification, pattern recognition, as well as machine learning applications.},
  archive      = {J_IJCIS},
  author       = {Ferreira, Jodavid and Machado, Liliane S. and de Moraes, Ronei Marcos},
  doi          = {10.1007/s44196-025-00816-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A new double weighted fuzzy hypergeometric naive bayes network and its application for user’s assessment in virtual reality simulators},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A federated weighted learning algorithm against poisoning attacks. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00819-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of Federated Learning (FL) has enabled privacy-preserving distributed machine learning, yet its vulnerability to poisoning attacks remains a critical challenge. Existing defense methods often rely on static aggregation rules or centralized verification mechanisms, which lack adaptability to dynamic adversarial behaviors and incur high computational costs. To address these limitations, this paper proposes the Federated Weighted Learning Algorithm (FWLA), a novel framework designed to mitigate poisoning attacks through client-specific weight adaptation and asynchronous collaboration. The core of FWLA lies in two components: (1) a residual testing mechanism that dynamically identifies malicious clients by analyzing deviations between local and global model updates, and (2) an asynchronous training protocol that allows clients to independently upload parameters, thereby avoiding synchronization bottlenecks. Extensive experiments on three benchmark datasets (CICIDS2017, UNSW-NB15, NSL-KDD) demonstrate FWLA’s superiority over state-of-the-art methods. Specifically, FWLA achieves 98.9% accuracy and reduces the false acceptance rate to 2.9% on CICIDS2017. The robustness analysis further reveals that FWLA maintains 83% accuracy even when 20% of clients are malicious, outperforming FedAvg and FedSGD by 12%. These improvements stem from FWLA’s ability to suppress poisoned updates through iterative weight adjustments, validated by ablation studies showing a 3.3% accuracy drop when removing residual testing. Nonetheless, FWLA has its limitations in that when the number of clients is large, it may lead to increased resource consumption, so future work will concentrate on developing strategies to reduce these resource costs.},
  archive      = {J_IJCIS},
  author       = {Ning, Yafei and Zhang, Zirui and Li, Hu and Xia, Yuhan and Li, Ming},
  doi          = {10.1007/s44196-025-00819-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A federated weighted learning algorithm against poisoning attacks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Enhanced YOLOv8 for efficient parcel identification in disordered logistics environments. <em>IJCIS</em>, <em>18</em>(1), 1-2. (<a href='https://doi.org/10.1007/s44196-025-00831-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Yu, Han and Fengshou, Zhang and Gaoshuai, Zhuang and Yuanhao, Qu and Aohui, He and Qingyang, Duan},
  doi          = {10.1007/s44196-025-00831-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Correction: Enhanced YOLOv8 for efficient parcel identification in disordered logistics environments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent monitoring and treatment system of heavy metal contaminated soil based on artificial bee colony algorithm and edge computing nanotechnology. <em>IJCIS</em>, <em>18</em>(1), 1-15. (<a href='https://doi.org/10.1007/s44196-025-00834-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CGABC algorithm improves the convergence speed of the algorithm by crossing the solution generated by the neighborhood search of bees with the current global optimal solution. By selecting reasonable cross-operation coefficients to balance the algorithm's global optimization ability and local search ability, and adding random interference terms to increase population diversity. By optimizing the test function, it was verified that the performance of the CGABC algorithm is superior to the ABC algorithm and GABC algorithm. In the context of environmental remediation, particularly in the treatment of heavy metal-contaminated soil, optimizing treatment methods through advanced computational algorithms has become increasingly critical. This paper introduces an intelligent monitoring and treatment system based on the Artificial Bee Colony (ABC) algorithm, incorporating edge computing and nanotechnology for the remediation of heavy metal-contaminated soils. To improve the efficiency and effectiveness of the system, we propose two enhanced algorithms: the Cooperative Global Artificial Bee Colony (CGABC) and Chaotic Tabu Search Artificial Bee Colony (CTABC) algorithms. Magnetic core iron trioxide was prepared by co-precipitation method, with an average particle size of 10–15 nm. Magnetic mesoporous nanoparticles Fe3O4 and SiO2 with uniform particle size and good dispersion were prepared using the Stober method. The specific surface area before calcination was 305.8 m2/g, and the pore size was 2.5 nm. The average particle size after calcination was around 200 nm. EDTA ferric oxide and silica were obtained by modification with N triacetate sodium salt. Fourier transform infrared spectroscopy showed successful EDTA modification with a specific surface area.},
  archive      = {J_IJCIS},
  author       = {Hu, Ke and Li, Dongdong and Cui, Xiaolei and Hu, Donghua and Chen, Junliang and Zhuan, Shaopeng and Chang, Hao and Zhang, Yaping and An, Tingting and Zhang, Juqin},
  doi          = {10.1007/s44196-025-00834-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Intelligent monitoring and treatment system of heavy metal contaminated soil based on artificial bee colony algorithm and edge computing nanotechnology},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neutrosophic gompertz distribution: Applications in analyzing complex environmental datasets. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00836-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world problems are characterized by uncertainty, indeterminacy, vagueness, and ambiguity. In situations where the random variable has ambiguous values, classical probability distributions may not be effective. Instead, neutrosophic probability distributions often yield better results. The Gompertz distribution, widely applied across various fields, is extended in this research to introduce the neutrosophic Gompertz distribution (NGoD) for modeling ambiguous data. Key neutrosophic properties such as moments, Shannon entropy, and reliability measures of NGoD are derived. Neutrosophic parameters are estimated using maximum likelihood estimation, and a simulation study is conducted to examine parameter behavior and compare the indeterminacy between parameters. Finally, the NGoD is applied to two real-world ambiguous data sets, demonstrating the effectiveness and suitability of the neutrosophic Gompertz distribution in uncertain contexts. The analysis shows that the neutrosophic Gompertz model is appropriate, reasonable, and useful for such applications.},
  archive      = {J_IJCIS},
  author       = {Saleem, Muhammad and Bashir, Shakila and Tayyab, Ammara and Aslam, Muhammad and Rasul, Mujahid},
  doi          = {10.1007/s44196-025-00836-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Neutrosophic gompertz distribution: Applications in analyzing complex environmental datasets},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enterprise tax assessment and risk avoidance based on deep learning. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00837-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business tax assessments ensure financial solidity and regulatory adherence. Inaccurate or late tax returns can harm an organization’s reputation and economic health. Conventional tax risk appraisal techniques usually fail to estimate taxable input risks and future fiscal burdens correctly and hence are left with ineffective mitigation of risks. The proposed method forecasts the possibilities of taxable inputs and their depreciation, which results in risks over different financial quarters. The cumulative taxable input risks are updated based on the previous inputs and their claimable part to improve the risk prediction. In this prediction process, deep learning is employed; this learning model is designed with two conditional layers. The first conditional layer is responsible for identifying the taxable inputs, and the second is responsible for determining the risks due to external input changes. These two factors are combined using the previous risk factor impact to verify their existence. Based on this existing factor, the number of assessments is increased or benchmarked for further audit. The topic model accurately predicts taxable input risk by financial quarters, continuously refining risk estimates based on the incorporation of prior data. Continuous learning and benchmarking enable the model to adapt to changing tax conditions. By incorporating deep learning into tax evaluation, businesses can enhance financial stability, streamline risk management, and facilitate improved compliance. The approach simplifies business complexity and allows for more precise tax planning.},
  archive      = {J_IJCIS},
  author       = {Lan, Yali},
  doi          = {10.1007/s44196-025-00837-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enterprise tax assessment and risk avoidance based on deep learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A global attention mechanism-based EfficientNet model for road pavement-type identification. <em>IJCIS</em>, <em>18</em>(1), 1-18. (<a href='https://doi.org/10.1007/s44196-025-00842-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate pavement-type recognition remains a critical challenge for intelligent transportation systems. However, the general CNN-based methods, such as ResNet and VGG, exhibit significant limitations when addressing the high degree of similarity between surface modifications and dissimilar pavements caused by changes in illumination or partial shading. To address the challenges posed by complex texture variations and surface modifications across road pavements, in this study, an enhanced method for pavement-type identification is proposed. First, an application-oriented dataset encompassing seven pavement types is constructed based on existing open-source road surface classification datasets. Secondly, the EfficientNet-Global Attention Mechanism (GAM) model is developed through the integration of a GAM module into the EfficientNet architecture. Within this model, the GAM module undergoes a process where it synergistically refines channel–spatial features, utilizing 3D permutation and multilayer perceptron operations. This enables the effective isolation of discriminative patterns, such as crack density, from complex backgrounds. Then, to mitigate inter-class confusion, a label smoothing strategy is implemented, while cosine learning rate decay is employed to ensure stable convergence during training. The experimental results demonstrate that the proposed model achieves high-precision recognition of various pavement types, with an accuracy rate of 98.11%, while simultaneously maintaining computational efficiency.},
  archive      = {J_IJCIS},
  author       = {Ni, Zhe-Yuan and Wang, Jun-Cheng},
  doi          = {10.1007/s44196-025-00842-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A global attention mechanism-based EfficientNet model for road pavement-type identification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Spark-based label diffusion and label selection community detection algorithm for metagenome sequence clustering. <em>IJCIS</em>, <em>18</em>(1), 1-3. (<a href='https://doi.org/10.1007/s44196-025-00850-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Wu, Zhengjiang and Wu, Xuyang and Luo, Junwei},
  doi          = {10.1007/s44196-025-00850-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Correction: Spark-based label diffusion and label selection community detection algorithm for metagenome sequence clustering},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing automotive networks from DoS and fuzzy attacks with optimized LSTM models. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00782-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligently connected automobiles have come a long way thanks to the deep integration of cutting-edge networked gadgets and advancements in automotive technology. The increasing connectivity of vehicles has introduced significant cybersecurity concerns, particularly in controller area network (CAN) bus systems, which are crucial for vehicle communication. For this, the research presents a novel hybrid model combining and integrating a long short-term memory (LSTM) neural network and bacterial foraging optimization (BFO) technique to address challenges like identifying fuzzy and denial of service (DoS) attacks on the CAN bus system. The model’s efficacy is demonstrated by evaluating various cyber-attacks from the Car-Hacking dataset. An adaptive feature selection method using BFO to identify optimal CAN bus characteristics for accurate attack detection. The LSTM-based temporal pattern recognition system detects anomalous message sequences and real-time countermeasures for mitigating DoS and fuzzy attacks. Reducing attack detection time to 0.0838 s, an enhancement over LSTM-AE, suggests improving detection speed. With a higher precision of 94.6% and F1-scores of 95.8%, LSTM-BFO outperforms other models based on accuracy, precision, and F1-score under the current experimental setup.},
  archive      = {J_IJCIS},
  author       = {Dennyson, W. Beniel and Jothikumar, C.},
  doi          = {10.1007/s44196-025-00782-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Securing automotive networks from DoS and fuzzy attacks with optimized LSTM models},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy encryption search scheme and data verification mechanism based on blockchain. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00804-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of blockchain technology, the security of blockchain data verification has received increasing attention. A fuzzy encryption search scheme and data verification mechanism are proposed in the study. First, privacy protection of the raw data is achieved through a hierarchical Mini Batch K-Means clustering algorithm and locally sensitive hash functions. Second, the study proposes a blockchain data verification mechanism that employs a fuzzy encryption search scheme, in conjunction with cloud storage and a fuzzy encryption algorithm. This mechanism is designed to ensure data integrity and confidentiality. When the number of attributes was 30, the initialization time cost of the proposed fuzzy encryption search scheme was 98 ms, which was reduced by about 51.6% compared to the blockchain-assisted sorting method. The encryption time cost of the proposed search method was 61 ms, which was reduced by about 35.6% compared to the multi-permission data access method. When the number of parallel transactions reached 1000, the transaction duration for smart contract Upload was 23.68 s, while the transaction duration for Search was 24.36 s. The proposed fuzzy encryption search scheme and data verification mechanism not only protect data privacy and ensure data integrity and confidentiality, but also have high search efficiency and low time overhead, providing better security for blockchain data.},
  archive      = {J_IJCIS},
  author       = {Li, Kuan},
  doi          = {10.1007/s44196-025-00804-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Fuzzy encryption search scheme and data verification mechanism based on blockchain},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-strategy improved horned lizard optimization algorithm and its application in engineering optimization. <em>IJCIS</em>, <em>18</em>(1), 1-31. (<a href='https://doi.org/10.1007/s44196-025-00824-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address NP-hard optimization challenges prevalent in engineering applications, meta-heuristic algorithms are highly regarded for their ability to provide high-quality solutions. In this study, a multi-strategy improved horned lizard optimization algorithm (MSHLOA) is proposed to overcome the limitations of the standard HLOA in terms of premature convergence and slow optimization search. The algorithm is innovated through four synergistic strategies: (1) logistic chaotic population initialization to enhance initial solution diversity; (2) dynamic lens imaging-based adversarial learning to enhance global search capability; (3) sub-linear probability decay selection-based crisscross strategy to effectively break through dimensional local optima; and (4) golden sine factor-guided local exploitation to balance the trade-off between exploration and exploitation. Experimental validation on 15 benchmark functions and the CEC2021 test set demonstrates the superior performance of MSHLOA, with an overall effectiveness improvement of 53.35% compared to standard HLOA. Statistical analyses including the Wilcoxon rank sum test (p < 0.05), Friedman’s test, and solution distribution visualization validate the robustness of the algorithm against local optimal stagnation. In the engineering optimization example, the average cost of the pressure vessel problem was reduced by 55.6%, while the optimal value with the lowest standard deviation verified its stability in terms of solution accuracy, convergence speed, and stability. These advances establish the computational efficiency and reliability of MSHLOA for engineering problems, providing a general example of an augmented meta-heuristic algorithm that offers a new, more efficient solution to engineering structural optimization problems.},
  archive      = {J_IJCIS},
  author       = {Li, Yancang and Zhang, Jinfan and Jin, Zidong and Qiao, Weitao},
  doi          = {10.1007/s44196-025-00824-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A multi-strategy improved horned lizard optimization algorithm and its application in engineering optimization},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On generalized overlap and grouping indices in n-dimensional contexts. <em>IJCIS</em>, <em>18</em>(1), 1-16. (<a href='https://doi.org/10.1007/s44196-025-00796-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overlap and grouping indices are functions measuring, respectively, the fuzzy intersection and fuzzy union of two fuzzy sets. They have been applied successfully in several fields, such as in interpolative fuzzy systems, fuzzy rule-based classification systems and comparison of fuzzy inference rules. Overlap and grouping indices can be built employing overlap and grouping functions, respectively, which are possibly non-associative aggregation functions with features that provide good results when applied to practical bivariate problems. Many studies have generalized the concepts of overlap and grouping functions to be applied in n-dimensional problems. However, the concepts of overlap/grouping indices have not been generalized in similar pattern. Since the associative property may not hold, their application in n-dimensional domains, for comparing more than two fuzzy sets at a time, is not immediate, which limit their application in such contexts. The objective of this paper is to introduce the concepts of n-dimensional and general overlap/grouping indices, with special attention to the development of their construction methods based on generalized overlap/grouping functions. As an application example, we introduce the concept of n-dimensional Jaccard index, with a construction method based on n-dimensional overlap/grouping indices, providing an n-dimensional fuzzy set similarity score.},
  archive      = {J_IJCIS},
  author       = {Asmus, Tiago and Dimuro, Graçaliz and Lucca, Giancarlo and Marco-Detchart, Cedric and Santos, Helida and Camargo, Heloisa and Bustince, Humberto},
  doi          = {10.1007/s44196-025-00796-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {On generalized overlap and grouping indices in n-dimensional contexts},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved TODIM method for probabilistic linguistic MAGDM based on new distance measure. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00806-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic linguistic term sets (PLTSs), which assign different weights to various linguistic terms, offer an efficient framework for expressing preferences. Meanwhile, the TODIM method, grounded in prospect theory, is adept at incorporating the cognitive behaviors of decision-makers into the decision-making process. In this paper, we extend the TODIM method to solve multi-attribute group decision making (MAGDM) problems with PLTSs. At first, we extend the Frank operators to PLTSs and propose the probabilistic linguistic Frank weighted averaging (PLFWA) operator based on adjusted rules of PLTSs. Further, some desirable properties of them are studied. Meanwhile, we present an innovative distance measure, deeply anchored in linguistic scale functions, designed to overcome the shortcomings of current distance metrics. What’s more, the combined weights for attributes can be obtained by the criteria importance though intercriteria correlation (CRITIC) method and the best-worst method (BWM) and the steps of the extended TODIM method for PLTSs are proposed. Finally, a numerical example for the purchase selection of electric vehicles is given, and some sensitivity and comparative analysis are used to illustrate the effectiveness and rationality of this new method.},
  archive      = {J_IJCIS},
  author       = {Li, Ke and Xu, Lei and Liu, Yi and Wang, Hongjuan and Rong, Yuan},
  doi          = {10.1007/s44196-025-00806-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Improved TODIM method for probabilistic linguistic MAGDM based on new distance measure},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defects detection in screen-printed circuits based on an enhanced YOLOv8n algorithm. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00815-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection is a crucial task in screen-printed circuit (SPC) production, where image processing method based on deep learning is often used. This field frequently encounters challenges, such as minute surface defects, a large number of model parameters, and high computational complexity. To address these challenges, a self-made SPC defect data set and an enhanced CAAB-YOLOv8n detection algorithm were developed. A CAD module was integrated into the backbone network to improve the model’s ability to detect bar-shaped features. In addition, the ASF feature fusion and RMT modules were combined to construct the ASF-CR neck structure, which enhances the model’s capability to detect small, localized defects. To expedite inference speed, the DBB-Head reparameterization module was incorporated. Experimental results show that the enhanced algorithm achieves 88.4 $$\%$$ accuracy, a mAP@50 of 90.2 $$\%$$ , and a parameter count of just 33.27 million, with a detection speed of 35.2 frames per second. The real-time requirements for SPC defect detection are met by these findings. This work lays a solid theoretical foundation for subsequent defect traceability and the optimization of printing process parameters.},
  archive      = {J_IJCIS},
  author       = {Zhang, Xinyu and Wang, Jia and Jiang, Dan and Li, Yang and Wang, Xuewei and Zhang, Han},
  doi          = {10.1007/s44196-025-00815-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Defects detection in screen-printed circuits based on an enhanced YOLOv8n algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced global optimization using a novel hybrid sine cosine-gazelle algorithm with brownian motion and lévy flight mechanisms. <em>IJCIS</em>, <em>18</em>(1), 1-55. (<a href='https://doi.org/10.1007/s44196-025-00823-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms are crucial for solving intricate optimization problems in diverse fields. The Sine Cosine Algorithm (SCA), known for its efficiency and simplicity in global search, sometimes struggles with premature convergence and inadequate exploitation. To address these challenges, this study introduces a novel hybrid SCA-Gazelle Optimisation Algorithm (HSCAGOA) by integrating the Gazelle Optimisation Algorithm’s (GOA) exploitation strategy. Inspired by gazelle behaviour, GOA enhances local search capabilities, improving the balance between the exploration and exploitation phases. Additionally, HSCAGOA incorporates Brownian motion and Lévy flight mechanisms to further enhance exploration capabilities. This research rigorously evaluates HSCAGOA through extensive computational experiments on 33 benchmark test problems and six engineering design challenges. Comparisons with classical SCA and various state-of-the-art optimisation algorithms show that HSCAGOA consistently achieves faster convergence and higher solution quality across diverse optimisation landscapes. To validate these results, ranking analysis is conducted using the Wilcoxon rank-sum test and the Wilcoxon signed-rank test, confirming the efficacy of HSCAGOA. Furthermore, employing the Combined Compromise Solution (CoCoSo) method for multi-criteria decision-making enables systematic ranking and comparison of HSCAGOA’s performance against other algorithms. Additionally, a comparative analysis was conducted against renowned CEC competition winners, including LSHADEcnEpSin, LSHADESPACMA, and CMA-ES. HSCAGOA is evaluated through extensive computational experiments involving CEC 2017 benchmark test functions. Moreover, sensitivity analysis is performed to assess the robustness of HSCAGOA under varying configurations, including different population sizes and maximum iteration counts on CEC 2022 benchmark test functions. The findings highlight the algorithm’s adaptability and reliability in addressing complex optimization challenges. In summary, this study introduces HSCAGOA as a robust optimisation framework that mitigates the limitations of traditional SCA. It provides an effective solution for addressing complex real-world optimisation problems across different domains.},
  archive      = {J_IJCIS},
  author       = {Singh, Gyan and Biswas, Saptadeep and Ezugwu, Absalom El-Shamir and Simic, Vladimir and Bera, Uttam Kumar and Saleem, Kashif and Abualigah, Laith},
  doi          = {10.1007/s44196-025-00823-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-55},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhanced global optimization using a novel hybrid sine cosine-gazelle algorithm with brownian motion and lévy flight mechanisms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Min3GISG: A synergistic feature selection framework for industrial control system security with the integrating genetic algorithm and filter methods. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00827-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial control systems (ICS) are crucial for automating and optimizing industrial operations but are increasingly vulnerable to cyberattacks due to their interconnected nature. High-dimensional ICS datasets pose challenges for effective anomaly detection and classification. This study aims to enhance ICS security by improving attack detection through an optimized feature selection framework that balances dimensionality reduction and classification accuracy. The study utilizes the HAI dataset, comprising 54,000 time series records with 225 features representing normal and anomalous ICS behaviors. A hybrid feature selection approach integrating wrapper and filter methods was employed. Initially, a Genetic Algorithm (GA) identified 118 relevant features. Further refinement was conducted using filter-based methods—Symmetrical Uncertainty (SU), Information Gain (IG), and Gain Ratio (GR)—leading to a final subset of 104 optimal features. These features were used to train classification models (Naive Bayes (NB), Random Forest (RF), and Support Vector Machine (SVM)) with a 70:30 train-test split and tenfold cross-validation. The proposed feature selection method significantly improved classification accuracy, achieving 98.86% (NB), 99.91% (RF), and 97.97% (SVM). Compared to the full dataset (225 features), which yielded 97.51%, 99.93%, and 96.17%, respectively, our optimized feature subset maintained or enhanced classification performance while reducing computational complexity. This research demonstrates the effectiveness of a hybrid feature selection approach in improving ICS anomaly detection. By reducing feature dimensionality without compromising accuracy, the proposed method enhances ICS security, offering a scalable and efficient solution for real-time attack detection.},
  archive      = {J_IJCIS},
  author       = {Potharaju, Saiprasad and Tambe, Swapnali N. and Rao, G. Madhukar and Kantipudi, M. V. V. Prasad and Bamane, Kalyan Devappa and Bendre, Mininath},
  doi          = {10.1007/s44196-025-00827-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Min3GISG: A synergistic feature selection framework for industrial control system security with the integrating genetic algorithm and filter methods},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coevolutionary algorithm based on constraints decomposition for constrained multi-objective optimization problems. <em>IJCIS</em>, <em>18</em>(1), 1-34. (<a href='https://doi.org/10.1007/s44196-025-00830-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multi-objective optimization problems (CMOPs) are challenging for evolutionary algorithms (EAs). Due to the interaction of multiple constraints, the constrained Pareto fronts (CPFs) exhibit various complex characteristics, e.g., degeneracy, discontinuity or irregularity. Most algorithms achieve poor convergence and diversity performance on these problems. Therefore, we proposed a coevolutionary framework based on constraints decomposition to solve complex CMOPs. Specifically, this framework decomposes the CMOP into multiple help subproblems with a single constraint, thereby decoupling the complex constraints. Then, multiple subpopulations optimize these subproblems to assist in solving the original problem. In addition, a two-stage strategy is used to fully utilize the auxiliary populations to search for feasible solutions. In addition, an evolutionary state detection strategy based on historical information is proposed, which is used to determine whether the evolution moves to the next stage. The framework can take the advantage of the low complexity of single-constraint problems to help algorithm search the complete feasible regions. Experiments on benchmark problems show that the proposed algorithm is competitive with eight other most representative constrained evolutionary algorithms in terms of convergence and diversity performance.},
  archive      = {J_IJCIS},
  author       = {Li, Guangpeng and Li, Li and Cai, Guoyong},
  doi          = {10.1007/s44196-025-00830-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A coevolutionary algorithm based on constraints decomposition for constrained multi-objective optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on civil aviation airport site selection considering group consensus level under large-scale uncertain information. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00840-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the decision-making challenges posed by large-scale information and complex data sources in civil aviation airport site selection, this paper proposes a novel method that integrates group consensus within a framework of substantial uncertainty. The method comprises five key processes: (1) Evaluation process: Based on the constructed multicriteria evaluation system for airport site selection, the q-Rung Orthopair Fuzzy (q-ROF) information is employed to represent evaluations from large-scale decision makers, which effectively characterizes the uncertainty of information and broadens the evaluative scope. (2) Clustering process: A clustering procedure is designed for large-scale q-ROF evaluation data and weight information of criteria, identifying and removing outliers. (3) Consensus reaching process: Considering the characteristics of q-ROF evaluations and multiplicative preference relations, two adaptive consensus reaching algorithms are developed to enhance group consensus levels, thereby improving the rationality of decision-making results. (4) Weight determination process: Criteria and subcriteria weights are calculated using multiplicative preference weighting approach and a deviation maximization model, respectively, derived from aggregated group evaluations. (5) Ranking process: The q-ROF Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) method is applied, in conjunction with the induced q-ROF information integration paradigm, to comprehensively rank the alternative sites. Finally, the feasibility and effectiveness of the proposed method are demonstrated through a case study of civil aviation branch airport planning in a specific city.},
  archive      = {J_IJCIS},
  author       = {Wang, Rui and Zeng, Jing-Han and Huang, Jing-Yang and Kang, Rui and Yuan, Jiang and Zhong, Qing-Wei},
  doi          = {10.1007/s44196-025-00840-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Research on civil aviation airport site selection considering group consensus level under large-scale uncertain information},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selective audio perturbations for targeting specific phrases in speech recognition systems. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00844-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel approach for creating audio adversarial examples that specifically target speech recognition systems. The proposed method involves adding optimized noise to a particular region of an audio sample that corresponds to a specified word or phrase. By doing so, the generated adversarial example is designed to deceive the targeted model into interpreting it as a modified sentence, where the specified phrase has been altered. This method offers advantages compared to existing techniques, including reduced distortion since noise is only added to the targeted area, and the ability for the attacker to selectively modify or add specific words as desired. The experimental evaluation utilized the Mozilla Common Voice dataset. The results demonstrate that the adversarial examples generated using the proposed method, which only transform the specified word or phrase by adding noise to that specific region, can successfully fool the speech recognition system into misclassifying them as the intended target sentence.},
  archive      = {J_IJCIS},
  author       = {Ko, Kyoungmin and Kim, SungHwan and Kwon, Hyun},
  doi          = {10.1007/s44196-025-00844-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Selective audio perturbations for targeting specific phrases in speech recognition systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incomplete dynamic fuzzy linguistic reasoning approach based on concept lattice. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00845-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, much fuzzy data is described by evaluative linguistic expressions, which typically exhibit dynamic and changing characteristics. To tackle the challenges of dynamic fuzzy knowledge acquisition and reasoning in uncertain environments, this paper proposes an incomplete dynamic fuzzy linguistic reasoning approach based on concept lattices. First, the dynamic fuzzy linguistic concept lattice is constructed based on dynamic fuzzy linguistic formal context, which can represent linguistic information more effectively in dynamic fuzzy environments. Second, to compensate for information loss, an incomplete dynamic fuzzy linguistic formal context completion algorithm involving two-pass completions is proposed. In addition, dynamic fuzzy linguistic rules are extracted using the finer relation of dynamic fuzzy linguistic concept lattices, which are utilized to construct a dynamic fuzzy linguistic rule base. On this basis, antecedent similarity degree of dynamic fuzzy linguistic rules is introduced, thereby an incomplete dynamic fuzzy linguistic reasoning approach is proposed for obtaining decision-making results. Finally, a practical example is used to verify the effectiveness and rationality of the proposed approach.},
  archive      = {J_IJCIS},
  author       = {Zhang, Chuyi and Sun, Deshan and Jia, Nan and Pang, Kuo and Zou, Li and Pedrycz, Witold},
  doi          = {10.1007/s44196-025-00845-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Incomplete dynamic fuzzy linguistic reasoning approach based on concept lattice},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual adapter tuning of Vision–Language models using large language models. <em>IJCIS</em>, <em>18</em>(1), 1-36. (<a href='https://doi.org/10.1007/s44196-025-00853-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision–language models (VLMs) pre-trained on large-scale image–text pairs have shown impressive results in zero-shot vision tasks. Knowledge transferability of these models can be further improved with the help of a limited number of samples. Feature adapter tuning is a prominent approach employed for efficient transfer learning (ETL). However, most of the previous ETL models focus on tuning either prior-independent or prior-dependent feature adapters. We propose a novel ETL approach that leverages both adapter styles simultaneously. Additionally, most existing ETL models rely on using textual prompts constructed by completing general pre-defined templates. This approach neglects the descriptive knowledge that can assist VLM by presenting an informative prompt. Instead of pre-defined templates for prompt construction, we use a pre-trained LLM to generate attribute-specific prompts for each visual category. Furthermore, we guide the VLM with context-aware discriminative information generated by the pre-trained LLM to emphasize features that distinguish the most probable candidate classes. The proposed ETL model is evaluated on 11 datasets and sets a new state of the art. Our code and all collected prompts are publicly available at https://github.com/mrzarei5/DATViL .},
  archive      = {J_IJCIS},
  author       = {Zarei, Mohammad Reza and Akkasi, Abbas and Komeili, Majid},
  doi          = {10.1007/s44196-025-00853-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Dual adapter tuning of Vision–Language models using large language models},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-population optimization framework based on plant evolutionary strategy and its application to engineering design problems. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00779-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization problems are widespread across various fields, including industry, agriculture, and healthcare. Metaheuristic algorithms (MAs) are commonly employed to solve these problems due to their flexibility and robustness. However, despite their success, MAs inspired by plant evolutionary strategies remain underexplored. This paper introduces a novel multi-population optimization framework based on the plant evolutionary strategy (PES_MPOF), which leverages plant evolutionary principles to improve optimization performance by maintaining population diversity and accelerating convergence in complex tasks. PES_MPOF integrates multiple subpopulations, each evolving according to different plant evolutionary models. These subpopulations mimic natural distribution and reproduction strategies, fostering solution diversity through both cooperation and competition. Additionally, PES_MPOF adapts population parameters based on the evolutionary performance of subpopulations, further enhancing its robustness and efficiency. The PES_MPOF algorithm was tested on the IEEE CEC 2020 benchmark suite and several classic engineering design problems. It outperforms other state-of-the-art optimization algorithms, demonstrating significant improvements in global optimization, solution accuracy, and convergence speed. PES_MPOF effectively addresses the challenges of premature convergence and loss of diversity, making it a robust and efficient optimization tool. Its innovative multi-population framework, inspired by plant evolutionary strategies, enhances both exploration and exploitation. Experimental results validate its effectiveness across a broad range of optimization problems, including those with constraints. The part of algorithm’s code will be made available upon the paper’s acceptance: https://github.com/ChengHongwei430/PES_MPOF .},
  archive      = {J_IJCIS},
  author       = {Cheng, Hongwei and Li, Jun and Zhang, Xiaoming and Li, Tingjuan and Zhang, Panpan},
  doi          = {10.1007/s44196-025-00779-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Multi-population optimization framework based on plant evolutionary strategy and its application to engineering design problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving oil pipeline surveillance with a novel 3D drone simulation using dynamically constrained accumulative membership fuzzy logic algorithm (DCAMFL) for crack detection. <em>IJCIS</em>, <em>18</em>(1), 1-36. (<a href='https://doi.org/10.1007/s44196-025-00818-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cracks in oil pipelines pose significant risks to the environment, public safety, and the overall integrity of the infrastructure. In this paper, we propose a novel approach for crack detection in oil pipes using a combination of 3D drone simulation, convolutional neural network (CNN) feature extraction, and the dynamically constrained accumulative membership fuzzy logic algorithm (DCAMFL). The algorithm leverages the strengths of CNNs in extracting discriminative features from images and the DCAMFL’s ability to handle uncertainties and overlapping linguistic variables. We evaluated the proposed algorithm on a comprehensive dataset containing images of cracked oil pipes, achieving remarkable results. The precision, recall, and F1-score for crack detection were found to be 96.5%, 97.3%, and 95.6%, respectively. These high-performance metrics demonstrate the algorithm’s accuracy and reliability in identifying and classifying cracks. Our findings highlight the effectiveness of integrating advanced simulation techniques, deep learning, and fuzzy logic for crack detection in oil pipelines. The proposed algorithm holds promise for enhancing pipeline surveillance, improving safety measures, and extending the lifespan of oil infrastructure. Future work involves expanding the dataset, fine-tuning the CNN architecture, and validating the algorithm on large-scale pipelines to further enhance its performance and applicability.},
  archive      = {J_IJCIS},
  author       = {Muhi, Omar Saber and Farhan, Hameed Mutlag and Kurnaz, Sefer},
  doi          = {10.1007/s44196-025-00818-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Improving oil pipeline surveillance with a novel 3D drone simulation using dynamically constrained accumulative membership fuzzy logic algorithm (DCAMFL) for crack detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive analytics in maternal health: A machine learning approach for classification of preeclampsia. <em>IJCIS</em>, <em>18</em>(1), 1-31. (<a href='https://doi.org/10.1007/s44196-025-00825-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The condition known as preeclampsia is a hypertensive disorder that occurs during pregnancy and has serious implications for both the mother and the fetus. Proper management of the condition depends on early detection of preeclampsia to make a correct prognosis. In this study, we classify pre-eclampsia using three datasets: two of which are the public datasets acquired from Mendeley and Kaggle, respectively, while the third is a real-world clinical dataset obtained from a local hospital. Recursive feature elimination, principal component analysis, correlation-based feature selection, and particle swarm optimization were used to select significant features from the predictor variables of the public datasets. To improve the classification performance, several models were created, with an emphasis on ensemble learning methods. Specifically, we propose three models: the alternative classification models include the Soft Decision Fusion Model, which applies soft-voting; the Stacking-Based Classifier, which is an ensemble stacking; and the Hybrid Soft Stacking Model. These models were assessed in detail concerning their quantitative indicators for the AUC-ROC criterion. The performance of our proposed models in the public datasets was an AUC-ROC of more than 95% and in the clinical dataset an even higher 96%. These ensemble methods accurately show that they have effective results in improving the precision and reliability of pre-eclampsia forecasts. With the help of real and public clinical data, the present work presents an effective and ecological approach that can help healthcare professionals make appropriate and timely decisions about the management of pre-eclampsia. In particular, the results of the Hybrid Soft Stacking Model look quite convincing in terms of predictive value, so the model could be considered a useful tool in the clinical context.},
  archive      = {J_IJCIS},
  author       = {Amin, Pakiza and Gulzar Ahmad, Saima and Khan, Hikmat Ullah and Munir, Ehsan Ullah and Ramzan, Naeem},
  doi          = {10.1007/s44196-025-00825-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Predictive analytics in maternal health: A machine learning approach for classification of preeclampsia},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved hypernymy detection algorithm based on heterogeneous graph neural network. <em>IJCIS</em>, <em>18</em>(1), 1-27. (<a href='https://doi.org/10.1007/s44196-025-00828-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept mapping is a knowledge representation method used to represent and understand concepts, entities, and the relationships between them, which are referred to as hyponymy–hypernymy semantic relations. These relations are primarily used to describe the hierarchical and categorical relationships between different concepts or entities. The detection of hyponymy–hypernymy semantic relations is an important task in the field of natural language processing, crucial to many downstream tasks such as information extraction, automatic reasoning, and personalized recommendations. These tasks often require understanding the semantic relations between concepts or entities in text for more accurate analysis and reasoning. Currently, algorithms for identifying and detecting hyponymy–hypernymy semantic relations face two main challenges: first, candidate hyponymy–hypernymy relation tuples do not exist in the same contextual sentence, failing to meet the co-occurrence requirement; second, distributed algorithms have issues with lexical memory. To address these issues, this paper proposes an improved algorithm for detecting hyponymy–hypernymy relations based on heterogeneous graph neural networks, aiming to detect hyponymy–hypernymy relations in various candidate word sets and then construct a hierarchical system. To meet the co-occurrence requirement of candidate word pairs, an open-source large model is utilized to generate contextual sentences for the candidate pairs. Sub-word features are adopted to capture the intrinsic semantic connections between nested phrases, thus alleviating the issue of reversed predictions for nested phrases. The representation of relation nodes is modified by encoding relation definitions through a pre-trained model, enabling the model to understand the semantic relations between concept nodes. To address the problem of overfitting in traditional graph attention networks, the calculation order of adjacency node aggregation is changed in the heterogeneous graph to capture dynamic attention features. In addition, a pipeline for hierarchical system construction is designed and implemented, combining the divide-and-conquer approach with loop detection algorithms. Compared to baseline metrics, the proposed method achieves a 4.14% improvement in accuracy and a 0.62% increase in F1 score on the EVALution dataset; a 4.89% increase in accuracy and a 0.71% improvement in F1 score on the Bansal dataset; and a 1.05% increase in accuracy, a 2.21% increase in recall, and a 1.79% improvement in F1 score on a self-annotated Chinese dictionary dataset.},
  archive      = {J_IJCIS},
  author       = {Ren, Li and Huang, Jing and Jia, Hai-Tao and Sun, Shu-Bao and Wang, Kai-Shi and Wu, Yi-Le},
  doi          = {10.1007/s44196-025-00828-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Improved hypernymy detection algorithm based on heterogeneous graph neural network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensor infused quantum CNN for diabetes disease prediction and diet recommendation. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00833-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes detection requires evidence-based recommendations that enable people to manage their health. A rising diabetes rate can lead to significant health risks and financial hardships. An early diagnosis and efficient treatment are essential to reduce the effects of diabetes. Therefore, a novel Sensor-infused QUantum CNN for diabetes Identification and Diet recommendation (SQUID) technique has been proposed in this paper, which identifies diabetes in the early stage using an IoT system and provides diet recommendations for reducing diabetes. The proposed SQUID system collects data from remote patients using IoT sensors and uses the Namib Beetle Optimization (NBO) technique to select the features. The prediction phase uses the Quantum CNN technique for classifying the input into diabetes and non-diabetes. After prediction, the suggestion phase will provide the diet recommendation using the fuzzy rule for the person affected with diabetes through the mobile application. The efficacy of the proposed SQUID framework has been assessed using specific parameters such as Accuracy (AC), Precision (PN), F1 score (F1_S), Recall (RL) and Diagnostic Odds Ratio (DOR). The SQUID framework achieves a higher AC of 98.69%, whereas HCBDA, IWBSOA, e-diagnosis and GlucoBreath achieve the AC of 92%, 94%, 96.5% and 97.35% in the diabetes dataset. In the diabetes prediction dataset, the proposed SQUID model achieves a higher accuracy of 98.87%, whereas existing techniques such as HCBDA, IWBSOA, e-diagnosis and GlucoBreath achieve the AC of 91.35%, 93.56%, 97.21% and 96.43% in diabetes prediction dataset respectively.},
  archive      = {J_IJCIS},
  author       = {Kotwal, Jameer and Futane, Pravin and Chavan, Gurunath and Chaudhari, Archana and Jose, Jithina and Khan, Vajid},
  doi          = {10.1007/s44196-025-00833-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Sensor infused quantum CNN for diabetes disease prediction and diet recommendation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of fuzzy decision support in deep learning model of english translation pattern classification. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00839-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {English translation requires an intense knowledge of words, lexical arrangement, and sentence formation. The translation pattern follows either of the above to provide an understandable output. This article assimilates deep learning and fuzzy decision systems to ensure a highly understandable classification of English translations. Integrated translation pattern classification (ITPC) relies on deep learning to classify patterns based on words, lexicons, and sentences. The network is trained for the highest understandable classification output from the translated sentences. The fuzzy decision process is used to validate and extract new possibilities of the translated patterns. The identified patterns (new) increase the chance of translation efficiency of any complex sentence/ words. This process is a single turn of the learning process until the target pattern with the highest efficiency is observed. Based on the number of turns, the training iterations are varied to confine the complexity of pattern classification.},
  archive      = {J_IJCIS},
  author       = {Liu, Jinlian},
  doi          = {10.1007/s44196-025-00839-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Application of fuzzy decision support in deep learning model of english translation pattern classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal medical image fusion in NSST domain in coupled neural systems. <em>IJCIS</em>, <em>18</em>(1), 1-30. (<a href='https://doi.org/10.1007/s44196-025-00841-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image fusion through multiple modalities functions as an essential tool for improved diagnosis because it harmonizes varied imaging results together. The research introduces an enhanced image fusion solution which unites Nonsubsampled Shearlet Transform (NSST) with Coupled Neural P (CNP) Systems to maximize the fusion results between MRI and PET brain images. The proposed system performs image decomposition starting from low and high frequency regions before it applies CNP systems for low-frequency fusion and WF-SML for high-frequency fusion. The proposed method proves superior in merging MRI and PET brain image pairs through experimental testing of 48 pairs when evaluated against seven mainstream fusion methods including CNN-based and neuro-fuzzy models. The proposed methodology grants improvements of 19.2% on Structural Similarity Index (SSIM) as well as 17.8% additional entropy while delivering improved standard deviation values to provide the best possible contrast and texture preservation and information maintenance. Medical image fusion techniques using our method generate detailed observations about Alzheimer’s disease along with brain tumors and neurodegenerative conditions which help medical professionals make better choices. Enhanced medical capabilities in imaging result from this modern fusion approach which improves both perceptible quality and clinical interpretation of combined images.},
  archive      = {J_IJCIS},
  author       = {Satyanarayana, Vella and Mohanaiah, P.},
  doi          = {10.1007/s44196-025-00841-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Multimodal medical image fusion in NSST domain in coupled neural systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel human action recognition model by grad-CAM visualization with multi-level feature extraction using global average pooling with sequence modeling by bidirectional gated recurrent units. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00848-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is essential in many real-world scenarios, such as video surveillance, human–computer interaction, and behavior analysis. Despite the progress in deep learning, issues such as occlusion, distraction from the background, and motion pattern variability still exist, thus restricting the generalization ability of current models. Most methods are based only on spatial or temporal features and cannot efficiently capture both in one framework, causing lower accuracy in realistic situations. In response to these shortcomings, a multilevel feature extraction approach was proposed by integrating spatial and temporal features to improve the action recognition precision. The method captures RGB frames, optical flow, spatial saliency maps, and temporal saliency maps to enable an overall inspection of video streams. Efficient feature extraction was achieved by applying a pre-trained Inception V3 model and then bidirectional gated recurrent units (Bi-GRUs) to include sequential modeling. An attention mechanism was also included to boost the classification process by focusing on key temporal segments. UCF101 and HMDB51 benchmark datasets evaluated the efficiency of the strategy. The model’s accuracy was 98.13% on UCF101 and 81.45% on HMDB51, which validated the superior discrimination ability of the model in processing heterogeneous human actions. These results confirm that the provided framework is an efficient and discriminative action recognition approach, thus suitable for applications requiring extensive motion analysis and real-time deployment.},
  archive      = {J_IJCIS},
  author       = {Manoharan, Jayamohan and Sivagnanam, Yuvaraj},
  doi          = {10.1007/s44196-025-00848-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel human action recognition model by grad-CAM visualization with multi-level feature extraction using global average pooling with sequence modeling by bidirectional gated recurrent units},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized recommendation algorithm for cultural and creative products based on fuzzy decision support system. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00857-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cultural and creative products are extensive, difficult to mass produce, and undersupplied. They can demonstrate their distinctive qualities through multifunctional design while raising cultural and economic value. Three circumstances often emerge when cultural and creative customers purchase stationery: purchasing inclination, purchasing behavior, and reflection. Limited mass manufacturing, undersupply, and the need to balance cultural and commercial value are common cultural and creative product issues. Hence, this study proposes a fuzzy context-aware neural recommendation algorithm (F-CANRA) to analyze consumers’ purchase behaviors regarding cultural and creative products (CCPs). A graph neural network (GNN) is established by studying users’ cultural and creative product-buying behavior. In this environment, a fuzzy decision support system (FDSS) could assist in making product recommendations for artistic and innovative actions (e.g., artwork, music, crafts, etc.) by taking into account a variety of criteria, including users’ preferences, current trends, cultural importance, and the fuzziness of these factors (e.g., a user’s approximate preference for “modern yet traditional” designs). This article proposes a decision-making framework utilizing a fuzzy decision support system (FDSS) integrated with a fuzzy analytic hierarchy process (FAHP). This framework aims to prioritize design elements, develop cultural and creative design components, and identify and analyze the key criteria influencing user needs. This research shows that the strategy may assist industrial designers in creating better color schemes for creative and cultural products by incorporating group users’ visual preferences into purchasing intention via multiuser decision consistency. The simulation outcome demonstrates that the suggested model increases the purchase intention prediction ratio of 97.8%, customer emotional satisfaction ratio of 98.5%, product development ratio of 96.2%, recommendation accuracy ratio of 95.2%, and product design costs of 7.3% compared to other existing models. As these outcomes show, the approach can help companies and industrial designers create CCPs that are culturally important and financially feasible. The F-CANRA platform provides a strong answer to the challenges of tailored product suggestions and well-informed design choices by connecting sophisticated algorithms with the cultural industry's complex requirements.},
  archive      = {J_IJCIS},
  author       = {Shi, Lin and Yang, Xiaoqing},
  doi          = {10.1007/s44196-025-00857-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Personalized recommendation algorithm for cultural and creative products based on fuzzy decision support system},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing plant disease detection: Incorporating advanced CNN architectures for better accuracy and interpretability. <em>IJCIS</em>, <em>18</em>(1), 1-15. (<a href='https://doi.org/10.1007/s44196-025-00835-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have proven effective in automated plant disease diagnosis, significantly contributing to crop health monitoring. However, their limited interpretability hinders practical deployment in real-world agricultural settings. To address this, we explore advanced CNN architectures, namely ResNet-50 and EfficientNet, augmented with attention mechanisms. These models enhance accuracy by optimizing depth, width, and resolution, while attention layers improve transparency by focusing on disease-relevant regions. Experiments using the PlantVillage dataset show that basic CNNs achieve 46.69% accuracy, while ResNet-50 and EfficientNet attain 63.79% and 98.27%, respectively. On a 39-class extended dataset, our proposed EfficientNet-B0 with attention (EfficientNetB0-Attn), integrating an attention module at layer 262, achieves 99.39% accuracy. This approach significantly enhances interpretability without compromising performance. The attention module generates weights via backpropagation, allowing the model to emphasize disease-relevant image regions, thereby enhancing both accuracy and interpretability.},
  archive      = {J_IJCIS},
  author       = {González-Briones, Alfonso and Florez, Sebastián López and Chamoso, Pablo and Castillo-Ossa, Luis F. and Corchado, Emilio S.},
  doi          = {10.1007/s44196-025-00835-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhancing plant disease detection: Incorporating advanced CNN architectures for better accuracy and interpretability},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational brain imaging framework for neurological mapping and disorder classification using multimodal image processing. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00852-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex neurological illnesses necessitate modern facilities’ computational methods for precise mapping and categorization of brain activities. For the diagnosis and tracking of conditions including epilepsy, Parkinson’s disease, and Alzheimer’s, precise brain mapping is essential. When it comes to diagnostic accuracy and scalability, traditional methods frequently encounter problems, including data heterogeneity, low resolution, and computational inefficiencies. Dealing with large-scale imaging datasets, enhancing the reliability of disease categorization in varied patient groups, and overcoming the limitations of cross-modality data fusion are the primary challenges. The multimodal neuro-cognitive imaging computational technique (MN-CICT) has been suggested in this research to overcome the above challenges. MN-CICT thoroughly extracts and analyses structural and functional brain data by integrating multiple imaging modalities, such as MRI, fMRI, PET, and CT. Improved resolution and interpretability of neurological mappings are achieved by the use of adaptive feature extraction, multimodal data fusion, and advanced machine learning techniques by MN-CICT. Because of its focus on computing efficiency, the framework additionally seems well suited for use in real-time scenarios. Neurodegenerative disease research, therapy planning, and clinical diagnostics are among the many areas that can benefit greatly from the framework. In comparison to current methods, the simulation results show a considerable decrease in processing time and an improvement in classification accuracy. This exemplifies its promise to enhance diagnostic results and simplify neuroimaging operations. The MN-CICT is a revolutionary method to brain imaging, which paves the way for the development of novel applications in the fields of brain–computer interfaces, customized medicine, and automated diagnostics.},
  archive      = {J_IJCIS},
  author       = {Karthikeyan, S. and Muthu Kumar, B. and Kiran, M. L. and Srivatsan, K.},
  doi          = {10.1007/s44196-025-00852-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Computational brain imaging framework for neurological mapping and disorder classification using multimodal image processing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusion of visible and infrared images using a reinforcement learning system based on fuzzy logic and convolution optimized with wild horse algorithm. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00856-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To create a fusion image with more information, complementary data from similar images taken by multiple types of sensors are combined into a single image through the process of infrared and visible image fusion. Existing fusion approaches based on machine learning still struggle with how to better preserve the detail information in the source images. To improve and supplement the fusion image information, a hybrid reinforcement learning system based on fuzzy logic and convolutional network and filtering designed to fuse visible and infrared images is used in this work. This hybrid reinforcement learning system was optimized using algorithms including wild horse optimization (WHO), genetic algorithm (GA), and particle swarm optimization (PSO) to improve specific fusion metrics such as image correlation, similarity coefficient, image entropy, and signal-to-noise ratio. The system aims to preserve the detail information in the final image by adding thermal information from the infrared image to the visible image, which is achieved by performing a series of fusion operations on the input images, including image detail enhancement with the help of a convolutional network and meaningful image fusion with fuzzy logic semantic model and filtering operations to increase clarity. As part of the validation process, the advantages of the proposed algorithm were compared with other classical algorithms using the TNO dataset. Successfully, the proposed method has been able to increase the SSIM parameters to 1.8594 and PSNR to 61.42. Based on the experimental results, our proposed method outperforms previous fusion methods in terms of subjective and objective evaluations.},
  archive      = {J_IJCIS},
  author       = {Zarimeidani, Mahvash and Amirabadi, Amir and Amiri, Nasrin and Ahanian, Iman and Es’haghi, Siavash},
  doi          = {10.1007/s44196-025-00856-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Fusion of visible and infrared images using a reinforcement learning system based on fuzzy logic and convolution optimized with wild horse algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid deep learning-ViT model and a meta-heuristic feature selection algorithm for efficient remote sensing image classification. <em>IJCIS</em>, <em>18</em>(1), 1-37. (<a href='https://doi.org/10.1007/s44196-025-00838-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep learning techniques driven by large datasets demonstrate the significant impact of feature learning in remote sensing for land use and cover classification, particularly exemplified by CNNs. While the pre-trained models showed good classification performance, they struggled to classify remote-sensing images with high precision accurately. In this study, we introduced XNANet, a self-attention-based CNN network for image classification. Bayesian optimization has been used to initialize the hyperparameters of the proposed model to improve training on the radiographic images. We suggested a novel network-level approach via the fusion of deep structure utilizing tiny-32 ViT and XNANet. For the first time, the tiny-32 vision transformer architecture has been utilized for RS images and combined with XNANet through network-level fusion. Following the fusion process, the model focused on RS image datasets and obtained deep features from the self-attention layer. The features that have been extracted are subject to selection, utilizing a novel meta-heuristic feature selection algorithm, RF-DE. The selected features are categorized using three popular classifiers. The proposed architecture’s experimental process was executed on the AID, RSSCN7, and SIRI-WHU datasets, resulting in accuracies of 98.9%, 99.3%, and 99.7%, respectively. Similarly, RF-DE was evaluated against six popular feature selection algorithms, yielding accuracies of 98.9%, 99.3%, and 99.7%, respectively. An in-depth statistical analysis was conducted to evaluate the suggested ensemble and RF-DE and demonstrate that the fusion model attained enhanced accuracy with RF-DE. Furthermore, recent techniques and proposed methods are compared, demonstrating enhanced precision, recall, and accuracy.},
  archive      = {J_IJCIS},
  author       = {Ahmed, Bilal and Naqvi, Syed Rameez and Akram, Tallha and Peng, Lu and Almarshad, Fahdah},
  doi          = {10.1007/s44196-025-00838-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A hybrid deep learning-ViT model and a meta-heuristic feature selection algorithm for efficient remote sensing image classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on intelligent prediction of inorganic building materials performance. <em>IJCIS</em>, <em>18</em>(1), 1-44. (<a href='https://doi.org/10.1007/s44196-025-00843-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction industry is crucial for economic and social development. Inorganic materials, which rely on natural minerals and are affected by uncertainties, hold a large share in the construction market. As building materials are from process—intensive industries, complex and continuous processing magnifies deviations, directly affecting product quality. Computational intelligent methods are effective for accurately predicting product quality. This paper focuses on inorganic building materials and systematically reviews computational intelligent techniques in this field. It comprehensively explores 234 related studies in 6 key areas (concrete, ceramics, glass, clay bricks, cement, and steel), compiles prediction models, evaluates them, analyzes model configurations and properties to gain insights into the field and identify optimal approaches. It points out model limitations, such as high computational costs, data-hungry, and suggests future research directions like practicality and promoting green initiatives through material circulation.},
  archive      = {J_IJCIS},
  author       = {Li, Mengru and Zhang, Zhenya and Zeng, Xianyi and He, Zhenglei},
  doi          = {10.1007/s44196-025-00843-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-44},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A systematic review on intelligent prediction of inorganic building materials performance},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization scheduling of multiple heterogeneous energy sources. <em>IJCIS</em>, <em>18</em>(1), 1-16. (<a href='https://doi.org/10.1007/s44196-025-00822-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large-scale emergence of photovoltaic and wind power generation has led to significant changes in the energy structure, thereby creating new challenges in the optimization and control of diverse and heterogeneous energy sources. Therefore, numerous researchers have conducted extensive studies aimed at improving the utilization rate of these diverse energy sources and reducing waste. This article reviews the research background, objectives, current status, and prospects of optimizing energy. It introduces the challenges and opportunities that energy systems face with the development of renewable energy and energy storage technologies. The discussion revolves around the methods of multi-source heterogeneous energy optimization scheduling to achieve intelligent scheduling and efficient operation of energy systems. The study summarizes the mainstream mathematical modeling and optimization algorithms, intelligent optimization techniques, and real-time data processing technologies. It compares the significant roles of different scheduling methods in improving the efficiency of heterogeneous energy utilization and reducing energy waste, as well as their practical application effects and limitations. By summarizing the role of multi-source heterogeneous energy optimization scheduling in energy system optimization and the current research status, this article aims to provide reference and guidance for future research and practical implementation.},
  archive      = {J_IJCIS},
  author       = {Zhao, Ying and Yu, Zhiwen and Wang, Xiaobin and Tang, Jianlin and Lin, Xiaoming and Zhang, Fan and Qian, Bin},
  doi          = {10.1007/s44196-025-00822-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimization scheduling of multiple heterogeneous energy sources},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entrepreneurial skill augmented neural network (ESANN): A deep learning approach for enhancing entrepreneurial competencies in teachers. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00851-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern education requires teachers to develop entrepreneurial competencies because traditional training practices fail to provide individualized training methods. The developments in deep learning provide educational institutions with new data-based methods to develop professional development training programs. Entrepreneurial Skill Augmented Neural Network (ESANN) presents itself as a deep learning framework that serves to individualize educator training while upgrading their acquisition of entrepreneurial skills. The model leverages Convolutional Neural Networks (CNNs), Bidirectional Long Short-Term Memory (Bi-LSTM) networks, and a Transformer-based recommendation engine to deliver adaptive, real-time feedback and optimize training pathways based on individual learning patterns. A model training process used data from 300 teachers, including their activity records, questionnaires, and evaluation feedback, to achieve valid model outcomes. Educators received assessment through an established entrepreneurial competency framework at the commencement and completion of ESANN-based instruction. The quantitative evaluation included measuring skill score improvement between participants in the ESANN group and those in traditional training through accuracy metrics evaluated within ten-fold cross-validation methods. Members of the training group that received ESANN training outperformed traditional training participants by 24% in their entrepreneurial competency scores. The developed model delivered 92.5% predictive accuracy, surpassing other baseline approaches regarding performance efficiency and accuracy. ESANN serves as a deep learning framework for educational development, which provides individualized and flexible learning solutions for teacher training on a large scale. These findings highlight artificial intelligence's (AI) transformative potential in fostering entrepreneurial education through real-time, data-informed feedback.},
  archive      = {J_IJCIS},
  author       = {Li, Jian},
  doi          = {10.1007/s44196-025-00851-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Entrepreneurial skill augmented neural network (ESANN): A deep learning approach for enhancing entrepreneurial competencies in teachers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sustainable production inventory model for power-pattern demand with carbon emissions and shelf life considerations. <em>IJCIS</em>, <em>18</em>(1), 1-32. (<a href='https://doi.org/10.1007/s44196-025-00861-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Growing environmental concerns, especially those related to carbon emissions from production and supply chain activities, have highlighted the need for sustainable practices. Implementing policies that incentivize low-carbon operations is crucial. To foster sustainable development, implementing penalties for high-emission commercial activities is crucial. This study investigates a sustainable production-inventory system for a product with a power-pattern demand, considering limited shelf life and gradual deterioration. The analysis integrates carbon emissions from stockholding, transportation, and deterioration, along with the impact of shortages on total costs. Two distinct scenarios are examined: Case I (without deterioration) and Case II (with deterioration). Each case is further divided into two models: one allowing shortages and the other prohibiting them. The primary objective is to determine an optimal production-inventory policy that maximizes profit per unit time while accounting for carbon emission taxes related to transportation, storage, and deterioration. Numerical results demonstrate that the total cost in Case I (without deterioration) is significantly lower than in Case II (with deterioration), with reductions of $$24.89\%$$ and $$22.02\%$$ , respectively. These findings underscore the financial implications of deterioration and shortages on sustainable inventory management. The proposed model offers valuable insights for decision-makers seeking to optimize production cycles, manage carbon footprints, and develop sustainable supply chain policies.},
  archive      = {J_IJCIS},
  author       = {Suvetha, R. and Rangarajan, K. and Dey, Bikash Koli and Alrasheedi, Adel Fahad and Ivkovic, Nikola and Jana, Chiranjibe},
  doi          = {10.1007/s44196-025-00861-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A sustainable production inventory model for power-pattern demand with carbon emissions and shelf life considerations},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving aerobics posture evaluation by transfer learning: Humanized computational application of BERT-PTA domain adaptive methods. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00867-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the influence of datasets, traditional pose evaluation methods have insufficient generalization ability, high computational resource requirements, and low efficiency. To address this issue, this article applied transfer learning into the field of aerobics posture evaluation and achieved automation and objectivity of posture evaluation through BERT-PTA (Bidirectional Encoder Representations from Transformers-Prototype-based Transfer Assistants) domain adaptive methods. BERT and PTA methods were chosen because BERT’s bidirectional language understanding and transfer learning capabilities can effectively adapt to the language instructions in the field of aerobics, and PTA’s posture tracking and humanized computing features provide an accurate and user-friendly solution specifically for the assessment of aerobics poses. First, a BERT-PTA model was established based on the collection of aerobics posture data. Second, the BERT-PTA model was used to extract features from the preprocessed posture data. Next, a convolutional neural network was used to construct a key point localization model for aerobics poses, and transfer learning was used to train and fine-tune the model. Finally, experimental verification was conducted on using transfer learning to improve aerobics posture evaluation. The results showed that the precision of using transfer learning to improve aerobics posture evaluation was 4.88% and 8.86% higher than that of the other two methods, respectively. The recall rate of using transfer learning to improve aerobics posture evaluation was 3.45% and 7.14% higher than that of the other two methods, respectively. The evaluation efficiency of using transfer learning to improve aerobics posture evaluation was 6.52–7.69% higher than that of the other two methods, respectively. In short, using transfer learning to improve aerobics can provide more scientific guidance for aerobics sports.},
  archive      = {J_IJCIS},
  author       = {Zhou, Wenting and Guo, Biao and Cao, Feng},
  doi          = {10.1007/s44196-025-00867-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Improving aerobics posture evaluation by transfer learning: Humanized computational application of BERT-PTA domain adaptive methods},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative dombi aggregation operators in linguistic intuitionistic fuzzy environments for optimizing telecommunication networks. <em>IJCIS</em>, <em>18</em>(1), 1-29. (<a href='https://doi.org/10.1007/s44196-025-00868-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network optimization is accomplished by integrating AI-powered technologies that are industry leading throughout the network lifecycle to optimize network performance in accordance with strategic objectives and maximize return on investment. These technologies utilize live and predictive network data to advance the network to its full potential, proactively resolving performance issues prior to the impact on subscribers. By employing predictive forecasting and active monitoring, these systems also assess future interconnection requirements and determine the optimal time and location to increase capacity to achieve the highest possible return, months in advance. This yields a network that is consistently operational and provides exceptional performance, customized to the strategic business objectives, and prepared to satisfy the growing performance requirements of future 5G use cases. Linguistic intuitionistic fuzzy sets (LIFSs) offer an adequate base to represent and manage unpredictability linked to intuitionistic assessments and linguistic structures. Aggregation operators (AOs) play a critical role in enhancing the decision-making (DM) procedure by adeptly managing preferences and uncertainties in multiple attribute decision-making (MADM) problems. This leads to decisions that are both more accurate and reliable. Dynamic AOs, which adjust to time-varying data, further improve flexibility and precision in DM. This research builds upon these concepts to develop novel AOs, including the LIF dynamic Dombi weighted averaging operator (LIFDyDWA), and the LIF dynamic Dombi weighted geometric operator (LIFDyDWG), and illustrates their key structural properties. An algorithm is also proposed to address the challenges of handling imprecise data in DM using the LIF dynamic Dombi aggregation approaches. These strategies are successfully applied to present a solution to an MADM problem concerning the selection of an optimal strategy to enhance the efficiency of telecommunication network systems to demonstrate their effectiveness and superiority. A comparative analysis is provided to validate the efficacy and advantages of the suggested methods over existing approaches.},
  archive      = {J_IJCIS},
  author       = {Alghazzawi, Dilshad and Hayat, Misbah and Alhamzi, Ghaliah and Baidar, Abdul Wakil},
  doi          = {10.1007/s44196-025-00868-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Innovative dombi aggregation operators in linguistic intuitionistic fuzzy environments for optimizing telecommunication networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-adjusted deep reinforcement learning for portfolio optimization: A multi-reward approach. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00875-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio optimization is a widely studied topic in quantitative finance. Recent advances in portfolio optimization have shown promising capabilities of deep reinforcement learning algorithms to dynamically allocate funds across various potential assets to meet the objectives of prospective investors. The reward function plays a crucial role in providing feedback to the agent and shaping its behavior to attain the desired goals. However, choosing an optimal reward function poses a significant challenge for risk-averse investors aiming to maximize returns while minimizing risk or pursuing multiple investment objectives. In this study, we attempt to develop a risk-adjusted deep reinforcement learning (RA-DRL) approach leveraging three DRL agents trained using distinct reward functions, namely, log returns, differential Sharpe ratio, and maximum drawdown to develop a unified policy that incorporates the essence of these individual agents. The actions generated by these agents are then fused by employing a convolutional neural network to provide a single risk-adjusted action. Instead of relying solely on a singular reward function, our approach integrates three different functions aiming at diverse objectives. The proposed approach is tested on daily data of four real-world stock market instances: Sensex, Dow, TWSE, and IBEX. The experimental results demonstrate the superiority of our proposed approach based on several risk and return performance metrics when compared with base DRL agents and benchmark methods.},
  archive      = {J_IJCIS},
  author       = {Choudhary, Himanshu and Orra, Arishi and Sahoo, Kartik and Thakur, Manoj},
  doi          = {10.1007/s44196-025-00875-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Risk-adjusted deep reinforcement learning for portfolio optimization: A multi-reward approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Many-to-one matching decision for elderly care service considering subjective and objective picture fuzzy preferences. <em>IJCIS</em>, <em>18</em>(1), 1-32. (<a href='https://doi.org/10.1007/s44196-025-00858-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an innovative many-to-one two-sided matching decision method for complex decision problems in the matching of supply and demand of elderly care services is proposed. Faced with the challenges of diversified demand and uneven supply of elderly care services in the context of an aging population, existing matching methods have significant shortcomings in subjective preference expression, objective evaluation integration, and consideration of time factors. Therefore, a many-to-one matching model in picture fuzzy environment to achieve the best matching scheme between the elderly and pension institutions is constructed in this study. By using the minimum variance method and cumulative prospect theory, subjective preference satisfaction and objective matching results for the elderly and pension institutions are obtained, respectively. Combined with regret theory, the two are compared to obtain adjusted satisfaction, and an innovative time satisfaction index based on expected check-in time is proposed to construct a three-objective optimization model that includes virtual subjects. The research in this paper expresses the subjective evaluation of two-sided subjects in the form of picture fuzzy preference relation, which enriches the research of picture fuzzy theory. At the same time, the quality of pension service matching is further improved considering time satisfaction and objective matching results in the pension service model.},
  archive      = {J_IJCIS},
  author       = {Yue, Qi and Huang, He and Hu, Bin and Tao, Yuan and Liu, Liezhang},
  doi          = {10.1007/s44196-025-00858-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Many-to-one matching decision for elderly care service considering subjective and objective picture fuzzy preferences},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep embedded auto-encoder for end-to-end unsupervised image anomaly detection. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00860-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image anomaly detection plays a critical role in industrial quality control, medical diagnostics, and security surveillance, yet existing unsupervised methods often suffer from limited detection accuracy and poor adaptability. To overcome these limitations, we propose UAD-ADC, a novel framework that automatically identifies anomalies in images without requiring labeled training data. Our approach integrates deep representation learning with adaptive clustering to effectively separate normal patterns from anomalies by learning robust feature representations and dynamically refining decision boundaries. A key innovation is our intelligent sample selection mechanism, which enhances model stability by prioritizing high-confidence normal samples during training, along with an iterative optimization strategy that progressively improves anomaly discrimination. Extensive experiments on benchmark datasets demonstrate that UAD-ADC significantly outperforms state-of-the-art unsupervised methods, with particular effectiveness under varying anomaly ratios. These advancements pave the way for more reliable and scalable unsupervised anomaly detection in practical scenarios.},
  archive      = {J_IJCIS},
  author       = {Huang, Xuan and Tang, Hailin},
  doi          = {10.1007/s44196-025-00860-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Deep embedded auto-encoder for end-to-end unsupervised image anomaly detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-UAV trajectory optimization under dynamic threats: An enhanced GWO algorithm integrating a priori and real-time data. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00863-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though the widespread use of multi-UAV systems offers significant tactical and operational advantages, achieving efficient and secure collaborative planning remains a critical challenge in dynamic threat environments. Traditional methods struggle to balance path optimization with threat avoidance, particularly in fluctuating environments where UAVs must adapt to changing threats. To address this, an enhanced Grey Wolf Optimization (GWO) algorithm is proposed for multi-UAV collaborative planning in dynamic threat zones. Our research integrates a priori knowledge of threat zone locations, speeds, and directions with real-time data on the UAVs position relative to the threat zones to effectively manage dynamic threat zones, allowing UAVs to dynamically decide whether to navigate around or through these zones, thus significantly reducing trajectory costs. To further improve search efficiency and solution quality, strategies such as greedy initialization and K-means clustering are incorporated, enhancing the algorithms multi-objective optimization capabilities. Experimental results demonstrate that the dynamic threat zone crossing strategy significantly reduces trajectory costs compared to the traditional bypass strategy. Furthermore, the enhanced GWO algorithm outperforms both the traditional GWO and MP-GWO algorithms in terms of trajectory cost and convergence accuracy. Our approach provides novel insights and methodologies for the advancement of multi-UAV collaborative trajectory planning, while extending the applicability of the GWO algorithm in complex environments},
  archive      = {J_IJCIS},
  author       = {Zhou, Zihan and Guo, Yanhong and Wang, Yitao and Lyu, Jingfan and Gong, Haoran and Ye, Xin and Li, Yachao},
  doi          = {10.1007/s44196-025-00863-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Multi-UAV trajectory optimization under dynamic threats: An enhanced GWO algorithm integrating a priori and real-time data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIP flooding attack detection technology of multi-agent system covert network based on BiGRU algorithm. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00864-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hidden network environment of multi-agent systems is complex and intricate. The data characteristics generated by SIP flood attacks may overlap and confuse with normal traffic characteristics, and the network traffic characteristics will dynamically change over time, thereby affecting the accuracy of SIP flood attack detection. Therefore, an SIP flood detection technique based on BiGRU algorithm is proposed for covert networks in multi-agent systems. This technology is divided into two levels of detection. The primary detection collects and analyzes the hidden network data of multi-agent systems, and determines whether the traffic is abnormal by calculating the Renyi entropy value; abnormal traffic enters the second-level attack detection stage, extracting abnormal traffic from multi-agent covert networks and using the BiGRU model to learn features bidirectionally to determine whether it is an SIP flooding attack. If it is, the result of the SIP flooding attack on the multi-agent covert network is output. The experimental results show that this technology can accurately determine abnormal traffic and accurately detect the time, attacker IP, and attack frequency of SIP flooding attacks in the hidden network of multi-agent systems. The application effect is good.},
  archive      = {J_IJCIS},
  author       = {Wu, Tong and Liu, Hengyu and Li, Tong and Fan, Wei and Hu, Dawei and Bai, Jianshi},
  doi          = {10.1007/s44196-025-00864-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {SIP flooding attack detection technology of multi-agent system covert network based on BiGRU algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced wheat stripe rust segmentation approach using vision transformer model. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00873-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Worldwide, the wheat industry encounters major obstacles caused by stripe rust disease, triggered by the fungus Puccinia striiformis. This disease results in considerable losses of wheat crops and has significant economic repercussions. Accurate detection of crop diseases is vital for sustainable agriculture and food security. By effectively identifying and managing crop diseases, yield losses can be prevented and global food production can be ensured. This not only protects farmers’ livelihoods but also contributes to human health and well-being by safeguarding the food supply. Wheat plays a crucial role in Pakistan’s agriculture, covering 37% of the cultivated land and contributing 70% to total production. Stripe rust, a serious fungal disease, heavily affects wheat yield, causing a global loss of 5.5 million tons annually. Current models exhibit shortcomings in accurately detecting wheat stripe rust, necessitating improvements for more precise identification and diagnosis of the disease. Fortunately, recent advances in deep learning have led to significant improvements in object-detection accuracy, thus offering hope for better disease management. The purpose of this study is to introduce a method to minimize losses by accurately and promptly detecting stripe rust disease, thereby avoiding the need for manual inspection. To achieve this goal, we propose a vision transformer and a hybrid model to identify wheat stripe rust by analyzing multi-spectral and high-resolution image data. Additionally, we utilize two models, ViT-Base/16 and a transformer, which prove to be highly effective in accurately detecting diseases using the same datasets as the vision transformer model. The proposed method provides optimal results with a vision transformer and a hybrid approach with 98% accuracy and 97.9%, respectively. ViT-Base/16 and transformer models achieve an accuracy of 98% and 95%, almost. Using an improved vision transformer, we achieved precise detection of wheat stripe rust compared to previous methods. This has various benefits, such as safeguarding yields, reducing costs, supporting sustainable agriculture, facilitating crop monitoring, and improving disease management.},
  archive      = {J_IJCIS},
  author       = {Usman, Nosheen and Ahmad, Tauqir and Iqbal, Faiza and Altaf, Ayesha and Samee, Nagwan Abdel and Alohali, Manal Abdullah and Ashraf, Imran},
  doi          = {10.1007/s44196-025-00873-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An enhanced wheat stripe rust segmentation approach using vision transformer model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On fuzzy implication functions based on admissible orders on the set of discrete fuzzy numbers. <em>IJCIS</em>, <em>18</em>(1), 1-15. (<a href='https://doi.org/10.1007/s44196-025-00874-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on the construction of logical connectives using total (admissible) orders is a prolific area of study. Using such orders, a new method for constructing implication functions is defined on the set of discrete fuzzy numbers with support of a closed interval of a given finite chain and whose membership values belong to a finite set of fixed values. This method is based on the use of discrete implication functions defined on a finite chain. Furthermore, a bijective correspondence between the set of implication functions on the aforementioned subset of discrete fuzzy numbers and the set of discrete implication functions defined on the discrete chain is shown. Basic properties of these implication functions are thoroughly investigated, concluding that they are preserved under the proposed construction method. This result highlights the robustness and generality of the method, providing a systematic way to extend discrete implication functions to more complex structures while preserving their underlying properties.},
  archive      = {J_IJCIS},
  author       = {González-Hidalgo, Manuel and Massanet, Sebastia and Mir, Arnau and Riera, Juan Vicente and De Miguel, Laura},
  doi          = {10.1007/s44196-025-00874-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {On fuzzy implication functions based on admissible orders on the set of discrete fuzzy numbers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming early breast cancer detection: A deep learning approach using convolutional neural networks and advanced classification techniques. <em>IJCIS</em>, <em>18</em>(1), 1-30. (<a href='https://doi.org/10.1007/s44196-025-00876-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is a major global health problem, with the WHO estimating approximately 2.3 million new cases every year. In this study, we present a new approach to improve the early detection of breast cancer using deep learning methods and visual inspection of histopathological images. In a world where access to a doctor with specialised knowledge is limited, this study attempts to address the important limitations of current diagnostic strategies that facilitate the efficient detection of diseases. In the proposed method, we use transfer learning with a combination of classifiers, such as SVM, decision trees, and K-nearest neighbours, while implementing two different feature extraction approaches, PCA for dimensionality reduction and no PCA. The approach includes a full evaluation system using metrics such as recall, accuracy, precision, and ROC curves to evaluate the performance of the models. This yields major performance gains for almost all classifiers. The experimental results showed that the SVM classifier with PCA feature extraction obtained the best accuracy of 99.5% with 99.2% precision and 99.6% recall, indicating a significant improvement over the current approach. Even without PCA implementation, the Decision Tree classifier also performed well, scoring 99.4% accuracy. In particular, the application of PCA improved the accuracy of the Boosted Tree from 82.9% to 91.01%. The execution times of classifiers varied significantly; for example, SVM, which is the fastest one as of now, with an execution time without PCA of 38.24 s. This study suggests a potential clinical tool that combines advanced deep learning methods and subsequent classification in real healthcare systems to improve breast cancer detection capabilities. This shows that the high accuracy of this framework, coupled with its computational efficiency, makes it an invaluable tool for real-life clinical applications, which could minimise misdiagnosis and lead to better patient outcomes through earlier detection of respiratory viruses worldwide, especially in remote areas with limited health-care resources.},
  archive      = {J_IJCIS},
  author       = {Singh, Arun Kumar and Mazumdar, Bireshwar Dass and Raja, Rohit and Singh, Kamred Udham and Kumar, Ankit and Shah, Mohd Asif},
  doi          = {10.1007/s44196-025-00876-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Transforming early breast cancer detection: A deep learning approach using convolutional neural networks and advanced classification techniques},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A holistic strategy of modified superpixel segmentation and randomized adam hyperparameter tuning with deep learning approaches for the classification of breast cancer from BreakHis images: In the quest for precision. <em>IJCIS</em>, <em>18</em>(1), 1-33. (<a href='https://doi.org/10.1007/s44196-025-00877-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is a prevalent cancer type in women worldwide, and therefore it is necessary to do early detection that is accurate for effective treatment. However, traditional ways of diagnosing through mammogram or histopathological examination may take more time and also require an interpretation from an expert. In the past few years, deep learning techniques especially Convolutional Neural Networks (CNNs) have changed medical imaging by making possible automated diagnosis which is fast. In this study, an integrated breast cancer detection approach using BreakHis images is proposed focusing on balanced accuracy rate analysis to solve imbalanced datasets problem. The methodology starts with pre-processing these images by means of Adaptive Fuzzy Filter which removes the artifacts while improving the image quality. The next step involves superpixel segmentation through SLIC-K-Means-BIRCH followed by feature extraction using SIFT with Bag of Features (BoF). The extracted features are analyzed with machine learning models such as Gaussian Mixture Model (GMM), Decision Tree (DT), Softmax Discriminant Classifier (SDC), SVM using RBF kernel and Naive Bayes Classifier (NBC) as well as deep learning models ResNet-50, VGG16, VGG19 and EfficientNet-B0. Data augmentation techniques such as image rotation and brightness adjustments were applied to enrich our dataset. R-Adam hyperparameter tuning technique was utilized to optimize these deep learning models. Results show that the modified SLIC-K-Means-BIRCH segmentation, when combined with the SIFT with BoF and EfficientNet-B0 optimized with R-Adam, yields a classification accuracy of 99.11% on augmented images. In addition, balanced classification rate analysis confirmed that this method could classify breast cancer from an imbalanced dataset effectively.},
  archive      = {J_IJCIS},
  author       = {Manivannan, Gowri Shankar and Shanmugam, Karthikeyan and Rajaguru, Harikumar and Talawar, Satish V. and Siddaiah, Rajanna},
  doi          = {10.1007/s44196-025-00877-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A holistic strategy of modified superpixel segmentation and randomized adam hyperparameter tuning with deep learning approaches for the classification of breast cancer from BreakHis images: In the quest for precision},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedMDKGE: Multi-granularity dynamic knowledge graph embedding in federated learning. <em>IJCIS</em>, <em>18</em>(1), 1-18. (<a href='https://doi.org/10.1007/s44196-025-00878-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As knowledge is time-sensitive, some researchers have started to focus on dynamic knowledge graphs to provide time-dimensioned knowledge content thus reflecting richer information. But they have not yet combined temporal information at different granularities. Also, in the case of multiple knowledge graphs distributed across different clients, it is of interest to ensure that the knowledge graph embedding representations are learned without exposing data and collaboratively. Therefore, in this paper, we propose a framework for multi-granularity dynamic knowledge graph embedding in federated learning (FedMDKGE), which allows multiple parties to interact securely with temporal information at different granularities. In the client, we present a multi-granularity dynamic knowledge graph embedding model that improves the capability of dynamic knowledge graph embedding representation by focusing on multi-granularity temporal facts from the perspective of knowledge utilization. On the server, we design a multi-granularity aggregation rule to accommodate multi-party information aggregation at different granularities. Finally, we conduct extensive experiments to demonstrate the superior performance of our model. The results on these real datasets show that FedMDKGE considering multi-granularity temporal information performs better than all comparative baselines and interconnect information for multi-party dynamic knowledge graph embedding without exposing data.},
  archive      = {J_IJCIS},
  author       = {Huang, Wei and Chen, Junling and Wang, Dexian and Zhang, Pengfei and Liu, Jia and Li, Tianrui},
  doi          = {10.1007/s44196-025-00878-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {FedMDKGE: Multi-granularity dynamic knowledge graph embedding in federated learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and development of gorilla optimized deep resilient architecture for prediction of agro-climatic changes to increase the Crop–Yield production. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00880-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting climatic changes is considered one of the most important economic parameters as it remains a catalyst for the agricultural system of any country. Climatic data and services are crucial for agriculturalists to withstand the rising frequency of strong meteorological conditions, which negatively impact crop yields. Weather forecasts play a key role in managing resources for agricultural operations, enabling farmers to plan and protect their crops from natural disasters. In addition, global warming has fueled climatic unpredictability, creating challenges like hurricanes that damage the foundational roots of agricultural production. In recent times, Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) techniques have been predominantly adopted for daily forecasting climatic conditions, including rainfall, maximum temperature, and humidity. However, the existing models for climatic prediction require improvements in computational complexity and performance. This research article proposes the ensemble Residual Long Short-Term Memory (R-LSTM) along with Artificial Gorilla Troops Optimized Deep Learning Networks (AGTO-DLN) as a solution for climatic condition prediction to boost crop–yield production. Performance metrics for the proposed model examining precision, F1 score, accuracy, specificity, and recall operate through evaluation using 5,04,647 climatic parameters with various advanced learning techniques. The proposed method achieved 97.2% accuracy and 96.9% precision together with 96.5% recall and 96.6% specificity and 97.5% F1 score. The research proves that the proposed model demonstrates high potential for climate forecast in agricultural production environments consequently boosting crop yields to enhance farmer incomes.},
  archive      = {J_IJCIS},
  author       = {Devarashetti, Deepa and Aravinth, S. S.},
  doi          = {10.1007/s44196-025-00880-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Design and development of gorilla optimized deep resilient architecture for prediction of agro-climatic changes to increase the Crop–Yield production},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-granularity heterogeneous ensemble model for point and interval forecasting of carbon prices. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00881-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent complexities of carbon price fluctuations and the variability of influencing factors make the accurate prediction of carbon emission prices a significant challenge. This study introduces a novel multi-granularity heterogeneous ensemble model for point and interval forecasting. First, three feature selection methods are used to identify key factors affecting carbon price and to construct multiple granular spaces. Second, the distinct features identified through feature selection methods are input into six point forecasting models. Third, the Grey Wolf Optimization (GWO) algorithm is applied to calculate the optimal weights for each individual model. Finally, the interval predictions of carbon prices are obtained by integrating the point predictions with the kernel density estimation (KDE) model. The research results indicate that the proposed model outperforms comparative models in both predictive accuracy and statistical validation, showcasing outstanding predictive performance.},
  archive      = {J_IJCIS},
  author       = {Sha, Di and Zeng, Xianyi and Tran, Kim-Phuc and Xia, Lin and Wang, Ruolin},
  doi          = {10.1007/s44196-025-00881-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A multi-granularity heterogeneous ensemble model for point and interval forecasting of carbon prices},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The two-stage alzheimer’s disease automatic diagnosis algorithm based on ST-MBV3 model. <em>IJCIS</em>, <em>18</em>(1), 1-27. (<a href='https://doi.org/10.1007/s44196-025-00883-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD), commonly referred to as senile dementia, is a progressive and degenerative brain disorder that significantly impacts cognitive function and memory. As the population ages, the need for early and accurate diagnostic tools has become increasingly critical to mitigate the impact of the disease. Recent developments in medical imaging technologies, particularly magnetic resonance imaging (MRI), alongside the application of artificial intelligence (AI), have provided new avenues for improving the early detection and diagnosis of AD. This study focuses on leveraging these advancements to enhance the accuracy of AD diagnosis through a novel two-stage algorithm. The proposed model combines an improved 3D DenseNet segmentation model with an enhanced ST-MBV3 classification model. The first stage of the proposed algorithm involves processing the collected images. The resulting brain MRI images are then segmented using an enhanced 3D DenseNet model, thereby constructing a comprehensive dataset for AD classification. In the second stage, the segmented images are then classified using the ST-MBV3 model, a deep learning architecture designed for high-accuracy classification. Experimental results demonstrate impressive classification accuracies, including 98.6% for distinguishing between AD and normal control (NC), 95.97% for mild cognitive impairment (MCI) versus NC, 94.57% for AD versus MCI, and 93.12% for AD/MCI/NC classification. The proposed approach offers promising results for automated and accurate AD diagnosis. This method could play a crucial role in advancing diagnostic procedures and improving patient outcomes.},
  archive      = {J_IJCIS},
  author       = {Li, Guiping and Jin, Zhenhao and Deng, Minghui and Gong, Junjie and Zheng, Piaoyi},
  doi          = {10.1007/s44196-025-00883-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {The two-stage alzheimer’s disease automatic diagnosis algorithm based on ST-MBV3 model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using a novel neutrosophic set-based method to evaluate the design risk of aerospace manufacturing projects in a neutrosophic environment. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00886-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During product design and manufacturing stages, unexpected challenges often occur, leading to product development or production failures. These failures can adversely affect the manufacturer's production efficiency, profitability, and reputation. Therefore, the industry often uses the failure mode and effects analysis (FMEA) approach to assess risk and identify possible risk factors. Moreover, with the rapid advancement of science and technology, shortened product life cycle, and the need for diversified product functions, factors such as time constraints, increased product manufacturing complexity, and difficulties in handling cross-domain information have made risk assessment in the design and manufacturing stages increasingly complex and challenging. These complexities can be considered a complicated multi-criteria decision-making (MCDM) problem. Due to the nature of the data, qualitative and quantitative data may coexist. On the other hand, decision-makers often lack a comprehensive understanding of cross-field expertise and information, leading to incomplete and inaccurate fuzzy information judgments. This, in turn, hinders their ability to provide accurate assessments of decision-making parameters in risk assessment issues. To address these challenges, this study integrates the FMEA method, the analytic hierarchy process (AHP) method, and the neutrosophic set (NS). It then applies these methodologies to numerical examples from an aerospace electronics manufacturing project for analysis and calculation. Compared with other research methods, the experiment results revealed that the novel NS-based risk analysis method yields more reliable and accurate rankings in failure mode risk in practical applications.},
  archive      = {J_IJCIS},
  author       = {Chung, Hsiang-Yu and Chang, Kuei-Hu},
  doi          = {10.1007/s44196-025-00886-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Using a novel neutrosophic set-based method to evaluate the design risk of aerospace manufacturing projects in a neutrosophic environment},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Many-objective cheetah optimizer: A novel paradigm for solving complex engineering problems. <em>IJCIS</em>, <em>18</em>(1), 1-63. (<a href='https://doi.org/10.1007/s44196-025-00859-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex many-objective optimization problems (MaOPs) generate multiple challenges for obtaining convergence alongside diversity within extensive multi-dimensional solution areas. Optimization approaches currently face limitations when trying to balance exploration and exploitation especially when resources become limited. MaOCO represents the Many-Objective Cheetah Optimization Algorithm which draws its concepts from the hunting behavior of cheetahs. MaOCO includes adaptive search functions that use attack and sit-and-wait approaches to optimize exploration and exploitation capabilities. MaOCO produces hypervolume (HV) results that exceed NSGA-III and MaOMVO by 50% while also delivering inverse generational distance (IGD) results which reach 40% better than both competing methods. The algorithm demonstrates superior efficiency in solving complex MaOPs, because it requires lower computational costs by 15%. MaOCO successfully traverses Pareto-optimal fronts according to theoretical evaluations, and its modular structure allows for both scale-up and hybridization features. The implemented applications of this approach include optimizing energy systems along with designing structures for engineering projects. Future researchers plan to integrate MaOCO with additional metaheuristic techniques to improve its performance when dealing with dynamic and irregular Pareto front problems.},
  archive      = {J_IJCIS},
  author       = {Patel, Pinank and Adalja, Divya and Mashru, Nikunj and Jangir, Pradeep and Arpita and Jangid, Reena and Gulothungan, G. and Hourani, Ahmad O. and Alshammari, Kaznah},
  doi          = {10.1007/s44196-025-00859-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-63},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Many-objective cheetah optimizer: A novel paradigm for solving complex engineering problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acoustic fault diagnosis method for rotating machinery based on collaborative perception information aggregation guidance network. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00862-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-contact and directional nature of acoustic fault diagnosis offers a distinct advantage for fault detection in complex environments, ensuring the safe and stable operation of rotating machinery. However, challenges such as complex working conditions and data imbalance significantly hinder the effectiveness of acoustic-based diagnostic systems. To address these issues, this paper proposes a Collaborative Perception Information Aggregation Guidance Network (CPAGN). In the proposed method, a Stem layer is utilized at the front end to extract nonlinear acoustic features. Subsequently, the collaborative perception layer integrates local frequency information extraction, multi-receptive field temporal information fusion, and redundancy reduction, thereby capturing the time-frequency variations of acoustic signals under complex operating conditions and enhancing condition awareness. Finally, inter-layer feature correlation computation is employed to generate global channel guidance, facilitating the effective interaction and fusion of global information. The CPAGN successfully extracts critical acoustic features and identifies essential fault information, demonstrating remarkable robustness and generalizability in handling data imbalance. To validate the effectiveness of CPAGN, comparative experiments were conducted on two datasets, comparing it with several existing deep learning methods. The results indicate that CPAGN excels at capturing fault information from acoustic signals, achieving superior diagnostic performance in complex working conditions and imbalanced data scenarios.},
  archive      = {J_IJCIS},
  author       = {Li, Chaofan and Zhang, Fan and Wang, Qichen and Han, Yufei and Li, Tianrui and Teng, Fei and Yi, Cai and Wu, Yongmeng},
  doi          = {10.1007/s44196-025-00862-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Acoustic fault diagnosis method for rotating machinery based on collaborative perception information aggregation guidance network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gas consumption accounting and prediction for medium and thick steel slabs: A method combining batch integrity and heat absorption ratio. <em>IJCIS</em>, <em>18</em>(1), 1-33. (<a href='https://doi.org/10.1007/s44196-025-00869-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heating process of steel slabs is a critical stage in the production of medium and thick plates, significantly impacting production costs and energy efficiency. Existing research faces two major scientific challenges in gas consumption accounting and prediction for medium-thick steel slabs: (1) due to the dynamic movement of slabs, high-temperature environments, and structural constraints, it is difficult to directly measure slab temperatures and accurately quantify heat loss during the reheating process; and (2) traditional methods lack systematic consideration of batch integrity and the heat absorption ratio during the heating process, leading to low accuracy in energy consumption accounting and prediction. This paper proposes an innovative gas consumption accounting and prediction method based on batch integrity and heat absorption ratio, integrating metallurgical mechanisms and industrial big data. In the accounting phase, a 2D heat conduction finite difference method is used to calculate the heat absorption of slabs, and the batch energy consumption is allocated proportionally based on heat absorption ratios. In the prediction phase, XGBoost models are employed to predict batch gas consumption, with results distributed to individual slabs using the heat absorption ratio.},
  archive      = {J_IJCIS},
  author       = {Guo, Qiang and Liang, Xinyu and Wang, Kai and Song, Yong},
  doi          = {10.1007/s44196-025-00869-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Gas consumption accounting and prediction for medium and thick steel slabs: A method combining batch integrity and heat absorption ratio},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized DenseNet architectures for precise classification of edible and poisonous mushrooms. <em>IJCIS</em>, <em>18</em>(1), 1-25. (<a href='https://doi.org/10.1007/s44196-025-00871-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The subtle differences between edible and toxic mushroom species make classification difficult. Traditional methods often result in errors which led to misclassifications and conventional machine learning models often struggle in feature extraction due to subtle differences in mushroom species. Deep learning models, such as DenseNet architectures, offer potential solutions, but due to model complexity, deep architecture and large number of parameters these models suffer from overfitting and computational costs. These can be handled by optimizing the model. This study’s primary goal is to enhance the precision and reliability of mushroom classification through deep learning by enhancing the DenseNet-121 structure. This is done through the implementation of more sophisticated model regularization techniques and automating the hyperparameter optimization process. The study intends to show how model architectural changes and optimization approaches can offer solutions to issues like overfitting and significant resource expenditure on computation and ultimately improve mushroom classification systems for efficiency and efficacy. The study analyzes the basic DenseNet-121 model as well as a modified DenseNet-121 with frozen upper layers which preserve important lower level features. Automated hyperparameter tuning is done with KerasTuner, while dropout and weight decay regularization methods are used to control overfitting. Evaluation metrics include accuracy, precision, recall, F1-score, confusion matrices, and other graphical methods. The enhanced DenseNet-121 model surpasses the standard DenseNet-121 in mushroom classification. While DenseNet-121 obtained an overall accuracy of 0.90 along with a macro average precision, recall, and F1-score of 0.90 $$-$$ 0.91, Modified DenseNet-121 achieved 0.97 for all those metrics. The improvements increased the precision and recall for all the classes which means the model has less trustability and more accuracy in classification. The study demonstrates the effectiveness of architectural modifications and regularization strategies in improving model performance. Despite problems such as possible over-reliance on pre-trained features and computational complexity, the modified DenseNet-121 is useful for accurate mushroom classification. Future study could look into improving freezing procedures and lowering computational demands to extend applicability.},
  archive      = {J_IJCIS},
  author       = {Singh, Jay Prakash and Ghosh, Debolina and Singh, Jagannath and Bhattacharjee, Anurag and Gourisaria, Mahendra Kumar},
  doi          = {10.1007/s44196-025-00871-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimized DenseNet architectures for precise classification of edible and poisonous mushrooms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling fuzzy moral hazard in credit default swap pricing: A reduced-form approach. <em>IJCIS</em>, <em>18</em>(1), 1-33. (<a href='https://doi.org/10.1007/s44196-025-00872-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In existing literature, moral hazard is often modeled as a constant. However, moral hazard can be “fuzzy” rather than “precisely defined.” As moral hazard is dynamic and variable, exhibiting both constancy and differentiation, its representation through fuzzy intervals—rather than fixed constants—has emerged as a meaningful research direction. This paper integrates fuzzy set theory into credit default analysis, combining moral hazard, fuzzy risk, and credit risk in a cross-disciplinary study to explore their intrinsic interdependencies. First, a novel default intensity model incorporating market state variables and moral hazard state variables is proposed. By accounting for the fuzzy risks inherent in trading environments, the moral hazard indicators are transformed into fuzzy intervals, thereby establishing interval-based moral hazard metrics to characterize default intensity. Subsequently, to address the phenomenon of default clustering caused by default dependence in markets, a circular default intensity model involving two reference assets is constructed. Within this framework, the cross-influence mechanisms of moral hazard and fuzzy risk are further investigated. Furthermore, the proposed model, which integrates moral hazard and fuzzy risk, is applied to derivative pricing. A new pricing formula for default management costs is derived. Finally, through comparative analysis and simulation experiments, the study concludes that: (1) Under the parameter settings representing economic stagnation, the triangular fuzzy interval of survival probability narrows progressively as the credibility parameter $$\gamma$$ increases, eventually converging to a real number. (2) The cost of credit default management is significantly influenced by the number of reference assets. In trading environments where moral hazard, fuzzy risk, and credit risk intertwine, effective mitigation of their impacts on default risk requires strategic information utilization and reduction in the number of reference assets. This approach not only lowers credit management costs but also fosters the healthy development of financial markets.},
  archive      = {J_IJCIS},
  author       = {Wu, Liang and Hua, Hongtao},
  doi          = {10.1007/s44196-025-00872-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Modeling fuzzy moral hazard in credit default swap pricing: A reduced-form approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven predictive modeling for lung cancer detection and management using synthetic data augmentation and random forest classifier. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00879-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) transforms multiple businesses, including medical research, where AI-driven developments bring significant advantages. The application of machine learning algorithms enables medical researchers to examine large amounts of data accurately, which leads to the development of precise and effective treatment approaches. Lung cancer leads the list of critical healthcare issues because it remains the world’s most lethal form of cancer thus demanding innovative diagnostic tools for faster and accurate identification. The proposed study introduces an innovative method called CTGAN-RF which uses conditional tabular generative adversarial networks (CTGAN) and random forest (RF) classifier to detect lung cancer through synthetic data generation. The proposed model demonstrated superior performance by achieving a 0.9893 score of accuracy and 0.99 value for precision, F1 score, and recall. Extensive experimental evaluation for this method included testing nine classification algorithms. The implementation of different classifiers employed data balancing methods including SMOTE and borderline-SMOTE along with SMOTE ENN and unbalanced data configurations. Comparative analysis showed that CTGAN-RF consistently performed significantly better than traditional classifiers in dealing with class imbalance and improving prediction accuracy. After testing with fivefold cross-validation, the reliability of the model was further validated. In comparison to cutting-edge approaches for lung cancer diagnosis, the proposed methodology outperformed in terms of classification metrics. This in-depth evaluation of synthetic data augmentation with machine learning in lung cancer detection has helped in the development of personalized treatment strategies in the fight against such a life-threatening disease.},
  archive      = {J_IJCIS},
  author       = {Innab, Nisreen and Aldrees, Asma and AlHammadi, Dina Abdulaziz and Hakeem, Abeer and Umer, Muhammad and Alsubai, Shtwai and Trelova, Silvia and Ashraf, Imran},
  doi          = {10.1007/s44196-025-00879-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {AI-driven predictive modeling for lung cancer detection and management using synthetic data augmentation and random forest classifier},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid DRL-enhanced ACO-WWO for efficient resource allocation and load-balancing in cloud computing. <em>IJCIS</em>, <em>18</em>(1), 1-40. (<a href='https://doi.org/10.1007/s44196-025-00882-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing complexity of cloud computing necessitates astute workload allocation and adaptive resource management to enhance performance while minimizing expenses and energy consumption. Conventional optimization methods, including Improved Ant Colony Optimization (IACO) and Water Wave Optimization (IWWO), face challenges in real-time adaptability, exhibit slow convergence, and are inadequate for managing rapidly varying workloads. Although IACO enhances local search efficiency and IWWO specializes in global exploration, neither adequately resolves the complexities of dynamic cloud environments. To address this gap, we propose a Hybrid DRL-IACO-IWWO model, a novel hybrid model that combines DRL with advanced iterations of IACO and IWWO. The model presents an adaptive dual-phase optimization strategy, wherein IACO conducts initial task scheduling, and IWWO enhances global optimization, informed by real-time DRL feedback. Furthermore, DRL dynamically adjusts its heuristic parameters to improve operational cost and energy efficiency, ensuring real-time adaptability. To expedite convergence, our model utilizes a wavelet transformation-based perturbation in WWO, thereby preventing premature convergence and promoting a more balanced equilibrium between exploration and exploitation. An energy-efficient scheduling mechanism is integrated to reduce energy consumption and improve cloud sustainability. The proposed model was evaluated using the workflow dataset, considering constraints, such as task deadlines, resource availability, and cost efficiency. The results indicate that our methodology outperforms leading hybrid techniques, such as ACO-GA, ACO-SMO, and WWO-GA. The proposed model achieved a scheduling duration of 1.25 s, compared to 1.75 s for ACO-GA and 1.68 s for WWO-GA, while reducing operational expenses to $23.80, lowering energy consumption to 15.6 kWh, and achieving a resource utilization score of 0.92. These findings underscore the transformative capacity of our Enhanced ACO-WWO with DRL, offering a highly efficient, cost-effective, and adaptive solution for next-generation cloud resource management.},
  archive      = {J_IJCIS},
  author       = {Lilhore, Umesh Kumar and Simaiya, Sarita and Rao, K. B. V. Brahma and Rao, V. V. R. Maheswara and Sharma, Yogesh Kumar and Alroobaea, Roobaea and Alsufyani, Hamed and Alsafyani, Majed and Khan, M. D. Monish},
  doi          = {10.1007/s44196-025-00882-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-40},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hybrid DRL-enhanced ACO-WWO for efficient resource allocation and load-balancing in cloud computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving search accuracy in large-scale biased multiobjective optimization through local search. <em>IJCIS</em>, <em>18</em>(1), 1-38. (<a href='https://doi.org/10.1007/s44196-025-00884-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biased multiobjective optimization problems pose a challenge for evolutionary algorithms in obtaining high-accuracy solutions, and as the number of decision variables increases, this challenge becomes increasingly difficult to overcome. To address this issue, we propose a three-particle-based local search method (TPS) for multiobjective evolutionary algorithms (MOEAs). The main concept is to use three particles to maintain three equidistant values of a decision variable and gradually approach the local optimal value by adaptively adjusting their differences. Specifically, the TPS maintains a population with three particles and uses five proposed population state-transition operations to gradually move these three particles to a better state. A local optimal value can be obtained when these three particles become indistinguishable. The TPS is then embedded into an MOEA to form a new algorithm, called MOEA/TPS. To enable the TPS to search along the convergence and diversity directions, the two aggregation functions of the target problem are alternately used. Compared with twelve competitive MOEAs on various biased test problems with 30 to 2000 decision variables, our proposed algorithm demonstrates significant advantages in obtaining high-accuracy solutions.},
  archive      = {J_IJCIS},
  author       = {Yin, Feng and Cao, Bin},
  doi          = {10.1007/s44196-025-00884-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Improving search accuracy in large-scale biased multiobjective optimization through local search},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMFOSOM: A novel multi-objective Moth–Flame optimization algorithm based on self-organizing mapping. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00891-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An optimization technique seeks to identify the best solution for a given problem. When the problem involves a single objective function, the optimal solution yields the best value for that objective. However, in cases with multiple objectives, selecting the best solution becomes more complex, as these objective functions often conflict with one another. For such multi-objective optimization problems, relying on analytical or exact methods is often not feasible. This paper proposes a modified MFO called multi-objective moth–flame optimization based on self-organizing mapping (MMFOSOM) to solve multi-objective optimization problems. For attaining high-quality initial moths, dual opposition-based learning mechanism is employed to generate two distinct populations, referred to as the exploration moth (EM) and the auxiliary moth (AM). In order to enhance the search capabilities of the algorithm, each population is assigned a SOM network, the adaptive neighborhood structure of which contributes to forming modal clusters in the mapping, organizing similar solutions together. In order to select the excellent individual as the training data for SOM, a novel environmental selection mechanism is introduced. To validate the effectiveness of the algorithm, experiments will be conducted on a set of multi-objective optimization test functions. The experimental results indicate that the proposed algorithm can effectively address multi-objective optimization problems.},
  archive      = {J_IJCIS},
  author       = {Li, Zhifu and Zheng, Ziyang and Ni, Chengkai and Li, Mingzong},
  doi          = {10.1007/s44196-025-00891-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {MMFOSOM: A novel multi-objective Moth–Flame optimization algorithm based on self-organizing mapping},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Publisher correction: Cross-dataset analysis of language models for generalised multi-label review note distribution in animated productions. <em>IJCIS</em>, <em>18</em>(1), 1-2. (<a href='https://doi.org/10.1007/s44196-025-00854-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Garcés, Diego and Santos, Matilde and Fernández-Llorca, David},
  doi          = {10.1007/s44196-025-00854-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Publisher correction: Cross-dataset analysis of language models for generalised multi-label review note distribution in animated productions},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy logic-based computational framework for precision triage in androgenetic alopecia: A simulated biomarker-driven approach. <em>IJCIS</em>, <em>18</em>(1), 1-30. (<a href='https://doi.org/10.1007/s44196-025-00887-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Androgenetic alopecia (AGA) is a common cause of hair loss affecting both men and women. Although its precise etiology remains uncertain, genetic and hormonal factors are recognized as major contributors. This study introduces a computational intelligence framework employing fuzzy logic and multi-criteria decision-making (MCDM) to simulate a robust triage system for AGA management. Using a simulated dataset of 100 AGA patients, we applied the fuzzy-weighted zero-inconsistency (FWZIC) method to assign weights to 11 bioactive criteria associated with AGA. These weights informed a novel triage procedure for alopecia patients (TPAP), which stratified patients into seven severity levels (level 1: minor; level 7: severe). This study presents a computationally intelligent triage model tailored for AGA, emphasizing the applicability of fuzzy MCDM techniques in medical decision support. The TPAP framework can assist in resource allocation and treatment planning, paving the way for personalized and timely interventions in hair loss management.},
  archive      = {J_IJCIS},
  author       = {Al-Samarraay, Mohammed S. and Magableh, Aws A. and Mahmood, Rana I. and Joudar, Shahad Sabbar and Zahid, Idrees A. and Al-Obaidi, Jameel R. and Al-Saffar, Ali Z. and Albahri, A. S. and Albahri, O. S. and Alamoodi, A. H. and Abdullah, Mohd Faizal Nizam Lee},
  doi          = {10.1007/s44196-025-00887-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A fuzzy logic-based computational framework for precision triage in androgenetic alopecia: A simulated biomarker-driven approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrusion detection and mitigation method for the industrial internet of things using bidirectional convolutional long short-term memory and deep recurrent convolutional Q-networks. <em>IJCIS</em>, <em>18</em>(1), 1-38. (<a href='https://doi.org/10.1007/s44196-025-00890-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical system (CPS) security has become more important in the age of Industry 4.0 because of the quick integration of automation and the Internet of Things. The goal of this project is to create a strong intrusion detection and control system that can recognize and lessen security risks in CPS settings. The suggested approach makes use of deep learning (DL) and reinforcement learning (RL) techniques. To guarantee data consistency, pre-processing procedures such as mean-based imputation and min–max scaling come after data collection. ADASYN data augmentation is used to address class imbalance, while entropy analysis and statistical techniques are used to extract key features. The intrusion detection phase uses a combination of deep convolutional neural networks (DCNN) and bidirectional long short-term memory (BI-LSTM) networks to capture both spatial and temporal relationships in the data, while a hybrid feature selection technique improves the model’s performance. A deep Q-network (DQN) handles attack mitigation and uses reinforcement learning to adjust to new threats. Detecting attack patterns with high sensitivity (0.984), specificity (0.983), and accuracy (0.991626) for dataset 1, the accuracy of dataset 2 is 0.985 for 70% of training and 0.988 for 80% of training, and the Proposed-DBID-Net architecture enhances CPS security in Industry 4.0. The evaluation phase emphasizes how crucial feature selection is to maximize the model’s accuracy. In conclusion, this study offers a thorough and flexible method for protecting CPS in Industry 4.0, guaranteeing accuracy and scalability across changing cyber threats.},
  archive      = {J_IJCIS},
  author       = {Yan, Zhang and Shukla, Piyush Kumar and Shukla, Prashant Kumar and Thakur, Kanika and Sinha, Anurag and Khalid, Saifullah},
  doi          = {10.1007/s44196-025-00890-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Intrusion detection and mitigation method for the industrial internet of things using bidirectional convolutional long short-term memory and deep recurrent convolutional Q-networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential evolution-based transient search optimizer for image multi-thresholding problem. <em>IJCIS</em>, <em>18</em>(1), 1-30. (<a href='https://doi.org/10.1007/s44196-025-00821-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A significant number of image processing and evaluation processes are presented and applied, due to their practical significance, in a variety of disciplines in the literature related to image processing and computer vision. One pre-processing approach that is frequently used to improve the information in a class of images is thresholding. By grouping correlated pixels according to the selected thresholds, the thresholding approach improves the image. For the benchmark image suite in this study, an entropy-based threshold is put into place. This study aims to investigate the thresholding performance of well-recognized fitness function called Kapur’s entropy, for a selected threshold. To facilitate the automatic identification of the optimum threshold (Th) on the benchmark images for a specified threshold value $$(Th=3, 5, 7)$$ , there is a need for an optimization algorithm that determines the optimum threshold values for the given image under study. This research proposed a self-adaptive hybrid differential evolution (DE) based transient search optimizer (TSO) called DETSO. DETSO uses the search equations of DE algorithm to improve the exploration capability of original TSO. Additionally, self-adaptivity has been included by modifying scaling factor and crossover rate using exponential decreasing and logarithmic decreasing mutation operator, respectively. The CEC 2019 numerical test problems have been used to confirm the working efficiency of DETSO. The experimental investigation confirms that utilizing a benchmark image test suite with varying dimensions, the DETSO helps to get superior results in achieving better threshold values in terms of performance metrics, such as PSNR, SSIM, FSIM, MSE, etc., compared to the other competitive heuristic algorithms.},
  archive      = {J_IJCIS},
  author       = {Mittal, Nitin and Singh, Supreet and Kumar, Lalit and Kaur, Gurpreet and Mittal, Vikas and Santhosh, A. Johnson},
  doi          = {10.1007/s44196-025-00821-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Differential evolution-based transient search optimizer for image multi-thresholding problem},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid optimizer based on coati optimization algorithm and differential evolution for global optimization and constrained engineering problems. <em>IJCIS</em>, <em>18</em>(1), 1-70. (<a href='https://doi.org/10.1007/s44196-025-00855-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel hybrid metaheuristic, the Hybrid Coati Optimization Algorithm with Differential Evolution (HCOADE), developed to address complex global optimization tasks and constrained engineering design problems. HCOADE integrates the exploration-driven behaviour of the Coati Optimization Algorithm (COA)-inspired by the social foraging and predation strategies of coatis-with the powerful mutation and crossover mechanisms of Differential Evolution (DE), thereby achieving a balanced and adaptive search process. The hybridization enhances global exploration and local exploitation, enabling the algorithm to efficiently navigate diverse and challenging optimization landscapes. To rigorously evaluate its performance, HCOADE is tested on benchmark suites from CEC 2014, 2017, 2020, and 2022, which encompass unimodal, multimodal, hybrid, and composition functions. It is also applied to real-world constrained engineering problems, such as pressure vessel design, cantilever beam optimization, and reinforced concrete beam design. Comparative experiments against state-of-the-art algorithms—including COA, DE, RSA, PSO, SSA, BBO, QIO, DMOA, and others—demonstrate that HCOADE consistently delivers superior solution quality, faster convergence, and higher robustness. Quantitative results show that HCOADE achieved the 1st place average rank across all four benchmark suites. It obtained top performance on 80% of the functions in CEC 2014, 66.7% in CEC 2017, 70% in CEC 2020, and 66.7% in CEC 2022. Furthermore, HCOADE outperformed or matched CEC competition-winning algorithms such as LSHADE-cnEpSin, LSHADE-SPACMA, and CMA-ES on numerous challenging functions of CEC 2017. Statistical analyses, including Wilcoxon Rank Sum Tests and ranking evaluations, confirm the significance and reliability of HCOADE’s performance. Furthermore, convergence behaviour, measurement of exploration and exploitation, and sensitivity analysis highlight the algorithm’s adaptability and stability across varied problem domains. This study contributes a computationally efficient and generalizable hybrid optimization framework, offering a promising solution for theoretical benchmarks and real-world engineering applications.},
  archive      = {J_IJCIS},
  author       = {Biswas, Saptadeep and Maiti, Binanda and Singh, Gyan and Ezugwu, Absalom E. and Saleem, Kashif and Abualigah, Laith and Smerat, Aseel and Bera, Uttam Kumar},
  doi          = {10.1007/s44196-025-00855-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-70},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel hybrid optimizer based on coati optimization algorithm and differential evolution for global optimization and constrained engineering problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classifying and detecting live insects with computationally effective deep learning object detection models. <em>IJCIS</em>, <em>18</em>(1), 1-27. (<a href='https://doi.org/10.1007/s44196-025-00885-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial part of agriculture is detecting insects that increase yield productivity. Insects in agricultural land are both helpful and harmful. The harmful insects are detected and controlled as early as possible, but these control measures should not affect the beneficial insects that help crops to grow. The existing pest detection models are image-based models where the preciseness of the insect detection is based on their appearance in the respective image, which may lead to the misclassification of insect classes if the insects are not present in the image properly. By analyzing consecutive frames rather than a still image, the proposed approach detects live insect objects from the video rather than a still image, where the presence of insects is identified by analyzing consecutive frames. As a result, insects can be detected without relying on the appearance of a single still image, which helps mitigate insects' misclassification. A wide range of applications in computer vision has proven that deep learning approaches are highly effective and popular. This study employs a variety of three deep learning-based object detection networks coupled with multiple backbone networks to maximize their efficiency. Each model is initially pre-trained using the COCO dataset to improve its performance. Experimental results show that SSD_MobileNet_V2 outperformed other models on insect classification and detection tasks. Regarding insect classification tasks, the SSD_MobileNet_V2 achieved an accuracy and F1 score of 98.02% and 97.99%, respectively. On the insect detection task, the mAP is 98.8% at a detection time of about 0.18 s. Also, it is delivered with a smaller model size of 6.5 MB, making it suitable for handheld devices.},
  archive      = {J_IJCIS},
  author       = {Rajeswaran, Arumuga Arun and Katara, Karthik and Selvaraj, Yoganand and Sundarasamy, Ranjithkumar},
  doi          = {10.1007/s44196-025-00885-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Classifying and detecting live insects with computationally effective deep learning object detection models},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating shallow and deep features for precision evaluation of corn grain quality: A novel fusion approach. <em>IJCIS</em>, <em>18</em>(1), 1-12. (<a href='https://doi.org/10.1007/s44196-025-00889-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the precision evaluation of corn grain quality, focusing on categorizing seeds into four classes: broken, discolored, pure, and silk cut. We evaluated 13 pre-trained CNN models, including AlexNet, VGG19, and ResNet, with AlexNet emerging as the top performer, achieving 71% accuracy validated through statistical analysis using Duncan’s multiple range test. To further enhance accuracy, we propose a novel fusion approach that integrates shallow features—extracted via 2D Discrete Fourier Transform (2D-DFT) and Hilbert transform—from the images with deep features from AlexNet. The deep features are combined with these shallow features to create an enriched feature vector. This vector, consisting of 1000 deep features, 140 2D-DFT features, and 140 Hilbert transform features, is classified using a Support Vector Machine (SVM). The hybrid model achieved an accuracy of 86%. Manual grain quality assessment is subjective and time-consuming; our automated framework addresses this challenge by providing a more objective and efficient evaluation. The integration of diverse features not only improves classification accuracy but also underscores the potential of combining various information sources for robust grain quality assessment.},
  archive      = {J_IJCIS},
  author       = {Mishra, Kunal and Behera, Santi Kumari and Devi, A. Geetha and Sethy, Prabira Kumar and Nanthaamornphong, Aziz},
  doi          = {10.1007/s44196-025-00889-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Integrating shallow and deep features for precision evaluation of corn grain quality: A novel fusion approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel YOLO algorithm integrating attention mechanisms and fuzzy information for pavement crack detection. <em>IJCIS</em>, <em>18</em>(1), 1-25. (<a href='https://doi.org/10.1007/s44196-025-00894-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pavement crack detection is widely spread over road maintenance, ensuring the longevity and safety of infrastructure. Traditional manual inspection methods are time-consuming, labor-intensive, and prone to errors. In response, automated crack detection systems based on deep learning have emerged, offering more efficient and accurate solutions. However, existing models often face challenges such as large model sizes, slow inference speeds, and limited applicability in real-time applications. In this paper, we propose a novel light-weight Crack Regional Segmentation method based on YOLOv11, which introduces attention mechanisms to address challenges in pavement images, such as varying crack sizes, occlusion, and irregular surface textures. By embedding a region-based attention mechanism into the YOLOv11 network, the method enhances the model’s ability to focus on crack features. Specifically, the model network layers are progressively pruned to reduce the number of parameters and floating-point operations, thereby further improving operational efficiency and refining detection in the target regions. Furthermore, to tackle issues with blurred or indistinct crack boundaries, we present a fuzzy information-guided YOLOv11-based model, FIG-YOLO. This model integrates fuzzy logic and fuzzy membership functions to handle uncertainty in crack detection. The fuzzy membership functions are used to quantify the degree of crack features, allowing the model to better distinguish between crack and non-crack regions, especially in cases where crack boundaries are unclear. This approach significantly improves the accuracy of crack detection and segmentation. Extensive experiments demonstrate that our approach effectively addresses challenges such as complex backgrounds and blurred crack edges in pavement images. This research not only provides a novel solution for the automated detection of pavement cracks but also offers insights into the development of intelligent road maintenance systems. With the expansion of large-scale datasets and the advancement of deep learning models, pavement crack detection algorithms are expected to further enhance their accuracy and efficiency, offering significant support for road infrastructure management.},
  archive      = {J_IJCIS},
  author       = {Li, Qingqing and Wu, Tianshu and Xu, Tingfa and Lei, Jianmei and Liu, Jiu},
  doi          = {10.1007/s44196-025-00894-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel YOLO algorithm integrating attention mechanisms and fuzzy information for pavement crack detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting detection accuracy: An enhanced YOLOv8 for small target detection in remote sensing. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00895-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in remote sensing images has broad applications in military reconnaissance, urban planning, and disaster management. However, detecting small targets and extracting multi-scale features in complex scenes remain challenging. This paper presents YOLOv8s-Improved, enhancing small-target detection via PPHGNetV2, Progressive Feature Pyramid Network (AFPN-P2), and Diverse Branch Block (DBB) modules. Experiments on the DIOR and VisDrone2019 datasets show that YOLOv8s-Improved achieves mAP scores of 0.824 and 45.3%, respectively, representing improvements of 1.5 and 6.4 percentage points over the baseline YOLOv8s model (0.809 and 38.9%). The improved model demonstrates strong performance in multi-category object detection, particularly in complex scenes. The results suggest that the proposed method addresses the challenges of small target detection in remote sensing and exhibits generalization capabilities across different datasets.},
  archive      = {J_IJCIS},
  author       = {Chen, Boyuan and Ma, Zheng and Li, Xiang},
  doi          = {10.1007/s44196-025-00895-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Boosting detection accuracy: An enhanced YOLOv8 for small target detection in remote sensing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive BlockVax distribution: Enhancing healthcare supply chain resilience with blockchain and LSTM. <em>IJCIS</em>, <em>18</em>(1), 1-26. (<a href='https://doi.org/10.1007/s44196-025-00897-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pandemic outbreak has revealed significant flaws in the complex and highly fragmented Healthcare Supply Chain’s (HSC’s). However, two major issues persist in the HSCs, leading to inefficiencies: transparency in vaccine distribution and accuracy in demand forecasting. The recent pandemic has highlighted and intensified existing vulnerabilities in HSC’s, leading to the effective utilization of digital technologies to manage them. This research proposes a novel framework that merges Blockchain (BC) and Machine Learning (ML) to bolster the HSCs amidst pandemics, by developing a framework named the Predictive BlockVax Distribution Network (PBDN) model. The proposed PBDN model utilizes BC for securing transactions and Long Short-Term Memory (LSTM), for precise demand prediction. Leveraging Hyperledger Besu, which represents an Ethereum client that is accessible for public use, the PBDN framework ensures BC’s privacy, scalability, and efficient network operations, while LSTM’s advanced forecasting outperforms traditional models and Deep Learning (DL) techniques. This integration showcases a significant leap in managing vaccine distribution and enhancing system resilience, fairness, and transparency. The proposed PBDN model illustrates the potential of BC and ML together to tackle pandemic-induced Supply Chains (SC’s) disruptions, providing a decentralized solution that supports autonomous, informed decision-making without third-party dependency. This approach not only addresses immediate challenges but also sets a precedent for future crisis response, emphasizing the need for robust, Transparent Supply Chain’s (TSC’s).},
  archive      = {J_IJCIS},
  author       = {Nair, Raji Ramakrishnan and Rattan, Punam and Kumar, Mukesh and Bhardwaj, Vivek},
  doi          = {10.1007/s44196-025-00897-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Predictive BlockVax distribution: Enhancing healthcare supply chain resilience with blockchain and LSTM},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Construction scheduling optimization of prefabricated buildings under resource constraints based on an improved whale optimization algorithm. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00898-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prefabricated construction offers significant advantages in efficiency, resource savings, and environmental sustainability, yet its scheduling remains challenging due to complex resource constraints and task interdependencies. This study proposes a dual-objective optimization model to minimize project duration and resource fluctuation in prefabricated construction projects. To solve this model effectively, an Improved Whale Optimization Algorithm (IWOA) is developed, addressing the limitations of the standard WOA such as premature convergence and poor local search ability. The proposed IWOA incorporates Tent chaotic mapping for population diversification, a nonlinear convergence factor to balance exploration and exploitation, and a hybrid search mechanism that integrates crossover operations and multiple neighborhood strategies. A real-world case study demonstrates the algorithm’s superiority over conventional approaches in both solution accuracy and stability. The results show that the optimized schedule significantly improves resource allocation efficiency and reduces construction time, offering a promising approach for intelligent scheduling in resource-constrained prefabricated building projects.},
  archive      = {J_IJCIS},
  author       = {Su, Rui and Aviles, Joey S.},
  doi          = {10.1007/s44196-025-00898-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Construction scheduling optimization of prefabricated buildings under resource constraints based on an improved whale optimization algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing and rewarding credit card spending habits in india: A machine learning approach. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00899-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid adoption of digital payments in India, credit card companies are focusing on customer loyalty and planning rewards to incentivize spending, especially during peak periods like festivals. However, there is a gap in developing a tailored system that optimizes sales and reward structures for these companies. The proposed work addresses this gap by leveraging machine learning techniques to analyze and assess credit card spending patterns and propose design targeted reward programs. Besides this, this study focuses on categories as luxury, travel, groceries, EMIs payments, and others and employs ML methods, using K-Means clustering to segment users based on card types (Silver, Gold, Platinum, and Signature). Feature engineering is another key in improving the model’s understanding and providing insights, particularly in calculating reward points based on various attributes and spending behavior. The usage of original and synthetic datasets ensured scalability and adaptability across different financial domains as well. The results highlight the potential and need of ML to optimize reward allocation and provide real-time predictions, enabling financial institutions to tailor their offerings for increased customer engagement and retention. By aligning rewards with high-margin spending categories and leveraging adaptive frameworks, this study offers strategies to enhance credit card reward programs. The proposed ML model achieved an R2 value of 0.99, demonstrating superior accuracy in optimizing reward point distribution.},
  archive      = {J_IJCIS},
  author       = {Agrawal, Renuka and Khanna, Aryan and Hamdare, Safa},
  doi          = {10.1007/s44196-025-00899-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Analyzing and rewarding credit card spending habits in india: A machine learning approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aspect sentiment triplet extraction with syntax-semantics graph convolutional network. <em>IJCIS</em>, <em>18</em>(1), 1-17. (<a href='https://doi.org/10.1007/s44196-025-00900-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the traditional task of aspect sentiment triplet extraction, existing approaches typically focus on either syntactic or semantic features independently, failing to leverage the complementary integration of these two types of information. Although graph convolutional network-based approaches have demonstrated impressive performance in triplet extraction tasks, they often ignore distance features and semantic information when capturing sentence information. As a result, the integration of syntactic and semantic information remains suboptimal, negatively impacting sentiment analysis performance. To address this limitation, we propose a novel Syntax-Semantics Graph Convolutional Network for aspect sentiment triplet extraction. Our method first extracts syntactic structural information using the probability matrix of dependency trees, from which a mask matrix is constructed based on the varying distances between words. Next, semantic information is captured via a self-attention mechanism and an aspect-attention mechanism, utilizing an attention score matrix. Finally, an interaction module is introduced to effectively integrate syntactic and semantic features. Extensive experiments on several benchmark datasets demonstrate that our approach significantly outperforms existing baselines, achieving an average F1-score improvement of at least 1.083%.},
  archive      = {J_IJCIS},
  author       = {Zhang, Jingyun and Xu, Shuwei and Gao, Xin and Tang, Zhiwei},
  doi          = {10.1007/s44196-025-00900-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Aspect sentiment triplet extraction with syntax-semantics graph convolutional network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drought detection in satellite imagery: A layered ensemble machine learning approach. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00903-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drought has been a major calamity due to climate change in recent years. Predicting drought has grabbed the attention of meteorologists and climate scientists, who study and look for modern techniques. The early detection of drought leads to better management of resources and timely decisions to avoid damage. Machine learning techniques have proven their potential in classification and prediction problems. This study proposes a layered ensemble machine learning approach to detect drought from satellite imagery. The satellite imagery is collected using Google Earth Pro for the region ’Tharparkar’ in Pakistan’s Sindh province. Tharparkar is one of the most drought-stricken regions in Pakistan. The proposed approach combines conventional machine learning algorithms (Support Vector Machine (SVM), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), Naive Bayes (NB), and k-Nearest Neighbor (k-NN)) with ensemble methods (Bagging and Voting) in a layered fashion for detecting drought from satellite imagery. The novelty of the study lies in its layered ensemble architecture that integrates multiple conventional classifiers with ensemble techniques for improved drought detection accuracy using satellite imagery. The validation of the model is computed using the stratified split method. The developed classification model is evaluated using well-established indexes, including accuracy, precision, recall, F measure, and Area Under Curve (AUC). Among the classical models, the Decision Tree classifier performed best with an accuracy of 82.17%, a precision of 82.53%, a recall of 82. 28%, and an F1 score of 82.35%. Within the Bagging models, Bagged Decision Tree achieved the highest performance, attaining an accuracy of 84.78%, a precision of 85.14%, a recall of 84.87%, and an F1 score of 84.91%. The final-layer Voting ensemble outperformed all previous models, yielding the highest F1 score of 84.80%. Based on experimental results, the proposed model has strong potential for practical deployment in real-world environmental monitoring systems.},
  archive      = {J_IJCIS},
  author       = {Raza, Muhammad Owais and Mahoto, Naeem Ahmed and Al Reshan, Mana Saleh and Alqazzaz, Ali and Rajab, Adel and Shaikh, Asadullah},
  doi          = {10.1007/s44196-025-00903-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Drought detection in satellite imagery: A layered ensemble machine learning approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy decision systems for sustainable public transport: A literature review. <em>IJCIS</em>, <em>18</em>(1), 1-29. (<a href='https://doi.org/10.1007/s44196-025-00904-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of sustainable transport has been proposed and implemented in cities around the world to facilitate transition to sustainability. Reducing carbon emissions and increasing accessibility in an economically feasible way has been the core objective of sustainable transport initiatives in cities. The complexity of the transport systems requires a decision framework that would take this complexity as well as uncertainty into account while proposing solutions for sustainable transport issues. Current worldwide developments, such as the COVID-19 outbreak and international tensions, economic crises, and climate change, have heightened uncertainty regarding decisions that affect sustainable transportation challenges. Classical decision methods are no longer sufficient to tackle sustainable transport issues in this economically, socially, and environmentally uncertain environment. This is evidenced by the rapid proliferation of the studies using some sort of fuzzy decision systems on developing solutions for sustainable transport problems. Fuzzy decision systems provide new frameworks of decision-making that would appropriately take the uncertainty into account for the sustainable transport decisions and problems. This study provides an overview of existing research on studies that utilize fuzzy decision systems for sustainable public transport, excluding logistics transport from its scope. Additionally, a thematic analysis is conducted on the types of fuzzy decision methods and sustainable transport topics, aiming to summarize the existing literature and guide future research in this field.},
  archive      = {J_IJCIS},
  author       = {Kilic, Mehmet and Demirel, Tufan},
  doi          = {10.1007/s44196-025-00904-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Fuzzy decision systems for sustainable public transport: A literature review},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study on container storage optimization in yards based on a hyper-heuristic algorithm with a Q-learning mechanism. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00905-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of a low-carbon economy, scientific methods to reduce carbon emissions have become an important issue for many ports. Carbon emissions in port areas mainly arise from vessels and handling equipment. Therefore, an effective resource assignment and equipment arrangement system could not only reduce carbon emissions, but also improve the port’s operational efficiency. This study considers factors such as the arrival order of container trailers, the cargo weight, and the number of container rehandling operations. The objective is to minimize the carbon emissions and the number of container rehandling operations in ports, for which a mixed-integer linear programming model is built. Both heuristic algorithms and hyper-heuristic algorithms are employed to optimize the container storage plan, and their applicability in storage optimization is compared. The results indicate that hyper-heuristic algorithms outperform heuristic algorithms in terms of solution quality and stability, effectively satisfying the storage requirements of the yard while minimizing the carbon emissions and the number of container rehandling operations. The results provide theoretical support for port enterprises in improving their operational efficiency and achieving their goals regarding low carbon emissions.},
  archive      = {J_IJCIS},
  author       = {Chen, Lifen and Lin, Jiajun and Xu, Shihao},
  doi          = {10.1007/s44196-025-00905-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A study on container storage optimization in yards based on a hyper-heuristic algorithm with a Q-learning mechanism},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in industry, knowledge computing and decision-making. <em>IJCIS</em>, <em>18</em>(1), 1-3. (<a href='https://doi.org/10.1007/s44196-025-00910-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Montero, Javier and Rodríguez, J. Tinguaro and Flores-Vidal, Pablo},
  doi          = {10.1007/s44196-025-00910-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Artificial intelligence in industry, knowledge computing and decision-making},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Face anti-spoofing detection based on novel encoder convolutional neural network and texture’s grayscale structural information. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00757-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise in popularity of face anti-spoofing is attributed to its crucial role in safeguarding face recognition systems. Perpetrators use either a photo or a video to execute a face-spoofing attack on the authentication system of an authorized user, aiming to gain access to the user’s resources. Traditional face anti-spoofing detection techniques often rely on grey texture elements while disregarding RGB color intensity features in face images, which lead to a loss of certain facial information. On the other hand, RGB-based methods could introduce noise or superfluous elements that reduce the effectiveness of spoofing detection. Furthermore, in real-world situations, color disparities may be introduced by lighting, camera settings, and other elements. Therefore, a face anti-spoofing detection approach based on a novel Encoder Convolutional Neural Network (ECNN) architecture, Local Binary Pattern (LBP) and Local Ternary Pattern (LTP) descriptors is proposed. The proposed ECNN can more effectively extract the RGB color brightness features. It encodes the brightness intensity change of the RGB color transition, which can recognize intricate spoofing attempts. The texture descriptors of LBP and LTP are utilized to extract the texture's grayscale structural information to overcome different environmental situations. This work combined the human face's intensity RGB color and grey texture categorization parameters to increase the detection accuracy of face spoofing and produce positive experimental outcomes. We contrast our proposed method with alternative algorithms and verify its performance using three publicly available datasets, showcasing its superiority.},
  archive      = {J_IJCIS},
  author       = {Radad, Marwa and Enab, Amira E. and Elagooz, Salah S. and El-Fishawy, Nawal A. and El-Rashidy, Mohamed A.},
  doi          = {10.1007/s44196-025-00757-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Face anti-spoofing detection based on novel encoder convolutional neural network and texture’s grayscale structural information},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MM-HGNN: Multimodal representation learning heterogeneous graph neural network. <em>IJCIS</em>, <em>18</em>(1), 1-26. (<a href='https://doi.org/10.1007/s44196-025-00820-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning heterogeneous graphs are very challenging because of the diverse structures and data modalities. The existing graph neural networks cannot efficiently capture both the multimodality of the data and the inherent heterogeneity of such graphs. In this paper, we propose Multimodal Representation Learning Heterogeneous Graph Neural network (MM-HGNN) to tackle these challenges. MM-HGNN introduces a novel Modality Transferability Function to quantify the heterogeneity between different modalities, which allows the model to dynamically adjust the attention scores and give precedence to unique information that is non-redundant. Additionally, it integrates modality-level attention that distributes attention in an adaptive way over different modalities according to their relevance, enhancing feature representations for tasks such as node classification. To further improve representation learning, a splicing mechanism is proposed to integrate outputs from multiple network layers, combining high-level features for more expressive node embeddings. We validate the effectiveness of MM-HGNN through extensive experiments on the IMDB and Amazon datasets. Our model outperforms several state-of-the-art methods under the Macro-F1, Micro-F1, and AUC metrics by a large margin, which well demonstrates its strong capability in dealing with the challenging multimodal and heterogeneous data. Comprehensive ablation studies further emphasize the contributions of each key component in improving the overall performance.},
  archive      = {J_IJCIS},
  author       = {Bachiri, Khalil and Yahyaouy, Ali and Malek, Maria and Rogovschi, Nicoleta},
  doi          = {10.1007/s44196-025-00820-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {MM-HGNN: Multimodal representation learning heterogeneous graph neural network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An anomaly detection method for industrial system cybersecurity based on GGL-WAVE-CNN. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00832-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting anomalies in industrial system cybersecurity is critical for enabling automated decision-making. Current approaches often struggle to handle complex, unknown topological time series data, thereby necessitating improved anomaly detection accuracy. This paper introduces a novel two-level anomaly detection framework that combines the generalized graph Laplacian (GGL), wavelet decomposition (WAVE), and an enhanced convolutional neural network (CNN). In the first level, the proposed method employs the GGL to efficiently identify abnormal windows in industrial time series data. In the second level, a precise anomaly detection technique is developed to analyze the abnormal windows identified by GGL, leveraging wavelet decomposition for feature extraction and a refined CNN for classification. The effectiveness of the proposed GGL-WAVE-CNN approach is validated using a real-world dataset capturing SCADA system network traffic from a facility in China. Experimental results demonstrate a true positive rate (TPR) of 97.54%, highlighting the robustness and accuracy of the proposed method in addressing complex industrial cybersecurity challenges.},
  archive      = {J_IJCIS},
  author       = {Zou, Bing and Zhang, Ke jun and Yu, Xin Ying and Jin, Yu han and Wang, Jun and Liu, Ling yu},
  doi          = {10.1007/s44196-025-00832-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An anomaly detection method for industrial system cybersecurity based on GGL-WAVE-CNN},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid domain perception combined with multi-expert decoding to improve image forgery localization. <em>IJCIS</em>, <em>18</em>(1), 1-15. (<a href='https://doi.org/10.1007/s44196-025-00892-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancements in multimedia software and hardware technology, image forgery localization has become an important challenge in digital forensics. To improve the efficiency and stability of image forgery detection, we propose a mixed-domain perception and multi-expert decoding recognition model. First, we design an alignment strategy that utilizes both RGB and frequency domain information of images. This strategy adapts to the multi-dimensional distribution characteristics of the original data, enhancing the discrimination of tampered regions. Next, we employ a hybrid expert modeling approach to improve the model’s robustness in the representation space through feature selection and recombination. Additionally, we introduce a region-weighted contrastive learning method to better localize and focus on tampered regions. Experiments on four datasets (CASIA, NIST, COVERAGE, and IMD) show that our proposed model achieves an improvement in AUC ranging from 0.15 to 1.9% compared to the existing advanced methods. These results indicate that our approach contributes to more accurate image forgery localization, offering potential benefits for digital forensics and multimedia security applications.},
  archive      = {J_IJCIS},
  author       = {Gong, Xuchao and Duan, Hongjie and Zhang, Peiying and Wang, Jian and Liu, Kai and Li, Zhaohui},
  doi          = {10.1007/s44196-025-00892-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hybrid domain perception combined with multi-expert decoding to improve image forgery localization},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy optimization and efficiency improvement model for enterprise production process based on deep learning under the background of carbon peak and carbon neutrality. <em>IJCIS</em>, <em>18</em>(1), 1-25. (<a href='https://doi.org/10.1007/s44196-025-00901-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving carbon peak and carbon neutrality requires industries to enhance energy efficiency and optimize resource utilization. Traditional energy management methods rely on rule-based or static optimization approaches, which struggle to adapt to dynamic production environments and fluctuating energy demands. These limitations lead to inefficient energy use, increased operational costs, and challenges in meeting sustainability goals. This research introduces Deep Learning-based Green Optimization for Enterprise Production (DeepGreen-Opt), a deep learning-driven framework designed to analyze energy consumption patterns, predict demand, and optimize resource allocation in real time. The DeepGreen-Opt framework integrates Long Short-Term Memory (LSTM) for accurate energy consumption forecasting and Adaptive Hybrid Particle Swarm Optimization (AHPSO) for dynamic energy optimization. A fuzzy logic-based decision system is incorporated to enhance adaptability under uncertain conditions, enabling real-time adjustments to fluctuating energy demands. The DeepGreen-Opt framework was specifically validated across multiple industrial sectors, including automotive manufacturing, steel production facilities, and chemical processing plants, where intelligent energy management demonstrates significant operational improvements. By implementing DeepGreen-Opt, enterprises can achieve cost-effective production while aligning with sustainability objectives. The framework ensures energy-efficient operations, reducing resource waste and improving production efficiency. Experimental validation on industrial datasets demonstrates a 15% increase in energy efficiency and a 12% improvement in overall production performance compared to existing approaches. This research highlights the potential of DeepGreen-Opt in industrial energy management, providing a foundation for future advancements in intelligent and sustainable production processes.},
  archive      = {J_IJCIS},
  author       = {Bai, Hui and Chen, Yiyi and Bai, Hua and Liu, Meiling and Fan, Yu},
  doi          = {10.1007/s44196-025-00901-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Energy optimization and efficiency improvement model for enterprise production process based on deep learning under the background of carbon peak and carbon neutrality},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SUMMARIA — A XAI support methodology by generating composite linguistic summaries of qualitative data. <em>IJCIS</em>, <em>18</em>(1), 1-39. (<a href='https://doi.org/10.1007/s44196-025-00908-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main stream of Linguistic Data Summarization involves modeling numerical attributes using linguistic variables, which makes it difficult addressing real-world problems with qualitative or mixed data. Literature highlights challenges such as the limited expressiveness of summaries from classical protoforms, the need to explore relationships between them to find more useful patterns and refining language to improve their interpretability and usefulness. The latter is increasingly significant as the demand for explainability grows with the rise of black-box AI applications. This paper proposes SUMMARIA, a XAI support methodology by generating composite (enriched) linguistic summaries of qualitative data. A framework is formalized integrating Linguistic Data Summarization with the concept of rhetorical relation and defining the structure and quality metrics of a composite linguistic summary. Three abstract forms of composite linguistic summaries representing Evidence, Contrast and Emphasis relations are specified, inspired by Rhetorical Structure Theory. Also, a method based on Association Rules Mining, implements SUMMARIA in problem-solving via five algorithms. An empirical study tested SUMMARIA’s application on two judicial datasets and a substantially different behavior was found for four different scenarios, which reveals its sensitivity to the nature and distribution of the primary data. A human-expert validation was performed showing that the linguistic summaries are understandable and the relation type implicit in them is recognizable by the users.},
  archive      = {J_IJCIS},
  author       = {Rodríguez-Rodríguez, Carlos Rafael and Zulueta-Veliz, Yeleny and Gainza-Reyes, Dainys},
  doi          = {10.1007/s44196-025-00908-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-39},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {SUMMARIA — A XAI support methodology by generating composite linguistic summaries of qualitative data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio–Visual synchronization and lip movement analysis for real-time deepfake detection. <em>IJCIS</em>, <em>18</em>(1), 1-30. (<a href='https://doi.org/10.1007/s44196-025-00911-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancements in Artificial Intelligence (AI) and deep learning techniques have led to the creation of highly realistic synthetic media known as deepfakes. These manipulated images, videos, and audio pose significant ethical, security, and privacy concerns. To address this issue, we propose a novel Audio–Visual Synchronisation and Fusion Framework (AVSFF) for real-time detection of deepfakes. This approach focuses on fine-grained lip movement analysis by detecting subtle inconsistencies between lip movements and corresponding audio. By integrating visual and audio features using multimodal fusion techniques, AVSFF aims to distinguish between authentic and manipulated media. The proposed framework is evaluated on diverse datasets, including FakeAVCeleb, AV-Deepfake1M, TVIL, and LAV-DF, demonstrating promising results with accuracies of 0.9973, 0.9760, 0.9890, and 0.9786, respectively. This study contributes to the field by providing a robust real-time solution for detecting deepfakes in audio–visual synthetic data, ensuring enhanced detection accuracy and effective generalization across various deepfake manipulations and demographic data.},
  archive      = {J_IJCIS},
  author       = {Javed, Muhammad and Zhang, Zhaohui and Dahri, Fida Hussain and Laghari, Asif Ali and Krajčík, Martin and Almadhor, Ahmad},
  doi          = {10.1007/s44196-025-00911-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Audio–Visual synchronization and lip movement analysis for real-time deepfake detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resilience analysis of airport systems based on improved bayesian networks. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00914-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern airports, as pivotal nodes in global transportation networks, face increasing resilience challenges from compound threats such as extreme weather events and cyberattacks. However, current assessment methods primarily rely on subjective evaluations and lack probabilistic reasoning to account for the dynamic interdependencies among resilience factors. To address this gap, this study presents a hybrid Bayesian network–best worst method (BN–BWM) framework aimed at improving the accuracy and practicality of airport system resilience assessments. While Bayesian networks are effective for modeling complex probabilistic dependencies, expert-based probability assignments often introduce subjectivity. To mitigate this, we apply the best worst method (BWM) to conduct systematic pairwise comparison. Building on this, we leverage the BWM’s systematic pairwise comparisons, conducted with 10 aviation experts, to generate conditional probability tables for the Bayesian network. The results indicate that large airports demonstrate higher resilience levels (84–85%), whereas medium-sized airports exhibit moderate resilience (79%). Sensitivity analysis identifies key factors influencing resilience, including emergency repair systems and personnel capabilities, thereby offering actionable insights into improving airport operations. This study presents a robust, data-driven framework that enhances the objectivity and accuracy of resilience evaluations, providing theoretical support for sustainable airport management and operational safety.},
  archive      = {J_IJCIS},
  author       = {Guo, Jiuxia and Tong, Xin and Yang, Yungui and Yuan, Jiang and Xu, Siying},
  doi          = {10.1007/s44196-025-00914-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Resilience analysis of airport systems based on improved bayesian networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Attention pyramid convolutional neural network optimized with big data for teaching aerobics. <em>IJCIS</em>, <em>18</em>(1), 1. (<a href='https://doi.org/10.1007/s44196-025-00917-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Chen, Chunmei},
  doi          = {10.1007/s44196-025-00917-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Retraction note: Attention pyramid convolutional neural network optimized with big data for teaching aerobics},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cyberbullying-related automated hate speech detection on social media platforms using stack ensemble classification method. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00919-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hate speech (HS) has grown because of increasing social media platform usage, which includes Twitter, YouTube, and Facebook. The frequent attempts to implement automated detection systems remain unsuccessful at separating hate speech from objectionable language, because user-generated content tends toward informal, brief, and diverse expressions. The determination of hate speech within texts proves exceptionally hard, since precise context detection is needed to distinguish abusive language from neutral statements. Precision in hate speech identification and filtering stands essential, because these online content forms have negative impacts on both minority and majority groups while heightening their conflicts. The research presents a stacked ensemble classification system that classifies tweets into three groups: hate speech, abusive language, or neutral. The framework uses term frequency–inverse document frequency (TF–IDF) extracted from tweet texts for which support vector machine (SVM), together with Random Forest, XGBoost, and Logistic Regression, base machine learning models function as classifiers. The final model outcome results from linking several base learning models into an ensemble configuration. The Kaggle Hate Speech data set served as training material for the system, because it contained 24,784 tweets along with eight attributes. The model performance received improvement through exclusion of manually derived features. The proposed ensemble model demonstrated superior performance with 96% accuracy, while each single classifier had lower accuracy rates (SVM: 93%, Random Forest: 94%, and XGBoost: 88%). The research outcomes show stacking represents an effective method to enhance systems for detecting hate speech operating on social media platforms.},
  archive      = {J_IJCIS},
  author       = {Mubeen, Muhammad and Muskan, Aliza and Akram, Arslan and Rashid, Javed and Alshalali, Tagrid Abdullah N. and Sarwar, Nadeem},
  doi          = {10.1007/s44196-025-00919-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Cyberbullying-related automated hate speech detection on social media platforms using stack ensemble classification method},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced multistep prediction of multivariate market indices using weighted optical reservoir computing. <em>IJCIS</em>, <em>18</em>(1), 1-17. (<a href='https://doi.org/10.1007/s44196-025-00906-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and experimentally demonstrate an innovative weighted optical reservoir computing system for market index prediction. By integrating fundamental market data with macroeconomic indicators and technical metrics, we capture the broader dynamics of the stock market. Our system shows significantly higher performance than the state-of-the-art methods such as linear regression, decision trees, and neural network architectures including long short-term memory. It effectively captures the market’s high volatility and nonlinear behaviors under limited data conditions, demonstrating strong potential for real-time, parallel, multi-dimensional data processing, and prediction.},
  archive      = {J_IJCIS},
  author       = {Wang, Fang and Bu, Ting and Huang, Yuping},
  doi          = {10.1007/s44196-025-00906-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhanced multistep prediction of multivariate market indices using weighted optical reservoir computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The application of deep learning in dance movement design. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00907-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed deep learning for designing dance movements by estimating dance poses. In this study, we proposed Fusion-based Global Dance Pose Patterns with ResNet-152 an approach that resolves class imbalance existing in dance pose datasets with the help of high-resolution global feature fusion during pose estimation. A fusion layer has been added for discerning patterns on both the local and global levels, resulting in a considerable improvement in classifier performance thereby yielding a referred reliability for discriminative power to dance pose classification applications. Refinement in feature extraction and using deep new models such as ResNet-152 for pose recognition have helped to capitalize on model overfitting and worse generalization problems. This approach indeed goes a long way in making dance pose classification more accurate and efficient with possible real-time applications, in addition to a better understanding of the dance movements themselves. Experimental results indicate promising performances, with high accuracies (0.9870), precisions (0.9851), sensitivities (0.9873), F-measures (0.9861), and Kappa (0.9841) constituting proof of the model competency. The workflow comprises a stage for data collection, pre-processing is then applied using Gaussian filtering and histogram equalization to improve image features, the class imbalance is countered using SMOTE, feature extraction on HR-Net, and global and local feature fusion leads to robust pose estimations. ResNet-152 acts as a classifier with an SGD optimizer for better model parameter optimization. This system highly accurately predicts dance poses and efficiently approaches pose estimation in various dance application fields.},
  archive      = {J_IJCIS},
  author       = {Ju, Xiang},
  doi          = {10.1007/s44196-025-00907-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {The application of deep learning in dance movement design},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image classification of imbalanced wood microscope by integrating multi-source features. <em>IJCIS</em>, <em>18</em>(1), 1-16. (<a href='https://doi.org/10.1007/s44196-025-00912-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to factors such as human activities and environmental pollution, the global forest area is continuously diminishing. Consequently, rapid and accurate identification of wood species becomes extremely crucial. With the recent advancements in deep learning, wood classification based on digital images is currently relatively popular method. Nevertheless, there still exist some challenges, such as the difficulty in integrating plant phenological and texture features and the inability to automatically select features of inter-class images. This paper presents a multi-source feature fusion classification, which combines traditional local feature description with automatic feature extraction by deep learning. The proposed method reduces misclassification and addresses the imbalance problem in wood data. Besides, the method is validated on the microscopic cross-sectional images of 75 broad-leaved wood species, with an accuracy of 94.0%, indicating its potential for application in the classification of imbalanced wood data. The comparison experiments with other existing models on the wood dataset demonstrate the effectiveness of the proposed method.},
  archive      = {J_IJCIS},
  author       = {Tian, Zhikang and Zhang, Na and Wang, Jiwei and Sha, Liwei and Liu, Hongping and Zou, Li},
  doi          = {10.1007/s44196-025-00912-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Image classification of imbalanced wood microscope by integrating multi-source features},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning framework for oil shale pyrolysis state recognition using bionic electronic nose. <em>IJCIS</em>, <em>18</em>(1), 1-18. (<a href='https://doi.org/10.1007/s44196-025-00913-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time monitoring of the pyrolysis state of oil shale is crucial for precisely controlling heating temperature and duration, which can significantly reduce extraction costs. However, due to the complexity of in-situ environments, this task is highly challenging and remains one of the key technological barriers in in-situ mining. To address this issue, this paper proposes an end-to-end recognition technology solution for in-situ pyrolysis state of oil shale using electronic nose. The proposed solution integrates Graph Convolutional Network (GCN) and Long Short-Term Memory (LSTM) to capture the spatial correlations among different sensors in the electronic nose and the temporal characteristics of the data, respectively. It is designed to identify both the pyrolysis state classification and the oil shale maturity regression tasks. Our model achieves 93.87% accuracy on the task of classifying the pyrolysis stage of oil shale; the $$R^2$$ on the regression task reaches 0.93. To evaluate its effectiveness, we compare its performance with state-of-the-art (SOTA) methods in this field. Experimental results demonstrate the superiority of our proposed framework, highlighting its effectiveness and advantages over existing methods.},
  archive      = {J_IJCIS},
  author       = {Yuan, Yuping and Weng, Xiaohui and Qiao, Yuheng and Shi, Xiaohu and Chang, Zhiyong},
  doi          = {10.1007/s44196-025-00913-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Deep learning framework for oil shale pyrolysis state recognition using bionic electronic nose},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy-based novel cross-layer RPL objective function for energy-aware routing in IoT. <em>IJCIS</em>, <em>18</em>(1), 1-25. (<a href='https://doi.org/10.1007/s44196-025-00916-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy consumption remains a critical challenge for low-power, resource-constrained Internet of Things (IoT) devices operating over Low-Power and Lossy Networks (LLNs). Addressing this issue requires the development of energy-efficient Objective Functions (OFs) within the RPL (Routing Protocol for Low-Power and Lossy Networks) routing protocol. Traditional OFs primarily used routing layer metrics for parent selection. Therefore, our analysis demonstrates that transmission dynamics at the MAC layer significantly impact overall energy consumption. To tackle this, we introduce a cross-layer energy-efficient objective function (CL-RPL-OF) that incorporates a novel metric, energy per packet (EPP), which quantifies the energy consumed during the transmission and reception of a single data packet. This metric integrates strobe per packet rate (SPPR) and strobe packet success rate, both of which are influenced by radio duty cycling (RDC) mechanisms at the media access control (MAC) layer. The proposed CL-RPL-OF considers node-to-node communication variations arising from relative phase shifts by combining EEP with expected transmission count (ETX) and SPPR using fuzzy logic to select the best path to optimize energy consumption across both routing and MAC layers. Simulation using Cooja and real-world experimentation using the FIT IoT-LAB testbed demonstrate that CL-RPL-OF significantly improves energy efficiency, increases packet delivery ratio, and reduces strobe overhead compared to existing objective functions.},
  archive      = {J_IJCIS},
  author       = {Poornima, M. R. and Vimala, H. S. and Shreyas, J.},
  doi          = {10.1007/s44196-025-00916-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Fuzzy-based novel cross-layer RPL objective function for energy-aware routing in IoT},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating the impact of labeling inaccuracies on 3D human body reconstruction from monocular videos. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00921-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of labeling inaccuracies in 3D human pose and shape reconstruction from monocular videos. Existing methods often rely on noisy pseudo ground truth, which introduces performance degradation such as jittering and drifting. To overcome these limitations, we propose a confidence-aware framework grounded in biomechanics. Our method adopts the SKEL model to provide anatomically constrained pose representations, reducing dependency on imprecise annotations. We further introduce a conditional normalizing flow to model per-parameter uncertainty conditioned on visual and motion features. Additionally, a novel evaluation metric, confidence-weighted procrustes aligned MPJPE, is proposed to incorporate confidence scores into performance assessment. Extensive experiments show that our approach outperforms existing methods on multiple datasets in both accuracy and motion smoothness. It demonstrates strong robustness against noisy annotations, and confidence estimates align closely with actual prediction errors. Ablation studies validate the contributions of both biomechanical modeling and confidence learning. Overall, our framework provides a unified and robust solution for 3D human reconstruction in real-world settings.},
  archive      = {J_IJCIS},
  author       = {Hou, Yupeng and Zeng, Guangping},
  doi          = {10.1007/s44196-025-00921-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Mitigating the impact of labeling inaccuracies on 3D human body reconstruction from monocular videos},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A decision-making framework for multi-objective transportation problem using fully triangular intuitionistic fuzzy sets. <em>IJCIS</em>, <em>18</em>(1), 1-29. (<a href='https://doi.org/10.1007/s44196-025-00915-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a fully triangular intuitionistic fuzzy number model for the multi-objective transportation problem that successfully manages ambiguity and hesitation in decision making. To provide a reliable transportation plan in the case of unpredictability, the suggested methodology uses a multi-objective optimization framework that strikes a balance between cost, time and reliability. A significant component of this study is the weight assignment and multiple decision levels, which enables decision makers to rank goals and customize solutions according to specific transportation needs. This technique makes decision making easier while also provides flexibility in dealing with difficult real-world situations. Furthermore, we compare our proposed model with other approaches that are currently in use, such as the intuitionistic fuzzy approach and the goal programming approach. According to the outcomes, our model provides a more effective and realistic transportation strategy by offering a more balanced trade-off between cost, time and reliability. A numerical example is provided to illustrate the efficacy of the method, showing how combining centre, expected, and ranking values ​​increases solution accuracy and decision reliability while providing a systematic way of comparing and defuzzifying fuzzy numbers.},
  archive      = {J_IJCIS},
  author       = {Joshi, Vishwas Deep and Agarwal, Priya and Alsaud, Huda and Čepová, Lenka and Swarna, B.},
  doi          = {10.1007/s44196-025-00915-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A decision-making framework for multi-objective transportation problem using fully triangular intuitionistic fuzzy sets},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image denoising using quantum deep convolutional generative adversarial network for medical images. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00920-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A significant role is played by medical images in diagnosing diseases and planning the course of treatment. Noise can potentially degrade the quality of images which can lead to misdiagnosis. One of the oldest challenges in computer vision for restoring images that have been corrupted is image denoising. Generative adversarial networks (GANs) are among the most extensively used deep learning methods for various computer vision tasks. Utilizing an innovative quantum adversarial denoising architecture, denoised image samples are produced from a noisy distribution. In this paper, the authors employ an architecture of quantum deep convolutional generative adversarial networks (QDCGAN) for denoising medical images. The architecture of the DCGAN (deep convolutional generative adversarial networks) is augmented with a quantum computing layer to enhance the performance through quantum-generated inputs. The research is performed on the BraTS dataset via the TensorFlow Quantum platform. The study demonstrates that QDCGAN outperforms traditional methods. The proposed method achieves a better PSNR (peak signal-to-noise ratio) and SSIM (structural similarity index measure) value. The study underscores its effectiveness in improving the diagnostic quality of medical images with an 3.4% enhancement in SSIM and 7.35% in PSNR over existing methods, thereby offering tangible benefits for healthcare practitioners and patients alike.},
  archive      = {J_IJCIS},
  author       = {Nandal, Priyanka and Pahal, Sudesh and Upadhyay, Govind Murari},
  doi          = {10.1007/s44196-025-00920-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Image denoising using quantum deep convolutional generative adversarial network for medical images},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative study of hybrid deep learning models for kannada sign language recognition. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00922-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language recognition (SLR) systems continue to face significant challenges in accurately interpreting dynamic gestures, particularly for underrepresented languages like Kannada sign language (KSL). This study presents a novel hybrid deep learning architecture that synergistically combines convolutional neural networks (CNNs), hand keypoints (HKPs), long short-term memory (LSTM) networks, and transformers to achieve robust spatial-temporal-contextual learning for KSL recognition. Developed on a newly curated dataset of 1080 medical-domain KSL gestures, our model addresses critical gaps in dataset diversity and model generalizability. The proposed framework demonstrates superior performance with 97.6% training accuracy, 96.75% validation accuracy, and 81% testing accuracy on unseen data—outperforming conventional CNN-LSTM (46%) and HKP-LSTM (71%) baselines. By hierarchically integrating CNN-extracted spatial features, HKP-derived structural priors, LSTM-processed temporal dynamics, and Transformer-modeled long-range dependencies, this work establishes a new benchmark for KSL recognition while providing a scalable solution for real-world healthcare and assistive technology applications.},
  archive      = {J_IJCIS},
  author       = {Hugar, Gurusiddappa and Kagalkar, Ramesh M. and Das, Abhijit},
  doi          = {10.1007/s44196-025-00922-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Comparative study of hybrid deep learning models for kannada sign language recognition},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Construction of industrial economic management decision model based on three parameter interval grey numbers. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00927-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As China’s industrial strength continues to develop, a growing number of challenges have emerged in the field of industrial management. This study proposes an industrial economic management decision model based on three parameter interval grey numbers to address the issues of high information uncertainty and complexity in industrial economic management decisions. This model characterizes the uncertainty of decision information by introducing three parameter interval grey numbers, and innovatively adopts grey relational clustering decision-making method, combined with genetic algorithm to dynamically optimize key parameters. The results demonstrated that the model exhibited excellent performance on multiple datasets, with an accuracy stable between 95 and 96%. In the application of industrial economic management datasets, the model increased the average profit margin of enterprises by 5.3% and reduced unit costs by an average of 13.3%, which was significantly better than other comparative models. In addition, through comparative experiments with other decision models, this model has shown superiority in improving profit margins, increasing sales, reducing unit costs, and reducing prediction errors. Therefore, the proposed industrial management decision model can effectively handle uncertainty factors, enhance the scientific and effective nature of industrial economic management decisions, and provide more accurate and reliable solutions for industrial enterprises.},
  archive      = {J_IJCIS},
  author       = {Lin, Pao-Ching and Wu, Tzu-Jung and Huang, Jui-Chan},
  doi          = {10.1007/s44196-025-00927-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Construction of industrial economic management decision model based on three parameter interval grey numbers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial network structure characteristics of tourism supply in china: A social network analysis method. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00929-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the spatial network structure of tourism supply is a prerequisite for achieving high-quality development in the tourism industry. Research on the evolutionary characteristics of tourism supply spatial network structures remains unclear. In this study, integrated the modified gravity model and social network analysis (SNA) to investigate the spatial network structure characteristics of tourism supply in China at the provincial level from the years 2004–2019. The results indicate that while the provincial tourism supply spatial network maintained a relatively loose structure, its accessibility and balance showed progressive enhancement. Significant regional disparities emerged in the spatial network characteristics among eastern, central, and western regions. Furthermore, the tourism supply network exhibits a dual-polarized core-periphery structure. Different role subgroups were divided into four levels: core leader, important leader, general partner, and marginal partner. This research advances theoretical and methodological frameworks in tourism supply studies. The findings can inform policy formulation for enhancing regional tourism collaboration and optimizing network relationships.},
  archive      = {J_IJCIS},
  author       = {Yu, Hongyan and Zhou, Wenming},
  doi          = {10.1007/s44196-025-00929-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Spatial network structure characteristics of tourism supply in china: A social network analysis method},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive feature-driven PCOS predictor: A reinforcement learning-based binary equilibrium optimization approach. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00931-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poly-cystic ovary syndrome (PCOS) is a prevalent condition that impinges women in their prime reproductive years. Its primary trait is the elevated levels of androgen, a male hormone in female body and leads to several symptoms, including ovarian cysts, irregular menstruation cycles, infertility, obesity, and excessive hair development. Since PCOS exact cause is unknown and its symptoms are uncertain, a timely and accurate diagnosis is essential. In these situations, a (ML) Machine Learning-based PCOS prediction model aids in the diagnostic procedure, addresses time constraints and potential inaccuracies. A plethora of feature selection techniques have been built to discover the best optimal features for the classification of medical dataset. However, finding an efficient solution is still difficult due to noise and redundant information which may degrade the model performance. In this article, a hybrid filter-wrapper approach is proposed to identify the optimal attributes. An ensemble filter method is built, the union of top k attributes from individual filters is considered for further process. Then, Reinforcement Learning-based Binary Equilibrium Optimizer is used to find the reduced optimal features. Here, RL uses SARSA (State–Action–Reward–State–Action) to increase the population diversity in the search space, to avoid the problem of local optima, finally balances the exploration capability during the search. Then, identified optimal features are given as input to BMFK (Bonferroni Mean Fuzzy KNN) classifier and achieved 96.32% of accuracy. Further, several machine learning classifiers have been employed for the prompt diagnosis of PCOS. Finally, Random Forest has produced the highest accuracy of 95.62% than other models.},
  archive      = {J_IJCIS},
  author       = {Reka, S. and Praba, T. Suriya and Manchala, Krishna Kumar and Venkateswarlu, Anna},
  doi          = {10.1007/s44196-025-00931-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Comprehensive feature-driven PCOS predictor: A reinforcement learning-based binary equilibrium optimization approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction and optimization of civil aviation flight delays based on machine learning algorithms. <em>IJCIS</em>, <em>18</em>(1), 1-26. (<a href='https://doi.org/10.1007/s44196-025-00932-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The civil aviation industry continues to face the significant problem of flight delays, which impacts operational efficiency and passenger satisfaction. This research aims to develop an innovative model that accurately predicts civil aviation flight delays and provides insights related to performance enhancement. The proposed Flight Delay Prediction Network with Spatio-Temporal Learning (FlightNet-ST) is a hybrid deep learning architecture that combines Long Short-Term Memory (LSTM) networks, Graph Convolutional Networks (GCNs), and 3D Convolutional Neural Networks (3D-CNNs) to achieve this goal. The model is trained using datasets of domestic flights that include geographical, operational, and temporal data, such as geographical separation, origin–destination pairs, airline rules, scheduled departure times, and dates. The approach involves running time series data through LSTM to capture temporal dependencies, applying 3D Convolutional Neural Networks (3D-CNNs) to analyze aircraft route grids dynamically, and utilizing Graph Convolutional Networks (GCNs) to discover topological patterns from spatial airport connectivity. Delay prediction is powered by a unified representation that fuses these disparate elements. Based on the experimental data, FlightNet-ST achieves a 14.47% reduction in Mean Absolute Error (MAE). Additionally, an attention mechanism enhances interpretability by highlighting key aspects that influence delays, such as departure time blocks and airport-specific trends. Finally, FlightNet-ST helps with civil aviation flight delay prediction and management with its data-driven, interpretable, and robust solution. This methodology facilitates real-time operational decision-making and provides tactics to mitigate delays.},
  archive      = {J_IJCIS},
  author       = {Zhong, Qingwei and Yu, Yingxue and Huang, Yiru and Zhang, Tianhang},
  doi          = {10.1007/s44196-025-00932-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Prediction and optimization of civil aviation flight delays based on machine learning algorithms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coal price forecasting using CEEMDAN decomposition and IFOA-optimized LSTM model. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00923-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel hybrid forecasting model for coking coal prices, integrating complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) and long short-term memory (LSTM) neural networks, enhanced by an improved fruit fly optimization algorithm (IFOA). The approach begins with CEEMDAN decomposing the coking coal price sequence into intrinsic mode functions (IMFs) and a residual component, effectively mitigating non-stationarity and nonlinearity. High- and low-frequency IMFs are differentiated using a single-sample T-test, with high-frequency components consolidated to minimize noise interference. Subsequently, the IFOA algorithm optimizes LSTM hyperparameters, boosting both generalization and prediction precision. Empirical validation, leveraging the Platts price index for four major imported coking coal varieties, demonstrates that the CEEMDAN-IFOA-LSTM model significantly outperforms a broad range of benchmarks, including ANN, IFOA-LSSVR, CEEMDAN-LSTM, LSTM, BiLSTM, TCN, IFOA-LSTM, CEEMDAN-FOA-LSTM, CEEMDAN-PSO-LSTM, and CEEMDAN-GA-LSTM, achieving reduced root mean square error (RMSE) and mean absolute percentage error (MAPE). The study concludes that this model adeptly addresses the challenges of nonlinear coupling and hyperparameter optimization, offering a reliable tool for coking coal price forecasting. Future research will aim to refine the model further to adapt to diverse market conditions and enhance forecasting accuracy.},
  archive      = {J_IJCIS},
  author       = {Liu, Zhuang and Li, Xiaotuan},
  doi          = {10.1007/s44196-025-00923-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Coal price forecasting using CEEMDAN decomposition and IFOA-optimized LSTM model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling flight operation hazards’ interrelation via TEM model and network analysis. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00935-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent severe aviation accidents have underscored the intricate challenges within the aviation system, necessitating a more comprehensive understanding of hazards’ interrelation in flight operation. This study integrates the Threat and Error Management (TEM) framework with complex network theory to construct a Flight Operation Hazard Network (FOHN) based on 260 aviation occurrence reports. The FOHN, comprising 120 nodes (hazards) and 208 edges (causal links), exhibits unique structural characteristics: a low clustering coefficient (0.03), long average path length (4.20), and an exponential degree distribution (R2 = 0.994), confirming the absence of small-world and scale-free properties. These features reflect aviation’s defense-in-depth strategy, where sparse connectivity aims to isolate hazards. Analysis of the FOHN’s response to simulated hazard mitigation reveals its relative stability against non-targeted hazard removal but significant disruption when critical high-betweenness hazards are targeted for mitigation. The study identifies crew navigation errors as key risk mediators, bridging technical failures and organizational deficiencies. By proposing actionable enhancements grounded in these network findings—including resilience-augmented training, flight data-driven risk prioritization, and regulatory updates—this work provides a network-centric framework to address “black swan” risks in modern aviation systems.},
  archive      = {J_IJCIS},
  author       = {He, Peng and Zhang, Zhaoning and Sun, Ruishan},
  doi          = {10.1007/s44196-025-00935-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Unveiling flight operation hazards’ interrelation via TEM model and network analysis},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved crime prediction using hybrid neural architecture search together with hyperparameter tuning. <em>IJCIS</em>, <em>18</em>(1), 1-38. (<a href='https://doi.org/10.1007/s44196-025-00888-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different parts of the world have recorded an escalating number of criminal incidents, burdened the judicial system, and adversely impacted national security and economic development. Accurate prediction of crimes is crucial for law enforcement agencies to prevent proactively criminal activity and allocate resources effectively. Existing methods often address architecture design and hyperparameter tuning as separate processes. We present a combination of neural architecture search and hyperparameter tuning for improved crime prediction. The study method achieved automation of architecture discovery and fine-tuning of hyperparameters by utilizing Neural Architecture Search (NAS) to explore a wide range of neural network architectures for crime prediction and optimizing the hyperparameters of the discovered architecture for peak performance in binary crime prediction, respectively. The study used three datasets: criminal cases dataset (self-collected dataset), Vancouver crime data, and Austin crime data. The criminal cases dataset is extracted from a confidential database from certain countries, focusing on sensitive parts of those countries. The Vancouver crime and Austin crime datasets were sourced from the Kaggle website. The study considered the robust rank aggregation (RRA) feature selection method to rank and select the best features to predict crime behavior in some countries. The chosen features using robust rank aggregation included current position, age range, month, prisoner condition, and identified/unidentified (ide/unide). The hyperparameter tuning model of Architecture Search (NAS +) produced superior results across all datasets with an accuracy of 89.29% (AUC-ROC = 94.82% and recall = 64.54%) in the criminal cases dataset, 60.37% (AUC-ROC = 50.00% and recall = 100.00%) in the Vancouver dataset, and 86.68% (AUC_ROC = 65.40% and recall = 100.00%) in the findings which demonstrated that the proposed approach consistently outperforms conventional methods, making it an effective solution for the prediction of real-world crimes.},
  archive      = {J_IJCIS},
  author       = {Alshahrani, Rami Ayied and Khanzada, Tariq Jamil Saifullah},
  doi          = {10.1007/s44196-025-00888-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Improved crime prediction using hybrid neural architecture search together with hyperparameter tuning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved gaining sharing knowledge-based algorithm for solving resource allocation problems. <em>IJCIS</em>, <em>18</em>(1), 1-63. (<a href='https://doi.org/10.1007/s44196-025-00909-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript presents a procedure to deal with the complicated unbounded knapsack optimization problem with a combination of Total Value greedy heuristic (TV) and Integer Gaining Sharing metaheuristic (IGSK) algorithms in the framework of a divide-and-conquer strategy to lessen the search space and point the searching endeavor to an intensive, further hopeful area. IGSK is an integer version of lately evolved Gaining Sharing knowledge-based optimization algorithm (GSK), dependent upon the properties of GSK, IGSK is used to deal with the problem having integer decision variables. The GSK algorithm imitates the operation of gaining and sharing knowledge throughout the individual living cycle. It is established upon twain pivotal phases, apprentice gaining and sharing step and elder gaining and sharing step. Moreover, to enhance the execution of IGSK and prohibit the solutions from entrapping toward the inside of local optima, IGSK with dynamic elitism scheme is presented. It reduces the elite population size progressively with a linear decreasing, non-linear slow decreasing, non-linear rapid decreasing and non-linear exponential decreasing functions, (LDE, NLDSE, NLDRE, NLDEE), respectively. The proposed hybridizing of an Integer Gaining-Sharing Knowledge-based with Population Size Reduction metaheuristic and Total Value greedy heuristic (PR-IGSK - TV) algorithm with fixed and dynamic elitism schemes implemented in collection of unbounded knapsack problems with different dimensions and correlation categories, which demonstrate that PR-IGSK - TV hybrid algorithm with fixed and dynamic elitism scheme proved a capability to deal with unbounded knapsack problem concerning convergence, goodness and robustness.},
  archive      = {J_IJCIS},
  author       = {Kamal, Ayman and Roshdy, Heba Said and Hassan, Naglaa Ragaa Said and Mohamed, Ali Wagdy},
  doi          = {10.1007/s44196-025-00909-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-63},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An improved gaining sharing knowledge-based algorithm for solving resource allocation problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A residual-corrected hybrid ARIMA–CNN–LSTM framework for high-accuracy tobacco sales forecasting in regulated markets. <em>IJCIS</em>, <em>18</em>(1), 1-25. (<a href='https://doi.org/10.1007/s44196-025-00930-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a common consumer product threatening public health, tobacco not only hinders the development of national public health, but also plays a significant impact on the national economy. The ARIMA model is reliable in learning linear or regular relationships, while the deep learn, such as convolutional neural network (CNN) and long short-term memory network (LSTM), is superior when capturing and learning nonlinear relationships. Combining time-series forecasting models with deep learning technologies, the hybrid architecture could integrate advantages and optimize forecasting effect. In this paper, leveraging 2023 daily sales data from a Southern Chinese tobacco company, this study proposes a new hybrid deep learning framework that integrates ARIMA, CNN, and LSTM models to address these inherent limitations and enhance prediction accuracy. This architecture decomposes forecasting tasks into linear trend analysis and nonlinear residual learning. The ARIMA component learns the linear relationship, and the CNN–LSTM component plays the role in the residual-driven correction. They enable synergistic capture of temporal dependencies and localized anomalies and enhancing the fitting effect. This hybrid model's optimization primarily relies on the residual-driven correction mechanism in the CNN–LSTM component, which significantly enhanced the model interpretability ( $${R}^{2}$$ : 0.95, enhance 10.5% compare with ARIMA model, enhance 13.1% compare with CNN-LSTM model). This research not only advances hybrid deep learning methods, but also provides a scalable solution for precise predictions in dynamic markets. This excellent forecasting results could also be practiced in inventory optimization and policy impact studies.},
  archive      = {J_IJCIS},
  author       = {Huang, Shiyu and Zhou, Lili},
  doi          = {10.1007/s44196-025-00930-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A residual-corrected hybrid ARIMA–CNN–LSTM framework for high-accuracy tobacco sales forecasting in regulated markets},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEDSWIN-net: Dual encoder dilated convolution and swin transformer network for the classification of liver CT images. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00937-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic liver disease (CLD) segmentation and classification present significant challenges due to inaccurate diagnosis and misleading clinical treatment processes. In contemporary times, artificial intelligence (AI) and deep learning (DL) models are considered important tools for driving quantitative biomarkers for efficient stratification and computer-aided diagnosis support. However, prevailing DL models necessitate further enhancements as they rely on flawed assumptions that tumors exhibit non-complex physical structures and uniform image boundaries. To resolve this concern, this paper suggests a novel dual encoder deep learning structure named DEDSWIN-Net to mitigate this problem. The proposed framework consists of four components: a dilated convolution-based encoder, a transformer-based encoder, a multi-scale multiple feature fusion decoder (MSMFD), and a DL training model. The dilated convolution layers can retrieve fine spatial features, whereas Swin models are designed to obtain global information. Both these features are integrated by the MSMFD component that enhances segmentation. Finally, the deep learning training model is constructed using feed-forward principles for better classification of CLD. Comprehensive evaluation is conducted utilizing the LiTS2017-CT Image dataset comprising 100 hepatic disorders, and its performance is evaluated using ten powerful metrics: Jaccard Index (Jcc), dice similarity coefficient (DICE), precision (P), accuracy (Acc), specificity (Spec), recall (R), F1 score (FS), average symmetric surface distance (ASSD), Hausdorff distance (HD), and Intersection over Union (IoU). Furthermore, existing state-of-the-art DL frameworks are considered for comprehensive examination. Results demonstrate that the suggested methodology reveals supreme performance in terms of segmentation and classification with DICE: 0.984, JC: 0.92, IoU: 0.02, precision: 0.95, recall: 0.940, ASSD: 0.64, and HD: 0.32. Moreover, the exploratory outcomes prove the effectiveness of the suggested methodology in obtaining relevant characteristics, fuelling a better computer-aided diagnosis system for CLD.},
  archive      = {J_IJCIS},
  author       = {Allenki, Jyoshna and Soni, Hemant Kumar},
  doi          = {10.1007/s44196-025-00937-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {DEDSWIN-net: Dual encoder dilated convolution and swin transformer network for the classification of liver CT images},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic approach for malware detection in IoT devices: Enhancing security and performance. <em>IJCIS</em>, <em>18</em>(1), 1-17. (<a href='https://doi.org/10.1007/s44196-025-00939-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet of Things (IoT) devices has introduced significant security challenges, particularly concerning the detection and mitigation of malware threats. This study presents a systematic approach to malware detection that aims to improve both the security and performance of IoT systems. Using the IoT23 dataset, which contains a wide range of network traffic patterns from various IoT devices and malware families, the research explores and evaluates multiple machine learning techniques. These include ensemble methods such as Bagging, Stacking, Voting, AdaBoost, and H2O AutoML, as well as advanced models such as sparse neural networks with pruning and feature selection and regularized classifiers L1. The primary objective is to develop lightweight yet highly accurate models suitable for deployment on resource-constrained IoT devices. A comprehensive comparison of these techniques demonstrates the importance of achieving a balance between detection accuracy and computational efficiency. Among the models evaluated, the SNIPE approach shows the best performance, achieving an accuracy of 91.9% while maintaining minimal computational overhead. This makes it particularly well suited for real-world IoT environments, where performance and energy efficiency are critical. The findings of this study provide valuable insights for the development of robust, scalable, and resource-aware malware detection systems, laying a strong foundation for future research and practical cybersecurity solutions in the rapidly evolving IoT landscape.},
  archive      = {J_IJCIS},
  author       = {Pai, Vasudeva and Karthik Pai, B. H. and Sudhiksha, G. S. and Kamath, Vandya and Varsha, K. and Manjunatha, S.},
  doi          = {10.1007/s44196-025-00939-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Systematic approach for malware detection in IoT devices: Enhancing security and performance},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault diagnosis of hot rolling equipment under uncertain conditions based on cloud rough model, game theory, and improved GRA-TOPSIS. <em>IJCIS</em>, <em>18</em>(1), 1-32. (<a href='https://doi.org/10.1007/s44196-025-00945-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hot-rolled strip steel is a critical process in steel production, where equipment stability directly impacts product quality. Traditional evaluation methods struggle to balance uncertainties between influencing factors and expert judgments. This study proposes a novel fault evaluation framework: first, a cloud rough model is adopted to handle multi-source uncertainties in expert evaluations; second, game theory is used to optimize the combined weights of subjective and objective criteria; finally, an improved Grey Relational Analysis-Technique for Order Preference by Similarity to Ideal Solution (GRA-TOPSIS) method constructs a dual-scale measurement model by fusing grey relational degree and Euclidean distance, effectively reducing the influence of geometric sequence similarity and distance on fault prioritization and enhancing the robustness of ranking results under complex working conditions. A case study on hot rolling loopers demonstrates a diagnostic accuracy of 90%. Comparative analyses with seven typical multicriteria decision-making methods indicate the proposed method's feasibility and effectiveness in practical applications, offering a novel reference for equipment status evaluation.},
  archive      = {J_IJCIS},
  author       = {Hu, Bo and Zhang, Yongjun and Jing, Fengwei},
  doi          = {10.1007/s44196-025-00945-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Fault diagnosis of hot rolling equipment under uncertain conditions based on cloud rough model, game theory, and improved GRA-TOPSIS},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPOT-CRIME: Suspicious person and object tracking system for real-time crime monitoring. <em>IJCIS</em>, <em>18</em>(1), 1-21. (<a href='https://doi.org/10.1007/s44196-025-00893-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research work proposes a novel SPOT-CRIME model for detecting and tracking suspicious objects and people from video footage to assist the crime scene analysis. The SPOT-CRIME model has three different monitoring phases for detecting and tracking suspicious object and person. The deep learning-based YOLO network is used in the Primary Monitoring Phase to identify potentially suspicious activities based on individual pose and motion estimates. A suspicious activity database is used for further analysis to detect the activities accurately. Secondary Monitoring Phase uses stochastic gradient Grad-CAM to identify objects related to individuals in the scene using suspicious object detection. Correlations between suspicious objects and activities detected in the first phase enable the detection of these objects. The Tertiary Monitoring Phase combines the outputs of the previous phases for tracking people and objects. In Track RCNN, individuals and suspicious objects are tracked robustly and associated with multiple frames. Detection accuracy of 98.54% was achieved by the proposed SPOT-CRIME model on benchmark datasets. Moreover, the performance of the SPOT-CRIME model progresses the overall accuracy range by 6.63, 1.05, 8.66 and 3.15% better than Seven-layered CNN, Enhanced CNN, Lightweight multiclass-CNN and CNN-BiLSTM respectively.},
  archive      = {J_IJCIS},
  author       = {Jaffrin, Lijetha C. and Rani, D. Esther and Sobia, M. Carmel and Radhakrishnan, P. and Ahilan, A. and Senthil, D Siva},
  doi          = {10.1007/s44196-025-00893-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {SPOT-CRIME: Suspicious person and object tracking system for real-time crime monitoring},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic state cluster-based particle swarm optimization algorithm. <em>IJCIS</em>, <em>18</em>(1), 1-37. (<a href='https://doi.org/10.1007/s44196-025-00902-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitness landscape (FL) is an effective tool for describing and analyzing the real-time dynamics of the search process, offering valuable insights into the population’s varying states. In particle swarm optimization for complex optimization challenges, parameter selection significantly influences performance across various population states. However, current methods for constructing fitness landscapes demonstrate insufficient theoretical analysis of state parameters and involve high construction time costs. To address these limitations, this paper introduces a dynamic state cluster-based particle swarm optimization (DSCPSO) algorithm, which employs population phenotypic entropy based on clustering technique. (1) The algorithm provides theoretical splitting points by mathematically analyzing the population into four states: convergence, exploitation, escape, and exploration, enabling more effective parameter adaptive mechanisms. (2) DSCPSO incorporates sinusoidal chaos mapping to dynamically adjust inertia weights, allowing particles to better align with the population’s evolutionary state. (3) During the convergence state, an intelligent particle migration strategy (IPMS) enhances search efficiency within the solution space, preventing unnecessary computational resource consumption. Eventually, comparative analysis with 10 advanced existing algorithms on the CEC2017 and CEC2022 benchmark suites demonstrates that DSCPSO achieves competitive performance across over 70% of the functions, validating the algorithm’s effectiveness and superiority. In addition, the Wilcoxon-test of the algorithm verifies the validity of the algorithm, and also applies the algorithm to a high-dimensional feature selection problem, which demonstrates the ability of the proposed algorithm to solve real-world problems.},
  archive      = {J_IJCIS},
  author       = {Diao, Zhenya and Yu, Fei and Wu, Hongrun and Xia, Xuewen},
  doi          = {10.1007/s44196-025-00902-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A dynamic state cluster-based particle swarm optimization algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study of multi-distributed resource equalization allocation for virtual power plants based on genetic-heuristic algorithm. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00941-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-resource balanced allocation method using a genetic-heuristic fusion algorithm is proposed to address the imbalance in distributed power generation resource allocation and the over-generation problem in virtual power plants. By establishing models of wind, solar, storage, and controllable load characteristics, an optimization model is constructed with objectives of resource allocation balance and minimization of call costs, subject to constraints such as power balance. Combining the global search capability of a genetic algorithm and the local optimization capability of an ant colony algorithm, the genetic algorithm stage adopts real-number encoding and a dynamic crossover-mutation strategy, while the ant colony algorithm stage optimizes the pheromone update mechanism to avoid premature convergence. The experimental results show that this method achieves 100% accurate allocation of resources without any over-generation occurrences and reduces the resource allocation deviation rate by 32–67% compared to alternative methods. The algorithm demonstrates fast convergence, yielding solutions in less than 0.6 s across 14 repeated experiments, with an average convergence time reduction of 42% compared to traditional algorithms. Under a comprehensive fluctuation scenario with 30% renewable energy fluctuation rate and 15% load forecasting error, the system stability index remains at 0.865, demonstrating the algorithm’s efficiency and robustness under complex conditions and providing an effective approach for optimizing virtual power plant resource allocation.},
  archive      = {J_IJCIS},
  author       = {Li, Haifeng and Jin, Tao and Xu, Xian and Shi, Lin},
  doi          = {10.1007/s44196-025-00941-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A study of multi-distributed resource equalization allocation for virtual power plants based on genetic-heuristic algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning based acceptance criteria for metaheuristic algorithms. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00924-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes novel reinforcement learning-based acceptance criteria for metaheuristic algorithms. We develop Q-learning and Deep Q-learning-based acceptance criteria and integrate them into simulated annealing (SA) and artificial bee colony (ABC) algorithms. Also, we design two versions of these novel acceptance criteria: the online version and the offline version of Q-learning and deep Q-learning based acceptance criteria. The online version starts to train itself and to make decisions to accept or reject the candidate solution with the start of the metaheuristic. The offline version uses a trained and well-tuned Q-learning and deep Q-learning based acceptance criteria to make accept/reject decisions. Our experimental study compares our proposed acceptance criteria with existing ones, such as fuzzy rule-based acceptance (FRBA) and simulated annealing-like acceptance (SALA) criteria. The experiment reveals that metaheuristics with deep Q-learning-based offline acceptance criteria outperform metaheuristics with existing acceptance criteria and other variants in this study.},
  archive      = {J_IJCIS},
  author       = {Arık, Oğuzhan Ahmet and Toğa, Gülhan and Atalay, Berrin},
  doi          = {10.1007/s44196-025-00924-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Reinforcement learning based acceptance criteria for metaheuristic algorithms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAGDM for evaluating the role of computer science in education with PLq-ROF heronian mean operators and maximizing deviation-MOORA methodology. <em>IJCIS</em>, <em>18</em>(1), 1-33. (<a href='https://doi.org/10.1007/s44196-025-00925-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer science (CS) has changed educational processes in a wide range of academic fields and is now considered to be an essential part of modern education. This manuscript presents a novel methodology in which CS can be used to solve judgment concerns in education. The conventional education system frequently finds it difficult to deal with the ambiguities and linguistic expressions that are an integral component of human judgment and choice-making processes. By combining probabilistic uncertainty with linguistic concepts, a probabilistic linguistic q-rung orthopair fuzzy set (PLq-ROFS) presents an efficient mathematical methodology for describing inaccurate and unclear facts to handle these concerns. For aggregating the assessment data, we first propose the PLq-ROF Heronian mean operators. The maximizing deviation methodology is a helpful tool when dealing with situations where the knowledge about the attribute weights is either partially or fully unknown. To find attribute weights, this study presents the PLq-ROF-maximizing deviation methodology. We also propose the multi-objective optimization by ratio analysis (MOORA) methodology with PLq-ROFNs to evaluate the options and identify an optimal choice. Furthermore, a case study related to the judgment process on CS applications in education is provided from the PLq-ROFS perspective to illustrate the effectiveness and use of the developed technique. The numerical results show that $${\mathbb {A}}_{4}$$ (namely: skill development) is the best CS application in education.},
  archive      = {J_IJCIS},
  author       = {Shafiq, Aqsa and Rasheed, Muhammad Waheed and Habeeb, Marwah Shaker and Tasneem, Rabia and Shabbir, Nimra and Alameri, Abdu},
  doi          = {10.1007/s44196-025-00925-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {MAGDM for evaluating the role of computer science in education with PLq-ROF heronian mean operators and maximizing deviation-MOORA methodology},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy neural network approach for nanomaterials analysis in nanoelectronics under fuzzy credibility information. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00938-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nanomaterials are a key component of nanoelectronics and selecting the most suitable nanomaterial for nanoelectronics remains a significant challenge for companies. The classical decision making process is often difficult and uncertain when identifying the ideal nanomaterial. To address this, we develop a novel decision making model based on a fuzzy credibility neural network with Hamacher aggregation operators. We apply the proposed model to select the most suitable nanomaterial for nanoelectronics. For this purpose, we collect information matrices from three experts regarding various nanomaterials. To analyze these matrices, we use entropy to calculate the weights of each criterion. After determining the weights, we apply fuzzy credibility Hamacher weighted aggregation operators to combine the input signals and their corresponding weights in order to compute the hidden layer information for the nanomaterials. To ensure accurate and reliable results, we apply the fuzzy credibility Hamacher weighted aggregation operator once again to the hidden layer information, aggregating it with the appropriate weights to generate the output layer information. Next, we use a score function based on fuzzy credibility numbers to calculate the score values of the output information. After this, we apply three activation functions to compute the final output of the proposed model. Based on the results, graphene is identified as the best nanomaterial for nanoelectronics. Furthermore, we perform a sensitivity analysis of the proposed model by varying the Hamacher parameter. To confirm the effectiveness and accuracy of the proposed approach, we finally validate the results using three well-known MCDM methods.},
  archive      = {J_IJCIS},
  author       = {Ullah, Ihsan and Abdullah, Saleem and Nawaz, Marya and Ahmadzai, Hameed Gul},
  doi          = {10.1007/s44196-025-00938-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A fuzzy neural network approach for nanomaterials analysis in nanoelectronics under fuzzy credibility information},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive covariance and quaternion-focused hybrid error-state EKF/UKF for visual-inertial odometry. <em>IJCIS</em>, <em>18</em>(1), 1-32. (<a href='https://doi.org/10.1007/s44196-025-00942-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.},
  archive      = {J_IJCIS},
  author       = {Asil, Ufuk and Nasibov, Efendi},
  doi          = {10.1007/s44196-025-00942-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Adaptive covariance and quaternion-focused hybrid error-state EKF/UKF for visual-inertial odometry},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust InceptionV3 with novel EYENET weights for di-EYENET ocular surface imaging dataset: Integrating chain foraging and cyclone aging techniques. <em>IJCIS</em>, <em>18</em>(1), 1-26. (<a href='https://doi.org/10.1007/s44196-025-00943-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting diabetic types from ocular surface eye images is a challenging task due to subtle variations in features and the potential overlap in presentations among different diabetic types. While AI-based algorithms have shown promise in distinguishing these nuances, gaps remain in the accuracy and adaptability of existing models, especially in the context of medical imaging for diabetes classification. This study addresses these gaps by proposing a novel integration of AI and medical imaging through the Manta-ray Foraging Optimization (MRFO) algorithm, which leverages cyclone aging (CA) and chain foraging (CF) strategies. We couple MRFO with hierarchical feature learning to optimize the InceptionV3 model, achieving optimal hyperparameter configuration and enhancing both accuracy and computational efficiency. The novelty of this work lies in the introduction of a newly curated dataset, Di-EYENET, which is specifically designed for diabetic eye studies and contains multiclass categories (Type-1, Type-2, and non-diabetic). Di-EYENET fills a significant gap in diabetic ocular research by offering a reliable, validated resource for training models on eye image datasets with distinct characteristics. Our results demonstrate that the InceptionV3 model, fine-tuned with the newly developed EYENET weights, outperforms both traditional ImageNet weights and other pretrained models, showing a 2% accuracy improvement. This research highlights the potential of nature-inspired optimization algorithms and tailored datasets to enhance AI model robustness and adaptability in the context of medical disease diagnosis, particularly in the field of diabetic eye disease.},
  archive      = {J_IJCIS},
  author       = {Khan, Muhammad Ahmad and Khan, Saif Ur Rehman and Rehman, Hafeez Ur and Aladhadh, Suliman and Lin, Ding},
  doi          = {10.1007/s44196-025-00943-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Robust InceptionV3 with novel EYENET weights for di-EYENET ocular surface imaging dataset: Integrating chain foraging and cyclone aging techniques},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced bio-inspired GWO–DE technique for efficient feature selection in the EEG-RSVP paradigm. <em>IJCIS</em>, <em>18</em>(1), 1-33. (<a href='https://doi.org/10.1007/s44196-025-00944-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent noise and high dimensionality of EEG signal make it more difficult to accomplish fast and accurate classification, which is particularly challenging in rapid serial visual presentations (RSVP) target recognition tasks based on EEG. Conventional feature selection approaches have difficulty in reducing dimensions and keeping a good classification performance at the same time. To resolve these issues, we present a bio-inspired hybrid optimization framework where Differential Evolution (DE) is integrated with Grey Wolf Optimization (GWO) to improve the efficiency of feature selections. Our method utilizes the focal loss for combating data imbalance in EEG datasets, the Fisher score for enhancing the discrimination capability of the classes, and the average pairwise Pearson correlation for the reduction of redundant information between features in RSVP datasets. In such a sense, GWO functions as an exploration tool of the feature space, while DE further refines good candidates through a local exploitation. CFSFs are tested with a variety of classifiers ranging from traditional classifiers to deep learning, such as support vector machine (SVM), light gradient boosting machine (LightGBM), convolutional neural network (CNN), long short-term memory (LSTM), etc. Compared with those traditional optimization and classification pipelines, our hybrid GWO–DE method converges faster displacement, more efficient dimension reduction, higher detection accuracy, and classification accuracy, and F1 score result showed us 98.79% and 97.7%, respectively. This efficient method is applicable to real-time EEG applications, including cognitive monitoring, neuro-feedback, and biometric identification.},
  archive      = {J_IJCIS},
  author       = {Abinayaa, S. and Sridhar, S. S.},
  doi          = {10.1007/s44196-025-00944-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An enhanced bio-inspired GWO–DE technique for efficient feature selection in the EEG-RSVP paradigm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach for diagnosis of osteoporosis integrating clinical decision support with generative adversarial networks. <em>IJCIS</em>, <em>18</em>(1), 1-25. (<a href='https://doi.org/10.1007/s44196-025-00846-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Osteoporosis is a bone illness that minimizes bone strength and increases fracture risk. Machine learning methods have been applied to diagnose osteoporosis. However, the accuracy of osteoporosis disease prediction has not improved, nor has the time taken been reduced. To improve the accuracy of osteoporosis disease prediction efficiently, the Rosenthal canonical correlative explainable deep convolutional generative adversarial network (RCCEDCGAN) is proposed. It consists of four main processes: data acquisition, preprocessing, feature selection, and classification. Data samples are collected during data acquisition. Preprocessing includes using multivariate linear regression to fill in missing values and a Q statistical proximal test to identify outliers. Feature selection is carried out using Rosenthal canonical variants analysis for identifying and selecting pertinent features with less time. Finally, explainable deep convolutional generative adversarial network (EDCGAN) is employed for classifying and predicting osteoporosis. The clinical decision support system utilizes EDCGANs for osteoporosis risk prediction analysis based on the Rand indexive decision stump model to assist healthcare professionals in diagnosis and treatment planning. The quantitatively analyzed results show that the RCCEDCGAN method improved by 5% in disease prediction accuracy, precision, recall, F1 score, and 9% specificity compared to the RR model and modified GP classifier. RCCEDCGAN method showed a p-value of less than 0.05 and a confidence interval of 95% for developing osteoporosis. In addition, the RCCEDCGAN method reduced prediction time by 8% compared to the RR model and modified GP classifier techniques. Hence, the RCCEDCGAN is an effective approach for early diagnosis and risk reduction of osteoporosis, aiding in prevention and management strategies.},
  archive      = {J_IJCIS},
  author       = {Raja, M. and Reddy, Avulapalli Jayaram},
  doi          = {10.1007/s44196-025-00846-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel approach for diagnosis of osteoporosis integrating clinical decision support with generative adversarial networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of spectral clustering algorithm in cognitive diagnosis model: Approach for student’s psychological growth. <em>IJCIS</em>, <em>18</em>(1), 1-15. (<a href='https://doi.org/10.1007/s44196-025-00849-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of intelligent education, the diagnostic performance of the traditional cognitive diagnostic model has been unable to meet the needs of today’s education. This study uses the Gaussian mixture model (GMM) to model and optimize model parameters through maximum probability estimation. The spectral clustering (SC) algorithm iterative optimization was combined with a similarity matrix and Laplacian matrix to construct an improved spectral clustering cognitive diagnosis model. The proposed SC algorithm’s clustering accuracy was 0.95, ARI was 0.86, and FMI was 0.85, and its clustering performance was better than that of other comparison algorithms. The cognitive diagnosis model based on the SC algorithm showed 4.01 SC, and the psychological status score was 3.97. The clustering performance of the model proposed in this study showed a favorable outcome. Moreover, the cognitive diagnosis model based on SC can meet the cognitive diagnosis needs of most students and help improve their cognitive ability. The enhanced cognitive diagnostic model combining SC and the GMM proposed has significant advantages in clustering performance and educational application effects, providing technical support for promoting students’ psychological growth.},
  archive      = {J_IJCIS},
  author       = {Chang, Xiao},
  doi          = {10.1007/s44196-025-00849-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Development of spectral clustering algorithm in cognitive diagnosis model: Approach for student’s psychological growth},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AQST-ClustNet: Hybrid aquila quantum sooty tern optimization for user profile clustering in social network. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00866-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User profile clustering is the process of grouping users on social media sites based on common characteristics identified in their profile data, such as demographics, interests, and interactions. Profile clustering allows users to engage in targeted marketing, skill-matching, and collaborative networking by grouping them based on similar attributes, interests, or professional criteria. However, one key drawback of user profile clustering is its sensitivity to noisy and missing data, high-dimensional feature spaces, poor semantic understanding, and complexity limitations. To overcome these issues, a novel Hybrid Aquila Quantum Sooty tern optimization for clustering (AQST-ClustNet) approach based on user profiles (UP) has been proposed in this paper. The user profile data is preprocessed using NLP techniques involving data stemming, handling of missing data or values, removal of stop words, and data extraction for eliminating inappropriate data. A Hybrid Aquila Quantum Sooty Tern Optimization (HAQSTO) algorithm is employed for clustering the user profile into healthcare professionals, marketing professionals, software developers, and educators. The efficiency of the developed method is assessed employing various metrics, including Calinski–Harabasz score (CHS), Silhouette score (SHS), and Davies–Bouldin score (DBS). The proposed model achieves less runtime of 45 s, whereas the existing techniques, such as MCEMS, DBSTexC, and TSMIUSC-Miner, achieve runtimes of 70 s, 79 s, and 60 s. Using the effective dual-stage feature extraction and clustering approach, the complexity of clustering and a high-dimensional feature space is effectively reduced.},
  archive      = {J_IJCIS},
  author       = {Babu, K. Dinesh and Sujihelen, L. and Singh, C. Senthil},
  doi          = {10.1007/s44196-025-00866-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {AQST-ClustNet: Hybrid aquila quantum sooty tern optimization for user profile clustering in social network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of an IoT-based MANET for healthcare monitoring system using data loss aware routing protocol. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00896-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare needs a major shift to more measurable and affordable solutions. The answer to these challenges is to focus on restructuring the healthcare system to prevent illness, not illness, and on disease prevention and early detection. The Internet of Things (IoT) communicates with mobile ad hoc networks (MANETs) in a smart environment, making it more attractive and cost-effective for consumers. Recently, MANET-IoT systems have been used in many areas of live applications. In addition, most routing protocols are for MANET, but they are not compatible with MANET-IoT. However, one of the key apparatuses of the MANET-IoT system is the loss of records due to unreliable routing. In this article, we suggest an optimal cluster founded data loss aware routing protocol, MANET-IoT, for healthcare monitoring systems (OCDL-HM). First, we introduce efficient cluster formation using the butterfly-induced sunflower optimization (BSFO) algorithm, which enhances the energy efficiency of routing. Then, the cluster head (CH) of every cluster is computed through a cuckoo search based deep probability neural network (CS-DPNN) with different design metrics. The CH node is acting as an intermediate node between cluster members and the next neighbouring CH node. After that, the next neighbouring CH node is selected by a hybrid recurrent dynamic neural network (RDNN), which provides data lossless routing between nodes. Finally, the simulation results of proposed and existing routing protocols analyzed with different simulation scenarios in terms of energy consumption, packet loss ratio, network lifetime, number of active nodes, packet delivery ratio, throughput, and latency.},
  archive      = {J_IJCIS},
  author       = {Balasubramanian, K. and Senthilkumar, S. and Kopperundevi, N. and Sivakumar, S.},
  doi          = {10.1007/s44196-025-00896-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Development of an IoT-based MANET for healthcare monitoring system using data loss aware routing protocol},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new multi-granular 2-tuple WASPAS-H approach for evaluating water resources for olive tree irrigation under group decision-making. <em>IJCIS</em>, <em>18</em>(1), 1-32. (<a href='https://doi.org/10.1007/s44196-025-00918-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiple criteria decision aid (MCDA) problems, evaluating and selecting alternatives across numerous criteria often presents a significant challenge. An excessive number of criteria can induce cognitive overload in decision-makers, diminishing the reliability and validity of the outcomes. To address this complexity, organizing criteria into a hierarchical tree structure has been recommended, allowing the main problem to be decomposed into more manageable sub-problems. However, existing ranking approaches rarely offer efficient tools to handle hierarchical criteria structures. This study proposes the development of hierarchical decision support systems aimed at improving the quality of decision-making. Specifically, an extension of the well-established weighted aggregated sum product assessment (WASPAS) method, termed 2-Tuple WASPAS-H, is introduced. This new method generates partial pre-orders at each sub-level and an overall pre-order at the highest level, enabling a step-by-step analysis and interpretation of the problem. A major strength of 2-Tuple WASPAS-H lies in its capacity to produce detailed rankings at each node of the hierarchy, while also accommodating imprecise and ambiguous evaluations through a symbolic treatment of multi-granular linguistic assessments provided by groups of experts. To evaluate the relative importance of criteria across different levels, the 2-Tuple BWM is utilized. The proposed approach is validated through a real case study: the evaluation of water resources for olive tree irrigation. The analysis employs a four-level hierarchy of criteria, incorporating environmental, productive, pomological, physico-chemical, social, technological, and financial dimensions. Ultimately, a sensitivity analysis is undertaken to evaluate the stability and performance of the developed methodology.},
  archive      = {J_IJCIS},
  author       = {Daoud-BenAmor, Wiem and Frikha, Hela Moalla},
  doi          = {10.1007/s44196-025-00918-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A new multi-granular 2-tuple WASPAS-H approach for evaluating water resources for olive tree irrigation under group decision-making},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid DL with battle royal optimisation algorithm for accurate tree counting using satellite images. <em>IJCIS</em>, <em>18</em>(1), 1-46. (<a href='https://doi.org/10.1007/s44196-025-00928-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree enumeration is a fundamental task in environmental monitoring, sustainable forestry management, and urban planning, yet manual methods remain prohibitively time-consuming and labor-intensive. This study presents an innovative approach, named BRO (Briefly Optimized Recognition with Deep Learning (DL) for accurate and efficient tree enumeration utilizing high-resolution satellite imagery and advanced machine learning techniques, specifically leveraging DL and transfer learning for robust tree detection and counting in complex environments. Experimental results demonstrate the significant effectiveness of the BRO approach compared to baseline methods, achieving a high accuracy of 97.8%. Furthermore, BRO shows substantial improvements in counting precision, resulting in a 5% reduction in Root Mean Squared Error (RMSE) and a 7% decrease in Mean Absolute Error (MAE) over existing techniques. Beyond performance metrics, execution time benchmarks highlight BRO’s computational efficiency, processing large datasets significantly faster than conventional optimization methods, which is crucial for large-scale applications. This research provides a robust and efficient system critical for various real-world applications, including large-scale deforestation monitoring, afforestation project planning and evaluation, and detailed urban forest inventories, thereby facilitating informed decision-making for environmental conservation and resource management.},
  archive      = {J_IJCIS},
  author       = {Bansal, Himanshu and Sinha, Anurag and Agarwal, Garvit and Mishra, Shantanu Kumar and Gupta, Shelly and Chaudhary, Parul and Ashokrao, Patil Rahul and Kushwaha, Ajay and Bagaria, Mukesh Kumar and Reza, Md.Sazid and Agrawal, Anupam and Bhad, Sandeep and Khalid, Saifullah and Lasisi, Ayodele and Aseere, Ali M.},
  doi          = {10.1007/s44196-025-00928-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-46},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A hybrid DL with battle royal optimisation algorithm for accurate tree counting using satellite images},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer-aided application in medicine and biomedicine. <em>IJCIS</em>, <em>18</em>(1), 1-39. (<a href='https://doi.org/10.1007/s44196-025-00936-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided applications in medicine and biomedicine drive the advancement of diagnostics, treatment, and research by leveraging data processing, analysis, and innovation. Computer technologies are used in medicine in imaging, diagnosis, storing and processing information, and staff management. The review will organize these progressions by significant features, which are medical imaging, diagnostic systems, data management, and their impact on daily clinical work. It explores diverse computer applications in medicine, including simulations, modeling, data visualization, and advanced data processing. Pattern classification techniques, decision support systems, and the integration of supercomputers—particularly in drug development—are discussed, alongside natural user interfaces and the application of Computer Methods and Programs in Biomedicine, with attention to human–computer interface integration. Modern computer-based imaging modalities like CT and MRI are also examined in detail, as are clinical applications of CAD for improving medical procedures. Lastly, the review projects future trends in cost-effective, high-quality telemedicine, remote consultations, integrated health records, computer-based learning, and disease management. Overall, this comprehensive discussion highlights the multifaceted impact of computing on the continued evolution of healthcare.},
  archive      = {J_IJCIS},
  author       = {Liao, Qi-Ming and Hussain, Wahab and Liao, Zhong-Xia and Hussain, Sarfraz and Jiang, Zhi-Liang and Zhu, Yong-Hao and Luo, Huang-Yin and Ji, Xin-Ying and Wen, Hong-Wei and Wu, Dong-Dong},
  doi          = {10.1007/s44196-025-00936-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-39},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Computer-aided application in medicine and biomedicine},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating handwritten answers using DeepSeek: A comparative analysis of deep learning-based assessment. <em>IJCIS</em>, <em>18</em>(1), 1-16. (<a href='https://doi.org/10.1007/s44196-025-00946-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence is revolutionizing the education sector by making learning more accessible, efficient, and customized. Recent advancements in artificial intelligence have sparked significant interest in automating the evaluation of handwritten answers. Traditional handwritten evaluation techniques are influenced by the evaluator's mental and physical state, environmental factors, human bias, emotional swings, and logistical challenges like storage and retrieval. Although sequence-to-sequence neural networks and other existing AI evaluation methods have demonstrated promise, they are constrained by their reliance on high-performance hardware, such as GPUs, lengthy training periods, and challenges in managing a variety of scenarios. The state-of-the-art technique known as Bidirectional Encoders Representation from Transformer (BERT) has overcome the drawbacks of previous NLP techniques like Bag of Words, TF-IDF, and Word2Vec. But BERT depends on surface-level keyword similarity, if the keywords are different then the accuracy is not perfect. This study presents a technique that combines optical character recognition (OCR) technology with DeepSeek-R1 1.5B model to create a robust, efficient, and accurate grading system. To overcome the above-mentioned challenges, we proposed an evaluation technique that uses the Google Cloud Vision API to extract and convert handwritten responses into machine-readable text, thereby providing a pre-processed input for further evaluation in this study. The main aim of this study is to develop a scalable, automated, and effective system for grading handwritten responses by combining DeepSeek for response evaluation with the Google Cloud Vision API for text extraction. To check the performance of the proposed DeepSeek evaluation method, we compare its results with cosine similarity metrics. After testing on multiple assignments, DeepSeek’s independent evaluation method gave the best results: lowest MAE—0.0580, lowest RMSE—0.147, and strongest correlation—0.895. The finding of this technique has shown that the proposed technique is reliable and accurate.},
  archive      = {J_IJCIS},
  author       = {Bansal, Sanskar and Gupta, Vinay and Gupta, Eshita and Garg, Peeyush},
  doi          = {10.1007/s44196-025-00946-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Evaluating handwritten answers using DeepSeek: A comparative analysis of deep learning-based assessment},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using cuckoo search algorithm to predict corporate financial risks and alleviate economic uncertainty. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00950-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting financial risk accurately is crucial for maintaining economic sustainability and investor trust in an era of increasing economic volatility and business instability. There are several problems with traditional financial risk assessment models, including their inflexibility when confronted with high-dimensional, non-linear data and their inability to dynamically adjust to changing financial situations. Advanced forecasting models must be combined with robust optimization methods to address these challenges effectively. Conventional methods, such as logistic regression, decision trees, and linear discriminant analysis, often struggle to accurately detect early financial risks because they are deterministic and unsuited for exploring global optimal solutions. Problems like this contribute to macroeconomic uncertainty by delaying the identification of potential defaults. This research presents a new hybrid framework, the Financial Risk Prediction Framework utilizing Cuckoo Search Optimization (FRPF-CSO), to enhance prediction accuracy and optimize feature selection in high-dimensional financial datasets. This framework combines a Backpropagation Neural Network (BPNN) with the Cuckoo Search Algorithm (CSA) to build an adaptive learning system. CSA is used to tune the network weights and feature parameters worldwide to improve the neural model’s convergence speed and predictive ability. The system’s dynamic learning capabilities identify patterns connected to company failures and financial decline. Various real-world corporate finance datasets have been used for experimental validation. The experimental results demonstrate that the proposed FRPF-CSO model achieves a high early warning lead time of 2.1 quarters, a prediction accuracy of 95.72%, a convergence rate of 9.87 s, a risk detection ratio of 97.63%, and a computational efficiency ratio of 98.2% compared to other existing methods.},
  archive      = {J_IJCIS},
  author       = {Cai, Muqiao},
  doi          = {10.1007/s44196-025-00950-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Using cuckoo search algorithm to predict corporate financial risks and alleviate economic uncertainty},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using convolutional neural networks for material surface quality inspection and classification. <em>IJCIS</em>, <em>18</em>(1), 1-24. (<a href='https://doi.org/10.1007/s44196-025-00951-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industries, undetected material surface defects lead to increased scrap rates and costly rework, primarily due to the limitations of manual inspection done in a slow and inconsistent process with poor small-defect identification. This lack of high-speed inspection solutions for multistage quality control creates critical gaps in production efficiency and product reliability. Hence, the research introduces a Hybridized Convolutional Neural Network for Surface Quality Control model that integrates U-Net with a ResNet34 backbone for precise defect localization and EfficientNet-B4 for defect classification, enhanced by stroboscopic illuminant preprocessing to optimize defect visibility. The research is validated on the Metal Surface Defect Dataset containing 147,824 high-resolution images capturing eight critical industrial defect types. The research results provide 98.2% classification accuracy, 96.5% defect localization precision, minimizes false alarms, and 98.2% recall for incoming material inspection, preventing defective inputs for industrial quality inspection. By integrating these innovations, the research helps manufacturers with a unified, scalable quality inspection platform that reduces human inspection workload by 12% while operating at production line speeds of 20.6 frames/sec and achieves 83.2 fps. The research model delivers a production-ready quality inspection system, which leads to maintaining a significant leap forward in automated surface quality assurance for Industry 4.0 applications.},
  archive      = {J_IJCIS},
  author       = {Ke, HanLin},
  doi          = {10.1007/s44196-025-00951-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Using convolutional neural networks for material surface quality inspection and classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based expression generation technology for virtual characters in film and television art. <em>IJCIS</em>, <em>18</em>(1), 1-28. (<a href='https://doi.org/10.1007/s44196-025-00952-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression generation played an essential role in virtual avatars, television, film art, and human–computer interaction. The existing synthesis model faces difficulties due to poor generalization, unnatural distortion, identity inconsistency, and weak lip synchronization with audio. In addition, conventional approaches fail to balance expression fidelity, real-time synthesis, and identity preservation. The research difficulties are addressed by introducing the Machine Learning-based Expression Generation Model to improve the expression generation accuracy. The ML-EGM model uses spatial and temporal features to generate expression by integrating generative adversarial networks, transformer encoding, and lip synchronization. A generator synthesizes expressive face outputs in the generative framework, while the discriminator is trained to distinguish actual and created expressions. This adversarial process teaches the generator to mimic actual facial emotions. During this process, transformer encoding is applied to manage the temporal consistency using the self-attention mechanism to minimize the error and improve the similarity rate. Further, the expressions are integrated with the speech synthesis process to enhance the overall frame expression generation efficiency. The system uses the MEAD dataset to evaluate the system efficiency, in which the model attains 99.1% expression generation accuracy, which is a 5.7% improvement compared to the MFA methods. The ML-EGM model proposal sets a new standard in achieving realism in expression generation, consistency of emotion realism, and speed of synthesis; its possible use cases include virtual aides, AI avatars, deepfake image detection, robotics systems that convey expression, and emotion-sensitive human–computer systems.},
  archive      = {J_IJCIS},
  author       = {Zhang, Yi and Qian, Junting},
  doi          = {10.1007/s44196-025-00952-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Machine learning-based expression generation technology for virtual characters in film and television art},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Objective and intelligent acquisition of two types of decision data based on online comment information and its application in emergency decision-making. <em>IJCIS</em>, <em>18</em>(1), 1-30. (<a href='https://doi.org/10.1007/s44196-025-00953-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the urgent, dynamic, and uncertain nature of emergencies, the rapid response to emergencies and their control is challenging, with the intelligent, real-time acquisition of decision data being a key issue. However, most previous studies relied on simulations, experts, assumptions, questionnaires, and borrowed literature, which are impractical. Therefore, this study developed two methods for the intelligent and objective acquisition of quantitative decision data based on machine learning and online review data and applied the methods to solve practical emergency problems. First, a reasonable decision index system was constructed using latent Dirichlet allocation thematic-cluster analysis of the focus word frequency from online public opinions. Python programming was then used to capture the big data of social media comments on emergencies in real time to obtain multisource comment data, which were preprocessed and visualized. Second, based on the optimized SnowNLP, emotional tendencies and statistical analyses were performed on the processed comment data, resulting in two quantitative decision matrices, which were represented as single-valued neutrosophic numbers (SVNNs) and probabilistic linguistic terms (PLTs). Next, the bidirectional projection method was extended to an environment of SVNNs and PLTs, and the alternatives were sorted and selected. Finally, a sudden natural disaster event was used as a numerical case to verify the proposed method. Through a comparative analysis, the experimental results show the practicability and feasibility of the proposed decision-making method, demonstrating the superiority and intelligence of our research. Our study can conduct real-time monitoring of emergencies and intelligently and objectively obtain quantified decision-making data, thereby providing auxiliary decision-making support for relevant emergency management departments, ensuring the positive development of public opinion, social stability, and the safety of people's lives and property.},
  archive      = {J_IJCIS},
  author       = {Tan, Ruipu and Yang, Lehua and Li, Jing},
  doi          = {10.1007/s44196-025-00953-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Objective and intelligent acquisition of two types of decision data based on online comment information and its application in emergency decision-making},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving diabetes and heart disease prediction via federated learning and WCO. <em>IJCIS</em>, <em>18</em>(1), 1-26. (<a href='https://doi.org/10.1007/s44196-025-00956-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes, afflicting 537 million worldwide, is a prevalent and lethal non-communicable ailment. Its onset, influenced by factors like obesity and family history, manifests symptoms such as frequent urination. Long-term complications encompass heart, kidney, and nerve ailments. Early prediction mitigates risks. All these encompassing strategies are designed to improve prediction precision and facilitate proactive diabetes control. This research employed SMOTE methods to tackle imbalanced classes, utilizing various classification algorithms such as Random Forest, XGBoost, Multilayer Perceptron, Gradient Boost, and AdaBoost. Following extensive training and evaluation, the AdaBoost classifier delivered superior outcomes, achieving a 94.02% accuracy rate, an F1 score of 93.32%, and an AUC of 0.95. In the healthcare industry, accurately forecasting diabetes mellitus is crucial; however, privacy laws hinder the transfer of medical information from the Internet of Medical Things (IoMT), causing delays in diagnosis. This study introduces the Federated Learning with Weighted Conglomeration Optimization (FLWCO) model as a solution to these challenges. In Centralized Learning, AdaBoost with WCO achieves an accuracy of 95.32% when tested on a Kaggle dataset consisting of 96,146 instances. During the second stage, FLWCO achieves a superior 97.27% accuracy rate compared to other federated learning techniques. The method not only guarantees privacy conformity but also decreases communication expenses. FLWCO demonstrates superiority over existing federated learning algorithms in real-world heart illness prediction. Furthermore, the proposed model can be employed to estimate the likelihood of heart disease in individuals with diabetes. This highlights the potential of federated learning, especially FLWCO, in leveraging distributed data while preserving privacy, facilitating accurate diabetes mellitus diagnosis, and addressing challenges in sharing medical information securely and efficiently.},
  archive      = {J_IJCIS},
  author       = {Dash, Sachikanta and Padhy, Sasmita and Suman, Preetam and Mal, Sandip and Malviya, Lokesh and Suman, Amrit and Kishore, Jaydeep},
  doi          = {10.1007/s44196-025-00956-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Privacy-preserving diabetes and heart disease prediction via federated learning and WCO},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating label encoding and preprocessing techniques for breast cancer prediction using machine learning algorithms. <em>IJCIS</em>, <em>18</em>(1), 1-35. (<a href='https://doi.org/10.1007/s44196-025-00957-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast Cancer (BC) is a major health concern in the world, accounting for a disproportionately high number of new cases each year among females. Its frequency as a health issue has increased significantly in recent years. Early detection of BC is the most straightforward method of coping with the diagnosis. In 2020, approximately 2.3 million women worldwide were diagnosed with breast cancer, resulting in around 685,000 deaths. Therefore, early detection of BC is crucial for effective treatment and improved survival rates. The results and assessments of many Machines Learning (ML) models for detecting BC survivability are presented in this manuscript using the BC dataset. Although the dataset is relatively small, it provides valuable insights. The data was evaluated with many ML techniques and different predictive models were built with the valuable results. Several ML techniques were applied to build predictive models, including Gaussian Naïve Bayes (GNB), k-Nearest Neighbors (k-NN), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), and Support Vector Machine (SVM). Data scaling and encoding techniques, including StandardScaler and MinMaxScaler, are employed to enhance the accuracy of these machine learning models. Additionally, preprocessing steps, such as Numerical Variable Correlation, Categorical Variables Analysis, Continuous Variables Analysis, Bivariate Analysis, Balancing Classes (oversampling function) are applied to enhance the model’s performance. The results show that out of all the ML techniques tested, the k-NN method gives the most accurate predictions, which is close to 94.00%.},
  archive      = {J_IJCIS},
  author       = {Kumar, Mukesh and Bhardwaj, Vivek},
  doi          = {10.1007/s44196-025-00957-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Evaluating label encoding and preprocessing techniques for breast cancer prediction using machine learning algorithms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of combat resource allocation based on restricted tournament selection social genetic algorithm. <em>IJCIS</em>, <em>18</em>(1), 1-18. (<a href='https://doi.org/10.1007/s44196-025-00958-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To tackle the challenge of combat resource allocation problem (CRAP), especially under resource constraints, the dilemma between the efficiency of combat resource utilization and the efficiency of problem-solving. We propose a novel genetic algorithm that integrates a restricted tournament selection strategy with sociological principles, named the restricted tournament selection social genetic algorithm (RTS2GA). Initially, we develop a comprehensive model for the allocation of combat resources that takes into account multiple constraints. Then, building upon the traditional Genetic Algorithm, we introduce a novel selection strategy known as restricted tournament selection to enhance the diversity of the algorithm’s population. In addition, we innovatively incorporate the concept of ‘group effect’ from sociology, adding a socialization operator to the algorithm to accelerate convergence and improve the quality of optimal solutions. Comprehensive evaluation confirms RTS2GA’s trade-off profile: though incurring added computational costs, it achieves competitive convergence speed (marginally behind PSO/MPSO; comparable to GA/DE/GA-APSO) while establishing definitive superiority in global optimization across all five benchmarks.},
  archive      = {J_IJCIS},
  author       = {Yuan, Shandong and Ren, Yun and Zhou, Han and Cheng, Yongjing and Yan, Kai},
  doi          = {10.1007/s44196-025-00958-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimization of combat resource allocation based on restricted tournament selection social genetic algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ontology-based approach for heart disease prediction leveraging decision trees in healthcare. <em>IJCIS</em>, <em>18</em>(1), 1-27. (<a href='https://doi.org/10.1007/s44196-025-00960-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ontology-driven system for the diagnosis of heart disease combines data-driven methods with semantic reasoning. The dataset consists of major healthcare attributes like patient information, clinical findings, and diagnostic indicators. This information is processed in a methodical way into a clinical ontology, which facilitates structured, semantically enhanced representation of medical entities and their relationships. The system uses a hybrid strategy based on decision trees and Graph Neural Networks (GNNs) to enhance predictive performance. Decision trees offer interpretability through the identification of key decision paths, whereas GNNs utilize connected clinical ontology for richer relational analysis. Semantic Web Rule Language (SWRL) rules also enrich reasoning by deducing new knowledge from implicit relationships and patterns. The rules combine risk factors such as hypertension, cholesterol, and lifestyle to predict heart disease with high accuracy. The integration of rule-based reasoning and machine learning provides a complete and context-sensitive system. Performance is measured using accuracy, precision, recall, F1-score, and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) to provide solid diagnostic evaluation. The hybrid model of decision trees, GNNs, and ontology-based reasoning with boosting via Semantic Web Rule Language (SWRL) rules shows great predictive accuracy and decision support improvements. The clinical ontology provides interpretability, flexibility, and scalability for future data sets and changing medical knowledge. This work emphasizes the opportunity of combining ontology engineering, machine learning, and semantic reasoning in enhancing heart disease prediction and diagnosis. The system assists healthcare workers in making well-informed decisions, improving diagnosis accuracy, and maximizing patient management.},
  archive      = {J_IJCIS},
  author       = {Priyadharshini, U. and Vijayan, R.},
  doi          = {10.1007/s44196-025-00960-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An ontology-based approach for heart disease prediction leveraging decision trees in healthcare},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fractal fuzzy-based multi-criteria assessment of sustainability in rare earth use for hydrogen storage. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00962-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of rare earth elements in hydrogen storage processes offers significant advantages in terms of increasing technological efficiency and ensuring system security. However, this process also creates some serious problems in terms of environmental and economic sustainability. It is necessary to determine the most critical indicators affecting the sustainable use of these elements. Studies on this subject in the literature are quite limited, and this may lead to wrong investment decisions. The main purpose of this study is to determine the most important indicators to increase the sustainable use of rare earth elements in hydrogen storage processes. An original decision-making model in which Siamese network, logarithmic percentage-change driven objective weighting (LOPCOW), fractal fuzzy numbers, and weighted influence super matrix with precedence (WISP) approaches are integrated in the study. This study provides an original contribution to the literature by identifying the most critical indicators affecting the sustainable use of rare earths in hydrogen storage processes by presenting an innovative model. Fractal structures such as Koch Snowflake, Cantor Dust, and Sierpinski Triangle can model complex uncertainties more successfully. Fractal structures are particularly effective in modeling linguistic fuzziness because their recursive nature closely mirrors the layered and imprecise way humans often express subjective judgments. Unlike linear fuzzy sets, fractals can capture the patterns of ambiguity found in expert evaluations. Hydrogen storage capacity and government supports are determined as the most vital criteria affecting sustainability in rare earth use.},
  archive      = {J_IJCIS},
  author       = {Kou, Gang and Yüksel, Serhat and Eti, Serkan and Dinçer, Hasan and Acar, Merve},
  doi          = {10.1007/s44196-025-00962-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Fractal fuzzy-based multi-criteria assessment of sustainability in rare earth use for hydrogen storage},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DenseNet model with attention mechanisms for robust date fruit image classification. <em>IJCIS</em>, <em>18</em>(1), 1-33. (<a href='https://doi.org/10.1007/s44196-025-00809-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dates in Saudi Arabia hold immense cultural, economic, and nutritional importance, being a staple food and a symbol of heritage. Saudi Arabia produces approximately 1.5 million metric tons of dates annually, accounting for nearly 17% of global date production, underscoring its pivotal role in the global market. Precise and automated classification of fruits is a crucial aspect of modern agriculture, yet it remains a challenging endeavor due to the diverse appearances of fruits. The classification of date fruit varieties presents additional complexities, given variations in size, shape, and texture, making it a critical focus for technological advancements. Dates are highly nutritious, providing approximately 277 cal per 100 g and serving as an excellent source of dietary fiber, natural sugars, and energy. Their substantial nutritional value makes them indispensable in addressing food security challenges and promoting global health benefits. In this paper, we introduce a novel DenseNet-based model augmented with attention mechanisms and optimized using the Nadam algorithm. Unlike traditional DenseNet variants, the model integrates attention mechanisms to enhance focus on pertinent image features, thereby improving classification accuracy under challenging conditions. To evaluate its efficacy, the model was benchmarked against several state-of-the-art deep learning architectures, including DenseNet with Adam optimization, EfficientNet, GoogleNet, HRNet, MobileNet, and VGG, optimized with both Adam and Nadam algorithms. The proposed model achieved outstanding performance metrics, including 98.05% accuracy, 98.00% precision, 97.04% recall, and a 98.32% F1-score.},
  archive      = {J_IJCIS},
  author       = {Hassan, Esraa and Ghazalah, Sarah Abu and El-Rashidy, Nora and El-Hafeez, Tarek Abd and Shams, Mahmoud Y.},
  doi          = {10.1007/s44196-025-00809-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {DenseNet model with attention mechanisms for robust date fruit image classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement-learning-based V2G scheduling: Peak load mitigation and financial benefits. <em>IJCIS</em>, <em>18</em>(1), 1-23. (<a href='https://doi.org/10.1007/s44196-025-00959-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops a centralized reinforcement learning framework, employing Q-learning, for efficient vehicle-to-grid (V2G) scheduling of large electric vehicle (EV) fleets. To address scalability challenges and inherent uncertainties in user behavior, the framework utilizes an aggregated state representation. This state captures the time-of-day, the distribution of the fleet’s state of charge (SOC) across discrete bins, and an estimated user adherence factor. The central agent learns a control policy based on this aggregated state to dynamically issue charging, discharging, or idle commands to EVs grouped within specific SOC bins. The primary objectives are to enhance power grid stability by minimizing the peak-to-average ratio (PAR) and to improve the economic viability of V2G participation for EV owners. Simulations conducted under a 40% EV penetration level (relative to a 300,000 vehicle base fleet) demonstrate the proposed method’s effectiveness: it reduced the grid PAR to 1.0683, compared to 1.0729 for a baseline uncontrolled charging scenario. Critically, the proposed method transformed the economic outcome, achieving positive average daily earnings of $1.17 per EV, in stark contrast to an average daily loss of $6.75 per EV under the baseline. These results validate the potential of the proposed intelligent, centralized control strategy using aggregated information to effectively manage large-scale V2G systems, enhance grid stability, and provide economic benefits under practical behavioral assumptions.},
  archive      = {J_IJCIS},
  author       = {Xiao, Yong and Tang, Jianlin and Lin, Xiaoming and Feng, Xiangyong and Qian, Bin and Zhang, Fan},
  doi          = {10.1007/s44196-025-00959-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Reinforcement-learning-based V2G scheduling: Peak load mitigation and financial benefits},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biogeography-based optimization of machine learning models for accurate penetration rate prediction using rock texture coefficient. <em>IJCIS</em>, <em>18</em>(1), 1-27. (<a href='https://doi.org/10.1007/s44196-025-00973-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting drill penetration rate (PR) in rock environments remains a significant challenge due to the complex interplay between rock texture, drilling fluid properties, and operational parameters. Traditional empirical models often lack generalizability and are based on inconsistent datasets, limiting their reliability. To address these limitations, this study develops a comprehensive experimental dataset using rock samples collected from various mines in Iran, tested under controlled laboratory conditions with different drilling fluids, bit loads, and rotational speeds. Texture coefficient (TC), electrical conductivity (EC), load on bit (LOB), and bit rotational velocity (BRV) were selected as input features. Four machine learning models—support vector regression (SVR), stochastic gradient descent (SGD), K-nearest neighbors (KNN), and decision tree (DT)—were trained to predict PR. A biogeography-based optimization (BBO) algorithm was employed to fine-tune hyperparameters and enhance model accuracy. Additionally, a novel hybrid error index (HEI) was introduced to comprehensively evaluate model performance. Among all models, the DT achieved the best accuracy with an HEI of 0.3753, followed by KNN, SVR, and SGD. These findings demonstrate the potential of the DT model, combined with optimized learning and a robust dataset, to reliably predict penetration rate in rock-based engineering projects.},
  archive      = {J_IJCIS},
  author       = {Esmaeilzadeh, Akbar and Mikaeil, Reza and Khosravimanesh, Shahrokh and Shaffiee Haghshenas, Sina and Simic, Vladimir and Pamucar, Dragan},
  doi          = {10.1007/s44196-025-00973-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Biogeography-based optimization of machine learning models for accurate penetration rate prediction using rock texture coefficient},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-learning enabled score-based generative adversarial networks (GANs). <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00933-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capacity of Generative Adversarial Networks (GANs) to provide high-quality data has led to their significant attention. On the other hand, hyperparameter adjustment is a common part of GAN training, which may increase computing costs and result in less-than-ideal performance on certain tasks. Meta-Learning Enabled Score-Based GANs (MLS-GAN) is a new framework we provide in this study that combines meta-learning with score-based generative models. Our method employs meta-learning to enhance the training of score-based models via the collection of priors customized to individual tasks and tactics for dynamic adaptability. This improves the generative process’s generalizability and robustness, and it also allows for more efficient learning with fewer data and hyperparameter modifications. By conducting comprehensive tests on image synthesis and data creation tasks, we demonstrate that our Meta-SB-GANs are successful. The results reveal higher-quality samples, quicker convergence, and better transferability to other domains. Integrating meta-learning with generative models can achieve state-of-the-art performance with decreased computing resources, as shown by our findings.},
  archive      = {J_IJCIS},
  author       = {Navaneethakrishnan, P. and Peter, Smitha Elsa and Simon, Sishaj P. and Ahamed, M. Irshad},
  doi          = {10.1007/s44196-025-00933-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Meta-learning enabled score-based generative adversarial networks (GANs)},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart diagnosis of cholangiocarcinoma from microscopic images using a modified visual geometry group network with adaptive augmentation. <em>IJCIS</em>, <em>18</em>(1), 1-19. (<a href='https://doi.org/10.1007/s44196-025-00965-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral Microscopic Images (HSMI) offer a more comprehensive spectral range, making them particularly useful in medical diagnostics, including early detection of cancerous tissues. Cholangiocarcinoma, a highly lethal bile duct cancer, traditionally requires a histopathological analysis of tissue samples under a microscope. However, this method is prone to subjectivity and error, often leading to delays in diagnosis, contributing to patient mortality. This study focuses on the automated diagnosis of cholangiocarcinoma using deep learning techniques on microscopic hyperspectral data. Leveraging the spectral richness of HSMI, we developed a framework utilizing a modified Visual Geometry Group (VGG) architecture to process this hyperspectral data, aiming to detect cholangiocarcinoma at its early stages. With the growing role of artificial intelligence in pathology, this approach minimizes human error and enhances diagnostic accuracy. The dataset used in this study includes 880 cholangiocarcinoma tissue samples from 174 individuals, comprising 689 partial cancer regions, 49 full cancer regions, and 142 healthy scenes, all meticulously labeled by expert pathologists. Our ensemble learning technique integrates image preprocessing, spectral feature extraction, and classification, significantly improving diagnostic accuracy. The proposed system demonstrates a substantial improvement in the early detection of cholangiocarcinoma and offers a valuable tool for smart microscopy-based diagnosis, potentially facilitating the diagnostic burden in clinical settings.},
  archive      = {J_IJCIS},
  author       = {Mujahid, Muhammad and Kanwal, Khadija and Abubakar, Muhammad and Al-Otaibi, Shaha and Elyassih, Alex and Rehman, Amjad},
  doi          = {10.1007/s44196-025-00965-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Smart diagnosis of cholangiocarcinoma from microscopic images using a modified visual geometry group network with adaptive augmentation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced facial emotion recognition and age estimation using modified residual network and support vector machine. <em>IJCIS</em>, <em>18</em>(1), 1-32. (<a href='https://doi.org/10.1007/s44196-025-00966-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial emotion recognition and age estimation are critical for human–computer interaction, yet existing methods struggle with variable image quality, diverse expressions, and aging patterns. To address these challenges, this study proposes a novel hybrid model combining optimized image enhancement with deep feature extraction and machine learning classification. The methodology integrates three key phases: (1) image enhancement using white balancing and adaptive gamma correction to improve edge intensity (82.235 vs. 62.89 in original images) and texture clarity (gradient: 8.248 vs. 6.183); (2) feature extraction via a modified ResNet-18 trained from scratch with monochromatic inputs; and (3) SVM classification of concatenated features from original and enhanced images. Evaluated on the UTKFace dataset (1,000 images) and savory/unsavory expression datasets (600 images), the model achieves 98.75–100% accuracy, outperforming AlexNet (94.17%) and ResNet-KNN alternatives. Key findings demonstrate: (a) 5.4% higher age estimation accuracy (96.41% vs. 87.41%) with enhanced gradient metrics, (b) 3.75% improvement in emotion recognition through edge preservation, and (c) interpretable Grad-CAM visualizations validating feature relevance. The proposed system reduces misclassification errors by 60% for subtle expressions (e.g., fear vs. surprise) compared to state-of-the-art models. Practical applications in healthcare show 40% faster patient check-in processing and robust performance under low-light conditions. This work establishes that integrating physics-based enhancement with deep residual networks significantly improves reliability for real-world deployment. Future research will optimize computational efficiency for edge devices and expand multi-modal biometric integration.},
  archive      = {J_IJCIS},
  author       = {El-Hag, Noha A. and El-Shafai, Walid and El-Samie, Fathi E. Abd and Soliman, Naglaa F.},
  doi          = {10.1007/s44196-025-00966-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhanced facial emotion recognition and age estimation using modified residual network and support vector machine},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SqueezeNet-based deep learning framework for accurate tomato (Solanum lycopersicum) leaf disease diagnosis and classification. <em>IJCIS</em>, <em>18</em>(1), 1-22. (<a href='https://doi.org/10.1007/s44196-025-00978-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture is crucial for food security but is severely threatened by crop disease and climate variability and causes severe yield loss. As the population grows worldwide, quick and accurate disease detection is critical. Deep learning, in particular through transfer learning, offers promising solutions, but most are computationally costly and unsuitable for real-time use in low-resource settings. There is limited research on lightweight models like SqueezeNet with optimized training parameters. This suggests the need for an efficient, high-accuracy, and deployable model to facilitate timely detection of tomato leaf diseases under real-world agricultural settings. This study presents a deep learning model based on the SqueezeNet framework for the detection and classification of tomato leaf diseases. Various combinations of optimizers (SGDM, ADAM, RMSProp) and learning rates (0.0004, 0.004) were employed during both training and testing phases, resulting in six configurations per case. The SqueezeNet model achieved 99.91% and 99.86% accuracy for TMC class classification during testing and training, with ADAM learning at 0.0004. ADAM at 0.0004 had ideal recall (100%) for the TH class during testing, and SGDM had 99.65% recall for the TYLCV class at the same learning rate, proving the model’s usefulness. The proposed framework is robust, with F1-Scores of 99.42% in ADAM testing at 0.0004 and 99.38% in SGDM training at 0.0004 for the TYLCV class. The model’s low misclassification rate (0–0.21%) boosts confidence. The ability to demonstrate classification performance and the minimal computational requirements of the proposed SqueezeNet-based system enhance the latter’s feasibility for use in real-time agricultural environments that are resource-constrained. Its scalability and resilience make it an excellent choice for utilization in advanced disease monitoring systems for tomato leaf diseases, facilitating quick, accurate diagnosis at the field level to facilitate enhanced precision agriculture practices.},
  archive      = {J_IJCIS},
  author       = {Jagdev, Siddhant and Sundararaman, Bharathwaaj and Khatri, Narendra and Gaur, Pramod and Mewada, Hiren},
  doi          = {10.1007/s44196-025-00978-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {SqueezeNet-based deep learning framework for accurate tomato (Solanum lycopersicum) leaf disease diagnosis and classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A decision-making method for lung diseases recognition using generalized complex fermatean fuzzy distance and entropy measures. <em>IJCIS</em>, <em>18</em>(1), 1-20. (<a href='https://doi.org/10.1007/s44196-025-00926-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex Fermatean fuzzy sets (CFFSs) represent an advanced development of fuzzy sets by integrating aspects of complex fuzzy sets and Fermatean fuzzy sets. Such integration facilitates enhanced modeling and depiction of uncertain information within complex environments, surpassing the capabilities of complex intuitionistic and Pythagorean fuzzy sets. Despite these advantages, an urgent challenge is how to accurately evaluate the dissimilarity between CFFSs. Therefore, in this paper, we propose two generalized complex Fermatean fuzzy distance measures for CFFSs as well as their weighted counterparts. Furthermore, we demonstrate some key properties of the proposed distance measures, which illustrate their feasibility and effectiveness. Under certain circumstances, the proposed distance measures can be transformed into a hybrid complex Fermatean Hamming–Hausdorff and Euclidean–Hausdorff distance measures. Building on the proposed distance measures, we present a new entropy measure for CFFS. Finally, we propose a new decision-making method based on the proposed distance and entropy measures and apply the proposed method to lung disease recognition. The experiments demonstrate that the proposed distance measures effectively capture the dissimilarities between diseases represented as CFFSs, with the method achieving accurate classification results.},
  archive      = {J_IJCIS},
  author       = {Liu, Zhe and Senapati, Tapan and Santina, Dania and Jamil, Muhammad Kamran and Mlaiki, Nabil},
  doi          = {10.1007/s44196-025-00926-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A decision-making method for lung diseases recognition using generalized complex fermatean fuzzy distance and entropy measures},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijcv">IJCV - 40</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Investigating self-supervised methods for label-efficient learning. <em>IJCV</em>, <em>133</em>(9), 6638. (<a href='https://doi.org/10.1007/s11263-025-02455-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Nandam, Srinivasa Rao and Atito, Sara and Feng, Zhenhua and Kittler, Josef and Awais, Muhammad},
  doi          = {10.1007/s11263-025-02455-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6638},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Investigating self-supervised methods for label-efficient learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Parameter efficient fine-tuning for multi-modal generative vision models with möbius-inspired transformation. <em>IJCV</em>, <em>133</em>(9), 6637. (<a href='https://doi.org/10.1007/s11263-025-02456-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Duan, Haoran and Shao, Shuai and Zhai, Bing and Shah, Tejal and Han, Jungong and Ranjan, Rajiv},
  doi          = {10.1007/s11263-025-02456-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6637},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Parameter efficient fine-tuning for multi-modal generative vision models with möbius-inspired transformation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on DAGM GCPR 2023. <em>IJCV</em>, <em>133</em>(9), 6636. (<a href='https://doi.org/10.1007/s11263-025-02490-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-025-02490-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6636},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on DAGM GCPR 2023},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OCCO: LVM-guided infrared and visible image fusion framework based on object-aware and contextual contrastive learning. <em>IJCV</em>, <em>133</em>(9), 6611-6635. (<a href='https://doi.org/10.1007/s11263-025-02507-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion is a crucial technique in the field of computer vision, and its goal is to generate high-quality fused images and improve the performance of downstream tasks. However, existing fusion methods struggle to balance these two factors. Achieving high quality in fused images may result in lower performance in downstream visual tasks, and vice versa. To address this drawback, a novel LVM (large vision model)-guided fusion framework with Object-aware and Contextual COntrastive learning is proposed, termed as OCCO. The pre-trained LVM is utilized to provide semantic guidance, allowing the network to focus solely on fusion tasks while emphasizing learning salient semantic features in form of contrastive learning. Additionally, a novel feature interaction fusion network is also designed to resolve information conflicts in fusion images caused by modality differences. By learning the distinction between positive samples and negative samples in the latent feature space (contextual space), the integrity of target information in fused image is improved, thereby benefiting downstream performance. Finally, compared with eight state-of-the-art methods on four datasets, the effectiveness of the proposed method is validated, and exceptional performance is also demonstrated on downstream visual task.},
  archive      = {J_IJCV},
  author       = {Li, Hui and Bian, Congcong and Zhang, Zeyang and Song, Xiaoning and Li, Xi and Wu, Xiao-Jun},
  doi          = {10.1007/s11263-025-02507-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6611-6635},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {OCCO: LVM-guided infrared and visible image fusion framework based on object-aware and contextual contrastive learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unaligned RGB guided hyperspectral image super-resolution with spatial-spectral concordance. <em>IJCV</em>, <em>133</em>(9), 6590-6610. (<a href='https://doi.org/10.1007/s11263-025-02466-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) super-resolution (SR) aims to improve the spatial resolution, yet its performance is often limited at high-resolution ratios. The recent adoption of high-resolution reference images for super-resolution is driven by the poor spatial detail found in low-resolution HSIs, presenting it as a favorable method. However, these approaches cannot effectively utilize information from the reference image, due to the inaccuracy of alignment and its inadequate interaction between alignment and fusion modules. In this paper, we introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution (SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the issues of inaccurate alignment and poor interactivity of the previous approaches. Specifically, to ensure spatial concordance, i.e., align images more accurately across resolutions and refine textures, we construct a Two-Stage Image Alignment (TSIA) with a synthetic generation pipeline in the image alignment module, where the fine-tuned optical flow model can produce a more accurate optical flow in the first stage and warp model can refine damaged textures in the second stage. To enhance the interaction between alignment and fusion modules and ensure spectral concordance during reconstruction, we propose a Feature Aggregation (FA) module and an Attention Fusion (AF) module. In the feature aggregation module, we introduce an Iterative Deformable Feature Aggregation (IDFA) block to achieve significant feature matching and texture aggregation with the fusion multi-scale results guidance, iteratively generating learnable offset. Besides, we introduce two basic spectral-wise attention blocks in the attention fusion module to model the inter-spectra interactions. Extensive experiments on three natural or remote-sensing datasets show that our method outperforms state-of-the-art approaches on both quantitative and qualitative evaluations. Our code is publicly available to the community ( https://github.com/BITYKZhang/SSC-HSR ).},
  archive      = {J_IJCV},
  author       = {Zhang, Yingkai and Lai, Zeqiang and Zhang, Tao and Fu, Ying and Zhou, Chenghu},
  doi          = {10.1007/s11263-025-02466-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6590-6610},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unaligned RGB guided hyperspectral image super-resolution with spatial-spectral concordance},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos. <em>IJCV</em>, <em>133</em>(9), 6578-6589. (<a href='https://doi.org/10.1007/s11263-025-02493-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using unmanned aerial vehicles (UAVs) to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding the collective behavior of primates. Previous studies have demonstrated the feasibility of automating primate behavior classification from video data, but these studies have been carried out in captivity or from ground-based cameras. However, to understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can be seen in relation to the natural environment in which ecological decisions are made. To tackle this challenge, this study presents a novel dataset for baboon detection, tracking, and behavior recognition from drone videos where troops are observed on-the-move in their natural environment as they move to and from their sleeping sites. Videos were captured from drones at Mpala Research Centre, a research station located in Laikipia County, in central Kenya. The baboon detection dataset was created by manually annotating all baboons in drone videos with bounding boxes. A tiling method was subsequently applied to create a pyramid of images at various scales from the original 5.3K resolution images, resulting in approximately 30K images used for baboon detection. The baboon tracking dataset is derived from the baboon detection dataset, where bounding boxes are consistently assigned the same ID throughout the video. This process resulted in half an hour of dense tracking data. The baboon behavior recognition dataset was generated by converting tracks into mini-scenes, a video subregion centered on each animal. These mini-scenes were annotated with 12 distinct behavior types and one additional category for occlusion, resulting in over 20 hours of data. Benchmark results show mean average precision (mAP) of 92.62% for the YOLOv8-X detection model, multiple object tracking precision (MOTP) of 87.22% for the DeepSORT tracking algorithm, and micro top-1 accuracy of 64.89% for the X3D behavior recognition model. Using deep learning to rapidly and accurately classify wildlife behavior from drone footage facilitates non-invasive data collection on behavior enabling the behavior of a whole group to be systematically and accurately recorded. The dataset can be accessed at https://baboonland.xyz .},
  archive      = {J_IJCV},
  author       = {Duporge, Isla and Kholiavchenko, Maksim and Harel, Roi and Wolf, Scott and Rubenstein, Daniel I and Crofoot, Margaret C and Berger-Wolf, Tanya and Lee, Stephen J and Barreau, Julie and Kline, Jenna and Ramirez, Michelle and Stewart, Charles V},
  doi          = {10.1007/s11263-025-02493-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6578-6589},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised salient object detection with oversize bounding box. <em>IJCV</em>, <em>133</em>(9), 6558-6577. (<a href='https://doi.org/10.1007/s11263-025-02482-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to laborious pixel-level annotations, scribbles, bounding boxes, and points are much more efficient in salient object detection (SOD). However, the annotation cost of these forms linearly increases with the number of salient objects in the image, which is not ideal for real-world scenarios. To address this issue, we propose a novel annotation form called oversize bounding box (OBB), i.e., a box that encompasses all salient objects without the need for tight enclosure. It has two characteristics: (1) All pixels outside the box are from the background. (2) A subset of the pixels inside the box belongs to salient objects. Therefore, the core issue is how to highlight integral and accurate object regions as pseudo-labels. Inspired by the powerful visual understanding and vision-language correlations demonstrated by large multimodal models, we devise the first multimodal model-based pseudo-label generation method for SOD. It utilizes MiniGPT-4 to generate descriptions of salient objects as text prompts for CLIP, combined with the CAM-based technique and proposed refinement algorithm based on the multi-head self-attention and superpixel to activate the object regions. Then we use OBB to further correct the activation regions based on its two characteristics. Given the potential errors of pseudo-labels within the box, we propose a center-pixel-based cross entropy loss, as activated pixels closer to the center are generally more reliable. Moreover, we establish an OBB-supervised training dataset by relabeling the DUTS dataset. Extensive experiments on six benchmarks demonstrate that our method achieves the state-of-the-art performance.},
  archive      = {J_IJCV},
  author       = {Wu, Zhihao and Xu, Yong and Yang, Jian and Zhang, David},
  doi          = {10.1007/s11263-025-02482-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6558-6577},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Weakly supervised salient object detection with oversize bounding box},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit diffusion models for continuous super-resolution. <em>IJCV</em>, <em>133</em>(9), 6535-6557. (<a href='https://doi.org/10.1007/s11263-025-02462-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) has attracted increasing attention due to its widespread applications. However, current SR methods generally suffer from over-smoothing and artifacts, and most of them work only with fixed magnifications. To address these problems, this paper introduces an Implicit Diffusion Model (IDM) for high-fidelity continuous image super-resolution. IDM integrates an implicit neural representation and a denoising diffusion model in a unified end-to-end framework, where the implicit neural representation is adopted in the decoding process to learn continuous-resolution representation. Moreover, we design a scale-adaptive conditioning mechanism that consists of a low-resolution (LR) conditioning network and a scaling factor. The LR conditioning network adopts a parallel architecture to provide multi-resolution LR conditions for the denoising model. The scaling factor further regulates the resolution and accordingly modulates the proportion of the LR information and generated features in the final output, which enables the model to accommodate the continuous-resolution requirement. Furthermore, we accelerate the inference process by adjusting the denoising equation and employing post-training quantization to compress the learned denoising network in a training-free manner. Extensive experiments on six benchmark datasets validate the effectiveness of our IDM and demonstrate its superior performance over prior arts. The source code is available at https://github.com/Ree1s/IDM .},
  archive      = {J_IJCV},
  author       = {Liu, Xuhui and Gao, Sicheng and Zeng, Bohan and Zhang, Luping and Wang, Tian and Liu, Jianzhuang and Zhang, Baochang},
  doi          = {10.1007/s11263-025-02462-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6535-6557},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Implicit diffusion models for continuous super-resolution},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal prompt alignment with fine-grained LLM knowledge for unsupervised domain adaptation. <em>IJCV</em>, <em>133</em>(9), 6513-6534. (<a href='https://doi.org/10.1007/s11263-025-02497-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge from source domain to target domain, which always struggles with severe domain shift between the data. The recent progress on visual-language models (VLMs) have provided a promising way to address UDA, leveraging the knowledge from text for more guided adaptation. However, directly deploying such models on downstream UDA tasks with conventional prompt learning can be challenging, which neglects the diversity of visual samples and can cause mis-alignment between modalities, thus lacking flexibility to adapt both modalities dynamically and limiting the cross-domain knowledge transfer. In this paper, we propose an innovative domain-invariant prompt learning method to align the prompts from different modalities. Specifically, we first introduce a hybrid-modality guided prompting module that leverages the prompted multi-modal representation to synergistically help uni-modal learning, thus mutually aligning visual and textual embeddings. We also take advantage of the the category-wise attributes derived from Large Language Model (LLM) to incorporate fine-grained semantic knowledge into prompt learning, ensuring better discrimination among different classes. Besides, to further minimize domain discrepancy, we propose to fuse the textual prototypes with the visual prototypes from each domain, thus to make the input attend to overall domain distribution, which effectively integrates self-enhanced and cross-domain features into the model prediction. With our framework, the two modalities can be mutually promoted to better enhance the adaptation of VLMs for UDA. Experiments on several different benchmarks demonstrate the superiority of our method over previous approaches.},
  archive      = {J_IJCV},
  author       = {Xing, Bowei and Ying, Xianghua and Wang, Ruibin and Guo, Ruohao},
  doi          = {10.1007/s11263-025-02497-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6513-6534},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-modal prompt alignment with fine-grained LLM knowledge for unsupervised domain adaptation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ricci curvature tensor-based volumetric segmentation. <em>IJCV</em>, <em>133</em>(9), 6491-6512. (<a href='https://doi.org/10.1007/s11263-025-02492-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing level set models employ regularization based only on gradient information, 1D curvature or 2D curvature. For 3D image segmentation, however, an appropriate curvature-based regularization should involve a well-defined 3D curvature energy. This is the first paper to introduce a regularization energy that incorporates 3D scalar curvature for 3D image segmentation, inspired by the Einstein-Hilbert functional. To derive its Euler-Lagrange equation, we employ a two-step gradient descent strategy, alternately updating the level set function and its gradient. The paper also establishes the existence and uniqueness of the viscosity solution for the proposed model. Experimental results demonstrate that our proposed model outperforms other state-of-the-art models in 3D image segmentation.},
  archive      = {J_IJCV},
  author       = {Huang, Jisui and Chen, Ke and Alpers, Andreas and Lei, Na},
  doi          = {10.1007/s11263-025-02492-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6491-6512},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Ricci curvature tensor-based volumetric segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $$E^{3}DGE$$: Self-supervised geometry-aware encoder for style-based 3D GAN inversion. <em>IJCV</em>, <em>133</em>(9), 6473-6490. (<a href='https://doi.org/10.1007/s11263-025-02496-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {StyleGAN has excelled in 2D face reconstruction and semantic editing, but the extension to 3D lacks a generic inversion framework, limiting its applications in 3D reconstruction. In this paper, we address the challenge of 3D GAN inversion, focusing on predicting a latent code from a single 2D image to faithfully recover 3D shapes and textures. The inherent ill-posed nature of the problem, coupled with the limited capacity of global latent codes, presents significant challenges. To overcome these challenges, we introduce an efficient self-training scheme that does not rely on real-world 2D-3D pairs but instead utilizes proxy samples generated from a 3D GAN. Additionally, our approach goes beyond the global latent code by enhancing the generation network with a local branch. This branch incorporates pixel-aligned features to accurately reconstruct texture details. Furthermore, we introduce a novel pipeline for 3D view-consistent editing. The efficacy of our method is validated on two representative 3D GANs, namely StyleSDF and EG3D. Through extensive experiments, we demonstrate that our approach consistently outperforms state-of-the-art inversion methods, delivering superior quality in both shape and texture reconstruction.},
  archive      = {J_IJCV},
  author       = {Lan, Yushi and Meng, Xuyi and Yang, Shuai and Loy, Chen Change and Dai, Bo},
  doi          = {10.1007/s11263-025-02496-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6473-6490},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {$$E^{3}DGE$$: Self-supervised geometry-aware encoder for style-based 3D GAN inversion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StimuVAR: Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models. <em>IJCV</em>, <em>133</em>(9), 6456-6472. (<a href='https://doi.org/10.1007/s11263-025-02495-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting and reasoning how a video would make a human feel is crucial for developing socially intelligent systems. Although Multimodal Large Language Models (MLLMs) have shown impressive video understanding capabilities, they tend to focus more on the semantic content of videos, often overlooking emotional stimuli. Hence, most existing MLLMs fall short in estimating viewers’ emotional reactions and providing plausible explanations. To address this issue, we propose StimuVAR, a spatiotemporal Stimuli-aware framework for Video Affective Reasoning (VAR) with MLLMs. StimuVAR incorporates a two-level stimuli-aware mechanism: frame-level awareness and token-level awareness. Frame-level awareness involves sampling video frames with events that are most likely to evoke viewers’ emotions. Token-level awareness performs tube selection in the token space to make the MLLM concentrate on emotion-triggered spatiotemporal regions. Furthermore, we create VAR instruction data to perform affective training, steering MLLMs’ reasoning strengths towards emotional focus and thereby enhancing their affective reasoning ability. To thoroughly assess the effectiveness of VAR, we provide a comprehensive evaluation protocol with extensive metrics. StimuVAR is the first MLLM-based method for viewer-centered VAR. Experiments demonstrate its superiority in understanding viewers’ emotional responses to videos and providing coherent and insightful explanations.},
  archive      = {J_IJCV},
  author       = {Guo, Yuxiang and Siddiqui, Faizan and Zhao, Yang and Chellappa, Rama and Lo, Shao-Yuan},
  doi          = {10.1007/s11263-025-02495-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6456-6472},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {StimuVAR: Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EventEgo3D++: 3D human motion capture from a head-mounted event camera. <em>IJCV</em>, <em>133</em>(9), 6432-6455. (<a href='https://doi.org/10.1007/s11263-025-02489-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular egocentric 3D human motion capture remains a significant challenge, particularly under conditions of low lighting and fast movements, which are common in head-mounted device applications. Existing methods that rely on RGB cameras often fail under these conditions. To address these limitations, we introduce EventEgo3D++, the first approach that leverages a monocular event camera with a fisheye lens for 3D human motion capture. Event cameras excel in high-speed scenarios and varying illumination due to their high temporal resolution, providing reliable cues for accurate 3D human motion capture. EventEgo3D++ leverages the LNES representation of event streams to enable precise 3D reconstructions. We have also developed a mobile head-mounted device (HMD) prototype equipped with an event camera, capturing a comprehensive dataset that includes real event observations from both controlled studio environments and in-the-wild settings, in addition to a synthetic dataset. Additionally, to provide a more holistic dataset, we include allocentric RGB streams that offer different perspectives of the HMD wearer, along with their corresponding SMPL body model. Our experiments demonstrate that EventEgo3D++ achieves superior 3D accuracy and robustness compared to existing solutions, even in challenging conditions. Moreover, our method supports real-time 3D pose updates at a rate of 140Hz. This work is an extension of the EventEgo3D approach (CVPR 2024) and further advances the state of the art in egocentric 3D human motion capture. For more details, visit the project page at https://eventego3d.mpi-inf.mpg.de .},
  archive      = {J_IJCV},
  author       = {Millerdurai, Christen and Akada, Hiroyasu and Wang, Jian and Luvizon, Diogo and Pagani, Alain and Stricker, Didier and Theobalt, Christian and Golyanik, Vladislav},
  doi          = {10.1007/s11263-025-02489-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6432-6455},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {EventEgo3D++: 3D human motion capture from a head-mounted event camera},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast sampling through the reuse of attention maps in diffusion models. <em>IJCV</em>, <em>133</em>(9), 6422-6431. (<a href='https://doi.org/10.1007/s11263-025-02463-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image diffusion models have demonstrated unprecedented capabilities for flexible and realistic image synthesis. Nevertheless, these models rely on a time-consuming sampling procedure, which has motivated attempts to reduce their latency. When improving efficiency, researchers often use the original diffusion model to train an additional network designed specifically for fast image generation. In contrast, our approach seeks to reduce latency directly, without any retraining, fine-tuning, or knowledge distillation. In particular, we find the repeated calculation of attention maps to be costly yet redundant, and instead suggest reusing them during sampling. Our specific reuse strategies are based on ODE theory, which implies that the later a map is reused, the smaller the distortion in the final image. We empirically compare our reuse strategies with few-step sampling procedures of comparable latency, finding that reuse generates images that are closer to those produced by the original high-latency diffusion model.},
  archive      = {J_IJCV},
  author       = {Hunter, Rosco and Dudziak, Łukasz and Abdelfattah, Mohamed S. and Mehrotra, Abhinav and Bhattacharya, Sourav and Wen, Hongkai},
  doi          = {10.1007/s11263-025-02463-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6422-6431},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fast sampling through the reuse of attention maps in diffusion models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RML++: Regroup median loss for combating label noise. <em>IJCV</em>, <em>133</em>(9), 6400-6421. (<a href='https://doi.org/10.1007/s11263-025-02494-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep neural networks (DNNs) typically necessitates large-scale, high-quality annotated datasets. However, due to the inherent challenges of precisely annotating vast numbers of training samples, label noise—characterized by potentially erroneous annotations—is common yet detrimental in practice. Currently, to combat the negative impacts of label noise, mainstream studies follow a pipeline that begins with data sampling and is followed by loss correction. Data sampling aims to partition the original training dataset into clean and noisy subsets, but it often suffers from biased sampling that can mislead models. Additionally, loss correction typically requires knowledge of the noise rate as a priori information, of which the precise estimation can be challenging. To this end, we propose a novel method, Regroup Median Loss Plus Plus (RML++), that addresses both of the previous drawbacks. Specifically, the training dataset is partitioned into clean and noisy subsets using a newly designed separation approach, which synergistically combines prediction consistency with an adaptive threshold to ensure a reliable sampling. Moreover, to ensure the noisy subsets can be robustly learned by models, we suggest to estimate the losses of noisy training samples by utilizing the same-class samples from the clean subset. Subsequently, the proposed method corrects the labels of noisy samples based on the model predictions with the regularization of RML++. Compared to state-of-the-art (SOTA) methods, RML++ achieves significant improvements on both synthetic and challenging real-world datasets. The source code is available at https://github.com/Feng-peng-Li/RML-Extension .},
  archive      = {J_IJCV},
  author       = {Li, Fengpeng and Li, Kemou and Wang, Qizhou and Han, Bo and Tian, Jinyu and Zhou, Jiantao},
  doi          = {10.1007/s11263-025-02494-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6400-6421},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RML++: Regroup median loss for combating label noise},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A physics-informed deep learning deformable medical image registration method based on neural ODEs. <em>IJCV</em>, <em>133</em>(9), 6374-6399. (<a href='https://doi.org/10.1007/s11263-025-02476-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An unsupervised machine learning method is introduced to align medical images in the context of the large deformation elasticity coupled with growth and remodeling biophysics. The technique, which stems from the principle of minimum potential energy in solid mechanics, consists of two steps: Firstly, in the predictor step, the geometric registration is achieved by minimizing a loss function composed of a dissimilarity measure and a regularizing term. Secondly, the physics of the problem, including the equilibrium equations along with growth mechanics, are enforced in a corrector step by minimizing the potential energy corresponding to a Dirichlet problem, where the predictor solution defines the boundary condition and is maintained by distance functions. The features of the new solution procedure, as well as the nature of the registration problem, are highlighted by considering several examples. In particular, registration problems containing large non-uniform deformations caused by extension, shearing, and bending of multiply-connected regions are used as benchmarks. In addition, we analyzed a benchmark biological example (registration for brain data) to showcase that the new deep learning method competes with available methods in the literature. We then applied the method to various datasets. First, we analyze the regrowth of the zebrafish embryonic fin from confocal imaging data. Next, we evaluate the quality of the solution procedure for two examples related to the brain. For one, we apply the new method for 3D image registration of longitudinal magnetic resonance images of the brain to assess cerebral atrophy, where a first-order ODE describes the volume loss mechanism. For the other, we explore cortical expansion during early fetal brain development by coupling the elastic deformation with morphogenetic growth dynamics. The method and examples show the ability of our framework to attain high-quality registration and, concurrently, solve large deformation elasticity balance equations and growth and remodeling dynamics.},
  archive      = {J_IJCV},
  author       = {Amiri-Hezaveh, Amirhossein and Tan, Shelly and Deng, Qing and Umulis, David and Cunniff, Lauren and Weickenmeier, Johannes and Buganza Tepole, Adrian},
  doi          = {10.1007/s11263-025-02476-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6374-6399},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A physics-informed deep learning deformable medical image registration method based on neural ODEs},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time neural radiance talking portrait synthesis via audio-spatial decomposition. <em>IJCV</em>, <em>133</em>(9), 6362-6373. (<a href='https://doi.org/10.1007/s11263-025-02481-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Neural Radiance Fields (NeRF) have been successful in high-fidelity 3D modeling of talking portraits. However, slow training and inference speed have obstructed their potential usage. This paper proposes an efficient NeRF-based framework, which enables faster convergence and real-time synthesizing of stable talking portraits, by utilizing the recent success of grid-based NeRF. This is accomplished by decomposing the inherently high-dimensional talking portrait representation into three low-dimensional feature grids. Specifically, a Decomposed Audio-Spatial Encoding Module models the dynamic head with a 3D spatial grid and a 2D audio grid, where audio dynamics are modeled in a spatial-dependent manner to avoid undesirable flickering. The torso is handled with another 2D grid in a lightweight Pseudo-3D Deformable Module. Extensive experiments demonstrate that our method can generate realistic and audio-lips synchronized talking portrait videos, while also being highly efficient. Our project page is available at https://me.kiui.moe/radnerf/ .},
  archive      = {J_IJCV},
  author       = {Tang, Jiaxiang and Wang, Kaisiyuan and Zhou, Hang and Chen, Xiaokang and He, Dongliang and Hu, Tianshu and Liu, Jingtuo and Liu, Ziwei and Zeng, Gang and Wang, Jingdong},
  doi          = {10.1007/s11263-025-02481-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6362-6373},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Real-time neural radiance talking portrait synthesis via audio-spatial decomposition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skim then focus: Integrating contextual and fine-grained views for repetitive action counting. <em>IJCV</em>, <em>133</em>(9), 6347-6361. (<a href='https://doi.org/10.1007/s11263-025-02471-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to action counting is accurately locating each video’s repetitive actions. Instead of estimating the probability of each frame belonging to an action directly, we propose a dual-branch network, i.e., SkimFocusNet, working in a two-step manner. The model draws inspiration from empirical observations indicating that humans initially engage in coarse skimming of entire sequences to quickly locate potential target action frames and grasp general motion patterns. This is followed by finer, frame-by-frame focusing to precisely determine whether the located frames align with the target actions. Specifically, SkimFocusNet incorporates a skim branch and a focus branch. The skim branch scans the global contextual information throughout the sequence to identify potential target action for guidance. Subsequently, the focus branch utilizes the guidance to diligently identify repetitive actions using a long-short adaptive guidance (LSAG) block. Additionally, we have observed that videos in existing datasets often feature only one type of repetitive action, which inadequately represents real-world scenarios. To more accurately describe real-life situations, we establish the Multi-RepCount dataset, which includes videos containing multiple repetitive motions. On Multi-RepCount, our SkimFoucsNet can perform specified action counting, that is, to enable counting a particular action type by referencing an exemplary video. This capability substantially exhibits our method’s robustness, particularly in accurately performing action counting despite the presence of interfering actions. Extensive experiments demonstrate that SkimFocusNet achieves state-of-the-art performances with significant improvements. We also conduct a thorough ablation study to evaluate the network components. The source code will be published upon acceptance https://github.com/isotopezzq/SkimFocusNet .},
  archive      = {J_IJCV},
  author       = {Zhao, Zhengqi and Huang, Xiaohu and Zhou, Hao and Yao, Kun and Ding, Errui and Wang, Jingdong and Wang, Xinggang and Liu, Wenyu and Bin, Feng},
  doi          = {10.1007/s11263-025-02471-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6347-6361},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Skim then focus: Integrating contextual and fine-grained views for repetitive action counting},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DogRecon: Canine prior-guided animatable 3D gaussian dog reconstruction from a single image. <em>IJCV</em>, <em>133</em>(9), 6332-6346. (<a href='https://doi.org/10.1007/s11263-025-02485-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle animatable 3D dog reconstruction from a single image, noting the overlooked potential of animals. Particularly, we focus on dogs, emphasizing their intrinsic characteristics that complicate 3D observation. First, the considerable variation in shapes across breeds presents a complexity for modeling. Additionally, the nature of quadrupeds leads to frequent joint occlusions compared to humans. These challenges make 3D reconstruction from 2D observations difficult, and it becomes dramatically harder when constrained to a single image. To address these challenges, our insight is to combine the acquisition of appearance from generative models, without additional data, with geometric guidance provided by a parametric representation, aiming to achieve complete geometry. To this end, we present DogRecon, our framework consists of two key components: Canine-centric novel view synthesis with canine prior for multi-view generation of dog and a reliable sampling weight strategy with Gaussian Splatting for animatable 3D dog reconstruction. Extensive experiments on the GART, DFA, and internet-sourced datasets confirm our framework has state-of-the-art performance in image-to-3D generation and comparable performance in animatable 3D reconstruction. Additionally, we demonstrate novel pose animation and text-to-3D dog reconstruction as applications. Project page: https://vision3d-lab.github.io/dogrecon/},
  archive      = {J_IJCV},
  author       = {Cho, Gyeongsu and Kang, Changwoo and Soon, Donghyeon and Joo, Kyungdon},
  doi          = {10.1007/s11263-025-02485-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6332-6346},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DogRecon: Canine prior-guided animatable 3D gaussian dog reconstruction from a single image},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thread counting in plain weave for old paintings using regression deep learning models. <em>IJCV</em>, <em>133</em>(9), 6316-6331. (<a href='https://doi.org/10.1007/s11263-025-02473-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel algorithm designed to improve thread density estimation in canvas analysis. Our approach incorporates three major contributions. First, we eliminate the need for post-segmentation processing by integrating regression techniques, enabling the deep learning (DL) model to directly compute thread density. This does not only reduce computational time but also shifts the training focus from locating crossing points to minimizing thread counting errors, thereby enhancing accuracy. We develop and rigorously evaluate various models, selecting the one with optimal performance through a hyperparameter search. Second, we refine the data generation process by dynamically adjusting filter lengths based on initial thread density estimates and incorporating equalization. We also enhance data augmentation. Third, we implement semi-supervised training to expand the dataset and fine-tune model weights. This involves incorporating new inputs into the training set when both the DL model and Fourier transform yield similar density estimates for new paintings. Our proposed algorithm demonstrates superior performance in thread density error reduction and operational efficiency compared to previous DL segmentation solutions for masterpieces from Ribera, Velázquez, or Poussin. Additionally, it has been effectively applied to identify fabric matches between canvases attributed to different authors, showcasing its practical applicability in art analysis.},
  archive      = {J_IJCV},
  author       = {Delgado, Antonio and Murillo-Fuentes, Juan José and Alba-Carcelén, Laura},
  doi          = {10.1007/s11263-025-02473-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6316-6331},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Thread counting in plain weave for old paintings using regression deep learning models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). About time: Advances, challenges, and outlooks of action understanding. <em>IJCV</em>, <em>133</em>(9), 6251-6315. (<a href='https://doi.org/10.1007/s11263-025-02478-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have witnessed impressive advances in video action understanding. Increased dataset sizes, variability, and computation availability have enabled leaps in performance and task diversification. Current systems can provide coarse- and fine-grained descriptions of video scenes, extract segments corresponding to queries, synthesize unobserved parts of videos, and predict context across multiple modalities. This survey comprehensively reviews advances in uni- and multi-modal action understanding across a range of tasks. We focus on prevalent challenges, overview widely adopted datasets, and survey seminal works with an emphasis on recent advances. We broadly distinguish between three temporal scopes: (1) recognition tasks of actions observed in full, (2) prediction tasks for ongoing partially observed actions, and (3) forecasting tasks for subsequent unobserved action(s). This division allows us to identify specific action modeling and video representation challenges. Finally, we outline future directions to address current shortcomings.},
  archive      = {J_IJCV},
  author       = {Stergiou, Alexandros and Poppe, Ronald},
  doi          = {10.1007/s11263-025-02478-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6251-6315},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {About time: Advances, challenges, and outlooks of action understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for text recognition: A critical survey. <em>IJCV</em>, <em>133</em>(9), 6221-6250. (<a href='https://doi.org/10.1007/s11263-025-02487-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text Recognition (TR) refers to the research area that focuses on retrieving textual information from images, a topic that has seen significant advancements in the last decade due to the use of Deep Neural Networks (DNN). However, these solutions often necessitate vast amounts of manually labeled or synthetic data. Addressing this challenge, Self-Supervised Learning (SSL) has gained attention by utilizing large datasets of unlabeled data to train DNN, thereby generating meaningful and robust representations. Although SSL was initially overlooked in TR because of its unique characteristics, recent years have witnessed a surge in the development of SSL methods specifically for this field. This rapid development, however, has led to many methods being explored independently, without taking previous efforts in methodology or comparison into account, thereby hindering progress in the field of research. This paper, therefore, seeks to consolidate the use of SSL in the field of TR, offering a critical and comprehensive overview of the current state of the art. We will review and analyze the existing methods, compare their results, and highlight inconsistencies in the current literature. This thorough analysis aims to provide general insights into the field, propose standardizations, identify new research directions, and foster its proper development.},
  archive      = {J_IJCV},
  author       = {Penarrubia, Carlos and Valero-Mas, Jose J. and Calvo-Zaragoza, Jorge},
  doi          = {10.1007/s11263-025-02487-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6221-6250},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Self-supervised learning for text recognition: A critical survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Img2Tab: Automatic class relevant concept discovery from StyleGAN features for explainable image classification. <em>IJCV</em>, <em>133</em>(9), 6201-6220. (<a href='https://doi.org/10.1007/s11263-025-02474-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional tabular classifiers provide explainable decision-making with interpretable features (concepts). However, using their explainability in vision tasks has been limited due to the pixel representation of images. In this paper, we design Img2Tabs that classify images by concepts to harness the explainability of tabular classifiers. Img2Tabs encode image pixels into tabular features by StyleGAN inversion. Since not all of the resulting features are class-relevant or interpretable due to their generative nature, the Img2Tab classifier should automatically discover class-relevant concepts from the StyleGAN features. Thus, we propose a novel algorithm using the Wasserstein-1 metric to quantify class-relevancy and interpretability simultaneously. By this method of concept visualization, we quantitatively investigate whether important features extracted by tabular classifiers are class-relevant concepts. Consequently, we determine the most effective classifier for Img2Tabs in terms of discovering class-relevant concepts automatically from StyleGAN features. In evaluations, we demonstrate concept-based explanations through importance and visualization. Img2Tab achieves top-1 accuracy on par with CNN classifiers and deep feature learning baselines. Additionally, we show that users can interactively debug Img2Tab classifier to prevent erroneous decision-making from data bias without sacrificing accuracy. The source and demo code for Img2Tab are available at https://github.com/songsnim/Img2Tab_pytorch},
  archive      = {J_IJCV},
  author       = {Song, Youngjae and Shyn, Sung Kuk and Kim, Kwang-su},
  doi          = {10.1007/s11263-025-02474-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6201-6220},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Img2Tab: Automatic class relevant concept discovery from StyleGAN features for explainable image classification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal transport with arbitrary prior for dynamic resolution network. <em>IJCV</em>, <em>133</em>(9), 6187-6200. (<a href='https://doi.org/10.1007/s11263-025-02483-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic resolution network is proved to be crucial in reducing computational redundancy by automatically assigning satisfactory resolution for each input image. However, it is observed that resolution choices are often collapsed, where prior works tend to assign images to the resolution routes whose computational cost is close to the required FLOPs. In this paper, we propose a novel optimal transport dynamic resolution network (OTD-Net) by establishing an intrinsic connection between resolution assignment and optimal transport problem. In this framework, each sample owns a resolution assignment choice viewed as supplier, and each resolution requires unallocated images considered as demander. With two assignment priors, OTD-Net benefits from the non-collapse division under theoretical support, and produces the desired assignment policy by balancing the computation budget and prediction accuracy. On that basis, a multi-resolution inference is proposed to ensemble low-resolution predictions. Extensive experiments including image classification, object detection and depth estimation, show our approach is both efficient and effective for both ResNet and Transformer, achieving state-of-the-art performance on various benchmarks.},
  archive      = {J_IJCV},
  author       = {Zhang, Zhizhong and Li, Shujun and Zhang, Chenyang and Ma, Lizhuang and Tan, Xin and Xie, Yuan},
  doi          = {10.1007/s11263-025-02483-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6187-6200},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Optimal transport with arbitrary prior for dynamic resolution network},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoViT: Achieving real-time vision transformers on mobile via latency-aware coarse-to-fine search. <em>IJCV</em>, <em>133</em>(9), 6170-6186. (<a href='https://doi.org/10.1007/s11263-025-02480-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their impressive performance on various tasks, vision transformers (ViTs) are heavy for mobile vision applications. Recent works have proposed combining the strengths of ViTs and convolutional neural networks (CNNs) to build lightweight networks. Still, these approaches rely on hand-designed architectures with a pre-determined number of parameters. In this work, we address the challenge of finding optimal light-weight ViTs given constraints on model size and computational cost using neural architecture search. We use a search algorithm that considers both model parameters and on-device deployment latency. This method analyzes network properties, hardware memory access pattern, and degree of parallelism to directly and accurately estimate the network latency. To prevent the need for extensive testing during the search process, we use a lookup table based on a detailed breakdown of the speed of each component and operation, which can be reused to evaluate the whole latency of each search structure. Our approach leads to improved efficiency compared to testing the speed of the whole model during the search process. Extensive experiments demonstrate that, under similar parameters and FLOPs, our searched lightweight ViTs achieve higher accuracy and lower latency than state-of-the-art models. For instance, on ImageNet-1K, AutoViT_XXS (71.3% Top-1 accuracy, 10.2ms latency) outperforms MobileViTv3_XXS (71.0% Top-1 accuracy, 12.5ms latency) with 0.3% higher accuracy and 2.3ms lower latency.},
  archive      = {J_IJCV},
  author       = {Kong, Zhenglun and Xu, Dongkuan and Li, Zhengang and Dong, Peiyan and Tang, Hao and Wang, Yanzhi and Mukherjee, Subhabrata},
  doi          = {10.1007/s11263-025-02480-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6170-6186},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AutoViT: Achieving real-time vision transformers on mobile via latency-aware coarse-to-fine search},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking open-set object detection: Issues, a new formulation, and taxonomy. <em>IJCV</em>, <em>133</em>(9), 6145-6169. (<a href='https://doi.org/10.1007/s11263-025-02479-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-set object detection (OSOD), a task involving the detection of unknown objects while accurately detecting known objects, has recently gained attention. However, we identify a fundamental issue with the problem formulation employed in current OSOD studies. Inherent to object detection is knowing “what to detect,” which contradicts the idea of identifying “unknown” objects. This sets OSOD apart from open-set recognition (OSR). This contradiction complicates a proper evaluation of methods’ performance, a fact that previous studies have overlooked. Next, we propose a novel formulation wherein detectors are required to detect both known and unknown classes within specified super-classes of object classes. This new formulation is free from the aforementioned issues and has practical applications. Finally, we design benchmark tests utilizing existing datasets and report the experimental evaluation of existing OSOD methods. The results show that existing methods fail to accurately detect unknown objects due to misclassification of known and unknown classes rather than incorrect bounding box prediction. As a byproduct, we introduce a taxonomy of OSOD, resolving confusion prevalent in the literature. We anticipate that our study will encourage the research community to reconsider OSOD and facilitate progress in the right direction.},
  archive      = {J_IJCV},
  author       = {Hosoya, Yusuke and Suganuma, Masanori and Okatani, Takayuki},
  doi          = {10.1007/s11263-025-02479-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6145-6169},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rethinking open-set object detection: Issues, a new formulation, and taxonomy},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight structure-aware attention for visual understanding. <em>IJCV</em>, <em>133</em>(9), 6129-6144. (<a href='https://doi.org/10.1007/s11263-025-02475-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention operator has been widely used as a basic brick in visual understanding since it provides some flexibility through its adjustable kernels. However, this operator suffers from inherent limitations: (1) the attention kernel is not discriminative enough, resulting in high redundancy, and (2) the complexity in computation and memory is quadratic in the sequence length. In this paper, we propose a novel attention operator, called Lightweight Structure-aware Attention (LiSA), which has a better representation power with log-linear complexity. Our operator transforms the attention kernels to be more discriminative by learning structural patterns. These structural patterns are encoded by exploiting a set of relative position embeddings (RPEs) as multiplicative weights, thereby improving the representation power of the attention kernels. Additionally, the RPEs are approximated to obtain log-linear complexity. Our experiments and analyses demonstrate that the proposed operator outperforms self-attention and other existing operators, achieving state-of-the-art results on ImageNet-1K and other downstream tasks such as video action recognition on Kinetics-400, object detection & instance segmentation on COCO, and semantic segmentation on ADE-20K.},
  archive      = {J_IJCV},
  author       = {Kwon, Heeseung and Castro, Francisco M. and Marin-Jimenez, Manuel J. and Guil, Nicolas and Alahari, Karteek},
  doi          = {10.1007/s11263-025-02475-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6129-6144},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lightweight structure-aware attention for visual understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointOBB-v3: Expanding performance boundaries of single point-supervised oriented object detection. <em>IJCV</em>, <em>133</em>(9), 6108-6128. (<a href='https://doi.org/10.1007/s11263-025-02486-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing demand for oriented object detection (OOD), recent studies on point-supervised OOD have attracted significant interest. In this paper, we propose PointOBB-v3, a stronger single point-supervised OOD framework. Compared to existing methods, it generates pseudo rotated boxes without additional priors and incorporates support for the end-to-end paradigm. PointOBB-v3 functions by integrating three unique image views: the original view, a resized view, and a rotated/flipped (rot/flp) view. Based on the views, a scale augmentation module and an angle acquisition module are constructed. In the first module, a Scale-Sensitive Consistency (SSC) loss and a Scale-Sensitive Feature Fusion (SSFF) module are introduced to improve the model’s ability to estimate object scale. To achieve precise angle predictions, the second module employs symmetry-based self-supervised learning. Additionally, we introduce an end-to-end version that eliminates the pseudo-label generation process by integrating a detector branch and introduces an Instance-Aware Weighting (IAW) strategy to focus on high-quality predictions. We conducted extensive experiments on the DIOR-R, DOTA-v1.0/v1.5/v2.0, FAIR1M, STAR, and RSAR datasets. Across all these datasets, our method achieves an average improvement in accuracy of 3.56% in comparison to previous state-of-the-art methods. The code will be available at https://github.com/ZpyWHU/PointOBB-v3 .},
  archive      = {J_IJCV},
  author       = {Zhang, Peiyuan and Luo, Junwei and Yang, Xue and Yu, Yi and Li, Qingyun and Zhou, Yue and Jia, Xiaosong and Lu, Xudong and Chen, Jingdong and Li, Xiang and Yan, Junchi and Li, Yansheng},
  doi          = {10.1007/s11263-025-02486-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6108-6128},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PointOBB-v3: Expanding performance boundaries of single point-supervised oriented object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling scattering effect for under-display camera image restoration. <em>IJCV</em>, <em>133</em>(9), 6088-6107. (<a href='https://doi.org/10.1007/s11263-025-02454-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The under-display camera (UDC) technology furnishes users with an uninterrupted full-screen viewing experience, eliminating the need for notches or punch holes. However, the translucent properties of the display lead to substantial degradation in UDC images. This work addresses the challenge of restoring UDC images by specifically targeting the scattering effect induced by the display. We explicitly model this scattering phenomenon by treating the display as a homogeneous scattering medium. Leveraging this physical model, the image formation pipeline is enhanced to synthesize more realistic UDC images alongside corresponding ground-truth images, thereby constructing a more accurate UDC dataset. To counteract the scattering effect in the restoration process, we propose a dual-branch network. The scattering branch employs channel-wise self-attention to estimate the scattering parameters, while the image branch capitalizes on the local feature representation capabilities of CNNs to restore the degraded UDC images. Additionally, we introduce a novel channel-wise cross-attention fusion block that integrates global scattering information into the image branch, facilitating improved restoration. To further refine the model, we design a dark channel regularization loss during training to reduce the gap between the dark channel distributions of the restored and ground-truth images. Comprehensive experiments conducted on both synthetic and real-world datasets demonstrate the superiority of our approach over current state-of-the-art UDC restoration methods. Our source code is publicly available at: https://github.com/NamecantbeNULL/SRUDC_pp .},
  archive      = {J_IJCV},
  author       = {Song, Binbin and Zhou, Jiantao and Chen, Xiangyu and Xu, Shuning},
  doi          = {10.1007/s11263-025-02454-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6088-6107},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Modeling scattering effect for under-display camera image restoration},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIM4D: Masked modeling with multi-view video for autonomous driving representation learning. <em>IJCV</em>, <em>133</em>(9), 6074-6087. (<a href='https://doi.org/10.1007/s11263-025-02464-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning robust and scalable visual representations from massive multi-view video data remains a challenge in computer vision and autonomous driving. Existing pre-training methods either rely on expensive supervised learning with 3D annotations, limiting the scalability, or focus on single-frame or monocular inputs, neglecting the temporal information, which is fundamental for the ultimate application, i.e., end-to-end planning. We propose MIM4D, a novel pre-training paradigm based on dual masked image modeling (MIM). MIM4D leverages both spatial and temporal relations by training on masked multi-view video inputs. It constructs pseudo-3D features using continuous scene flow and projects them onto 2D plane for supervision. To address the lack of dense 3D supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable rendering to learn geometric representations. We demonstrate that MIM4D achieves state-of-the-art performance on the nuScenes dataset for visual representation learning in autonomous driving. It significantly improves existing methods on multiple downstream tasks, including end-to-end planning( $$9\%$$ collision decrease), BEV segmentation ( $$8.7\%$$ IoU), 3D object detection ( $$3.5\%$$ mAP), and HD map construction ( $$1.4\%$$ mAP). Our work offers a new choice for learning representation at scale in autonomous driving. Code and models are released at https://github.com/hustvl/MIM4D .},
  archive      = {J_IJCV},
  author       = {Zou, Jialv and Liao, Bencheng and Zhang, Qian and Liu, Wenyu and Wang, Xinggang},
  doi          = {10.1007/s11263-025-02464-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6074-6087},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MIM4D: Masked modeling with multi-view video for autonomous driving representation learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RigNet++: Semantic assisted repetitive image guided network for depth completion. <em>IJCV</em>, <em>133</em>(9), 6051-6073. (<a href='https://doi.org/10.1007/s11263-025-02470-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion aims to recover dense depth maps from sparse ones, where color images are often used to facilitate this task. Recent depth methods primarily focus on image guided learning frameworks. However, blurry guidance in the image and unclear structure in the depth still impede their performance. To tackle these challenges, we explore a repetitive design in our image guided network to gradually and sufficiently recover depth values. Specifically, the repetition is embodied in both the image guidance branch and depth generation branch. In the former branch, we design a dense repetitive hourglass network (DRHN) to extract discriminative image features of complex environments, which can provide powerful contextual instruction for depth prediction. In the latter branch, we present a repetitive guidance (RG) module based on dynamic convolution, in which an efficient convolution factorization is proposed to reduce the complexity while modeling high-frequency structures progressively. Furthermore, in the semantic guidance branch, we utilize the well-known large vision model, i.e., segment anything (SAM), to supply RG with semantic prior. In addition, we propose a region-aware spatial propagation network (RASPN) for further depth refinement based on the semantic prior constraint. Finally, we collect a new dataset termed TOFDC for the depth completion task, which is acquired by the time-of-flight (TOF) sensor and the color camera on smartphones. Extensive experiments demonstrate that our method achieves state-of-the-art performance on KITTI, NYUv2, Matterport3D, 3D60, VKITTI, and our TOFDC.},
  archive      = {J_IJCV},
  author       = {Yan, Zhiqiang and Li, Xiang and Hui, Le and Zhang, Zhenyu and Li, Jun and Yang, Jian},
  doi          = {10.1007/s11263-025-02470-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6051-6073},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RigNet++: Semantic assisted repetitive image guided network for depth completion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized contour vibration model for building extraction. <em>IJCV</em>, <em>133</em>(9), 6025-6050. (<a href='https://doi.org/10.1007/s11263-025-02468-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classic active contour models (ACMs) are becoming a great promising solution to the contour-based object extraction with the progress of deep learning recently. Inspired by the wave vibration theory in physics, we propose a Generalized Contour Vibration Model (G-CVM) by inheriting the force and motion principle of contour wave for automatically estimating building contours. The contour estimation problems, conventionally solved by snake and level-set based ACMs, are unified to formulate as second-order partial differential equation to model the contour evolution. In parallel with the current ACM methods, we propose two types of evolution paradigms: curve-CVM and surface-CVM, from the perspective of the vibration spaces of contour waves. To tailor personalization contours for specific targets, we parameterize the constant coefficient wave differential equation through a convolutional network, and hereby integrate them into a unified learnable model for contour extraction. Through adopting finite difference optimization, we can progressively perform the contour evolution from an initial state through a recursive computation on the contour vibration model. Both the building contour evolution and the model optimization are modulated to form a close-looping end-to-end network. Besides, we make a discussion of ours vs the conventional ACMs, all which can be interpreted uniformly from the view of differential equation in different evolution domains. Comprehensive evaluations on several building datasets demonstrate the effectiveness and superiority of our proposed G-CVM when compared with other state-of-the-art building extraction networks and deep active contour solutions.},
  archive      = {J_IJCV},
  author       = {Xu, Chunyan and Yao, Shuaizhen and Xu, Ziqiang and Cui, Zhen and Yang, Jian},
  doi          = {10.1007/s11263-025-02468-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6025-6050},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A generalized contour vibration model for building extraction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal transformer for single RGB-D camera synchronous tracking and reconstruction of non-rigid dynamic objects. <em>IJCV</em>, <em>133</em>(9), 6015-6024. (<a href='https://doi.org/10.1007/s11263-025-02469-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple and effective method that views the problem of single RGB-D camera synchronous tracking and reconstruction of non-rigid dynamic objects as an aligned sequential point cloud prediction problem. Our method does not require additional data transformations (truncated signed distance function or deformation graphs, etc.), alignment constraints (handcrafted features or optical flow, etc.), and prior regularities (as-rigid-as-possible or embedded deformation, etc.). We propose an end-to-end model architecture that is TRansformer for synchronous Tracking and Reconstruction of non-rigid dynamic target based on RGB-D images from a monocular camera, called TR4TR. We use a spatial-temporal combined 2D image encoder that directly encodes features from RGB-D sequence images, and a 3D point decoder to generate aligned sequential point cloud containing tracking and reconstruction results. The TR4TR model outperforms the baselines on the DeepDeform non-rigid dataset, and outperforms the state-of-the-art method by 8.82% on the deformation error evaluation metric. In addition, TR4TR is more robust when the target undergoes large inter-frame deformation. The code is available at https://github.com/xfliu1998/tr4tr-main .},
  archive      = {J_IJCV},
  author       = {Liu, Xiaofei and Yi, Zhengkun and Wu, Xinyu and Shang, Wanfeng},
  doi          = {10.1007/s11263-025-02469-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6015-6024},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Spatial-temporal transformer for single RGB-D camera synchronous tracking and reconstruction of non-rigid dynamic objects},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-aligned learning with collaborative refinement for unsupervised VI-ReID. <em>IJCV</em>, <em>133</em>(9), 5992-6014. (<a href='https://doi.org/10.1007/s11263-025-02461-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to match pedestrian images of the same individual across different modalities without human annotations for model learning. Previous methods unify pseudo-labels of cross-modality images through label association algorithms and then design contrastive learning framework for global feature learning. However, these methods overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. This insight results in insufficient modality-shared learning when only global features are optimized. To address this issue, we propose a Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework, which builds up optimization objective for specific fine-grained patterns emphasized by each modality, thereby achieving complementary alignment between the label distributions of different modalities. Specifically, we first introduce a Dual Association with Global Learning (DAGI) module to unify the pseudo-labels of cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances. Optimization objective is then formulated based on the semantic-aligned features and their corresponding label space. To alleviate the side-effects arising from noisy pseudo-labels, we propose a Global-Part Collaborative Refinement (GPCR) module to mine reliable positive sample sets for the global and part features dynamically and optimize the inter-instance relationships. Extensive experiments demonstrate the effectiveness of the proposed method, which achieves superior performances to state-of-the-art methods. Our code is available at https://github.com/FranklinLingfeng/code-for-SALCR .},
  archive      = {J_IJCV},
  author       = {Cheng, De and He, Lingfeng and Wang, Nannan and Zhang, Dingwen and Gao, Xinbo},
  doi          = {10.1007/s11263-025-02461-z},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5992-6014},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic-aligned learning with collaborative refinement for unsupervised VI-ReID},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to deblur polarized images. <em>IJCV</em>, <em>133</em>(9), 5976-5991. (<a href='https://doi.org/10.1007/s11263-025-02459-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A polarization camera can capture four linear polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of linear polarization (DoLP) and the angle of linear polarization (AoLP) can be directly computed from the captured polarized images. However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoLP and AoLP. Deblurring methods for conventional images often show degraded performance when handling the polarized images since they only focus on deblurring without considering the polarization constraints. In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively. Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image dehazing and reflection removal.},
  archive      = {J_IJCV},
  author       = {Zhou, Chu and Teng, Minggui and Zhou, Xinyu and Xu, Chao and Sato, Imari and Shi, Boxin},
  doi          = {10.1007/s11263-025-02459-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5976-5991},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to deblur polarized images},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized closed-form formulae for feature-based subpixel alignment in patch-based matching. <em>IJCV</em>, <em>133</em>(9), 5958-5975. (<a href='https://doi.org/10.1007/s11263-025-02457-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patch-based matching is a technique meant to measure the disparity between pixels in a source and target image and is at the core of various methods in computer vision. When the subpixel disparity between the source and target images is required, the cost function or the target image has to be interpolated. While cost-based interpolation is easier to implement, multiple works have shown that image-based interpolation can increase the accuracy of the disparity estimate. In this paper we review closed-form formulae for subpixel disparity computation for one dimensional matching, e.g., rectified stereo matching, for the standard cost functions used in patch-based matching. We then propose new formulae to generalize to high-dimensional search spaces, which is necessary for unrectified stereo matching and optical flow. We also compare the image-based interpolation formulae with traditional cost-based formulae, and show that image-based interpolation brings a significant improvement over the cost-based interpolation methods for two dimensional search spaces, and small improvement in the case of one dimensional search spaces. The zero-mean normalized cross correlation cost function is found to be preferable for subpixel alignment. A new error model, based on very broad assumptions is outlined in the Supplementary Material to demonstrate why these image-based interpolation formulae outperform their cost-based counterparts and why the zero-mean normalized cross correlation function is preferable for subpixel alignement.},
  archive      = {J_IJCV},
  author       = {Jospin, Laurent Valentin and Laga, Hamid and Boussaid, Farid and Bennamoun, Mohammed},
  doi          = {10.1007/s11263-025-02457-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5958-5975},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generalized closed-form formulae for feature-based subpixel alignment in patch-based matching},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HumanLiff: Layer-wise 3D human diffusion model. <em>IJCV</em>, <em>133</em>(9), 5938-5957. (<a href='https://doi.org/10.1007/s11263-025-02477-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human generation from 2D images has achieved remarkable progress through the synergistic utilization of neural rendering and generative models. Existing 3D human generative models mainly generate a clothed 3D human as an inseparable 3D model in a single pass, while rarely considering the layer-wise nature of a clothed human body, which often consists of the human body and various clothes such as underwear, outerwear, trousers, shoes, etc. In this work, we propose HumanLiff, the first layer-wise 3D human generative model with a unified diffusion process. Specifically, HumanLiff firstly generates minimal-clothed humans, represented by tri-plane features, in a canonical space, and then progressively generates clothes in a layer-wise manner. In this way, the 3D human generation is thus formulated as a sequence of diffusion-based 3D conditional generation. To reconstruct more fine-grained 3D humans with tri-plane representation, we propose a tri-plane shift operation that splits each tri-plane into three sub-planes and shifts these sub-planes to enable feature grid subdivision. To further enhance the controllability of 3D generation with 3D layered conditions, HumanLiff hierarchically fuses tri-plane features and 3D layered conditions to facilitate the 3D diffusion model learning. Extensive experiments on two layer-wise 3D human datasets, SynBody (synthetic) and TightCap (real-world), validate that HumanLiff significantly outperforms state-of-the-art methods in layer-wise 3D human generation. Our code and datasets are available at https://skhu101.github.io/HumanLiff .},
  archive      = {J_IJCV},
  author       = {Hu, Shoukang and Hong, Fangzhou and Hu, Tao and Pan, Liang and Mei, Haiyi and Xiao, Weiye and Yang, Lei and Liu, Ziwei},
  doi          = {10.1007/s11263-025-02477-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5938-5957},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HumanLiff: Layer-wise 3D human diffusion model},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending against adversarial examples via modeling adversarial noise. <em>IJCV</em>, <em>133</em>(9), 5920-5937. (<a href='https://doi.org/10.1007/s11263-025-02467-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples have become a major threat to the reliable application of deep learning models. Meanwhile, this issue promotes the development of adversarial defenses. Adversarial noise contains well-generalizing and misleading features, which can manipulate predicted labels to be flipped maliciously. Motivated by this, we study modeling adversarial noise for defending against adversarial examples by learning the transition relationship between adversarial labels (i.e., flipped labels caused by adversarial noise) and natural labels (i.e., real labels of natural samples). In this work, we propose an adversarial defense method from the perspective of modeling adversarial noise. Specifically, we construct an instance-dependent label transition matrix to represent the label transition relationship for explicitly modeling adversarial noise. The label transition matrix is obtained from the input sample by leveraging a label transition network. By exploiting the label transition matrix, we can infer the natural label from the adversarial label and thus correct wrong predictions misled by adversarial noise. Additionally, to enhance the robustness of the label transition network, we design an adversarial robustness constraint at the transition matrix level. Experimental results demonstrate that our method effectively improves the robust accuracy against multiple attacks and exhibits great performance in detecting adversarial input samples.},
  archive      = {J_IJCV},
  author       = {Zhou, Dawei and Wang, Nannan and Han, Bo and Liu, Tongliang and Gao, Xinbo},
  doi          = {10.1007/s11263-025-02467-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5920-5937},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Defending against adversarial examples via modeling adversarial noise},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring bidirectional bounds for minimax-training of energy-based models. <em>IJCV</em>, <em>133</em>(9), 5898-5919. (<a href='https://doi.org/10.1007/s11263-025-02460-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy-based models (EBMs) estimate unnormalized densities in an elegant framework, but they are generally difficult to train. Recent work has linked EBMs to generative adversarial networks, by noting that they can be trained through a minimax game using a variational lower bound. To avoid the instabilities caused by minimizing a lower bound, we propose to instead work with bidirectional bounds, meaning that we maximize a lower bound and minimize an upper bound when training the EBM. We investigate four different bounds on the log-likelihood derived from different perspectives. We derive lower bounds based on the singular values of the generator Jacobian and on mutual information. To upper bound the negative log-likelihood, we consider a gradient penalty-like bound, as well as one based on diffusion processes. In all cases, we provide algorithms for evaluating the bounds. We compare the different bounds to investigate, the pros and cons of the different approaches. Finally, we demonstrate that the use of bidirectional bounds stabilizes EBM training and yields high-quality density estimation and sample generation.},
  archive      = {J_IJCV},
  author       = {Geng, Cong and Wang, Jia and Chen, Li and Gao, Zhiyong and Frellsen, Jes and Hauberg, Søren},
  doi          = {10.1007/s11263-025-02460-0},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5898-5919},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploring bidirectional bounds for minimax-training of energy-based models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A norm regularization training strategy for robust image quality assessment models. <em>IJCV</em>, <em>133</em>(9), 5883-5897. (<a href='https://doi.org/10.1007/s11263-025-02458-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Quality Assessment (IQA) models predict the quality score of input images. They can be categorized into Full-Reference (FR-) and No-Reference (NR-) IQA models based on the availability of reference images. These models are essential for performance evaluation and optimization guidance in the media industry. However, researchers have observed that introducing imperceptible perturbations to input images can notably influence the predicted scores of both FR- and NR-IQA models, resulting in inaccurate assessments of image quality. This phenomenon is known as adversarial attacks. In this paper, we initially define attacks targeted at both FR-IQA and NR-IQA models. Subsequently, we introduce a defense approach applicable to both types of models, aimed at enhancing the stability of predicted scores and boosting the adversarial robustness of IQA models. To be specific, we present theoretical evidence showing that the magnitude of score changes is related to the $$\ell _1$$ norm of the model’s gradient with respect to the input image. Building upon this theoretical foundation, we propose a norm regularization training strategy aimed at reducing the $$\ell _1$$ norm of the gradient, thereby boosting the robustness of IQA models. Experiments conducted on three FR-IQA and four NR-IQA models demonstrate the effectiveness of our strategy in reducing score changes in the presence of adversarial attacks. To the best of our knowledge, this work marks the first attempt to defend against adversarial attacks on both FR- and NR-IQA models. Our study offers valuable insights into the adversarial robustness of IQA models and provides a foundation for future research in this area.},
  archive      = {J_IJCV},
  author       = {Liu, Yujia and Yang, Chenxi and Li, Dingquan and Jiang, Tingting and Huang, Tiejun},
  doi          = {10.1007/s11263-025-02458-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5883-5897},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A norm regularization training strategy for robust image quality assessment models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijdar">IJDAR - 11</h2>
<ul>
<li><details>
<summary>
(2025). Neurosymbolic information extraction from transactional documents. <em>IJDAR</em>, <em>28</em>(3), 475-485. (<a href='https://doi.org/10.1007/s10032-025-00530-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $$F_1$$ -scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.},
  archive      = {J_IJDAR},
  author       = {Hemmer, Arthur and Coustaty, Mickaël and Bartolo, Nicola and Ogier, Jean-Marc},
  doi          = {10.1007/s10032-025-00530-0},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {475-485},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Neurosymbolic information extraction from transactional documents},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SlimDoc: Lightweight distillation of document transformer models. <em>IJDAR</em>, <em>28</em>(3), 457-473. (<a href='https://doi.org/10.1007/s10032-025-00542-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying state-of-the-art document understanding models remains resource-intensive and impractical in many real-world scenarios, particularly where labeled data is scarce and computational budgets are constrained. To address these challenges, this work proposes a novel approach towards parameter-efficient document understanding models capable of adapting to specific tasks and document types without the need for labeled data. Specifically, we propose an approach coined SlimDoc to distill multimodal document transformer encoder models into smaller student models, using internal signals at different training stages, followed by external signals. Our approach is inspired by TinyBERT and adapted to the domain of document understanding transformers. We demonstrate SlimDoc to outperform both a single-stage distillation and a direct fine-tuning of the student. Experimental results across six document understanding datasets demonstrate our approach’s effectiveness: Our distilled student models achieve on average $$93.0\%$$ of the teacher’s performance, while the fine-tuned students achieve $$87.0\%$$ of the teacher’s performance. Without requiring any labeled data, we create a compact student which achieves $$96.0\%$$ of the performance of its supervised-distilled counterpart and $$86.2\%$$ of the performance of a supervised-fine-tuned teacher model. We demonstrate our distillation approach to pick up on document geometry and to be effective on the two popular document understanding models LiLT and LayoutLMv3. Our implementation and training data is available at https://github.com/marcel-lamott/SlimDoc .},
  archive      = {J_IJDAR},
  author       = {Lamott, Marcel and Shakir, Muhammad Armaghan and Ulges, Adrian and Weweler, Yves-Noel and Shafait, Faisal},
  doi          = {10.1007/s10032-025-00542-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {457-473},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {SlimDoc: Lightweight distillation of document transformer models},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing music score analysis with monte carlo dropout: A probabilistic approach to staff-region detection. <em>IJDAR</em>, <em>28</em>(3), 441-456. (<a href='https://doi.org/10.1007/s10032-025-00541-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layout Analysis (LA) is a critical process for detecting and isolating different components within a scanned document, allowing for more straightforward and precise processing of each part independently. In Optical Music Recognition (OMR), LA is essential for identifying and extracting music staves, which enables effective music notation recognition and processing. While the literature includes several studies exploring methods for staff retrieval, there remains room for improvement in terms of robustness and accuracy. In this work, we introduce a methodology that integrates Monte Carlo Dropout (MCD) into a neural network model in order to improve reliability in staff retrieval from scanned sheet music. Our approach leverages multiple non-deterministic predictions using standard dropout layers during inference and aggregates them through pixel-level combination policies. We extend the MCD technique, originally designed for classification and regression tasks using averaged predictions, to the LA task and introduce new combination strategies: maximum and voting criteria. Experiments on three diverse music score corpora, including printed and handwritten documents, demonstrated the effectiveness of our approach. The averaging and voting (with 25% and 50% of votes) criteria reduced the relative error by 63.6% compared to the baseline and achieved a 32.1% improvement over state-of-the-art methods. Our methodology notably enhanced detection accuracy without requiring modifications to the neural architecture, especially at the edges of staves, where conventional models tend to show higher error rates.},
  archive      = {J_IJDAR},
  author       = {Oliva-Bulpitt, Samuel B. and Martinez-Esteso, Juan P. and Galan-Cuenca, Alejandro and Castellanos, Francisco J. and Gallego, Antonio Javier},
  doi          = {10.1007/s10032-025-00541-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {441-456},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Enhancing music score analysis with monte carlo dropout: A probabilistic approach to staff-region detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level style control for chinese handwriting generation. <em>IJDAR</em>, <em>28</em>(3), 429-440. (<a href='https://doi.org/10.1007/s10032-025-00533-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of artificial intelligence-generated content (AIGC) has made significant progress in handwriting generation, including complex scripts such as Chinese. Previous offline Chinese handwriting generation methods focus on isolated character image generation, without addressing the task of offline Chinese handwritten text line generation. This paper proposed a novel multi-level style control method for generating Chinese handwritten text lines, which consists of two key steps: style-controlled character image generation and text-line layout generation. For style-controlled character image generation, we implement a pre-training method utilizing multi-modal radical-level contrastive loss to align image features with radical embeddings in encoders. Additionally, a multi-level style representation control is achieved through multi-modal feature aggregation. We also propose a style-consistent text-line layout generation scheme by using prompt engineering with a large language model. Experimental results demonstrate that our method achieves comparable or even better performance in character image generation compared to diffusion model-based methods, while also delivering faster generation speeds. By incorporating text-line layout generation, the generated text-line samples can be effectively used for training handwriting recognition models.},
  archive      = {J_IJDAR},
  author       = {Yao, Gang and Peng, Liangrui and Li, Zhiyu and zhao, Tianqi and Zhao, Kemeng and Ding, Ning and Tao, Yao},
  doi          = {10.1007/s10032-025-00533-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {429-440},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Multi-level style control for chinese handwriting generation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight cross-attention-based HookNet for historical handwritten document layout analysis. <em>IJDAR</em>, <em>28</em>(3), 409-427. (<a href='https://doi.org/10.1007/s10032-025-00519-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten document layout analysis is a fundamental step in digitizing scanned ancient documents for further processing (e.g., optical character recognition). So far, single branch-based fully convolutional networks (FCN) dominate this field. However, we contend that this task faces significant challenges, particularly in layouts with only semantic differences rather than differences in character appearance. For example, in the U-DIADS-Bib dataset, distinguishing between the main text and chapter headings can confuse existing FCNs due to the presence of similar distractors. It is, thus, critical to integrate layout structural information into the network learning processes. Moreover, the single branch-based networks have an upper limit of constructing document contextual relationships. Therefore, we propose a novel two-branch framework, called lightweight cross-attention-based HookNet (Light-HookNet), for handwritten document layout segmentation. The layout contextual information is connected and interacted with the cross-attention mechanism between a global context branch and a local target branch. This allows to achieve information enhancement inside the target branch and information exchange across both branches. Additionally, the reduced network parameters and computational costs make the proposed method both lightweight and efficient. Extensive experimental results and performance comparisons with state-of-the-art approaches on the newly proposed U-DIADS-Bib dataset and the popular DIVA-HisDB dataset demonstrate the superiority and effectiveness of the proposed method.},
  archive      = {J_IJDAR},
  author       = {Wu, Fei and Seuret, Mathias and Mayr, Martin and Kordon, Florian and Zöllner, Jochen and Wind, Sebastian and Maier, Andreas and Christlein, Vincent},
  doi          = {10.1007/s10032-025-00519-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {409-427},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Lightweight cross-attention-based HookNet for historical handwritten document layout analysis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A low-intervention dual-loop iterative process for efficient dataset expansion and classification in palm leaf manuscript analysis. <em>IJDAR</em>, <em>28</em>(3), 391-408. (<a href='https://doi.org/10.1007/s10032-025-00532-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palm leaf manuscripts, celebrated for their historical and cultural significance, pose considerable challenges for digital analysis due to their intricate script structures and age-related degradation. Existing studies frequently highlight the difficulties in assembling realistic datasets and generating accurate ground truth labels, as interpreting ancient scripts requires extensive human expertise and substantial computational resources. To address these challenges, this paper proposes a low-intervention, dual-loop iterative framework designed to efficiently expand datasets and enhance glyph classification in palm leaf manuscript analysis. The framework comprises two primary stages: preprocessing and classification. In the preprocessing stage, state-of-the-art methods are employed for text-line detection, glyph extraction, and synthetic data generation, significantly reducing reliance on manual annotation. The classification stage introduces tailored enhancements to vision transformers (ViTs), incorporating CNN-based feature extraction, Dynamic Stride Shift Patch Tokenization (DS-SPT), and Multi-Scale Locality Self-Attention (MS-LSA). These methods enhance the model’s flexibility and adaptability to the unique characteristics of palm leaf datasets. Moreover, the classification phase facilitates the generation of labels for newly extracted and generated datasets, employing an iterative process to progressively refine model performance. In our experiments, we evaluate the framework using both the ICFHR 2018 palm leaf collection and newly extracted datasets. The experimental results demonstrate improvements in classifying complex glyphs, providing a scalable and efficient solution for low-resource historical document analysis. This framework establishes a foundation for advanced research in the preservation and study of ancient scripts, enabling long-term accessibility and conservation of these cultural heritage documents with minimal human intervention.},
  archive      = {J_IJDAR},
  author       = {Thuon, Nimol and Du, Jun and Theang, Panhapin and Thuon, Ratana},
  doi          = {10.1007/s10032-025-00532-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {391-408},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A low-intervention dual-loop iterative process for efficient dataset expansion and classification in palm leaf manuscript analysis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The PARES database: Information extraction over historical parish records. <em>IJDAR</em>, <em>28</em>(3), 377-389. (<a href='https://doi.org/10.1007/s10032-025-00531-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historical census records convey information that is key to perform genealogical research and demographic studies. Given the large number of documents of this type that exist, it is crucial to research methods that allow the automatic extraction of information from this type of document. In this work, we present a new corpus of this kind, comprising 535 historical census tables from French archives. Alongside this dataset, we have assessed three different baseline methods for information extraction. The first two methods employ a traditional sequential approach, where table rows are detected before extracting information. The third baseline uses an end-to-end model that directly extracts information from the table images without prior row detection. Our results demonstrate the effectiveness of all three baselines in tackling the information extraction task.},
  archive      = {J_IJDAR},
  author       = {Andrés, José and Wall, Casey and Tarride, Solène and Coustaty, Mickaël and Toselli, Alejandro H. and Vidal, Enrique},
  doi          = {10.1007/s10032-025-00531-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {377-389},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {The PARES database: Information extraction over historical parish records},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tabular context-aware optical character recognition and tabular data reconstruction for historical records. <em>IJDAR</em>, <em>28</em>(3), 357-376. (<a href='https://doi.org/10.1007/s10032-025-00543-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digitizing historical tabular records is essential for preserving and analyzing valuable data across various fields, but it presents challenges due to complex layouts, mixed text types, and degraded document quality. This paper introduces a comprehensive framework to address these issues through three key contributions. First, it presents UoS_Data_Rescue, a novel dataset of 1,113 historical logbooks with over 594,000 annotated text cells, designed to handle the complexities of handwritten entries, aging artifacts, and intricate layouts. Second, it proposes a novel context-aware text extraction approach (TrOCR-ctx) to reduce cascading errors during table digitization. Third, it proposes an enhanced end-to-end OCR pipeline that integrates TrOCR-ctx with ByT5, combining OCR and post-OCR correction in a unified training framework. This framework enables the system to produce both the raw OCR output and a corrected version in a single pass, improving recognition accuracy, particularly for multilingual and degraded text, within complex table digitization tasks. The model achieves superior performance with a 0.049 word error rate and a 0.035 character error rate, outperforming existing methods by up to 41% in OCR tasks and 10.74% in table reconstruction tasks. This framework offers a robust solution for large-scale digitization of tabular documents, extending its applications beyond climate records to other domains requiring structured document preservation. The dataset and implementation are available as open-source resources.},
  archive      = {J_IJDAR},
  author       = {Singh, Loitongbam Gyanendro and Middleton, Stuart E.},
  doi          = {10.1007/s10032-025-00543-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {357-376},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Tabular context-aware optical character recognition and tabular data reconstruction for historical records},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Character recognition for greek squeezes. <em>IJDAR</em>, <em>28</em>(3), 345-356. (<a href='https://doi.org/10.1007/s10032-025-00540-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Squeezes are three-dimensional paper impressions made of stone inscriptions, a form of historical document where digital processing methods have received relatively little study to date. This paper reports on experiments in text recognition, working with a collection of approximately 30,000 squeezes originally collected from museums and archaeological sites in classical Greece. It explores a number of complementary strategies for character recognition in this medium, with the aim of establishing a strong benchmark in performance. Based on these experiments, we identify a recognition pipeline based on scene text recognition algorithms combined with line-based recognition that achieves a character error rate around 13%.},
  archive      = {J_IJDAR},
  author       = {Howe, Nicholas R. and Chang, Feiran and Falbo, Isabella and Brown, Tajhini and Hershkowitz, Aaron},
  doi          = {10.1007/s10032-025-00540-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {345-356},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Character recognition for greek squeezes},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On self-supervision in historical handwritten document segmentation. <em>IJDAR</em>, <em>28</em>(3), 329-344. (<a href='https://doi.org/10.1007/s10032-025-00538-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historical document analysis plays a crucial role in understanding and preserving our past. However, this task is often hindered by challenges such as limited annotated training data and the diverse nature of historical handwritten documents. In this paper, we explore the potential of self-supervised learning (SSL) in historical document analysis, with a particular focus on historical handwritten document segmentation, to overcome the need for extensive annotated data while enhancing efficiency and robustness. We present an overview of SSL methods suitable for historical document analysis and discuss their potential applications and benefits. Furthermore, we present an approach for SSL in the document domain, considering various setups, augmentations, and resolutions. We also provide experimental results that demonstrate its feasibility and effectiveness. Our findings indicate that most document segmentation tasks can be effectively addressed using SSL features, highlighting the potential of SSL to advance historical document analysis and pave the way for more efficient and robust document processing workflows.},
  archive      = {J_IJDAR},
  author       = {Baloun, Josef and Prantl, Martin and Lenc, Ladislav and Martínek, Jiří and Král, Pavel},
  doi          = {10.1007/s10032-025-00538-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {329-344},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {On self-supervision in historical handwritten document segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on advanced topics in document analysis (2025 ICDAR-IJDAR journal track). <em>IJDAR</em>, <em>28</em>(3), 327-328. (<a href='https://doi.org/10.1007/s10032-025-00551-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDAR},
  author       = {Lopresti, Daniel and Karatzas, Dimosthenis and Yin, Xu-Cheng},
  doi          = {10.1007/s10032-025-00551-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {327-328},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Special issue on advanced topics in document analysis (2025 ICDAR-IJDAR journal track)},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijfs">IJFS - 21</h2>
<ul>
<li><details>
<summary>
(2025). Retraction note: UPSO-FSVRNET: Fuzzy identification approach in a VANET environment based on fuzzy support vector regression and unified particle swarm optimization. <em>IJFS</em>, <em>27</em>(6), 1998. (<a href='https://doi.org/10.1007/s40815-025-02112-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Sellami, Lamaa and Alaya, Bechir},
  doi          = {10.1007/s40815-025-02112-y},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1998},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: UPSO-FSVRNET: Fuzzy identification approach in a VANET environment based on fuzzy support vector regression and unified particle swarm optimization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bipolar linguistic q-rung orthopair fuzzy modeling for sustainable tourism development and decision-making in tourism cities. <em>IJFS</em>, <em>27</em>(6), 1969-1997. (<a href='https://doi.org/10.1007/s40815-024-01886-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the dynamic landscape of tourism development, competitiveness hinges on the continuous progress of visitor destinations. This manuscript intricately navigates the dynamics of tourism, directing efforts toward the sustainable development and meticulous planning of tourism cities tailored to accommodate various forms of tourism. Within the inherent uncertainty of these dynamic scenarios, the theory of linguistic q-rung orthopair fuzzy sets emerges as a potent tool, surpassing conventional fuzzy set theories. This manuscript extends the utility of linguistic q-rung orthopair fuzzy sets by introducing the concept of bipolarity, giving rise to the innovative bipolar linguistic q-rung orthopair fuzzy model. The model seamlessly integrates bipolar linguistic q-rung orthopair fuzzy sets, providing a versatile extension of existing fuzzy set theories and enabling the effective handling of more generalized information. Precision in decision-making is heightened through the incorporation of negative linguistic-valued membership degrees, adeptly addressing preferences among parameters. Furthermore, the manuscript introduces aggregation operators, including bipolar linguistic q-rung orthopair fuzzy weighted averaging and geometric operators, delving into their properties and presenting impactful results. Validation of the proposed model is achieved through a rigorously tested multi-attribute decision-making algorithm applied to sustainable development and planning in tourism cities. A comprehensive comparative analysis with existing approaches meticulously showcases the advantages of the introduced model in effectively addressing challenges posed by the development of touristic cities. The results unequivocally affirm the superiority of the proposed bipolar linguistic q-rung orthopair fuzzy approach, endowing decision-makers with an enhanced capacity to navigate a diverse array of situations effectively. This scholarly contribution not only enhances tourism cities tailored to different forms of tourism but also catalyzes the sustainable development of tourism cities.},
  archive      = {J_IJFS},
  author       = {Yiarayong, Pairote},
  doi          = {10.1007/s40815-024-01886-x},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1969-1997},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Bipolar linguistic q-rung orthopair fuzzy modeling for sustainable tourism development and decision-making in tourism cities},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive robust control for concurrent combined disturbances of hypersonic morphing vehicle with optimal fuzzy approximation. <em>IJFS</em>, <em>27</em>(6), 1951-1968. (<a href='https://doi.org/10.1007/s40815-024-01885-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper designs a robust control scheme with optimal adaptive fuzzy approximation and variable structure nonlinear dynamic inversion (VSNDI) to solve the problems of multi-mode variable structure flight and unknown combined concurrent disturbances of hypersonic morphing vehicle (HMV). First, an VSNDI controller is proposed in the affine HMV. Then fuzzy approximation estimates the nonlinear parameters, where the iterative optimization strategy minimizes the estimation errors and makes the nonlinear parameters accurate enough to ensure the VSNDI’s effectiveness. Aiming at the combination disturbances formed by the approximation errors and external multi-disturbances, the variable structure adaptive law in each channel is proposed to adjust the amplitudes of the fuzzy free parameter weights, thence combining the independently designed fuzzy robust compensator to real-time suppress disturbances. The variable structure residual adaptive laws for cross-channel nonlinear matrices in VSNDI solve the coupling problem of the combined concurrent disturbances caused by sea-skimming flight. Finally, variable structure and fuzzy control scheme enable HMV to achieve accurate tracking under multi-mode and complex disturbances. The system stability is proved by Lyapunov theory, and the effectiveness of the method is proved by simulation experiment.},
  archive      = {J_IJFS},
  author       = {Hu, Kaiyu and Cheng, Yuqing and An, Pengfei},
  doi          = {10.1007/s40815-024-01885-y},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1951-1968},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Adaptive robust control for concurrent combined disturbances of hypersonic morphing vehicle with optimal fuzzy approximation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy dual closed-loop control scheme for precision stabilized platform. <em>IJFS</em>, <em>27</em>(6), 1935-1950. (<a href='https://doi.org/10.1007/s40815-024-01882-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel dual closed-loop control method based on fractional-order fuzzy sliding mode control and fractional-order active disturbance rejection control (FOFSMCFOADRC) is proposed for high-accuracy photoelectric tracking system subjected to inner uncertainties and external disturbances. First, in the inner loop, fractional-order fuzzy sliding mode control (FOFSMC) offers new flexibility and additional degrees of freedom, enabling the achievement of desired tracking performance. In addition, only a nominal model of the plant is required instead of a precise model in the practical control process. Second, in the outer loop, fractional-order active disturbance rejection control (FOADRC) is employed to decrease the adverse influence of disturbances on the platform. Furthermore, the stability of the proposed method is analyzed and guaranteed. Finally, a number of comparative simulations and real-world experimental validations are conducted to demonstrate the effectiveness and superiority of the proposed method in a comprehensive fashion.},
  archive      = {J_IJFS},
  author       = {Zhao, Jianjian and Zhao, Tao and Nie, Kang and Mao, Yao},
  doi          = {10.1007/s40815-024-01882-1},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1935-1950},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A fuzzy dual closed-loop control scheme for precision stabilized platform},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative EV charging optimization: Fuzzy DSS algorithm unveils benefits. <em>IJFS</em>, <em>27</em>(6), 1924-1934. (<a href='https://doi.org/10.1007/s40815-024-01881-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric vehicles (EVs) have become a crucial option for reducing greenhouse gas emissions by decreasing the reliance on fossil fuels and promoting the use of renewable energy. However, many research efforts have overlooked the detailed analysis of EV characteristics and charging networks, leading to scalability issues, inefficiencies in the charging process, and increased energy consumption. To address these challenges, this study introduces the Fuzzy Discrete Squirrel Search (FDSS) algorithm, which is designed to enhance charging capacity and reduce pollution. While EV manufacturing and network simulation models continue to evolve, there is a need for further optimization. This study summarizes key research findings related to EVs, including their market penetration, design methodologies, and innovative technologies. The primary objective of this work is to uncover the hidden benefits of EV charging characteristics using the Discrete Squirrel Search (DSS) optimization algorithm in combination with a fuzzy logic controller. The FDSS algorithm has been selected to improve performance and effectively identify the latent advantages of EV charging. In developing countries, this research is particularly significant, as it addresses critical obstacles such as inadequate charging infrastructure. Additionally, the concept of “vehicle-to-grid” provides a backup power source when renewable energy is unavailable. Our conclusion emphasizes the importance of recognizing the unique characteristics of EVs to enhance their mobility and overall effectiveness.},
  archive      = {J_IJFS},
  author       = {Wang, Zhiqiang and Raj, P. Justin and Babu, B. Ravindra and Ramaiah, Gurumurthy B.},
  doi          = {10.1007/s40815-024-01881-2},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1924-1934},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Innovative EV charging optimization: Fuzzy DSS algorithm unveils benefits},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Master–Slave synchronization for fuzzy markovian jump complex dynamical networks with coupling delay via fault-tolerant control. <em>IJFS</em>, <em>27</em>(6), 1904-1923. (<a href='https://doi.org/10.1007/s40815-024-01880-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the challenge of synchronizing master and slave systems in complex dynamic networks using Takagi-Sugeno (T-S) fuzzy Markovian jump models, with the presence of coupling delays. To enhance synchronization robustness, fault-tolerant control mechanisms are implemented. State feedback controller is converted to fault-tolerant control and control gain matrices are derived to improve system stability in the presence of faults. Using Lyapunov-krasovskii functional and Linear Matrix Inequalities (LMIs), this study establishes precise stability proofs and analytical constraints to ensure stochastic mean-square stability. The use of LMIs enables the systematic design of fault-tolerant controllers and provides a formal framework for handling faults while maintaining system performance. Moreover, numerical simulations using a MATLAB LMI toolbox validate the effectiveness of the proposed fault-tolerant control strategy under various fault scenarios, highlighting its practical applicability in complex dynamical networks.},
  archive      = {J_IJFS},
  author       = {Brundhashree, G. and Shanmugam, Saravanan and Magudeeswaran, S. and Vadivel, R. and Gunasekaran, Nallappan and Rhaima, Mohamed},
  doi          = {10.1007/s40815-024-01880-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1904-1923},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Master–Slave synchronization for fuzzy markovian jump complex dynamical networks with coupling delay via fault-tolerant control},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new fusion method of fuzzy numbers and linguistic terms based on individual semantics in mixed decision making problems. <em>IJFS</em>, <em>27</em>(6), 1887-1903. (<a href='https://doi.org/10.1007/s40815-024-01879-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because several groups of decision makers participate in modern fuzzy multiple attribute decision making problems, membership functions and linguistic terms may be simultaneously utilized by decision makers to represent assessments of alternatives. Due to linguistic terms mean different things to different decision makers in natural language, unifying membership functions and linguistic terms is a difficult problem in assessments aggregation process. Based on conceptual structure of Zadeh’s computing with words and decision makers’ individual semantics of linguistic terms, the formal linguistic concepts of linguistic terms in fuzzy decision environment are proposed in the paper, which provide a framework to uniformly represent various fuzzy assessments of alternatives. Then the representatives of extensions of formal linguistic concepts are generated by aggregating decision makers’ individual semantics of linguistic terms, which can be regarded as decision makers’ common recognition on meanings of linguistic terms. In essence, computing with linguistic terms in the decision process is to compute meanings of linguistic terms, which depends on precisiation of meanings of linguistic terms. However, precisiation of meaning is an open problem in Zadeh’s computing with words, the representatives of meanings of linguistic terms are reasonable substitutions to correctly understand and compute linguistic terms of mixed decision making problems. Meanwhile, the formal linguistic concepts of linguistic terms also present a new transformation method between membership functions and linguistic terms, which can be utilized to transform a mixed decision matrix into a linguistic decision matrix or a fuzzy decision matrix. Then the mixed decision making problems can be managed by existing linguistic or fuzzy number decision making methods. Based on the designed algorithm of the mixed decision analysis, a mixed decision making problem is employed to verify the effectiveness and usefulness of the proposed methods, furthermore our method in mixed decision making problem is compared with the 2-tuple linguistic decision making method and the fuzzy number decision making method.},
  archive      = {J_IJFS},
  author       = {Ren, Fangling and Hao, Fei},
  doi          = {10.1007/s40815-024-01879-w},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1887-1903},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A new fusion method of fuzzy numbers and linguistic terms based on individual semantics in mixed decision making problems},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated white matter lesions segmentation of MRIs for multiple sclerosis detection using fuzzy-entropy algorithm. <em>IJFS</em>, <em>27</em>(6), 1875-1886. (<a href='https://doi.org/10.1007/s40815-024-01878-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain magnetic resonance imaging (MRI) scans of patients with multiple sclerosis (MS) are analyzed in this study. Using the fuzzy-entropy algorithm, areas of damaged nerve cells, called lesions, are identified and extracted from the MRI scans. The effectiveness of the proposed method is assessed relative to state-of-the-art image segmentation methods, including improved k-means clustering, fast k-means clustering, modified fuzzy c-means, and adaptive fuzzy clustering. Performance evaluation metrics such as the mean-squared error, peak signal-to-noise ratio, Dice similarity coefficient, Jaccard similarity coefficient, and correlation coefficient demonstrate thffectiveness of the proposed method for segmenting MS lesions from brain MRIs.},
  archive      = {J_IJFS},
  author       = {Muchahari, Monoj Kumar and Singh, Pritpal and Das, Shirsendu},
  doi          = {10.1007/s40815-024-01878-x},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1875-1886},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Automated white matter lesions segmentation of MRIs for multiple sclerosis detection using fuzzy-entropy algorithm},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel convexification method for control synthesis analysis of continuous-time saturated positive polynomial fuzzy systems under imperfect premise matching. <em>IJFS</em>, <em>27</em>(6), 1859-1874. (<a href='https://doi.org/10.1007/s40815-024-01877-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the polynomial fuzzy controller design methodology for continuous-time positive polynomial fuzzy (PPF) systems to enlarge their feasible area. An improved Chebyshev membership function-dependent (MFD) convexification method is proposed to handle the nonconvex stability conditions and positivity conditions. Compared with the previous MFD convexification method, this improved convexification method removes the positive restriction imposed on the slack matrices, so it reduces the conservatism of the analysis results and helps to expand the system feasible area. In addition, this improved convexification method can also be extended to the saturated PPF systems (SPPF), and together with the convexification method proposed for the nonconvex estimation conditions of the polyhedron domain of attraction (DOA), the more relaxed results can be derived to obtain a larger estimation of DOA. In both cases, without and with input saturation constraints, a numerical example is used to illustrate the usefulness of the proposed controller design methodology and novel convexification methods in enlarging the feasible area and estimation of DOA.},
  archive      = {J_IJFS},
  author       = {Han, Meng and Huang, Yongjie and Guo, Ge and Lam, H. K. and Wang, Zhengsong and Sun, Liangliang},
  doi          = {10.1007/s40815-024-01877-y},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1859-1874},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A novel convexification method for control synthesis analysis of continuous-time saturated positive polynomial fuzzy systems under imperfect premise matching},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced IVIFN–ExpTODIM–MABAC technique for multi-attribute group decision-making and applications to college english teaching quality evaluation under interval-valued intuitionistic fuzzy sets. <em>IJFS</em>, <em>27</em>(6), 1846-1858. (<a href='https://doi.org/10.1007/s40815-024-01876-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of information technology, Massive Open Online Course (MOOC) has had a profound impact on the traditional college English teaching model with its rich learning content, innovative learning forms, and open learning resources. However, it itself has the shortcomings of an incomplete management system, difficulty in achieving credit recognition, serious resource waste, and a vague platform development path. Therefore, adopting a simple replacement approach is not feasible. Integrating traditional teaching models with MOOC teaching models is essential. By developing a distinctive English curriculum system, utilizing flipped classroom teaching methods, adapting to information-based online teaching models, and fostering positive learning concepts, MOOC can fully leverage its advantages under the guidance of traditional teaching models. This integration allows both models to complement each other’s strengths and weaknesses, thereby enhancing the reform of college English curriculum teaching. The evaluation of college English teaching quality based on MOOC involves Multi-Attribute Group Decision Making (MAGDM). Currently, methods such as Exponential TODIM (ExpTODIM) and MABAC are employed to implement MAGDM. Interval-valued intuitionistic fuzzy sets (IVIFSs) are used to represent fuzzy information during the evaluation of college English teaching quality based on MOOC. This study introduces the interval-valued intuitionistic fuzzy ExpTODIM–MABAC (IVIFN–ExpTODIM–MABAC) approach for MAGDM under IVIFSs. Finally, a numerical example for college English teaching quality evaluation based on MOOC is provided to validate the IVIFN–ExpTODIM–MABAC approach. The major contributions of this study are as follows: (1) The ExpTODIM–MABAC method was extended to incorporate IVIFSs along with Entropy model; (2) Entropy is used to determine the weights under IVIFSs; (3) The IVIFN–ExpTODIM–MABAC approach is proposed for MAGDM under IVIFSs; (4) numerical example and various comparative analyses for college English teaching quality evaluation based on MOOC are conducted to validate the IVIFN–ExpTODIM–MABAC approach.},
  archive      = {J_IJFS},
  author       = {Yang, Guojing},
  doi          = {10.1007/s40815-024-01876-z},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1846-1858},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Enhanced IVIFN–ExpTODIM–MABAC technique for multi-attribute group decision-making and applications to college english teaching quality evaluation under interval-valued intuitionistic fuzzy sets},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach for optimizing mixed hazardous material fleet vehicle routing problem method through intermediate bulk container sharing with adaptive intuitionistic fuzzy large neighborhood search. <em>IJFS</em>, <em>27</em>(6), 1827-1845. (<a href='https://doi.org/10.1007/s40815-024-01875-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the mixed fleet Vehicle Routing Problem (MFVRP) for transporting hazardous material (hazmat) goods in port hinterlands, aiming to balance transportation efficiency and sustainability. It refines transportation routes through intermediate bulk containers (IBCs) sharing, incorporating both fuel vehicles (FVs) and electric vehicles (EVs). The main contribution of this study is as follows. First, a mixed fleet strategy is introduced, integrating fuel hazmat vehicles (FHVs) and electric hazmat vehicles (EHVs) to boost efficiency and cut emissions. Second, this study develops a bi-objective Mixed-Integer Nonlinear Programming (MINLP) model with fuzzy time windows for flexible scheduling. Third, an adaptive intuitionistic fuzzy large neighborhood search (AIFLNS) algorithm is proposed, using intuitionistic fuzzy sets (IFSs) to refine scoring mechanisms and operation selection. In this way, this study not only enhances the efficiency of hazmat transportation, but also promotes green logistics in port hinterlands. Finally, experimental results show that the AIFLNS reduces transportation costs and enhances service satisfaction, outperforming adaptive large neighborhood search (ALNS), and ant colony optimization (ACO) algorithms.},
  archive      = {J_IJFS},
  author       = {Wang, Rui and Zhang, Fangwei and Ding, Lu and Jiang, Jun and Han, Zhu},
  doi          = {10.1007/s40815-024-01875-0},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1827-1845},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A novel approach for optimizing mixed hazardous material fleet vehicle routing problem method through intermediate bulk container sharing with adaptive intuitionistic fuzzy large neighborhood search},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The $$\alpha $$ -core for fuzzy games without the compact assumptions. <em>IJFS</em>, <em>27</em>(6), 1814-1826. (<a href='https://doi.org/10.1007/s40815-024-01874-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses significant gaps in the study of fuzzy games by exploring the existence (nonemptiness) and essential stability of the $$\alpha $$ -core for fuzzy games without assuming compactness. We demonstrate the nonemptiness of the $$\alpha $$ -core for fuzzy games under relaxed conditions, thereby extending the results established by Yang and Wang (2018) and highlighting the limitations of previous frameworks that depend on compactness. Additionally, we construct two classes of fuzzy game spaces to illustrate that most of these games are essential (or weakly essential) through set-valued analysis. Furthermore, we provide an example showing that not all fuzzy games with noncompact conditions guarantee an essential equilibrium point, emphasizing the complexity of essential equilibrium existence. Finally, we investigate the existence of essential components of the $$\alpha $$ -core within fuzzy games, affirming their existence even in the absence of compactness assumptions.},
  archive      = {J_IJFS},
  author       = {Li, Yunbing and Jia, Wensheng and Feng, Xudong},
  doi          = {10.1007/s40815-024-01874-1},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1814-1826},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {The $$\alpha $$ -core for fuzzy games without the compact assumptions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpolation functions of general type-2 fuzzy systems. <em>IJFS</em>, <em>27</em>(6), 1797-1813. (<a href='https://doi.org/10.1007/s40815-024-01872-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on simplifying the use procedure and calculation of general type-2 fuzzy systems by way of extracting their the interpolation functions. Firstly, four kinds of fuzzification methods with special laws are designed to construct the general type-2 fuzzy sets in the antecedents and consequents of type-2 inference rules. On this basis, together with the KM algorithm, $$\alpha $$ -plane representation and interpolation conclusions of interval type-2 fuzzy systems, three types of interpolation functions of general type-2 fuzzy systems are obtained. In the meantime, all of the interpolation functions have been proved as universal approximators. It can be seen that in future applications of general type-2 fuzzy systems, interpolation functions can be applied directly instead of conventional blocks. Because of the ingenious design of the general type-2 fuzzy sets on operations, the computation of general type-2 fuzzy systems has been greatly reduced. In order to verify the validity and superiority of the proposed methods, simulation results with a type-1 fuzzy system, an interval type-2 fuzzy system and two general type-2 fuzzy systems for the approximation problem of dynamic systems are presented. The simulations exhibit that the suggested approaches have good and desired performance.},
  archive      = {J_IJFS},
  author       = {Zhao, Shan and Shi, Kaibo},
  doi          = {10.1007/s40815-024-01872-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1797-1813},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Interpolation functions of general type-2 fuzzy systems},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive benefit evaluation of smart substation construction projects: A hybrid multi-criteria decision-making method for hybrid information. <em>IJFS</em>, <em>27</em>(6), 1771-1796. (<a href='https://doi.org/10.1007/s40815-024-01871-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The smart substations is the core part of smart grids which affect the development of the economy and people’s livelihood. This paper constructs a comprehensive benefit evaluation index system of smart substation construction project from four dimensions: intelligent benefit, economic benefit, green benefit, and safe benefit to meet the requirements of national development. The attribute value of the evaluation index is hybrid information including language variable, exact number, and normal distribution interval number, which can handle the complex sources of information. On this basis, this study constructs a hybrid multi-criteria decision-making model for hybrid information based on intuitionistic fuzzy theory, subjective and objective weighting method, and improved TOPSIS evaluation method. Firstly, the model uses different transformation rules to unify the hybrid information into the form of intuitionistic fuzzy numbers based on intuitionistic fuzzy theory. Secondly, based on the cooperative game model, this paper uses a weighting method which combines subjective weighting method (group eigenvalue method) and objective weighting method (intuitionistic fuzzy entropy) to determine the comprehensive weight of indicators, which can avoid the limitations of using single weighting method and makes the weights more scientific and reasonable. Thirdly, an improved TOPSIS evaluation model based on intuitionistic fuzzy sets is proposed. The Euclidean distance in traditional TOPSIS method is replaced by cosine similarity, which overcomes the shortcoming of using Euclidean distance. Finally, this paper analyzes 10 cases of smart substation construction projects and puts forward improvement strategies based on the results, the results prove that the model is feasible and it can assist decision makers in making scientific decisions based on different situations. In addition, this hybrid multi-criteria decision-making model is compared with three other models, which proves that the proposed model has good robustness and sensitivity.},
  archive      = {J_IJFS},
  author       = {Yang, Dianqing and Mao, Wenjie and Ye, Kunfeng},
  doi          = {10.1007/s40815-024-01871-4},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1771-1796},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Comprehensive benefit evaluation of smart substation construction projects: A hybrid multi-criteria decision-making method for hybrid information},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An extension of the CODAS method using $$\textbf{p,q}$$ -quasirung orthopair fuzzy information: Application in location selection for retail store. <em>IJFS</em>, <em>27</em>(6), 1754-1770. (<a href='https://doi.org/10.1007/s40815-024-01870-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $$p,q-$$ quasirung orthopair fuzzy sets represent an innovative advancement in fuzzy set theory, addressing the constraints inherent in $$q-$$ rung orthopair fuzzy sets. This new extension has gained considerable traction in multi-criteria group decision-making (MCGDM) scenarios. The Combinative Distance-based Assessment (CODAS) method is designed to enhance the multi-criteria selection process by maximizing both Euclidean and Taxicab distances relative to negative ideal solutions. In the literature, different extensions are proposed. However, these extensions are unable to address situations where decision makers may employ varying term levels for membership degrees. To bridge this gap, this paper enhances the traditional CODAS method by incorporating $$p,q-$$ quasirung orthopair fuzzy ( $$p,q-$$ QOF) numbers to effectively solve multi-criteria group decision-making (MCGDM) problems with incomplete weight information. The proposed CODAS approach enables decision makers to set distinct term levels (p and q) for membership and non-membership, enhancing evaluation precision. To illustrate the application of the proposed approach, a numerical example pertaining to retail location selection is presented. The comparative study demonstrates the superiority of the proposed approach over several relevant methods. In addition, a sensitivity analysis of the proposed CODAS method, conducted by varying the criteria weights, reveals a high degree of stability in the results.},
  archive      = {J_IJFS},
  author       = {Alballa, Tmader and Rahim, Muhammad and Aloraini, Najla M. and Khalifa, Hamiden Abd El-Wahed},
  doi          = {10.1007/s40815-024-01870-5},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1754-1770},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {An extension of the CODAS method using $$\textbf{p,q}$$ -quasirung orthopair fuzzy information: Application in location selection for retail store},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-triggered fuzzy adaptive asymptotic consensus control of nonlinear multi-agent systems against FDI attacks. <em>IJFS</em>, <em>27</em>(6), 1737-1753. (<a href='https://doi.org/10.1007/s40815-024-01869-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the issue of resilient consensus control for nonlinear multi-agent systems in the case of false data injection (FDI) attacks on actuators and sensors. Compared with existing results, this article investigates a category of nonlinear strict-feedback systems, and real system states as well as real control inputs is not available under FDI attacks, which renders the controller design extremely complicated. To counteract the impact caused by sensor attacks, a fuzzy adaptive defense mechanism is constructed by the variable separation technique. Furthermore, actuator attacks are addressed by the Nussbaum function. In order to decrease the communication load on the control channel, a dynamic event-triggered strategy is introduced. Moreover, all signals of the closed-loop systems under the proposed control method are bounded, and output consensus with asymptotic convergence is realized in the case of FDI attacks. Finally, simulation studies are offered to assess the feasibility of the presented control method.},
  archive      = {J_IJFS},
  author       = {Wang, Jilei and Wang, Wei and Yu, Yang},
  doi          = {10.1007/s40815-024-01869-y},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1737-1753},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Event-triggered fuzzy adaptive asymptotic consensus control of nonlinear multi-agent systems against FDI attacks},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex intuitionistic fuzzy trust propagation-based bilayer coupled social network group consensus model with opinion evolution. <em>IJFS</em>, <em>27</em>(6), 1712-1736. (<a href='https://doi.org/10.1007/s40815-024-01868-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social network group decision-making (SNGDM), it is difficult for decision-makers (DMs) to reach a consensus owing to individuals’ opinions being influenced by others and evolving over time. Moreover, the existing social trust functions are incapable for depicting uncertainty and interference effect of trusts simultaneously, which are driven by the unpredictable human cognition. To address these problems, this study proposes a novel complex intuitionistic fuzzy trust propagation-based group consensus model with opinion evolution under bilayer coupled social network. Firstly, the novel concept of complex intuitionistic fuzzy trust (CIFT) function is put forward to model the direct trust between DMs in SNGDM. Secondly, trust propagation operator on CIFTs is presented based on Einstein operations to derive the indirect trust between DMs. Then, a novel quantum weighted averaging trust aggregation operator considering interference effect is proposed to aggregate multiple trust propagation paths into a collective one. Thirdly, the propagation and aggregation of CIFTs under bilayer coupled social network are explored by obtaining the projection of the two-layer network. In addition, degree-centrality and clustering coefficient are further calculated to derive DMs’ weights. Fourthly, the network partition algorithm is conducted to divide the projection network into several sub-networks and a novel consensus optimization model with opinion evolution is developed to guide leaders on adjusting their opinions during consensus reaching process. Eventually, an illustrative case on joint venture capital is provided to show the feasibility of above methods. The superiority and rationality of our proposal are further revealed by a series of simulations and comparative analysis.},
  archive      = {J_IJFS},
  author       = {Liang, Yuanyuan and Ju, Yanbing and Zeng, Xiao-Jun and Xu, Yanxin and Ju, Tian and Dong, Peiwu},
  doi          = {10.1007/s40815-024-01868-z},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1712-1736},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Complex intuitionistic fuzzy trust propagation-based bilayer coupled social network group consensus model with opinion evolution},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph fuzzy attention network model for metastasis prediction of prostate cancer based on mRNA expression data. <em>IJFS</em>, <em>27</em>(6), 1702-1711. (<a href='https://doi.org/10.1007/s40815-024-01867-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prostate cancer (PCa) is a prevalent disease among men, with one in eight men experiencing PCa in their lifetime. Predicting the risk of developing metastasis in PCa patients is crucial for clinicians in determining appropriate treatment. This study aimed to identify significant genetic biomarkers for predicting PCa metastasis. The study’s method involved four steps: reading, preprocessing, feature selection, and classification. ANOVA and the ReliefF algorithm were used to select significant features (mRNAs). Additionally, a novel Graph Attention Network (GAT) model based on the Fuzzy theory was introduced, incorporating the Adaptive Neuro-Fuzzy Inference System (ANFIS) for the attention mechanism. The proposed Graph Fuzzy Attention Network (GFAT) model, as part of the proposed classifier, was utilized to classify two groups: PCa patients with and without metastasis. The results showed that the proposed approach achieved an accuracy of 74.4% and an AUC of 0.731, outperforming SVM and other recent studies.},
  archive      = {J_IJFS},
  author       = {Emdadi, Manijeh and Pedram, Mir Mohsen and Eshghi, Farshad and Mirzarezaee, Mitra},
  doi          = {10.1007/s40815-024-01867-0},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1702-1711},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Graph fuzzy attention network model for metastasis prediction of prostate cancer based on mRNA expression data},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of NMPC and fuzzy PID controllers for trajectory tracking in omni-drive robots: Design, simulation, and performance evaluation. <em>IJFS</em>, <em>27</em>(6), 1691-1701. (<a href='https://doi.org/10.1007/s40815-024-01866-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory tracking for an Omni-drive robot presents a challenging task that demands an efficient controller design. This paper introduces a self-optimizing controller, Type-1 fuzzyPID, which leverages dynamic and static system response analysis to overcome the limitations of manual tuning. To account for system uncertainties, an Interval Type-2 fuzzyPID controller is also developed. Both controllers are designed using Matlab/Simulink and tested through trajectory tracking simulations in the CoppeliaSim environment. In addition, a non-linear model predictive controller (NMPC) is proposed and compared against the fuzzyPID controllers. The impact of tunable parameters on NMPC’s tracking accuracy is thoroughly examined. We also present plots of the step-response characteristics and noise rejection experiments for each controller. Simulation results validate the precision and effectiveness of NMPC over fuzzyPID controllers while trading computational complexity. Access to code and simulation environment are available in the following link: https://github.com/love481/Omni-drive-robot-Simulation.git .},
  archive      = {J_IJFS},
  author       = {Panta, Love},
  doi          = {10.1007/s40815-024-01866-1},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1691-1701},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Comparative analysis of NMPC and fuzzy PID controllers for trajectory tracking in omni-drive robots: Design, simulation, and performance evaluation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-period portfolio optimization based on asymmetric credibilistic return-risk ratios with investors’ coherent perceptions. <em>IJFS</em>, <em>27</em>(6), 1670-1690. (<a href='https://doi.org/10.1007/s40815-024-01865-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce asymmetric CVaR ratios and Sharpe ratios of generalized coherent fuzzy numbers to evaluate asymmetric extreme risk-adjusted and volatility risk-adjusted returns, incorporating investors’ coherent perceptions. Furthermore, we construct a multi-period credibilistic portfolio selection model with asymmetric return-risk ratios. Data from China’s financial markets is used to demonstrate the effectiveness of the proposed portfolio optimization models. The performance of optimal portfolios differs with respect to investors’ coherent perceptions and outperforms the benchmark models. Rational investors assign equal importance to lower and upper return-risk ratios in the optimal portfolio, selecting a robust investment strategy in the first period. Optimistic investors prioritize upper ratios and choose an aggressive investment strategy while pessimistic investors prefer lower ratios and choose a conservative investment strategy. Due to the different sensitivity to risk, rational and optimistic investors reduce the proportions of high-risk assets, while pessimistic investors increase the proportions of high-risk assets during next periods. The findings demonstrate that the model offers diverse investment strategies and serves as a valuable reference for different categories of investors to engage in multi-period asset allocation and risk management.},
  archive      = {J_IJFS},
  author       = {Li, He and Jin, Xiu and Liu, Yueli},
  doi          = {10.1007/s40815-024-01865-2},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1670-1690},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Multi-period portfolio optimization based on asymmetric credibilistic return-risk ratios with investors’ coherent perceptions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval type-3 shadowed sets as an efficient way to approximate interval type-3 fuzzy systems. <em>IJFS</em>, <em>27</em>(6), 1651-1669. (<a href='https://doi.org/10.1007/s40815-024-01864-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, one of the most recent realms in Fuzzy Logic is the emergence of Type-3 Fuzzy Logic, with novel applications and potential to be explored. However, one of the biggest obstacles to realize the implementation of this kind of theory in real-world applications and systems is the computational complexity. The aim of this paper is to address this obstacle by proposing a new approach for modelling the Interval Type-3 Fuzzy Systems, in this case, based on Shadowed Sets, which have demonstrated to be a suitable approach for modelling Fuzzy Sets. In this case, Shadowed Sets theory is implemented in Interval Type-3 Fuzzy Sets to approximate the secondary membership functions, in this way reducing the number of $$\alpha $$ -planes to only two and contributing to the development of more efficient models based on Type-3 Fuzzy Logic. The obtained results with the proposed approximation are promising in terms of reducing the computational effort and offering at the same time good approximation of the original systems.},
  archive      = {J_IJFS},
  author       = {Ontiveros, Emanuel and Melin, Patricia and Castillo, Oscar},
  doi          = {10.1007/s40815-024-01864-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {9},
  number       = {6},
  pages        = {1651-1669},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Interval type-3 shadowed sets as an efficient way to approximate interval type-3 fuzzy systems},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijmir">IJMIR - 1</h2>
<ul>
<li><details>
<summary>
(2025). Ultra fast-inference depth completion with linear attention-based cascaded hourglass network. <em>IJMIR</em>, <em>14</em>(4), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00381-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion is vital for various CV applications such as autonomous driving, robotics, and augmented reality. However, most of the existing approaches suffer from high computational overhead and slow inference speeds, limiting their real-time applicability. In this paper, we present the Linear Attention-based Cascade Hourglass Network (LA-CHN), a lightweight yet robust depth completion model designed for efficient and accurate dense depth prediction. The core of LA-CHN is our Lightweight Linear Attention (LLA) block, which substitutes quadratic self-attention with a ReLU-kernel linear mechanism and a spatial-reduction strategy to maintain a global receptive field at linear cost. These LLA blocks are embedded within a three-stage cascaded hourglass backbone, enabling multiscale feature aggregation and progressive refinement. Experimental results on the outdoor KITTI DC benchmark and indoor NYUv2 dataset show that our approach achieves superior performance compared to previous lightweight depth completion models.},
  archive      = {J_IJMIR},
  author       = {Wu, Zirui and Hao, Yongtao},
  doi          = {10.1007/s13735-025-00381-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Ultra fast-inference depth completion with linear attention-based cascaded hourglass network},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijmlc">IJMLC - 50</h2>
<ul>
<li><details>
<summary>
(2025). Differential evolution-driven optimized ensemble network for brain tumor detection. <em>IJMLC</em>, <em>16</em>(9), 6447-6472. (<a href='https://doi.org/10.1007/s13042-025-02629-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors are serious and abnormal growth within the brain by posing significant challenges for medical diagnosis and treatment. The varied and complex nature of brain tumors complicates their detection and accurate classification. The need for precise classification of brain tumors from medical imaging is crucial for effective medical intervention. However, traditional methods are reliant on manual diagnosis, which are prone to human errors and inconsistencies. Previous research has struggled to enhance diagnostic precision by combining predictions from multiple models but often result in discrepancies and inconsistent outcomes by leading to complicated diagnostic processes. To address these challenges, researchers have increasingly turned to advanced computational methods to improve diagnostic accuracy. In this context, we propose a robust ensemble approach optimized using a Differential Evolution (DE)-based algorithm. Our method combines three high-performing pre-trained CNN models like MobileNetV1, MobileNetV2, ResNet50V2 and optimizes their contributions by assigning optimal weights through DE. This technique intelligently adjusts the weights allocation of models to maximize ensemble performance. During optimization, the probabilities from each model are extracted and integrated using a weighted average aggregation scheme, enhancing the diagnostic precision and overall predictive accuracy. To validate the effectiveness of our approach, we applied it to two publicly available datasets: a binary classification dataset (BR35H) and a multi-class (4-class) dataset. Through rigorous evaluations, our optimized ensemble approach demonstrated superior accuracy performance of 98% and 97.03%, respectively. We used several performance evaluation metrices and visualization techniques like Grad-CAM to highlight critical areas within the images. Additionally, statistical validation is conducted using the Friedman test followed by the Conover post hoc analysis to rigorously assess and compare the performance differences across models.},
  archive      = {J_IJMLC},
  author       = {Hekmat, Arash and Zuping, Zhang and Bilal, Omair and Khan, Saif Ur Rehman},
  doi          = {10.1007/s13042-025-02629-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6447-6472},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Differential evolution-driven optimized ensemble network for brain tumor detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layered security architecture for IoMT systems: Integrating dynamic key management, decentralized storage, and dependable intrusion detection framework. <em>IJMLC</em>, <em>16</em>(9), 6399-6446. (<a href='https://doi.org/10.1007/s13042-025-02628-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing complexity of cyber threats presents significant challenges to the security of Internet of Medical Things (IoMT) systems, where traditional security and intrusion detection methods often prove inadequate. The Key challenges include inefficient key management, fragmented security protocols, and limited scalability. To address these issues, this paper proposes a Dynamic Adaptive Deep Reinforcement Learning (DA-DRL) framework that enhances Advanced Encryption Standard (AES) encryption by dynamically adjusting key generation in response to real-time threats. Additionally, a multi-layered security architecture integrating AES, SHA-512, Non-Interactive Zero Knowledge Proof (NIZKPs), Practical Byzantine Fault Tolerance (PBFT), and Attribute-Based Access Control (ABAC) is introduced, ensuring robust protection against diverse attack vectors. The InterPlanetary File System (IPFS) is employed for decentralized and immutable data storage, enhancing data security and transparency. The proposed DA-DRL-AES-SHA-512 methodology significantly outperforms conventional encryption techniques, achieving an encryption time of 0.0975 s, decryption time of 0.0846 s, and a throughput of 75.63 transactions per second (Tx/s) with a network overhead of just 0.1289%. The Energy consumption and computational overhead are reduced to 0.3664 J and 0.48%, respectively. The Secure and Dependable Bi-LSTM GRU Intrusion Detection Framework (S-BiLSTMGRU-IDF) achieves 99.94% accuracy in binary classification and 99.89% in multiclass classification, improving detection efficiency by 0.6–3.5% over state-of-the-art models. This blockchain-based framework ensures real-time threat mitigation, enhanced data integrity, and superior system performance, establishing a secure, scalable, and efficient solution for IoMT security.},
  archive      = {J_IJMLC},
  author       = {Sharma, Nikhil and Shambharkar, Prashant Giridhar},
  doi          = {10.1007/s13042-025-02628-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6399-6446},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-layered security architecture for IoMT systems: Integrating dynamic key management, decentralized storage, and dependable intrusion detection framework},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A heterogeneous data fusion mortality prediction model based on time-aware self-attention mechanism. <em>IJMLC</em>, <em>16</em>(9), 6381-6397. (<a href='https://doi.org/10.1007/s13042-025-02627-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting mortality using medical information is a critical task in the healthcare profession. The common method is extracting features from electronic health records and choosing appropriate models to identify patients who may have a higher risk of death. However, the lack of standardization and identifiability in the storage of unstructured data in medical records poses a significant challenge in training models to predict patient mortality. In this paper, we propose a heterogeneous data fusion mortality prediction model called Fusion-transformer to address the above issues. The proposed Fusion-transformer model uses a time-aware self-attention mechanism to combine structured data represented by time signals and static information and unstructured data represented by clinical notes to enhance patient representation learning. Fusion-transformer also uses a time decay module to control the influence of the patient’s hospitalization experience on the current health status at different times. The results of in-hospital mortality prediction experiments on the Medical Information Mart for Intensive Care III dataset show that Fusion-transformer outperforms other machine learning and deep learning baseline models, improving the area under the receiver operating characteristics curve metric from 0.891 to 0.923. In addition, we explore the practical applications value of our method through real ICU patient electronic medical records.},
  archive      = {J_IJMLC},
  author       = {Chen, Shuxu and Wang, Decong and Che, Chao and Wei, Ziqi and Zhong, Zhaoqian},
  doi          = {10.1007/s13042-025-02627-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6381-6397},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A heterogeneous data fusion mortality prediction model based on time-aware self-attention mechanism},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constructing and learning heterogeneous patient graph representations from structured electronic medical records. <em>IJMLC</em>, <em>16</em>(9), 6367-6380. (<a href='https://doi.org/10.1007/s13042-025-02626-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph structure can reveal the relationships between feature nodes and improve the performance of feature-based models. However, more research is needed to construct a patient graph representation using electronic medical record (EMR) to meet modeling requirements. This study aims to propose a heterogeneous patient graph representation (HePGR) framework capable of discovering associations between medical concepts in EMR while simultaneously supporting both clustering and classification tasks. We construct HePGR’s edge connections by evaluating the correlations between medical concepts(e.g., laboratory tests, drugs, surgical codes) using positive pointwise mutual information, directly linking patients with their corresponding medical concepts. Graph attention networks are used to obtain patient node representations, with a supervised training method based on cross-entropy and a semi-supervised method leveraging pseudo-labeling and contrastive learning. To validate the effectiveness of the HePGR model, we design comparison and ablation experiments that are performed on a stroke patient dataset with two prediction tasks and one clustering task. HePGR shows superior performance in all tasks, achieving areas under the receiver operating characteristic curve of 0.990 and 0.806 in the two prediction tasks and a Jaccard coefficient of 0.810 in the clustering task. The proposed HePGR model effectively identifies associations between medical concepts and shows high performance in clinical tasks. This model is expected to be extended to more medical concepts for broad clinical applicability.},
  archive      = {J_IJMLC},
  author       = {Li, Yichen and Wang, Muyu and Gao, Binyu and Zhu, Congmin and Wei, Lan and Fei, Xiaolu and Chen, Hui},
  doi          = {10.1007/s13042-025-02626-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6367-6380},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Constructing and learning heterogeneous patient graph representations from structured electronic medical records},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal knowledge graph representation learning with temporal feature and complex evolution. <em>IJMLC</em>, <em>16</em>(9), 6347-6365. (<a href='https://doi.org/10.1007/s13042-025-02625-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal knowledge graph (TKG) representation learning is a pivotal task aimed at transforming entities and relations within TKG from a high-dimensional vector space to a lower-dimensional vector space, while preserving the relational features inherent in TKG. TKG comprises a sequence of knowledge graphs (KGs) at various timestamps. Presently, existing methodologies tend to either focus solely on learning historical event characteristics or exclusively model time-dependent relationships. There is a notable dearth of research concerning incomplete data, posing significant challenges to comprehending and capturing the intricate relationship characteristics within TKG. In response to this challenge, a novel method named TFCE is introduced to address the challenges posed by temporal evolution and incomplete data in TKGs. TFCE encompasses three core components: a Temporal Feature Module, a Complex Evolution Module, and a Temporally Embedded Decoder. TFCE incorporates a temporal feature module, enabling the temporal encoding of entities and relations within KGs. This module seamlessly integrates temporal information into the representation learning process. By discerning patterns of entities and relations across time, TFCE facilitates the comprehension and discovery of temporal order relations within KGs. The complex evolution module adeptly learns the evolutionary representation of entities and relationships at each timestamp through recursive modeling of the KG sequence. By systematically analyzing the KG sequence, this module captures the nuanced evolution of entities and relationships over time, enhancing the understanding of temporal dependencies between events. To accommodate incomplete temporal data, TFCE employs a temporally embedded decoder. This decoder effectively processes incomplete temporal data, facilitating the inference of representation learning. Experimental validation conducted across three real-world datasets, namely ICEWS14s, ICEWS 05-15, and ICEWS18, underscores the superiority of TFCE over baseline methods. The TFCE framework demonstrates remarkable efficacy in capturing temporal relationships within TKG, thus showcasing its potential for advancing temporal knowledge graph representation learning methodologies.},
  archive      = {J_IJMLC},
  author       = {Liu, Qian and Feng, Siling and Huang, Mengxing and Bhatti, Uzair Aslam},
  doi          = {10.1007/s13042-025-02625-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6347-6365},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Temporal knowledge graph representation learning with temporal feature and complex evolution},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented harris hawks optimization and for engineering design problems and UAV path planning. <em>IJMLC</em>, <em>16</em>(9), 6295-6345. (<a href='https://doi.org/10.1007/s13042-025-02624-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harris hawk optimization (HHO) is a popular metaheuristic algorithm recently proposed, but it suffers from slow convergence speed, low accuracy, and the problem of easily getting stuck in local optima when dealing with practical problems. To address these drawbacks, we propose an improved version called augmented Harris hawk optimization (AHHO). In the enhancement, we introduce Logistic chaotic population initialization to enhance AHHO’s global exploration capability; adopt a dynamic Lévy flight strategy to improve the algorithm’s early convergence speed; and propose a new surprise pounce exploitation strategy to enhance the algorithm’s optimization ability. Additionally, we introduce a random centroid dynamic backward learning to improve the algorithm’s search efficiency, convergence speed, and robustness, thereby effectively solving complex optimization problems. To validate the performance of AHHO, we conducted analyses from three aspects: population diversity, exploration and exploitation balance, and convergence behavior. We compared it with HHO and 12 other high-performance algorithms on the CEC2014 and CEC2017 test sets, in two test sets, 15, 17, 23, 14, 18, and 21 first-place rankings were obtained in three dimensions, respectively. The results show that compared to HHO and other algorithms, AHHO demonstrates superior comprehensive performance in all aspects. Finally, we applied AHHO to 5 engineering optimization problems and 1 unmanned aerial vehicle path planning problem, 5 engineering problems and UAVs both achieved first place rankings. The results indicate that AHHO outperforms other benchmark algorithms, demonstrating its strong scalability and outstanding optimization performance.},
  archive      = {J_IJMLC},
  author       = {Zhu, Lindan and Fu, Youfa},
  doi          = {10.1007/s13042-025-02624-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6295-6345},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Augmented harris hawks optimization and for engineering design problems and UAV path planning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loepso, local optimum escape particle swarm optimization, an algorithm for traffic forecasting in software-defined networking using deep-learning models. <em>IJMLC</em>, <em>16</em>(9), 6271-6294. (<a href='https://doi.org/10.1007/s13042-025-02623-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of computer networks has led to designing a more flexible and efficient architecture called Software-Defined Networking (SDN). SDN decouples the control plane from the physical networking devices in the data plane. SDN’s global perspective and programmability have inspired researchers to develop ideas that would have been difficult or even impossible in traditional networks. Traffic forecasting is an interesting area that has driven the efforts of this paper. In this study, an optimization of time series parameters is performed to achieve the most suitable time series structure for feeding deep-learning algorithms. Popular deep-learning algorithms, namely Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), are compared in traffic forecasting. An improved version of Particle Swarm Optimization (PSO), called Local Optimum Escape Particle Swarm Optimization (LOEPSO), is proposed. The algorithm is a variation of PSO inspired by Harmony Search optimization to replace the global worst particle with a randomly created particle. A Ryu controller has been customized to gather the network’s switches’ port statistics and generate a time series from traffic passing through the ports. A traffic generator application, implemented as a Python thread, runs at the SDN network’s hosts simulated in Mininet. For an extensive analysis, traffic is generated in two modes: an idle mode with regular traffic and the worst-case scenario where a chaotic function is used to determine traffic volume. The results show that CNN and LSTM provide reliable forecasting for regular traffic. However, despite having equivalent training results in chaotic traffic, LSTM performs better in forecasting validation data.},
  archive      = {J_IJMLC},
  author       = {Talarposhti, Khadijeh Mirzaei and Jabbehdari, Sam and Rahmani, Amir Masoud},
  doi          = {10.1007/s13042-025-02623-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6271-6294},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Loepso, local optimum escape particle swarm optimization, an algorithm for traffic forecasting in software-defined networking using deep-learning models},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reward design in multi-agent systems using successor features and multi-information source bayesian optimization. <em>IJMLC</em>, <em>16</em>(9), 6249-6270. (<a href='https://doi.org/10.1007/s13042-025-02622-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coordinating self-interested agents in multi-agent systems to achieve system-level objectives presents significant challenges due to the inherent misalignment between individual and collective goals. Mechanism design offers a solution by employing a bi-level optimization framework, where a designer agent intervenes in the reward structures to incentivize desired behaviors among self-interested agents. However, a major obstacle in reward optimization lies in solving multi-agent reinforcement learning problems given a reward structure. This paper addresses this challenge by introducing a novel algorithm that leverages successor features (SFs) at both levels of the optimization. Specifically, SFs help reduce the number of design iterations at the upper level by using previously learned equilibria as biased information sources and accelerate equilibrium learning at the lower level by transferring equilibria from previously solved Markov games. This innovative approach leads to significant computational savings, making the process up to ten times faster compared to traditional methods.},
  archive      = {J_IJMLC},
  author       = {Park, Kyeonghyeon and Molina Concha, David and Lee, Hyun-Rok and Lee, Taesik and Lee, Chi-Guhn},
  doi          = {10.1007/s13042-025-02622-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6249-6270},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Reward design in multi-agent systems using successor features and multi-information source bayesian optimization},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FFS-IML: Fusion-based statistical feature selection for machine learning-driven interpretability of chronic kidney disease. <em>IJMLC</em>, <em>16</em>(9), 6215-6248. (<a href='https://doi.org/10.1007/s13042-025-02621-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic kidney disease (CKD) is a prevalent and serious global health issue, with a significant impact on individuals globally. Hence, it is imperative to promptly obtain an accurate diagnosis and interpretation for the commencement of appropriate treatment as timely detection and intervention can enhance the probability of long-term survival. Existing projection-based methods for feature selection do not yield desired outcomes due to their different objectives necessitating the need for innovation approaches for a higher predictive performance. This study proposes a novel fusion-based feature selection (FFS) model for the optimization and selection of distinct features to enhance CKD diagnosis. This study utilizes the University of California, Irvine (UCI) CKD dataset and addresses missing data and imbalance issues through Multiple Imputations by Chain Equation (MICE) and Borderline Synthetic Minority Oversampling Technique (Borderline-SMOTE). The proposed model integrates different machine learning (ML) classifiers, conventionally known as black boxes, with SHAP values to provide interpretability and gain transparency in the decision-making process. The proposed FFS model performs better than single feature selection approaches, achieving 100% in all of the evaluation metrics for support vector machine, light gradient boosting, random forest, voting and extreme gradient boosting classifiers compared to other existing literature that also utilized the same dataset. Notably, the SHAP analysis reveals that features such as red blood cell, white blood cell count and the pus cell clumps show model specific interactions. This aids healthcare in understanding and effectively applying the model’s outputs. Empirical evidence demonstrates that our proposed approach exhibits superior performance which has the potential to complement physicians’ diagnosis of kidney diseases. Also, the incorporation of explainability enhances the clarity of outcomes and facilitates the identification of the underlying cause of the diseases, contributing to more transparency and ethically sound AI applications in healthcare.},
  archive      = {J_IJMLC},
  author       = {Nneji, Grace Ugochi and Monday, Happy Nkanta and Pathapati, Venkat Subramanyam Reddy and Nahar, Saifun and Mgbejime, Goodness Temofe and Umana, Edwin Sunday and Hossin, Md Altab},
  doi          = {10.1007/s13042-025-02621-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6215-6248},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {FFS-IML: Fusion-based statistical feature selection for machine learning-driven interpretability of chronic kidney disease},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel Q-learning-inspired mountain gazelle optimizer for solving global optimization problems. <em>IJMLC</em>, <em>16</em>(9), 6167-6213. (<a href='https://doi.org/10.1007/s13042-025-02620-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Q-learning, an eminent reinforcement learning (RL) approach, has garnered substantial research attention in recent years owing to its effectiveness in solving intricate problems and attain noteworthy results in a range of applications. In this study, the Mountain Gazelle Optimizer (MGO) is explored as a promising metaheuristic algorithm, primarily due to its biologically inspired mechanisms that emulate the adaptive and dynamic behaviors of gazelles in nature. However, despite its strong performance, MGO has inherent limitations, such as a tendency to become trapped in suboptimal search regions during early iterations, making it challenging to escape local optima. Therefore, to circumvent these shortcomings, this paper introduces a novel Q-learning-inspired Mountain Gazelle Optimizer (QLMGO), integrating chaotic and random opposite-based learning (ROBL) strategies to enhance optimization performance. The key innovation of QLMGO lies in its dynamic switching mechanism, enabled by Q-learning, which adaptively selects between ROBL and chaotic strategies to optimize the search process. Initially, Q-learning is utilized to regulate the switching mechanism, ensuring efficient exploitation of the search space. During the update phase, QLMGO dynamically chooses the most effective strategy, either ROBL for intensified local search or chaotic exploration for escaping local optima, to accelerate convergence towards the global optimal solution. The performance of QLMGO was rigorously evaluated against well-established optimization algorithms using 23 CEC2005 functions, 10 advanced CEC2019 functions, 30 CEC2017 test functions, and six real-world engineering problems. To ensure a robust and precise assessment, statistical analyses including the Wilcoxon rank-sum test, Friedman test, and t test were conducted. The empirical results from benchmark functions and engineering applications demonstrate the superiority of QLMGO in solving both constrained and unconstrained optimization problems efficiently, thereby validating its effectiveness as an innovative optimization approach.},
  archive      = {J_IJMLC},
  author       = {Sarangi, Priteesha and Mohapatra, Sarada and Mohapatra, Prabhujit},
  doi          = {10.1007/s13042-025-02620-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6167-6213},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A novel Q-learning-inspired mountain gazelle optimizer for solving global optimization problems},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial feature augmentation via transformer-level feature fusion for imbalanced data classification. <em>IJMLC</em>, <em>16</em>(9), 6149-6166. (<a href='https://doi.org/10.1007/s13042-025-02619-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying high-dimensional imbalanced data, such as images and audio, poses a significant challenge to the machine learning community. One potential solution is to employ generative adversarial networks (GANs) for augmenting the minority class data. However, augmentation in data space shows limited performance improvements due to the intricate relationship between balancing class distribution and enhancing feature representation for the minority class. This paper proposes a novel adversarial feature augmentation network via Transformer-level feature fusion (AFAN-TFF) for imbalanced data classification, which operates in feature space without regard to the modality of input data. AFAN-TFF adopts a modified GAN architecture comprising four key components: a feature generator, a feature discriminator, a feature extractor, and a feature classifier. Additionally, a lightweight vision Transformer (ViT) module is devised to enhance feature extraction. This module integrates a global feature pooling layer and a local feature pooling layer, which are used for capturing long-term dependency and contextual features, respectively. Subsequently, these two types of features are fused by a global–local feature fusion layer. These features can complement each other and overcome their individual limitations, resulting in a more robust and informative feature representation. We conducted extensive experiments on five benchmark datasets to compare AFAN-TFF against several established class-imbalance methods. Results demonstrate that AFAN-TFF has substantial advantages over the comparison methods, with consistently improved weighted accuracy, weighted F1, and weighted MCC scores. The effectiveness of employing AFAN-TFF for feature augmentation to enhance classification performance under class-imbalance conditions is also validated.},
  archive      = {J_IJMLC},
  author       = {Leng, Qiangkui and Yi, Gaocheng and Jiao, Erjie and Wang, Changzhong},
  doi          = {10.1007/s13042-025-02619-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6149-6166},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Adversarial feature augmentation via transformer-level feature fusion for imbalanced data classification},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency domain self-attention network with contrastive learning for sequential recommendation. <em>IJMLC</em>, <em>16</em>(9), 6135-6148. (<a href='https://doi.org/10.1007/s13042-025-02618-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation is used to model user preferences by analyzing historical interaction data. However, existing approaches based on self-attention mechanisms usually consider user preferences only from the time-domain perspective, ignoring the inherent periodicity in user behavior. To address this challenge, we introduce an innovative solution called FDSRec, which combines frequency domain self-attention networks with contrastive learning techniques. Specifically, we propose a frequency domain self-attention encoder to capture periodic variations in user behavior data. We convert user representations into frequency domain representations by employing the Fast Fourier Transform technique. Subsequently, the frequency domain self-attention networks are used to learn the correlation and importance among different frequency domain components. In addition, we design time-domain and frequency-domain enhancement methods to address data sparsity and noise issues. Furthermore, we employ a multi-task technique to optimize sequential recommendation and contrastive learning objectives. Finally, extensive experiments are conducted on three publicly available datasets, demonstrating that FDSRec outperforms the state-of-the-art methods.},
  archive      = {J_IJMLC},
  author       = {Ji, Huiqin and Zhang, Jinrui and Wang, Yingqi and Yu, Junyang and Xue, Hui and Zhai, Rui and Li, Han},
  doi          = {10.1007/s13042-025-02618-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6135-6148},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Frequency domain self-attention network with contrastive learning for sequential recommendation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial immune based intrusion detection and mitigation system using entropy fluctuation method and deep maxout classifier. <em>IJMLC</em>, <em>16</em>(9), 6111-6134. (<a href='https://doi.org/10.1007/s13042-025-02617-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of wireless communication systems and on-demand computing resources over the internet has created new opportunities and challenges in ensuring the accessibility and security of critical infrastructure. Cyberattacks in various forms like distributed denial-of-service (DDoS), resource exploitation or reconnaissance etc. can significantly reduce the accessibility, performance and security and exposing more and more flaws in these technologies, resulting in regular disruptions, monetary losses and reputational harm. This research study introduces an Artificial Immune System (AIS) based Intrusion detection System (IDS) that monitors, detects and mitigates attacks effectively by combining the entropy fluctuation method and a novel Deep Maxout classifier. By analyzing entropy variations in network traffic, the system identifies deviations indicative of potential attacks. An Improved Activation and Loss Function (IALF)-based Deep Maxout classifier and a Deep Belief Network (DBN) are combined in a hybrid model that is highly effective at detecting complex, nonlinear patterns and adjusting activation and loss functions. These insights are then further processed to distinguish malicious activity from normal behavior, resulting in enhanced detection accuracy. Upon detection, the Entropy-based Mitigation Process (EMP) isolates malicious nodes to ensure secure data transmission using normalized correlation coefficients and quartile deviation metrics. The proposed system is evaluated using diverse datasets, which represent a wide range of cyberattacks on computing resources and communication systems. Experimental findings show that the system outperforms traditional models in terms of detection accuracy and adaptability while minimizing false positives. A dynamic mitigation approach addresses both current and future security needs by enabling continuous, adaptive protection against dynamic threat landscape.},
  archive      = {J_IJMLC},
  author       = {Chitte, Pallavi and Chaudhari, Sangita},
  doi          = {10.1007/s13042-025-02617-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6111-6134},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Artificial immune based intrusion detection and mitigation system using entropy fluctuation method and deep maxout classifier},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SILTD: Structural information for LLM-generated text detection. <em>IJMLC</em>, <em>16</em>(9), 6095-6110. (<a href='https://doi.org/10.1007/s13042-025-02616-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of large language models (LLMs) has significantly improved the quality and diversity of AI-generated content(AIGC). LLM-Generated text detection plays an important role in preventing the harmful misuse of large language models. Existing approaches primarily analyze texts individually, overlooking the structural relationships between them. This limitation restricts their ability to generalize across diverse LLMs, as they fail to capture the shared statistical patterns inherent in generated texts. To address this, an unsupervised-based structural information for LLM-generated text detection (SILTD) method is proposed. The key insight is that texts from different LLMs exhibit latent similarities in their generative statistical space, which can be modeled to improve cross-model generalization. First, we construct a multi-relational text graph based on the similarity of text features, which aims to model the intricate similarities and correlations between texts. Second, we propose a novel unsupervised graph clustering method. The multi-relational graph is transformed into an encoding tree, which is then optimized based on a two-dimensional structure entropy minimization algorithm to achieve hierarchical clustering of texts. Structural entropy minimization enables achieving high-quality clusters, by measuring the uncertainty of random walks within the graph. Finally, we introduce a new method that measures text similarity and computes the intensity of text aggregation within each cluster, to perform in-cluster label inference. Extensive experiments show that, compared to baseline methods, our approach is more effective and generalizable in detecting six popular LLMs across five datasets.},
  archive      = {J_IJMLC},
  author       = {Yang, Jing and Wang, Shi and Zi, Kangli and Sun, Yanshun and Huang, Yuwei and Luo, Tianyu},
  doi          = {10.1007/s13042-025-02616-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6095-6110},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {SILTD: Structural information for LLM-generated text detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive learning distillation of non-target categories for facial expression recognition. <em>IJMLC</em>, <em>16</em>(9), 6081-6093. (<a href='https://doi.org/10.1007/s13042-025-02614-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) refers to the automated analysis of human emotional states using computer vision techniques, which are of significant importance in tasks such as fatigued driving detection, learning engagement analysis, and safety monitoring. In recent years, deep learning methods have made remarkable progress in facial expression recognition tasks. However, due to scarce samples, high intra-class variation, and inter-class similarity, current approaches perform poorly in recognizing difficult categories, such as Disgust and Fear. To alleviate the performance degradation resulting from these difficult categories, we propose a training framework called Contrastive Learning Distillation of Non-target Categories (CLDN). The proposed method consists of two stages: In the first stage, the model is jointly trained using contrastive learning and supervised learning to weaken the influence of imbalanced labels on visual representation learning, thereby generating robust soft labels. In the second stage, simple category knowledge is transferred to the difficult categories through self-distillation. During the distillation phase, a Non-target Categories Distillation Loss is introduced, alleviating the inhibition of classic distillation losses on non-target category knowledge and further promoting knowledge transfer during the distillation process. The proposed approach achieves competitive results on the RAF-DB, FERPlus, and AffectNet datasets. Ablation studies demonstrate that the method significantly improves the network’s recognition accuracy for difficult categories. The code is available at https://github.com/Greysahy/CLDN .},
  archive      = {J_IJMLC},
  author       = {An, Heng-Yu and Jia, Rui-Sheng},
  doi          = {10.1007/s13042-025-02614-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6081-6093},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Contrastive learning distillation of non-target categories for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image encoded time series classification of small datasets: An innovative architecture using deep learning ensembles. <em>IJMLC</em>, <em>16</em>(9), 6065-6080. (<a href='https://doi.org/10.1007/s13042-025-02613-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are often favored for their strong learning abilities in tackling automatic intelligent models. The classification of time series data streams spans across many applications of intelligent systems. However, the scarcity of effective Machine Learning architectures to handle limited time-series data adversely affects the realization of some crucial applications. In particular, healthcare-related applications are inherently concerned with limited time series datasets. Indeed, building effective artificial intelligence (AI) models for rare diseases using conventional techniques can pose a significant challenge. Utilizing recent advances in deep learning and signal processing techniques, this study introduces a new ensemble deep learning (DL) approach for time series categorization in the presence of limited datasets. Physiological data, such as ECG and voice, are used to demonstrate the functionality of the proposed DL architecture with data obtained from IoT and non-IoT devices. The proposed framework comprises a self-designed deep CNN-LSTM along with ResNet50 and MobileNet transfer learning approaches. The CNN-LSTM architecture includes an enhanced squeeze and excitation block that improves overall performance.This architecture processes time series data transformed into a 3-Channel image structure via improved recurrence plot (RP), Gramian angular field (GAF), and fuzzy recurrence plot (FRP) methods. The proposed model demonstrated superior classification accuracy on the ECG5000 and TESS datasets compared to other state-of-the-art techniques, validating its efficacy for binary and multiclass classification.},
  archive      = {J_IJMLC},
  author       = {Indrasiri, Pubudu L. and Kashyap, Bipasha and Pathirana, Pubudu N.},
  doi          = {10.1007/s13042-025-02613-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6065-6080},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Image encoded time series classification of small datasets: An innovative architecture using deep learning ensembles},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining data augmentation and model fine-tuning for learning from limited data. <em>IJMLC</em>, <em>16</em>(9), 6047-6064. (<a href='https://doi.org/10.1007/s13042-025-02611-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining sufficient data is often challenging, costly, or even infeasible in real-world applications. To address this issue, researchers have developed methods for learning from limited data, which involves discovering the underlying pattern using a small amount of available data. Data augmentation and model fine-tuning are two methods commonly used for learning with limited data. However, both methods have limitations: model fine-tuning methods often face difficulties in acquiring relevant data for pre-training in certain fields, such as drug discovery, whereas data augmentation methods have the potential risk of causing bias. To address this challenge, we propose a general learning framework for limited data using a combination of data augmentation and model fine-tuning (DAMFT). This framework comprises data augmentation and model fine-tuning modules. The data augmentation module is responsible for generating relevant data that are utilized to pre-train the target model, and the model fine-tuning module aims to correct the potential bias of the pre-trained model caused by data augmentation. DAMFT not only solves the difficulty in obtaining relevant data needed for model fine-tuning but also alleviates the bias caused by data augmentation. Furthermore, under the DAMFT framework, we provide a supervised learning algorithm, DAMFT_GH, which adopts a generative adversarial network and head fine-tuning to generate the new instances and tune the pre-trained model, respectively. The experimental results demonstrate that the proposed learning algorithm can effectively improve the performance of the classification model in the case of limited data.},
  archive      = {J_IJMLC},
  author       = {Shi, Hongbo and Zhang, Ying and Wan, Bowen},
  doi          = {10.1007/s13042-025-02611-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6047-6064},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Combining data augmentation and model fine-tuning for learning from limited data},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-lingual sentiment analysis empowered by emotional mutual reinforcement through emojis. <em>IJMLC</em>, <em>16</em>(9), 6031-6045. (<a href='https://doi.org/10.1007/s13042-025-02610-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-lingual sentiment analysis is a challenging task in natural language processing that aims to analyze and understand sentiments expressed in texts across different languages. The motivation behind this task is to address the issue faced by sentiment analysis, where the majority of languages lack sufficient labeled data for training. Nevertheless, cross-lingual sentiment analysis tasks mostly depend on translation tools and corpora. This dependency can prevent these models from capturing the unique sentiment characteristics in the target language due to the accuracy limitations of translation tools and corpora. Driven by the common belief that emojis have consistent associations with sentiments across languages, we introduce a method called emotional mutual reinforcement (EMR). Our EMR approach aims to transfer sentiment knowledge between different languages by using emojis as medium of connection. The central concept of EMR involves employing emojis as the medium to connect different languages, facilitating emotional mutual reinforcement across different linguistic contexts. Sentiment knowledge can effectively cross linguistic boundaries through reinforcing emotion-related features with similar sentiment tendencies. In contrast to existing approaches that utilize emojis as a bridge, EMR can incorporate more fine-grained sentiment knowledge from different languages. Results from a complete evaluation on several publicly available datasets confirm the efficiency of the approach we proposed.},
  archive      = {J_IJMLC},
  author       = {Li, Enping and Li, Tianrui and Liang, Tao and Kang, Azhen and Chen, Kexun and Luo, Haonan},
  doi          = {10.1007/s13042-025-02610-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {6031-6045},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Cross-lingual sentiment analysis empowered by emotional mutual reinforcement through emojis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-strategy improved electric eel foraging optimization algorithm: Continuous and binary variants for solving optimization problems. <em>IJMLC</em>, <em>16</em>(9), 5985-6030. (<a href='https://doi.org/10.1007/s13042-025-02609-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric Eel Foraging Optimization (EEFO) algorithm is a metaheuristic inspired by the social predation behavior of electric eels. It incorporates interactions, resting, migration, and hunting activities to enhance search efficiency. Although EEFO is effective for optimization tasks, it is characterized by slower convergence rates and a tendency to fall into local optima in certain cases. To overcome these limitations, this paper proposes a Multi-strategy Improved Electric Eel Foraging Optimization (MIEEFO) that integrates three key strategies: adaptive tent chaotic mapping, Differential Evolution (DE) mutation strategy, and an enhanced solution technique based on the Fibonacci search technique (FSM). Firstly, MIEEFO employs an adaptive tent chaotic mapping strategy for initializing a uniformly distributed high-quality population for effective search space exploration. Secondly, a novel DE-based mutation strategy is introduced to balance the exploration and exploitation phases. Additionally, to enhance solution quality and mitigate the risk of local optima, an FSM-based improved solution technique is applied to refine the current optimal solution. To conduct a thorough assessment of MIEEFO’s global optimization capabilities, the established numerical challenge of the CEC’22 test suite is utilized. MIEEFO undergoes a comparative analysis with a range of modern, enhanced algorithms, employing the Wilcoxon signed-rank test and the Friedman test to integrate the results of these comparisons. The findings reveal that MIEEFO stands out for its superior optimization abilities, evidenced by its lowest average Friedman ranking of 1.37. MIEEFO consistently outperforms its rivals in most test scenarios, offering solutions that are both more precise and reliable. In addition, the application of MIEEFO is presented through five real-world constrained engineering design challenges, indicating its practical utility. These results highlight MIEEFO’s robust optimization capabilities and its potential for widespread application. Moreover, the proficiency of MIEEFO in managing discrete feature selection tasks is examined through tests on 17 datasets, in conjunction with ten established classification techniques and two advanced classification methods. The results confirm that MIEEFO achieved an average feature selection reduction of 72.59% across datasets while improving classification accuracy by up to 7.2% compared to competing methods.},
  archive      = {J_IJMLC},
  author       = {Mostafa, Reham R. and Khedr, Ahmed M. and AL Aghbari, Zaher and Afyouni, Imad and Kamel, Ibrahim and Ahmed, Naveed},
  doi          = {10.1007/s13042-025-02609-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5985-6030},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A multi-strategy improved electric eel foraging optimization algorithm: Continuous and binary variants for solving optimization problems},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modulation recognition based on adaptive denoising residual neural network. <em>IJMLC</em>, <em>16</em>(9), 5967-5984. (<a href='https://doi.org/10.1007/s13042-025-02608-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing deep learning methods cannot accurately identify the modulation type of wireless signals in the circumstance of low signal-to-noise ratio (SNR). Therefore, the model based on adaptive denoising residual neural network is designed in this paper. At the input end, the pre-processing of the communication signal is carried out to extract the in-phase, orthogonal, amplitude, phase and frequency components of the complex signal as the recognition features. Then adaptive denoising module combining the soft threshold function and the improved channel attention mechanism removes the noise mixed in the signal. After that, the Inception-ResNet which is composed of Inception and residual structure is used to mine deep features. Finally the modulation recognition is realized by the full connection layer. The simulation results show that under the background of Gaussian white noise, its average recognition accuracy for common signals reaches about 94% and also performs well in the condition of Rician, Rayleigh and even unknown channel. Besides, the proposed model is better than visual geometry group-19 under low SNR, and has certain robustness to frequency offset and phase offset.},
  archive      = {J_IJMLC},
  author       = {Ma, Mingyue and Zhen, Jiaqi},
  doi          = {10.1007/s13042-025-02608-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5967-5984},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Modulation recognition based on adaptive denoising residual neural network},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRR-TM: Finding equilibria in adversarial team games via perfect-recall refinement and teammate modeling. <em>IJMLC</em>, <em>16</em>(9), 5955-5966. (<a href='https://doi.org/10.1007/s13042-025-02607-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial team games (ATGs) with en ante coordination involve a team competing against an adversary under conditions of incomplete information, where team members can coordinate their strategies before the game starts. Team-maxmin equilibrium with correlation (TMECor) is a core solution concept in this setting. While some existing algorithms leverage game transformation techniques to compute TMECor by converting ATGs into two-player zero-sum games, these methods often encounter scalability issues. The exponential growth in the transformed game tree size impedes computational speed and consumes significant storage resources. In this paper, we propose a novel method based on perfect-recall refinement that achieves game tree transformation while maintaining the original size. Additionally, we introduce an innovative teammate modeling approach, allowing team members to infer private information based on observations of their teammates’ actions. Experimental results on benchmark testbeds show a significant improvement in equilibrium computation efficiency without expanding the game size, indicating the effectiveness of our method. Our work not only addresses the computational complexities associated with ATGs but also provides valuable insights for further research on enhancing strategic decision-making in collaborative adversarial settings.},
  archive      = {J_IJMLC},
  author       = {Qiu, Chen and Huang, Weixin and Xiong, Hongji and Zhang, Jiajia and Wang, Xuan},
  doi          = {10.1007/s13042-025-02607-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5955-5966},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {PRR-TM: Finding equilibria in adversarial team games via perfect-recall refinement and teammate modeling},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Core structure-guided multi-modal classification via monte carlo tree search. <em>IJMLC</em>, <em>16</em>(9), 5943-5953. (<a href='https://doi.org/10.1007/s13042-025-02606-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Modal Classification (MMC) effectively integrates multiple data sources, for which the choice of fusion strategy is crucial for classification performance. Current research includes expertise-based and search-based techniques, among which Neural Architecture Search (NAS) approaches stand out, but face significant time and resource consumption issues. To address this issue, CSG-NAS narrows the search scope by using the core structure-guided search, which greatly improves search performance and efficiency. However, CSG-NAS only considers the complementarity between high-quality views when acquiring the core structures, leading to an incomplete narrowed search space. In this paper, we combine Monte Carlo Tree Search (MCTS) with Core Structure-Guided search to propose an efficient and credible MMC algorithm, MCTS-CSG. It includes the core structures acquisition module composed of learning and searching phases, and the optimal structure search module composed of evolving phase. Specifically, the learning phase partitions and ranks the entire space by learning the node regressor, the search phase is based on the MCTS method to sample the structure in the most promising subspace while avoiding falling into a local optimum. The core structures containing complementarity and optimality are obtained after cyclic execution of the learning and searching phases. In the evolving phase, high-quality search domains are constructed around the core structures, which are defined as areas likely to contain high-performance fusion structures, and the Evolutionary Algorithm (EA) is used to find the optimal model structure. Meanwhile, a knowledge inheritance strategy is introduced to improve search speed. The experimental results show that the MCTS-CSG algorithm performs well in the MMC task with the best known search efficiency and performance.},
  archive      = {J_IJMLC},
  author       = {Liu, Guoqing and Qian, Yuhua and Liang, Xinyan and Fu, Pinhan},
  doi          = {10.1007/s13042-025-02606-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5943-5953},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Core structure-guided multi-modal classification via monte carlo tree search},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active attack and defense on attribute reduction with fuzzy rough sets. <em>IJMLC</em>, <em>16</em>(9), 5923-5941. (<a href='https://doi.org/10.1007/s13042-025-02605-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction based on dynamically updated datasets in fuzzy rough sets plays a significant role in dealing with the uncertainty of time-evolving updated data. However, current research on attribute reduction lacks theoretical mechanisms to actively distinguish and defend against malicious interference in datasets. Aiming at this problem, an attribute reduction update framework with defense is proposed for dynamic datasets with adversarial attack. In this framework, an adversarial attack model is presented to select the optimal attacked attributes and construct the adversarial samples to generate the attack datasets. Based on this, a defense model is designed by constructing defense samples to avoid attacks. Firstly, the key identification sample pairs that determine the discernibility of the minimal element subset are defined, which are then used to define the attack target candidate set and construct adversarial samples. To alter the discernibility attributes of the key discernibility sample pairs, the attribute significance degree with attack preference is defined to select the unimportant attributes to attack. Then, the attack model is designed to select the optimal attacked candidate subset and generate the attack dataset. Targeting the attack strategy, defense samples for both the optimal attacked attribute subset and the useless attribute set are constructed to generate the defense matrix and defense datasets. Finally, a unified update strategy for attribute reduction after attack and defense is proposed to induce the updated reduct. Numerical experiments verify the rationality and effectiveness of the framework proposed in this paper based on the success rate of attack and defense, as well as the classification results.},
  archive      = {J_IJMLC},
  author       = {Gao, Yue and Chen, Degang and Wang, Hui},
  doi          = {10.1007/s13042-025-02605-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5923-5941},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Active attack and defense on attribute reduction with fuzzy rough sets},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on a multi-classification diagnosis method for anemia text medical records in traditional chinese medicine based on improved pre-trained LERT model. <em>IJMLC</em>, <em>16</em>(9), 5907-5921. (<a href='https://doi.org/10.1007/s13042-025-02604-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an innovative NLP hybrid model for diagnosing and predicting common types of anemia in Traditional Chinese Medicine (TCM), addressing challenges of limited medical text data and suitable NLP models. Utilizing the pre-trained LERT model as a foundation, we integrate it with the GRU deep learning model to enhance text feature extraction, improving prediction accuracy and reducing computational complexity. The introduced AGN(Adaptive Gate Network) Module mitigates long-term dependency challenges associated with GRU, offering flexible data attention control and enhancing classification performance. A novel loss function, FCS, is designed to handle unbalanced datasets, focusing on difficult-to-classify samples for improved training stability and reduced misclassifications. Evaluating on a TCM anemia diagnosis dataset, our model outperforms benchmarks with an accuracy of 0.9430, precision(MA/WA) of 0.9517/0.9459, recall(MA/WA) of 0.9326/0.9430, and F1-score(MA/WA) of 0.9408/0.9430, demonstrating its potential to intelligently assist doctors in anemia diagnosis and prediction.},
  archive      = {J_IJMLC},
  author       = {Peng, Chongxiao and Gao, Zhijun and Wang, Jinhuan and Yue, Xin and Sun, Lili and Sun, Yinhuan and Du, Fuquan},
  doi          = {10.1007/s13042-025-02604-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5907-5921},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Research on a multi-classification diagnosis method for anemia text medical records in traditional chinese medicine based on improved pre-trained LERT model},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimum-cost based hidden markov model with speed constraint for intruder trajectory recovery in utility tunnels. <em>IJMLC</em>, <em>16</em>(9), 5893-5905. (<a href='https://doi.org/10.1007/s13042-025-02603-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utility tunnels represent a critical component of urban infrastructure, often regarded as the city’s lifeline. Ensuring the security of utility tunnels is crucial for protecting urban infrastructure and preventing threats to essential services from intruders. Traditional trajectory recovery methods rely on GPS data, which are not suitable for underground environments with inferior signals. The widely deployed network of surveillance cameras in tunnels presents new opportunities, as it allows us to extract video signals and recover intruders’ trajectories, but this approach encounters several challenges. Firstly, the videos from camera network lacks clear identity labels, leading to ambiguous correspondence between intruders and video records. Secondly, the uneven and incomplete distribution of surveillance cameras results in data sparsity issues. In this paper, we propose a novel two-stage framework, TrajUT, which leverages machine learning algorithms to address these challenges. In the first stage, we introduce the multi-criteria-based trajectory segmentation to segment the raw record sequence into sub-sequences belonging to different groups of intruders, based on the continuity of sequences. For second stage, we design a hidden Markov model-based method that uses the records as observed sequences to infer the hidden sequences with the minimum cost, which correspond to the intruder movement trajectories. The proposed dynamic speed constraints enhance trajectory recovery by incorporating both spatial distances and temporal speed relationships into the cost adjustment mechanism. We conduct experiments using real-world data from the utility tunnels in Suzhou City. The results demonstrate the effectiveness of our framework, achieving a 10.38% improvement in precision compared to the best baseline method.},
  archive      = {J_IJMLC},
  author       = {Song, Wenbin and Yin, Baijian and Li, Xinwei and Wang, Shuai and He, Tian and Xu, Zhao-Dong and Liu, Shenghao},
  doi          = {10.1007/s13042-025-02603-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5893-5905},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Minimum-cost based hidden markov model with speed constraint for intruder trajectory recovery in utility tunnels},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARTEMIS: Animal recognition through enhanced multimodal integration system. <em>IJMLC</em>, <em>16</em>(9), 5877-5892. (<a href='https://doi.org/10.1007/s13042-025-02602-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Animal Recognition Through Enhanced Multimodal Integration System (ARTEMIS), a transformer-based framework designed for multilabel animal action recognition by fusing video, image, and textual modalities. ARTEMIS utilizes state-of-the-art captioning and language models, such as BLIP2 and Llama 3, to generate textual descriptions from video frames, which are input to the model, significantly enhancing its performance unlikely previous results that do not consider this modality. Through comprehensive ablation studies, we explore the contribution of various model components and propose optimization strategies, including genetic algorithms and reinforcement learning, to dynamically adjust ensemble weights. Our feature alignment techniques-using contrastive and cosine similarity losses-further improve multimodal integration. Evaluations on the Animal Kingdom dataset, which includes 30,100 clips across 140 action classes, demonstrate that ARTEMIS achieves a new state-of-the-art mAP of 79.82, outperforming existing methods. The combination of multimodal fusion and ensemble strategies makes ARTEMIS a robust solution for complex animal action recognition tasks. The code of our fusion method is available at https://github.com/edofazza/ARTEMIS.},
  archive      = {J_IJMLC},
  author       = {Fazzari, Edoardo and Romano, Donato and Falchi, Fabrizio and Stefanini, Cesare},
  doi          = {10.1007/s13042-025-02602-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5877-5892},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {ARTEMIS: Animal recognition through enhanced multimodal integration system},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gformer: A novel spatio-temporal transformer network for accurate traffic flow prediction. <em>IJMLC</em>, <em>16</em>(9), 5861-5875. (<a href='https://doi.org/10.1007/s13042-025-02601-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting traffic flow on urban road networks is extremely challenging due to complex spatio-temporal correlations. Currently, the most popular method is to stack multiple layers of spatio-temporal graph convolutions, which generally combines the graph convolutional network (GCN) with the temporal convolutional network (TCN) or the recurrent neural network (RNN) for joint spatio-temporal prediction. However, the existing methods for constructing the predefined adjacency matrices of the GCN fail to accurately capture the real world situation. In addition, these methods restrict the potential of GCN since the GCN module is limited to aggregating the features within a relatively limited local spatio-temporal neighborhood. In this paper, we propose a neural network called Gformer for predicting traffic flow based on the time lagged and cross correlation (TLCC) algorithm. Gformer captures the temporal and spatial features using the linear self-attention and TLCC-based GCN. The combination of these two models expands the range of the temporal and spatial neighborhoods where feature aggregation is effective, further unleashing the potential of GCN. Additionally, a novel parametrized skip-connection is used to merge the outputs of multiple layers of the encoder and feed them into the decoder for final prediction. To validate the effectiveness of Gformer, we conduct experiments on the two large-scale urban traffic flow datasets and find that our proposed model exhibits superior overall performance compared to all baseline models. The experiments demonstrate the validity and robustness of the proposed model.},
  archive      = {J_IJMLC},
  author       = {Zhao, Yuan and Li, Mingxin and Lei, Hang and Wen, Shixi and Zhao, Hui and Liu, Lichuan},
  doi          = {10.1007/s13042-025-02601-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5861-5875},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Gformer: A novel spatio-temporal transformer network for accurate traffic flow prediction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECODI: A novel evolutionary coreset distillation with LLM-assisted fitness evaluation for encrypted network traffics. <em>IJMLC</em>, <em>16</em>(9), 5841-5859. (<a href='https://doi.org/10.1007/s13042-025-02600-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, machine learning-based methods have become essential for classifying network data flows under encryption, as traditional deep packet inspection is ineffective due to encryption protocols like HTTPS and QUIC, which now cover over 85% of Internet traffic. However, the scale of modern Internet traffic introduces new challenges, particularly the massive size of datasets required for training these models. Handling such large datasets results in excessive computational costs, prompting the need for data condensation techniques that reduce dataset size without sacrificing performance. In this paper, we propose a novel Evolutionary Coreset Distillation method for network traffic classification. Our approach, named ECODI, combines the power of evolutionary algorithms with Large Language Models (LLMs) to condense large datasets into smaller, representative coresets. We employ LLMs to generate high-level embeddings that guide the evolutionary algorithm in selecting coresets, thus preserving the most important information while reducing the dataset size. Additionally, we introduce a gradient-based forgetting mechanism to further refine the coreset by eliminating redundant or low-impact data points. The extensive experiments demonstrate that ECODI outperforms both traditional methods (Random Sampling, K-Center, and Herding) and recent evolutionary approaches (EVA and DEvS) in achieving high classification performance with reduced dataset sizes. Notably, ECODI achieves a fitness score of 0.94 in as few as 10 generations, offering substantial improvements in terms of both convergence speed and final classification accuracy compared to EVA and DEvS.},
  archive      = {J_IJMLC},
  author       = {Tran, Hai-Anh and Tong, Van},
  doi          = {10.1007/s13042-025-02600-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5841-5859},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {ECODI: A novel evolutionary coreset distillation with LLM-assisted fitness evaluation for encrypted network traffics},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving intrusion detection with federated learning: Enhancing privacy and efficiency in IoT networks. <em>IJMLC</em>, <em>16</em>(9), 5821-5839. (<a href='https://doi.org/10.1007/s13042-025-02599-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection systems (IDS) are crucial in the identification of unauthorized activities on a digital network, enabling cybersecurity measures to initiate prevention protocols to protect the security of their networks and data. With Internet of Things (IoT) as a ubiquitous phenomenon, IDS has become increasingly important owing to the massive growth of connected devices. Machine learning algorithms can detect anomalies in large datasets, but centralized methods pose security risks by gathering all data in one place. Federated learning (FL) offers a safer alternative, allowing multiple clients to collaborate on a model without sharing data, reducing the risk of data leaks and enhancing privacy. In this paper, we propose a FL-based IDS. Herein, we have applied the FL approach to a number of clients that are grouped in sets where the number of clients in a set varies from 2 to 10. After that, we applied feature ranking via a random forest approach, wherein the features are ranked based on their importance to find the optimal and reduced dataset for reducing the inference time. Further, three models for each of the clients were trained using the refined dataset obtained. Next, the central server aggregates the data from the clients using three methods. We have updated the dataset with two types of scenarios, one with two classes (Benign and Attacks) and the second with a multi-class (Benign, Dodag, Flooding, Rank, Blackhole) for identifying the attack. The experimental results establish that the proposed methodology achieves superior performance as compared to similar existing methodologies.},
  archive      = {J_IJMLC},
  author       = {Sharma, Hemant and Yadav, Gyan Singh},
  doi          = {10.1007/s13042-025-02599-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5821-5839},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Privacy-preserving intrusion detection with federated learning: Enhancing privacy and efficiency in IoT networks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection based l1alm-DT and MHT-LSTM for human activity recogition via sensor. <em>IJMLC</em>, <em>16</em>(9), 5793-5819. (<a href='https://doi.org/10.1007/s13042-025-02598-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Activity Acknowledgment (HAR) from sensor information assumes a vital part in different fields, for example, medical care, sports examination, and security checking. Notwithstanding, the intricacy of precisely recognizing activity from sensor-created information presents critical difficulties. Challenges incorporate overseeing noise obstruction, tending to missing information, and proficiently choosing significant features while staying away from dimensionality issues. The work presents an exhaustive structure, MHT-LSTM, pointed toward improving the exactitude and unwavering quality of human activity recognition. The framework tends to follow certain coordinating high level techniques, starting with signal handling methods to alleviate commotion impedance. The presentation of the K-Adaptive Butterworth filter fundamentally reduces the noise, followed by regularization-based polynomial interpolation that handles the missing signal. The work involves feature extraction by taking into consideration both statistical features and Spatio temporal features. Then, the highlights enhancement utilizing features using an L1 Regularization Adaptive learning mechanism-based decision tree. This step decreases dimensionality, improving computational proficiency without compromising fundamental data. The last stage includes metaheuristic hyperparameter tuning to Long short-term memory (MHT-LSTM) organizations, catching fleeting conditions for exact activity recognition. The subsequent SRCHBO-LSTM system shows remarkable execution measurements in trial assessments. It accomplishes improved precision, accuracy, and f-score as associated to existing state of art methods. Moreover, its insightful accuracy across different action classes approves its adequacy in assorted genuine applications.},
  archive      = {J_IJMLC},
  author       = {Ram, R. Saravana and Boobalan, S. and Prakash, S. Arun and Sekhar, Velappagari},
  doi          = {10.1007/s13042-025-02598-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5793-5819},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Feature selection based l1alm-DT and MHT-LSTM for human activity recogition via sensor},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAEL-FER: A multi-aspect enhancement learning framework for robust facial emotion recognition through integrated learning modules. <em>IJMLC</em>, <em>16</em>(9), 5761-5792. (<a href='https://doi.org/10.1007/s13042-025-02597-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is an important area in computer vision and Artificial Intelligence focused on identifying emotions from facial movements. However, FER faces challenges such as variability in expressions, occlusions, and changing lighting conditions. Addressing these requires advanced solutions, including improved feature extraction techniques, enhanced model training processes, and comprehensive contextual data analysis. This paper proposes a multi-aspect enhancement learning based facial emotion recognition (MAEL-FER) model that exploits several unique modules to effectively classify facial emotions. This approach consists of one of the most specific features to learn the best details of facial expressions that allow for the classification of similar emotions. A feature enhancement module enhances features in some way and suppress those that may not be accurate or noisy, a region enhancing module enhances regions in face such as the eyes and the mouth to recognize emotions. Also, the model entails feature such as generalization learning for better adaptation with various dataset, Meta-learning to allow efficient learning with limited samples, Adversarial training for robustness against noise and adversarial attacks. The MAEL-FER model offers high accuracy in recognizing facial expressions and effectively extracts features, enabling it to distinguish between similar emotions even in challenging conditions. Simulation results highlight the MAEL-FER model's effectiveness with high accuracy rates of 85.78%, 96.98%, 94.83%, and 69.08% across four FER datasets of FER-2013, CK+, RAF-DB, and AFFECTNET-7, respectively. These results surpass previous state-of-the-art methods, demonstrating the model's superior performance, reliability, and efficiency in diverse FER tasks and conditions.},
  archive      = {J_IJMLC},
  author       = {Balachandran, G. and Ranjith, S. and Jagan, G. C. and Chenthil, T. R.},
  doi          = {10.1007/s13042-025-02597-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5761-5792},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {MAEL-FER: A multi-aspect enhancement learning framework for robust facial emotion recognition through integrated learning modules},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive control of direct-drive wave power generation system based on RBF neural network. <em>IJMLC</em>, <em>16</em>(9), 5747-5760. (<a href='https://doi.org/10.1007/s13042-025-02596-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the problems of time-varying disturbance and model parameter variation in the practical application of direct-driven wave power generation system, an adaptive control strategy based on Radial Basis Function (RBF) neural network is proposed. By analyzing the hydrodynamic model of a direct-drive wave energy converter (WEC), the maximum power capture condition is obtained by using the equivalent circuit method. The nonlinear state-space model of direct-drive wave power generation system is constructed based on the idea of maximum force per ampere (MFPA) control. Through the coordinate transformation and state feedback, the canonical form is achieved. An adaptive state feedback tracking controller is designed, in which RBF neural network is used to approximate the unknown function in the controller. The effects of time-varying disturbance and model parameter variation on the system are suppressed effectively. The system is in the desired operation state, and then maximum power tracking control is achieved. Simulation results show that the proposed control strategy effectively improves the transient and steady-state tracking performance, and has good fault-tolerant and anti-disturbance robustness.},
  archive      = {J_IJMLC},
  author       = {Wu, Zhong-Qiang and Jing, Xin},
  doi          = {10.1007/s13042-025-02596-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5747-5760},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Adaptive control of direct-drive wave power generation system based on RBF neural network},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph embedded subspace clustering with entropy-based feature weighting. <em>IJMLC</em>, <em>16</em>(9), 5727-5745. (<a href='https://doi.org/10.1007/s13042-025-02595-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering on high-dimensional data usually suffers from correlated and contaminated features, which seriously restrict the subspace distribution and graph embedding procedure in practical applications. Therefore, how to preserve intrinsic structure accurately on robust subspaces still needs to be further explored. In this paper, we propose a robust Graph Embedded Subspace Clustering model with Entropy-based Feature Weighting (GSCEFW) to differentiate feature weights and substantially facilitate manifold preserving during subspace learning. In particular, an optimal graph exploration term guided by pseudo-label learning is introduced to subspace clustering framework, which imposes dual-structural constraint on subspace representation to strengthen its block diagonal contour. Then, an entropy-based feature weighting term is considered to automatically mitigate the adverse effect from noisy or irrelevant features during data reconstruction. Finally, an alternative optimization method is developed to solve the challenging objective function, together with theoretical algorithm analysis. Extensive experiments on benchmark datasets demonstrate the effectiveness and superiority of the proposed GSCEFW model compared with the state-of-the-art models.},
  archive      = {J_IJMLC},
  author       = {Jiang, Kun and Liu, Zhaoli and Zhu, Lei and Cui, Lanlan},
  doi          = {10.1007/s13042-025-02595-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5727-5745},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Graph embedded subspace clustering with entropy-based feature weighting},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust speech recognition method based on dense time–frequency convolution and bispectral refinement enhancement. <em>IJMLC</em>, <em>16</em>(9), 5707-5725. (<a href='https://doi.org/10.1007/s13042-025-02594-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In noisy environments, the accuracy of speech recognition is often affected by noise interference, making it necessary to use speech enhancement techniques to mitigate this impact. Current methods that incorporate enhancement modules through joint training have made some progress in improving the robustness of speech recognition systems. However, these approaches primarily focus on directly modeling the original noisy signal without fully considering the use of potentially valuable information within the noise to support the enhancement modeling process. To address these issues, this work proposes a robust speech recognition method based on dense time–frequency convolution and bispectral refinement enhancement. This method attempts to avoid the problem of excessive suppression by repairing distortions that may be introduced by the speech enhancement module while performing noise reduction. First, a single-channel speech enhancement module based on dense time–frequency convolution is used for initial noise suppression. Then, a bispectral refinement enhancement module is designed to extract beneficial features from the estimated noise to improve speech quality. Finally, a proposed weighted speech distortion loss function is applied through multi-task joint training to further enhance recognition performance. Experimental results show that the proposed method reduces the word error rate in speech recognition by 17.57% compared to baseline methods.},
  archive      = {J_IJMLC},
  author       = {Wang, Wenjun and Dong, Ling and Yu, Zhengtao and Huang, Yuxin and Mo, Shangbin and Wang, Linqing},
  doi          = {10.1007/s13042-025-02594-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5707-5725},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Robust speech recognition method based on dense time–frequency convolution and bispectral refinement enhancement},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-dimensional PLSR with manifold optimization based CNN for image classification. <em>IJMLC</em>, <em>16</em>(9), 5689-5705. (<a href='https://doi.org/10.1007/s13042-025-02593-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Partial Least Squares Regression (PLSR) aims to establish a linear relationship between independent and dependent variables. It will convert the original data into one-dimensional vectors, thus destroying the structural information of the data and generating dimensional catastrophes. To address these limitations, this paper proposes a novel model CNNM2DPLSR called Two-Dimensional Partial Least Squares Regression (2DPLSR) with Manifold Optimization based Convolutional Neural Network (CNN). Firstly, deep features are extracted from the original data using CNN, which are taken as the new independent variables. To retain the structural information of the data, after performing bilateral dimensionality reduction on the independent variables, the resulting low-dimensional features are multiplied by the dependent variable to maximize the inner product, thereby obtaining the bilateral dimensionality reduction matrix. Finally, an objective function for the model is constructed and undergoes a detailed derivation process. Given that the dimensionality reduction matrix satisfies the column orthogonality constraints, the manifold optimization approach is operated in this model to obtain more accurate numerical solutions. Experimental results on various datasets show that the proposed method has lower classification error rates and better adaptability than other representative methods.},
  archive      = {J_IJMLC},
  author       = {Chen, Haoran and Wu, Kai and Song, Wenjun and Tao, Hongwei and Li, Zuhe and Li, Xiao and Du, Yanan},
  doi          = {10.1007/s13042-025-02593-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5689-5705},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Two-dimensional PLSR with manifold optimization based CNN for image classification},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive label modification based on uncertainty learning for facial expression recognition in the wild. <em>IJMLC</em>, <em>16</em>(9), 5673-5688. (<a href='https://doi.org/10.1007/s13042-025-02591-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) in natural settings is often hampered by label noise due to subjective annotations and ambiguous expressions. While previous studies have attempted to address this through uncertainty estimation and relabeling, they have overlooked the accuracy of uncertainty learning and the adaptability of label correction. We propose Adaptive Label Modification based on Uncertainty Learning (ALM-UL), a novel approach that dynamically adjusts noisy labels based on learned uncertainty without manual sample selection. ALM-UL consists of two key components: (1) an uncertainty learning module that obtains precise uncertainty value by focusing on relatively challenging samples, and (2) an adaptive label modification module that revises noisy labels using the learned uncertainty. This approach allows ALM-UL to concentrate on critical facial samples and implement a parameter-free relabeling mechanism, effectively mitigating the impact of uncertain samples. Our method is easily implementable with minimal additional parameters. Experiments on both synthetic and real-world datasets demonstrate that ALM-UL significantly outperforms state-of-the-art algorithms, achieving an average improvement of 2% in recognition accuracy.},
  archive      = {J_IJMLC},
  author       = {Tang, Hui and Li, Yichang and Jin, Zhong},
  doi          = {10.1007/s13042-025-02591-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5673-5688},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Adaptive label modification based on uncertainty learning for facial expression recognition in the wild},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-driven embedded feature selection method based on fuzzy decision consistency and classification reward mechanism. <em>IJMLC</em>, <em>16</em>(9), 5653-5672. (<a href='https://doi.org/10.1007/s13042-025-02590-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is vital in machine learning and data analysis, as it enhances model performance, reduces computational costs, and improves efficiency. However, existing embedded methods, particularly those based on regression, often assume a linear relationship between the feature and decision spaces, which is not suitable for complex and large-scale data. To overcome the limitation, the FDC model is proposed as a novel embedded feature selection approach grounded in granular computing theory. By incorporating a fuzzy consistency metric, the FDC model enables nonlinear mapping from the feature space to the decision space, capturing the intricate relationship between features and decisions. FDC integrates a dual mechanism of fusion fuzzy information decision learning and classification reward, allowing it to simultaneously account for both the fuzzy and explicit aspects of classification. Experimental evaluations on 15 real-world datasets demonstrate that the FDC model effectively improves feature selection accuracy, suggesting its potential and applicability in practical settings.},
  archive      = {J_IJMLC},
  author       = {Huang, Yang and Deng, Tingquan and Wang, Changzhong and Zhang, Yang},
  doi          = {10.1007/s13042-025-02590-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5653-5672},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Dual-driven embedded feature selection method based on fuzzy decision consistency and classification reward mechanism},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autoencoder-like non-negative matrix factorization with dual-graph constraints for multi-view clustering. <em>IJMLC</em>, <em>16</em>(9), 5637-5652. (<a href='https://doi.org/10.1007/s13042-025-02589-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization (NMF)-based multi-view data clustering has been widely used due to its simple formulation and strong interpretability.However, NMF-based multi-view clustering methods primarily focus on reconstructing the original data while neglecting the learning of low-dimensional representations, and emphasize learning the data manifold within the datasets while ignoring the learning of feature manifolds. To address these limitations, we propose a novel framework that combines autoencoder-like NMF with dual-graph constraints for multi-view clustering (ADGNMF). This approach unifies data representation learning and data reconstruction into a single framework, enhancing the learning of low-dimensional data representations. Additionally, to capture comprehensive information, we apply dual-graph constraints to both the data and feature manifolds. The algorithm employs an iterative updating strategy to optimize the objective function. Compared with several state-of-the-art multi-view clustering algorithms, ADGNMF has demonstrated superior performance across five key metrics on six public datasets.},
  archive      = {J_IJMLC},
  author       = {Ban, Yong and Cai, Yongming and Huang, Zhanpeng},
  doi          = {10.1007/s13042-025-02589-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5637-5652},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Autoencoder-like non-negative matrix factorization with dual-graph constraints for multi-view clustering},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IBBA: An improved binary bat algorithm for solving low and high-dimensional feature selection problems. <em>IJMLC</em>, <em>16</em>(9), 5605-5635. (<a href='https://doi.org/10.1007/s13042-025-02588-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advancements have resulted in the accumulation of vast amounts of data across various industries, often containing redundant or irrelevant features. As a result, the development of efficient feature selection methods has become increasingly critical. This paper proposes an Improved Binary Bat Algorithm (IBBA) to overcome the limitations of the original Bat Algorithm (BA), particularly its weak exploration ability and tendency to become trapped in local optima. IBBA enhances both exploration and exploitation through a novel Fitness-based Exploitation Strategy (FES) and an improved Harris Hawks Optimization (HHO). Additionally, random perturbations are introduced during iterations to adjust positions that deviate from the search space, thus preventing ineffective searches. Since the original BA is primarily designed for continuous optimization problems, this study also investigates the effect of four V-shaped transfer functions on the algorithm’s performance. Experimental results on 28 datasets with varying dimensionalities (ranging from nine to 12,600 features) demonstrate that IBBA outperforms 12 state-of-the-art metaheuristic algorithms in terms of fitness, accuracy, feature selection ratio, and runtime. Moreover, an analysis of exploration and exploitation shows that IBBA effectively balances these two processes, addressing BA’s exploration shortcomings. The Wilcoxon signed-rank test, conducted at a significance level of 0.05, validates the algorithm’s effectiveness, revealing that IBBA demonstrates significant advantages in 87.5% of the tests. Finally, comparisons with 14 recently proposed feature selection methods highlight IBBA’s competitive classification accuracy. Therefore, this study is expected to make a valuable contribution to solving feature selection problems across datasets with diverse dimensionalities.},
  archive      = {J_IJMLC},
  author       = {Wang, Tao and Xie, Minzhu},
  doi          = {10.1007/s13042-025-02588-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5605-5635},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {IBBA: An improved binary bat algorithm for solving low and high-dimensional feature selection problems},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coarse-to-fine domain adaptation object detection with feature disentanglement. <em>IJMLC</em>, <em>16</em>(9), 5589-5604. (<a href='https://doi.org/10.1007/s13042-025-02586-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation object detection (DAOD) uses the labeled data of one scene (i.e., the source domain) and the unlabeled data of another unfamiliar scene (i.e., the target domain) to train the cross-domain object detector. Most existing methods align the overall distribution of features by adversarial adaptive methods. Despite their success, these methods are primarily designed for two-stage detectors that are challenging to deploy, resulting in limited practical applications. In addition, owing to the instability of adversarial domain discriminator training, inducing the detector is difficult using only an adversarial adaptive strategy to extract instance-level domain-invariant features to align the overall distribution. To address these issues, we propose a new cross-domain object detection framework based on the You Only Look Once (YOLO) series of algorithms named Disentanglement Representation YOLO (DRY). The developed method achieves feature disentanglement in the channel dimension and spatial dimensions through domain-invariant feature disentanglement (DIFD) and instance-level feature disentanglement (ILFD) modules, respectively, prompting the detector to extract domain-invariant features. Experiments demonstrate that our model outperforms existing methods. It achieved an average accuracy value of 42.7 on the Cityscapes to FoggyCityscapes benchmark and significantly outperformed all other methods on human and car objects. The average accuracy values of 49.0 and 49.5 achieved on the SIM10K to Cityscapes and KITTI to Cityscapes scenarios, respectively, are superior to those of existing methods. Extensive experimental results on various datasets verify that the proposed DRY method is effective and widely applicable. The code is available at https://github.com/BJUTsipl/DRY .},
  archive      = {J_IJMLC},
  author       = {Li, Jiafeng and Zhi, Mengxun and Zheng, Yongyu and Zhuo, Li and Zhang, Jing},
  doi          = {10.1007/s13042-025-02586-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5589-5604},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Coarse-to-fine domain adaptation object detection with feature disentanglement},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-stationary fuzzy time series modeling and forecasting using deep learning with swarm optimization. <em>IJMLC</em>, <em>16</em>(9), 5569-5587. (<a href='https://doi.org/10.1007/s13042-025-02585-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this research is to explore the adaptation of fuzzy time series (FTS) modeling and forecasting for dynamically evolving non-stationary data. It is proposed that fuzzy time series modeling, with fuzzy logical relationships (FLRs) predicted by deep learning, and hyperparameters (fuzzy order and length of intervals) defined by swarm intelligence, can effectively forecast non-stationary time series data. The global outbreak of the COVID-19 pandemic highlights the need for accurate time series forecasting models that can adapt to multiple successive waves of the pandemic caused by dynamically evolving variants of the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) having distinctive spread patterns. Therefore, in this study, a hybrid time series forecasting model integrating high-order FTS, context-augmented variants of the long short-term memory network (LSTM), and particle swarm optimization (PSO), is proposed for accurate forecasting of COVID-19 cases associated with multiple COVID-19 waves. Attention and convolution mechanisms are explored for context augmentation in LSTM. The proposed hybrid model is evaluated on five different datasets of COVID-19 confirmed cases in USA, UK, India, Russia and Italy, in the duration of June 1, 2020 to April 15, 2022, encompassing multiple COVID-19 waves. The model forecasts are compared with five state-of-the-art time series forecasting models using five different performance metrics. Experimental results prove that the hybrid of FTS, attention-bidirectional-LSTM, and PSO (FTS+PSO+Attention-Bi-LSTM) performs consistently best for all countries. Nemenyi statistical significance test verifies that FTS+PSO+Attention-Bi-LSTM is the leading model for time series forecasting of the COVID-19 pandemic with 95% confidence.},
  archive      = {J_IJMLC},
  author       = {Kumar, Naresh and Susan, Seba},
  doi          = {10.1007/s13042-025-02585-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5569-5587},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Non-stationary fuzzy time series modeling and forecasting using deep learning with swarm optimization},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Profiling users with tag-enhanced spherical metric learning for recommendation. <em>IJMLC</em>, <em>16</em>(9), 5553-5567. (<a href='https://doi.org/10.1007/s13042-025-02584-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of user-item interactions on the Internet, it is important to profile users and model their preferences in recommender systems. Traditional methods, including metric learning, rely on historical user-item interactions to model preferences but struggle in sparse data scenarios. While item tags offer valuable auxiliary information to enhance representations, their shared nature across items makes it challenging to effectively profile users with tags, which requires preserving user personalization through high-quality tag representations. Moreover, traditional optimization for user/item representations always takes place in Euclidean space, where the unconstrained nature of embedding norms tends to lean toward trivial solutions. This may bias the system towards common or popular preferences, thus suppressing the variety in tag-aware user profiles. To this end, we propose to profile users with tag-enhanced spherical metric learning for recommendation, named UTRec. Specifically, we propose an adaptive tag selection mechanism to ensure the quality of tag representations and learn tag-enhanced representations of users/items, thereby effectively profiling users. Additionally, we introduce a spherical optimization strategy for tag-enhanced recommendations to alleviate the limitations imposed by lazy learning and traditional optimization, ensuring the accuracy and diversity of user and item representations within the spherical space. Numerous experiments have been conducted on four real-world datasets, where our proposed tag-enhanced UTRec framework can bring consistent performance gains and achieve a 13.67% improvement regarding both Recall and NDCG metrics.},
  archive      = {J_IJMLC},
  author       = {Tan, Yanchao and Lv, Hang and Huang, Xinyi and Ma, Guofang and Chen, Chaochao},
  doi          = {10.1007/s13042-025-02584-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5553-5567},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Profiling users with tag-enhanced spherical metric learning for recommendation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy levenberg marquart optimization algorithm with inexact line search technique to solve imprecisely defined nonlinear unconstrained optimization problems. <em>IJMLC</em>, <em>16</em>(9), 5527-5551. (<a href='https://doi.org/10.1007/s13042-025-02583-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a fuzzy inexact Levenberg–Marquardt optimization (FILMO) algorithm with a descent direction to handle the nonlinear systems influenced by the uncertain parameters. The main feature of this proposed inexact algorithm is to use the Armijo-type step size search approach via an uncertain environment. The level of inexactness search direction is controlled through the descent direction of the merit function. We establish the convergence analysis of the FILMO algorithm under the assumption of local error bound. Then, the global convergence of the FILMO algorithm is described. The FILMO algorithm is constructed using fuzzy parameters with an Armijo-type step size approach. Numerical examples are illustrated to investigate the effectiveness and efficiency of the algorithm. Then, the comparison is done with a previously existing conjugate gradient modified Fletcher–Reeves method and fuzzy inner outer direct search (FIODS) method. Furthermore, to quantify the uncertainties and sensitivity of the system, fuzzy and fully fuzzy systems are investigated through a case study.},
  archive      = {J_IJMLC},
  author       = {Panigrahi, Paresh Kumar and Nayak, Sukanta},
  doi          = {10.1007/s13042-025-02583-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5527-5551},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Fuzzy levenberg marquart optimization algorithm with inexact line search technique to solve imprecisely defined nonlinear unconstrained optimization problems},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep interactive query design and progressive search for end-to-end detection of tiny object in aerial images. <em>IJMLC</em>, <em>16</em>(9), 5509-5525. (<a href='https://doi.org/10.1007/s13042-025-02582-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting tiny objects in aerial images has always been a perennial challenge in computer vision. The tiny objects contain limited pixel representation and are susceptible to background noise, making accurate detection difficult. This paper proposes a novel framework for the detection of tiny objects, which is guided by an end-to-end query-based approach. There are two inherent drawbacks of previous query-based detectors: first, query-based detectors are inherently insensitive to the detection of tiny objects; second, the performance of the detectors gradually saturates as the network depth increases. This paper proposes a novel approach to solve the above problems by leveraging the property that queries are one-to-one label assignment rules. Specifically, the interactive query prediction selector first selects queries with high confidence scores as acceptable queries. Then, a contrastive learning information extractor is used to progressively assign samples to the accepted and improved noisy queries. Finally, a progressive search is used to generate refined prediction anchor boxes. We conducted extensive experiments on three publicly available aerial image datasets, namely DOTA, VisDrone, and AI-TOD, to demonstrate the usefulness and robustness of our proposed method. The results show that our proposed method yields significant performance improvements in the tiny object detection task, outperforming the existing benchmark models.},
  archive      = {J_IJMLC},
  author       = {Jin, Chuan and Zheng, Anqi and Wu, Zhaoying and Tong, Changqing},
  doi          = {10.1007/s13042-025-02582-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5509-5525},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Deep interactive query design and progressive search for end-to-end detection of tiny object in aerial images},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced toolface angle control of stabilized platform using I_DDPG in rotary steerable system. <em>IJMLC</em>, <em>16</em>(9), 5493-5507. (<a href='https://doi.org/10.1007/s13042-025-02581-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An improved deep deterministic policy gradient (I_DDPG) algorithm is developed to enhance the accuracy and robustness of toolface angle control in the stabilized platform of a rotary steerable system. The intrinsic frictional torques of the platform are analyzed, and the stability of the DDPG-controlled system is verified through a Lyapunov function. To address value overestimation and minimize error accumulation, a clipped double Q-learning strategy with a delayed update mechanism is integrated into the DDPG framework. Additionally, the maximum entropy principle is employed to enhance exploration capabilities, leading to the I_DDPG algorithm. Comparative simulation results show that I_DDPG outperforms traditional methods, reducing tracking error by 40.64%, increasing response speed by 78.31%, and significantly reducing overshoot by 97.33% compared to the PID algorithm. Furthermore, the algorithm demonstrates strong robustness and adaptability, effectively mitigating the effects of variations in armature resistance and viscous friction coefficient. This approach provides a reliable solution for precise toolface angle control in complex and dynamic drilling environments.},
  archive      = {J_IJMLC},
  author       = {Huo, Aiqing and Zhang, Kun and Jiang, Xue},
  doi          = {10.1007/s13042-025-02581-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5493-5507},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Enhanced toolface angle control of stabilized platform using I_DDPG in rotary steerable system},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining adaptive local aggregation average and test-time energy adaptation for federated learning. <em>IJMLC</em>, <em>16</em>(9), 5465-5492. (<a href='https://doi.org/10.1007/s13042-025-02580-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated test-time adaptation (FTTA) aims to adapt knowledge from diverse source models to different but related unlabeled target data in an online and privacy-aware manner. However, existing FTTA methods struggle with decreased adaptation performance caused by data distribution shifts among clients. In this paper, we propose an FTTA method (ALAA-TTEA) called adaptive local aggregation average and test-time energy adaptation. The method consists of two continuous stages. First, in the federated aggregation stage, clients adaptively aggregate the global model and local model through adaptive local aggregation (ALA) to initialize the client model. Then, they obtain the global model through personalized training and model averaging. Second, in the test-time adaptation stage, test-time energy adaptation (TTEA) uses an energy function to transform the global model into an energy-based model. It aligns the model distribution with the test data distribution, thereby enhancing the model’s ability to adapt to the test distribution and improving overall performance. Extensive experiments demonstrate that ALAA-TTEA effectively handles data distribution shifts, including feature shift, label shift, hybrid shift, and domain shift. Moreover, it consistently outperforms existing FTTA methods under most conditions.},
  archive      = {J_IJMLC},
  author       = {Liao, Juxin and Yi, Chang’an and Chen, Kai and Peng, Qiaoyi},
  doi          = {10.1007/s13042-025-02580-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5465-5492},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Combining adaptive local aggregation average and test-time energy adaptation for federated learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical structure analysis of linguistic expressions using concept lattice. <em>IJMLC</em>, <em>16</em>(9), 5441-5464. (<a href='https://doi.org/10.1007/s13042-025-02579-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing with words has proven to be a valuable tool for directly processing linguistic information. However, due to the different states of objects at different times or places, how to dynamically obtain the relations and hierarchical structures of linguistic expressions in different contexts is always a challenge. This paper proposes a computing with linguistic expressions (CWLE) model based on linguistic concept lattices to address this challenge. To handle uncertainty in linguistic expressions, interval type-2 fuzzy sets are first employed to model them, with an initial order established via the centroid mean, enabling flexible adaptation to varied contexts. Second, the linguistic label formal context automates fuzzy set generation, while a fuzzy linguistic-valued lattice is constructed based on the similarity and hierarchical relationships among linguistic expressions. In addition, a hierarchical generation algorithm further captures complex contextual relationships. Finally, comparative analysis demonstrates the CWLE model’s effectiveness in accurately representing the hierarchical structure of linguistic expressions.},
  archive      = {J_IJMLC},
  author       = {Pang, Kuo and Martínez, Luis and Liu, Jun and Zou, Li and Lu, Mingyu},
  doi          = {10.1007/s13042-025-02579-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5441-5464},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Hierarchical structure analysis of linguistic expressions using concept lattice},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Periodic frequent subgraph mining in dynamic graphs. <em>IJMLC</em>, <em>16</em>(9), 5419-5439. (<a href='https://doi.org/10.1007/s13042-025-02578-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because most data in the real world can be represented in graph structures, graph mining is essential in many fields. In recent decades, research on algorithms for mining frequent subgraphs in graph databases has matured. In addition to frequent patterns, periodic and closed patterns are important in real life. However, the definition of periodic patterns in the precedent study is rigorous and has certain limitations, so that many approximate periodic patterns cannot be mined. Thus, based on the study of periodic patterns in itemset databases, we define periodic frequent subgraphs (PFSs) in dynamic graphs using the three measures: average periodicity, maximum periodicity, and minimum periodicity. The task of PFS mining is to discover all the PFSs in a given database. We propose the PFS Miner (PFSM) algorithm to realize this task. In addition, we propose closed PFSs (CPFSs) and the corresponding mining algorithm named CPFS Miner (CPFSM) to make the excavated periodic frequent patterns more concise. Finally, we conduct experiments on real datasets to analyze the performance of the algorithms. The experimental results indicate that the PFSM algorithm can mine meaningful periodic frequent patterns and CPFSM can mine closed periodic frequent patterns with good efficiency and performance.},
  archive      = {J_IJMLC},
  author       = {Cai, Jiayu and Chen, Zhaoming and Chen, Guoting and Gan, Wensheng and Broustet, Amaël},
  doi          = {10.1007/s13042-025-02578-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5419-5439},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Periodic frequent subgraph mining in dynamic graphs},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on dynamic flatness feedback control strategy based on IGWO control efficiency identification for cold tandem rolling mill. <em>IJMLC</em>, <em>16</em>(9), 5397-5417. (<a href='https://doi.org/10.1007/s13042-025-02577-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flatness control is an important means to improve the quality of the strip, and modern tandem cold rolling mills are equipped with various control actuators. The existing flatness control efficiency is determined offline and remains constant throughout the manufacturing process. During actual manufacturing, equipment, and process states may change, making it necessary to identify flatness control efficiency online. The main flatness feedback control strategy is a fixed priority sequence in the existing CVC (Continuous Variable Crown) cold tandem rolling mill. The control system does not have the flexibility to adjust the sequence based on real-time flatness. Aiming at the above problems, in order to improve the effect of flatness control, this paper proposes a dynamic flatness feedback control strategy based on control efficiency identification. To address the issue of high cost and difficulty in updating the on-site flatness control efficiency, based on historical actual data, the IGWO (improved grey wolf optimization) algorithm is used to intelligently identify control efficiency. Data post-processing through IF (isolated forests) and CLT (central limit theorem) obtains results on control efficiency. At the same time, a similarity index is proposed to dynamically adjust the priority sequence of feedback control based on the identification results of the control efficiency and the measured flatness deviations. Finally, the field application results show that the flatness control accuracy is improved, the reduction value of the flatness deviations increased by 13.41%, and the reduction rate of the flatness deviations increased by 9.66%.},
  archive      = {J_IJMLC},
  author       = {Zhou, Xiaomin and Li, Liqi and Wang, Shuaikun and Xiong, Qingxia},
  doi          = {10.1007/s13042-025-02577-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5397-5417},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Research on dynamic flatness feedback control strategy based on IGWO control efficiency identification for cold tandem rolling mill},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selections based on fuzzy probability dominance rough sets in interval-valued ordered decision systems. <em>IJMLC</em>, <em>16</em>(9), 5365-5395. (<a href='https://doi.org/10.1007/s13042-025-02562-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selections (FSs) can greatly reduce the dimensionality and complexity of data, improving the efficiency of data mining and classification learning. For interval-valued ordered decision systems (IVODSs), FSs rely on dominance degrees and information measures; however, there are few selection algorithms based on fusion measurements and few semantic analyses of dominance degrees. Around IVODSs, this paper proposes fuzzy probability dominance rough sets (FPDRSs), an information measurement system and a fusion measure, and thus it constructs a systemic FSs framework. Firstly, we utilize a probability density function to propose the fuzzy probability dominance degree (FPD) that can deeply characterize the fuzzy probability dominance relation (FPDR) between any ordered interval values, and define fuzzy probability dominance dual approximations and dependency (FPDD), so FPDRSs are constructed. Then, the fuzzy probability dominance information entropy, conditional entropy (FPDCE), joint entropy and mutual information are obtained to constitute an information measurement system. Furthermore, a fuzzy probability dominance dependency-conditional entropy (FPDDCE) is defined. In addition, the monotonicity and nonmonotonicity of uncertainty measures are studied. Afterwards, three algorithms FPDD-FS, FPDCE-FS and FPDDCE-FS are constructed by using FPDD, FPDCE and FPDDCE, where attribute significance is used for heuristic searches. Finally, the effectiveness of the proposed uncertainty measures is verified through data experiments, and three proposed algorithms achieve better classification performance than six comparative algorithms.},
  archive      = {J_IJMLC},
  author       = {Liu, Xia and Zhang, Xianyong and Chen, Benwei},
  doi          = {10.1007/s13042-025-02562-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {9},
  number       = {9},
  pages        = {5365-5395},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Feature selections based on fuzzy probability dominance rough sets in interval-valued ordered decision systems},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="invent">INVENT - 6</h2>
<ul>
<li><details>
<summary>
(2025). Counting pseudo-anosovs as weakly contracting isometries. <em>INVENT</em>, <em>242</em>(1), 337-386. (<a href='https://doi.org/10.1007/s00222-025-01358-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that pseudo-Anosov mapping classes are generic in every Cayley graph of the mapping class group of a finite-type hyperbolic surface. Our method also yields an analogous result for rank-one CAT(0) groups and hierarchically hyperbolic groups with Morse elements. Finally, we prove that Morse elements are generic in every Cayley graph of groups that are quasi-isometric to (well-behaved) hierarchically hyperbolic groups. This gives a quasi-isometry invariant theory of counting group elements in groups beyond relatively hyperbolic groups.},
  archive      = {J_INVENT},
  author       = {Choi, Inhyeok},
  doi          = {10.1007/s00222-025-01358-5},
  journal      = {Inventiones Mathematicae},
  month        = {10},
  number       = {1},
  pages        = {337-386},
  shortjournal = {Invent. Math.},
  title        = {Counting pseudo-anosovs as weakly contracting isometries},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compatibility of canonical $\ell $ -adic local systems on adjoint shimura varieties. <em>INVENT</em>, <em>242</em>(1), 305-335. (<a href='https://doi.org/10.1007/s00222-025-01357-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a Shimura variety $(G, X)$ in the superrigid regime and neat level subgroup $K_{0}$ , we show that the canonical family of $\ell $ -adic representations associated to a number field point $y \in \mathrm{Sh}_{K_{0}}(G, X)(F)$ , $$ \left \{ \rho _{y, \ell } \colon \mathrm{Gal}(\overline{\mathbb{Q}}/F) \to G^{\mathrm{ad}}(\mathbb{Q}_{\ell }) \right \} _{\ell }, $$ form a compatible system of $G^{\mathrm{ad}}(\mathbb{Q}_{\ell })$ -representations: there is an integer $N(y)$ such that for all $\ell $ , $\rho _{y, \ell }$ is unramified away from $N(y) \ell $ , and for all $\ell \neq \ell '$ and $v \nmid N(y)\ell \ell '$ , the semisimple parts of the conjugacy classes of $\rho _{y, \ell }(\mathrm{Frob}_{v})$ and $\rho _{y, \ell '}(\mathrm{Frob}_{v})$ are (ℚ-rational and) equal. We deduce this from a stronger compatibility result for the canonical $G(\mathbb{Q}_{\ell })$ -valued local systems on connected Shimura varieties inside $\mathrm{Sh}_{K_{0}}(G, X)$ . Our theorems apply in particular to Shimura varieties of non-abelian type and represent the first such independence-of- $\ell $ results in non-abelian type.},
  archive      = {J_INVENT},
  author       = {Klevdal, Christian and Patrikis, Stefan},
  doi          = {10.1007/s00222-025-01357-6},
  journal      = {Inventiones Mathematicae},
  month        = {10},
  number       = {1},
  pages        = {305-335},
  shortjournal = {Invent. Math.},
  title        = {Compatibility of canonical $\ell $ -adic local systems on adjoint shimura varieties},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global solutions for 1D cubic dispersive equations, part III: The quasilinear schrödinger flow. <em>INVENT</em>, <em>242</em>(1), 221-304. (<a href='https://doi.org/10.1007/s00222-025-01356-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first target of this article is the local well-posedness question for 1D quasilinear Schrödinger equations with cubic nonlinearities. The study of this class of problems, in all dimensions, was initiated in pioneering work of Kenig-Ponce-Vega for localized initial data, and then continued by Marzuola-Metcalfe-Tataru for initial data in Sobolev spaces. Our objective here is to fully redevelop the study of this problem in the 1D case, and to prove a sharp local well-posedness result. The second goal of this article is to consider the long-time/global existence of solutions for the same problem. This is motivated by a broad conjecture formulated by the authors in earlier work, which reads as follows: “Cubic defocusing dispersive one dimensional flows with small initial data have global dispersive solutions”; the conjecture was initially proved for a class of semilinear Schrödinger type models. Our work here establishes the above conjecture for 1D quasilinear Schrödinger flows. Precisely, we show that if the problem has phase rotation symmetry and is conservative and defocusing, then small data in Sobolev spaces yields global, scattering solutions. This is the first result of this type for 1D quasilinear dispersive flows where no localization condition is imposed on the data. Furthermore, we prove the global well-posedness at the minimal Sobolev regularity as in our local well-posedness result. The defocusing condition is essential in our global result. Without it, the authors have conjectured that small, $\epsilon $ size data yields long-time solutions on the $\epsilon ^{-8}$ time-scale. A third goal of this paper is to also prove this second conjecture for 1D quasilinear Schrödinger flows, also at minimal regularity.},
  archive      = {J_INVENT},
  author       = {Ifrim, Mihaela and Tataru, Daniel},
  doi          = {10.1007/s00222-025-01356-7},
  journal      = {Inventiones Mathematicae},
  month        = {10},
  number       = {1},
  pages        = {221-304},
  shortjournal = {Invent. Math.},
  title        = {Global solutions for 1D cubic dispersive equations, part III: The quasilinear schrödinger flow},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The 27 geodesic networks in the directed landscape. <em>INVENT</em>, <em>242</em>(1), 123-220. (<a href='https://doi.org/10.1007/s00222-025-01355-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The directed landscape is a random directed metric on the plane that arises as the scaling limit of classical metric models in the KPZ universality class. Typical pairs of points in the directed landscape are connected by a unique geodesic. However, there are exceptional pairs of points connected by more complicated geodesic networks. We show that, up to isomorphism, exactly 27 geodesic networks exist in the directed landscape. We also find Hausdorff dimensions in a scaling-adapted metric on $\mathbb{R}^{4}_{\uparrow }$ for the sets of endpoints of each of these networks.},
  archive      = {J_INVENT},
  author       = {Dauvergne, Duncan},
  doi          = {10.1007/s00222-025-01355-8},
  journal      = {Inventiones Mathematicae},
  month        = {10},
  number       = {1},
  pages        = {123-220},
  shortjournal = {Invent. Math.},
  title        = {The 27 geodesic networks in the directed landscape},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extensions of characters in type d and the inductive McKay condition, II. <em>INVENT</em>, <em>242</em>(1), 45-122. (<a href='https://doi.org/10.1007/s00222-025-01354-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We determine the action of the automorphism group $\mathrm {Aut}(G)$ on the set of irreducible characters ${\mathrm {Irr}}(G)$ for all finite quasi-simple groups $G$ . For groups of Lie type, this includes the construction of an $\mathrm {Aut}(G)$ -equivariant Jordan decomposition of characters (Theorem B). We prove a property called ${{A}(\infty )}$ which includes an extendibility statement, known previously in all types not $\mathrm {D} $ (Theorem A). Our methods blend here Shintani descent ideas introduced for type $\mathrm{B}$ with an analysis of semisimple classes in the dual group $G^{*}$ . The property ${{A}(\infty )}$ originates in the program to prove the McKay conjecture using the classification of finite simple groups. Theorem C establishes the McKay conjecture for the prime 3.},
  archive      = {J_INVENT},
  author       = {Späth, Britta},
  doi          = {10.1007/s00222-025-01354-9},
  journal      = {Inventiones Mathematicae},
  month        = {10},
  number       = {1},
  pages        = {45-122},
  shortjournal = {Invent. Math.},
  title        = {Extensions of characters in type d and the inductive McKay condition, II},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularity of singular set in optimal transportation. <em>INVENT</em>, <em>242</em>(1), 1-44. (<a href='https://doi.org/10.1007/s00222-025-01353-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we establish a regularity theory for the optimal transport problem when the target is composed of two disjoint convex domains. This is an important model in which singularities arise. Even though the singular set does not exhibit any form of convexity a priori, we prove its higher order regularity by developing novel methods, which also have many other applications. Notably, our results are achieved without requiring any convexity of the source domain. This aligns with Caffarelli’s celebrated regularity theory.},
  archive      = {J_INVENT},
  author       = {Chen, Shibing and Liu, Jiakun},
  doi          = {10.1007/s00222-025-01353-w},
  journal      = {Inventiones Mathematicae},
  month        = {10},
  number       = {1},
  pages        = {1-44},
  shortjournal = {Invent. Math.},
  title        = {Regularity of singular set in optimal transportation},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jamc">JAMC - 50</h2>
<ul>
<li><details>
<summary>
(2025). Finite volume scheme for nonlinear image denoising. <em>JAMC</em>, <em>71</em>(4), 5817-5842. (<a href='https://doi.org/10.1007/s12190-025-02462-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the numerical analysis of image denoising equation descended from the Perona-Malik equation, with the addition of a Leray-Lions operator. Utilizing established a priori estimates, we showcase the convergence of the approximate solutions through the application of compactness arguments tailored to address degenerate parabolic problems. Some numerical results are presented using example images demonstrating the efficacy and robustness of our model.},
  archive      = {J_JAMC},
  author       = {El Moutaouakil, Hicham and Rhoudaf, Mohamed},
  doi          = {10.1007/s12190-025-02462-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5817-5842},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Finite volume scheme for nonlinear image denoising},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Some new operators based on touchard polynomials. <em>JAMC</em>, <em>71</em>(4), 5801-5816. (<a href='https://doi.org/10.1007/s12190-025-02452-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present paper provides the study on the composition operators due to Szász-Mirakyan with the generalized Szász-Durrmeyer operators. We get an interesting new operator based on Touchard polynomials. Also, the new operator can be further decomposed into two different operators. This study is extension of the recent work, where we have discussed such compositions with reverse order. We find some convergence behavior of these composition operators in sense of point-wise estimates and quantitative estimates in terms of modulus of continuity. We also provide some graphs to have convergence visualization for different n.},
  archive      = {J_JAMC},
  author       = {Gupta, Vijay},
  doi          = {10.1007/s12190-025-02452-w},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5801-5816},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Some new operators based on touchard polynomials},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fractional boole’s inequalities for twice differentiable functions for Riemann–Liouville fractional integrals. <em>JAMC</em>, <em>71</em>(4), 5781-5800. (<a href='https://doi.org/10.1007/s12190-025-02465-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boole’s type inequalities for first-order differentiable convex functions have been extensively studied, particularly in the context of numerical integration and fractional calculus. However, such inequalities for twice-differentiable functions remain relatively unexplored. This paper addresses this gap by establishing a novel Boole’s identity via the Riemann–Liouville fractional integral for twice-differentiable functions. Twice-differentiable functions allow for the use of second-order derivatives, which provide more precise information about the curvature and behavior of functions compared to first-order derivatives. This leads to sharper inequalities and better numerical approximations. Based on this identity, several fractional Boole’s type inequalities are derived, where the absolute value of the second derivatives is convex. The results include detailed error bounds, highlighting the effectiveness of the proposed approach in practical computations. Illustrative examples are provided to validate the theoretical findings and demonstrate their computational applicability.},
  archive      = {J_JAMC},
  author       = {Shehzadi, Asia and Budak, Hüseyin and Haider, Wali and Mateen, Abdul and Chen, Haibo},
  doi          = {10.1007/s12190-025-02465-5},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5781-5800},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Fractional boole’s inequalities for twice differentiable functions for Riemann–Liouville fractional integrals},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical and mathematical analysis of the novel fractal–fractional komarova’s coupled system for the bone mineralization: Existence and uniqueness of solution. <em>JAMC</em>, <em>71</em>(4), 5757-5779. (<a href='https://doi.org/10.1007/s12190-025-02472-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bone mineralization is a crucial physiological process that ensures skeletal strength and integrity, playing a vital role in bone development, repair, and overall health. Using the mathematical framework developed by Komarova in 2015, this manuscript attempts to investigate the practical implications of combining Fractal–Fractional ( $$\mathcal{F}\mathcal{F}$$ ) operators in mathematical modeling, with a particular focus on the setting of bone mineralization. Through investigating the potential advantages of fractal operators, we develop a more precise and sophisticated mathematical model, evaluate the existence and uniqueness of solutions, Ulam–Hyers (UH) stability, and present numerical results that highlight the improved performance offered by this innovative approach. Lagrange’s two-step method has been used to estimate solutions for the $$\mathcal{F}\mathcal{F}$$ order bone mineralization model. MATLAB software is used to perform the numerical simulations, which provide insightful information on the dynamics of the model. Our simulations suggest early detection of the mineralization.},
  archive      = {J_JAMC},
  author       = {Agarwal, Ritu and Airan, Pooja and Baskonus, Haci Mehmet},
  doi          = {10.1007/s12190-025-02472-6},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5757-5779},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Numerical and mathematical analysis of the novel fractal–fractional komarova’s coupled system for the bone mineralization: Existence and uniqueness of solution},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mathematical model for assessing the effectiveness of vaccination in controlling mpox dynamics and mitigating disease burden in nigeria and the democratic republic of congo. <em>JAMC</em>, <em>71</em>(4), 5729-5756. (<a href='https://doi.org/10.1007/s12190-025-02455-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study outlines the efforts to provide a comprehensive understanding of the impact of vaccination on the control of Mpox. The primary goal is to investigate the effects of vaccine characteristics on the Mpox burden in Nigeria and the DRC and to estimate the number of cases and deaths averted in each country due to vaccination. To this end, an extended Mpox model incorporating a vaccinated population is presented, and a sensitivity analysis was conducted using the partial rank correlation coefficient to identify the most significant parameters affecting the control reproduction number, the infected human population, and the deceased human population. The criteria for the global stability of the Mpox-free equilibrium were established using the Lyapunov function, and a numerical simulation was performed to estimate the number of cases and deaths that could be averted through the implementation of vaccination in both countries. We examined the Mpox mortality burden across different age groups in Nigeria and the DRC using Years of Life Lost (YLL) as a metric. The study emphasizes the severity of Mpox in this demographic segment by indicating that older age groups would have a lower fatality rate. Also, the estimate suggested that approximate of 40 years and 43 years can be lost per individual due to Mpox in Nigeria and DRC respectively. Furthermore, the probability of a major outbreak in these regions was estimated, along with the required herd immunity threshold needed to curtail the spread of the disease. The results of this study suggest that vaccination significantly impacts the transmission and mortality rates of Mpox in Nigeria and the DRC, emphasizing that achieving high vaccination coverage is crucial for reaching the herd immunity threshold and preventing outbreaks. These findings underscore the considerable benefits of vaccination in controlling Mpox transmission and reducing the disease burden in Nigeria and the DRC. By implementing robust vaccination programs and proactively addressing associated challenges, we can work toward achieving herd immunity and preventing Mpox outbreaks not only in these countries but also in other regions where the disease remains a threat.},
  archive      = {J_JAMC},
  author       = {Peter, Olumuyiwa James and Babasola, Oluwatosin and Ojo, Mayowa Micheal and Omame, Andrew},
  doi          = {10.1007/s12190-025-02455-7},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5729-5756},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A mathematical model for assessing the effectiveness of vaccination in controlling mpox dynamics and mitigating disease burden in nigeria and the democratic republic of congo},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exponential stability analysis of markovian jumping switched cellular neural networks via memory-event-triggered control. <em>JAMC</em>, <em>71</em>(4), 5697-5727. (<a href='https://doi.org/10.1007/s12190-025-02450-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on investigating Markovian jump-switched cellular neural networks (MJSCNNs) using an improved memory event-triggered control approach. The proposed memory event-triggered scheme (METS) has distinct advantages. The information of certain recent released signals are first utilized, which helps to improve the triggering instants and we design a time-varying, state-dependent threshold parameter that adjusts the packet transmission rate based on state information. We formulate a suitable Lyapunov function to demonstrate exponential stability by utilizing integral inequality approaches. The establishment of linear matrix inequalities facilitates the construction of an improved METS co-designed for MJSCNNs. Furthermore, using the average dwell-time method, a collection of adequate criteria. We provided numerical examples to validate and demonstrate the efficacy of our theoretical findings.},
  archive      = {J_JAMC},
  author       = {Suresh, R. and Meiyanathan, M. and Vadivel, R. and Saravanan, S.},
  doi          = {10.1007/s12190-025-02450-y},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5697-5727},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Exponential stability analysis of markovian jumping switched cellular neural networks via memory-event-triggered control},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics of re-infection in a hepatitis b virus epidemic model with constant vaccination and preventive measures. <em>JAMC</em>, <em>71</em>(4), 5669-5695. (<a href='https://doi.org/10.1007/s12190-025-02431-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hepatitis B Virus (HBV) remains a global health challenge, with re-infection adding complexity to control efforts. This article introduces a novel epidemiological model that incorporates re-infection dynamics with a Crowley–Martin incidence rate, along with the effects of constant vaccination and preventive measures. The model is structured into susceptible, infected, and recovered compartments, and its dynamic properties are rigorously analyzed. Key aspects such as the basic reproduction number $$(R_n)$$ , stability conditions, boundedness, non-negativity, and bifurcation behavior are explored. Using Lyapunov function, we establish the globally asymptotic stability of Hepatitis B Virus-free equilibrium for $$R_n < 1$$ and identify a transcritical bifurcation at $$R_n = 1$$ . Additionally, we demonstrate that the endemic equilibrium is globally asymptotic stable for $$R_n>1$$ . The results of our numerical simulations, which employ NSFD scheme, validate our findings and demonstrate its superior efficiency when compared to RK4 and Euler methods. Specifically, NSFD produces faster and more accurate results. Our findings emphasize the critical role of vaccination, re-infection and preventive measures in HBV transmission dynamics, offering valuable perspective for public health strategies aimed at controlling HBV spread.},
  archive      = {J_JAMC},
  author       = {Megala, Thangavel and Nandha Gopal, Thangaraj and Siva Pradeep, Manickasundaram and Sivabalan, Muthurathinam and Yasotha, Arunachalam},
  doi          = {10.1007/s12190-025-02431-1},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5669-5695},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Dynamics of re-infection in a hepatitis b virus epidemic model with constant vaccination and preventive measures},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics and optimal control of an SIQR epidemic model with vaccination and individual feedback on networks. <em>JAMC</em>, <em>71</em>(4), 5651-5668. (<a href='https://doi.org/10.1007/s12190-025-02453-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we develop an SIQR epidemic network model incorporating individual feedback mechanisms, vaccination strategies, and quarantine measures. Through rigorous mathematical analysis, we derive the basic reproduction number R0 and establish that the disease-free equilibrium is globally asymptotically stable when R0 < 1, while the endemic equilibrium is globally asymptotically stable when R0 > 1. Utilizing optimal control theory, we prove the existence and uniqueness of the optimal control solution, which is obtained using Pontryagin’s minimum principle and Hamiltonian theory. This approach significantly reduces control costs and achieves efficient and cost-effective epidemic containment. Furthermore, the model optimizes the allocation of prevention and control resources under constrained conditions, enhancing control efficiency while alleviating socioeconomic burdens, thereby offering a sustainable framework for long-term epidemic management. Numerical simulations validate the model’s stability and the effectiveness of the optimal control strategy, demonstrating that vaccination and quarantine measures effectively suppress epidemic spread. These findings provide robust guidance for practical epidemic prevention and control, enabling health authorities and policymakers to implement targeted measures, such as prioritizing vaccine distribution, determining optimal coverage rates, and strategically planning quarantine zones and durations. Such measures effectively curb the transmission of infectious diseases and safeguard public health.},
  archive      = {J_JAMC},
  author       = {Fu, Tingting and Li, Si and Liu, Maoxing},
  doi          = {10.1007/s12190-025-02453-9},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5651-5668},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Dynamics and optimal control of an SIQR epidemic model with vaccination and individual feedback on networks},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Limit cycles for a kind of generalized hill differential equation. <em>JAMC</em>, <em>71</em>(4), 5631-5649. (<a href='https://doi.org/10.1007/s12190-025-02442-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this manuscript, we apply averaging theory to investigate the maximum number of limit cycles in a type of generalized Hill differential equation. $$\mathop x\limits^{..} + \varepsilon (1 + \mathop {\mathop {\sum }\limits^m }\limits_{k = 0} \left( {\matrix{ m \cr k \cr } } \right)\mathop {\sin }\nolimits^k \theta \mathop {\cos }\nolimits^{m - k} \theta ))P(x,y) + x = 0,$$ that branch out from the periodic orbits of the linear center $$\dot x = y,\dot y = - x.$$ Here $$\varepsilon > 0$$ is a small parameter, $$\left( {\matrix{m \cr k \cr } } \right) = {{m!} \over {k!(m - k)!}},$$ P is a polynomial of degree $$n,$$ and $$m$$ is an arbitrary non-negative integer, and $$\theta = \arctan (\frac{y}{x})$$ .},
  archive      = {J_JAMC},
  author       = {Kina, Abdelkrim and Moumen, Abdelkader and Boulares, Hamid and Bouye, Mohamed},
  doi          = {10.1007/s12190-025-02442-y},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5631-5649},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Limit cycles for a kind of generalized hill differential equation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient modified two-grid algorithm upon MAC scheme for simulating wormhole propagation with the darcy-brinkman-forchheimer framework. <em>JAMC</em>, <em>71</em>(4), 5589-5630. (<a href='https://doi.org/10.1007/s12190-025-02443-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an efficient algorithm is presented for wormhole propagation under the Darcy-Brinkman-Forchheimer (DBF) framework. A modified-upwind scheme is provided for the solute transport equation in the model to avoid the numerical oscillation. The marker and cell (MAC) method is devoted for spatial discretization to reach second-order error estimates in space. The second-order backward difference formula (BDF2) is proposed to improve time accuracy to second-order. In order to handle nonlinear terms and improve computational efficiency, the modified two-grid algorithm is applied. A small positive parameter $$\varepsilon $$ is presented to transform a non-differentiable nonlinear term into a quadratic continuous differentiable term. The modified two-grid algorithm is used to achieve second-order error estimates of the pressure, velocity, porosity, concentration and auxiliary flux. The effectiveness of the second-order modified two-grid algorithm is verified through some numerical experiments. By comparing with traditional algorithm upon MAC scheme with the BDF2 method, the efficiency and accuracy of the algorithm is illustrated.},
  archive      = {J_JAMC},
  author       = {Wang, Huishan and Liu, Wei and Wang, Pengshan},
  doi          = {10.1007/s12190-025-02443-x},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5589-5630},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {An efficient modified two-grid algorithm upon MAC scheme for simulating wormhole propagation with the darcy-brinkman-forchheimer framework},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and analyzing the dynamics of brucellosis disease with vaccination in the fractional derivative under real cases. <em>JAMC</em>, <em>71</em>(4), 5567-5588. (<a href='https://doi.org/10.1007/s12190-025-02435-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present explores the brucellosis model in non-integer derivative by utilizing the real statistics from the mainland China. The formulation of the model first presented in integer order derivative and subsequently extended to fractional order using the Caputo derivative. The existence and uniqueness of the nonlinear fractional system is confirmed, which is the important requirement for a fractional nonlinear model. The local asymptotical stability of the fractional model when $$\mathcal {R}_0<1$$ is analyzed. When $$\mathcal {R}_0\le 1$$ , the model is found globally asymptotically stable. The existence of an endemic equilibria is given and found that the model has a unique endemic equilibrium. Using the reported cases of brucellosis in mainland China from 2004 to 2018 are considered. Graphical results for data fitting in cumulative and daily wise are presented with their respective residuals. The basic reproduction number is obtained from data fitting is $$\mathcal {R}_0=1.0327$$ . A numerical scheme for the Caputo case is provided in detailed and later the scheme was used to obtain the numerical results graphically. Various results regarding the disease curtail are presented graphically, that will be helpful for the disease elimination in the long run. The public health authority and the health agencies can utilize this work confidently for brucellosis control in mainland China.},
  archive      = {J_JAMC},
  author       = {Al-Hdaibat, Bashir and Khan, Muhammad Altaf and Ahmad, Irfan and Alzahrani, Ebraheem and Akgul, Ali},
  doi          = {10.1007/s12190-025-02435-x},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5567-5588},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Modeling and analyzing the dynamics of brucellosis disease with vaccination in the fractional derivative under real cases},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel high-accuracy numerical approach for initial value problems of fractional order with nonsmooth solutions. <em>JAMC</em>, <em>71</em>(4), 5555-5565. (<a href='https://doi.org/10.1007/s12190-025-02437-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel high-accuracy numerical approach is presented for Caputo fractional order initial value problems with nonsmooth solutions. Due to the nonsmoothness of solutions, the numerical treatment of such problems is difficult. Firstly, a variable transformation is introduced to transform the original problem to an integral-differential equation with smooth solution. Then a high-accuracy numerical method is proposed for the transformed problem. The present approach is verified by three numerical experiments.},
  archive      = {J_JAMC},
  author       = {Li, Xiuying and Wu, Boying},
  doi          = {10.1007/s12190-025-02437-9},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5555-5565},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A novel high-accuracy numerical approach for initial value problems of fractional order with nonsmooth solutions},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-inclusive g-extra diagnosability of regular networks under the MM* model. <em>JAMC</em>, <em>71</em>(4), 5531-5553. (<a href='https://doi.org/10.1007/s12190-025-02445-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault tolerance diagnosis plays a crucial role in maintaining the safe operation of interconnection networks. In 2024, Zheng et al. introduced the concept of non-inclusive $$g$$ -extra diagnosability, a novel measure for assessing network reliability, and provided a solution for determining the non-inclusive $$g$$ -extra diagnosability of network graphs under the PMC model. However, this method is applicable to fewer types of networks. In this paper, we present a general method for determining the non-inclusive 1-extra diagnosability of regular networks. This method has broad applications and is suitable for various well-known networks, such as data center networks, $$k$$ -ary $$n$$ -cube, hypercube-like networks, DQcube, dual-cube, and others.},
  archive      = {J_JAMC},
  author       = {Zhuo, Nengjin and Zhang, Shumin and Chang, Jou-Ming and Ye, Chengfu},
  doi          = {10.1007/s12190-025-02445-9},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5531-5553},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Non-inclusive g-extra diagnosability of regular networks under the MM* model},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics of fuzzy difference equations system with higher-order. <em>JAMC</em>, <em>71</em>(4), 5505-5530. (<a href='https://doi.org/10.1007/s12190-025-02440-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are few studies in the literature that focus on two-dimensional higher-order fuzzy difference equations, leaving a considerable gap in our understanding of their behavior and dynamics. This highlights the necessity to investigate this field in order to answer fundamental concerns and broaden its possible uses.This study looks into the existence, uniqueness, boundedness, persistence, and convergence of positive solutions to a two-dimensional system of higher-order fuzzy difference equations. These qualities are crucial to understanding the system’s behavior and stability.Theoretical analysis is used to rigorously establish the aforementioned system features. To validate the efficiency and application of the theoretical results, numerical simulations are provided, exhibiting the behavior and supporting the study’s findings.},
  archive      = {J_JAMC},
  author       = {Topan, Osman and Yazlik, Yasin and Atpinar, Sevda},
  doi          = {10.1007/s12190-025-02440-0},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5505-5530},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Dynamics of fuzzy difference equations system with higher-order},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Legendre-fourier spectral approximation and error analysis for nonlinear eigenvalue problems in complex domains. <em>JAMC</em>, <em>71</em>(4), 5477-5504. (<a href='https://doi.org/10.1007/s12190-025-02444-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop and analyze an efficient Legendre-Fourier spectral approximation for solving nonlinear eigenvalue problems in complex domains. The main idea is to employ the domain mapping method to convert the nonlinear eigenvalue problem on a complex domain into an equivalent form on a standard circular domain. Based on this, an effective Legendre-Fourier spectral method is implemented by utilizing Legendre polynomials and Fourier series approximations in the radial and tangential directions, respectively. As the initial step, we establish a priori error estimates for standard circular regions. Then, we define a new class of projection operators, demonstrate their approximation properties, and further prove the error estimates for approximating eigenvalues and their corresponding eigenfunctions. Subsequently, by employing region mapping techniques, we extend the algorithm to address nonlinear eigenvalue problems in two-dimensional complex domains, and validate its convergence and spectral accuracy through numerical examples.},
  archive      = {J_JAMC},
  author       = {Zheng, Jihui and An, Jing},
  doi          = {10.1007/s12190-025-02444-w},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5477-5504},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Legendre-fourier spectral approximation and error analysis for nonlinear eigenvalue problems in complex domains},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Innovative numerical method for solving heat conduction using subdivision collocation. <em>JAMC</em>, <em>71</em>(4), 5475-5476. (<a href='https://doi.org/10.1007/s12190-025-02460-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JAMC},
  author       = {Malik, Safia and Tehmina Ejaz, Syeda and Rezapour, Shahram and Inc, Mustafa and Mustafa, Ghulam},
  doi          = {10.1007/s12190-025-02460-w},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5475-5476},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Correction: Innovative numerical method for solving heat conduction using subdivision collocation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error analysis of SDFEM for time-dependent singularly perturbed problem with interior turning point. <em>JAMC</em>, <em>71</em>(4), 5439-5474. (<a href='https://doi.org/10.1007/s12190-025-02416-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a time-dependent singularly perturbed interior turning point problem. For small perturbation parameter, the solution of the considered problem exhibits twin boundary layers due to the presence of interior turning point. The spatial derivatives are discretized by employing the streamline-diffusion finite element method on a Shishkin-type mesh and an implicit finite difference scheme is applied on uniform mesh for the discretization of temporal derivative. The proposed numerical scheme is stabilized and error estimates are obtained by utilizing an appropriate choice of stabilization parameter and the estimates of discrete Green’s function. The robust convergence results are obtained in both the space and time directions in the maximum norm. The validation of theoretical results is accomplished by taking various numerical examples.},
  archive      = {J_JAMC},
  author       = {Aasna and Rai, Pratima},
  doi          = {10.1007/s12190-025-02416-0},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5439-5474},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Error analysis of SDFEM for time-dependent singularly perturbed problem with interior turning point},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uniqueness for an inverse source problem for weakly coupling nonlocal diffusion equations from interior measurement. <em>JAMC</em>, <em>71</em>(4), 5419-5438. (<a href='https://doi.org/10.1007/s12190-025-02383-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on determining the source term in coupled nonlocal diffusion equations from interior domain observations. Through Laplace transform argument, we establish a weak unique continuation principle for the solution of the system. By combining this principle with the Duhamel principle, we establish the uniqueness of the inverse source problem. For numerical simulations, we introduce an iterative threshold algorithm. Various numerical experiments are presented to validate the accuracy and efficiency of the algorithm.},
  archive      = {J_JAMC},
  author       = {Fan, Beibei and Li, Zhiyuan},
  doi          = {10.1007/s12190-025-02383-6},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5419-5438},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Uniqueness for an inverse source problem for weakly coupling nonlocal diffusion equations from interior measurement},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability and boundedness analysis for a system of nonlinear vector delay differential equations. <em>JAMC</em>, <em>71</em>(4), 5401-5418. (<a href='https://doi.org/10.1007/s12190-025-02446-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the stability and boundedness properties of solutions for a system of nonlinear vector delay differential equations (VDDEs) by establishing the conditions under which the solutions are stable and ultimately bounded as t → ∞. The method used is yapunov-Krasovskii’s (L–K), which involves constructing a continuous scalar function related to the differential equations describing the system’s dynamics and delay effects. A numerical example with geometrical arguments illustrates the effectiveness of the results obtained. These findings significantly improve upon those in existing literature and the impact of this study reaches beyond theoretical (VDDEs), influencing diverse areas such as control theory, population dynamics, and neurobiology, where systems may demonstrate delayed reactions to inputs or changes in state. By setting forth sufficient criteria for stability and boundedness, this research offers critical insights for developing and executing control strategies in engineering, forecasting population behaviors in ecology, and comprehending temporal patterns in biological systems.},
  archive      = {J_JAMC},
  author       = {Olutimo, A. L. and Adeyanju, A. A. and Ogbu, I. F. and Iyase, S. A.},
  doi          = {10.1007/s12190-025-02446-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5401-5418},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Stability and boundedness analysis for a system of nonlinear vector delay differential equations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blow-up time analysis of coupled parabolic equation systems under nonlinear boundary conditions. <em>JAMC</em>, <em>71</em>(4), 5381-5399. (<a href='https://doi.org/10.1007/s12190-025-02439-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the blow-up time for coupled parabolic equation systems with nonlinear boundary conditions. First, we prove that the solutions to these equations can blow-up within a finite time. Subsequently, by employing the auxiliary function method and modified differential inequality techniques,we derive upper and lower bounds of the blow-up time under specific conditions. To illustrate our findings and validate the theoretical framework, corresponding illustrative examples are provided.},
  archive      = {J_JAMC},
  author       = {Liu, Hongwei and Zhang, Lingling and Liu, Tao},
  doi          = {10.1007/s12190-025-02439-7},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5381-5399},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Blow-up time analysis of coupled parabolic equation systems under nonlinear boundary conditions},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approximation approach for nonsmooth multiobjective interval-valued mathematical problems. <em>JAMC</em>, <em>71</em>(4), 5343-5380. (<a href='https://doi.org/10.1007/s12190-025-02428-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approximation technique for solving a specific class of nonsmooth multiobjective interval-valued mathematical problems. We introduce the class of LU- $$(\Phi , \rho )$$ -invex interval-valued functions, which are connected to the Clarke generalized gradient. Furthermore, we provide significant examples to demonstrate the existence of this class of functions. We show the equivalence between the weakly LU-efficient solution of the considered nonsmooth multiobjective interval-valued mathematical problems and the weakly LU-efficient solution of the associated approximated problem under the proposed invexity assumptions. Additionally, the Mond–Weir dual problems corresponding to the approximated problem are formulated. The duality results for the Mond–Weir dual problems are obtained by applying the duality results previously established for the approximated Mond–Weir and dual problems. The paper also presents numerical examples to illustrate and validate the results.},
  archive      = {J_JAMC},
  author       = {Singh, Shubham and Shalini},
  doi          = {10.1007/s12190-025-02428-w},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5343-5380},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A new approximation approach for nonsmooth multiobjective interval-valued mathematical problems},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical threshold of split-step θ methods for stochastic age-structured population models. <em>JAMC</em>, <em>71</em>(4), 5327-5341. (<a href='https://doi.org/10.1007/s12190-025-02438-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study delves into the numerical threshold properties of split-step θ methods applied to stochastic age-structured population models. We establish that these methods can maintain the invariance of the total population, a pivotal attribute, by employing appropriate boundary conditions. The convergence of these methods is affirmed in both the mean- and mean-square senses under suitable boundary conditions. To evaluate the stability of numerical solutions, the numerical threshold $${R^h}$$ is introduced, paralleling the significance of the analysis threshold in stochastic age-structured population models. Numerical solutions are considered stable for $${R^h} < 1$$ and unstable for $${R^h} > 1$$ . Furthermore, the method is shown to maintain the basic reproduction number $${R_0}$$ for any sufficiently large step size, allowing the asymptotic behavior of these models to be represented graphically through numerical processes. The theoretical findings are corroborated with illustrative examples.},
  archive      = {J_JAMC},
  author       = {Wang, Yongqi and Yang, Huizi and Yang, Zhanwen},
  doi          = {10.1007/s12190-025-02438-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5327-5341},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Numerical threshold of split-step θ methods for stochastic age-structured population models},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation of the fractional HIV/AIDS epidemic model using a novel predictor–corrector technique based on singular and non-singular kernels. <em>JAMC</em>, <em>71</em>(4), 5299-5325. (<a href='https://doi.org/10.1007/s12190-025-02420-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study analyze the fractional HIV/AIDS epidemic model that incorporates an antiretroviral treatment compartment using Caputo and Caputo–Fabrizio fractional derivatives. Banach’s fixed-point approach is used to construct the existence theory for the solution of the model. A novel predictor–corrector method is proposed for numerical simulation of the model. This predictor–corrector method is a modified form of the fractional Adams–Bashforth method, formulated in terms of Caputo and Caputo–Fabrizio derivatives. The error estimate of the proposed method is presented. This numerical approach can be easily computed using the programming language MATLAB, and simulation results for distinct values of the fractional order are presented both graphically and in tabular form.},
  archive      = {J_JAMC},
  author       = {Garg, Rahul and Prakash, Amit},
  doi          = {10.1007/s12190-025-02420-4},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5299-5325},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Simulation of the fractional HIV/AIDS epidemic model using a novel predictor–corrector technique based on singular and non-singular kernels},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple positive solutions for second order one dimensional p-laplacian boundary value problems. <em>JAMC</em>, <em>71</em>(4), 5279-5298. (<a href='https://doi.org/10.1007/s12190-025-02425-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article is to study the boundary value problems of second order ordinary differential equations with p-Laplacian operators under Riemann–Stieltjes integral boundary conditions. First, by applying the invertibility of operator $$\Phi _{p}$$ and iterative methods, we transform the existence of positive solutions of the problems into the number of fixed points of the corresponding integral equations. Second, by utilizing the Gronwall-type inequality and integral factor methods, we get a priori bounds for norms of derivatives and allow the nonlinearity to be p-th growth with respect to the derivative. Finally, by using the existence property of fixed point index, we obtain the existence of multiple positive solutions of the problems, and illustrate our conclusions through two examples.},
  archive      = {J_JAMC},
  author       = {Yang, Youyuan and Wang, Qiru},
  doi          = {10.1007/s12190-025-02425-z},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5279-5298},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Multiple positive solutions for second order one dimensional p-laplacian boundary value problems},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On some new classes of permutation trinomials and pentanomials over $$ \mathbb f_{3^{2m}}$$. <em>JAMC</em>, <em>71</em>(4), 5259-5277. (<a href='https://doi.org/10.1007/s12190-025-02433-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $$\mathbb F_q$$ denote the finite field of order q. In this paper, we investigate the permutation property of some new classes of permutation trinomials and pentanomials over $$\mathbb F_{3^{2m}}.$$ More precisely, based on some lower degree trinomials, we construct two new classes of permutation trinomials. Further, we prove the permutation property of five new classes of permutation pentanomials using both reducible and irreducible pentanomials of degree four and six over $$\mathbb F_{3^{2m}}$$ .},
  archive      = {J_JAMC},
  author       = {Gupta, Shalini and Vinayak, Sagar and Nayyar, Anand and Singh, Manpreet},
  doi          = {10.1007/s12190-025-02433-z},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5259-5277},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {On some new classes of permutation trinomials and pentanomials over $$ \mathbb f_{3^{2m}}$$},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Several relaxed CQ-algorithms for the split feasibility problem with multiple output sets. <em>JAMC</em>, <em>71</em>(4), 5231-5258. (<a href='https://doi.org/10.1007/s12190-025-02408-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a pair of CQ-algorithms solving the split feasibility problem with multiple output sets and prove their strong convergence. Various methods are used in the algorithm to improve usability and increase the iteration speed. To broaden the algorithm’s usability, we incorporate an adaptive stepsize and the algorithm works based on replacing the projection to the half-space with that to the intersection of two half-spaces. To hasten the convergence process, the“dividing by norm” approach is selected and the subsequent output hinges on the prior step’s outcome. Finally, our numerical tests in signal recovery demonstrate the superior efficacy of our algorithms.},
  archive      = {J_JAMC},
  author       = {Cao, Yu and Peng, Yishuo and Chen, Yasong and Shi, Luoyi},
  doi          = {10.1007/s12190-025-02408-0},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5231-5258},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Several relaxed CQ-algorithms for the split feasibility problem with multiple output sets},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetry-based granulation in networks associated with commutative rings: Application in social network dynamics. <em>JAMC</em>, <em>71</em>(4), 5205-5230. (<a href='https://doi.org/10.1007/s12190-025-02430-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Granulation of a network is crucial for the structural analysis of a network. One of the efficient granulation methodology is provided by rough set theory (RST) whereas graph theory provides different metrics for network analysis. Symmetry and symmetry-breaking in a network provides information about the network dynamics. In this paper, we provide a novel method of studying the symmetries of a graph and finding all the possible fixing sets of a graph under RST. We study granulation in simple undirected graphs and zero-divisor graphs of finite commutative rings $$\mathbb {Z}_{n}$$ and $$\prod _{i=1}^{k}\mathbb {Z}_{2}$$ by defining a symmetry-based indiscernibility relation on its vertex set V. We study the partition structure of V, approximations of subsets of V and associate an indiscernibility partition lattice with V. We define the discernibility matrix and provide some of its properties. We show that the reducts obtained from the discernibility function are the fixing sets of graph. Furthermore, using the proposed method, we study a social network based on opinion conflict relationship associated with multi-criteria decision problem.},
  archive      = {J_JAMC},
  author       = {Javaid, Imran and Fatima, Abeer and Akram, Muhammad},
  doi          = {10.1007/s12190-025-02430-2},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5205-5230},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Symmetry-based granulation in networks associated with commutative rings: Application in social network dynamics},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics of the drug efficacy of a threshold-triggered control strategy for antibiotic-resistant bacteria. <em>JAMC</em>, <em>71</em>(4), 5185-5203. (<a href='https://doi.org/10.1007/s12190-025-02426-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug resistance is becoming a major health issue inducing the inefficiency of antimicrobial drugs. In this paper, a Filippov system with the threshold control strategy is proposed to investigate the effects of threshold and intermittent control strategy on bacteria-resistant model, which consists of an untreated subsystem coupled to a treated subsystem. The proposed model has been analyzed theoretically using the Filippov convex method and Utkin’s equivalent control method. In particular, the existence and stability of the real equilibria, virtual equilibria, pseudo-equilibrium, sliding segments, and sliding bifurcations are analyzed. The global stability of the real and pseudo-equilibrium indicates that the drug control strategy can be effective in keeping the concentration of resistant bacteria below the threshold range. Finally, the simulations are given to verify the conclusions.},
  archive      = {J_JAMC},
  author       = {Jia, Jing and Zhao, Zhong and Yang, Jingen and Zeb, Anwar},
  doi          = {10.1007/s12190-025-02426-y},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5185-5203},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Dynamics of the drug efficacy of a threshold-triggered control strategy for antibiotic-resistant bacteria},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). European option pricing under uncertain multifactor exponential Ornstein–Uhlenbeck volatility model. <em>JAMC</em>, <em>71</em>(4), 5159-5184. (<a href='https://doi.org/10.1007/s12190-025-02434-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In financial markets, option pricing has an important role to play and volatility is a very critical factor when considering investment, hedging and risk control. This paper proposes an uncertain multifactor volatility model under the exponential Ornstein–Uhlenbeck mean reversion process, modeling volatility in the long and short term. Then the European option pricing formula is derived and a numerical method is provided to price European options. Finally, an empirical analysis is performed based on the CSI 300 ETF option and gives a pricing forecast for option contracts compared to the real option price to verify the validity of the model.},
  archive      = {J_JAMC},
  author       = {Li, Li and Shi, Gang and Zhou, Lijing and Sheng, Yuhong},
  doi          = {10.1007/s12190-025-02434-y},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5159-5184},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {European option pricing under uncertain multifactor exponential Ornstein–Uhlenbeck volatility model},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative numerical method for solving heat conduction using subdivision collocation. <em>JAMC</em>, <em>71</em>(4), 5133-5158. (<a href='https://doi.org/10.1007/s12190-025-02429-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a Subdivision Collocation Algorithm (SCA) to numerically solve the Heat Conduction Equation (HCEq), taking into account both initial and boundary conditions. The algorithm reformulates the differential equation into a system of algebraic equations by using subdivision scheme basis functions to approximate spatial derivatives, while finite difference techniques are applied to discretize the time derivatives. The numerical solution of this system is then obtained through computational methods. The effectiveness of the algorithm is confirmed through both theoretical analysis and numerical testing. Additionally, the obtained solutions are presented both numerically and graphically, and compared with existing methods, showing that the proposed algorithm provides higher accuracy than conventional approaches.},
  archive      = {J_JAMC},
  author       = {Malik, Safia and Ejaz, Syeda Tehmina and Rezapour, Shahram and Inc, Mustafa and Mustafa, Ghulam},
  doi          = {10.1007/s12190-025-02429-9},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5133-5158},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Innovative numerical method for solving heat conduction using subdivision collocation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing the minimax affine fractional problem in reduced space. <em>JAMC</em>, <em>71</em>(4), 5103-5131. (<a href='https://doi.org/10.1007/s12190-025-02427-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research goal of this paper is to design a global optimization algorithm (GOA) for solving the minimax affine fractional programming problem (MAFP) globally, which benefits from a novel relaxation optimization plan and branch-and-bound framework in the reduced space. For this research goal, problem (MAFP) is first formulated to an equivalent lifted problem (ELP) by introducing some auxiliary variables to do equivalent conversion. For the form of problem (ELP), an initial rectangle and a latest equivalence problem (EP) are constructed by adding some new variables. Secondly, a novel relaxation optimization plan is formulated to get a linear optimization problem (LOP) of problem (EP). Next, the detailed procedures of the algorithm (GOA) are given. Furthermore, theoretical analysis of the algorithm (GOA) is presented from the perspectives of convergence and complexity. Finally, to verify the validity and robustness of the algorithm (GOA), the algorithm (GOA) is used to solve some test cases, the numerical experiment results are very exciting.},
  archive      = {J_JAMC},
  author       = {Li, Binbin and Gao, Yuelin},
  doi          = {10.1007/s12190-025-02427-x},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5103-5131},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Optimizing the minimax affine fractional problem in reduced space},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the MDS b-symbol of repeated-root constacyclic codes of prime power length over $$ \mathbb {F}_{p^{m}} + u\mathbb {F}_{p^{m}}+u^{2}\mathbb {F}_{p^{m}} $$. <em>JAMC</em>, <em>71</em>(4), 5087-5101. (<a href='https://doi.org/10.1007/s12190-025-02423-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the b-symbol distance of $$\varepsilon $$ -constacyclic codes of length $$p^\ell $$ over the ring $$\mathcal {A} = \mathbb {F}_{p^m} + u \mathbb {F}_{p^m} + u^2 \mathbb {F}_{p^m} = \frac{\mathbb {F}_{p^m}[u]}{\langle u^3 \rangle }$$ , where p is a prime number, $$\varepsilon \in \mathbb {F}_{p^m}^*$$ , $$\ell $$ and m are positive integers, and b is an integer satisfying $$1 \le b \le \left\lfloor \frac{p}{2} \right\rfloor $$ . Additionally, we classify all maximum distance separable (MDS) b-symbol $$\varepsilon $$ -constacyclic codes of length $$p^\ell $$ over $$\mathcal {A}$$ .},
  archive      = {J_JAMC},
  author       = {Ahendouz, Youssef and Akharraz, Ismail},
  doi          = {10.1007/s12190-025-02423-1},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5087-5101},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {On the MDS b-symbol of repeated-root constacyclic codes of prime power length over $$ \mathbb {F}_{p^{m}} + u\mathbb {F}_{p^{m}}+u^{2}\mathbb {F}_{p^{m}} $$},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit–explicit high-order methods for pricing options under merton’s jump-diffusion models. <em>JAMC</em>, <em>71</em>(4), 5057-5086. (<a href='https://doi.org/10.1007/s12190-025-02424-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes implicit–explicit (IMEX) high-order numerical methods for pricing European and American options under Merton’s jump diffusion model. We incorporate the convex combination parameter $$c\in [0,1]$$ of the zeroth-order term into the IMEX third-order and fourth-order semi-implicit backward differentiation formula to improve the stability and efficiency of the methods. In order to comprehensively understand the stability of IMEX methods in time discretization, we use Fourier method for analysis and draw a stability domain graph. To further improve the accuracy of numerical solutions in spatial discretization, a fourth-order compact finite difference scheme is designed. For the linear complementarity problem of the American options, we ingeniously combined IMEX high-order methods and operator splitting methods. Several numerical experiments are provided to investigate the accuracy and convergence order of IMEX high-order methods for pricing European and American options in both time and spatial. The experimental results have verified the effectiveness and advantages of the proposed numerical schemes.},
  archive      = {J_JAMC},
  author       = {Chen, Yingzi and Wang, Wansheng},
  doi          = {10.1007/s12190-025-02424-0},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5057-5086},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Implicit–explicit high-order methods for pricing options under merton’s jump-diffusion models},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unraveling the complexity of q-deformed dynamics: A study of novel solutions and their implications for nonlinear phenomena. <em>JAMC</em>, <em>71</em>(4), 5037-5055. (<a href='https://doi.org/10.1007/s12190-025-02414-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our goal is to study the (2 + 1) sinh-Gordon equation’s generalization with a q-deformation parameter. This equation describes systems with violated symmetries. By introducing noncommutativity and non-linearity, the q-deformed version expands upon the traditional sinhGordon equation. We investigate two approaches for solving this equation: the reduced q-differential transform method (RqDTM) and the variational iteration method (VIM). RqDTM is a modified version of the differential transform method designed for q-deformed equations, while VIM uses an iterative scheme. We compare the accuracy and efficiency of the solutions obtained from these methods and present numerical results. This analysis helps us assess the strengths and weaknesses of each approach in solving the (2 + 1) q-deformed sinh-Gordon equation, providing valuable insights into their applicability and performance.},
  archive      = {J_JAMC},
  author       = {Alaofi, Zaki Mrzog and Raslan, K. R. and Shehata, Ahmed S. and Ali, Khalid K.},
  doi          = {10.1007/s12190-025-02414-2},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5037-5055},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Unraveling the complexity of q-deformed dynamics: A study of novel solutions and their implications for nonlinear phenomena},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective hermite wavelet collocation method for 3D partial differential equations with convergence analysis. <em>JAMC</em>, <em>71</em>(4), 5015-5036. (<a href='https://doi.org/10.1007/s12190-025-02422-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we solve a three-dimensional partial differential equation using the Hermite wavelet. The Hermite wavelet collocation technique is applied to address the 3D Helmholtz and Poisson equations. The proposed method is straightforward to implement and results in a system of algebraic equations that can be easily solved. We have conducted a detailed error analysis to ensure the reliability of the method. To demonstrate its efficiency and accuracy, we solve two examples each of the Helmholtz and Poisson equations. Additionally, we compare the maximum absolute errors with existing methods in the literature, further validating the robustness of our approach.},
  archive      = {J_JAMC},
  author       = {Raza, Akmal and Alam, Mohammad Prawesh and Faheem, Mo},
  doi          = {10.1007/s12190-025-02422-2},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {5015-5036},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {An effective hermite wavelet collocation method for 3D partial differential equations with convergence analysis},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and analysis of a class of epidemic models with asymptomatic infection and transmission heterogeneity. <em>JAMC</em>, <em>71</em>(4), 4987-5014. (<a href='https://doi.org/10.1007/s12190-025-02417-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A series of epidemic models with asymptomatic infection and transmission heterogeneity are progressively developed and discussed in this paper. The aim is to explore the effect of asymptomatic infections and their management on disease dynamics. The effective reproductive numbers of three models are derived, and their global dynamical properties are proven by constructing appropriate Lyapunov functions. The sensitivity analysis of effective reproduction numbers is obtained by the PRCC method. The comparative result shows that the relevance of transmission, hospitalization and recovery rates of symptomatic patients weaken dramatically when asymptomatic infection is considered. Meanwhile, differences in the sensitivity of influences associated with asymptomatic infections further confirm transmission heterogeneity. It also reveals that vaccine coverage is a crucial factor that affects the dynamics of disease. Additionally, this study examines numerical simulations to assess the effectiveness of various sensitivity-related interventions. Both theoretical and numerical results consistently reveal that neglecting asymptomatic infection will underestimate the extent of infections. Moreover, considering the transmission heterogeneity of asymptomatic infection and strengthening the management for higher-transmission subgroup, can minimize critical vaccination coverage and achieve disease control.},
  archive      = {J_JAMC},
  author       = {Zhang, Panpan and Zhang, Qiang and Wei, Xuerui and Cui, Qianqian},
  doi          = {10.1007/s12190-025-02417-z},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4987-5014},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Modeling and analysis of a class of epidemic models with asymptomatic infection and transmission heterogeneity},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applicability of caputo-hadmard fractional operator in mathematical modeling of pantograph systems. <em>JAMC</em>, <em>71</em>(4), 4971-4986. (<a href='https://doi.org/10.1007/s12190-025-02421-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research examines the existence and uniqueness of solutions for a system of pantograph fractional differential equations that incorporate the Caputo-Hadamard fractional derivative. The existence of solutions is verified through Schaefer’s fixed point theorem, and a well-established technique in functional analysis. Additionally, the uniqueness of solutions is proven using the Banach contraction mapping principle. To support these theoretical results, numerical examples are provided, illustrating and validating the findings. This study offers a comprehensive analysis of the mathematical properties of fractional differential equations, contributing valuable insights and fostering future developments in this field.},
  archive      = {J_JAMC},
  author       = {Awadallah, Muath and Hannabou, Mohamed and Zaway, Hajer and Alahmadi, Jihan},
  doi          = {10.1007/s12190-025-02421-3},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4971-4986},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Applicability of caputo-hadmard fractional operator in mathematical modeling of pantograph systems},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics of a memory-based diffusion zooplankton–phytoplankton model with effect of toxins. <em>JAMC</em>, <em>71</em>(4), 4941-4969. (<a href='https://doi.org/10.1007/s12190-025-02415-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce and examine a diffusive zooplankton–phytoplankton model that incorporates the effects of toxins and memory-based diffusion, characterized by a delayed phytoplankton-taxis term. We first establish the existence and uniform boundedness of solutions. Following this, we analyze the stability of positive constant equilibria, discovering that memory delay can trigger Hopf bifurcation, leading to the emergence of spatially inhomogeneous periodic solutions. This finding suggests that memory-based diffusion facilitates pattern formation, contrasting with the outcomes of models that include only phytoplankton-taxis. Numerical simulations demonstrate that varying toxin input rates can lead to the extinction of phytoplankton and zooplankton, and that increasing the delay may also cause population collapse.},
  archive      = {J_JAMC},
  author       = {Shao, Yutong and Zhang, Xuebing and Li, Shunji},
  doi          = {10.1007/s12190-025-02415-1},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4941-4969},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Dynamics of a memory-based diffusion zooplankton–phytoplankton model with effect of toxins},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linearized nonconforming virtual element method for the semilinear sobolev equations. <em>JAMC</em>, <em>71</em>(4), 4915-4940. (<a href='https://doi.org/10.1007/s12190-024-02333-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linearization is a valid approach to improve the computational efficiency of numerical methods for nonlinear partial differential equations. A linearized nonconforming virtual element method is proposed for the semilinear Sobolev equations in this work, where the linearized Euler backward scheme is employed to discretize the temporal variable. Both the semi-discrete scheme and fully discrete scheme are established and analyzed. By employing a Ritz projection operator, the optimal convergence orders in broken H $$^1$$ semi-norm and L $$^2$$ norm are both derived. In the end, three numerical examples are conducted to inspect the correctness of theoretical analysis results.},
  archive      = {J_JAMC},
  author       = {Zhang, Buying and Zhu, Wenhao and Zhao, Jikun},
  doi          = {10.1007/s12190-024-02333-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4915-4940},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Linearized nonconforming virtual element method for the semilinear sobolev equations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A theta logistic model for the dynamics of whitefly borne mosaic disease in cassava: Impact of roguing and insecticide spraying. <em>JAMC</em>, <em>71</em>(4), 4897-4914. (<a href='https://doi.org/10.1007/s12190-025-02419-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research article, a theta logistic model is proposed and analyzed for the dynamics of vector borne plant disease in Cassava. The model contains two populations namely the plant and the vector population. The vector population follows theta logistic growth as theta logistic growth curve is a more natural choice in comparison with the classical logistic growth curve model. We have also include the effects of roguing and pesticide spraying on dynamics of the mosaic disease. We examine how different values of $$\theta $$ might impact crop survival during vector invasion by utilizing control measures which enhances the biological realism of the model. For the analytical analysis of the theta-logistic model system, we derive the discreet-time version of the continuous model. The analytical results are validated by numerical simulations. The results show that the system is stable when $$\theta $$ lies below a threshold value and unstable via limit cycle oscillation when $$\theta $$ crosses the critical value that lasts until $$\theta = 1$$ . The critical value of $$\theta $$ is 0.485 and this value depends on other parameters such as the infection rate, roguing and insecticides spraying. This study shows that the results obtained from theta-logistic model are more applicable for proper management of mosaic disease in Cassava.},
  archive      = {J_JAMC},
  author       = {Chowdhury, Jahangir and Al Basir, Fahad and Mukherjee, Anirban and Roy, Priti Kumar},
  doi          = {10.1007/s12190-025-02419-x},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4897-4914},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A theta logistic model for the dynamics of whitefly borne mosaic disease in cassava: Impact of roguing and insecticide spraying},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid newton’s method for solving tensor square root problem. <em>JAMC</em>, <em>71</em>(4), 4875-4895. (<a href='https://doi.org/10.1007/s12190-024-02351-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor equations have been widely studied in recent years. In this paper, we proposed a hybrid Newton’s method to solve tensor square root problem $${\mathcal {X}}*{\mathcal {X}}={\mathcal {A}}$$ via Einstein product. This algorithm makes use of tensor computations directly and combines the advantages of the Steepest descent method and the Newton’s method, overcoming their disadvantages. The global convergence and the local quadratic convergence are obtained. Numerical results demonstrate that the hybrid Newton’s method is competitive with the Newton’s method in Duan (Appl Math Lett 98:57–62, 2019).},
  archive      = {J_JAMC},
  author       = {Liu, Lixia and Gao, Yongjuan and Duan, Xuefeng and Wang, Chunfeng and Liu, Sanyang},
  doi          = {10.1007/s12190-024-02351-6},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4875-4895},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A hybrid newton’s method for solving tensor square root problem},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Higher-order caputo fractional integrodifferential inclusions of Volterra–Fredholm type with impulses and infinite delay: Existence results. <em>JAMC</em>, <em>71</em>(4), 4849-4874. (<a href='https://doi.org/10.1007/s12190-025-02412-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores a new type of impulsive nonlocal Caputo fractional dynamical differential inclusions of order $$1< \varrho < 2$$ in Banach space. The major goal is to identify mild solutions for impulsive nonlocal Caputo fractional Volterra–Fredholm integrodifferential inclusions with infinite delay. We use methods from fractional calculus, Dhage’s fixed-point concepts, integrodifferential equations, abstract phase space $${\mathscr {B}}_{g}$$ , multivalued analysis, and sectorial operator of type $$(P, \kappa , \varrho , \gamma )$$ to establish the necessary circumstances for the existence of mild solutions to these problems. Illustrative examples are provided to support the theoretical results and demonstrate their practical application.},
  archive      = {J_JAMC},
  author       = {Raja, Marimuthu Mohan and Vijayakumar, Velusamy and Veluvolu, Kalyana Chakravarthy},
  doi          = {10.1007/s12190-025-02412-4},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4849-4874},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Higher-order caputo fractional integrodifferential inclusions of Volterra–Fredholm type with impulses and infinite delay: Existence results},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimations for determinants of strictly $$\gamma $$ -diagonally dominant matrices. <em>JAMC</em>, <em>71</em>(4), 4839-4848. (<a href='https://doi.org/10.1007/s12190-025-02418-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class of strictly $$\gamma $$ -diagonally dominant matrices is an important subclass of the nonsingular H-matrices. In this paper, by use of the properties of Schur complements, we present lower and upper bounds for determinants of strictly $$\gamma $$ -diagonally dominant matrices. Moreover, these bounds may improve the corresponding results for the strictly diagonally dominant matrices. The effectiveness and superiority of the obtained bounds are demonstrated through several numerical examples.},
  archive      = {J_JAMC},
  author       = {Lyu, Zhen-Hua},
  doi          = {10.1007/s12190-025-02418-y},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4839-4848},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Estimations for determinants of strictly $$\gamma $$ -diagonally dominant matrices},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qualitative behavior for a discretized conformable fractional-order predator–prey model. <em>JAMC</em>, <em>71</em>(4), 4815-4837. (<a href='https://doi.org/10.1007/s12190-025-02413-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this study is to examine the dynamic behavior of a conformable fractional-order predator–prey system with a Holling-III type functional response. The fractional-order system is transformed into a discrete model through a discretization process. The fixed points are analyzed for both their existence and uniqueness. We then assess the local stability of the two fixed points using linearization techniques, and use the center manifold theorem and bifurcation theory to identify conditions for period-doubling and Neimark–Sacker bifurcations. To verify our findings, we perform numerical simulations and compute the maximum Lyapunov exponents to confirm the presence of chaotic behavior.},
  archive      = {J_JAMC},
  author       = {Berkal, Messaoud and Navarro, Juan F. and Hamada, M. Y. and Semmar, Billel},
  doi          = {10.1007/s12190-025-02413-3},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4815-4837},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Qualitative behavior for a discretized conformable fractional-order predator–prey model},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Determining the type of a solution to the fully pythagorean fuzzy linear equations system: Exact, restricted, or relaxed approximate solution. <em>JAMC</em>, <em>71</em>(4), 4787-4813. (<a href='https://doi.org/10.1007/s12190-025-02409-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In engineering and social research, linear systems are commonly used to address real-life problems of various dimensions. Therefore, many studies start by developing linear systems and then finding their solutions. Recent studies have demonstrated the effectiveness of Pythagorean fuzzy sets in capturing and representing complex forms of uncertainty, particularly when understanding the distinctions between membership and non-membership is crucial. This paper pioneers the finding a solution for a general (square or nonsquare) Fully Pythagorean Fuzzy Linear Equations System (FPFLES) with arbitrary triangular Pythagorean fuzzy numbers and fills a critical gap in the existing literature. Since an FPFLES consists of the sum of the multiplications of each arbitrary parameter and variable, and the fuzzy multiplication operation includes the min and max operators, a nonlinearity situation is observed in each equation. To overcome this situation, a transformation from fuzzy multiplication to inequalities is applied, and thus, a mixed integer programming (MIP) problem is formed. Depending on whether the MIP problems created by changing the constraints have an optimal solution, FPFLES has an exact solution or an approximate solution. The types of solutions are examined using a distance measure definition available in the literature. This paper also defines restricted and relaxed approximate solutions for FPFLES by determining whether the left-hand sides obtained from the substitution of solutions are completely covered by the right-hand sides of the equations. The approach is illustrated with some numerical examples, and the numerical results are analyzed within the distance measure to determine the closeness between the left-hand and right-hand sides of the system.},
  archive      = {J_JAMC},
  author       = {Ergenecosar, Gizem Temelcan},
  doi          = {10.1007/s12190-025-02409-z},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4787-4813},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Determining the type of a solution to the fully pythagorean fuzzy linear equations system: Exact, restricted, or relaxed approximate solution},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive behaviour of globally rumour propagation using a fractional order a time scale dynamical model with synthetical social factors. <em>JAMC</em>, <em>71</em>(4), 4751-4786. (<a href='https://doi.org/10.1007/s12190-025-02411-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study looks at how rumors spread, especially online, where false information can quickly spread to millions of people, disrupt public order, infringe on privacy, and endanger public safety. A dishonest user targets friends and strangers with marketing accounts on mobile social networking sites to propagate rumors. In this article, we propose a fractional order nonlinear mathematical framework explaining how rumors spread widely on Facebook and Twitter about different life interest topics with the Caputo operator. For the fractional-order Caputo model, there is an analytical solution verified with the Lipchitz condition for boundedness, positivity, and feasible solutions. The existence and uniqueness of solutions are established using the Banach contraction principle. We conduct qualitative analyses to validate the model and assess control efficiency by calculating the fundamental reproductive number. Additionally, we identify equilibrium points for rumor-free situation. Chaos control will use the regulate for linear responses approach to stabilize the system according to its equilibrium points. The global stability of equilibrium points is confirmed using the Lyapunov function. A numerical iterative approach based on the Newton polynomial interpolation is employed for the numerical approximation of the proposed problem. Numerical simulations have been conducted to illustrate the superior performance of the fractional-order model at different fractional-order values and the surface face graph shows the complete dynamics of rumor propagation in a time scale frame for different factors. This model can capture all historical information of the system under consideration, a capability not present in classical (integer-order) differential equations. The results, which are backed up by graphical representations, show how well the novel method solves this fractional model and how useful it is in combating the spread of rumors.},
  archive      = {J_JAMC},
  author       = {Farman, Muhammad and Jamil, Khadija and Nisar, Kottakkaran Sooppy and Ahmad, Hijaz and Sambas, Aceng},
  doi          = {10.1007/s12190-025-02411-5},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4751-4786},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Predictive behaviour of globally rumour propagation using a fractional order a time scale dynamical model with synthetical social factors},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the dirichlet problem for a class of augmented k-hessian equation. <em>JAMC</em>, <em>71</em>(4), 4729-4749. (<a href='https://doi.org/10.1007/s12190-025-02410-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the existence results of radial solutions for a Dirichlet boundary value problem of the following augmented k-Hessian equation: $$\begin{aligned} \left\{ \begin{array}{ll} S_{k } (D^{2} u + \alpha I ) = \lambda b(\vert x \vert ) f(-u ) , & \quad i n \ \Omega , \\ u = 0 , & \quad on \ \partial \Omega , \end{array} \right. \end{aligned}$$ where $$ \Omega $$ is an open unit ball in $$ \mathbb {R}^{N},$$ $$ S_{k } (D^{2} u + \alpha I ) $$ is an augmented k-Hessian operator, k is an integer, $$ 1 \le k \le N < 2 k,$$ $$ \alpha $$ is a constant $$\left( \alpha \ne 0 \right) $$ , I is the unit matrix, $$ \lambda $$ is a positive parameter, b and f are continuous functions. The augmented term $$\alpha I$$ in this paper can be positive or negative. It follows from the Guo–Krasnosel’skii fixed point theorem that there is at least one radial solution to the Dirichlet problem of the augmented k-Hessian equation for certain $$\lambda >0$$ .}},
  archive      = {J_JAMC},
  author       = {Feng, Shu and Mi, Ling},
  doi          = {10.1007/s12190-025-02410-6},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4729-4749},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Research on the dirichlet problem for a class of augmented k-hessian equation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical technique based on bernstein polynomials approach for solving auto-convolution VIEs and the initial value problem of auto-convolution VIDEs. <em>JAMC</em>, <em>71</em>(4), 4697-4727. (<a href='https://doi.org/10.1007/s12190-025-02400-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a computational technique aimed at solving the auto-convolution Volterra integral equation (AVIE) and the auto-convolution Volterra integro-differential equation (AVIDE). In this approach, we use the Bernstein approximation method to estimate solutions for these equations. By leveraging the characteristics of Bernstein polynomials, we simplify the problem, transforming these equations into a manageable system of algebraic equations. We provide a detailed description of the approach, and then its practicality for the suggested equations is presented. The suggested algorithm is computationally efficient, has greater stability, is straightforward to implement on computers, and demands less computer memory. This approach first converts these equations into a class of integral equations and then uses the proposed approach to estimate the solution. Some theorems have been proposed to demonstrate the existence and uniqueness of the suggested approach. In addition, an estimate of the error bound for this approach is provided. A comparison of this technique with previously known methods is examined. Finally, representative numerical tests are reported to demonstrate the precision and efficiency of the proposed solving method.},
  archive      = {J_JAMC},
  author       = {Aourir, E. and Dastjerdi, H. Laeli and Oudani, M. and Shah, Kamal and Abdeljawad, Thabet},
  doi          = {10.1007/s12190-025-02400-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4697-4727},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Numerical technique based on bernstein polynomials approach for solving auto-convolution VIEs and the initial value problem of auto-convolution VIDEs},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimality and duality relations for multiobjective fractional semi-infinite optimization problems with equilibrium constraints. <em>JAMC</em>, <em>71</em>(4), 4667-4695. (<a href='https://doi.org/10.1007/s12190-025-02403-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider a non-smooth multiobjective fractional semi-infinite optimization problems with equilibrium constraints. The concept of $$S $$ -stationary point is stated in terms of Clarke subdifferential for the non-smooth multiobjective fractional semi-infinite optimization problems with equilibrium constraints. Under the $$S $$ -stationary point and $$E $$ -convexity assumptions, we developed an $$E $$ -sufficient optimality condition for the considered model. Additionally, we have formulated the $$E $$ -Mond-Weir and $$E $$ -Schaible type dual models, and further, $$E $$ -weak, $$E $$ -strong, and $$E $$ -converse duality results are established for the proposed dual models under $$E $$ -convexity assumptions and using the idea of $$S $$ -stationary point. Moreover, the theoretical results of the work are also validated by citing non-trivial example.},
  archive      = {J_JAMC},
  author       = {Biswas, Bishal and Gupta, S. K.},
  doi          = {10.1007/s12190-025-02403-5},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4667-4695},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Optimality and duality relations for multiobjective fractional semi-infinite optimization problems with equilibrium constraints},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spherical fuzzy soft competition graphs with application. <em>JAMC</em>, <em>71</em>(4), 4615-4665. (<a href='https://doi.org/10.1007/s12190-024-02286-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When analyzing the competitive dynamics within a system, the relationships among its objects can exhibit variability depending on different attributes or perspectives. Spherical fuzzy soft theory offers a valuable framework for parameterizing problems involving vague data, combining the properties of spherical fuzzy sets and soft sets. This paper introduces the novel concept of spherical fuzzy soft competition graphs and spherical fuzzy soft economic competition graphs, a comprehensive framework for modeling and analyzing complex competition dynamics in various domains. By amalgamating the principles of spherical fuzzy sets and soft computing, this approach effectively captures and quantifies uncertainty and vagueness intrinsic to real-world competitive interactions. This paper presents an extension of the graph model, incorporating spherical fuzzy soft $$\kappa $$ -competition graphs, which sheds light on competition and economic competition-dependent systems. An application to the domain of illicit cross-border movements demonstrates the efficacy of this approach. It subsequently presents the algorithmic framework for constructing these competition graphs, enabling the assessment of truthness, abstinence, and falseness interactions among competing entities within a system. This paper also delves into comparative analyses, highlighting the unique advantages of spherical fuzzy soft competition graphs over existing models.},
  archive      = {J_JAMC},
  author       = {Shahzadi, Sundas and Zafar, Fariha and Nawaz, Moeed},
  doi          = {10.1007/s12190-024-02286-y},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {8},
  number       = {4},
  pages        = {4615-4665},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Spherical fuzzy soft competition graphs with application},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jar">JAR - 10</h2>
<ul>
<li><details>
<summary>
(2025). Constructing the lie algebra of smooth vector fields on a lie group in Isabelle/HOL. <em>JAR</em>, <em>69</em>(3), 1-29. (<a href='https://doi.org/10.1007/s10817-025-09724-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a formal theory of smooth vector fields, Lie groups and the Lie algebra of a Lie group in the theorem prover Isabelle. Lie groups are abstract structures that are composable, invertible and differentiable; they are useful in the study of continuous transformations in fields such as particle physics and robotics. The formalisation of this theory in an interactive theorem prover poses challenges beyond those encountered in textbook developments. We comment on representational choices we made to integrate involved concepts, such as smoothness of vector fields, with the simple type theory of higher-order logic (HOL) and existing material in Isabelle/HOL.},
  archive      = {J_JAR},
  author       = {Schmoetten, Richard and Fleuriot, Jacques D.},
  doi          = {10.1007/s10817-025-09724-x},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-29},
  shortjournal = {J. Auto. Reasoning},
  title        = {Constructing the lie algebra of smooth vector fields on a lie group in Isabelle/HOL},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active learning for SAT solver benchmarking. <em>JAR</em>, <em>69</em>(3), 1-23. (<a href='https://doi.org/10.1007/s10817-025-09729-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benchmarking is crucial for developing new algorithms. This also applies to solvers for the propositional satisfiability (SAT) problem. Benchmark selection is about choosing representative problem instances that reliably discriminate solvers based on their runtime. In this paper, we present a dynamic benchmark selection approach based on active learning. Our approach estimates the rank of a new solver among its competitors, striving to minimize benchmarking runtime but maximize ranking accuracy. Instead of using real-valued solver runtimes, our approach works with discretized runtime labels, which yielded better solver rank predictions. We evaluated this approach on the Anniversary Track dataset from the SAT Competition 2022. Our benchmark selection approach can predict the rank of a new solver after approximately 10 % of the time it would take to run the solver on all instances of this dataset, with a prediction accuracy of approximately 92 %. Additionally, we discuss the importance of instance families in the selection process. In conclusion, our tool offers a reliable method for solver engineers to assess a new solver’s performance efficiently.},
  archive      = {J_JAR},
  author       = {Fuchs, Tobias and Bach, Jakob and Iser, Markus},
  doi          = {10.1007/s10817-025-09729-6},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {J. Auto. Reasoning},
  title        = {Active learning for SAT solver benchmarking},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double auctions: Formalization and automated checkers. <em>JAR</em>, <em>69</em>(3), 1-33. (<a href='https://doi.org/10.1007/s10817-025-09732-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Double auctions are widely used in financial markets, such as those for stocks, derivatives, currencies, and commodities, to match demand and supply. Once all buyers and sellers have placed their trade requests, the exchange determines how these requests are to be matched. The two most common objectives for determining the matching are maximizing trade volume at a uniform price and maximizing trade volume through dynamic pricing. In this work, we present fully formalized matching algorithms for double auctions, along with their correctness proofs. We establish new uniqueness theorems, enabling automatic detection of violations in exchange systems by comparing their output to that of a verified program. All proofs are formalized in the Coq Proof Assistant; we extract verified OCaml and Haskell programs that could serve as a resource for exchanges and market regulators. We demonstrate the practical applicability of our work by running the verified program on real market data from an exchange to automatically check for violations in the exchange algorithm.},
  archive      = {J_JAR},
  author       = {Garg, Mohit and Raja, N. and Sarswat, Suneel and Singh, Abhishek Kr},
  doi          = {10.1007/s10817-025-09732-x},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-33},
  shortjournal = {J. Auto. Reasoning},
  title        = {Double auctions: Formalization and automated checkers},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated reasoning for proving non-orderability of groups. <em>JAR</em>, <em>69</em>(3), 1-32. (<a href='https://doi.org/10.1007/s10817-025-09734-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate how a generic automated theorem prover can be applied to establish the non-orderability of groups. Our approach incorporates various tools such as reasoning from the first principles, positive cones, torsions, generalised torsions and cofinal elements.},
  archive      = {J_JAR},
  author       = {Lisitsa, Alexei and Nie, Zipei and Vernitski, Alexei},
  doi          = {10.1007/s10817-025-09734-9},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-32},
  shortjournal = {J. Auto. Reasoning},
  title        = {Automated reasoning for proving non-orderability of groups},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving a problem with GeoGebra current possibilities and limits of CAS tools. <em>JAR</em>, <em>69</em>(3), 1-13. (<a href='https://doi.org/10.1007/s10817-025-09733-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the article we show what a Dynamic Geometry Environment (DGE) and Computer Algebra System (CAS) software is capable of exploring, computing and proving a geometry problem that is closely related to the well-known Wallace-Simson theorem. In our case, this problem turns out to be quite difficult for some software tools incorporated in the DGE software. The computation of a searched locus by the software works well in our case, but the problem must be formulated in a specific way. In addition to the conclusions attained by DGE software, we present computer-aided analytical solutions in the second half of paper. It is shown that the use of non-degeneracy conditions before elimination leads to a significant reduction of the computation time.},
  archive      = {J_JAR},
  author       = {Blažek, Jiří and Pech, Pavel},
  doi          = {10.1007/s10817-025-09733-w},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Auto. Reasoning},
  title        = {Solving a problem with GeoGebra current possibilities and limits of CAS tools},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formally verified suffix array construction. <em>JAR</em>, <em>69</em>(3), 1-38. (<a href='https://doi.org/10.1007/s10817-025-09735-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suffix arrays are a data structure with numerous real-world applications. They are extensively used in text retrieval and data compression applications, including query suggestion mechanisms in web search, and in bioinformatics tools for DNA sequencing and matching. This wide applicability means that algorithms for constructing suffix arrays are of great practical importance. The SA-IS algorithm is an efficient but conceptually complex suffix array construction technique, and implementing it requires a deep understanding of its underlying theory. As a critical step towards developing a provably correct and efficient implementation, we have developed the SA-IS algorithm in Isabelle/HOL and formally verified that it is equivalent to a mathematical functional specification of suffix arrays, a task that required verifying a wide range of underlying properties of strings and suffixes. We also used Isabelle’s code extraction facilities to extract an executable Haskell implementation of SA-IS, which albeit is inefficient due to using lists and natural numbers rather than arrays and machine words, demonstrates that our verified HOL implementation of SA-IS can be refined to an executable implementation in its current form.},
  archive      = {J_JAR},
  author       = {Cheung, Louis and Moffat, Alistair and Rizkallah, Christine},
  doi          = {10.1007/s10817-025-09735-8},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-38},
  shortjournal = {J. Auto. Reasoning},
  title        = {Formally verified suffix array construction},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structured monads for generic first-order syntax metatheory. <em>JAR</em>, <em>69</em>(3), 1-41. (<a href='https://doi.org/10.1007/s10817-025-09731-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning about substitution remains one of the most tedious and error-prone aspects of formal metatheory. We present Tealeaves, a framework implemented in Coq for developing such infrastructure generically and modularly. Tealeaves is centered on a novel categorical abstraction, decorated traversable monads (DTMs), which provide a unifying foundation for first-order syntax and enable local, compositional reasoning about syntactic operations, such as substitution, that are defined purely by their effect on individual variable occurrences. Within this framework, Tealeaves supports extensible backend modules, each implementing the metatheory of a specific concrete strategy for representing binders. Our current backends include implementations of de Bruijn indices in the style of Autosubst, as well as locally nameless in the style of LNgen. Tealeaves goes further by providing a certified translation between these representations, illustrating how DTMs reconcile their underlying structures. The framework also accommodates challenging features such as variadic and mutually-recursive binders, which are often overlooked by both theoretical treatments and practical tools. We describe the implementation and use of Tealeaves’ backends in formalized language developments, introduce the equational axioms that characterize DTMs, and conclude with a presentation of those axioms instantiated for the lambda calculus extended with a variadic binding constructor.},
  archive      = {J_JAR},
  author       = {Dunn, Lawrence and Tannen, Val and Zdancewic, Steve},
  doi          = {10.1007/s10817-025-09731-y},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-41},
  shortjournal = {J. Auto. Reasoning},
  title        = {Structured monads for generic first-order syntax metatheory},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reasoning about incompletely defined programs. <em>JAR</em>, <em>69</em>(3), 1-48. (<a href='https://doi.org/10.1007/s10817-025-09722-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider automated reasoning about recursively defined partial functions with decidable domain, i.e. functions computed by incompletely defined (or underspecified) but terminating functional programs. We define an interpreter for those programs, consider termination and investigate the semantics of incompletely defined programs. The interpreter may halt with a stuck computation, e.g. when dividing a number by zero, which represents a runtime error in a conventional programming environment. We show how so-called domain procedures are synthesized which decide the domain of incompletely defined procedures in almost all cases. As calls of domain procedures occur in proof obligations, domain procedures are optimized to make them as simple as possible. We also use domain procedures to refine the program semantics such that statements causing stuck computations do not hold. Our method to reason about incompletely defined programs is implemented in the verification tool VeriFun.},
  archive      = {J_JAR},
  author       = {Walther, Christoph},
  doi          = {10.1007/s10817-025-09722-z},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-48},
  shortjournal = {J. Auto. Reasoning},
  title        = {Reasoning about incompletely defined programs},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing expected visiting times and stationary distributions in markov chains: Fast and accurate. <em>JAR</em>, <em>69</em>(3), 1-50. (<a href='https://doi.org/10.1007/s10817-025-09736-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the accurate and efficient computation of the expected number of times each state is visited in discrete- and continuous-time Markov chains. To obtain sound accuracy guarantees efficiently, we lift interval iteration, optimistic value iteration and topological approaches developed to compute reachability probabilities and expected rewards and prove all these algorithms to be correct. We further establish that expected visiting times are preserved under backward probabilistic bisimilarity. We study various applications of expected visiting times. The reachability probabilities of multiple bottom strongly connected components (BSCCs) can be obtained by solving a single linear equation system—as opposed to solving an equation system per BSCC. Other applications include the sound computation of the stationary distribution as well as expected rewards conditioned on reaching multiple goal states. The implementation of our methods in the probabilistic model checker Storm scales to large systems with millions of states. Our experiments on the quantitative verification benchmark set show that the computation of stationary distributions via expected visiting times consistently outperforms existing approaches—sometimes by several orders of magnitude.},
  archive      = {J_JAR},
  author       = {Mertens, Hannah and Katoen, Joost-Pieter and Quatmann, Tim and Winkler, Tobias},
  doi          = {10.1007/s10817-025-09736-7},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-50},
  shortjournal = {J. Auto. Reasoning},
  title        = {Computing expected visiting times and stationary distributions in markov chains: Fast and accurate},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LTL reactive synthesis with a few hints. <em>JAR</em>, <em>69</em>(3), 1-50. (<a href='https://doi.org/10.1007/s10817-025-09737-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a variant of the problem of synthesizing Mealy machines that enforce LTL specifications against all possible behaviours of the environment including hostile ones. In the variant studied here, the user provides the high level LTL specification $$\varphi $$ of the system to design, and a set E of examples of executions that the solution must produce. Our synthesis algorithm works in two phases. First, it generalizes the decisions taken along the examples E using tailored extensions of automata learning algorithms. This phase generalizes the user-provided examples in E while preserving realizability of $$\varphi $$ . Second, the algorithm turns the (usually) incomplete Mealy machine obtained by the learning phase into a complete Mealy machine that realizes $$\varphi $$ . The examples are used to guide the synthesis procedure. We provide a completeness result that shows that our procedure can learn any Mealy machine M that realizes $$\varphi $$ with a small (polynomial) set of examples. We also show that our problem, that generalizes the classical LTL synthesis problem (i.e. when $$E=\emptyset $$ ), matches its worst-case complexity. The additional cost of learning from E is even polynomial in the size of E and in the size of a symbolic representation of solutions that realize $$\varphi $$ . This symbolic representation is computed by the synthesis algorithm implemented in Acacia-Bonzai when solving the plain LTL synthesis problem. We illustrate the practical interest of our approach on a set of examples.},
  archive      = {J_JAR},
  author       = {Balachander, Mrudula and Filiot, Emmanuel and Raskin, Jean-François},
  doi          = {10.1007/s10817-025-09737-6},
  journal      = {Journal of Automated Reasoning},
  month        = {9},
  number       = {3},
  pages        = {1-50},
  shortjournal = {J. Auto. Reasoning},
  title        = {LTL reactive synthesis with a few hints},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jco">JCO - 10</h2>
<ul>
<li><details>
<summary>
(2025). An improvement on the louvain algorithm using random walks. <em>JCO</em>, <em>50</em>(2), 1-26. (<a href='https://doi.org/10.1007/s10878-025-01337-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present improvements to famous algorithms for community detection, namely Newman’s spectral method algorithm and the Louvain algorithm. The Newman algorithm begins by treating the original graph as a single cluster, then repeats the process to split each cluster into two, based on the signs of the eigenvector corresponding to the second-largest eigenvalue. Our improvement involves replacing the time-consuming computation of eigenvalues with a random walk during the splitting process. The Louvain algorithm iteratively performs the following steps until no increase in modularity can be achieved anymore: each step consists of two phases–phase 1 for partitioning the graph into clusters, and phase 2 for constructing a new graph where each vertex represents one cluster obtained from phase 1. We propose an improvement to this algorithm by adding our random walk algorithm as an additional phase for refining clusters obtained from phase 1. It maintains a complexity comparable to the Louvain algorithm while exhibiting superior efficiency. To validate the robustness and effectiveness of our proposed algorithms, we conducted experiments using randomly generated graphs and real-world data.},
  archive      = {J_JCO},
  author       = {Do, Duy Hieu and Phan, Thi Ha Duong},
  doi          = {10.1007/s10878-025-01337-9},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-26},
  shortjournal = {J. Comb. Optim.},
  title        = {An improvement on the louvain algorithm using random walks},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategy-proof mechanisms for maximizing social satisfaction in the facility location game. <em>JCO</em>, <em>50</em>(2), 1-20. (<a href='https://doi.org/10.1007/s10878-025-01341-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The facility location game, where the agents’ locations are on a line, is considered in this paper. The input consists of the reported locations of agents, which are collected as part of the game setup. We introduce the concept of a fairness baseline and define a function to characterize each agent’s satisfaction with the facility location. Our objective is to establish a mechanism that obtains the true information of agents and outputs a single facility location so that the sum of all agents’ satisfaction with the location is maximized. For the game with two agents, we propose a $$\frac{5}{4}$$ -approximate strategy-proof mechanism, which is the best possible. In the general case, we demonstrate that the median mechanism achieves an approximation ratio of $$\frac{3}{2}$$ . In particular, the median mechanism is an optimal group strategy-proof mechanism for the game with three agents. Additionally, we devise a $$\frac{1+\sqrt{3}}{2}$$ -approximation group strategy-proof mechanism by modifying the median mechanism. We also consider social satisfaction in the obnoxious facility location game and design a mechanism based on the median of the input.},
  archive      = {J_JCO},
  author       = {Li, Xiaowei and Lu, Xiwen},
  doi          = {10.1007/s10878-025-01341-z},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-20},
  shortjournal = {J. Comb. Optim.},
  title        = {Strategy-proof mechanisms for maximizing social satisfaction in the facility location game},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum expert consensus models with both type- $$\alpha $$ and type- $$\varepsilon $$ constraints. <em>JCO</em>, <em>50</em>(2), 1-23. (<a href='https://doi.org/10.1007/s10878-025-01342-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum expert consensus model (MECM) aims to maximize the number of consensual decision-makers (DMs) within a limited budget. However, it may fail to achieve high group satisfaction or even cannot reach an acceptable consensus due to its neglect of the group consensus level, resulting in type- $$\alpha $$ constraints not being satisfied. To address this issue, we extend the existing MECM by considering both type- $$\alpha $$ and type- $$\varepsilon $$ consensus constraints to enable the group consensus level and the number of consensual DMs as large as possible. Firstly, we construct a dual-MECM that considers the above two constraints. Secondly, we further develop a dual-MECM considering compromise limits (dual-MECM-CL). To provide a reference for budgeting, a dual minimum cost consensus model (dual-MCCM) is established to determine the upper and lower bounds of the budget. Subsequently, we explore the relationships between the two proposed MECMs and the existing MECM. Finally, the effectiveness of the proposed models is illustrated by numerical examples. The results show that: (1) The dual-MECM can ensure that the majority of DMs reach consensus while maintaining a high group consensus level. (2) With a limited budget, the improvement of the overall consensus level will lead to the reduction in the number of consensual DMs. (3) Consideration of individual compromise limits may reduce the number of consensual DMs within the same budget. Therefore, the proposed models can derive a more reasonable consensus result due to full consideration of consensus measurements and DMs’ behaviors.},
  archive      = {J_JCO},
  author       = {Cheng, Dong and Zhang, Huina and Wu, Yong},
  doi          = {10.1007/s10878-025-01342-y},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-23},
  shortjournal = {J. Comb. Optim.},
  title        = {Maximum expert consensus models with both type- $$\alpha $$ and type- $$\varepsilon $$ constraints},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moving horizon capacitated arc routing problem. <em>JCO</em>, <em>50</em>(2), 1-35. (<a href='https://doi.org/10.1007/s10878-025-01344-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In transportation networks, routing problems are cursed with arbitrary changes occurring in the dataset due to unpredictable events like agent breakdown (sensor or vehicle failure), network connectivity changes, resource/demand fluctuations, etc. Moreover, capacity restriction on the agents may require multi-trip solutions for meeting large demands over networks. For example, a battery-powered inspection wagon can only service a limited number of track sections in a single trip. We investigate a moving horizon approach for the multi-trip dynamic capacitated arc routing problem with limited duration to mitigate the limitations of CARP variants in the literature. The proposed approach addresses arbitrary changes in the underlying network, agent unavailability scenarios, and simultaneously satisfies the time limit on meeting all demands. The moving horizon approach subdivides the planning horizon to determine the current trip (single-trip) for all agents, hence coined as Moving Horizon Capacitated Arc Routing Problem (MH-CARP). The proposed MH-CARP is formulated as a set covering problem that considers both partial and full trips (trips may not start at the depot), making it suitable for tackling arbitrary events by re-planning. Theoretical results for the computation of dual variables are derived and then implemented in the column generation algorithm to obtain lower bounds. The algorithm is validated on a widely available dataset for CARP, having instances of up to 147 tasks that require servicing by up to 20 agents. Using this benchmark data, the partial-trip based re-planning strategy is also validated. Lastly, a simulation study is presented to demonstrate the re-planning strategy and compare an MH-CARP solution to two CARP based solutions - one with no arbitrary events and the other with known arbitrary events. The results also convey that greedy solutions are avoided to satisfy the limited duration restriction, and automatic re-ordering of the trips is achieved to compensate for arbitrary events.},
  archive      = {J_JCO},
  author       = {Buriuly, Somnath and Vachhani, Leena and Sinha, Arpita and Ravitharan, Sivapragasam and Chauhan, Sunita},
  doi          = {10.1007/s10878-025-01344-w},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-35},
  shortjournal = {J. Comb. Optim.},
  title        = {Moving horizon capacitated arc routing problem},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semistrong edge colorings of planar graphs. <em>JCO</em>, <em>50</em>(2), 1-30. (<a href='https://doi.org/10.1007/s10878-025-01346-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strengthened notions of a matching M of a graph G have been considered, requiring that the matching M has some properties with respect to the subgraph $$G_M$$ of G induced by the vertices covered by M: If M is the unique perfect matching of $$G_M,$$ then M is a uniquely restricted matching of G; if all the edges of M are pendant edges of $$G_M,$$ then M is a semistrong matching of G; if all the vertices of $$G_M$$ are pendant, then M is an induced matching of G. Strengthened notions of edge coloring and of the chromatic index follow. In this paper, we consider the maximum semistrong chromatic index of planar graphs with given maximum degree $$\Delta .$$ We prove that graphs with maximum average degree less than 14/5 have semistrong chromatic index (hence uniquely restricted chromatic index) at most $$2\Delta +4,$$ and we reduce the bound to $$2\Delta +2$$ if the maximum average degree is less than 8/3. These cases cover, in particular, the cases of planar graphs with girth at least 7 (resp. at least 8). Our result makes some progress on the conjecture of Lužar et al. (J Graph Theory 105:612–632, 2024), which asserts that every planar graph G has a semistrong edge coloring with $$2\Delta +C$$ colors, for some universal constant C. (Note that such a conjecture would fail for strong edge coloring as there exist graphs with arbitrarily large maximum degree that are not strongly $$(4\Delta -5)$$ -edge-colorable.) We provide an example of a planar graph showing that the maximum semistrong chromatic index of planar graphs with maximum degree $$\Delta $$ is at least $$2\Delta +4.$$},
  archive      = {J_JCO},
  author       = {Lin, Yuquan and Lin, Wensong},
  doi          = {10.1007/s10878-025-01346-8},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-30},
  shortjournal = {J. Comb. Optim.},
  title        = {Semistrong edge colorings of planar graphs},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A divide-and-conquer based preprocessing for routing in a simple polygon. <em>JCO</em>, <em>50</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10878-025-01345-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a simple polygon P defined with n vertices in the plane, we preprocess P and compute routing tables at every vertex of P. In the routing phase, a packet originating at any source vertex of P is routed to its destination vertex belonging to P. At every vertex v of P along the routing path, until the packet reaches its destination, the next hop is determined using the routing tables at v and the additional information (including the packet’s destination vertex label) in the packet. We show our routing scheme constructs routing tables in $$O\big (n \big (1+\frac{1}{\epsilon }\big ) \big (\lg {n}\big )^3\big )$$ time and the routing tables at all the vertices of P together use $$O\big (n+\frac{n}{\epsilon }\big (\lg {n}\big )^3\big )$$ space. The multiplicative stretch factor of the routing path computed by our algorithm is upper bounded by $$(2+\epsilon )\lg {n}$$ . Here, $$\epsilon > 0$$ is an input parameter.},
  archive      = {J_JCO},
  author       = {Gaur, Siddharth and Inkulu, R.},
  doi          = {10.1007/s10878-025-01345-9},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-18},
  shortjournal = {J. Comb. Optim.},
  title        = {A divide-and-conquer based preprocessing for routing in a simple polygon},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embedding crossed cube into diverse product graphs and tree-derived architectures. <em>JCO</em>, <em>50</em>(2), 1-21. (<a href='https://doi.org/10.1007/s10878-025-01350-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The embedding of graphs plays a vital role in simulating parallel architectures into other parallel topologies. Out of all parallel computing architectures in super-computing, hypercube, and its variants cannot be ignored because of the desirable, easily implementable, and applicable properties. One such variant of hypercube is the crossed cube $$CQ^r$$ . With exponential growth in the layout of VLSI designs, the concept of embedding has attained paramount importance. Crossed cube, a highly cited one among the variants of hypercube is explored with respect to embedding in this work. As a sequel, we derive the exact wirelength of embedding crossed cube into certain tree-derived architectures, corona product of a path on $$2^{r-1}$$ nodes into an isolated node (comb), Cartesian product of path on 2 nodes into a path on $$2^{r-1}$$ nodes (ladder) and path (MinLA).},
  archive      = {J_JCO},
  author       = {Immanuel, Paul and Greeni, A. Berin},
  doi          = {10.1007/s10878-025-01350-y},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-21},
  shortjournal = {J. Comb. Optim.},
  title        = {Embedding crossed cube into diverse product graphs and tree-derived architectures},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph coloring problem solving using monte carlo tree search and deep reinforcement learning. <em>JCO</em>, <em>50</em>(2), 1-21. (<a href='https://doi.org/10.1007/s10878-025-01338-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graph coloring problem, as a well-known NP-hard problem, holds significant value in practical applications. In this paper, a self-learning method that combines Monte Carlo tree search with deep reinforcement learning is proposed to efficiently solve the graph coloring problem. This method offers two principal advantages. Firstly, it leverages deep reinforcement learning to eliminate the necessity for manual feature construction and data labeling. Secondly, by combining the neural network with Monte Carlo tree search, the neural network can provide comprehensive guidance based on the structural information of the graph, facilitating a more effective balance between exploration and exploitation, thereby leading to superior solutions. Finally, experimental results demonstrate that the method proposed herein has distinct advantages over existing graph coloring algorithms. Moreover, this approach also exhibits outstanding performance when dealing with graph instances whose vertex size surpasses those encountered during the training phase.},
  archive      = {J_JCO},
  author       = {Yang, Wenzhu and Li, Zhanshan},
  doi          = {10.1007/s10878-025-01338-8},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-21},
  shortjournal = {J. Comb. Optim.},
  title        = {Graph coloring problem solving using monte carlo tree search and deep reinforcement learning},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). General sombor index: A study of branching in trees and solution for maximal trees with prescribed maximum degree. <em>JCO</em>, <em>50</em>(2), 1-23. (<a href='https://doi.org/10.1007/s10878-025-01343-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The general Sombor ( $$\mathcal{S}\mathcal{O}_\alpha $$ ) index of a graph G is defined as the sum of weights $$\Big (d^2_x(G) +d^2_y(G)\Big )^\alpha $$ over all edges xy of G, where $$\alpha \ne 0$$ is a real number and $$d_x(G)$$ denotes the degree of a vertex x in G. In this paper, we focus on two specific classes of trees: $${{\mathcal {T}}}_{n,b}$$ , the set of all n-vertex trees with b branching vertices, and $${{\mathcal {T}}}_{n,\Delta }$$ , the set of all n-vertex trees with prescribed maximum degree $$\Delta $$ . Thus the purpose of this paper is twofold concerning the $$\mathcal{S}\mathcal{O}_\alpha $$ index: (i) to characterize the minimal trees in $${{\mathcal {T}}}_{n,b}$$ when $$\alpha > 0$$ , and (ii) to characterize the maximal trees in $${{\mathcal {T}}}_{n,\Delta }$$ when $$0<\alpha < 1$$ . The results of (i) hold true even when the class $${{\mathcal {T}}}_{n,b}$$ is confined to the class of chemical trees and also recover previously known results for the Sombor index. The findings in (ii) resolve a previously posed problem for the $$\mathcal{S}\mathcal{O}_{\alpha }\,(0<\alpha <1)$$ index and, moreover, establish analogous results for the well-known general sum-connectivity index, thereby addressing the corresponding unresolved cases for both indices.},
  archive      = {J_JCO},
  author       = {Ahmad, Sultan and Das, Kinkar Chandra},
  doi          = {10.1007/s10878-025-01343-x},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-23},
  shortjournal = {J. Comb. Optim.},
  title        = {General sombor index: A study of branching in trees and solution for maximal trees with prescribed maximum degree},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quaternion-based formulations for volume maximisation problems. <em>JCO</em>, <em>50</em>(2), 1-35. (<a href='https://doi.org/10.1007/s10878-025-01351-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a mathematical formulation for the problem of determining the optimal position for a three-dimensional item inside a convex container, where its scale can be increased the most and thus its volume maximised. Until now, no methods have been presented that guarantee optimal solutions to this volume maximisation problem while considering continuous free rotation of the item, with approaches relying on heuristics, approximations or enforcing a discrete number of rotations. We aim to find optimal solutions when considering continuous rotation, represented using quaternions. This enables modelling rotation through quadratic constraints. The resulting quadratically constrained problem can be solved to optimality by mathematical solvers. To keep the required computation time within reasonable limits, various improvements to the model such as symmetry breaking are introduced. Experiments show that the majority of our benchmark instances can be solved to optimality within minutes. The expansion to concave containers is also explored, but proves to be more challenging as the required number of quadratic constraints quickly becomes prohibitive.},
  archive      = {J_JCO},
  author       = {Tollenaere, Jonas and Wauters, Tony},
  doi          = {10.1007/s10878-025-01351-x},
  journal      = {Journal of Combinatorial Optimization},
  month        = {9},
  number       = {2},
  pages        = {1-35},
  shortjournal = {J. Comb. Optim.},
  title        = {Quaternion-based formulations for volume maximisation problems},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jgo">JGO - 10</h2>
<ul>
<li><details>
<summary>
(2025). A survey of exact and approximation algorithms for linear-parametric optimization problems. <em>JGO</em>, <em>93</em>(1), 299-333. (<a href='https://doi.org/10.1007/s10898-025-01512-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear-parametric optimization, where multiple objectives are combined into a single objective using linear combinations with parameters as coefficients, has numerous links to other fields in optimization and a wide range of application areas. In this survey, we provide a comprehensive overview of structural results and algorithmic strategies for solving linear-parametric optimization problems exactly and approximately. Transferring concepts from related areas such as multi-objective optimization provides further relevant results. The survey consists of two parts: First, we list strategies that work in a general fashion and do not rely on specific problem structures. Second, we look at well-studied parametric optimization problems and cover both important theoretical results and specialized algorithmic approaches for these problems. Among these problems are parametric variants of shortest path problems, minimum cost flow and maximum flow problems, spanning tree problems, the knapsack problem, and matching problems. Overall, we cover the results from 128 publications (and refer to 35 supplemental works) published between 1963 and 2024.},
  archive      = {J_JGO},
  author       = {Nemesch, Levin and Ruzika, Stefan and Thielen, Clemens and Wittmann, Alina},
  doi          = {10.1007/s10898-025-01512-6},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {299-333},
  shortjournal = {J. Glob. Optim.},
  title        = {A survey of exact and approximation algorithms for linear-parametric optimization problems},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solution methods for partial inverse combinatorial optimization problems in which weights can only be increased. <em>JGO</em>, <em>93</em>(1), 263-298. (<a href='https://doi.org/10.1007/s10898-025-01529-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial inverse combinatorial optimization problems are bilevel optimization problems in which the leader aims to incentivize the follower to include respectively not include given sets of elements in the solution of their combinatorial problem. If the sets of required and forbidden elements define a complete follower solution and the follower problem is solvable in polynomial time, then the inverse combinatorial problem is also solvable in polynomial time. In contrast, partial inverse problems can be NP-complete when the follower problem is solvable in polynomial time. This applies e.g. to the partial inverse min cut problem. In this paper, we consider partial inverse combinatorial optimization problems in which weights can only be increased. Furthermore, we assume that the lower-level combinatorial problem can be solved as a linear program. In this setting, we show that the partial inverse shortest path problem on a directed acyclic graph is NP-complete. Moreover, the partial inverse assignment problem is NP-complete. Both results even hold if there is only one required arc or edge, respectively. For solving partial inverse combinatorial optimization problems with only weight increases, we present a novel branch-and-bound scheme that exploits the difference in complexity between complete inverse and partial inverse versions of a problem. For both primal heuristics and node relaxations, we use auxiliary problems that are basically complete inverse problems on similar instances. Branching is done on follower variables. We test our approach on partial inverse shortest path, assignment and min cut problems, and computationally compare it to an MPCC reformulation as well as a decomposition scheme.},
  archive      = {J_JGO},
  author       = {Ley, Eva and Merkert, Maximilian},
  doi          = {10.1007/s10898-025-01529-x},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {263-298},
  shortjournal = {J. Glob. Optim.},
  title        = {Solution methods for partial inverse combinatorial optimization problems in which weights can only be increased},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A first-order regularized algorithm with complexity properties for the unconstrained and the convexly constrained low order-value optimization problem. <em>JGO</em>, <em>93</em>(1), 241-261. (<a href='https://doi.org/10.1007/s10898-025-01521-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider the minimization of the unconstrained low order-value function. We also consider the case in which the feasible region is given by a closed convex set, assuming that the projection operation is affordable. For both cases, we introduce regularized first-order algorithms and prove worst-case iteration and evaluation complexity results. Asymptotic convergence results are also presented. The proposed algorithm for the case of constraints given by an arbitrary closed convex set has the classical projected gradient method as a particular case. The algorithms are implemented and several numerical examples illustrate their application. From a theoretical point of view, there is no method for the low order-value problem that has a complexity analysis and for which the relation between complexity results and asymptotic results has been analyzed. From a practical point of view, one of the applications considered is the training of a neural network. In this example, it is shown that the introduced method outperforms another recently introduced method that represents the state of the art for solving low order-value problems.},
  archive      = {J_JGO},
  author       = {Álvarez, G. Q. and Birgin, E. G. and Martínez, J. M.},
  doi          = {10.1007/s10898-025-01521-5},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {241-261},
  shortjournal = {J. Glob. Optim.},
  title        = {A first-order regularized algorithm with complexity properties for the unconstrained and the convexly constrained low order-value optimization problem},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal transport-based distributionally robust optimization with polynomial uncertainty. <em>JGO</em>, <em>93</em>(1), 215-240. (<a href='https://doi.org/10.1007/s10898-025-01530-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies distributionally robust optimization (DRO) problems with polynomial uncertainty where the stochastic functions are polynomials of the random variables and the ambiguity set is defined as a ball in the space of probability measures centered at the empirical probability measure. This ball is measured using the optimal transport discrepancy, which includes Wasserstein’s distances as particular cases. The DRO problem can be equivalently reformulated as a linear conic optimization problem with nonnegative polynomial cones when the objective function is affine in the decision variables. We propose a Moment-SOS hierarchy relaxations method for solving the transformed problem and prove its convergent properties. Moreover, we can also obtain the worst-case probability measure. Numerical experiments are presented to illustrate the efficiency of our proposed algorithm.},
  archive      = {J_JGO},
  author       = {Rao, Bo and Yang, Liu and Cai, Jingmin},
  doi          = {10.1007/s10898-025-01530-4},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {215-240},
  shortjournal = {J. Glob. Optim.},
  title        = {Optimal transport-based distributionally robust optimization with polynomial uncertainty},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scenario consensus algorithms for solving stochastic and dynamic problems. <em>JGO</em>, <em>93</em>(1), 175-213. (<a href='https://doi.org/10.1007/s10898-025-01531-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In transportation problems and many other planning problems, there are important sources of uncertainty that must be addressed to find effective and efficient solutions. A common approach for solving these dynamic and stochastic problems is the multiple scenario approach (MSA), which has been proven effective for transportation problems, but it does not provide flexibility for finding solutions that account for all the uncertainty of the problem. Alternative approaches for solving problems with a finite number of scenarios are the progressive hedging algorithm (PHA) and the subgradient algorithm (SA). There are many similarities between PHA and SA; however, there are some differences that lead them to have very different theoretical guarantees and performance. We present a new exact algorithm, the dynamic progressive hedging algorithm (DPHA), for which we provide theoretical guarantees that help to understand both this algorithm and the PHA from a new point of view. We also propose a DPHA-based heuristic (DPHH) and show optimality guarantees for the solution obtained. Our analysis highlights the advantages and disadvantages of the DPHA, the SA, and the MSA, which gives guidance for future research in choosing the proper method for the problem in hand. In a computational study, we consider the stochastic server location problem (SSLP) and the two-stage stochastic assignment and team-orienteering problem (TSSATOP), and we show the empirical performance of the proposed algorithms.},
  archive      = {J_JGO},
  author       = {Lagos, Felipe},
  doi          = {10.1007/s10898-025-01531-3},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {175-213},
  shortjournal = {J. Glob. Optim.},
  title        = {Scenario consensus algorithms for solving stochastic and dynamic problems},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A theoretical and empirical study of new adaptive algorithms with additional momentum steps and shifted updates for stochastic non-convex optimization. <em>JGO</em>, <em>93</em>(1), 113-173. (<a href='https://doi.org/10.1007/s10898-025-01518-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that adaptive optimization algorithms represent the key pillar behind the rise of the machine learning field. In the optimization literature numerous studies have been devoted to accelerated gradient methods but only recently adaptive iterative techniques were analyzed from a theoretical point of view. In the present paper we introduce new adaptive algorithms endowed with momentum terms for stochastic non-convex optimization problems. Our purpose is to show a deep connection between accelerated methods endowed with different inertial steps and AMSGrad-type momentum methods. Our methodology is based on the framework of stochastic and possibly non-convex objective mappings, along with some assumptions that are often used in the investigation of adaptive algorithms. In addition to discussing the finite-time horizon analysis in relation to a certain final iteration and the almost sure convergence to stationary points, we shall also look at the worst-case iteration complexity. This will be followed by an estimate for the expectation of the squared Euclidean norm of the gradient. Various computational simulations for the training of neural networks are being used to support the theoretical analysis. For future research we emphasize that there are multiple possible extensions to our work, from which we mention the investigation regarding non-smooth objective functions and the theoretical analysis of a more general formulation that encompasses our adaptive optimizers in a stochastic framework.},
  archive      = {J_JGO},
  author       = {Alecsa, Cristian Daniel},
  doi          = {10.1007/s10898-025-01518-0},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {113-173},
  shortjournal = {J. Glob. Optim.},
  title        = {A theoretical and empirical study of new adaptive algorithms with additional momentum steps and shifted updates for stochastic non-convex optimization},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new branch-and-bound algorithm for generalized affine multiplicative programming. <em>JGO</em>, <em>93</em>(1), 87-112. (<a href='https://doi.org/10.1007/s10898-025-01519-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a type of affine multiplicative programming (AMP) problem with exponents, which is known to be NP-hard. We initially transform AMP into an equivalent problem (EP) by the logarithmic transformation and the introduction of auxiliary variables. Utilizing a piecewise linear technique, we then develop a mixed-integer linear programming (MILP) relaxation to determine a lower bound for the optimal value of EP. In addition, we propose a successive linear optimization (SLO) method that converges to a KKT point of EP, thereby tightening the upper bound to the optimum of EP. Also, a rectangular contraction rule is introduced to eliminate regions that do not contain the optimal solution of AMP. By combining the MILP relaxation, the SLO method and the rectangular contraction rule, we formulate a new branch-and-bound algorithm for solving EP. Moreover, the convergence and the maximum number of iterations for the algorithm are presented. Finally, numerical experiments are conducted to verify the effectiveness and feasibility of the constructed algorithm.},
  archive      = {J_JGO},
  author       = {Deng, Yaping and Shen, Peiping},
  doi          = {10.1007/s10898-025-01519-z},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {87-112},
  shortjournal = {J. Glob. Optim.},
  title        = {A new branch-and-bound algorithm for generalized affine multiplicative programming},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the numerical solution of lasserre relaxations of unconstrained binary quadratic optimization problem. <em>JGO</em>, <em>93</em>(1), 63-85. (<a href='https://doi.org/10.1007/s10898-025-01523-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to solve linear semidefinite programs arising from higher-order Lasserre relaxations of unconstrained binary quadratic optimization problems. For this we use an interior point method with a preconditioned conjugate gradient method solving the linear systems. The preconditioner utilizes the low-rank structure of the solution of the relaxations. In order to fully exploit this, we need to re-write the moment relaxations. To treat the arising linear equality constraints we use an $$\ell _1$$ -penalty approach within the interior-point solver. The efficiency of this approach is demonstrated by numerical experiments with the MAXCUT and other randomly generated problems and a comparison with a state-of-the-art semidefinite solver and the ADMM method. We further propose a hybrid ADMM-interior-point method that proves to be efficient for certain problem classes. As a by-product, we observe that the second-order relaxation is often high enough to deliver a globally optimal solution of the original problem.},
  archive      = {J_JGO},
  author       = {Habibi, Soodeh and Kočvara, Michal and Stingl, Michael},
  doi          = {10.1007/s10898-025-01523-3},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {63-85},
  shortjournal = {J. Glob. Optim.},
  title        = {On the numerical solution of lasserre relaxations of unconstrained binary quadratic optimization problem},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discretization algorithms for generalized semi-infinite programs with coupling equality constraints under local solution stability. <em>JGO</em>, <em>93</em>(1), 27-61. (<a href='https://doi.org/10.1007/s10898-025-01515-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing algorithms for generalized semi-infinite programs can only handle lower-level constraints containing equality constraints depending on upper-level variables (so-called coupling equality constraints) under limiting assumptions. More specifically, discretization-based algorithms require that the coupling equality constraints result in some lower-level variables being determined uniquely as implicit functions of the other lower-level and upper-level variables. We propose an adaptation of the discretization-based algorithm of Blankenship & Falk and demonstrate it can handle coupling equality constraints under the weaker assumption of stability of the solution set for these constraints in the sense of Lipschitz lower semi-continuity. The key idea is to allow a perturbation of the lower-level variable values from discretization points in connection with changes in the upper-level variables in the discretized upper-level problem. We enforce that these perturbed values satisfy the coupling equality constraints while remaining close to the discretization point, provided we can guarantee the stability of the solution in the sense that a nearby solution exists for small changes of the upper-level variables. We provide concrete realizations of the algorithm for three different situations: i) when knowledge about a certain Lipschitz constant is available, ii) when the coupling equality constraints are assumed to have full rank, and iii) when the coupling equality constraints are additionally linear in the lower-level variables. Numerical experiments on small test problems and a physically motivated problem related to power flow illustrate that the approach can be successfully applied to solve the challenging problems, but is currently limited in terms of scalability.},
  archive      = {J_JGO},
  author       = {Zingler, Aron and Lipow, Adrian W. and Mitsos, Alexander},
  doi          = {10.1007/s10898-025-01515-3},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {27-61},
  shortjournal = {J. Glob. Optim.},
  title        = {Discretization algorithms for generalized semi-infinite programs with coupling equality constraints under local solution stability},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed-integer bilevel optimization with nonconvex quadratic lower-level problems: Complexity and a solution method. <em>JGO</em>, <em>93</em>(1), 1-25. (<a href='https://doi.org/10.1007/s10898-025-01522-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study bilevel problems with a convex quadratic mixed-integer upper-level, integer linking variables, and a nonconvex quadratic, purely continuous lower-level problem. We prove $$\Sigma _2^p$$ -hardness of this class of problems, derive an iterative lower- and upper-bounding scheme, and show its finiteness and correctness in the sense that it computes globally optimal points or proves infeasibility of the instance. To this end, we make use of the Karush–Kuhn–Tucker conditions of the lower-level problem for the lower-bounding step, since these conditions are only necessary but not sufficient in our setting. Moreover, integer no-good cuts as well as a simple optimality cut are used to obtain finiteness of the method. Finally, we illustrate the applicability of our approach by the first large-scale numerical experiment for this class of problems in the literature.},
  archive      = {J_JGO},
  author       = {Bomze, Immanuel and Horländer, Andreas and Schmidt, Martin},
  doi          = {10.1007/s10898-025-01522-4},
  journal      = {Journal of Global Optimization},
  month        = {9},
  number       = {1},
  pages        = {1-25},
  shortjournal = {J. Glob. Optim.},
  title        = {Mixed-integer bilevel optimization with nonconvex quadratic lower-level problems: Complexity and a solution method},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jiis">JIIS - 16</h2>
<ul>
<li><details>
<summary>
(2025). Correction to: Evaluating user story quality with LLMs: A comparative study. <em>JIIS</em>, <em>63</em>(4), 1453-1454. (<a href='https://doi.org/10.1007/s10844-025-00948-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JIIS},
  author       = {Sharma, Amol and Tripathi, Anil Kumar},
  doi          = {10.1007/s10844-025-00948-2},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1453-1454},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Correction to: Evaluating user story quality with LLMs: A comparative study},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating user story quality with LLMs: A comparative study. <em>JIIS</em>, <em>63</em>(4), 1423-1451. (<a href='https://doi.org/10.1007/s10844-025-00939-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the quality of user stories is crucial for the success of agile software development. This paper investigates the efficacy of Large Language Models (LLMs) in assessing the quality of individual user stories using the Quality User Story (QUS) framework, which categorizes quality criteria into syntactic, semantic, and pragmatic dimensions. Leveraging three state-of-the-art LLMs—GPT-4o, GPT-4-Turbo, and GPT-3.5-Turbo—this study employs two prompting strategies: context minimal and context rich, to gauge performance across eight user story quality criteria. To ensure robust validation, we generated 960 user stories using alternative LLMs (Gemini and Meta AI’s LLaMA3), which were then assessed for quality by 69 postgraduate students. The quality assessments were further verified by a team comprising a research scholar and a senior postgraduate student. The evaluation of these 960 user stories by the three LLMs under study reveal significant insights into their relative strengths and weaknesses. The results demonstrate that GPT-4o and GPT-4-Turbo exhibit superior performance in evaluating user stories, particularly excelling in syntactic and pragmatic criteria with minimal impact from additional contextual details. Conversely, GPT-3.5-Turbo reveals noticeable limitations, struggling to maintain effectiveness, particularly when handling richer contextual inputs. This research marks a pivotal step towards automated quality assessment in requirements engineering, highlighting both the potential and areas for improvement in leveraging LLMs for robust user story evaluation.},
  archive      = {J_JIIS},
  author       = {Sharma, Amol and Kumar Tripathi, Anil},
  doi          = {10.1007/s10844-025-00939-3},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1423-1451},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Evaluating user story quality with LLMs: A comparative study},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rumor detection for emergency events via few-shot ensembled prompt learning. <em>JIIS</em>, <em>63</em>(4), 1391-1422. (<a href='https://doi.org/10.1007/s10844-025-00944-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rumors on social media can cause panic during emergency events. Unlike conventional rumor detection, it is more challenging to detect rumors about emergency events that have not happened in history, due to the shortage of relevant corpus. Therefore, we treat emergency events rumor detection as a few-shot learning problem. Recently, large language models (LLMs) such as ChatGPT have been widely considered in various NLP tasks. However, LLMs may face limitations due to non-real-time knowledge and unwillingness to provide direct answers. Prompt learning effectively leverages pre-trained language models (PLMs), and prompt tuning can inject the latest knowledge into PLMs with very few instances. Therefore, we propose a template-based Ensembled Prompt Tuning (EPT) model. We contribute an approach leveraging the knowledge in PLMs to generate label words for verbalizer construction. Furthermore, we treat few-shot rumor detection as an MLM problem and design two types of prompt templates for online posts and comments. An ensemble strategy is introduced to make the final prediction. Experimental results on three datasets demonstrate the effectiveness of the proposed EPT model, outperforming current SOTA on accuracy and F1-score. Ablation study has shown the necessity and effectiveness of the ensemble strategy. Besides, we make comparisons with prevalent LLMs under both few-shot and zero-shot settings, the results show the competitiveness of our EPT model.},
  archive      = {J_JIIS},
  author       = {Su, Chen and Zhou, Junkang and Jiang, Zhentao and Zhu, Shuwei and Li, Chao and Fang, Wei and Lu, Heng-yang},
  doi          = {10.1007/s10844-025-00944-6},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1391-1422},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Rumor detection for emergency events via few-shot ensembled prompt learning},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying influential nodes using semi local isolating centrality based on average shortest path. <em>JIIS</em>, <em>63</em>(4), 1361-1390. (<a href='https://doi.org/10.1007/s10844-025-00943-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex networks, identifying influential nodes becomes critical as these networks emerge rapidly. Extensive studies have been carried out on intricate networks to comprehend diverse real-world networks, including transportation networks, facebook networks, animal social networks, etc. Centrality measures like degree, betweenness, closeness, and clustering centralities are used to find influential nodes, but these measures have limitations in implementation with large-scale networks. These centrality measures are classified into global and local centralities. Semi-local structures perform well compared to local and global centralities but efficient centrality for finding influential nodes remains a challenging issue in large-scale networks. To address this challenge, a Semi-Local Average Isolating Centrality (SAIC) metric is proposed that integrates semi-local and local information to identify important nodes in large networks, along with the relative change in average shortest path. Here, we consider extended neighborhood concept for selecting the nodes nearest neighbors along with the weighted edge policy to find the best influential nodes by using SAIC. Along with these, SAIC also consider isolated nodes which significantly impact the network connectedness by maximizing the number of connected components upon removal. As a result SAIC differentiates itself from other centrality metrics by employing a distributed approach to define semi-local structure and utilizing an efficient edge weighting policy. The analysis of SAIC has been performed on multiple real-time datasets using Kendall tau’s coefficient. Using the Susceptible-Infected-Recovered (SIR) and Independent Cascade(IC) models, the performance of SAIC has been examined to determine maximum information spread in comparison to the most recent metrics in some real-world datasets. Our proposed method SAIC performs better in terms of information spreading when compare with other exisiting methods, with an improvement ranging from 4.11% to 17.9%.},
  archive      = {J_JIIS},
  author       = {Madupuri, ReddyPriya and C.C, Sobin and Enduri, Murali Krishna and Anamalamudi, Satish},
  doi          = {10.1007/s10844-025-00943-7},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1361-1390},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Identifying influential nodes using semi local isolating centrality based on average shortest path},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid transformer based model for sarcasm detection from news headlines. <em>JIIS</em>, <em>63</em>(4), 1339-1359. (<a href='https://doi.org/10.1007/s10844-025-00941-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis becomes significantly challenging when there are traces of sarcastic words or phrases present in text. A common finding of the researchers working on sentiment analysis is that the presence of sarcastic comments escalates the complexity of sentiment analysis drastically. Consequently, it is a common practice to detect the sarcastic elements in a text prior to performing the sentiment analysis so as to increase the accuracy of the same. Sarcastic phrases often have an intrinsic linguistic pattern which the research community is working on during the last few years with an objective to identify them in a generically. But most of the cases the researchers used either rule based or machine learning-driven approach for the detection of sarcasm present in text. The scope of application of deep learning for identifying sarcastic comments is principally due to the scarcity of dataset. In order to bridge the gap, this article presents a novel hybrid transformer based approach that leverages the strengths of RoBERTa, Bidirectional Long Short-Term Memory (Bi-LSTM), and Multi-Head Attention in order to enhance the efficiency of sarcasm detection especially from News Headlines. Our approach combines contextual embeddings, sequential modelling and attention mechanisms to effectively capture the nuances of sarcastic expressions in text. Experimental results demonstrate that our model achieves competitive accuracy and efficiency, providing a robust solution for sarcasm detection tasks.},
  archive      = {J_JIIS},
  author       = {Khan, Amit and Majumdar, Dipankar and Mondal, Bikromadittya},
  doi          = {10.1007/s10844-025-00941-9},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1339-1359},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {A hybrid transformer based model for sarcasm detection from news headlines},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating textual explanations for scheduling systems leveraging the reasoning capabilities of large language models. <em>JIIS</em>, <em>63</em>(4), 1287-1337. (<a href='https://doi.org/10.1007/s10844-025-00940-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling systems are critical for planning projects, resources, and activities across many industries to achieve goals efficiently. As scheduling requirements grow in complexity, the use of Artificial Intelligence (AI) solutions has received more attention. However, providing comprehensible explanations of these decision-making processes remains a challenge and blocker to adoption. The emergent field of eXplainable Artificial Intelligence (XAI) aims to address this by establishing human-centric interpretation of influencing factors for machine decisions. The leading field of autonomous interpretation in Natural Language Processing (NLP) is Large Language Model (LLM)s, for their generalist knowledge and reasoning capabilities. To explore LLMs’ potential to generate explanations for scheduling queries, we selected a benchmark set of Job Shop scheduling problems. A novel framework that integrates the selected language models, GPT-4 and Large Language Model Meta AI (LLaMA), into scheduling systems is introduced, facilitating human-like explanations to queries from different categories through few-shot learning. The explanations were analysed for accuracy, consistency, completeness, conciseness, and language across different scheduling problem sizes and complexities. The approach achieved an overall accuracy of 59% with GPT-4 and 35% with LLaMA, with minimal impact from the varied schedule sizes observed, proving the approach can handle different datasets and is performance scalable. Several responses demonstrated high comprehension of complex queries; however, response quality fluctuated due to the few-shot learning approach. This study establishes a baseline for measuring generalist LLM capabilities in handling explanations for autonomous scheduling systems, with promising results for an LLM providing XAI interactions to explain scheduling decisions.},
  archive      = {J_JIIS},
  author       = {Powell, Cheyenne and Riccardi, Annalisa},
  doi          = {10.1007/s10844-025-00940-w},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1287-1337},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Generating textual explanations for scheduling systems leveraging the reasoning capabilities of large language models},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MF-EDNet: Predicting stock market sector indices based on multi-feature fusion under emergency events. <em>JIIS</em>, <em>63</em>(4), 1265-1286. (<a href='https://doi.org/10.1007/s10844-025-00938-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impact of emergency events on the stock market cannot be underestimated, as their unpredictability poses significant challenges to investors’ stock operations. This calls for researchers and investors to seek more effective features and reasonable methods to mitigate risks. In the context of multi-feature prediction methods, analyzing the correlation between multi-dimensional features or data has always been a challenging issue. This paper proposes a stock market index prediction framework based on an encoder-decoder architecture (MF-EDNet). The framework leverages the dynamic correlation between stock data and futures data as prior knowledge, integrating features of both internal sequences (industry indices) and external sequences (futures data) to capture the impact of emergency events on the stock market. The newly proposed Multi-Dimensional Convolutional Attention Module (MCAM) further enhances the feature extraction and attention capabilities of the attention mechanism. Experiments on multiple industry indices in the Chinese stock market demonstrate that MF-EDNet can effectively extract important features from stock and futures data, exhibiting good predictive performance under emergency events. The proposed MF-EDNet model achieved improvements of 35.8% and 22.9% in the Matthews correlation coefficient (MCC), a 3.3% increase in accuracy (ACC) and a 7.86% enhancement in profit compared to previous state-of-the-art methods.},
  archive      = {J_JIIS},
  author       = {Han, Tianjiao and Yuan, Chenxun and Wang, Pengcheng and Hao, Xingwei and Guo, Fenghua},
  doi          = {10.1007/s10844-025-00938-4},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1265-1286},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {MF-EDNet: Predicting stock market sector indices based on multi-feature fusion under emergency events},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid transformer-CNN architecture for multivariate time series forecasting: Integrating attention mechanisms with convolutional feature extraction. <em>JIIS</em>, <em>63</em>(4), 1233-1264. (<a href='https://doi.org/10.1007/s10844-025-00937-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately forecasting time series data remains a critical challenge, particularly in financial markets where volatility and noise obscure underlying patterns. Traditional deep learning approaches often struggle to simultaneously capture local and global dependencies, limiting their effectiveness in detecting directional changes. To address these challenges, we propose an innovative hybrid model that integrates Transformers with 1D Convolutional Neural Networks (1D-CNN), leveraging their complementary strengths. The self-attention mechanism of Transformers enhances the model’s ability to capture long-term dependencies, while 1D-CNN excels at extracting local patterns and refining feature representations. Unlike conventional models that aim to predict exact values, our approach is explicitly designed to learn and detect changes in trends rather than forecasting precise numerical values. The primary motivation of this work is to improve Directional Accuracy (DA) and Signal Directional Change Detection, two critical factors for robust time series forecasting in financial applications. Our model effectively mitigates the impact of market fluctuations by enhancing trend detection and reducing false signals. Performance evaluation is conducted using specialized metrics, including DA, Trend Consistency Index (TCI), Signal Shift Error (SSE), and Precision of Trend Change (PTC), ensuring a comprehensive assessment of the model’s predictive capabilities. Experimental results demonstrate that our hybrid model significantly outperforms both custom and state-of-the-art architectures, achieving superior accuracy in detecting trend reversals and signal shifts. This research contributes to advancing time series modeling by introducing a modular, scalable, and high-precision forecasting framework, applicable across various domains.},
  archive      = {J_JIIS},
  author       = {El Zaar, Abdellah and Mansouri, Amine and Benaya, Nabil and Bakir, Toufik and El Allati, Abderrahim},
  doi          = {10.1007/s10844-025-00937-5},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1233-1264},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Hybrid transformer-CNN architecture for multivariate time series forecasting: Integrating attention mechanisms with convolutional feature extraction},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective optimization approach for session-based recommendation systems. <em>JIIS</em>, <em>63</em>(4), 1203-1232. (<a href='https://doi.org/10.1007/s10844-025-00935-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems face the persistent challenge of balancing multiple conflicting objectives, such as relevance, diversity, and user engagement, while adapting to the complexities of session-based data. Traditional methods often struggle to address these challenges effectively, particularly when user interactions are diverse, sparse, or structured in short sessions. Moreover, the trade-offs between accuracy-focused metrics and diversity-oriented metrics pose additional hurdles in achieving well-rounded recommendations. This paper presents a novel session-based multi-objective recommendation approach designed to address these challenges. The method employs session clustering to group similar behavioral patterns, reducing complexity and enhancing focus. Within each cluster, focused item subsetting refines the recommendation space, enabling efficient identification of high-performing solutions. Additionally, Differential Evolution (DE)-based optimization facilitates a balance between competing objectives, while cross-cluster knowledge sharing accelerates convergence and enhances generalization by transferring effective strategies between session clusters. Extensive experiments conducted on real-world datasets demonstrate that the proposed approach consistently outperforms state-of-the-art session-based and multi-objective optimization-based recommender systems. These results highlight its ability to adapt to diverse interaction patterns and effectively balance conflicting objectives, making it a practical and scalable solution for modern recommendation systems.},
  archive      = {J_JIIS},
  author       = {Zaizi, Fatima Ezzahra and Qassimi, Sara and Rakrak, Said},
  doi          = {10.1007/s10844-025-00935-7},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1203-1232},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {A multi-objective optimization approach for session-based recommendation systems},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-enhanced representation learning for graph collaborative filtering recommendation models. <em>JIIS</em>, <em>63</em>(4), 1179-1202. (<a href='https://doi.org/10.1007/s10844-025-00933-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph collaborative filtering recommendation models have gained significant attention in recommender systems due to their ability to capture complex user-item relationships through interaction graphs. However, these models often overlook the rich information contained in textual and tabular data, which can provide valuable insights into user preferences and item characteristics. To address this limitation, we propose a model-agnostic multi-task representation learning framework, LLMGCF, which aims to enhance the performance of graph-based recommendation models by integrating textual data enriched by large language models (LLMs) and feature-engineered tabular data. Specifically, our framework leverage contrastive learning to align semantic signals derived from LLM-enhanced textual data, attribute-based tabular signals, and collaborative graph signals, enabling effective cross-modal knowledge fusion. Additionally, LLMGCF utilizes multi-task learning to jointly optimize the supervised recommendation retrieval task and the cross-modal knowledge alignment task. Experimental results on two public datasets demonstrate that LLMGCF outperforms state-of-the-art (SOTA) graph collaborative filtering models, achieving average improvements of approximately 4.17% in Recall and 3.68% in NDCG. Furthermore, our framework exhibits strong robustness against random noise, highlighting its practical applicability in real-world scenarios.},
  archive      = {J_JIIS},
  author       = {Mou, Daen and Wei, Zhihua and Ni, Lin and Song, Na and Sun, Yiwei and Chu, Weizhong and Jin, Benkai},
  doi          = {10.1007/s10844-025-00933-9},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1179-1202},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {LLM-enhanced representation learning for graph collaborative filtering recommendation models},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distant supervised relation extraction with label entailment and collaborative denoising. <em>JIIS</em>, <em>63</em>(4), 1153-1177. (<a href='https://doi.org/10.1007/s10844-025-00932-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distant supervision automatically generates large-scale annotated data for relation extraction by aligning texts with knowledge bases, reducing the dependence on human annotation. However, distant supervision relation extraction inevitably introduces label noise, including false positive (FP) noise caused by neglecting sentence meanings and false negative (FN) noise due to the incompleteness of knowledge bases. Previous sentence-level methods mainly focus on the FP noise and ignore the FN noise, which induces severe misleading in both training and testing procedures. To address this issue, we propose a novel two-stage sentence-level noise reduction framework that explicitly tackles both the FN and FP problems. At stage one, we perform noise-filtering with label entailment, which filters out the FN noise before training through semantic matching between the negative instance and every relation label. At stage two, we propose robust training with collaborative denoising, which dynamically removes FP noise during training by maintaining two relation classifiers simultaneously and enabling them to learn useful knowledge from each other. Experimental results show that our method achieves significant improvements over previous state-of-the-art methods on two widely-used benchmarks. For example, a 2.55% F1 score improvements on NYT-10 dataset with BiLSTM implementation is achieved. Moreover, we validate the effectiveness of our method in reducing both the FN and FP noise.},
  archive      = {J_JIIS},
  author       = {Xie, Tingyu and Li, Qi and Wang, Gaoang and Wang, Hongwei},
  doi          = {10.1007/s10844-025-00932-w},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1153-1177},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Distant supervised relation extraction with label entailment and collaborative denoising},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-training dual-network for denoising federated recommendation. <em>JIIS</em>, <em>63</em>(4), 1129-1151. (<a href='https://doi.org/10.1007/s10844-025-00930-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated recommendation (FR) offers users personalized recommendation services while safeguarding their privacy. Conventional FR considers all items that users interact with as items that users like. However, this assumption fails to capture genuine user preferences due to some noisy samples, in which users interact with items they do not like. Most research in FR disregards such noisy samples, consequently inducing local models to learn inaccurate user preferences. Since the global model is aggregated from local models, its performance is compromised, adversely affecting user experience. Furthermore, data heterogeneity, privacy protection, and communication burden requirements make denoising FR more complicated. In this paper, we propose a Self-Training Dual-Network (STDFed) for denoising FR. Specifically, each client’s global and local models inherently form a dual-network. On each client, STDFed considers samples with high predicted probabilities from the dual-network as clean samples to construct a self-training dataset, while the remaining are treated as unlabeled samples. The self-training dataset replaces the original dataset to support local training. In subsequent rounds, STDFed identifies the unlabeled samples with low predicted probabilities as noisy samples, which will be either discarded or labeled as 0 and then added to the self-training dataset. STDFed is model-agnostic and can be applied to most FR. Extensive experiments show that STDFed improves the performance of FR by an average of 6.96% across multiple datasets without increasing communication costs.},
  archive      = {J_JIIS},
  author       = {Liu, Pingshan and He, Haoning and Lu, Guoxin},
  doi          = {10.1007/s10844-025-00930-y},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1129-1151},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Self-training dual-network for denoising federated recommendation},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantification of part-of-speech relationships for aspect sentiment triplet extraction. <em>JIIS</em>, <em>63</em>(4), 1105-1127. (<a href='https://doi.org/10.1007/s10844-025-00929-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect Sentiment Triple Extraction (ASTE) is a fine-grained aspect-level sentiment analysis aimed at extracting aspect words, opinion words, and the sentiment polarity between them in text. Existing end-to-end ASTE models tend to ignore information such as the potential lexical relationships between words and their differences in importance, and fail to take full advantage of the interaction between lexical features and dependency features. To address the above problem, we propose a Quantification of Part-of-speech Relationships for Aspect Sentiment Triplet Extraction (QoPR-ASTE). The model first extracts contextual semantic features by combinatorial encoder; then analyzes the connection and dependency between lexical pairs of words to mine potential syntactic information, and assigns weights according to the importance, constructs lexical adjacency weight matrices and dependency type weight matrices, so as to remove the redundant information and strengthen the expression of syntactic features; then extracts the syntactic information by the two-group graphical convolutional network and the syntactic interaction module and fusion; finally, the semantic and syntactic information is fused through the multi-head attention graph transformation module, and the sentence is labeled using word pair relations and decoded to extract the triad. The F1 values of this model on the four public datasets demonstrate an improvement of 0.80%, 1.46%, 0.92%, and 0.78% compared to the PBLUN model, respectively. The experimental results indicate that the model is capable of efficiently mining potential semantic and syntactic information, thereby significantly enhancing the accuracy of triplet extraction.},
  archive      = {J_JIIS},
  author       = {Wang, Jiacan and Liu, Jianhua and Ke, Tianci and Chen, Kewei and Cai, Zijie and Xu, Ge},
  doi          = {10.1007/s10844-025-00929-5},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1105-1127},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Quantification of part-of-speech relationships for aspect sentiment triplet extraction},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble time series models for stock price prediction and portfolio optimization with sentiment analysis. <em>JIIS</em>, <em>63</em>(4), 1079-1103. (<a href='https://doi.org/10.1007/s10844-025-00928-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The work introduces a hybrid ensemble model that combines conventional stock market prediction models with sentiment analysis of news articles in order to improve the accuracy of predictions. By utilizing the advantages of Long Short-Term Memory(LSTM), Gated Recurrent Unit(GRU), Bidirectional LSTM (BiLSTM), and Recurrent Neural Network(RNN) models in an ensemble framework, we attain an impressive average prediction accuracy of 91.89% where our model was evaluated on ten stocks and surpassed the performance of current models. This outcome underscores the need to integrate news sentiment with technical indicators to get a thorough comprehension of market dynamics. Moreover, the proposed model-driven portfolio regularly outperforms the Nifty 50 benchmark at different risk tolerance levels (0.3, 0.5, and 0.7), generating a stable positive alpha. This indicates greater returns when adjusted for risk. The model’s ability to adapt to the varying needs of investors is demonstrated by the performance it achieved across risk profiles. The proposed model is also compared with the existing models to show the model’s efficiency.},
  archive      = {J_JIIS},
  author       = {Narayana, Malineni Lakshmi and Kartha, Arundhati J and Mandal, Ankur Kumar and P, Roshini and Suresh, Akshaya and Jose, Arun Cyril},
  doi          = {10.1007/s10844-025-00928-6},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1079-1103},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Ensemble time series models for stock price prediction and portfolio optimization with sentiment analysis},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multi-round retrieval with knowledge distillation for sequential recommendation. <em>JIIS</em>, <em>63</em>(4), 1055-1077. (<a href='https://doi.org/10.1007/s10844-025-00926-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential Recommendation (SR) involves predicting the next item that a user is likely to interact with based on their historical interactions. SR models examine the sequence of a user’s actions to analyze complex behavioral patterns and capture diverse user preferences. However, existing works primarily rely on a single-round inference paradigm, which limits their ability to capture the ever-changing diversity of user preferences, and overlooks the influence of user noisy interactions. In this work, we propose MRKD, an adaptive multi-round retrieval framework for sequential recommendation via past-future knowledge distillation. MRKD comprises three key modules: user-wise translator, item-wise translator and past-future knowledge distillation. User-wise and item-wise translator extract meaningful context information from multi-round retrieval processes for refining the representations of items and users in proximity to the target item. The past-future knowledge distillation is to supervise the contextual aggregation process and prevent information loss via distilling valuable knowledge from users’ future interactions. We conduct experiments on five datasets and compare MRKD with 10 competitive baselines to evaluate its performance. Experimental results demonstrate the superiority of our MRKD, equipped with the adaptive multi-round retrieval strategy, over existing state-of-the-art models.},
  archive      = {J_JIIS},
  author       = {Mo, Yuhua and Liu, Yang and Ye, Chaowen and Cheng, Zhangtao and Deng, Chao and Zhuo, Zhencheng and Chen, Kaidi and Zhou, Fan},
  doi          = {10.1007/s10844-025-00926-8},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1055-1077},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Adaptive multi-round retrieval with knowledge distillation for sequential recommendation},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network translations for building SentiWordNets. <em>JIIS</em>, <em>63</em>(4), 1033-1054. (<a href='https://doi.org/10.1007/s10844-024-00911-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A SentiWordNet (SWN) is a WordNet, in which the synsets are annotated with sentiment scores. The initial steps for building a new SWN in a target language are similar to those for building a new WordNet. In particular, the creator may translate existing SWNs or WordNets to the target language or expand a seed sentiment lexicon. The next step involves training classifiers to identify the sentiment of the synset members. A major issue in building a new SWN is the lack of language resources for translating existing SWNs or WordNets to the target language and creating a dataset for training sentiment classifiers. Bilingual dictionaries are reliable aids in translation, but are expensive, may not be available, and time-consuming to construct. With the rapid development of artificial neural networks, machine translation systems have become commonplace and effective for a significant number of language pairs from around the world. Creating datasets for training machine translation models is also arguably cheaper than constructing bilingual dictionaries from scratch. This paper proposes effective approaches for constructing new SWNs using neural network translation systems. We introduce strategies for selecting synset members from translation candidates and computing sense-orders for these synset members. Our approaches are able to construct a new SWN in any language if the language is supported by machine translators.},
  archive      = {J_JIIS},
  author       = {Lam, Khang Nhut and Le, Trung Phuong and Ngu, Khanh Cong and Le, Kien Trung and Le, Phuc Minh and Nguyen, Huy Hoang-Dang and Kalita, Jugal},
  doi          = {10.1007/s10844-024-00911-7},
  journal      = {Journal of Intelligent Information Systems},
  month        = {8},
  number       = {4},
  pages        = {1033-1054},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Neural network translations for building SentiWordNets},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jim">JIM - 35</h2>
<ul>
<li><details>
<summary>
(2025). Vision-centric 3D point cloud technique and custom gripper process for parcel depalletisation. <em>JIM</em>, <em>36</em>(7), 5179-5195. (<a href='https://doi.org/10.1007/s10845-024-02497-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based in-truck parcel recognition plays a key role in providing picking guidance for automated robotic in-truck parcel-unloading systems. The complexity of the parcel system and the variety of colours and shapes of the target objects significantly affect the quality of the results. To establish an effective in-truck parcel depalletisation system, it is crucial to develop a method that can automatically recognise parcels in a 3D environment and guide robots during unloading tasks. To address these requirements, this study proposes a system for detecting geometric point clouds in parcels that uses regression knn to find the nearest pick-up point of a detected parcel box by calculating the minimum Euclidean distance, thereby improving detection accuracy. The validation of the robotic system underlines its practical utility, demonstrating its potential to replace humans and reduce labour costs in factory environments.},
  archive      = {J_JIM},
  author       = {Kim, Seongje and Lee, Kwang-Hee and Kim, Changgyu and Yoon, Jonghun},
  doi          = {10.1007/s10845-024-02497-x},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {5179-5195},
  shortjournal = {J. Intell. Manuf.},
  title        = {Vision-centric 3D point cloud technique and custom gripper process for parcel depalletisation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Welding defects recognition based on DCP-MobileViT network. <em>JIM</em>, <em>36</em>(7), 5163-5178. (<a href='https://doi.org/10.1007/s10845-024-02500-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Welding defects recognition based on machine vision provides a reliable basis for robot arc welding. However, due to such factors as severe noise interference of welding images, slight morphological differences of same defect, and restricted computational capacity of on-site devices, it has been a challenge to learn distinguishable characteristics from various weld seam defects as well as to improve defects recognition accuracy and model generalizability, with a lightweight network. Consequently, we propose a welding defects recognition approach by designing a novel DCP-MobileViT network to address this challenge. First, the denoised image and its corresponding transmission map are obtained by Dark Channel Prior (DCP) algorithm and served as two inputs of the proposed network. Then, a dual-branch network is designed to adaptively extract and merge feature information of two input images through the convolution and transformer mechanism. Finally, the proposed DCP-MobileViT model is tested and compared with three other models using datasets from different welding scenarios. The results indicate that the DCP-MobileViT model achieves superior welding defects recognition accuracy compared to the other models, demonstrating its excellent generalizability in different welding scenarios.},
  archive      = {J_JIM},
  author       = {Zhang, Yue and Zhan, Qiang},
  doi          = {10.1007/s10845-024-02500-5},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {5163-5178},
  shortjournal = {J. Intell. Manuf.},
  title        = {Welding defects recognition based on DCP-MobileViT network},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep convolutional neural network object net model based cognitive digital twin for trust in human–robot collaborative manufacturing. <em>JIM</em>, <em>36</em>(7), 5141-5161. (<a href='https://doi.org/10.1007/s10845-024-02501-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Cognitive Digital Twin (CDT) is an artificial intelligence enhanced version of a digital twin, mirroring and learning from its physical counterpart. A cognitive digital twin in Human–Robot Collaborative (HRC) manufacturing generates synthetic robot data to build trust through simulating and monitoring collaborative behaviours, enhancing efficiency and safety. This encounter issues like restricted trust, inefficiency in extracting crucial details, difficulties in precise detection and alignment of bounding boxes with object boundaries, and high computational complexity. To overcome these challenges, a Deep Convolutional Neural Network Object Net (DCNNONet) model is proposed. The adaptive non local moment mean filter enhances image preprocessing, optimizing noise suppression and texture preservation. Utilizing proposed model, which includes featuring EfficientNetB7, a hybrid pixel unshuffled network, and long-edge decomposition rotated bounding box encoding, effectively tackles these issues, surpasses in feature extraction and precise object detection. Additionally, the introduced enhanced growth optimizer refines model parameters, reducing overall computational complexity. The introduced scheme exhibits better performance, attaining high accuracy, mean average precision, and sensitivity, and F1-Score of 99.96%, 98.5%, 99.75% and 99.89%, respectively. This model enhances trust by providing reliable insights, increasing accuracy in manufacturing scenario interpretation, and improving precision, optimizing performance in collaborative environments.},
  archive      = {J_JIM},
  author       = {Ramkumar, A. and Balasubramanian, Gopinath},
  doi          = {10.1007/s10845-024-02501-4},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {5141-5161},
  shortjournal = {J. Intell. Manuf.},
  title        = {Deep convolutional neural network object net model based cognitive digital twin for trust in human–robot collaborative manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A feature extraction method for intelligent chatter detection in the milling process. <em>JIM</em>, <em>36</em>(7), 5113-5139. (<a href='https://doi.org/10.1007/s10845-024-02486-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machining, especially in milling, chatter refers to self-induced vibrations that arise and grow due to the dynamic interaction of the cutting process. It can result in poor surface finish, inaccurate dimensions, and increased tool wear. Although the development of strategies for extracting chatter features in milling processes has improved over time, the milling environment’s complexity continues to pose challenges. Variational mode decomposition (VMD) is a commonly employed method for extracting features from chatter signals and has gained widespread use. However, the optimal determination of critical VMD parameters, such as the parameter of penalty (α) and the modes (K), is challenging because they significantly affect the decomposition result. To address this limitation, the basic VMD algorithm is embedded into the Bayesian Optimization Algorithm (BO), called the VMD-BO algorithm, to automatically select the best parameter combination of (α) and (K). To enhance the capabilities of extracting chatter features, the VMD-BO algorithm is combined with slope entropy (SlopEn), and the decision tree (DT) algorithm is used to intelligently detect the severity level of chatter. Therefore, this study introduces a novel hybrid method for extracting chatter features, combining VMD-BO, SlopEn, and DT algorithms to enhance intelligent chatter detection. Optimization results of the simulation chatter indicate that the VMD-BO algorithm achieves the highest kurtosis value, demonstrating superior decomposition performance compared to existing optimization methods. Additionally, results of the experiment with the measured vibration signals reveal that SlopEn outperforms three other entropy measures—fuzzy entropy (FuzzEn), sample entropy (SampEn), and permutation entropy (PermEn)—in terms of detection accuracy. However, its detection rate can be improved with an increase in the number of features. Finally, the proposed method can outperform other methods with two features, where the achievement of classification accuracy on the validation set is 96.67%. In addition, a milling test is done on a stepped work-piece with varying machining conditions to confirm the efficacy of chatter detection for online monitoring. In model application, the built model accurately identifies all machining states, including those in transition.},
  archive      = {J_JIM},
  author       = {Jauhari, Khairul and Rahman, Achmad Zaki and Al Huda, Mahfudz and Azka, Muizuddin and Widodo, Achmad and Prahasto, Toni},
  doi          = {10.1007/s10845-024-02486-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {5113-5139},
  shortjournal = {J. Intell. Manuf.},
  title        = {A feature extraction method for intelligent chatter detection in the milling process},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive fabrication of material extrusion-AM process using machine learning algorithms for print process optimization. <em>JIM</em>, <em>36</em>(7), 5087-5111. (<a href='https://doi.org/10.1007/s10845-024-02495-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material extrusion (ME) is an extensively used technique in additive manufacturing for making parts and prototypes. Optimizing the relationship between build time and material usage in ME is challenging but essential for improving production performance. This study aims to find the ideal parameter settings to minimize both time and material consumption. Machine learning (ML) models and a multidisciplinary evolution algorithm were used to predict and optimize the fused deposition process parameters. A definitive screening design (DSD) was conducted on 14 process parameters to identify those that significantly affect the output. Using the response surface method, a dataset of these influential parameters was created for the ML models. The prediction accuracy of the ML models was evaluated using regression metrics. It has been found that the random forest algorithm has a prediction accuracy greater than 90% over the other models after training and testing the dataset. To further optimize the process, the Non-dominated Sorting Genetic Algorithm was utilized to fine-tune the hyperparameters of the best-performing machine-learning model. The results showed that using the optimal parameter settings identified by NSGA-II, build time was reduced by 37% and material usage by 40% compared to standard print settings. A statistical two-way ANOVA test confirmed that these optimized settings significantly (p < 0.05) reduced both time and material consumption.},
  archive      = {J_JIM},
  author       = {Sridhar, S. and Venkatesh, K. and Revathy, G. and Venkatesan, M. and Venkatraman, R.},
  doi          = {10.1007/s10845-024-02495-z},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {5087-5111},
  shortjournal = {J. Intell. Manuf.},
  title        = {Adaptive fabrication of material extrusion-AM process using machine learning algorithms for print process optimization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open-set domain adaptation fusion method based on weighted adversarial learning for machinery fault diagnosis. <em>JIM</em>, <em>36</em>(7), 5067-5086. (<a href='https://doi.org/10.1007/s10845-024-02496-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional closed-set diagnostic methods assume identical label spaces for the source and target domains. Nevertheless, unlike the source domain, the target domain can include emerging unknown categories, and the target known categories are usually the subset of the source known categories. To deal with this open-set diagnostic problem, the paper presents a new open-set domain adaptation fusion approach using weighted adversarial learning (OSDAF). The OSDAF method requires no threshold judgment, and integrates three sub-models to achieve higher accuracy. An adaptive weighted learning strategy is developed and introduced to two adversarial learning classifiers for the first sub-model. The output difference between the constructed classifiers is maximized to identify target unknown category. Also, the output discrepancy is minimized to align target known-category samples with the same category of the source known-category samples. Then, three binary cross-entropy strategies and an entropy minimization scheme are designed to promote the discrepancy between known and unknown categories, thereby generating discriminant features and establishing the decision boundaries. For the other two sub-models, discriminant features are extracted from the first sub-model and applied to two different label propagation methods, so as to enhance the diversity of the recognition sub-models and facilitate the identification of the target samples. Finally, the efficiency and advantage of the presented approach are assessed using three machinery datasets. The comparison results reveal that the presented OSDAF is an effective method and outperforms typical closed-set and open-set diagnostic methods.},
  archive      = {J_JIM},
  author       = {She, Bo and Tan, Fangyin and Zhao, Yang and Dong, Haidi},
  doi          = {10.1007/s10845-024-02496-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {5067-5086},
  shortjournal = {J. Intell. Manuf.},
  title        = {Open-set domain adaptation fusion method based on weighted adversarial learning for machinery fault diagnosis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard task-based dual-aligned meta-transfer learning for cross-domain few-shot fault diagnosis. <em>JIM</em>, <em>36</em>(7), 5051-5065. (<a href='https://doi.org/10.1007/s10845-024-02489-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mainstream transfer learning techniques are highly effective in addressing the issue of limited target domain samples in fault diagnosis. However, when there are insufficient samples in the source domain, the transfer results are often poor. Meta-learning is a method that involves training models by constructing meta-tasks and generalizing them to new unseen tasks, offering a solution to the challenge of limited training samples. To address the few-shot problem of poor transfer effect caused by limited source domain samples under variable working conditions, this paper proposes a hard task-based dual-aligned meta-transfer learning (HT-DAMTL) method. Firstly, a dual-aligned meta-transfer framework is proposed, which embeds the designed cross-domain knowledge transfer structure (CDKTS) into the outer loop of meta-learning to achieve external transfer of meta-knowledge. The CDKTS method combines the use of multi-kernel maximum mean discrepancy (MK-MMD) with a domain discriminator to extract features that are invariant across different domains. Secondly, a meta-training method called information entropy-based reorganization hard task (RHT) is introduced to enhance the meta-model’s feature learning on hard samples, leading to improved fault diagnosis accuracy. Finally, HT-DAMTL’s performance is validated on public and private bearing datasets, showing its superiority over other methods.},
  archive      = {J_JIM},
  author       = {Shang, Zhiwu and Liu, Hu and Li, Wanxiang and Wu, Zhihua and Cheng, Hongchuan},
  doi          = {10.1007/s10845-024-02489-x},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {5051-5065},
  shortjournal = {J. Intell. Manuf.},
  title        = {Hard task-based dual-aligned meta-transfer learning for cross-domain few-shot fault diagnosis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and efficient computing for deep learning-based defect detection models in lightweight devices. <em>JIM</em>, <em>36</em>(7), 5035-5050. (<a href='https://doi.org/10.1007/s10845-024-02487-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect anomaly detection is beneficial in the production cycle of various industries. It is widely used in areas such as metal surface and fabric industries. This paper focuses on deep learning-driven defect detection models using energy-efficient computing. We concentrate on a segmentation-based defect detection model for metal surface anomaly detection, while we deal with a deconvolution-based defect detection model for fabric defects in this work. We propose a depth-wise convolution structure for the segmentation-based visual defect detection model. In addition, we apply the optimizations supported by the inference engine to two models. The segmentation-based defect detection model inference is approximately 10 $$\times $$ faster than the original. Furthermore, the real-time requirement is achieved in a lightweight vision processing unit (VPU) device with a power consumption of only 1.5 Watts for the fabric defect detection model. The practical values of this work are multifaceted, offering substantial benefits in terms of cost reduction, product quality, real-time processing, energy efficiency, and scalability. These advancements not only improve operational efficiency but also contribute to sustainability efforts and provide a competitive advantage in the industry.},
  archive      = {J_JIM},
  author       = {Fişne, Alparslan and Kalay, Alperen and Eken, Süleyman},
  doi          = {10.1007/s10845-024-02487-z},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {5035-5050},
  shortjournal = {J. Intell. Manuf.},
  title        = {Fast and efficient computing for deep learning-based defect detection models in lightweight devices},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated model selection for multivariate anomaly detection in manufacturing systems. <em>JIM</em>, <em>36</em>(7), 5015-5033. (<a href='https://doi.org/10.1007/s10845-024-02479-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As machine learning is widely applied to improve the efficiency and effectiveness of manufacturing systems, the automated selection of appropriate algorithms and hyperparameters becomes increasingly important. This paper presents a model selection approach to multivariate anomaly detection for applications in manufacturing systems using a multi-output regression-based meta-learning method. The proposed method exploits the capabilities of meta-learning to explore and learn the intricate relationships within multivariate data sets in order to select the best anomaly detection model. It also facilitates the construction of an ensemble of algorithms with dynamically assigned weights based on their respective performance levels. In addition to the framework, new meta-features for the application domain are presented and evaluated. Experiments show the proposed method can be successfully applied to achieve significantly better results than benchmark approaches. This enables an automated selection of algorithms that can be used for enhanced anomaly detection under changing operating conditions.},
  archive      = {J_JIM},
  author       = {Engbers, Hendrik and Freitag, Michael},
  doi          = {10.1007/s10845-024-02479-z},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {5015-5033},
  shortjournal = {J. Intell. Manuf.},
  title        = {Automated model selection for multivariate anomaly detection in manufacturing systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic confidence-based constraint adjustment in distributional constrained policy optimization: Enhancing supply chain management through adaptive reinforcement learning. <em>JIM</em>, <em>36</em>(7), 4997-5013. (<a href='https://doi.org/10.1007/s10845-024-02492-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce the dynamic confidence-based constraint adjustment (DCCA) approach, an innovative enhancement to the distributional constrained policy optimization (DCPO) algorithm, tailored to optimize decision-making process in intricate supply chain management environments. DCCA continuously tunes the reshaping parameter in response to real-time confidence estimations in satisfying operational constraints, enabling more adaptive and risk-aware policy updates. Through a comprehensive evaluation involving a multi-echelon, multi-period supply chain case study, DCCA demonstrates superior performance in balancing return maximization with stringent constraint adherence, outperforming traditional baseline algorithms such as Vanilla TRPO, Saute TRPO, CPO, and DCPO. Our results, highlighted by reduced variability in performance metrics and improved average returns, underscore DCCA’s effectiveness in navigating the intricate trade-offs between risk and reward in dynamic supply chain scenarios. This study not only validates DCCA’s theoretical underpinnings but also establishes its practical applicability, offering a promising avenue for advancing supply chain optimization methodologies.},
  archive      = {J_JIM},
  author       = {Boutyour, Youness and Idrissi, Abdellah},
  doi          = {10.1007/s10845-024-02492-2},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4997-5013},
  shortjournal = {J. Intell. Manuf.},
  title        = {Dynamic confidence-based constraint adjustment in distributional constrained policy optimization: Enhancing supply chain management through adaptive reinforcement learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Web tension AI modeling and reconstruction for digital twin of roll-to-roll system. <em>JIM</em>, <em>36</em>(7), 4977-4995. (<a href='https://doi.org/10.1007/s10845-024-02488-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twins (DT) are gaining attention as an emerging technology in Smart manufacturing systems. These DTs comprise various units that enable simulation, monitoring, and prediction of the manufacturing process. This study introduces a predictive model for web tension and a tension reconstruction algorithm for the DT of the roll-to-roll (R2R) system. The observed web tension signals from tension sensors decomposed into a mean component, a sinusoidal wave, and a random noise. Utilizing deep neural networks, the predictive model integrated various sub-models to forecast statistical (mean, standard deviation) and frequency domain (main frequency, signal-to-noise ratio) features of the web tension signal. Through fivefold cross-validation, 23 model architectures were optimized, with selected architectures ranging from 16-32-32-1 to 16-32-64-32-1 nodes per layer. Overall, R2 scores on the test set ranged from approximately 52 to 100%. The proposed reconstruction algorithm generated tension signals from the model’s predictions that closely resemble the original tension signals, indicating credible reconstructions. The proposed predictive model and reconstruction algorithm were integrated into the DT of the R2R system, offering a valuable tool for monitoring and optimizing the R2R process.},
  archive      = {J_JIM},
  author       = {Gafurov, Anton Nailevich and Kim, Jaeyoung and Kim, Inyoung and Lee, Taik-Min},
  doi          = {10.1007/s10845-024-02488-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4977-4995},
  shortjournal = {J. Intell. Manuf.},
  title        = {Web tension AI modeling and reconstruction for digital twin of roll-to-roll system},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Industrial vision inspection using digital twins: Bridging CAD models and realistic scenarios. <em>JIM</em>, <em>36</em>(7), 4963-4975. (<a href='https://doi.org/10.1007/s10845-024-02485-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a new industrial visual inspection method that emphasizes the application of computer-aided design (CAD) models. This method significantly reduces the dependence on acquiring and annotating extensive real-scene data, subsequently expediting the development of visual inspection models. The paper highlights two pivotal contributions. Firstly, we introduce a configurable 3D rendering technology that digitally simulates different states of the product, achieving automatic batch generation and labeling of training data. This feature distinguishes our work from existing methods. Secondly, we designed a domain generalization method based on second-order statistics. This approach effectively addresses the domain shift challenge between synthetic and actual production data, enhancing the model’s generalization capabilities. This represents a noteworthy advancement in the field as it boosts the model’s adaptability to real-world scenarios. Our method has demonstrated impressive performance, achieving accuracy rates of 94.30 $$\%$$ , 96.75 $$\%$$ , and 97.35 $$\%$$ on component model classification, motor defect recognition, and rotating motor brush holder datasets, respectively. These results not only validate the efficacy of our domain generalization method but also underscore the potential of using CAD model data for industrial visual inspection. In summary, our research has created a new method for integrating industrial visual inspection into digital twin ecosystems, highlighting the potential for significant improvements in this field.},
  archive      = {J_JIM},
  author       = {Wang, Fangjun and Wu, Jianhao and Yang, Zhouwang and Song, Yanzhi},
  doi          = {10.1007/s10845-024-02485-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4963-4975},
  shortjournal = {J. Intell. Manuf.},
  title        = {Industrial vision inspection using digital twins: Bridging CAD models and realistic scenarios},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability-improved machine learning model using knowledge-embedded learning approach for smart manufacturing. <em>JIM</em>, <em>36</em>(7), 4941-4962. (<a href='https://doi.org/10.1007/s10845-024-02482-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models play a crucial role in smart manufacturing by revolutionizing industrial automation so as to boost productivity and product quality. However, the reliability of these models often faces challenges from factors such as data drift, concept drift, adversarial attacks, and increasing model complexity. In addressing these challenges, this paper proposes a novel approach called Reliability Improved Machine Learning (RIML), which leverages on prior knowledge by incorporating it into the machine learning pipeline through a secondary output that is easily verifiable and assessable within the application domain. Built upon the Knowledge-embedded Machine Learning (KML) framework, RIML differs from conventional strategies by modifying the model’s architecture. In its implementation, additional layers were introduced, specifically designed to identify and discard misclassified cases to improve the model’s reliability. RIML’s efficacy was successfully demonstrated through a simulated dataset and three real use-case studies, namely, a general walk/run scenario, an industry-related case using metro railway dataset, and a smart manufacturing application on gas detection. The promising results highlighted RIML’s ability to significantly reduce misclassifications, thereby enhancing model reliability in diverse real-world scenarios.},
  archive      = {J_JIM},
  author       = {Farbiz, Farzam and Aggarwal, Saurabh and Karol Maszczyk, Tomasz and Habibullah, Mohamed Salahuddin and Hamadicharef, Brahim},
  doi          = {10.1007/s10845-024-02482-4},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4941-4962},
  shortjournal = {J. Intell. Manuf.},
  title        = {Reliability-improved machine learning model using knowledge-embedded learning approach for smart manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic multi-layer cognitive model for intelligent machine tool. <em>JIM</em>, <em>36</em>(7), 4915-4939. (<a href='https://doi.org/10.1007/s10845-024-02481-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the basic manufacturing capabilities provide unit of the production system, the intelligent level of the CNC machine tool will affect the realization of intelligent manufacturing. Academia has carried out a lot of intelligent research on CNC machine tool from technical perspective, but there still needs a systematic cognitive model to promote the construction of cognitive abilities, to support the intelligent realization and continuous improvement of CNC machine tool. Therefore, this paper proposes a three-part, seven-layer cognitive model based on cognitive informatics to promote the construction of cognitive abilities and the intelligent transformation of CNC machine tool. Firstly, a systematic multi-layer cognitive model is proposed, and each cognitive layer is introduced to promote the different cognitive abilities construction of CNC machine tool. Then, this paper introduces the cognitive analysis loop and the cognitive learning loop contained in the multi-layer cognitive model, which can promote the construction of the adaptive and continuous learning abilities of CNC machine tool. The evaluation indicators of the intelligence machine tool are given, which is used to evaluate machine tool intelligence model. Furthermore, the cognitive enabling technologies of the multi-layer cognitive model for intelligent machine tool is presented, which supports the realization of cognitive abilities such as analysis, decision making, and learning. Finally, the feasibility of the proposed systematic multi-layer cognitive model is verified by the developed computable digital twin platform and comparison before and after implementation for intelligent machine tool.},
  archive      = {J_JIM},
  author       = {Jiang, Tengyuan and Zhou, Jingtao and Luo, Xiang and Wang, Mingwei and Zhang, Shusheng},
  doi          = {10.1007/s10845-024-02481-5},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4915-4939},
  shortjournal = {J. Intell. Manuf.},
  title        = {A systematic multi-layer cognitive model for intelligent machine tool},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital-triplet: A new three entities digital-twin paradigm for equipment fault diagnosis. <em>JIM</em>, <em>36</em>(7), 4895-4914. (<a href='https://doi.org/10.1007/s10845-024-02471-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current equipment fault diagnosis faces challenges due to the difficulties in arranging sensors to collect effective data and obtaining diverse fault data for studying fault mechanisms. The lack of data results in disconnection between data from different spaces, posing a challenge to forming a closed loop of data and hindering the development of digital twin (DT) driven fault diagnosis (FD). To address these issues, a new DT paradigm Digital-Triplet is proposed. This paradigm comprises three entities: a physical entity, a semi-physical entity, and a virtual entity. A semi-physical entity is created by implementing the "six-D" process on the physical entity. A new six dimensional structure is formed through the addition of the semi-physical entity. The new structure streamlines the construction of fault datasets, enhances sensor data acquisition, and tightly links different data spaces, thereby promoting the application of DT in equipment FD. Subsequently, the elevator is selected as a case study to illustrate the Digital-Triplet framework in detail. The results demonstrate that the Digital-Triplet framework can effectively expand the fault dataset and improve data collection efficiency through optimized sensor placement, thereby promoting fault diagnosis.},
  archive      = {J_JIM},
  author       = {Zhang, Huang and Wang, Zili and Zhang, Shuyou and Qiu, Lemiao and Wang, Yang and Xiang, Feifan and Pan, Zhiwei and Zhu, Linhao and Tan, Jianrong},
  doi          = {10.1007/s10845-024-02471-7},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4895-4914},
  shortjournal = {J. Intell. Manuf.},
  title        = {Digital-triplet: A new three entities digital-twin paradigm for equipment fault diagnosis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive acquisition planning for visual inspection in remanufacturing using reinforcement learning. <em>JIM</em>, <em>36</em>(7), 4867-4893. (<a href='https://doi.org/10.1007/s10845-024-02478-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In remanufacturing, humans perform visual inspection tasks manually. In doing so, human inspectors implicitly solve variants of visual acquisition planning problems. Nowadays, solutions to these problems are computed based on the object geometry of the object to be inspected. In remanufacturing, however, there are often many product variants, and the existence of geometric object models cannot be assumed. This makes it difficult to plan and solve visual acquisition planning problems for the automated execution of visual inspection tasks. Reinforcement learning offers the possibility of learning and reproducing human inspection behavior and solving the visual inspection problem, even for problems in which no object geometry is available. To investigate reinforcement learning as a solution, a simple simulation environment is developed, allowing the execution of reproducible and controllable experiments. Different reinforcement learning agent modeling alternatives are developed and compared for solving the derived visual planning problems. The results of this work show that reinforcement learning agents can solve the derived visual planning problems in use cases without available object geometry by using domain-specific prior knowledge. Our proposed framework is available open source under the following link: https://github.com/Jarrypho/View-Planning-Simulation.},
  archive      = {J_JIM},
  author       = {Kaiser, Jan-Philipp and Gäbele, Jonas and Koch, Dominik and Schmid, Jonas and Stamer, Florian and Lanza, Gisela},
  doi          = {10.1007/s10845-024-02478-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4867-4893},
  shortjournal = {J. Intell. Manuf.},
  title        = {Adaptive acquisition planning for visual inspection in remanufacturing using reinforcement learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random convolution layer: An auxiliary method to improve fault diagnosis performance. <em>JIM</em>, <em>36</em>(7), 4845-4866. (<a href='https://doi.org/10.1007/s10845-024-02458-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real industry, it is often difficult to obtain large-scale labeled data. Existing Convolutional Neural Network (CNN)-based fault diagnosis methods often struggle to achieve accurate diagnoses of machine conditions due to the scarcity of labeled data, hindering the ability of models to develop strong inductive biases. We propose a plug-and-play auxiliary method, random convolution layer (RCL), to improve the generalization performance of the fault diagnosis models. This method delves into the fundamental commonalities across diverse tasks and varying network structures, thereby enhancing the diversity of samples to establish a more robust source domain environment. The RCL preserves the dimensional nature of the data in the time domain while randomly altering the kernel sizes during convolution operations, thus generating new data without compromising global information. During the training process, the newly generated data is mixed with the original data and fed into the fault diagnosis model. RCL is incorporated as a module into the inputs of different fault diagnosis models, and its effectiveness is validated on three public datasets as well as a self-built testbed. The results show that the present auxiliary method improves the domain generalization performance of the baselines, and can improve the accuracy of the corresponding fault diagnosis models. Our code is available at https://github.com/zhiqan/Random-convolution-layer .},
  archive      = {J_JIM},
  author       = {Zhao, Zhiqian and Zhao, Runchao and Jiao, Yinghou},
  doi          = {10.1007/s10845-024-02458-4},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4845-4866},
  shortjournal = {J. Intell. Manuf.},
  title        = {Random convolution layer: An auxiliary method to improve fault diagnosis performance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of data-driven, physics-based, and hybrid prognosis frameworks: A case study for gear remaining useful life prediction. <em>JIM</em>, <em>36</em>(7), 4823-4843. (<a href='https://doi.org/10.1007/s10845-024-02477-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven, physics-based, and hybrid prognosis frameworks can be developed to estimate remaining useful life, depending on the availability of condition monitoring sensor data and physics-governing equations. No systematic study is available that shows the comparative performance of these frameworks. The present study, for the first time, attempts to show how these three frameworks can be developed under different scenarios and assumptions. The data-driven prognosis framework is developed using an accelerometer signal and an Artificial Intelligence-based random forest regression (RFR) model. A pit growth model inspired by the Paris crack growth law has been used for physics-based prognosis framework development. In this framework, sensor data is needed to know the gear’s current health status, as the prognosis framework can't be developed purely on physics. A hybrid prognosis framework is developed using two alternate approaches: one in which current health status is obtained directly from a visual inspection camera and the other in which this status is indirectly inferred from the accelerometer sensor data. In each case, the RUL prediction is made using a physics-based pit growth model coupled with the current health status obtained from either of the two approaches mentioned. To enhance the prediction accuracy, Bayesian inference is used to update the physics-based pit growth model parameters in both hybrid frameworks. Data obtained from five run-to-failure experiments performed on a specially designed gearbox test setup are used to show the comparative performance of these frameworks. The strengths and weaknesses of each of the frameworks are discussed based on the type of data requirement, model definition, parameter estimation, and prediction error.},
  archive      = {J_JIM},
  author       = {Kundu, Pradeep and Darpe, Ashish K. and Kulkarni, Makarand S.},
  doi          = {10.1007/s10845-024-02477-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4823-4843},
  shortjournal = {J. Intell. Manuf.},
  title        = {Development of data-driven, physics-based, and hybrid prognosis frameworks: A case study for gear remaining useful life prediction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HG-XAI: Human-guided tool wear identification approach through augmentation of explainable artificial intelligence with machine vision. <em>JIM</em>, <em>36</em>(7), 4807-4822. (<a href='https://doi.org/10.1007/s10845-024-02476-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying tool wear state is essential for machine operators as it assists in informed decisions for timely tool replacement and subsequent machining operations. As each wear state corresponds to a unique mitigation strategy, timely identification is vital while implementing solutions to minimize tool wear. The paper presents a novel Human Guided-eXplainable Artificial Intelligence (HG-XAI) approach for identifying the tool wear state by integrating human intelligence and eXplainable AI with a pre-trained Convolutional Neural Network (CNN), Efficient-Net-b0 model. The tool wear states were identified based on different wear mechanisms during the machining of IN718. The study considers four distinct tool wear states, i.e., Flank, Flank+BUE, Flank+Face, and Chipping, representing abrasion, adhesion, diffusion, and fracture wear mechanisms. The image-based datasets were created to depict various tool wear states by machining IN718 at varying surface speeds. The effectiveness of the proposed HG-XAI approach was evaluated by comparing its prediction accuracy with a standalone Efficient-Net-b0 model lacking human intelligence and XAI. Further, the scalability of the HG-XAI approach was examined by predicting wear states from images acquired at different cutting parameters. The results from the present study showed that the HG-XAI approach can predict the tool wear state with an accuracy of 93.08% and is scalable to variations in cutting conditions. Also, the proposed approach can be extended while developing vision-based on-machine tool wear monitoring systems.},
  archive      = {J_JIM},
  author       = {Kumar, Aitha Sudheer and Agarwal, Ankit and Jansari, Vinita Gangaram and Desai, K. A. and Chattopadhyay, Chiranjoy and Mears, Laine},
  doi          = {10.1007/s10845-024-02476-2},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4807-4822},
  shortjournal = {J. Intell. Manuf.},
  title        = {HG-XAI: Human-guided tool wear identification approach through augmentation of explainable artificial intelligence with machine vision},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-task effectiveness metric and an adaptive co-training method for enhancing learning performance with few samples. <em>JIM</em>, <em>36</em>(7), 4785-4806. (<a href='https://doi.org/10.1007/s10845-024-02475-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of deep learning (DL) into vision inspection methods is increasingly recognized as a valuable approach to substantially enhance the adaptability and robustness. However, it is well known that high-performance neural networks typically require large training datasets with high-quality manual annotations, which are difficult to obtain in many manufacturing processes. To enhance the performance of DL methods for vision task with few samples, this paper proposes a novel metric called Effectiveness of Auxiliary Task (EAT) and presents a multi-task learning approach utilizing this metric for selecting effective auxiliary task branch and adaptive co-training them with main tasks. Experiments conducted on two vision tasks with few samples show that the proposed approach effectively eliminates ineffective task branches and enhances the contribution of the selected tasks to the main task: reducing the average normalized pixel error from 0.0613 to 0.0143 in pose key-points detection and elevating the Intersection over Union (IoU) from 0.6383 to 0.6921 in surface defect segmentation. Remarkably, these enhancements are achieved without necessitating additional manual labeling efforts.},
  archive      = {J_JIM},
  author       = {Wang, Xiaoyao and Du, Fuzhou and Zhao, Delong and Liu, Chang},
  doi          = {10.1007/s10845-024-02475-3},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4785-4806},
  shortjournal = {J. Intell. Manuf.},
  title        = {A multi-task effectiveness metric and an adaptive co-training method for enhancing learning performance with few samples},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated optical inspection based on synthetic mechanisms combining deep learning and machine learning. <em>JIM</em>, <em>36</em>(7), 4769-4783. (<a href='https://doi.org/10.1007/s10845-024-02474-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality inspection of products before delivery plays a critical role in ensuring manufacturing quality. Quick and accurate inspection of samples is realized by highly automated inspection based on pattern recognition in smart manufacturing. Conventional ensemble methods have been demonstrated to be effective for defect detection. This study further proposed synthetic mechanisms based on using various features and learning classifiers. A database of 6000 sample images of printed circuit board (PCB) connectors collected from factories was compiled. A novel confidence synthesis mechanism was proposed to prescreen images using deep learning features. Spatially connected texture features were then used to reclassify images with low reliabilities. The synthetic mechanism was found to outperform a single classifier. In particular, the highest improvement in accuracy (from 96.00 to 97.83%) was obtained using the confidence-based synthesis. The synthetic mechanism can be used to achieve high accuracy in defect detection and make automation in smart manufacturing more practicable.},
  archive      = {J_JIM},
  author       = {Lo, Chung-Ming and Lin, Ting-Yi},
  doi          = {10.1007/s10845-024-02474-4},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4769-4783},
  shortjournal = {J. Intell. Manuf.},
  title        = {Automated optical inspection based on synthetic mechanisms combining deep learning and machine learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A transformer-based deep reinforcement learning approach for dynamic parallel machine scheduling problem with family setups. <em>JIM</em>, <em>36</em>(7), 4735-4768. (<a href='https://doi.org/10.1007/s10845-024-02470-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parallel machine scheduling problem (PMSP) involves the optimized assignment of a set of jobs to a collection of parallel machines, which is a proper formulation for the modern manufacturing environment. Deep reinforcement learning (DRL) has been widely employed to solve PMSP. However, the majority of existing DRL-based frameworks still suffer from generalizability and scalability. More specifically, the state and action design still heavily rely on human efforts. To bridge these gaps, we propose a practical reinforcement learning-based framework to tackle a PMSP with new job arrivals and family setup constraints. We design a variable-length state matrix containing full job and machine information. This enables the DRL agent to autonomously extract features from raw data and make decisions with a global perspective. To efficiently process this novel state matrix, we elaborately modify a Transformer model to represent the DRL agent. By integrating the modified Transformer model to represent the DRL agent, a novel state representation can be effectively leveraged. This innovative DRL framework offers a high-quality and robust solution that significantly reduces the reliance on manual effort traditionally required in scheduling tasks. In the numerical experiment, the stability of the proposed agent during training is first demonstrated. Then we compare this trained agent on 192 instances with several existing approaches, namely a DRL-based approach, a metaheuristic algorithm, and a dispatching rule. The extensive experimental results demonstrate the scalability of our approach and its effectiveness across a variety of scheduling scenarios. Conclusively, our approach can thus solve the scheduling problems with high efficiency and flexibility, paving the way for application of DRL in solving complex and dynamic scheduling problems.},
  archive      = {J_JIM},
  author       = {Li, Funing and Lang, Sebastian and Tian, Yuan and Hong, Bingyuan and Rolf, Benjamin and Noortwyck, Ruben and Schulz, Robert and Reggelin, Tobias},
  doi          = {10.1007/s10845-024-02470-8},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4735-4768},
  shortjournal = {J. Intell. Manuf.},
  title        = {A transformer-based deep reinforcement learning approach for dynamic parallel machine scheduling problem with family setups},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Next-generation vision inspection systems: A pipeline from 3D model to ReCo file. <em>JIM</em>, <em>36</em>(7), 4711-4734. (<a href='https://doi.org/10.1007/s10845-024-02456-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes and implements a novel pipeline for the self-reconfiguration of a flexible, reconfigurable, CAD-based, and autonomous Vision Inspection System (VIS), expanding upon the modular framework theoretically outlined in (Lupi, F., Maffei, A., & Lanzetta, M. (2024). CAD-based Autonomous Vision Inspection Systems. Procedia Computer Science, 232, 2127–2136. https://doi.org/10.1016/J.PROCS.2024.02.033 .). The pipeline automates the extraction and processing of inspection features manually incorporated by the designer into the Computer Aided Design (CAD) 3D model during the design stage, in accordance with Model Based Design (MBD) principles, which, in turn, facilitate virtuous approaches such as concurrent engineering and design for (Dfx), ultimately minimizing the time to market. The enriched CAD, containing inspection annotations (textual or dimensional) attached to geometrical entities, serving as the pipeline’s input, can be exported in a neutral file format, adhering to the Standard for Product Data Exchange (STEP) Application Protocol (AP)242, regardless of the modeling software used. The pipeline’s output is a Reconfiguration (ReCo) file, enabling the flexible hardware (e.g., robotic inspection cell) and software components of the VIS to be reconfigured via software (programmable). The main achievements of this work include: (i) demonstrating the feasibility of an end-to-end (i.e., CAD-to-ReCo file) pipeline that integrates the proposed software modules via Application Programming Interfaces (API)s, and (ii) formally defining the ReCo file. Experimental results from a demonstrative implementation enhance the clarity of the paper. The accuracy in defect detection achieved a 96% true positive rate and a 6% false positive rate, resulting in an overall accuracy of 94% and a precision of 88% across 72 quality inspection checks for six different inspection features of two product variants, each tested on six samples.},
  archive      = {J_JIM},
  author       = {Lupi, Francesco and Freitas, Nelson and Arvana, Miguel and Rocha, Andre Dionisio and Maffei, Antonio and Barata, José and Lanzetta, Michele},
  doi          = {10.1007/s10845-024-02456-6},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4711-4734},
  shortjournal = {J. Intell. Manuf.},
  title        = {Next-generation vision inspection systems: A pipeline from 3D model to ReCo file},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive transfer fault detection method for rotary machine with multi-sensor information fusion. <em>JIM</em>, <em>36</em>(7), 4695-4710. (<a href='https://doi.org/10.1007/s10845-024-02469-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-sensor information fusion method has good performance in fault detection of rotary machine, in which each sensor information has made different contributions. The contribution of each sensor changes based on the working conditions of the machine, which can lead to a degradation in the performance of the transfer method when used in cross-domain mechanical fault detection. To solve this problem, an adaptive transfer fault detection method for rotary machine with multi-sensor information fusion is proposed. Firstly, multi-sensor data under different working conditions is collected, and features of different sensors are extracted by the corresponding deep learning model. Secondly, the multi-information interaction fusion network is designed to exchange sensor information and obtain fusion features. Then the fusion feature transfer model is proposed for cross-domain fault detection. Finally, the model is trained with the bearing dataset of the University of Paderborn. The results show that the transfer fault detection method with multi-sensor information fusion achieves state-of-the-art performances in cross-domain fault detection. It can adjust adaptively the contribution of each sensor information in the cross-domain fault detection.},
  archive      = {J_JIM},
  author       = {Wang, Qibin and Yu, Linyang and Hao, Liang and Yang, Shengkang and Zhou, Tao and Ji, Wanghui},
  doi          = {10.1007/s10845-024-02469-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4695-4710},
  shortjournal = {J. Intell. Manuf.},
  title        = {An adaptive transfer fault detection method for rotary machine with multi-sensor information fusion},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep-learning based artificial intelligence tool for melt pools and defect segmentation. <em>JIM</em>, <em>36</em>(7), 4679-4694. (<a href='https://doi.org/10.1007/s10845-024-02457-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerating fabrication of additively manufactured components with precise microstructures is important for quality and qualification of built parts, as well as for a fundamental understanding of process improvement. Accomplishing this requires fast and robust characterization of melt pool geometries and structural defects in images. This paper proposes a pragmatic approach based on implementation of deep learning models and self-consistent workflow that enable systematic segmentation of defects and melt pools in optical images. Deep learning is based on an image-to-image translation–conditional generative adversarial neural network architecture. An artificial intelligence (AI) tool based on this deep learning model enables fast and incrementally more accurate predictions of the prevalent geometric features, including melt pool boundaries and printing-induced structural defects. We present statistical analysis of geometric features that is enabled by the AI tool, showing strong spatial correlation of defects and the melt pool boundaries. The correlations of widths and heights of melt pools with dataset processing parameters show the highest sensitivity to thermal influences resulting from laser passes in adjacent and subsequent layer passes. The presented models and tools are demonstrated on the aluminum alloy and datasets produced with different sets of processing parameters. However, they have universal quality and could easily be adapted to different material compositions. The method can be easily generalized to microstructural characterizations other than optical microscopy.},
  archive      = {J_JIM},
  author       = {Peles, Amra and Paquit, Vincent C. and Dehoff, Ryan R.},
  doi          = {10.1007/s10845-024-02457-5},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4679-4694},
  shortjournal = {J. Intell. Manuf.},
  title        = {Deep-learning based artificial intelligence tool for melt pools and defect segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deformation prediction model for hollow thin-walled aluminum alloy structural parts under multiple load-sequence coupling conditions. <em>JIM</em>, <em>36</em>(7), 4663-4677. (<a href='https://doi.org/10.1007/s10845-024-02464-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hollow thin-walled aluminum alloy structural parts (HTWASP) must be cut after welding during the actual processing process, and a welding stress field is inevitably generated owing to the welding heat effect in the process of welding the workpiece, leading to distortion of workpiece. To accurately and efficiently predict the deformation of HTWASP after multiprocess processing, equivalent load caused by process of welding was computed through connection inherent strain theory with welding thermal parameters. A dynamic simulation model of the milling process of HTWASP was established by welding equivalent load. The influences of spindle speed and tool diameter on the deformation remain stress of structural workpieces were analyzed. Additionally, the simulated values for the deformation of the workpiece were compared and analyzed through milling tests on these structural parts. The results showed that the range of stress values and stress effects was smaller when the spindle speed was higher. The distance of the stress effect was the smallest when machine speed was 2500 rpm and tool diameter was 20 mm. Milling stress value was the smallest when machine speed was 2500 rpm and tool diameter was 6 mm. Most of the deformation occurred in the hollow position of the upper and diagonal plate; in contrast, the distortion of vertical plate and weld seam was not significant. The minimum deformation was 0.501 mm at machine speed is 2500 rpm and tool diameter is 6 mm. In the non-high-speed cutting state, high speed reduced workpiece quality of aluminum alloy workpiece, and slot milling quality was the best when machine speed was 1000 r/min and tool diameter was 6 mm. The proposed model sequentially couples the welding and milling process loads, and a multiprocess deformation prediction model that increasingly conforms to the actual processing sequence is constructed, providing a reference for the high-precision and efficient prediction of the multiprocess deformation in hollow thin-walled structural parts.},
  archive      = {J_JIM},
  author       = {Ma, Jiaheng and Zhang, Shengfang and Ma, Fujian and Song, Xiuying and Wang, Ziguang and Sha, Zhihua},
  doi          = {10.1007/s10845-024-02464-6},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4663-4677},
  shortjournal = {J. Intell. Manuf.},
  title        = {Deformation prediction model for hollow thin-walled aluminum alloy structural parts under multiple load-sequence coupling conditions},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-fog-cloud hybrid collaborative computing solution with an improved parallel evolutionary strategy for enhancing tasks offloading efficiency in intelligent manufacturing workshops. <em>JIM</em>, <em>36</em>(7), 4635-4662. (<a href='https://doi.org/10.1007/s10845-024-02463-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In intelligent manufacturing workshops, the lack of an efficient collaborative mechanism among the various computational resources leads to higher latency, increased costs, and uneven computational load distribution, compromising the response efficacy of intelligent manufacturing services. To address these challenges, this paper introduces an edge-fog-cloud hybrid collaborative computing architecture (EFCHC) that enhances the interaction among multi-layer computational resources. Furthermore, the computational tasks offloading model under EFCHC is formulated to minimize objectives such as latency and cost. To refine the offloading solution, a novel multi-group parallel evolutionary strategy is proposed, which includes a two-stage pre-allocation scheme and a hyper-heuristic evolutionary operator for effective solution identification. In multi-objective benchmark testing experiments, the proposed algorithm substantially outperforms other comparative algorithms in terms of accuracy, convergence, and stability. In simulated workshop scenarios, the proposed offloading strategy reduces the total computational latency and cost by 17.81% and 21.89%, and enhances the load balancing efficiency by up to 52.50%, compared to six typical benchmark algorithms and architectures.},
  archive      = {J_JIM},
  author       = {Lin, Zhiwen and Liu, Zhifeng and Zhang, Yueze and Yan, Jun and Liu, Shimin and Qi, Baobao and Wei, Kaien},
  doi          = {10.1007/s10845-024-02463-7},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4635-4662},
  shortjournal = {J. Intell. Manuf.},
  title        = {Edge-fog-cloud hybrid collaborative computing solution with an improved parallel evolutionary strategy for enhancing tasks offloading efficiency in intelligent manufacturing workshops},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CNC linear axis condition-based monitoring: A statistics-based framework to establish a baseline dataset and case study. <em>JIM</em>, <em>36</em>(7), 4613-4634. (<a href='https://doi.org/10.1007/s10845-024-02461-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The linear axis of computer numerical control (CNC) machines is a critical subsystem that provides precise position capabilities. The unexpected failure of its components may lead to part quality issues and machine breakdowns. Therefore, it is crucial to examine and understand its healthy condition when newly commissioned or repaired so that it can be used as a reference when monitoring its operational health. In this paper, a framework to establish a baseline reference dataset is proposed utilizing vibration monitoring and time domain statistical feature analysis. The framework was applied as a case study in a newly commissioned linear axis testbed. The results demonstrated that a linear axis under a known healthy condition exhibits low variability of its time domain features, negligible difference between forward and reverse stroke directions and a robust baseline dataset can be established by collecting data for approximately an hour of operation instead of a full day of operation (6 h of operation).},
  archive      = {J_JIM},
  author       = {Hurtado Carreon, Andres and DePaiva, Jose Mario and Barooah, Rohan and Veldhuis, Stephen C.},
  doi          = {10.1007/s10845-024-02461-9},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4613-4634},
  shortjournal = {J. Intell. Manuf.},
  title        = {CNC linear axis condition-based monitoring: A statistics-based framework to establish a baseline dataset and case study},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic scenario-enhanced diverse human motion prediction network for proactive human–robot collaboration in customized assembly tasks. <em>JIM</em>, <em>36</em>(7), 4593-4612. (<a href='https://doi.org/10.1007/s10845-024-02462-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion prediction is crucial for facilitating human–robot collaboration in customized assembly tasks. However, existing research primarily focuses on predicting limited human motions using static global information, which fails to address the highly stochastic nature of customized assembly operations in a given region. To address this, we propose a dynamic scenario-enhanced diverse human motion prediction network that extracts dynamic collaborative features to predict highly stochastic customized assembly operations. In this paper, we present a multi-level feature adaptation network that generates information for dynamically manipulating objects. This is accomplished by extracting multi-attribute features at different levels, including multi-channel gaze tracking, multi-scale object affordance detection, and multi-modal object’s 6 degree-of-freedom pose estimation. Notably, we employ gaze tracking to locate the collaborative space accurately. Furthermore, we introduce a multi-step feedback-refined diffusion sampling network specifically designed for predicting highly stochastic customized assembly operations. This network refines the outcomes of our proposed multi-weight diffusion sampling strategy to better align with the target distribution. Additionally, we develop a feedback regulatory mechanism that incorporates ground truth information in each prediction step to ensure the reliability of the results. Finally, the effectiveness of the proposed method was demonstrated through comparative experiments and validation of assembly tasks in a laboratory environment.},
  archive      = {J_JIM},
  author       = {Ding, Pengfei and Zhang, Jie and Zheng, Pai and Zhang, Peng and Fei, Bo and Xu, Ziqi},
  doi          = {10.1007/s10845-024-02462-8},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4593-4612},
  shortjournal = {J. Intell. Manuf.},
  title        = {Dynamic scenario-enhanced diverse human motion prediction network for proactive human–robot collaboration in customized assembly tasks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of different machine vision algorithms for tool wear measurement during machining. <em>JIM</em>, <em>36</em>(7), 4567-4591. (<a href='https://doi.org/10.1007/s10845-024-02467-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic tool condition monitoring becomes crucial in metal cutting because tool wear impacts the final product’s quality. The optical microscope approach for assessing tool wear is offline, time-consuming, and subject to measurement error by humans. To accomplish this, the machine must be stopped, and the tool must be removed, which causes downtime. As a result, numerous research attempts have been made to develop robust systems for direct tool wear measurement during machining. Therefore, the proposed work focused on developing a direct tool condition monitoring system using machine vision to calculate tool wear parameters, specifically flank wear. The cutting tool insert images are collected using a machine vision setup equipped with an industrial camera, bi-telecentric lens, and a proper illumination system during the machining of AISI 4140 steel. The comparative analysis of image processing algorithms for tool wear measurement is proposed under the selected machining environment. The wear boundary is extracted using digital image processing tools such as image enhancement, image segmentation, image morphology operation, and edge detection. The wear amount on the tool insert is extracted and recorded using the Hough line transformation function and pixel scanning. The comparison of results revealed the measurement accuracy and repeatability of the proposed image processing algorithm with a maximum of 6.25% and minimum of 1.10% error compared to manual measurement. Hence, the proposed approach eliminates manual measurements and improves the machining productivity.},
  archive      = {J_JIM},
  author       = {Makhesana, Mayur A. and Bagga, Prashant J. and Patel, Kaushik M. and Patel, Haresh D. and Balu, Aditya and Khanna, Navneet},
  doi          = {10.1007/s10845-024-02467-3},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4567-4591},
  shortjournal = {J. Intell. Manuf.},
  title        = {Comparative analysis of different machine vision algorithms for tool wear measurement during machining},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven linear quadratic tracking based temperature control of a big area additive manufacturing system. <em>JIM</em>, <em>36</em>(7), 4549-4565. (<a href='https://doi.org/10.1007/s10845-024-02428-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing efficient closed-loop control algorithms is a key issue in Additive Manufacturing (AM), as various aspects of the AM process require continuous monitoring and regulation, with temperature being a particularly significant factor. Here we study closed-loop control for the temperatures in the extruder of a Material Extrusion AM system, specifically a Big Area Additive Manufacturing (BAAM) system. Previous approaches for temperature control in AM either require the knowledge of exact model parameters, or involve discretisation of the state and action spaces to employ traditional data-driven control techniques. On the other hand, modern algorithms that can handle continuous state and action space problems require a large number of hyperparameter tuning to ensure good performance. In this work, we circumvent the above limitations by making use of a state space temperature model while focusing on both model-based and data-driven methods. We adopt the Linear Quadratic Tracking (LQT) framework and utilise the quadratic structure of the value function in the model-based analytical solution to produce a data-driven approximation formula for the optimal controller. We demonstrate these approaches using a simulator of the temperature evolution in the extruder of a BAAM system and perform an in-depth comparison of the performance of these methods. We find that we can learn an effective controller using solely simulated input–output process data. Our approach achieves parity in performance compared to model-based controllers and so lessens the need for estimating a large number of parameters of the often intricate and complicated process model. We believe this result is an important step towards achieving autonomous intelligent manufacturing.},
  archive      = {J_JIM},
  author       = {Zavrakli, Eleni and Parnell, Andrew and Dickson, Andrew and Dey, Subhrakanti},
  doi          = {10.1007/s10845-024-02428-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4549-4565},
  shortjournal = {J. Intell. Manuf.},
  title        = {Data-driven linear quadratic tracking based temperature control of a big area additive manufacturing system},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards AI driven surface roughness evaluation in manufacturing: A prospective study. <em>JIM</em>, <em>36</em>(7), 4519-4548. (<a href='https://doi.org/10.1007/s10845-024-02493-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Industry 4.0 and the digital transformation of the manufacturing sector, this article explores the significant potential of machine learning (ML) and deep learning (DL) techniques in evaluating surface roughness—a critical metric of product quality. The integration of edge computing with current computational resources and intelligent sensors has revolutionized the application of AI-driven algorithms in smart manufacturing. It provides real-time data analysis and decision-making capabilities that were unattainable only a decade ago. The research effort intends to improve data-driven decision-making for product quality evaluation by leveraging data integration from manufacturing operations and surface quality measurements. Although a substantial amount of research has been conducted in the related fields, it is still difficult to comprehend and compile all the data on surface roughness research predictive assessment in the form of a process pipeline. This thorough systematic analysis examines scholarly articles published between 2014 and 2024 focusing on surface roughness assessment in precision manufacturing settings. The article is thoroughly classified based on the manufacturing processes, datasets, and ML models used, giving light on the present status, prominent approaches, and existing issues in this sector. A table summarizing the relevant works in this domain providing an easy access to the current trends have been provided. The article not only compiles essential findings and identifies research gaps and similarities in existing methodologies, but it also discusses future research directions and open issues in AI-aided surface roughness evaluation. The critical analysis of the literature reveals a scientific gaps which includes consistent development of benchmarked datasets and making the AI models more explainable using the state-of-the-art explainable AI (XAI) algorithms. The ultimate objective of the article is not only to provide a guide for the practitioners in either of the three domains of AI, manufacturing or surface metrology but also to pave the path for more robust, efficient, and accurate surface quality evaluation processes in production.},
  archive      = {J_JIM},
  author       = {Ghosh, Sourish and Knoblauch, Ricardo and El Mansori, Mohamed and Corleto, Cosimi},
  doi          = {10.1007/s10845-024-02493-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4519-4548},
  shortjournal = {J. Intell. Manuf.},
  title        = {Towards AI driven surface roughness evaluation in manufacturing: A prospective study},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Best practices for machine learning strategies aimed at process parameter development in powder bed fusion additive manufacturing. <em>JIM</em>, <em>36</em>(7), 4477-4517. (<a href='https://doi.org/10.1007/s10845-024-02490-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process parameters used for building a part utilizing the powder-bed fusion (PBF) additive manufacturing (AM) system have a direct influence on the quality—and therefore performance—of the final object. These parameters are commonly chosen based on experience or, in many cases, iteratively through experimentation. Discovering the optimal set of parameters via trial and error can be time-consuming and costly, as it often requires examining numerous permutations and combinations of parameters which commonly have complex interactions. However, machine learning (ML) methods can recommend suitable processing windows using models trained on data. They achieve this by efficiently identifying the optimal parameters through analyzing and recognizing patterns in data described by a multi-dimensional parameter space. We reviewed ML-based forward and inverse models that have been proposed to unlock the process–structure–property–performance relationships in both directions and assessed them in relation to data (quality, quantity, and diversity), ML method (mismatches and neglect of history), and model evaluation. To address the common shortcomings inherent in the published works, we propose strategies that embrace best practices. We point out the need for consistency in the reporting of details relevant to ML models and advocate for the development of relevant international standards. Significantly, our recommendations can be adopted for ML applications outside of AM where an optimum combination of process parameters (or other inputs) must be found with only a limited amount of training data.},
  archive      = {J_JIM},
  author       = {Samadiani, Najmeh and Barnard, Amanda S. and Gunasegaram, Dayalan and Fayyazifar, Najmeh},
  doi          = {10.1007/s10845-024-02490-4},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4477-4517},
  shortjournal = {J. Intell. Manuf.},
  title        = {Best practices for machine learning strategies aimed at process parameter development in powder bed fusion additive manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart scheduling for next generation manufacturing systems: A systematic literature review. <em>JIM</em>, <em>36</em>(7), 4447-4476. (<a href='https://doi.org/10.1007/s10845-024-02484-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current scenario, smart scheduling has become an essential requirement to generate dynamic schedules, prescribe, and adjust scheduling plans in response to dynamic events such as machine failures, unpredictable demand, customer order cancellations, worker unavailability, and mass customization. Such scheduling techniques must also take advantage of intelligence continuously being built for next-generation manufacturing systems. This study presents a systematic literature review on smart scheduling, analysing 123 identified literature from 2010 to May 2024 using the PRISMA technique. The analysis includes scientometric and content analysis to identify paradigm shifts in development (concepts, methodologies, practices) along with their maturity levels, and provides recommendations for the next generation of smart scheduling. This study is significant for advancing knowledge and addressing current and future needs/requirements in smart scheduling. This would serve as a reference in understanding the maturity status of various developments, assist researchers and practitioners in identifying research gaps, and direct future advancements in the smart scheduling domain.},
  archive      = {J_JIM},
  author       = {Chorghe, Shriprasad and Kumar, Rishi and Kulkarni, Makarand S. and Pandhare, Vibhor and Lad, Bhupesh Kumar},
  doi          = {10.1007/s10845-024-02484-2},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4447-4476},
  shortjournal = {J. Intell. Manuf.},
  title        = {Smart scheduling for next generation manufacturing systems: A systematic literature review},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An overview of traditional and advanced methods to detect part defects in additive manufacturing processes. <em>JIM</em>, <em>36</em>(7), 4411-4446. (<a href='https://doi.org/10.1007/s10845-024-02483-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive manufacturing (AM) or 3-dimensional (3D) printing processes have been adopted in several industrial sectors including aerospace, automotive, medical, architecture, arts and design, food, and construction for the past few decades due to their numerous advantages over other conventional subtractive manufacturing processes. However, some flaws and defects associated with 3D-printed components hinder its extensive adoption in industries. Therefore, real-time detection and elimination of these defects by analyzing the defects-causing process parameters is very important to obtain a defect-free final component. While global efforts are in progress to develop defect detection techniques with the rise of Industry 4.0, there is still a limited scope of comprehensive research that encapsulates various defect detection techniques in the AM sector on a global scale. Thus, this systematic review explores defects in parts manufactured via metallic and non-metallic AM processes. It covers traditional defect detection methods and extends to recent advanced machine learning (ML) and deep learning (DL) based techniques. The paper also delves into challenges associated with the implementation of ML and DL approaches for defect detection, providing a comprehensive understanding of the current state and future directions in AM research.},
  archive      = {J_JIM},
  author       = {Bhandarkar, Vivek V. and Shahare, Harshal Y. and Mall, Anand Prakash and Tandon, Puneet},
  doi          = {10.1007/s10845-024-02483-3},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {10},
  number       = {7},
  pages        = {4411-4446},
  shortjournal = {J. Intell. Manuf.},
  title        = {An overview of traditional and advanced methods to detect part defects in additive manufacturing processes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jirs">JIRS - 2</h2>
<ul>
<li><details>
<summary>
(2025). Loosely coupled 4D-radar-inertial odometry for ground robots. <em>JIRS</em>, <em>111</em>(4), 1-15. (<a href='https://doi.org/10.1007/s10846-025-02301-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate robot odometry is essential for autonomous navigation. While numerous techniques have been developed based on various sensor suites, odometry estimation using only radar and IMU remains an underexplored area. Radar proves particularly valuable in environments where traditional sensors, like cameras or LiDAR, may struggle, especially in low-light conditions or when faced with environmental challenges like fog, rain or smoke. However, despite its robustness, radar data is noisier and more prone to outliers, requiring specialized processing approaches. In this paper, we propose a graph-based optimization approach ( https://github.com/robotics-upo/4D-Radar-Odom.git ) using a sliding window for radar-based odometry, designed to maintain robust relationships between poses by forming a network of connections, while keeping computational costs fixed (specially beneficial in long trajectories). Additionally, we introduce an enhancement in the ego-velocity estimation specifically for ground vehicles, both holonomic and non-holonomic, which subsequently improves the direct odometry input required by the optimizer. Finally, we present a comparative study of our approach against existing algorithms, showing how our pure odometry approach improves the state of art in all trajectories of the NTU4DRadLM dataset, achieving promising results when evaluating key performance metrics.},
  archive      = {J_JIRS},
  author       = {Coto-Elena, Lucía and Caballero, Fernando and Merino, Luis},
  doi          = {10.1007/s10846-025-02301-9},
  journal      = {Journal of Intelligent & Robotic Systems},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Loosely coupled 4D-radar-inertial odometry for ground robots},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formation control and stability analysis of underactuated unmanned surface vehicles based on improved extended state observer: Addressing disturbance challenges. <em>JIRS</em>, <em>111</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10846-025-02316-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on unmanned surface vehicles (USVs) and introduces a formation control method that addresses both preset transient performance and steady-state performance constraints for fixed-configuration formations of USVs, taking into account model uncertainties and external disturbances. By designing the control law for different regions, we overcome the singularity issues inherent in traditional PID control. The control algorithm is integrated with graph theory to achieve effective formation control of the USVs. A dimension-reduced extended state observer is employed to detect unknown time-varying side-slip angles and external disturbances, providing accurate estimation and compensation for these composite disturbances, including ocean current interference. The stability of the ship formation control law is verified using Lyapunov theory. Additionally, the cascade system stability analysis method is used to demonstrate that the closed-loop system possesses input-to-state stability. Simulation results indicate that this method significantly enhances the stability and robustness of the USV formation.},
  archive      = {J_JIRS},
  author       = {Xiuhan, Jiang and Xi, Fang},
  doi          = {10.1007/s10846-025-02316-2},
  journal      = {Journal of Intelligent & Robotic Systems},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Formation control and stability analysis of underactuated unmanned surface vehicles based on improved extended state observer: Addressing disturbance challenges},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jlli">JLLI - 7</h2>
<ul>
<li><details>
<summary>
(2025). Comprehension and knowledge. <em>JLLI</em>, <em>34</em>(1), 169-196. (<a href='https://doi.org/10.1007/s10849-025-09431-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of an agent to comprehend a sentence is tightly connected to the agent’s prior experiences and background knowledge. The article suggests to interpret comprehension as a modality and proposes a complete bimodal logical system that describes an interplay between comprehension and knowledge modalities. The main technical result is a completeness theorem for the proposed system},
  archive      = {J_JLLI},
  author       = {Naumov, Pavel and Ros, Kevin},
  doi          = {10.1007/s10849-025-09431-1},
  journal      = {Journal of Logic, Language and Information},
  month        = {5},
  number       = {1},
  pages        = {169-196},
  shortjournal = {J. Log. Lang. Inf.},
  title        = {Comprehension and knowledge},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A family whose computable numberings are all complete. <em>JLLI</em>, <em>34</em>(1), 155-167. (<a href='https://doi.org/10.1007/s10849-025-09430-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we construct a computable family $$\mathcal {R}$$ of r.e. sets whose every computable numbering $$\alpha $$ is complete and encodes the Gödel numbering $$x\mapsto W_x$$ of the family of all r.e. sets within itself in the sense that there exists a recursive function r such that for every $$b\in \mathbb N$$ there is a $$B\subseteq \mathbb N$$ with $$\alpha (r(b))=B\oplus W_b$$ . Then we prove that, for all $$n\geqslant 2$$ , every non-trivial $$\Sigma ^0_n$$ -computable family has a non-complete (and even non-cylindrical) $$\Sigma ^0_n$$ -computable numbering, but there exists a $$\Sigma ^0_n$$ -computable family $$\mathcal {A}$$ whose every $$\Sigma ^0_n$$ -computable numbering $$\beta $$ has the fixed point property (i.e., for every recursive function f there is a $$p\in \mathbb N$$ with $$\beta (f(p))=\beta (p)$$ ) and encodes within itself the numbering $$x\mapsto W^{\emptyset ^{(n-1)}}_x$$ .},
  archive      = {J_JLLI},
  author       = {Faizrahmanov, Marat},
  doi          = {10.1007/s10849-025-09430-2},
  journal      = {Journal of Logic, Language and Information},
  month        = {5},
  number       = {1},
  pages        = {155-167},
  shortjournal = {J. Log. Lang. Inf.},
  title        = {A family whose computable numberings are all complete},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A logical characterization of weak determinism as simultaneous application. <em>JLLI</em>, <em>34</em>(1), 89-153. (<a href='https://doi.org/10.1007/s10849-025-09429-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly deterministic functions are a subregular class of functions which have been claimed to describe the complexity of most attested phonological maps. This paper proposes a characterization of the weak deterministic functions within the formalism of Boolean Monadic Recursive Schemes (BMRS), in terms of a simultaneous application operator over BMRS programs. This paper provides proof that more complex patterns such as Sour Grapes harmony are not weakly deterministic, and shows that the proposed definition can decisively distinguish between weakly deterministic and properly regular maps. The consequence of this work is a logical characterization of the weakly deterministic boundary, and a testable hypothesis about the complexity of natural language phonological maps.},
  archive      = {J_JLLI},
  author       = {Yolyan, Tatevik},
  doi          = {10.1007/s10849-025-09429-9},
  journal      = {Journal of Logic, Language and Information},
  month        = {5},
  number       = {1},
  pages        = {89-153},
  shortjournal = {J. Log. Lang. Inf.},
  title        = {A logical characterization of weak determinism as simultaneous application},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One head is better than two: A polynomial restriction for propositional definite horn forgetting. <em>JLLI</em>, <em>34</em>(1), 49-88. (<a href='https://doi.org/10.1007/s10849-025-09428-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logical forgetting is NP-complete as a decision problem even in the simple case of propositional Horn formulae, and may exponentially increase their size. A way to forget is to replace each variable to forget with the body of each clause whose head is the variable. It takes polynomial time in the single-head case: each variable is the head of at most a clause. Some formulae are not single-head but can be made so to simplify forgetting. They are called single-head equivalent. The first contribution of this article is the study of a semantical characterization of single-head equivalence. Two necessary conditions are given. They are sufficient when the formula is interequivalent, that is, the formula makes two sets of variables equivalent only if they are also equivalent to their intersection. All acyclic formulae are interequivalent. The second contribution of this article is an incomplete algorithm for turning a formula single-head. In case of success, forgetting becomes possible in polynomial time and produces a polynomial-size formula, none of which is otherwise guaranteed. The algorithm is complete on interequivalent formulae.},
  archive      = {J_JLLI},
  author       = {Liberatore, Paolo},
  doi          = {10.1007/s10849-025-09428-w},
  journal      = {Journal of Logic, Language and Information},
  month        = {5},
  number       = {1},
  pages        = {49-88},
  shortjournal = {J. Log. Lang. Inf.},
  title        = {One head is better than two: A polynomial restriction for propositional definite horn forgetting},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Non-monotonicity and contraposition. <em>JLLI</em>, <em>34</em>(1), 47-48. (<a href='https://doi.org/10.1007/s10849-025-09427-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JLLI},
  author       = {Crupi, Vincenzo and Dalmonte, Tiziano and Iacona, Andrea},
  doi          = {10.1007/s10849-025-09427-x},
  journal      = {Journal of Logic, Language and Information},
  month        = {5},
  number       = {1},
  pages        = {47-48},
  shortjournal = {J. Log. Lang. Inf.},
  title        = {Correction: Non-monotonicity and contraposition},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-monotonicity and contraposition. <em>JLLI</em>, <em>34</em>(1), 27-46. (<a href='https://doi.org/10.1007/s10849-024-09425-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a formal theory of non-monotonic consequence which differs from most extant theories in that it assumes Contraposition as a basic principle of defeasible reasoning. We define a minimal logic that combines Contraposition with three uncontroversial inference rules, and we prove some key results that characterize this logic and its possible extensions.},
  archive      = {J_JLLI},
  author       = {Crupi, Vincenzo and Dalmonte, Tiziano and Iacona, Andrea},
  doi          = {10.1007/s10849-024-09425-5},
  journal      = {Journal of Logic, Language and Information},
  month        = {5},
  number       = {1},
  pages        = {27-46},
  shortjournal = {J. Log. Lang. Inf.},
  title        = {Non-monotonicity and contraposition},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the strongest principles of rational belief assignment. <em>JLLI</em>, <em>34</em>(1), 1-26. (<a href='https://doi.org/10.1007/s10849-024-09426-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that in Polyadic Pure Inductive Logic the Invariance Principle, based on consideration of symmetry with respect to automorphisms, has only a trivial solution, namely the polyadic equivalent of Carnap’s $$c_0$$ . (This extends a result proved earlier in the unary case.) We then consider the Exchangeable Invariance Principle, a symmetry principle which is a weakening of the Invariance Principle and has been proven to be strictly stronger than the Permutation Invariance Principle. We show that the Exchangeable Invariance Principle follows from Spectrum Exchangeability, a principle not obviously based on symmetry but based on irrelevance, that is treating certain features as irrelevant for the purpose of belief assignment, and that the converse does not hold. We conclude that Spectrum Exchangeability is the strongest currently known rational principle of belief assignment in Pure Inductive Logic that does not lead to the aforementioned trivial solution.},
  archive      = {J_JLLI},
  author       = {Paris, J. B. and Vencovská, A.},
  doi          = {10.1007/s10849-024-09426-4},
  journal      = {Journal of Logic, Language and Information},
  month        = {5},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Log. Lang. Inf.},
  title        = {On the strongest principles of rational belief assignment},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jmiv">JMIV - 5</h2>
<ul>
<li><details>
<summary>
(2025). An adaptively inexact method for bilevel learning using Primal–Dual-style differentiation. <em>JMIV</em>, <em>67</em>(5), 1-15. (<a href='https://doi.org/10.1007/s10851-025-01262-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a bilevel learning framework for learning linear operators. In this framework, the learnable parameters are optimized via a loss function that also depends on the minimizer of a convex optimization problem (denoted lower-level problem). We utilize an iterative algorithm called ‘piggyback’ to compute the gradient of the loss and minimizer of the lower-level problem. Given that the lower-level problem is solved numerically, the loss function and thus its gradient can only be computed inexactly. To estimate the accuracy of the computed hypergradient, we derive an a-posteriori error bound, which provides guides for setting the tolerance for the lower-level problem, as well as the piggyback algorithm. To efficiently solve the upper-level optimization, we also propose an adaptive method for choosing a suitable step size. To illustrate the proposed method, we consider a few learned regularizer problems, such as training an input-convex neural network.},
  archive      = {J_JMIV},
  author       = {Bogensperger, Lea and Ehrhardt, Matthias J. and Pock, Thomas and Salehi, Mohammad Sadegh and Wong, Hok Shing},
  doi          = {10.1007/s10851-025-01262-w},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {An adaptively inexact method for bilevel learning using Primal–Dual-style differentiation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical bayesian image restoration by langevin sampling with a denoising diffusion implicit prior. <em>JMIV</em>, <em>67</em>(5), 1-19. (<a href='https://doi.org/10.1007/s10851-025-01265-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Score-based diffusion methods provide a powerful strategy to solve image restoration tasks by flexibly combining a pre-trained foundational prior model with a likelihood function specified during test time . Such methods are predominantly derived from two stochastic processes: reversing Ornstein–Uhlenbeck, which underpins the celebrated denoising diffusion probabilistic models (DDPM) and denoising diffusion implicit models (DDIM), and the Langevin diffusion process. The solutions delivered by DDPM and DDIM are often remarkably realistic, but they are not always consistent with measurements because of likelihood intractability issues and the associated required approximations. Alternatively, using a Langevin process circumvents the intractable likelihood issue, but usually leads to restoration results of inferior quality. This paper presents a novel and highly computationally efficient image restoration method that carefully embeds a foundational DDPM denoiser within an empirical Bayesian Langevin algorithm, which jointly calibrates key model hyper-parameters as it estimates the model’s posterior mean. Extensive experimental results on three canonical tasks (image deblurring, super-resolution, and inpainting) demonstrate that the proposed approach improves on state-of-the-art strategies both in image estimation accuracy and computing time.},
  archive      = {J_JMIV},
  author       = {Kemajou Mbakam, Charlesquin and Giovannelli, Jean-Francois and Pereyra, Marcelo},
  doi          = {10.1007/s10851-025-01265-7},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Empirical bayesian image restoration by langevin sampling with a denoising diffusion implicit prior},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interior object geometry via fitted frames. <em>JMIV</em>, <em>67</em>(5), 1-16. (<a href='https://doi.org/10.1007/s10851-025-01266-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a means of computing fitted frames on the boundary and in the interior of objects and using them to provide the basis for producing geometric features from them that are not only alignment-free but most importantly can be made to correspond locally across a population of objects. We describe a representation targeted for anatomic objects which is designed to enable this strong locational correspondence within object populations and thus to provide powerful object statistics. It accomplishes this by understanding an object as the diffeomorphic deformation of the closure of the interior of an ellipsoid and by using a skeletal representation fitted throughout the deformation to produce a model of the target object, where the object is provided initially in the form of a boundary mesh. Via classification performance on hippocampi shape between individuals with a disorder vs. others, we compare our method to two state-of-the-art methods for producing object representations that are intended to capture geometric correspondence across a population of objects and to yield geometric features useful for statistics, and we show notably improved classification performance by this new representation, which we call the evolutionary s-rep. The geometric features that are derived from each of the representations, especially via fitted frames, are discussed.},
  archive      = {J_JMIV},
  author       = {Pizer, Stephen M. and Liu, Zhiyuan and Zhao, Junjie and Tapp-Hughes, Nicholas and Damon, James and Zhang, Miaomiao and Marron, J. S. and Taheri, Mohsen and Vicory, Jared},
  doi          = {10.1007/s10851-025-01266-6},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Interior object geometry via fitted frames},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matrix-valued LogSumExp approximation for colour morphology. <em>JMIV</em>, <em>67</em>(5), 1-28. (<a href='https://doi.org/10.1007/s10851-025-01267-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematical morphology is a part of image processing that employs a moving window to modify pixel values through the application of specific operations. The supremum and infimum are pivotal concepts, yet defining them in a general sense for high-dimensional data such as colour is a challenging endeavour. As a result, a number of different approaches have been taken to try to find a solution, with certain compromises being made along the way. In this paper, we present an analysis of a novel approach that replaces the supremum within a morphological operation with the LogExp approximation of the maximum for matrix-valued colours. This approach has the advantage of extending the associativity of dilation from the one-dimensional to the higher-dimensional case. Furthermore, the minimality property is investigated and a relaxation specified to ensure that the approach is continuously dependent on the input data.},
  archive      = {J_JMIV},
  author       = {Kahra, Marvin and Breuß, Michael and Kleefeld, Andreas and Welk, Martin},
  doi          = {10.1007/s10851-025-01267-5},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {10},
  number       = {5},
  pages        = {1-28},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Matrix-valued LogSumExp approximation for colour morphology},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vertex characterization via second-order topological derivatives. <em>JMIV</em>, <em>67</em>(5), 1-19. (<a href='https://doi.org/10.1007/s10851-025-01269-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on identifying vertex characteristics in 2D images using topological asymptotic analysis. Vertex characteristics include both the location and the type of the vertex, with the latter defined by the number of lines forming it and the corresponding angles. This problem is crucial for computer vision tasks, such as distinguishing between fore- and background objects in 3D scenes. We compute the second-order topological derivative of a Mumford–Shah-type functional with respect to inclusion shapes representing various vertex types. This derivative assigns a likelihood to each pixel that a particular vertex type appears there. Numerical tests demonstrate the effectiveness of the proposed approach.},
  archive      = {J_JMIV},
  author       = {Gangl, Peter and Mejri, Bochra and Scherzer, Otmar},
  doi          = {10.1007/s10851-025-01269-3},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Vertex characterization via second-order topological derivatives},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jmui">JMUI - 4</h2>
<ul>
<li><details>
<summary>
(2025). How bare-hand clicking influences eye movement behavior in virtual button selection tasks: A comparative analysis with keyboard control. <em>JMUI</em>, <em>19</em>(3), 285-304. (<a href='https://doi.org/10.1007/s12193-025-00455-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The flexibility of virtual reality systems has fostered the development of various interaction techniques, yet our understanding of users’ eye movement behaviors with different interaction methods remains limited. This study compares eye movement details in goal-driven and stimulus-based virtual button selection tasks, using both direct bare-hand interaction and indirect keyboard control in virtual environments. Our findings show that both interface type and interaction mode significantly affect multiple eye movement metrics, including saccades, fixations, and blinks. These insights enhance our understanding of visual attention and action execution in virtual selection tasks, offering data-driven conclusions that can serve as a foundation for developing design guidelines for interaction techniques and interface designs in virtual reality. We believe these results will contribute to the creation of more natural, efficient, and user-friendly virtual reality systems, further advancing the technology.},
  archive      = {J_JMUI},
  author       = {Du, Xiaoxi and Jia, Lesong and Wu, Jinchun and Peng, Ningyue and Zhou, Xiaozhou and Xue, Chengqi},
  doi          = {10.1007/s12193-025-00455-2},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {285-304},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {How bare-hand clicking influences eye movement behavior in virtual button selection tasks: A comparative analysis with keyboard control},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Web-based multimodal learning system to develop social communication skills. <em>JMUI</em>, <em>19</em>(3), 271-284. (<a href='https://doi.org/10.1007/s12193-025-00460-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual agents offer a scalable and cost-effective alternative to traditional human-led social skills training, which is often limited by the availability of professional trainers. Our web-based learning system, developed following Bellack et al.’s training model, integrates speech recognition, response selection, speech synthesis, and nonverbal behavior generation to provide automated training. To evaluate its effectiveness, we conducted a four-week study with 60 Japanese participants from the general population, focusing on four key social skills. Participants completed questionnaires assessing autistic traits, social anxiety, and changes in social communication skills post-training. Results demonstrated significant improvements, with notable results in participants’ confidence in declining requests. These findings highlight the potential of web-based virtual agents for enhancing social communication skills and suggest promising applications for social communication research and intervention programs.},
  archive      = {J_JMUI},
  author       = {Tanaka, Hiroki and Lisitsyna, Alexandra},
  doi          = {10.1007/s12193-025-00460-5},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {271-284},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Web-based multimodal learning system to develop social communication skills},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring user interactions with commercial machines via real-world application logs in the lab. <em>JMUI</em>, <em>19</em>(3), 253-269. (<a href='https://doi.org/10.1007/s12193-025-00456-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log analysis is an effective user experience testing method for exploring subtle user interactions with commercial machines. The rapid iteration of commercial machines and their applications calls for a more cost-effective alternative to traditional in-the-wild approaches. However, lab-based log studies typically rely on customized testing programs, which do not represent real-world usage. Therefore, this study investigates the feasibility of log analysis using real-world applications in lab settings to explore user interaction. We present a case study exploring smartphone gesture interaction in more than 50 everyday operations across 10 real-world applications. We identified and analyzed detailed gesture parameters from log data on three common gesture sets, including tapping (single and double), zooming (pinching and spreading), and swiping (horizontal and vertical). Grounding in related works, we discuss observed behavior patterns, their potential causes, and implications for future design in commercial machine interfaces. We highlight the applicability of the presented approach by discussing its advantages, direction for improvements, and potential applications. We call for more studies to explore the norms and values of this approach.},
  archive      = {J_JMUI},
  author       = {Song, Fangli and Wang, Wei and Zhou, Dasen and Bryan-Kinns, Nick and Zhang, Jun and Chen, Qi and Du, Le},
  doi          = {10.1007/s12193-025-00456-1},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {253-269},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Exploring user interactions with commercial machines via real-world application logs in the lab},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of music tempo reflecting group activity on multi-participant exercise. <em>JMUI</em>, <em>19</em>(3), 235-251. (<a href='https://doi.org/10.1007/s12193-025-00457-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During training, it is common to perform certain movements repeatedly within a short period—such as squats, jumping jacks, and burpees—where the frequency and rhythm of the exercises significantly impact their effectiveness. In this study, we focused on group training sessions involving repetitive exercises—a frequent scenario in gyms, home workouts, and remote training environments. Specifically, we investigated the potential of using music tempo to enhance group training effectiveness, diverging from traditional methods that primarily rely on visual cues for guidance and synchronization. We conducted a user study in an in-person group exercise setting with 18 participants organized into six groups of three. They performed squats under conditions with and without music aligned to their average tempo. We then examined how adding group-synchronized musical feedback—on top of natural interactions such as observing each other’s movements and listening to footsteps—affected training outcomes. By analyzing skeletal data and questionnaire responses, we found that music tempo contributed to improvements in both movement synchronization and aspects of the overall training experience. We also gathered suggestions for future enhancements in music-based support, including applying the proposed method to groups with smaller physical ability differences and considering differences in users’ musical backgrounds.},
  archive      = {J_JMUI},
  author       = {Wang, Ruiyun and Jin, Yuchen and Huang, Jiayun and Takahashi, Shin},
  doi          = {10.1007/s12193-025-00457-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {235-251},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Effects of music tempo reflecting group activity on multi-participant exercise},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="joh">JOH - 1</h2>
<ul>
<li><details>
<summary>
(2025). A random-key optimizer for combinatorial optimization. <em>JOH</em>, <em>31</em>(4), 1-47. (<a href='https://doi.org/10.1007/s10732-025-09568-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the Random-Key Optimizer (RKO), a versatile and efficient stochastic local search method tailored to combinatorial optimization problems. Using the random-key concept, RKO encodes solutions as vectors of random keys that are subsequently decoded into feasible solutions via problem-specific decoders. The RKO framework is able to combine a plethora of classic metaheuristics, each capable of operating independently or in parallel, with solution sharing facilitated through an elite solution pool. This modular approach allows for the adaptation of various metaheuristics, including simulated annealing, iterated local search, and greedy randomized adaptive search procedures, among others. The efficacy of the RKO framework, implemented in C++ and publicly available, is demonstrated through its application to three NP-hard combinatorial optimization problems: the $$\alpha $$ -neighborhood p-median problem, the tree of hubs location problem, and the node-capacitated graph partitioning problem. The results highlight the framework’s ability to produce high-quality solutions across diverse problem domains, underscoring its potential as a robust tool for combinatorial optimization.},
  archive      = {J_JOH},
  author       = {Chaves, Antonio A. and Resende, Mauricio G. C. and Schuetz, Martin J. A. and Brubaker, J. Kyle and Katzgraber, Helmut G. and de Arruda, Edilson F. and Silva, Ricardo M. A.},
  doi          = {10.1007/s10732-025-09568-z},
  journal      = {Journal of Heuristics},
  month        = {12},
  number       = {4},
  pages        = {1-47},
  shortjournal = {J. Heuristics},
  title        = {A random-key optimizer for combinatorial optimization},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jota">JOTA - 14</h2>
<ul>
<li><details>
<summary>
(2026). A positive semidefinite safe approximation of multivariate distributionally robust constraints determined by simple functions. <em>JOTA</em>, <em>208</em>(1), 1-27. (<a href='https://doi.org/10.1007/s10957-025-02791-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-level reformulations of (nonconvex) distributionally robust optimization (DRO) problems are often intractable, as they contain semi-infinite dual constraints. Based on such a semi-infinite reformulation, we present a safe approximation that allows for the computation of feasible solutions for DROs that depend on nonconvex multivariate simple functions. Moreover, the approximation allows to address ambiguity sets that can incorporate information on moments as well as confidence sets. The typical strong assumptions on the structure of the underlying constraints, such as convexity in the decisions or concavity in the uncertainty found in the literature were, at least in part, recently overcome in [16]. We start from the duality-based reformulation approach in [16] that can be applied for DRO constraints based on simple functions that are univariate in the uncertainty parameters. We significantly extend their approach to multivariate simple functions, which leads to a considerably wider applicability of the proposed reformulation approach. In order to achieve algorithmic tractability, the presented safe approximation is then realized by a discretized counterpart for the semi-infinite dual constraints. The approximation leads to a computationally tractable mixed-integer positive semidefinite problem for which state-of-the-art software implementations are readily available. The tractable safe approximation provides sufficient conditions for distributional robustness of the original problem, i.e., obtained solutions are provably robust.},
  archive      = {J_JOTA},
  author       = {Dienstbier, Jana and Liers, Frauke and Rolfes, Jan},
  doi          = {10.1007/s10957-025-02791-5},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-27},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {A positive semidefinite safe approximation of multivariate distributionally robust constraints determined by simple functions},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). On the optimal control existence for multi-term multi-order fractional differential equations with impulsive conditions. <em>JOTA</em>, <em>208</em>(1), 1-24. (<a href='https://doi.org/10.1007/s10957-025-02830-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the existence of solutions to nonlinear impulsive fractional optimal control problems, where the state equations are linear with respect to their control variables and governed by multi-order systems of multi-term fractional differential ones. Initial and impulsive conditions are also given. The performance index is considered as an integral functional, whose integrand is continuous with respect to its variables. At first, we transform the state equations into the integral ones to prove the existence and uniqueness of solutions, assuming that the nonlinear functions in the equations are Lipschitz continuous in a bounded domain. Secondly, using the generalized Arzela-Ascoli theorem, we establish that sets of our admissible processes are compact ones in a piecewise continuous function space to show the optimal control existence.},
  archive      = {J_JOTA},
  author       = {Choe, HuiChol and Han, SuRim and Pak, SunAe and Kim, GwangHyok and Kim, GyongGuk and U, DanOh},
  doi          = {10.1007/s10957-025-02830-1},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {On the optimal control existence for multi-term multi-order fractional differential equations with impulsive conditions},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). On the existence and the stability of solutions in nonconvex vector optimization $$^\dagger $$. <em>JOTA</em>, <em>208</em>(1), 1-26. (<a href='https://doi.org/10.1007/s10957-025-02831-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper is devoted to the existence of weak Pareto solutions and the weak sharp minima at infinity property for a general class of constrained nonconvex vector optimization problems with unbounded constraint set via asymptotic cones and generalized asymptotic functions. Then we show that these conditions are useful for studying the solution stability of nonconvex vector optimization problems with linear perturbation. We also provide some applications for a subclass of robustly quasiconvex vector optimization problems.},
  archive      = {J_JOTA},
  author       = {Nghi, Tran Van and Kien, Le Ngoc and Tuyen, Nguyen Van},
  doi          = {10.1007/s10957-025-02831-0},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {On the existence and the stability of solutions in nonconvex vector optimization $$^\dagger $$},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Derivative-free optimization on riemannian manifolds using simplex gradient approximations. <em>JOTA</em>, <em>208</em>(1), 1-22. (<a href='https://doi.org/10.1007/s10957-025-02832-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In optimization problems with complex or unknown gradients, using a derivative-free algorithm is an efficient approach. In this paper, we present a new derivative-free optimization method designed for problems in which the search space is a Riemannian manifold. The method utilizes a simplex gradient approximation and incorporates a line search strategy. We state the conditions under which the proposed algorithm is well-defined and establish its convergence to critical points on Riemannian manifolds from any starting point. Lastly, we demonstrate the practical implementation of this technique on two commonly used manifolds and compare its performance to some existing Riemannian derivative-free methods.},
  archive      = {J_JOTA},
  author       = {Najafi, Shahabeddin and Hajarian, Masoud},
  doi          = {10.1007/s10957-025-02832-z},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Derivative-free optimization on riemannian manifolds using simplex gradient approximations},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A three-term conjugate gradient-type method with sufficient descent property for vector optimization. <em>JOTA</em>, <em>208</em>(1), 1-42. (<a href='https://doi.org/10.1007/s10957-025-02815-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of vector optimization represents a critical domain within the broader spectrum of optimization problems. Extensive research efforts are currently dedicated to developing solution methods for vector optimization problems. A range of classical approaches, originally designed for scalar optimization, have been adapted to address issues in vector optimization. These include techniques such as the steepest descent method, Newton’s method, quasi-Newton method and conjugate gradient method, among others. However, limited attention has been given to the three-term conjugate gradient method in the context of vector optimization. In this paper, based on the modified self-scaling memoryless Broyden-Fletcher-Goldfarb-Shanno (BFGS) method proposed by Kou and Dai (J. Optim. Theory Appl., 165(1): 209-224, 2015), we propose a novel three-term conjugate gradient-type method specifically designed for vector optimization problems. This method ensures the sufficient descent property independent of any line search strategy. Furthermore, the improved Wolfe line search is extended to vector optimization. The global convergence of the proposed method under the improved Wolfe line search is analyzed, demonstrating that at least one accumulation point of the sequence generated by the proposed algorithm is a K-critical point of vector optimization problem. Numerical experiments conducted on a set of benchmark test problems highlight the effectiveness of the proposed method compared to some existing gradient-based approaches.},
  archive      = {J_JOTA},
  author       = {Chen, Yu and Chen, Helong and Zhu, Zhibin},
  doi          = {10.1007/s10957-025-02815-0},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-42},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {A three-term conjugate gradient-type method with sufficient descent property for vector optimization},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Preconditioned barzilai-borwein methods for multiobjective optimization problems. <em>JOTA</em>, <em>208</em>(1), 1-43. (<a href='https://doi.org/10.1007/s10957-025-02824-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preconditioning is a powerful strategy for addressing ill-conditioned problems in optimization. It involves utilizing a preconditioning matrix to reduce the condition number and speed up the convergence of first-order methods. However, in multiobjective optimization, capturing the curvature of all objective functions using a single preconditioning matrix is challenging. Consequently, second-order methods tailored for multiobjective optimization problems (MOPs) employ distinct matrices for each of the objectives in direction-finding subproblems, resulting in expensive per-step costs. To strike a balance between per-step costs and better curvature exploration, we develop a “preconditioning” $$+$$ “preconditioning” strategy to devise a preconditioned Barzilai-Borwein descent method for MOPs (PBBMO). Specifically, this method integrates a single scaling matrix to capture the local geometry of an implicit scalarization problem, leading to reduced per-step costs. We then incorporate the Barzilai-Borwein rule relative to the matrix metric to tune the gradients within the direction-finding subproblem. This can be interpreted as an additional diagonal preconditioner tailored to each objective for better curvature exploration. From a preconditioning perspective, we employ the BFGS update formula to approximate a trade-off of Hessian matrices. Subsequently, we develop a Barzilai-Borwein quasi-Newton method with Wolfe line search for MOPs. Under mild assumptions, we provide a convergence analysis for the Barzilai-Borwein quasi-Newton method. Finally, comparative numerical results validate the efficiency of the proposed method, even when applied to higher-dimensional and ill-conditioned problems.},
  archive      = {J_JOTA},
  author       = {Chen, Jian and Chen, Wang and Tang, Liping and Yang, Xinmin},
  doi          = {10.1007/s10957-025-02824-z},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-43},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Preconditioned barzilai-borwein methods for multiobjective optimization problems},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A new infeasible projection method for stochastic variational inequality problem. <em>JOTA</em>, <em>208</em>(1), 1-28. (<a href='https://doi.org/10.1007/s10957-025-02825-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new infeasible stochastic approximation projection method based on the golden ratio for a nonmonotone stochastic variational inequality problem. In the traditional golden ratio methods, the constant $$\phi $$ is taken as $$\frac{1+\sqrt{5}}{2}$$ . However, the constant is relaxed to the interval $$(1,\infty )$$ in our method. A new self-adaptive step size which is admitted to be increasing is generated for dealing with the unknown Lipschitz constant of the mapping. The almost sure convergence and convergence rate of the proposed method are shown. Some numerical examples are given to illustrate the competitiveness of our algorithm compared to the related algorithms in the literature. Finally, we apply our method to solve a network bandwidth allocation problem.},
  archive      = {J_JOTA},
  author       = {Wang, Shenghua and Zhang, Yueyao and Cho, Yeol Je},
  doi          = {10.1007/s10957-025-02825-y},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-28},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {A new infeasible projection method for stochastic variational inequality problem},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A unified derivative-free projection framework for convex-constrained nonlinear equations. <em>JOTA</em>, <em>208</em>(1), 1-26. (<a href='https://doi.org/10.1007/s10957-025-02826-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a framework and a unified convergence analysis for derivative-free projection methods to solve large-scale constrained nonlinear equations. The framework combines the inertial extrapolation technique with the concept of approximate projections, thereby encompassing and generalising the results of previous studies. Additionally, we introduce a new function-based line search based on the stabilised Barzilai and Borwein method, as introduced by Burdakov et al. The framework further explores the impact of six distinct, well-known line search schemes on its overall performance. Through numerical experiments, we highlight the theoretical findings.},
  archive      = {J_JOTA},
  author       = {Ibrahim, Abdulkarim Hassan and Alshahrani, Mohammed and Al-Homidan, Suliman},
  doi          = {10.1007/s10957-025-02826-x},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {A unified derivative-free projection framework for convex-constrained nonlinear equations},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust n-agent heterogeneous investment-consumption game under $$\alpha $$ -maxmin mean-variance-utility criterion. <em>JOTA</em>, <em>208</em>(1), 1-38. (<a href='https://doi.org/10.1007/s10957-025-02834-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a robust heterogeneous n-agent stochastic differential game under a mean-variance-utility criterion, where agents compete based on relative performance in the presence of model uncertainty. Model uncertainty is represented by a set of equivalent probability measures, with Novikov’s condition imposed to guarantee their mutual equivalence. Ambiguity attitudes are characterized by the $$\alpha $$ -maxmin model. Agents invest and consume in a financial market exposed to both common and idiosyncratic risks, aiming to maximize relative terminal wealth with the mean-variance criterion and the expected utility of relative consumption under the $$\alpha $$ -maxmin model. We formulate a heterogeneous game that emphasizes outperforming a specific group of competitors. Agents focus on a weighted average of their peers’ wealth and consumption. This optimization problem is inherently time-inconsistent, and we derive the associated extended Hamilton-Jacobi-Bellman-Isaacs (HJBI) equations within a game-theoretic framework. The robust best response strategies are composed of a myopic component and another component that reacts to the actions of other agents. We obtain closed-form solutions for the robust Nash equilibrium investment-consumption strategies through a system of linear equations. This paper explores how levels of ambiguity, ambiguity aversion, risk aversion, and competition affect Nash equilibrium strategies, uncovering the herd effect that competition has on agents’ strategies.},
  archive      = {J_JOTA},
  author       = {Guan, Guohui and Liang, Zongxia},
  doi          = {10.1007/s10957-025-02834-x},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-38},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Robust n-agent heterogeneous investment-consumption game under $$\alpha $$ -maxmin mean-variance-utility criterion},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unique solvability of infinite dimensional differential sweeping systems. <em>JOTA</em>, <em>208</em>(1), 1-37. (<a href='https://doi.org/10.1007/s10957-025-02837-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the existence and uniqueness of solutions to a differential sweeping system. This system is an implicit coupled dynamical system consisting of a nonlinear differential equation and a history/state-dependent sweeping process. First, an existence result to a perturbed state-dependent sweeping process is proved based on Schauder’s fixed-point theorem. Next, the unique solvability of a history/state-dependent sweeping process is established by employing a fixed-point theorem for a history-dependent operator. Finally, using tools from nonsmooth analysis and Banach’s fixed-point theorem, we establish the existence and uniqueness of solutions to a differential sweeping system.},
  archive      = {J_JOTA},
  author       = {Du, Jinsheng and Migórski, Stanisław and Vilches, Emilio and Zeng, Shengda},
  doi          = {10.1007/s10957-025-02837-8},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-37},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Unique solvability of infinite dimensional differential sweeping systems},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Variational analysis of generalized ordinal nash games on banach spaces. <em>JOTA</em>, <em>208</em>(1), 1-25. (<a href='https://doi.org/10.1007/s10957-025-02838-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the generalized ordinal Nash games defined over Banach spaces by employing variational techniques. To reformulate these games in terms of quasi-variational inequality problems, we will first form a suitable principal operator and study some significant properties of this operator. Then, we deduce the sufficient conditions to obtain an equilibrium for the proposed game by solving an auxiliary quasi-variational inequality. Based on this quasi-variational reformulation, we derive the existence of equilibrium for generalized ordinal Nash games with mid-point continuous preference maps. We apply the derived results to ensure the presence of Pareto equilibrium for multi-objective games and dynamic electricity markets.},
  archive      = {J_JOTA},
  author       = {Valecha, Shivani and Sultana, Asrifa},
  doi          = {10.1007/s10957-025-02838-7},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-25},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Variational analysis of generalized ordinal nash games on banach spaces},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Estimating mean and variance of random coefficients in stochastic variational problems using second-order methods. <em>JOTA</em>, <em>208</em>(1), 1-48. (<a href='https://doi.org/10.1007/s10957-025-02805-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the need to identify both deterministic and stochastic coefficients in various stochastic partial differential equations, we have developed an abstract inversion framework. The inverse problem is studied in a stochastic optimization framework. Essential properties of solution maps are derived to prove the solvability of the optimization problems and to establish optimality conditions. A comprehensive regularization framework, including total-variation regularization, has been created to identify rapidly varying coefficients. By using the Bregman distance, we provide new convergence rates for stochastic inverse problems in the abstract formulation without the need for the so-called smallness condition. Assuming finite-dimensional noise, the inverse problem is parameterized and solved using the stochastic Galerkin framework. The numerical schemes utilize Hessian-based optimization methods, resulting in rapid convergence. The numerical results are promising, demonstrating the feasibility and effectiveness of the proposed framework.},
  archive      = {J_JOTA},
  author       = {Gong, Zi-Jia and Khan, Akhtar A. and Sama, Miguel and Starkloff, Hans-Jörg},
  doi          = {10.1007/s10957-025-02805-2},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-48},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Estimating mean and variance of random coefficients in stochastic variational problems using second-order methods},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Differential inclusions for measures and lyapunov stability. <em>JOTA</em>, <em>208</em>(1), 1-22. (<a href='https://doi.org/10.1007/s10957-025-02828-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on differential inclusions for measures, that are differential relations whose solutions are time-evolving measures. The definition of evolution equations for measures attracted a lot of attention recently. We start by recalling the main concepts developed in the latest literature and comparing them. In particular, we show how the definition of Measure Differential Inclusion is the most general allowing to model phenomena as diffusion from a Dirac delta. Then we pass to Lyapunov-type stability proposing two concepts of stability, based on the measure support and first moment, and show relationships between such definitions depending on the assumptions on the evolution equation used.},
  archive      = {J_JOTA},
  author       = {D’Apice, Ciro and Manzo, Rosanna and Piccoli, Benedetto},
  doi          = {10.1007/s10957-025-02828-9},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Differential inclusions for measures and lyapunov stability},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Majorization-minimization bregman proximal gradient algorithms for NMF with the Kullback–Leibler divergence. <em>JOTA</em>, <em>208</em>(1), 1-34. (<a href='https://doi.org/10.1007/s10957-025-02833-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) is a popular method in machine learning and signal processing to decompose a given nonnegative matrix into two nonnegative matrices. In this paper, we propose new algorithms, called majorization-minimization Bregman proximal gradient algorithm (MMBPG) and MMBPG with extrapolation (MMBPGe) to solve NMF. These iterative algorithms minimize the objective function and its potential function monotonically. Assuming the Kurdyka–Łojasiewicz property, we establish that a sequence generated by MMBPG(e) globally converges to a stationary point. We apply MMBPG and MMBPGe to the Kullback–Leibler (KL) divergence-based NMF. While most existing KL-based NMF methods update two blocks or each variable alternately, our algorithms update all variables simultaneously. MMBPG and MMBPGe for KL-based NMF are equipped with a separable Bregman distance that satisfies the smooth adaptable property and that makes its subproblem solvable in closed form. Using this fact, we guarantee that a sequence generated by MMBPG(e) globally converges to a Karush–Kuhn–Tucker (KKT) point of KL-based NMF. In numerical experiments, we compare proposed algorithms with existing algorithms on synthetic data and real-world data.},
  archive      = {J_JOTA},
  author       = {Takahashi, Shota and Tanaka, Mirai and Ikeda, Shiro},
  doi          = {10.1007/s10957-025-02833-y},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-34},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Majorization-minimization bregman proximal gradient algorithms for NMF with the Kullback–Leibler divergence},
  volume       = {208},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

<h2 id="jrtip">JRTIP - 1</h2>
<ul>
<li><details>
<summary>
(2025). YOLO-GSD: A real-time pedestrian detection algorithm based on YOLOv8 in dense environments. <em>JRTIP</em>, <em>22</em>(6), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01771-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent advancements in vision intelligence, pedestrian detection in autonomous driving has become a critical research focus within computer vision. Dense pedestrian scenarios present significant challenges from multi-scale variations and occlusions. Traditional detection methods can be used for pedestrian detection in ordinary scenarios, but they face challenges, such as high computational complexity and overly sophisticated models, that prevent effective deployment on mobile devices like in-vehicle cameras, along with unsatisfactory detection accuracy under multi-scale pedestrian scenarios and heavy occlusion conditions. To address these challenges, this paper proposes YOLO-GSD, an improved lightweight real-time pedestrian detection algorithm based on YOLOv8. The algorithm first introduces a dedicated detection layer specifically designed for small-scale targets. It then incorporates lightweight Ghost convolution and designs a DG-C2f module by integrating Ghost convolution and Dynamic Convolution, aiming to reduce computational complexity while enhancing the algorithm’s multi-scale feature fusion capability. Additionally, it employs the ultra-lightweight DySample upsampler for efficient feature reconstruction and integrates the SEAM attention mechanism to improve occlusion-aware detection. Finally, WIoUv3 is used to replace the CIoU loss function, which improves the generalization ability and overall performance of the algorithm. Experimental results demonstrate mAP@0.5 scores of 90.7% on the WiderPerson dataset (1.4% higher than the baseline) and 86.5% on the CrowdHuman dataset (2.2% improvement). The algorithm’s parameter count is reduced to 6.24 M, its FLOPs are lowered to 22.7 G, and its FPS is increased to 106.6. In addition, a homogeneous training comparison was conducted on the small-object dataset RSOD, demonstrating the advantages of YOLO-GSD in small-object detection. Therefore, the YOLO-GSD algorithm proposed in this paper is suitable for real-time pedestrian detection in multi-scale and occlusion scenarios on mobile platforms with limited computational resources.},
  archive      = {J_JRTIP},
  author       = {Zhang, Zuhao and Li, Weiwei and Luo, Lin},
  doi          = {10.1007/s11554-025-01771-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-GSD: A real-time pedestrian detection algorithm based on YOLOv8 in dense environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="kis">KIS - 29</h2>
<ul>
<li><details>
<summary>
(2025). SecPPAccess: Secured privacy protection access control on big data in cloud computing paradigm. <em>KIS</em>, <em>67</em>(9), 8195-8218. (<a href='https://doi.org/10.1007/s10115-025-02443-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of cloud computing, preserving the privacy of big data while also allowing for secure access control is a critical concern. With the increasing adoption of cloud technology, it is imperative to address the challenges associated with safeguarding sensitive data while enabling authorized access. This paper develops an efficient privacy-preserving security model that uses cryptographic techniques to protect sensitive data and ensure that only authorized individuals can access it. The research puts together a secure data authentication technique, named secured privacy protection access control (SecPPAccess), allowing secured communication in cloud computing. For the protection of privacy for sensitive data, the protected transferring of data is commenced among the elements, like a user, cloud server, registration authority, key generation center and data owner, by using many phases mainly the key generation phase, setup phase, server registration, user registration, data upload, data encryption, requester authentication, data access, and data download phase. Here, a method is designed newly for securing data privacy using various operations, like secret keys, hashing, encryption, etc. The study proves that the initiated SecPPAccess model achieves the highest rate of detection of 0.85, the lowest usage for memory of 0.505 MB, and less computation time of 51.50 s.},
  archive      = {J_KIS},
  author       = {Gupta, Lalit Mohan and Garg, Hitendra and Samad, Abdus and Singh, Atul Kumar},
  doi          = {10.1007/s10115-025-02443-0},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8195-8218},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SecPPAccess: Secured privacy protection access control on big data in cloud computing paradigm},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective and lightweight lossy compression of tensors: Techniques and applications. <em>KIS</em>, <em>67</em>(9), 8143-8193. (<a href='https://doi.org/10.1007/s10115-025-02471-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world data from various domains can be represented as tensors, and a significant portion of them is large scale. Thus, tensor compression is crucial for their storage and transmission. Recently, deep learning-based methods have emerged to enhance compression performance. However, they require considerable compression time to fulfill their performance. In this work, to achieve both speed and performance, we develop ELiCiT, an effective and lightweight lossy tensor compression method. When designing ELiCiT, we avoid deep auto-regressive neural networks and index reordering, which incur high computational costs of deep learning-based tensor compression. Specifically, instead of using the orders of indices as parameters, we introduce a feature-based model for indices, which enhances the model’s expressive capacity and simplifies the overall end-to-end training procedure. Moreover, to reduce the size of the parameters and computational cost for inference, we adopt end-to-end clustering-based quantization, as an alternative to deep auto-regressive architecture. As a result, ELiCiT becomes easy to optimize with enhanced expressiveness. We prove that it (partially) generalizes deep learning-based methods and also traditional ones. Using eight real-world tensors, we show that ELiCiT yields compact outputs that fit the input tensor accurately. Compared to the best competitor with similar fitness, it offers 1.51 $$-$$ 5.05 $$\times $$ smaller outputs. Moreover, compared to deep learning-based compression methods, ELiCiT is 11.8 $$-$$ 96.0 $$\times $$ faster with 5–48% better fitness for a similarly sized output. We also demonstrate that ELiCiT is extended to matrix completion, neural network compression , and tensor stream summarization, providing the best trade-offs between model size and application performance.},
  archive      = {J_KIS},
  author       = {Ko, Jihoon and Kwon, Taehyung and Jung, Jinhong and Shin, Kijung},
  doi          = {10.1007/s10115-025-02471-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8143-8193},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Effective and lightweight lossy compression of tensors: Techniques and applications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective feature selection and classification technique for palmprint biometric identification systems. <em>KIS</em>, <em>67</em>(9), 8115-8142. (<a href='https://doi.org/10.1007/s10115-025-02478-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint recognition’s outstanding sanitary, non-invasive, and user-friendliness properties have sparked a lot of research attention. The majority of palmprint recognition techniques used today are deep learning approaches, which typically use palmprint images to learn discriminative features. In most cases, considerable labelled samples are needed to perform well enough for identification. Here, we propose employing a deep learning method for palmprint recognition to get around the problems. First, the original picture should be (a) converted to greyscale, (b) cropped and resized, and (c) contrast-enhanced using the contrast-limited adaptive histogram equalization method. Next, use the ConvNeXt approach to extract the most essential characteristics from contrast-enhanced palm pictures. After removing redundant collected attributes, the improved spotted hyena optimizer algorithm selects the most important features. Finally, the deep fuzzy neural network (DFNN) technique determines if the palmprint image matches. Use the sooty tern optimization algorithm for increased identification and categorization accuracy to increase categorization accuracy. The proposed approach effectively reduces the number of attributes and computation time while increasing the accuracy of the optimized DFNN—an approximation of the proposed methods using Tongji, IITD, and CASIA open palmprint datasets. The approach has better accuracy with 99.62%, 99.72%, and 99.67% on IITD, Tongji, and CASIA palmprint databases. Experiments show that our approach achieves a high identification rate while using a substantially fewer number of features.},
  archive      = {J_KIS},
  author       = {Nalamothu, Aravind and Rayachoti, Eswaraiah},
  doi          = {10.1007/s10115-025-02478-3},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8115-8142},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An effective feature selection and classification technique for palmprint biometric identification systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VCGAE++: Variational collective graph autoEncoder for multi-behavior recommendation. <em>KIS</em>, <em>67</em>(9), 8085-8114. (<a href='https://doi.org/10.1007/s10115-025-02467-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoder (VAE) is known as a classic and effective method in modeling users’ homogeneous behaviors in recommender systems. In recent years, graph neural networks (GNNs) have achieved promising performance in learning users’ preferences by modeling complex relationships between users and items. However, most VAE- and GNN-based methods are for single-behavior recommendation, rather than the more prevalent counterpart in real-world applications, i.e., multi-behavior recommendation. This motivates us to leverage VAE and GNNs to address the more important and challenging problem of multi-behavior recommendation. Traditional multi-behavior recommendation models have not captured the complex transition relationships across different types of behaviors well, ignoring the varying semantic strength of different types of behaviors. In addition, most of them just construct separate behavior-specific subgraphs and learn separate collaborative filtering embeddings, overlooking the information of the global graph. Moreover, existing methods rarely explored how to deal with the sparse data under the target behavior (e.g., purchase). To tackle the above four challenges, we propose a novel multi-behavior recommendation framework named VCGAE++, which inherits the advantages of VCGAE (Variational Collective Graph AutoEncoder) to fully exploit the multi-behavior data and model the high-order relationships to improve the accuracy of the recommendations. Specifically, we design a multi-behavior contrastive learning framework to capture the complex transitions between diverse behaviors. In addition, we design a global graph information fusion network to capture the high-level relationships across different user-item interaction graphs. Extensive experiments on four real-world datasets clearly demonstrate the effectiveness of our VCGAE++ compared with the state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Zhuang, Yingxuan and Liu, Yang and Pan, Weike and Ming, Zhong},
  doi          = {10.1007/s10115-025-02467-6},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8085-8114},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {VCGAE++: Variational collective graph autoEncoder for multi-behavior recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task multi-modal graph neural network for recommender system. <em>KIS</em>, <em>67</em>(9), 8059-8084. (<a href='https://doi.org/10.1007/s10115-025-02456-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of online information, users may also face information overload. To handle this problem, recommender systems have become an effective strategy, which can analyze the characters of users and items to provide valuable information. One of the important types of information is the item’s side information. For example, in Amazon dataset, side information mainly includes visual side information (e.g., image and video), textual side information (e.g., title and description), and auxiliary side information (e.g., brand and category). To analyze various types of side information, some research designed multiple modalities for different types of side information, which can improve the performance of the recommender system. To analyze the deeper relationships between users and items, recent works also use a graph structure to represent the interactions. Existing works on multi-modal recommender systems using graph neural networks largely depend on the interaction records, while little effort focuses on the relationships between interactions and various types of side information. In this paper, we propose a novel multi-task learning model. First, we construct the interaction records to graphs for each modality to gather the representations, and then we analyze the representations of each modality and the specific side information based on the similarities. We design a multi-task multi-modal graph neural network framework built upon message passing with the attention mechanism of graph neural networks, which can generate the representations of users and items from interaction records, and then analyze the relationships between the representations from GNNs and item’s side information. We conduct experiments on three public datasets, Amazon, Modcloth and MovieLens. The results of our model outperform the state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Jiao, Shengzhe and Zhang, Yihong and Hara, Takahiro},
  doi          = {10.1007/s10115-025-02456-9},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8059-8084},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-task multi-modal graph neural network for recommender system},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view multi-label personalized classification via generalized exclusive sparse tensor factorization. <em>KIS</em>, <em>67</em>(9), 8023-8057. (<a href='https://doi.org/10.1007/s10115-025-02449-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification (MLC) assigns multiple relevant labels to each sample simultaneously, while multi-view MLC aims to apply MLC to handle heterogeneous data represented by multiple feature subsets. In recent years, a variety of methods have been proposed to handle these problems and have achieved great success in a wide range of applications. MLC saves global label correlation by building a single model shared by all samples but ignores sample-specific local structures, while personalized learning (PL) is able to preserve sample-specific information by learning local models but ignores the global structure. Integrating PL with MLC is a straightforward way to overcome the limitations, but it still faces three key challenges. (1) capture both local and global structures in a unified model; (2) efficiently preserve full-order interactions between labels, samples and features or multi-view features in heterogeneous data; (3) learn a concise and interpretable model where only a fraction of interactions are associated with multiple labels. In this paper, we propose a novel Multi-label Personalized Classification (MLPC) method and its multi-view extension to handle these challenges. For (1), it integrates local and global components to preserve sample-specific information and global structure shared across samples, respectively. For (2), a multilinear model is developed to capture full-order label-feature-sample interactions, and over-parameterization is avoided by tensor factorization. For (3), exclusive sparsity regularization penalizes factorization by promoting intra-group competition, thereby eliminating irrelevant and redundant interactions during Exclusive Sparse Tensor Factorization (ESTF). Moreover, theoretical analysis generalizes the proposed ESTF and reveals the equivalence between MLPC and a family of jointly regularized counterparts. We develop an alternating algorithm to solve the optimization problem, and demonstrate its effectiveness based on comprehensive experiments on both synthetic and real-world benchmark datasets.},
  archive      = {J_KIS},
  author       = {Fei, Luhuan and Lin, Weijia and Wang, Jiankun and Sun, Lu and Kudo, Mineichi and Kimura, Keigo},
  doi          = {10.1007/s10115-025-02449-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8023-8057},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-view multi-label personalized classification via generalized exclusive sparse tensor factorization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond pairwise relationships: A transformer-based hypergraph learning approach for fraud detection. <em>KIS</em>, <em>67</em>(9), 7987-8022. (<a href='https://doi.org/10.1007/s10115-025-02476-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraud detection in online networks has become increasingly challenging as fraudsters adopt sophisticated camouflage tactics to evade detection, making it imperative to combat their deceptive strategies. Graph-based fraud detection has gained significant attention in recent years, reflecting its growing potential to mitigate sophisticated fraudulent activities. The main objective of graph-based fraud detection is to distinguish between fraudsters and normal entities within graphs. While real-world networks contain complex, high-order relationships, existing graph-based fraud detection methods focus solely on pairwise interactions, overlooking non-pairwise relationships and the broader dependencies among entities within fraud graphs. Thus, we highlight the importance of exploring non-pairwise relationships to build a more effective fraud detection model. In this paper, we propose TROPICAL, a novel TRansfOrmer-based hyPergraph LearnIng framework for detecting CAmouflaged maLicious actors in online social networks. To capture comprehensive high-order relations, we construct a hypergraph from the original input graph. However, constructing the hypergraph can be computationally intensive. TROPICAL addresses this challenge by carefully selecting moderate hyperparameters, creating a balance between computational efficiency and effectively capturing high-order relationships. TROPICAL learns node representations by processing multiple hyperedge groups and incorporates positional encodings into the aggregated information to enhance their distinctiveness. The aggregated sequential information is then passed through a transformer encoder, enabling the model to generate rich, high-order representations to detect camouflaged fraudsters. Extensive experiments on two real-world datasets demonstrate TROPICAL’s superior performance compared to the state-of-the-art fraud detection models. The source codes and the datasets of our work are available at https://github.com/VenusHaghighi/TROPICAL .},
  archive      = {J_KIS},
  author       = {Haghighi, Venus and Soltani, Behnaz and Shabani, Nasrin and Wu, Jia and Zhang, Yang and Yao, Lina and Yang, Jian and Sheng, Quan Z.},
  doi          = {10.1007/s10115-025-02476-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7987-8022},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Beyond pairwise relationships: A transformer-based hypergraph learning approach for fraud detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid long-range dependency-aware graph convolutional network for node classification. <em>KIS</em>, <em>67</em>(9), 7955-7986. (<a href='https://doi.org/10.1007/s10115-025-02473-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks, despite their effectiveness, face inherent limitations such as over-squashing within the message-passing mechanism. This impedes their ability to effectively capture long-range dependencies, thereby constraining their performance on graph mining tasks, such as node classification. Recent studies attempt to address this issue by developing two-step aggregation-based methods, whose key idea is to seek nodes with high similarity in topology or attribute feature space for information aggregation. Despite effectiveness, we observe that previous studies share two common deficiencies: single-type feature-based strategies of sifting nodes, and limited to preserving low-frequency information. These limitations result in the inadvertent inclusion of irrelevant nodes and the neglect of multi-frequency information within long-range dependencies. To this end, we propose a novel method, called hybrid long-range dependency-aware graph convolutional network (HLDGCN) to overcome the above deficiencies. Specifically, HLDGCN first employs a flexible node selection process that considers both topological and attribute features across diverse graph structures. Subsequently, the original graph is transformed into a long-range dependency-aware graph by incorporating additional edges that connect selected nodes. Contrary to prior approaches that focus solely on high-similarity nodes, HLDGCN incorporates both high- and low-similarity nodes, assigning positive and negative edge weights, respectively. This design ensures the preservation of diverse frequency information within the graph. Furthermore, HLDGCN employs a specialized graph convolution layer that utilizes a separated fusion strategy to aggregate information effectively on the transformed graph. Extensive experiments on various benchmark datasets, including homophily and heterophily graphs, demonstrate the superiority of HLDGCN on the node classification task.},
  archive      = {J_KIS},
  author       = {Chen, Jinsong and Wang, Meng and He, Kun},
  doi          = {10.1007/s10115-025-02473-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7955-7986},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hybrid long-range dependency-aware graph convolutional network for node classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H2LFR: A hybrid two-layered feature ranking approach for enhanced data analysis. <em>KIS</em>, <em>67</em>(9), 7901-7953. (<a href='https://doi.org/10.1007/s10115-025-02463-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is a critical process in machine learning that involves identifying a subset of relevant features from a larger set. The primary objective of FS is to eliminate irrelevant and redundant features, thereby reducing storage requirements and computational costs, enhancing data interpretability, and improving model accuracy. FS methods can be classified as either supervised or unsupervised, depending on the availability of a target variable. Furthermore, FS techniques are broadly categorized into two subtypes: feature subset selection and feature ranking (FR). Typically, various feature subsets are identified through different FR techniques, each of which operates under specific assumptions regarding the regression function that relates input features to output. In this paper, we introduce a novel supervised hybrid two-layered feature ranking technique, referred to as H2LFR, which integrates a learning model, a metaheuristic algorithm, and weighted metrics. The first layer of this approach (H2LFR-L1) focuses on extracting encoded solutions alongside their performance metrics. Subsequently, the second layer (H2LFR-L2) is tasked with ranking the features. To evaluate the proposed method, we utilize twelve publicly available datasets and the results obtained from H2LFR are compared against 16 established FR methods. Our comparative analysis demonstrates that H2LFR consistently outperforms 12 of these methods in many cases, yielding notably robust results.},
  archive      = {J_KIS},
  author       = {Balaha, Hossam Magdy and Hassan, Asmaa El-Sayed and Balaha, Magdy Hassan},
  doi          = {10.1007/s10115-025-02463-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7901-7953},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {H2LFR: A hybrid two-layered feature ranking approach for enhanced data analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit graph neural network for deep graph transformation. <em>KIS</em>, <em>67</em>(9), 7871-7900. (<a href='https://doi.org/10.1007/s10115-025-02468-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel graph neural network architecture for the general problem of attributed graph transformation, where both the input and output are attributed graphs, and the evolution of the output graphs, including the attributes of nodes and edges, is governed by complex interactions that capture the intricate dependencies within the transformation process. Research in this area has been limited due to two key challenges: (1) the complexity of jointly modeling four types of atomic interactions, i.e., node-to-edge, node-to-node, edge-to-node, and edge-to-edge; and (2) the challenge of modeling dependencies between nodes and edges that span distant parts of the graph and develop through multiple iterative steps in the transformation process. To overcome these challenges, we present a scalable equilibrium model, NEC $$^{\infty }$$ , which incorporates both node-to-edge and edge-to-node message passing. Furthermore, we develop an efficient optimization algorithm based on the implicit function theorem [1] and provide a well-posedness analysis of NEC $$^{\infty }$$ . Experiments were conducted on four synthetic random graph datasets, four real-world datasets, and two synthetic dynamical system datasets, employing multiple evaluation metrics for node and edge prediction. The results show that NEC $$^{\infty }$$ consistently outperforms all baseline models, achieving up to a tenfold decrease in MSE on BA random graphs, edge prediction accuracy ranging from 94% to nearly 100% on synthetic datasets, and exceptional results in tasks involving molecular reactions and high-order brain networks, highlighting its strength in modeling complex graph transformations.},
  archive      = {J_KIS},
  author       = {Zhang, Lei and Zhang, Qisheng and Chen, Zhiqian and Sun, Yanshen and Lu, Chang-Tien and Zhao, Liang},
  doi          = {10.1007/s10115-025-02468-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7871-7900},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Implicit graph neural network for deep graph transformation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple feature-anisotropic regularization for out-of-domain intent detection. <em>KIS</em>, <em>67</em>(9), 7847-7869. (<a href='https://doi.org/10.1007/s10115-025-02459-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-domain (OOD) intent detection is one of the research hotspots in task-based AI dialog. Aiming at the problems of entangling features and insufficient extraction of discriminative information, multiple feature-anisotropy regularization for OOD intent detection bidirectional encoder representations from transformer (MFAR-BERT) is proposed. The instance-level and class-level features of the intent are decoupled by a multi-head feature anisotropy disentangled strategy. To extract feature information of instance-level intent samples from different perspectives, multi-view KNN contrastive learning is designed. Correlation-matrix regularization is devised to adjust the direction of class-level sentence embeddings. Feature distributions are avoided from being confined in cone space. The experimental results show that the MFAR-BERT model achieves a minimum improvement of 0.67, 0.44, 0.49, and 1.85% on ACC_ALL, F1_ALL, F1_OOD, and F1_IND compared to DCL, SCL, ABD, HybridCL, and KNNBERT models on the publicly available datasets BANKING, OverStackflow, CLINC-SMALL, and CLINCX-FULL},
  archive      = {J_KIS},
  author       = {Wu, Di and Wang, Xiaoyu and Feng, Liming},
  doi          = {10.1007/s10115-025-02459-6},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7847-7869},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multiple feature-anisotropic regularization for out-of-domain intent detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating rule-based and generative data augmentation techniques for legal document classification. <em>KIS</em>, <em>67</em>(9), 7825-7846. (<a href='https://doi.org/10.1007/s10115-025-02454-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated text classification is a fundamental research topic within the legal domain as it is the foundation for building many intelligent legal solutions. There is a scarcity of publicly available legal training data and these classification algorithms struggle to perform in low data scenarios. Text augmentation techniques have been proposed to enhance classifiers through artificially synthesised training data. In this paper we present and evaluate a combination of rule-based and advanced generative text augmentation methods designed to create additional training data for the task of classification of legal contracts. We introduce a repurposed CUAD contract dataset, modified for the task of document level classification, and compare a deep learning distilBERT model with an optimised support vector machine baseline for useful comparison of shallow and deep strategies. The deep learning model significantly outperformed the shallow model on the full training data (F1-score of 0.9738 compared to 0.599). We achieved promising improvements when evaluating the combined augmentation techniques on three reduced datasets. Augmentation caused the F1-score performance to increase by 66.6%, 17.5% and 2.6% for the 25%, 50% and 75% reduced datasets respectively, compared to the non-augmented baseline. We discuss the benefits augmentation can bring to low data regimes and the need to extend augmentation techniques to preserve key terms in specialised domains such as law.},
  archive      = {J_KIS},
  author       = {Duffy, William and O’Connell, Eoin and McCarroll, Niall and Sloan, Katie and Curran, Kevin and McNamee, Eugene and Clist, Angela and Brammer, Andrew},
  doi          = {10.1007/s10115-025-02454-x},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7825-7846},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Evaluating rule-based and generative data augmentation techniques for legal document classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Providing and evaluating a model for big data anonymization streams by using in-memory processing. <em>KIS</em>, <em>67</em>(9), 7791-7824. (<a href='https://doi.org/10.1007/s10115-025-02417-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting valuable information from vast sources of social networks while protecting confidentiality and preventing data disclosure is a significant challenge in big data environments. Traditional anonymization methods often fall short in handling the volume, variety, and velocity of big data, leading to high data loss and inefficiency. This article addresses these challenges by proposing a novel anonymization method based on K-means clustering within the Spark framework, leveraging its in-memory processing capabilities. Our model uses K-means clustering to determine optimal cluster heads, significantly reducing data loss and identity disclosure risks. By utilizing Spark's RDD abilities and the MLlib component, our method achieves faster processing times compared to traditional methods that rely on non-in-memory big data tools. Performance evaluation demonstrates that at k = 9, the cost factor is minimized to 0.20, indicating the efficiency and effectiveness of our approach. The proposed method not only enhances processing speed but also ensures minimal data loss, making it suitable for real-time anonymization of big data streams. This work provides a balanced solution that addresses the critical need for high-speed data anonymization while maintaining data privacy and utility.},
  archive      = {J_KIS},
  author       = {Shamsinejad, Elham and Banirostam, Hamid and BaniRostam, Touraj and Pedram, Mir Mohsen and Rahmani, Amir Masoud},
  doi          = {10.1007/s10115-025-02417-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7791-7824},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Providing and evaluating a model for big data anonymization streams by using in-memory processing},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TIDRec: A novel triple-graph interactive distillation method for paper recommendation. <em>KIS</em>, <em>67</em>(9), 7757-7789. (<a href='https://doi.org/10.1007/s10115-025-02457-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of academic graph networks, over-fitting has become a great challenge for graph-based paper recommendation. Existing methods mainly focus on knowledge distillation to solve above problem by compressing the volume of graph networks. However, incomplete distillation between two graphs would lead to the neglect of author’s diverse interests, resulting in biases for research interests. Therefore, we propose a new triple-graph interactive distillation recommendation (TIDRec) method for paper recommendation. Specifically, we construct a triple-graph interaction to complete the distillation knowledge between graphs to correct biased research interests of authors. First, a main model is built that integrates knowledge from both graphs (i.e., writing–citation relationship and author–author co-authorship graph) to initialize inner product vectors, capturing global research interests. Then, two auxiliary models with single graph knowledge are constructed to generate distinct inner product vectors, mining local research interests, respectively. Next, a triple-graph interactive distillation approach is designed to continuously correct the global research interests with distill vectors of each other. Finally, papers highly relevant to global research interests are recommended to authors. Extensive experiments prove that TIDRec surpasses state-of-the-art approaches, with an average performance improved by 8–25% for all four metrics.},
  archive      = {J_KIS},
  author       = {Xiao, Xia and Liu, Yan and Huang, Jiaying and Jin, Dawei and Shen, Zuwu and Zhang, Chengde},
  doi          = {10.1007/s10115-025-02457-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7757-7789},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {TIDRec: A novel triple-graph interactive distillation method for paper recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity-enhanced conversational recommendation via multi-agent reinforcement learning. <em>KIS</em>, <em>67</em>(9), 7727-7755. (<a href='https://doi.org/10.1007/s10115-025-02455-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-round conversational recommendation (MRCR) system assists users in finding the items they need with the fewest dialogue rounds by inquiring about desired features or making tailored recommendations. Numerous models employ single-agent Reinforcement Learning (RL) to accomplish MRCR and improve recommendation accuracy. However, they overlook the diversity of conversational recommendations and primarily focus on popular features or items. It impacts the fair visibility of the items and results in an unbalanced user experience. We propose a diversity-enhanced conversational recommendation model (DECREC), which is built on our proposed multi-agent RL framework. Compared to single-agent methods, this collaborative approach enables broader exploration of the action space, leading to more diverse decisions and recommendation results. Furthermore, we introduce a dynamic experience replay method that balances long-tail and head data. It ensures that each learning batch includes long-tail samples, keeping the model attentive to these less common but important data. Moreover, we incorporate feature entropy into the value estimation process, encouraging broader feature exploration and ultimately enhancing recommendation diversity. Extensive experiments on four public datasets demonstrate that DECREC reduces bias in MRCR and achieves optimal recommendation diversity and accuracy. Our code is available at https://github.com/wzhwzhwzh0921/DECREC .},
  archive      = {J_KIS},
  author       = {Wang, Zihan and Feng, Shi and Wang, Daling and Song, Kaisong and Wu, Gang and Zhang, Yifei and Zhao, Han and Yu, Ge},
  doi          = {10.1007/s10115-025-02455-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7727-7755},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Diversity-enhanced conversational recommendation via multi-agent reinforcement learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal associated learning with spatial–temporal attention for hot topic detection. <em>KIS</em>, <em>67</em>(9), 7699-7726. (<a href='https://doi.org/10.1007/s10115-025-02444-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosion in the number of web videos, it has become a common practice to detect hot topics with web videos. However, each video clip contains multiple patterns, in which object actions might only appear in specific spatial areas or specific time periods, posing a huge challenge for web video hot topic detection. Fortunately, visual information during a specific time period and area will significantly enhance the rapid capture of key information, which is particularly important for detecting hot topics. Therefore, we propose a cross-modal associated learning method with spatial–temporal attention. It can automatically select discriminative time segments to detect hot topics by focusing on spatial regions with rich information. Firstly, after focusing on important keyframes related to the topic through temporal attention, spatial attention emphasizes the salient regions in the frame, thus incorporating discriminative features at the spatial level. Secondly, after integrating text structure knowledge into text semantic features, it can adaptively learn the weights of text features and visual features. Thirdly, adaptive learning of cross-modal fusion weights, achieving mutual guidance between text and visual information in attention dispersed association models to enhance feature learning. Finally, under the constraint of contrast loss, hot topics are detected with the similarity between features. Extensive experiments conducted on web videos from YouTube indicate that our method outperforms 8 leading state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Zhang, Chengde and Liu, Shiyu and Li, Xinyu and Xiao, Xia},
  doi          = {10.1007/s10115-025-02444-z},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7699-7726},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cross-modal associated learning with spatial–temporal attention for hot topic detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chronological squirrel search algorithm enabled deep recurrent neural network for employability prediction. <em>KIS</em>, <em>67</em>(9), 7669-7698. (<a href='https://doi.org/10.1007/s10115-025-02437-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scholars are still debating the idea of employability, and no agreement has been reached as of yet. A solid theoretical foundation is still not established with the empirical investigation. The ability of the graduate to find fulfilling employment emphasizes the satisfaction that the graduate finds in their job. Therefore, employability prediction models are crucial in assessing a student's ability to find employment. This study aims to create a hybrid optimization-enhanced deep learning model for predicting employability. For that, primarily, input data is given to the pre-processing phase, where quantile normalization is used. Then, feature fusion is done by using Renyi entropy with Generative Adversarial Network. The employability prediction is done by utilizing a Deep Recurrent Neural Network (Deep RNN) in which its weight is trained using the proposed Chronological Squirrel Search Algorithm (CSSA). Here, CSSA is constructed by the combination of the Chronological concept and Squirrel search algorithm to optimize the predicted result. Moreover, the predicted output is noted. Furthermore, the introduced Chronological Squirrel Search Algorithm_Deep Recurrent Neural Network (CSSA_Deep RNN) compared with different algorithms illustrates better performance concerning the evaluation metrics such as Root-Mean-Square Error and Mean Square Error with a minimal error value of 0.458 and 0.210, respectively.},
  archive      = {J_KIS},
  author       = {Kamakshamma, V. and Bharati, K. F.},
  doi          = {10.1007/s10115-025-02437-y},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7669-7698},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Chronological squirrel search algorithm enabled deep recurrent neural network for employability prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SkSpO-L2TDBM: Optimized drug name recognition using a large language-based time-distributed deep learning model. <em>KIS</em>, <em>67</em>(9), 7641-7667. (<a href='https://doi.org/10.1007/s10115-025-02409-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poor handwriting of doctors in medical prescriptions can cause misunderstanding, misreading, misinterpretation risks and cause medical errors. Despite being rare, identifying the correct drugs with the unclear prescription seriously affects patients and also requires a quite lot of time, attention, and effort for recognition. As a result, a drug name recognition system is implemented, but previously developed models are not significant in terms of accuracy, interpretability, and reliability. Therefore, the skill split optimization enabled large language-based time-distributed bidirectional long short-term memory (SkSpO-L2TDBM) model for drug name recognition is proposed in this research. The SkSpO-L2TDBM model exploits deep features concerning bidirectional encoder representations from transformers and bidirectional long short-term memory techniques that are employed to increase the model’s reliability and interpretability for effective recognition. Moreover, the SkSpO algorithm tunes the hyper-parameters of the proposed model based on the effect of skillful learning and sharing ability that makes easy recognition with maximum convergence speed. The major advantages of the proposed model are simplicity, robustness, and endue complex computation for accurate recognition. Compared to other existing techniques, the SkSpO-L2TDBM model achieved a minimal mean squared error rate of 4.46, and minimal root mean squared error rate of 2.11 using the hybrid dataset comprising the ‘Handwritten Medical Prescriptions Collection’ dataset, and a proprietary set of handwritten medical prescriptions collected from various doctors across the cities such as Nagpur, Pune in Maharashtra, India. Moreover, the proposed approach is robust in recognizing the biomedical entities in the text.},
  archive      = {J_KIS},
  author       = {Nair, Sruthi and Sahare, Parul and Peshwe, Paritosh},
  doi          = {10.1007/s10115-025-02409-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7641-7667},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SkSpO-L2TDBM: Optimized drug name recognition using a large language-based time-distributed deep learning model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive hierarchical knowledge distillation from GNNs to MLPs. <em>KIS</em>, <em>67</em>(9), 7619-7639. (<a href='https://doi.org/10.1007/s10115-025-02447-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation has attracted much attention in knowledge and information systems. The latest distillation methods use Kullback–Leibler divergence to distill complex models from simple models, yielding surprising results. However, due to the difference in the capacity of graph neural networks (GNNs) and MLPs models, distilling GNNs through fixed dimensions and layers of MLPs will inevitably cause information loss. In addition, the gap between the features produced by GNNs and the features produced by MLPs gradually becomes larger as the number of layers increases. To this end, we propose an adaptive hierarchical distillation framework that distills GNNs through MLPs with variable dimensions and layers to ensure the integrity of the distilled information. Specifically, we use a neural architecture search to adaptively find MLPs with appropriate dimensions and layers for each layer of GNNs. Then, the graph structure information is distilled from GNNs layer by layer, which makes the student neural network structure can better match with the teacher model, and the features are better aligned, so as to better learn the graph structure information. At last, the student model distilled to the complete information is used in the downstream learning task. Experimental results on various datasets show impressive improvements in node classification tasks compared with previous state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Zhang, Junfeng and Xie, Cheng and Yu, BeiBei and Yang, Rui},
  doi          = {10.1007/s10115-025-02447-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7619-7639},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adaptive hierarchical knowledge distillation from GNNs to MLPs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conflict-aware influence maximization on hostile-labeled social networks. <em>KIS</em>, <em>67</em>(9), 7597-7618. (<a href='https://doi.org/10.1007/s10115-025-02466-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) is a well-known problem in social network analysis, which aims to identify a strategic set of seeds to maximize the influence propagation. However, a significant downside has emerged: influence propagation can unintentionally activate previously isolated but mutually hostile nodes in cyberspace, increasing the likelihood of online conflicts. To mitigate this issue, we propose the Conflict-Aware Influence Maximization (CAIM) problem, which seeks to maximize influence while minimizing the activation of mutually hostile nodes on hostile-labeled social networks. We demonstrate that CAIM remains NP-hard and #P-hard, making it a complex non-submodular optimization problem. To overcome this challenge, we propose an efficient estimation method for the objective function and design a convergent algorithm with a data-driven approximation of $$z_\lambda ^+ / b^+$$ , where the parameter computations are intricately connected to the solution. Experiments on real-world networks demonstrate that our algorithms outperform multiple baselines in effectively preventing conflicts while maximizing influence.},
  archive      = {J_KIS},
  author       = {Rao, Guoyao and Li, Deying and Zhu, Yuqing},
  doi          = {10.1007/s10115-025-02466-7},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7597-7618},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Conflict-aware influence maximization on hostile-labeled social networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding user satisfaction: Explainable artificial intelligence-based user-centric analysis of mobile health applications adoption. <em>KIS</em>, <em>67</em>(9), 7563-7596. (<a href='https://doi.org/10.1007/s10115-025-02451-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile health applications (mHealth) have revolutionized healthcare sector by leveraging mobile technology to provide personalized services. As a rapidly growing industry, mHealth aligns with the World Health Organization’s goal of empowering patients to take control of their healthcare journey. In the realm of mHealth, ensuring user satisfaction always remains a key concern. Therefore, recognizing crucial factors that determine customer satisfaction levels can help mHealth applications improve their quality of service. Leveraging machine learning techniques for this task can prove to be highly beneficial. However, the existing machine learning methods in this domain are ‘black-box’ and possess several limitations, such as less accuracy, lack of explainability, and many more. To resolve these challenges, the current research introduces a novel approach based on deep transformers and explainable artificial intelligence (EAI). This approach aims to analyze user-generated content to determine mHealth ratings and the factors influencing user satisfaction. The proposed pipeline encompasses several steps, namely, data preprocessing and anonymization, feature extraction, feature selection, transformer architecture building, and evaluating performance using a dataset containing reviews of several different mHealth applications. The sensitivity analysis of the proposed approach is performed by utilizing several feature selection techniques and comparing the prediction performance with existing benchmark solutions available in the literature. From the comparative evaluation, it is observed that the proposed approach outperforms existing techniques by providing 98% accuracy and 99% F1-score, with a 3–5% relative improvement over benchmark solutions. In addition, the proposed method incorporates EAI to determine several crucial factors that affect user satisfaction or app rating scores. This information will be beneficial for the stakeholders in devising better platforms and strategies for enhancing user satisfaction and experience in the mHealth domain.},
  archive      = {J_KIS},
  author       = {Rai, Stuti and Bedi, Jatin and Anand, Ashima},
  doi          = {10.1007/s10115-025-02451-0},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7563-7596},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Decoding user satisfaction: Explainable artificial intelligence-based user-centric analysis of mobile health applications adoption},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature disentanglement, selection, and reaggregation method for multi-task learning. <em>KIS</em>, <em>67</em>(9), 7537-7562. (<a href='https://doi.org/10.1007/s10115-025-02460-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning utilizes the dependencies among tasks to make the tasks boost each other and achieve better results. However, there are not only dependencies but also conflicts among tasks. Task conflict is a critical issue that needs to be addressed to avoid mutual interference among tasks. Previous studies focus on weighting each task to intervene between two or more tasks to reach a compromise. The issue of dependency and conflict among tasks is more complex, where there may be both dependencies and conflicts between two tasks. Simply adjusting weights cannot effectively handle such complex relationships and achieve better results. This paper analyzes the reasons for the task conflicts and discovers that different requirements exist for features in different tasks. For a single task, the features required for other tasks may interfere with the decoder. A new method named feature disentanglement, selection, and reaggregation method is proposed based on the reason for task conflict, which is to disentangle encoder output to obtain high-level features and then select and aggregate high-level features according to the requirements of the task. Experiments show that our method achieves state-of-the-art results on the Multi-Domain Sentiment dataset and 20 Newsgroups dataset. The results prove that our method effectively alleviates conflicts among tasks.},
  archive      = {J_KIS},
  author       = {Liu, Renyuan and Zhang, Xuejie and Wang, Jin and Zhou, Xiaobing},
  doi          = {10.1007/s10115-025-02460-z},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7537-7562},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature disentanglement, selection, and reaggregation method for multi-task learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A priority-based blockchain transaction packaging algorithm in a cloud-edge-end collaboration computing environment. <em>KIS</em>, <em>67</em>(9), 7503-7536. (<a href='https://doi.org/10.1007/s10115-025-02432-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile internet, intelligent IoT, and 5G communication technologies, many IoT devices connect to the Industrial Internet of Things, generating significant data. Blockchain is widely used in identity authentication and trust management due to its reliability. In blockchain technology, transaction packaging is a crucial component of the consensus mechanism and is critical to enhancing fairness and service quality in request processing. However, the flat structure of the blockchain, the necessity for multi-party consensus, and the profit-seeking nature of nodes lead to issues such as unfair transaction processing and prolonged response times in cloud-edge-end architectures, which are critical for empowering intelligent edge applications. To address these challenges, we have refined the blockchain consensus mechanism and introduced a novel packaging algorithm, ITFPA (Improved time-fee packaging algorithm), within the cloud-edge-end environment. This algorithm models the transaction packaging problem as a 0–1 knapsack problem and employs a branch-and-bound method to find the optimal solution. The proposed model considers both the transaction waiting time and transaction fee, using the weighted result of these factors as the priority for the transaction. We compared the proposed algorithm with the WaitTime and TxFee algorithms across four metrics: system fairness, transaction response time, and block priority. The experimental results demonstrate that the proposed algorithm enhances system fairness, reduces transaction response times, and improves service quality to a significant degree.},
  archive      = {J_KIS},
  author       = {Li, Chunlin and Zeng, Haibo and Jiang, Kun and Yang, Kaijun and Yang, Shaohua and Jia, Qingren},
  doi          = {10.1007/s10115-025-02432-3},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7503-7536},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A priority-based blockchain transaction packaging algorithm in a cloud-edge-end collaboration computing environment},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain feature transfer-based multi-domain fake news detection. <em>KIS</em>, <em>67</em>(9), 7473-7501. (<a href='https://doi.org/10.1007/s10115-025-02412-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news is spreading rapidly throughout social media, and is having serious negative consequences on both individuals and society. Currently, fake news detection methods often only predict news for a single domain, neglecting the domain information contained within the text. This may result in an inability to make effective predictions in domains where there is a low quantity or quality of data. With the aid of multi-task learning and transfer learning concepts, this paper presents a domain feature transfer-based multi-domain fake news detection (DFTD). First, we construct a multi-task feature extractor to obtain news text features in different domains. Then, we build an implicit domain gatherer to mine hidden domain information in the news. Next, the domain feature transferor is combined to obtain cross-domain text features. Finally, these features are inputted into the fake news detector for prediction. Our model maintains the extensive association information between domains while segmenting them. Additionally, it employs features from several source domains to aid in determining the authenticity of news in the target domain. Relevant experiments conducted on Weibo21 provide proof of the effectiveness of this model.},
  archive      = {J_KIS},
  author       = {Meng, Xuan and Zhao, Di and Meng, Jiana and Guo, Xu and Ma, Tengfei and Wang, Xiaopei},
  doi          = {10.1007/s10115-025-02412-7},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7473-7501},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Domain feature transfer-based multi-domain fake news detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XLR-KGDD: Leveraging LLM and RAG for knowledge graph-based explainable disease diagnosis using multimodal clinical information. <em>KIS</em>, <em>67</em>(9), 7451-7471. (<a href='https://doi.org/10.1007/s10115-025-02465-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are making a big impact in Artificial Intelligence due to their ability to perform tasks as humans. However, using LLMs in many domain-specific tasks is a relatively unexplored area, specifically in disease diagnosis. This is due to challenges such as multiple modalities in patient clinical information and LLM's high memory and computational power requirements. This study proposes a framework, XLR-KGDD, that overcomes these challenges and performs an LLM-based disease diagnosis. Additionally, XLR-KGDD generates explanations to establish the trust of clinicians and support the diagnosis. These explanations are precise and unambiguous as they adhere to the standard medical guidelines and are presented in natural language. The proposed framework maps multimodal patient clinical information to a patient Knowledge Graph (KG) using the N2K mapper and CheXzero. Prompt Engineering is then applied to create an LLM-compatible input prompt from the patient KG. The framework employs a Parameter Efficient Fine-tuning technique to fine-tune LLM efficiently by optimizing numerical computations and memory requirements. The framework uses Retrieval Augmented Generation to provide standard medical guidelines as context to the LLM, addressing the issue of hallucinations in LLMs and generating coherent explanations. A system based on the XLR-KGDD framework was developed and tested on the multimodal MIMIC-Eye dataset. The LLaMA-3 LLM shows an AUC value of 0.88 and 0.91 in ROC and PR curves, respectively, in diagnosing patients with CHF disease. Furthermore, the system-generated explanations support the diagnosis with evidence from medical guidelines.},
  archive      = {J_KIS},
  author       = {Bedi, Punam and Thukral, Anjali and Dhiman, Shivani},
  doi          = {10.1007/s10115-025-02465-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7451-7471},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {XLR-KGDD: Leveraging LLM and RAG for knowledge graph-based explainable disease diagnosis using multimodal clinical information},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic token pruning for LLMs: Leveraging task-specific attention and adaptive thresholds. <em>KIS</em>, <em>67</em>(9), 7431-7450. (<a href='https://doi.org/10.1007/s10115-025-02450-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have achieved state-of-the-art performance across a wide range of natural language processing (NLP) tasks. With their high inference computational costs, deployment is extremely challenging, especially to resource-constrained environments. Dynamic pruning methods, because they are efficient, are likely to assign uniform policies to all tasks and even forget task-specific knowledge and make optimal behavior complicated. To counter this limitation, we propose task-specific dynamic token pruning (TS-DTP), a novel optimization framework that reaches maximum efficiency for LLMs at inference without compromising task-specific performance and, in certain cases, improving upon it. TS-DTP utilizes task-specific knowledge to regulate the token selection process by applying task-specific attention weights and adaptive threshold learning. This approach enables better token importance decision-making through the dynamically adjustable pruning policy according to the downstream task need. It enables very high-grained control, keeping the meaningful contextual information, therefore promoting better performance compared to regular pruning methods. Experimental findings on a variety of NLP tasks (question answering, machine translation, sentiment analysis) validate that TS-DTP achieves extremely large reductions in computational expense and memory demands and similar or marginal gains in accuracy. Our findings are at the forefront of efficient and deployable LLM development and highlight the importance of task adaptation for optimal performance in low-resource settings.},
  archive      = {J_KIS},
  author       = {Ahmadpanah, Seyed Hossein and Sobhanloo, Sanaz and Afsharfarnia, Pania},
  doi          = {10.1007/s10115-025-02450-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7431-7450},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamic token pruning for LLMs: Leveraging task-specific attention and adaptive thresholds},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based advances in sarcasm detection: A study of contextual models and methodologies. <em>KIS</em>, <em>67</em>(9), 7399-7430. (<a href='https://doi.org/10.1007/s10115-025-02469-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm detection is a subset of sentiment analysis and poses significant challenges due to its inherent linguistic complexity and the contextual subtleties required for accurate interpretation. In this paper, we present a comprehensive survey of the current state of sarcasm detection, covering models from traditional machine learning to large language models. Our review examines primary datasets and corpora used for training and evaluation, evaluates the effectiveness of existing sentiment analysis techniques, and highlights predominant methodologies. Additionally, we discuss the challenges and limitations facing sarcasm detection, including issues related to linguistic complexity and contextual interpretation. We compared all datasets, and how they impact the model performance and generalizability, assess the ability of sentiment analysis techniques to capture sarcasm and irony, and explore the strengths and limitations of leading methodologies. By exploring current limitations, including cross-cultural variances and the adaptability of deep learning models, this survey underscores ongoing challenges and highlights future directions in AI-driven sarcasm detection research.},
  archive      = {J_KIS},
  author       = {Bodige, Ramakrishna and Akarapu, Rameshbabu and Poladi, Pramod Kumar},
  doi          = {10.1007/s10115-025-02469-4},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7399-7430},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Transformer-based advances in sarcasm detection: A study of contextual models and methodologies},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative analysis of predictive process monitoring: Object-centric versus classical event logs. <em>KIS</em>, <em>67</em>(9), 7355-7398. (<a href='https://doi.org/10.1007/s10115-025-02461-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive Process Monitoring (PPM) techniques are emerging as part of the more general research scenario of Process Mining (PM). They play a crucial role in the continuously evolving process of digital transformation by constantly supporting the organizational decision-making processes providing (accurate) predictions on the future behavior of processes. The state of the art of PPM application methodologies is mainly focused on Single ID Event Logs, commonly known as Traditional Event Logs or Classical Event Logs. As a matter of fact, the importance of Object-Centric Event Logs (OCEL) is being increasingly recognized as many emerging PPM approaches benefited of the usage of OCEL by obtaining a significative increase of the prediction accuracy. This survey aims to explore the current proposals in the context of OCEL-based PPM approaches. More in detail, we contribute to the state of the art by adding new classification features by differentiating between the approaches based on the input Event Log (Traditional or OCEL). We also analyzed the existing literature considering the prediction task addressed, the methodology used, the specific contribution area they addressed and the application domain.},
  archive      = {J_KIS},
  author       = {Fioretto, Simona and Masciari, Elio},
  doi          = {10.1007/s10115-025-02461-y},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7355-7398},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A comparative analysis of predictive process monitoring: Object-centric versus classical event logs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chatbot development strategies: A review of current studies and applications. <em>KIS</em>, <em>67</em>(9), 7319-7354. (<a href='https://doi.org/10.1007/s10115-025-02462-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chatbots have become increasingly popular by transforming interactions across numerous fields. As the technology behind chatbots has rapidly developed, new methodologies have arisen, each contributing unique strengths and addressing different challenges. This paper systematically reviews the methods used in chatbot development from 2019 to 2024, comprehensively analyzing the studies. We categorize the techniques into three main groups: machine learning (ML)-based, deep learning (DL)-based, and large language model (LLM)-based methods. We present a broad and inclusive survey by exploring the foundational principles of chatbot technologies and their applications across diverse domains such as education, healthcare, and interviews. Our analysis reveals that while traditional ML-based methods remain widely used, DL models are gaining prominence for handling complex tasks, and LLM-based systems are advancing the field by offering more coherent, contextually aware responses. However, challenges remain, especially in ethical concerns like hallucination and privacy-preserving technologies, particularly with LLMs. The paper also identifies gaps in existing research, notably the need for improved privacy-preserving mechanisms and better strategies for mitigating hallucinations in chatbot responses. Future research directions are suggested to address these challenges, particularly in developing LLM-based chatbots, with a focus on enhancing privacy, accuracy, and ethical standards.},
  archive      = {J_KIS},
  author       = {Yigit, Gulsum and Bayraktar, Rabia},
  doi          = {10.1007/s10115-025-02462-x},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7319-7354},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Chatbot development strategies: A review of current studies and applications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="mam">MAM - 1</h2>
<ul>
<li><details>
<summary>
(2025). Designing with uncertainty: LLM interfaces as transitional spaces for democratic revival. <em>MAM</em>, <em>35</em>(4), 1-23. (<a href='https://doi.org/10.1007/s11023-025-09736-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of large language models (LLMs) into our conversational infrastructure presents a critical inflection point for democratic practice. While contemporary digital platforms systematically erode transitional conversational spaces—interfaces between private intuition and public deliberation where tentative thoughts can be explored—this paper argues that specialized LLM interfaces could potentially reconstruct these essential democratic environments. I propose a design framework for ‘transitional conversational spaces’ that leverages uncertainty expression not merely to prevent unwarranted epistemic confidence but to create communicative environments conducive to democratic capability development. Drawing on theories of democratic deliberation and moral perception, this paper distinguishes between epistemic uncertainty (addressable through additional information) and hermeneutic uncertainty (concerning the inherently contestable nature of interpretation). The proposed framework emphasizes ‘ensemble interfaces’ that make visible the contingent nature of value judgments by presenting outputs from multiple models trained on different datasets. The design principles outlined challenge tokenistic participation by advocating for substantive participatory infrastructure with features like ‘tinkerability’—enabling communities to experiment with system configurations—and mechanisms that counter designer-centric development models. These principles stand in contrast to conventional ‘participatory AI’ approaches that treat engagement as merely instrumental to system optimization rather than as constitutive of democratic practice. This paper does not claim to solve all challenges of democratic participation but rather identifies one valuable design direction that could potentially enhance our collective capacity for exploratory dialogue. Implementation would require institutional transformations that align technological development with democratic values beyond current procedural approaches to AI governance.},
  archive      = {J_MAM},
  author       = {Delacroix, Sylvie},
  doi          = {10.1007/s11023-025-09736-x},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Minds Mach.},
  title        = {Designing with uncertainty: LLM interfaces as transitional spaces for democratic revival},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="mcap">MCAP - 3</h2>
<ul>
<li><details>
<summary>
(2025). A latent variable approach to the analysis of progressively hybrid censored masked data. <em>MCAP</em>, <em>27</em>(4), 1-28. (<a href='https://doi.org/10.1007/s11009-025-10197-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we are dealing with two important issues that arise in the competing risks analysis of series system lifetime data. First, we deal with incomplete lifetimes of the system’s components, observed under a Type-I progressive hybrid censoring scheme. Second, we examine situations where the exact cause of failure of any system is unknown. To address these issues, we develop models incorporating cause dependent and time dependent masking probabilities. The Maxwell distribution is considered as the lifetime model for components, and parameter estimation is performed using maximum likelihood and Bayesian approaches. In a simulation study, the derived methodology is explored for varying sample sizes and different censoring patterns under cause and time dependent masking mechanisms. For real-life illustration, the data set of 10,000 hard drives is analyzed. To choose a better model in the presence of various masking options, the predictive power and deviance information criterion are also explored.},
  archive      = {J_MCAP},
  author       = {Tomer, Sanjeev K and Panwar, M S and Rai, Himanshu},
  doi          = {10.1007/s11009-025-10197-z},
  journal      = {Methodology and Computing in Applied Probability},
  month        = {12},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Meth. Comput. Appl. Prob.},
  title        = {A latent variable approach to the analysis of progressively hybrid censored masked data},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perpetual american compound fixed-strike lookback options on maxima drawdowns. <em>MCAP</em>, <em>27</em>(4), 1-33. (<a href='https://doi.org/10.1007/s11009-025-10199-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present closed-form solutions to the problem of pricing of the perpetual American compound lookback options on the maximum drawdown with fixed strikes in the Black-Merton-Scholes model. It is shown that the optimal exercise times are the first times at which the underlying risky asset price process reaches either lower or upper stochastic boundaries depending on the current values of its running maximum and maximum drawdown processes. The proof is based on the reduction of the original double optimal stopping problem to a sequence of two single optimal stopping problems for the resulting three-dimensional continuous Markov process. The latter problems are solved as the equivalent free-boundary problems by means of the smooth-fit and normal-reflection conditions for the value functions at the optimal stopping boundaries and at the edges of the three-dimensional state space. We show that the optimal exercise boundaries are determined as the maximal and minimal solutions to the appropriate first-order nonlinear ordinary differential equations.},
  archive      = {J_MCAP},
  author       = {Gapeev, Pavel V.},
  doi          = {10.1007/s11009-025-10199-x},
  journal      = {Methodology and Computing in Applied Probability},
  month        = {12},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Meth. Comput. Appl. Prob.},
  title        = {Perpetual american compound fixed-strike lookback options on maxima drawdowns},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The marcinkiewicz laws for weighted sums of heavy-tailed random variables and applications to the value-at-risk estimators and semiparametric regression models. <em>MCAP</em>, <em>27</em>(4), 1-32. (<a href='https://doi.org/10.1007/s11009-025-10204-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, based on the theory of regularly varying functions we investigate the general Marcinkiewicz laws of large numbers for weighted sums of negatively associated random variables with heavy-tail. As applications of our main results, we study the consistency for conditional Value-at-Risk estimator with heavy-tailed samples as well as the consistency for the weighted estimator in a semiparametric regression model based on heavy-tailed errors.},
  archive      = {J_MCAP},
  author       = {Son, Ta Cong and Dung, Le Van and Hang, Bui Khanh and Cuong, Tran Manh},
  doi          = {10.1007/s11009-025-10204-3},
  journal      = {Methodology and Computing in Applied Probability},
  month        = {12},
  number       = {4},
  pages        = {1-32},
  shortjournal = {Meth. Comput. Appl. Prob.},
  title        = {The marcinkiewicz laws for weighted sums of heavy-tailed random variables and applications to the value-at-risk estimators and semiparametric regression models},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="meco">MECO - 2</h2>
<ul>
<li><details>
<summary>
(2025). Hierarchical prompt fusion and image denoising for multimodal aspect-based sentiment analysis. <em>MECO</em>, <em>17</em>(4), 1-12. (<a href='https://doi.org/10.1007/s12293-025-00475-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Aspect-Based Sentiment Analysis (MABSA) is a fine-grained task that aims to analyze users’ sentiment polarity towards target aspects through different and rich modal contents. In conjunction with this topic, many methods have been proposed to link modalities to form interactive judgments of emotional tendencies. However, the currently proposed methods have some disadvantages: (1) Since some image modalities are unrelated to text modalities, information irrelevant to the aspect will be introduced; (2) Different modalities are difficult to complement each other during the fusion process, resulting in poor fusion performance, and the fusion process may also introduce additional noise. To address these issues, we propose a novel MABSA network model that combines a simple noise filtering approach with an innovative fast learning method for effective classification. Specifically, for the visual modality, we introduce an image content filtering (ICF) layer to filter out information that is not relevant to the aspect and text modalities. In addition, in the fusion stage, we proposed an excellent bidirectional interactive prompt fusion (BIPF) layer, which integrates prompt learning into the query and fusion stages, realizes the fusion of different modalities and contextual semantic information, and makes modal fusion more comprehensive. The outcomes of our experiments indicate that the model we developed attains leading-edge performance levels when tested on two MABSA datasets. Furthermore, a large number of experiments have shown that the model we put forward exhibits remarkable performance and robustness.},
  archive      = {J_MECO},
  author       = {Fuxian, Zhu and Xiaoli, Xu},
  doi          = {10.1007/s12293-025-00475-1},
  journal      = {Memetic Computing},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Memet. Comput.},
  title        = {Hierarchical prompt fusion and image denoising for multimodal aspect-based sentiment analysis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parallel cooperative coevolutionary multi-verse algorithm for large-scale multi-objective UAV path planning problems. <em>MECO</em>, <em>17</em>(4), 1-32. (<a href='https://doi.org/10.1007/s12293-025-00473-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning of Unmanned Aerial Vehicles (UAVs) in complex environments with high dimensionality attempts to search for waypoint sequences always of increased size. Such a challenging task can be considered as a multi-objective Large-Scale Global Optimization (LSGO) problem where efficient solving requires more sophisticated algorithms. In this paper, a novel Parallel Cooperative Coevolutionary Multi-Objective Multi-Verse Optimization (PCCMOMVO) algorithm is developed and successfully applied. In this Cooperative Coevolutionary (CC) framework, a MOMVO algorithm is considered to design an improved subcomponent optimizer based on an allocated multi-core CPU architecture and a Message Passing Interface (MPI). The MOMVO population is divided into sub-populations, called species, where each of them is responsible for optimizing a subcomponent of the LSGO problem according to the “divide-and-conquer” concept of CC framework. To form a complete candidate solution of the whole LSGO path planning problem, each species shares a number of representative solutions selected from the Pareto non-dominated ones found so far. The selection process is based on the Pareto ranks achieved by the use of a Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS). A parallelization mechanism based on a simple yet efficient master–slave model and allocated MPI is implemented to provide acceleration in computation runtime. Demonstrative results and ANOVA tests are presented over planning scenarios with increased complexity to demonstrate the superiority and effectiveness of the PCCMOMVO algorithm. Extensive evaluations and comparisons are carried out in terms of collision-avoidance capabilities, path shortness and efficiency against the curse of dimensionality and computational time consumption.},
  archive      = {J_MECO},
  author       = {Jarray, Raja and Bouallègue, Soufiene},
  doi          = {10.1007/s12293-025-00473-3},
  journal      = {Memetic Computing},
  month        = {12},
  number       = {4},
  pages        = {1-32},
  shortjournal = {Memet. Comput.},
  title        = {A parallel cooperative coevolutionary multi-verse algorithm for large-scale multi-objective UAV path planning problems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="mir">MIR - 13</h2>
<ul>
<li><details>
<summary>
(2025). Guided proximal policy optimization with structured action graph for complex decision-making. <em>MIR</em>, <em>22</em>(4), 797-816. (<a href='https://doi.org/10.1007/s11633-024-1503-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning encounters formidable challenges when tasked with intricate decision-making scenarios, primarily due to the expansive parameterized action spaces and the vastness of the corresponding policy landscapes. To surmount these difficulties, we devise a practical structured action graph model augmented by guiding policies that integrate trust region constraints. Based on this, we propose guided proximal policy optimization with structured action graph (GPPO-SAG), which has demonstrated pronounced efficacy in refining policy learning and enhancing performance across sophisticated tasks characterized by parameterized action spaces. Rigorous empirical evaluations of our model have been performed on comprehensive gaming platforms, including the entire suite of StarCraft II and Hearthstone, yielding exceptionally favorable outcomes. Our source code is at https://github.com/sachiel321/GPPO-SAG .},
  archive      = {J_MIR},
  author       = {Yang, Yiming and Xing, Dengpeng and Xia, Wannian and Wang, Peng},
  doi          = {10.1007/s11633-024-1503-7},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {797-816},
  shortjournal = {Mach. Intell. Res.},
  title        = {Guided proximal policy optimization with structured action graph for complex decision-making},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaGPAR: Generalizable pedestrian attribute recognition via test-time adaptation. <em>MIR</em>, <em>22</em>(4), 783-796. (<a href='https://doi.org/10.1007/s11633-024-1504-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizable pedestrian attribute recognition (PAR) aims to learn a robust PAR model that can be directly adapted to unknown distributions under varying illumination, different viewpoints and occlusions, which is an essential problem for real-world applications, such as video surveillance and fashion search. In practice, when a trained PAR model is deployed to real-world scenarios, the unseen target samples are fed into the model continuously in an online manner. Therefore, this paper proposes an efficient and flexible method, named AdaGPAR, for generalizable PAR (GPAR) via test-time adaptation (TTA), where we adapt the trained model through exploiting the unlabeled target samples online during the test phase. As far as we know, it is the first work that solves the GPAR from the perspective of TTA. In particular, the proposed AdaGPAR memorizes the reliable target sample pairs (features and pseudo-labels) as prototypes gradually in the test phase. Then, it makes predictions with a non-parametric classifier by calculating the similarity between a target instance and the prototypes. However, since PAR is a task of multi-label classification, only using the same holistic feature of one pedestrian image as the prototypes of multiple attributes is not optimal. Therefore, an attribute localization branch is introduced to extract the attribute-specific features, where two kinds of memory banks are further constructed to cache the global and attribute-specific features simultaneously. In summary, the AdaGPAR is training-free in the test phase and predicts multiple pedestrian attributes of the target samples in an online manner. This makes the AdaGPAR time efficient and generalizable for real-world applications. Extensive experiments have been performed on the UPAR benchmark to compare the proposed method with multiple baselines. The superior performance demonstrates the effectiveness of the proposed AdaGPAR that improves the generalizability of a PAR model via TTA.},
  archive      = {J_MIR},
  author       = {Li, Da and Zhang, Zhang and Zhang, Yifan and Jia, Zhen and Shan, Caifeng},
  doi          = {10.1007/s11633-024-1504-6},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {783-796},
  shortjournal = {Mach. Intell. Res.},
  title        = {AdaGPAR: Generalizable pedestrian attribute recognition via test-time adaptation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AICAttack: Adversarial image captioning attack with attention-based optimization. <em>MIR</em>, <em>22</em>(4), 769-782. (<a href='https://doi.org/10.1007/s11633-024-1535-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models’ robustness against adversarial attacks has not been well studied. This paper presents a novel adversarial attack strategy, attention-based image captioning attack (AICAttack), designed to attack image captioning models through subtle perturbations to images. Operating within a black-box attack scenario, our algorithm requires no access to the target model’s architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels for attack, followed by a customized differential evolution method to optimize the perturbations of the pixels’ RGB values. We demonstrate AICAttack’s effectiveness through extensive experiments on benchmark datasets against multiple victim models. The experimental results demonstrate that our method outperforms current leading-edge techniques by achieving consistently higher attack success rates.},
  archive      = {J_MIR},
  author       = {Li, Jiyao and Ni, Mingze and Dong, Yifei and Zhu, Tianqing and Gong, Yongshun and Liu, Wei},
  doi          = {10.1007/s11633-024-1535-z},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {769-782},
  shortjournal = {Mach. Intell. Res.},
  title        = {AICAttack: Adversarial image captioning attack with attention-based optimization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMSL: Cross-modal style learning for few-shot image generation. <em>MIR</em>, <em>22</em>(4), 752-768. (<a href='https://doi.org/10.1007/s11633-024-1511-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training generative adversarial networks is data-demanding, which limits the development of these models on target domains with inadequate training data. Recently, researchers have leveraged generative models pretrained on sufficient data and fine-tuned them using small training samples, thus reducing data requirements. However, due to the lack of explicit focus on target styles and disproportionately concentrating on generative consistency, these methods do not perform well in diversity preservation which represents the adaptation ability for few-shot generative models. To mitigate the diversity degradation, we propose a framework with two key strategies: 1) To obtain more diverse styles from limited training data effectively, we propose a cross-modal module that explicitly obtains the target styles with a style prototype space and text-guided style instructions. 2) To inherit the generation capability from the pretrained model, we aim to constrain the similarity between the generated and source images with a structural discrepancy alignment module by maintaining the structure correlation in multiscale areas. We demonstrate the effectiveness of our method, which outperforms state-of-the-art methods in mitigating diversity degradation through extensive experiments and analyses.},
  archive      = {J_MIR},
  author       = {Jiang, Yue and Lyu, Yueming and Peng, Bo and Wang, Wei and Dong, Jing},
  doi          = {10.1007/s11633-024-1511-7},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {752-768},
  shortjournal = {Mach. Intell. Res.},
  title        = {CMSL: Cross-modal style learning for few-shot image generation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPM-solver++: Fast solver for guided sampling of diffusion probabilistic models. <em>MIR</em>, <em>22</em>(4), 730-751. (<a href='https://doi.org/10.1007/s11633-025-1562-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion probabilistic models (DPMs) have achieved impressive success in high-resolution image synthesis, especially in recent large-scale text-to-image generation applications. An essential technique for improving the sample quality of DPMs is guided sampling, which usually needs a large guidance scale to obtain the best sample quality. The commonly-used fast sampler for guided sampling is denoising diffusion implicit models (DDIM), a first-order diffusion ordinary differential equation (ODE) solver that generally needs 100 to 250 steps for high-quality samples. Although recent works propose dedicated high-order solvers and achieve a further speedup for sampling without guidance, their effectiveness for guided sampling has not been well-tested before. In this work, we demonstrate that previous high-order fast samplers suffer from instability issues, and they even become slower than DDIM when the guidance scale grows larger. To further speed up guided sampling, we propose DPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++ solves the diffusion ODE with the data prediction model and adopts thresholding methods to keep the solution matches training data distribution. We further propose a multistep variant of DPM-Solver++ to address the instability issue by reducing the effective step size. Experiments show that DPM-Solver++ can generate high-quality samples within only 15 to 20 steps for guided sampling by pixel-space and latent-space DPMs.},
  archive      = {J_MIR},
  author       = {Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  doi          = {10.1007/s11633-025-1562-4},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {730-751},
  shortjournal = {Mach. Intell. Res.},
  title        = {DPM-solver++: Fast solver for guided sampling of diffusion probabilistic models},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal pretrained knowledge for real-world object navigation. <em>MIR</em>, <em>22</em>(4), 713-729. (<a href='https://doi.org/10.1007/s11633-024-1537-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most visual-language navigation (VLN) research focuses on simulate environments, but applying these methods to real-world scenarios is challenging because of misalignments between vision and language in complex environments, leading to path deviations. To address this, we propose a novel vision-and-language object navigation strategy that uses multimodal pretrained knowledge as a cross-modal bridge to link semantic concepts in both images and text. This improves navigation supervision at key-points and enhances robustness. Specifically, we 1) randomly generate key-points within a specific density range and optimize them on the basis of challenging locations; 2) use pretrained multimodal knowledge to efficiently retrieve target objects; 3) combine depth information with simultaneous localization and mapping (SLAM) map data to predict optimal positions and orientations for accurate navigation; and 4) implement the method on a physical robot, successfully conducting navigation tests. Our approach achieves a maximum success rate of 66.7%, outperforming existing VLN methods in real-world environments.},
  archive      = {J_MIR},
  author       = {Yuan, Hui and Huang, Yan and Yu, Naigong and Zhang, Dongbo and Du, Zetao and Liu, Ziqi and Zhang, Kun},
  doi          = {10.1007/s11633-024-1537-x},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {713-729},
  shortjournal = {Mach. Intell. Res.},
  title        = {Multimodal pretrained knowledge for real-world object navigation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online iterative learning enhanced sim-to-real transfer for efficient manipulation of deformable objects. <em>MIR</em>, <em>22</em>(4), 696-712. (<a href='https://doi.org/10.1007/s11633-025-1566-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deformable manipulation has attracted a lot of attention in the field of robotics, especially in medical applications. However, manipulating deformable objects faces various challenges, mainly including their complex dynamic properties and unpredictable nonlinear deformations. It is difficult to provide a basis for deformable object measurements without effective control methods that provide intelligent and accurate position control, and this research also provides a premise for deformable object measurements. To address these issues, this paper proposes an online iterative perception policy (IPP) method, which does not require large-scale deep network training. This method is able to perceive transformations through an iterative process, and achieve efficient and accurate control of deformable objects. Extensive experiments in the simulation environment and the real scene are conducted to validate the effectiveness and superiority of the proposed method, as well as to compare with advanced algorithms (linear-quadratic regulator (LQR), sliding mode control (SMC), model predictive control (MPC), and heuristic). The experimental results reveal that IPP outperforms other approaches in terms of convergence, stability, robustness and flexibility in both the simulation and real-world scenarios, regardless of textile properties or initial conditions.},
  archive      = {J_MIR},
  author       = {Chen, Zuyan and Huang, Jian-An and Röning, Juha and Angrisani, Leopoldo and Li, Shuai},
  doi          = {10.1007/s11633-025-1566-0},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {696-712},
  shortjournal = {Mach. Intell. Res.},
  title        = {Online iterative learning enhanced sim-to-real transfer for efficient manipulation of deformable objects},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-like observation-inspired universal image acquisition system for complex surfaces in industrial product inspection. <em>MIR</em>, <em>22</em>(4), 677-695. (<a href='https://doi.org/10.1007/s11633-025-1561-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial surface inspection is crucial for the manufacture of high-end equipment across industries, with precise image acquisition being fundamental. Existing imaging systems often lack flexibility, they are restricted to specific objects, and face challenges in industrial structures without standardized computer-aided design (CAD) models or with complex surfaces. Inspired by human-like multidimensional observation, this study developed a universal image acquisition system based on measured point clouds, offering strong adaptability and robustness in complex industrial settings. The system is divided into three layers: the physical layer responsible for hardware integration, the interaction layer that facilitates bidirectional data exchange with the control layer, and the control layer integrating a new paradigm of multiple intelligent algorithms. The physical layer incorporates 2D and 3D cameras, turntables and industrial robots, enhancing the flexibility and compatibility of imaging. The interaction layer manages bidirectional information transmission and data exchange, offering a visualized area to enhance the user interaction experience. The control layer consists of point cloud preprocessing, primitive segmentation, viewpoint generation and pose estimation algorithms, using point cloud-based viewpoint generation and trajectory planning for high-precision image acquisition applicable to complex surface inspections across scenarios and structures. The system’s utility is demonstrated through a software and hardware algorithm platform and an interactive interface. Experimental validation on curved surfaces of different configurations and sizes confirms its universal image acquisition advantages. This system promises to introduce a cost-effective, versatile solution for complex surfaces, driving adoption across diverse industrial scenarios.},
  archive      = {J_MIR},
  author       = {Yang, Tianbo and Wang, Shaohu and Tong, Yuchuang and Zou, Menghan and Shang, Xiuqin and Ma, Wenzhi and Zhang, Zhengtao},
  doi          = {10.1007/s11633-025-1561-5},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {677-695},
  shortjournal = {Mach. Intell. Res.},
  title        = {Human-like observation-inspired universal image acquisition system for complex surfaces in industrial product inspection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised vision-driven trajectory planning for intelligent robotic deburring. <em>MIR</em>, <em>22</em>(4), 655-676. (<a href='https://doi.org/10.1007/s11633-025-1560-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent robotic manufacturing systems are revolutionizing the production industry. These next-generation systems employ robots as actuators, multi-source sensors for perception, and artificial intelligence for decision-making, aiming to execute routine manufacturing tasks with greater autonomy and flexibility. In footwear manufacturing, sole deburring presents a specific challenge in detecting defects and elaborating deburring paths, which skilled workers traditionally handle. The present research goes beyond solving such problems traditionally with computer vision and hard robot programming. Instead, it focuses on developing a learning structure mimicking human motion planning capability from vision inputs. Like humans who mentally visualize and predict a path before refining it in real-time, we want to give the robot the ability to predetermine the trajectory needed for a finishing task, exploiting only vision data. The system is designed to learn how to identify defects and directly correlate this information with motions by utilizing a latent space representation, transitioning from simple programmed responses to more adaptive and intelligent behaviors. We call it a self-supervised vision-proprioception model, an AI framework that autonomously learns to correlate visual observations to proprioceptive data (end effector trajectories) for effective task execution. This is achieved by integrating a vision-based latent space learning phase (learn to see), followed by a reinforcement learning stage, where the agent learns to associate the latent space with deburring actions in a simulated environment (learn to act). Recognizing the common performance degradation when transferring learned policies to real robots, this research also employs Sim-to-Real methods to bridge the reality gap (learn to transfer). Experimental results validate the whole approach.},
  archive      = {J_MIR},
  author       = {Tafuro, Alessandra and Molinaro, Martin and Zanchettin, Andrea Maria and Rocco, Paolo},
  doi          = {10.1007/s11633-025-1560-6},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {655-676},
  shortjournal = {Mach. Intell. Res.},
  title        = {Self-supervised vision-driven trajectory planning for intelligent robotic deburring},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPU-accelerated conflict-based search for multi-agent embodied intelligence. <em>MIR</em>, <em>22</em>(4), 641-654. (<a href='https://doi.org/10.1007/s11633-025-1568-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied intelligence applications, such as autonomous robotics and smart transportation systems, require efficient coordination of multiple agents in dynamic environments. A critical challenge in this domain is the multi-agent pathfinding (MAPF) problem, which ensures that agents can navigate conflict-free while optimizing their paths. Conflict-based search (CBS) is a well-established two-level solver for the MAPF problem. However, as the scale of the problem expands, the computation time becomes a significant challenge for the implementation of CBS. Previous optimizations have mainly focused on reducing the number of nodes explored by the high-level or low-level solver. This paper takes a different perspective by proposing a parallel version of CBS, namely GPU-accelerated conflict-based search (GACBS), which significantly exploits the parallel computing capabilities of GPU. GACBS employs a task coordination framework to enable collaboration between the high-level and low-level solvers with lightweight synchronous operations. Moreover, GACBS leverages a parallel low-level solver, called GATSA, to efficiently find the shortest path for a single agent under constraints. Experimental results show that the proposed GACBS significantly outperforms CPU-based CBS, with the maximum speedup ratio reaching over 46.},
  archive      = {J_MIR},
  author       = {Tang, Mingkai and Xin, Ren and Fang, Chao and Li, Yuanhang and Liu, Hongji and Wu, Jin},
  doi          = {10.1007/s11633-025-1568-y},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {641-654},
  shortjournal = {Mach. Intell. Res.},
  title        = {GPU-accelerated conflict-based search for multi-agent embodied intelligence},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reactive whole-body locomotion-integrated manipulation based on combined learning and optimization. <em>MIR</em>, <em>22</em>(4), 627-640. (<a href='https://doi.org/10.1007/s11633-024-1538-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reactive planning and control capacity for collaborative robots is essential when the tasks change online in an unstructured environment. This is more difficult for collaborative mobile manipulators (CMM) due to high redundancies. To this end, this paper proposed a reactive whole-body locomotion-integrated manipulation approach based on combined learning and optimization. First, human demonstrations are collected, where the wrist and pelvis movements are treated as whole-body trajectories, mapping to the end-effector (EE) and the mobile base (MB) of CMM, respectively. A time-input kernelized movement primitive (T-KMP) learns the whole-body trajectory, and a multi-dimensional kernelized movement primitive (M-KMP) learns the spatial relationship between the MB and EE pose. According to task changes, the T-KMP adapts the learned trajectories online by inserting the new desired point predicted by M-KMP. Then, the updated reference trajectories are sent to a hierarchical quadratic programming (HQP) controller, where the EE and the MB trajectories tracking are set as the first and second priority tasks, generating the feasible and optimal joint level commands. An ablation simulation experiment with CMM of the HQP is conducted to show the necessity of MB trajectory tracking in mimicking human whole-body motion behavior. Finally, the tasks of the reactive pick-and-place and reactive reaching were undertaken, where the target object was randomly moved, even out of the region of demonstrations. The results showed that the proposed approach can successfully transfer and adapt the human whole-body loco-manipulation skills to CMM online with task changes.},
  archive      = {J_MIR},
  author       = {Zhao, Jianzhuang and Teng, Tao and De Momi, Elena and Ajoudani, Arash},
  doi          = {10.1007/s11633-024-1538-9},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {627-640},
  shortjournal = {Mach. Intell. Res.},
  title        = {Reactive whole-body locomotion-integrated manipulation based on combined learning and optimization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of embodied learning for object-centric robotic manipulation. <em>MIR</em>, <em>22</em>(4), 588-626. (<a href='https://doi.org/10.1007/s11633-025-1542-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied learning for object-centric robotic manipulation is a rapidly developing and challenging area in embodied AI. It is crucial for advancing next-generation intelligent robots and has garnered significant interest recently. Unlike data-driven machine learning methods, embodied learning focuses on robot learning through physical interaction with the environment and perceptual feedback, making it especially suitable for robotic manipulation. In this paper, we provide a comprehensive survey of the latest advancements in this field and categorize the existing work into three main branches: 1) Embodied perceptual learning, which aims to predict object pose and affordance through various data representations; 2) Embodied policy learning, which focuses on generating optimal robotic decisions using methods such as reinforcement learning and imitation learning; 3) Embodied task-oriented learning, designed to optimize the robot’s performance based on the characteristics of different tasks in object grasping and manipulation. In addition, we offer an overview and discussion of public datasets, evaluation metrics, representative applications, current challenges, and potential future research directions. A project associated with this survey has been established at https://github.com/RayYoh/OCRM_survey .},
  archive      = {J_MIR},
  author       = {Zheng, Ying and Yao, Lei and Su, Yuejiao and Zhang, Yi and Wang, Yi and Zhao, Sicheng and Zhang, Yiyi and Chau, Lap-Pui},
  doi          = {10.1007/s11633-025-1542-8},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {588-626},
  shortjournal = {Mach. Intell. Res.},
  title        = {A survey of embodied learning for object-centric robotic manipulation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial for special issue on embodied intelligence. <em>MIR</em>, <em>22</em>(4), 585-587. (<a href='https://doi.org/10.1007/s11633-025-1572-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MIR},
  author       = {He, Wei and Yang, Chenguang and Cheng, Long and Wang, Zhichuang and Wu, Jin and Peternel, Luka},
  doi          = {10.1007/s11633-025-1572-2},
  journal      = {Machine Intelligence Research},
  month        = {8},
  number       = {4},
  pages        = {585-587},
  shortjournal = {Mach. Intell. Res.},
  title        = {Editorial for special issue on embodied intelligence},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ml">ML - 1</h2>
<ul>
<li><details>
<summary>
(2025). Causality from bottom to top: A survey. <em>ML</em>, <em>114</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10994-025-06855-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality has become a fundamental approach for explaining the relationships between events, phenomena, and outcomes in various fields of study. It has invaded various fields and applications, such as medicine, healthcare, economics, finance, fraud detection, cybersecurity, education, public policy, recommender systems, anomaly detection, robotics, control, sociology, marketing, and advertising. In this paper, we survey its development over the past five decades, shedding light on the differences between causality and other approaches, as well as the preconditions for using it. Furthermore, the paper illustrates how causality interacts with new approaches such as Artificial Intelligence (AI), Generative AI (GAI), Machine and Deep Learning, Reinforcement Learning (RL), and Fuzzy Logic. We study the impact of causality on various fields, its contribution, and its interaction with state-of-the-art approaches. Additionally, the paper exemplifies the trustworthiness and explainability of causality models. We offer several ways to evaluate causality models and discuss future directions.},
  archive      = {J_ML},
  author       = {Weinberg, Abraham Itzhak and Premebida, Cristiano and Faria, Diego Resende},
  doi          = {10.1007/s10994-025-06855-5},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Causality from bottom to top: A survey},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="mp">MP - 31</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Regular packing of rooted hyperforests with root constraints in hypergraphs. <em>MP</em>, <em>213</em>(1), 1279. (<a href='https://doi.org/10.1007/s10107-025-02203-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MP},
  author       = {Hoppenot, Pierre and Martin, Mathis and Szigeti, Zoltán},
  doi          = {10.1007/s10107-025-02203-6},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1279},
  shortjournal = {Math. Program.},
  title        = {Correction: Regular packing of rooted hyperforests with root constraints in hypergraphs},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean robust optimization. <em>MP</em>, <em>213</em>(1), 1235-1277. (<a href='https://doi.org/10.1007/s10107-024-02170-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust optimization is a tractable and expressive technique for decision-making under uncertainty, but it can lead to overly conservative decisions when pessimistic assumptions are made on the uncertain parameters. Wasserstein distributionally robust optimization can reduce conservatism by being data-driven, but it often leads to very large problems with prohibitive solution times. We introduce mean robust optimization, a general framework that combines the best of both worlds by providing a trade-off between computational effort and conservatism. We propose uncertainty sets constructed based on clustered data rather than on observed data points directly thereby significantly reducing problem size. By varying the number of clusters, our method bridges between robust and Wasserstein distributionally robust optimization. We show finite-sample performance guarantees and explicitly control the potential additional pessimism introduced by any clustering procedure. In addition, we prove conditions for which, when the uncertainty enters linearly in the constraints, clustering does not affect the optimal solution. We illustrate the efficiency and performance preservation of our method on several numerical examples, obtaining multiple orders of magnitude speedups in solution time with little-to-no effect on the solution quality.},
  archive      = {J_MP},
  author       = {Wang, Irina and Becker, Cole and Van Parys, Bart and Stellato, Bartolomeo},
  doi          = {10.1007/s10107-024-02170-4},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1235-1277},
  shortjournal = {Math. Program.},
  title        = {Mean robust optimization},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regular packing of rooted hyperforests with root constraints in hypergraphs. <em>MP</em>, <em>213</em>(1), 1211-1233. (<a href='https://doi.org/10.1007/s10107-024-02167-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The seminal papers of Edmonds (Combinatorial algorithms, Academic Press, New York, 1973), Nash-Williams (J Lond Math Soc 36:445–450, 1961) and Tutte (J Lond Math Soc 36:221–230, 1961) have laid the foundations of the theories of packing arborescences and packing trees. The directed version has been extensively investigated, resulting in a great number of generalizations. In contrast, the undirected version has been marginally considered. The aim of this paper is to further develop the theories of packing trees and forests. Our main result on graphs characterizes the existence of a packing of k forests, $$F_1, \ldots , F_k$$ , in a graph G such that each vertex of G belongs to exactly h of the forests, the number of connected components of each $$F_i$$ is between $$\ell (i)$$ and $$\ell '(i)$$ and the total number of connected components in the packing is between $$\alpha $$ and $$\beta $$ . Finally, we extend this result to hypergraphs and dypergraphs, the latter giving a generalization of a theorem of Bérczi and Frank (Math Oper Res 43(3):726–753, 2018). As a matter of fact, this research was motivated by the paper of Bérczi and Frank (Math Oper Res 43(3):726–753, 2018).},
  archive      = {J_MP},
  author       = {Hoppenot, Pierre and Martin, Mathis and Szigeti, Zoltán},
  doi          = {10.1007/s10107-024-02167-z},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1211-1233},
  shortjournal = {Math. Program.},
  title        = {Regular packing of rooted hyperforests with root constraints in hypergraphs},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On rank-monotone graph operations and minimal obstruction graphs for the Lovász–Schrijver SDP hierarchy. <em>MP</em>, <em>213</em>(1), 1169-1209. (<a href='https://doi.org/10.1007/s10107-024-02166-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the lift-and-project rank of the stable set polytopes of graphs with respect to the Lovász–Schrijver SDP operator $${{\,\textrm{LS}\,}}_+$$ , with a particular focus on finding and characterizing the smallest graphs with a given $${{\,\textrm{LS}\,}}_+$$ -rank (the needed number of iterations of the $${{\,\textrm{LS}\,}}_+$$ operator on the fractional stable set polytope to compute the stable set polytope). We introduce a generalized vertex-stretching operation that appears to be promising in generating $${{\,\textrm{LS}\,}}_+$$ -minimal graphs and study its properties. We also provide several new $${{\,\textrm{LS}\,}}_+$$ -minimal graphs, most notably the first known instances of 12-vertex graphs with $${{\,\textrm{LS}\,}}_+$$ -rank 4, which provides the first advance in this direction since Escalante, Montelar, and Nasini’s discovery of a 9-vertex graph with $${{\,\textrm{LS}\,}}_+$$ -rank 3 in 2006.},
  archive      = {J_MP},
  author       = {Au, Yu Hin and Tunçel, Levent},
  doi          = {10.1007/s10107-024-02166-0},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1169-1209},
  shortjournal = {Math. Program.},
  title        = {On rank-monotone graph operations and minimal obstruction graphs for the Lovász–Schrijver SDP hierarchy},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantifying low rank approximations of third order symmetric tensors. <em>MP</em>, <em>213</em>(1), 1119-1168. (<a href='https://doi.org/10.1007/s10107-024-02165-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a method to certify the approximation quality of a low rank tensor to a given third order symmetric tensor. Under mild assumptions, best low rank approximation is attained if a control parameter is zero or quantified quasi-optimal low rank approximation is obtained if the control parameter is positive. This is based on a primal-dual method for computing a low rank approximation for a given tensor. The certification is derived from the global optimality of the primal and dual problems, and is characterized by easily checkable relations between the primal and the dual solutions together with another rank condition. The theory is verified theoretically for orthogonally decomposable tensors as well as numerically through examples in the general case.},
  archive      = {J_MP},
  author       = {Hu, Shenglong and Sun, Defeng and Toh, Kim-Chuan},
  doi          = {10.1007/s10107-024-02165-1},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1119-1168},
  shortjournal = {Math. Program.},
  title        = {Quantifying low rank approximations of third order symmetric tensors},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acceleration by stepsize hedging: Silver stepsize schedule for smooth convex optimization. <em>MP</em>, <em>213</em>(1), 1105-1118. (<a href='https://doi.org/10.1007/s10107-024-02164-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a concise, self-contained proof that the Silver Stepsize Schedule proposed in our companion paper directly applies to smooth (non-strongly) convex optimization. Specifically, we show that with these stepsizes, gradient descent computes an $$\varepsilon $$ -minimizer in $$O(\varepsilon ^{-\log _{\rho } 2}) = O(\varepsilon ^{-0.7864})$$ iterations, where $$\rho = 1+\sqrt{2}$$ is the silver ratio. This is intermediate between the textbook unaccelerated rate $$O(\varepsilon ^{-1})$$ and the accelerated rate $$O(\varepsilon ^{-1/2})$$ due to Nesterov in 1983. The Silver Stepsize Schedule is a simple explicit fractal: the i-th stepsize is $$1 + \rho ^{\nu (i)-1}$$ where $$\nu (i)$$ is the 2-adic valuation of i. The design and analysis are conceptually identical to the strongly convex setting in our companion paper, but simplify remarkably in this specific setting.},
  archive      = {J_MP},
  author       = {Altschuler, Jason M. and Parrilo, Pablo A.},
  doi          = {10.1007/s10107-024-02164-2},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1105-1118},
  shortjournal = {Math. Program.},
  title        = {Acceleration by stepsize hedging: Silver stepsize schedule for smooth convex optimization},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A first-order augmented lagrangian method for constrained minimax optimization. <em>MP</em>, <em>213</em>(1), 1063-1104. (<a href='https://doi.org/10.1007/s10107-024-02163-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study a class of constrained minimax problems. In particular, we propose a first-order augmented Lagrangian method for solving them, whose subproblems turn out to be a much simpler structured minimax problem and are suitably solved by a first-order method developed in this paper. Under some suitable assumptions, an operation complexity of $$\mathcal{O}(\varepsilon ^{-4}\log \varepsilon ^{-1})$$ , measured by its fundamental operations, is established for the first-order augmented Lagrangian method for finding an $$\varepsilon $$ -KKT solution of the constrained minimax problems.},
  archive      = {J_MP},
  author       = {Lu, Zhaosong and Mei, Sanyou},
  doi          = {10.1007/s10107-024-02163-3},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1063-1104},
  shortjournal = {Math. Program.},
  title        = {A first-order augmented lagrangian method for constrained minimax optimization},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended formulations for binary optimal control problems. <em>MP</em>, <em>213</em>(1), 1039-1062. (<a href='https://doi.org/10.1007/s10107-024-02162-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extended formulations are an important tool in polyhedral combinatorics. Many combinatorial optimization problems require an exponential number of inequalities when modeled as a linear program in the natural space of variables. However, by adding artificial variables, one can often find a small linear formulation, i.e., one containing a polynomial number of variables and constraints, such that the projection to the original space of variables yields a perfect linear formulation. Motivated by binary optimal control problems with switching constraints, we show that a similar approach can be useful also for optimization problems in function space, in order to model the closed convex hull of feasible controls in a compact way. More specifically, we present small extended formulations for switches with bounded variation and for dwell-time constraints. For general linear switching point constraints, we devise an extended model linearizing the problem, but show that a small extended formulation that is compatible with discretization cannot exist unless P = NP.},
  archive      = {J_MP},
  author       = {Buchheim, Christoph},
  doi          = {10.1007/s10107-024-02162-4},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1039-1062},
  shortjournal = {Math. Program.},
  title        = {Extended formulations for binary optimal control problems},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved global guarantees for the nonconvex Burer–Monteiro factorization via rank overparameterization. <em>MP</em>, <em>213</em>(1), 1009-1038. (<a href='https://doi.org/10.1007/s10107-024-02160-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider minimizing a twice-differentiable, L-smooth, and $$\mu $$ -strongly convex objective $$\phi $$ over an $$n\times n$$ positive semidefinite matrix $$M\succeq 0$$ , under the assumption that the minimizer $$M^{\star }$$ has low rank $$r^{\star }\ll n$$ . Following the Burer–Monteiro approach, we instead minimize the nonconvex objective $$f(X)=\phi (XX^{T})$$ over a factor matrix X of size $$n\times r$$ . This substantially reduces the number of variables from $$O(n^{2})$$ to as few as O(n) and also enforces positive semidefiniteness for free, but at the cost of giving up the convexity of the original problem. In this paper, we prove that if the search rank $$r\ge r^{\star }$$ is overparameterized by a constant factor with respect to the true rank $$r^{\star }$$ , namely as in $$r>\frac{1}{4}(L/\mu -1)^{2}r^{\star }$$ , then despite nonconvexity, local optimization is guaranteed to globally converge from any initial point to the global optimum. This significantly improves upon a previous rank overparameterization threshold of $$r\ge n$$ , which we show is sharp in the absence of smoothness and strong convexity, but would increase the number of variables back up to $$O(n^{2})$$ . Conversely, without rank overparameterization, we prove that such a global guarantee is possible if and only if $$\phi $$ is almost perfectly conditioned, with a condition number of $$L/\mu <3$$ . Therefore, we conclude that a small amount of overparameterization can lead to large improvements in theoretical guarantees for the nonconvex Burer–Monteiro factorization.},
  archive      = {J_MP},
  author       = {Zhang, Richard Y.},
  doi          = {10.1007/s10107-024-02160-6},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1009-1038},
  shortjournal = {Math. Program.},
  title        = {Improved global guarantees for the nonconvex Burer–Monteiro factorization via rank overparameterization},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact and approximation algorithms for routing a convoy through a graph. <em>MP</em>, <em>213</em>(1), 985-1008. (<a href='https://doi.org/10.1007/s10107-024-02159-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study routing problems of a convoy in a graph, generalizing the shortest path problem (SPP), the travelling salesperson problem (TSP), and the Chinese postman problem (CPP) which are all well-studied in the classical (non-convoy) setting. We assume that each edge in the graph has a length and a speed at which it can be traversed and that our convoy has a given length. While the convoy moves through the graph, parts of it can be located on different edges. For safety requirements, at all time the whole convoy needs to travel at the same speed which is dictated by the slowest edge on which currently a part of the convoy is located. For Convoy-SPP, we give a strongly polynomial time exact algorithm. For Convoy-TSP, we provide an $$O(\log n)$$ -approximation algorithm and an O(1)-approximation algorithm for trees. Both results carry over to Convoy-CPP which—maybe surprisingly—we prove to be $$\textsf{NP}$$ -hard in the convoy setting. This contrasts the non-convoy setting in which the problem is polynomial time solvable.},
  archive      = {J_MP},
  author       = {van Ee, Martijn and Oosterwijk, Tim and Sitters, René and Wiese, Andreas},
  doi          = {10.1007/s10107-024-02159-z},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {985-1008},
  shortjournal = {Math. Program.},
  title        = {Exact and approximation algorithms for routing a convoy through a graph},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Algebraic combinatorial optimization on the degree of determinants of noncommutative symbolic matrices. <em>MP</em>, <em>213</em>(1), 941-984. (<a href='https://doi.org/10.1007/s10107-024-02158-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the computation of the degrees of minors of a noncommutative symbolic matrix of form $$ A[c] :=\sum _{k=1}^m A_k t^{c_k} x_k, $$ where $$A_k$$ are matrices over a field $$\mathbb {K}$$ , $$x_k$$ are noncommutative variables, $$c_k$$ are integer weights, and t is a commuting variable specifying the degree. This problem extends noncommutative Edmonds’ problem (Ivanyos et al. in Comput Complex 26:717–763, 2017), and can formulate various combinatorial optimization problems. Extending the study by Hirai 2018, and Hirai, Ikeda 2022, we provide novel duality theorems and polyhedral characterization for the maximum degrees of minors of A[c] of all sizes, and develop a strongly polynomial-time algorithm for computing them. This algorithm is viewed as a unified algebraization of the classical Hungarian method for bipartite matching and the weight-splitting algorithm for linear matroid intersection. As applications, we provide polynomial-time algorithms for weighted fractional linear matroid matching and for membership of rank-2 Brascamp–Lieb polytopes.},
  archive      = {J_MP},
  author       = {Hirai, Hiroshi and Iwamasa, Yuni and Oki, Taihei and Soma, Tasuku},
  doi          = {10.1007/s10107-024-02158-0},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {941-984},
  shortjournal = {Math. Program.},
  title        = {Algebraic combinatorial optimization on the degree of determinants of noncommutative symbolic matrices},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A projection-free method for solving convex bilevel optimization problems. <em>MP</em>, <em>213</em>(1), 907-940. (<a href='https://doi.org/10.1007/s10107-024-02157-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When faced with multiple minima of an inner-level convex optimization problem, the convex bilevel optimization problem selects an optimal solution which also minimizes an auxiliary outer-level convex objective of interest. Bilevel optimization requires a different approach compared to single-level optimization problems since the set of minimizers for the inner-level objective is not given explicitly. In this paper, we propose a new projection-free conditional gradient method for convex bilevel optimization which requires only a linear optimization oracle over the base domain. We establish $$O(t^{-1/2})$$ convergence rate guarantees for our method in terms of both inner- and outer-level objectives, and demonstrate how additional assumptions such as quadratic growth and strong convexity result in accelerated rates of up to $$O(t^{-1})$$ and $$O(t^{-2/3})$$ for inner- and outer-levels respectively. Lastly, we conduct a numerical study to demonstrate the performance of our method.},
  archive      = {J_MP},
  author       = {Giang-Tran, Khanh-Hung and Ho-Nguyen, Nam and Lee, Dabeen},
  doi          = {10.1007/s10107-024-02157-1},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {907-940},
  shortjournal = {Math. Program.},
  title        = {A projection-free method for solving convex bilevel optimization problems},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Riemannian trust-region methods for strict saddle functions with complexity guarantees. <em>MP</em>, <em>213</em>(1), 863-905. (<a href='https://doi.org/10.1007/s10107-024-02156-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difficulty of minimizing a nonconvex function is in part explained by the presence of saddle points. This slows down optimization algorithms and impacts worst-case complexity guarantees. However, many nonconvex problems of interest possess a favorable structure for optimization, in the sense that saddle points can be escaped efficiently by appropriate algorithms. This strict saddle property has been extensively used in data science to derive good properties for first-order algorithms, such as convergence to second-order critical points. However, the analysis and the design of second-order algorithms in the strict saddle setting have received significantly less attention. In this paper, we consider second-order trust-region methods for a class of strict saddle functions defined on Riemannian manifolds. These functions exhibit (geodesic) strong convexity around minimizers and negative curvature at saddle points. We first show that the standard trust-region method with exact subproblem minimization finds an approximate local minimizer in a number of iterations that depends logarithmically on the accuracy parameter, which significantly improves known results for general nonconvex optimization. We then propose a new inexact variant of the algorithm that explicitly leverages the strict saddle property to compute the most appropriate step at every iteration. Our bounds for the inexact variant also improve over the general nonconvex case, and illustrate the benefit of using strict saddle properties within optimization algorithms.},
  archive      = {J_MP},
  author       = {Goyens, Florentin and Royer, Clément W.},
  doi          = {10.1007/s10107-024-02156-2},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {863-905},
  shortjournal = {Math. Program.},
  title        = {Riemannian trust-region methods for strict saddle functions with complexity guarantees},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From coordinate subspaces over finite fields to ideal multipartite uniform clutters. <em>MP</em>, <em>213</em>(1), 823-861. (<a href='https://doi.org/10.1007/s10107-024-02155-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Take a prime power q, an integer $$n\ge 2$$ , and a coordinate subspace $$S\subseteq GF(q)^n$$ over the Galois field GF(q). One can associate with S an n-partite n-uniform clutter $$\mathcal {C}$$ , where every part has size q and there is a bijection between the vectors in S and the members of $$\mathcal {C}$$ . In this paper, we determine when the clutter $$\mathcal {C}$$ is ideal, a property developed in connection to Packing and Covering problems in the areas of Integer Programming and Combinatorial Optimization. Interestingly, the characterization differs depending on whether q is 2, 4, a higher power of 2, or otherwise. Each characterization uses crucially that idealness is a minor-closed property: first the list of excluded minors is identified, and only then is the global structure determined. A key insight is that idealness of $$\mathcal {C}$$ depends solely on the underlying matroid of S. Our theorems also extend from idealness to the stronger max-flow min-cut property. As a consequence, we prove the Replication and $$\tau =2$$ Conjectures for this class of clutters.},
  archive      = {J_MP},
  author       = {Abdi, Ahmad and Lee, Dabeen},
  doi          = {10.1007/s10107-024-02155-3},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {823-861},
  shortjournal = {Math. Program.},
  title        = {From coordinate subspaces over finite fields to ideal multipartite uniform clutters},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated-gradient-based generalized Levenberg–Marquardt method with oracle complexity bound and local quadratic convergence. <em>MP</em>, <em>213</em>(1), 771-822. (<a href='https://doi.org/10.1007/s10107-024-02154-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing the sum of a convex function and a composite function appears in various fields. The generalized Levenberg–Marquardt (LM) method, also known as the prox-linear method, has been developed for such optimization problems. The method iteratively solves strongly convex subproblems with a damping term. This study proposes a new generalized LM method for solving the problem with a smooth composite function. The method enjoys three theoretical guarantees: iteration complexity bound, oracle complexity bound, and local convergence under a Hölderian growth condition. The local convergence results include local quadratic convergence under the quadratic growth condition; this is the first to extend the classical result for least-squares problems to a general smooth composite function. In addition, this is the first LM method with both an oracle complexity bound and local quadratic convergence under standard assumptions. These results are achieved by carefully controlling the damping parameter and solving the subproblems by the accelerated proximal gradient method equipped with a particular termination condition. Experimental results show that the proposed method performs well in practice for several instances, including classification with a neural network and nonnegative matrix factorization.},
  archive      = {J_MP},
  author       = {Marumo, Naoki and Okuno, Takayuki and Takeda, Akiko},
  doi          = {10.1007/s10107-024-02154-4},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {771-822},
  shortjournal = {Math. Program.},
  title        = {Accelerated-gradient-based generalized Levenberg–Marquardt method with oracle complexity bound and local quadratic convergence},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural spectrahedra and semidefinite lifts: Global convex optimization of degree-two polynomial activation neural networks in polynomial-time. <em>MP</em>, <em>213</em>(1), 737-769. (<a href='https://doi.org/10.1007/s10107-024-02153-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training of two-layer neural networks with nonlinear activation functions is an important non-convex optimization problem with numerous applications and promising performance in layerwise deep learning. In this paper, we develop exact convex optimization formulations for two-layer neural networks with second degree polynomial activations based on dual relaxations and semidefinite programming. Remarkably, we show that our semidefinite relaxations are always tight. Therefore, the computational complexity for global optimization is polynomial in the input dimension and sample size for all input data. The developed convex formulations are proven to achieve the same globally optimal solution set as their non-convex counterparts. Specifically, globally optimal two-layer neural networks with degree-two polynomial activations can be found by solving a semidefinite program (SDP) and decomposing the solution using a procedure we call Neural Decomposition. Moreover, the choice of regularizers plays a crucial role in the computational tractability of neural network training. We show that the standard weight decay regularization formulation is NP-hard, whereas other simple convex penalties render the problem tractable in polynomial time via convex programming. The techniques go beyond the fully connected architecture to encompass various neural network structures including those with vector outputs and convolutional architectures. We provide extensive numerical simulations showing that the standard backpropagation approach often fails to achieve the global optimum of the training loss. The proposed approach is significantly faster to obtain better test accuracy compared to the standard backpropagation procedure.},
  archive      = {J_MP},
  author       = {Bartan, Burak and Pilanci, Mert},
  doi          = {10.1007/s10107-024-02153-5},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {737-769},
  shortjournal = {Math. Program.},
  title        = {Neural spectrahedra and semidefinite lifts: Global convex optimization of degree-two polynomial activation neural networks in polynomial-time},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The mixed integer trust region problem. <em>MP</em>, <em>213</em>(1), 699-736. (<a href='https://doi.org/10.1007/s10107-024-02152-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider the problem of minimizing a general quadratic function over the mixed integer points in an ellipsoid. This problem is strongly NP-hard, NP-hard to approximate within a constant factor, and optimal solutions can be irrational. In our main result we show that an arbitrarily good solution can be found in polynomial time, if we fix the number of integer variables. This algorithm provides a natural extension to the mixed integer setting, of the polynomial solvability of the trust region problem proven by Ye, Karmarkar, Vavasis, and Zippel. As a result, our findings pave the way for designing efficient trust region methods for mixed integer nonlinear optimization problems. The techniques that we introduce are of independent interest and can be used in other mixed integer nonlinear optimization problems. As an example, we consider the problem of minimizing a general quadratic function over the mixed integer points in a polyhedron. For this problem, we show that a solution satisfying weak bounds with respect to optimality can be computed in polynomial time, provided that the number of integer variables is fixed. It is well known that finding a solution satisfying stronger bounds cannot be done in polynomial time, unless P = NP.},
  archive      = {J_MP},
  author       = {Del Pia, Alberto},
  doi          = {10.1007/s10107-024-02152-6},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {699-736},
  shortjournal = {Math. Program.},
  title        = {The mixed integer trust region problem},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear distributionally robust optimization. <em>MP</em>, <em>213</em>(1), 639-698. (<a href='https://doi.org/10.1007/s10107-024-02151-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on a class of distributionally robust optimization (DRO) problems where, unlike the growing body of the literature, the objective function is potentially nonlinear in the distribution. Existing methods to optimize nonlinear functions in probability space use the Frechet derivatives, which present both theoretical and computational challenges. Motivated by this, we propose an alternative notion for the derivative and corresponding smoothness based on Gateaux (G)-derivative for generic risk measures. These concepts are explained via three running risk measure examples of variance, entropic risk, and risk on finite support sets. We then propose a G-derivative based Frank–Wolfe (FW) algorithm for generic nonlinear optimization problems in probability spaces and establish its convergence under the proposed notion of smoothness in a completely norm-independent manner. We use the set-up of the FW algorithm to devise a methodology to compute a saddle point of the nonlinear DRO problem. Finally, we validate our theoretical results on two cases of the entropic and variance risk measures in the context of portfolio selection problems. In particular, we analyze their regularity conditions and “sufficient statistic”, compute the respective FW-oracle in various settings, and confirm the theoretical outcomes through numerical validation.},
  archive      = {J_MP},
  author       = {Sheriff, Mohammed Rayyan and Mohajerin Esfahani, Peyman},
  doi          = {10.1007/s10107-024-02151-7},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {639-698},
  shortjournal = {Math. Program.},
  title        = {Nonlinear distributionally robust optimization},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALSO-x#: Better convex approximations for distributionally robust chance constrained programs. <em>MP</em>, <em>213</em>(1), 575-638. (<a href='https://doi.org/10.1007/s10107-024-02150-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies distributionally robust chance constrained programs (DRCCPs), where the uncertain constraints must be satisfied with at least a probability of a prespecified threshold for all probability distributions from the Wasserstein ambiguity set. As DRCCPs are often nonconvex and some DRCCPs may not have mixed-integer reformulations, researchers have been developing various convex inner approximations. Recently, ALSO-X has been proven to outperform the conditional value-at-risk (CVaR) approximation of a regular chance constrained program when the deterministic set is convex. In this work, we relax this assumption by introducing a new ALSO-X# method to solve DRCCPs. Namely, in the bilevel structures of ALSO-X and CVaR approximation, we observe that the lower-level ALSO-X is a special case of the lower-level CVaR approximation, and the upper-level CVaR approximation is more restricted than the one in ALSO-X. This observation motivates us to propose ALSO-X#, which has a bilevel structure—in the lower-level problem, we adopt the more general CVaR approximation, and for the upper-level one, we choose the less restricted ALSO-X. We show that ALSO-X# is always better than CVaR approximation and can outperform ALSO-X under regular chance constrained programs and type $$\infty $$ -Wasserstein ambiguity set. We also provide new sufficient conditions under which ALSO-X# outputs an optimal solution to a DRCCP. We apply ALSO-X# to a wireless communication problem and numerically demonstrate that the solution quality can be even better than the exact method.},
  archive      = {J_MP},
  author       = {Jiang, Nan and Xie, Weijun},
  doi          = {10.1007/s10107-024-02150-8},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {575-638},
  shortjournal = {Math. Program.},
  title        = {ALSO-x#: Better convex approximations for distributionally robust chance constrained programs},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of trigonometric polynomials with crystallographic symmetry and spectral bounds for set avoiding graphs. <em>MP</em>, <em>213</em>(1), 517-573. (<a href='https://doi.org/10.1007/s10107-024-02149-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a new approach to the optimization of trigonometric polynomials with crystallographic symmetry. This approach widens the bridge between trigonometric and polynomial optimization. The trigonometric polynomials considered are supported on weight lattices associated to crystallographic root systems and are assumed invariant under the associated reflection group. On one hand the invariance allows us to rewrite the objective function in terms of generalized Chebyshev polynomials of the generalized cosines; On the other hand the generalized cosines parameterize a compact basic semi algebraic set, this latter being given by an explicit polynomial matrix inequality. The initial problem thus boils down to a polynomial optimization problem that is straightforwardly written in terms of generalized Chebyshev polynomials. The minimum is to be computed by a converging sequence of lower bounds as given by a hierarchy of relaxations based on the Hol–Scherer Positivstellensatz and indexed by the weighted degree associated to the root system. This new method for trigonometric optimization was motivated by its application to estimate the spectral bound on the chromatic number of set avoiding graphs. We examine cases of the literature where the avoided set affords crystallographic symmetry. In some cases we obtain new analytic proofs for sharp bounds on the chromatic number while in others we compute new lower bounds numerically.},
  archive      = {J_MP},
  author       = {Hubert, Evelyne and Metzlaff, Tobias and Moustrou, Philippe and Riener, Cordian},
  doi          = {10.1007/s10107-024-02149-1},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {517-573},
  shortjournal = {Math. Program.},
  title        = {Optimization of trigonometric polynomials with crystallographic symmetry and spectral bounds for set avoiding graphs},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dyadic linear programming and extensions. <em>MP</em>, <em>213</em>(1), 473-516. (<a href='https://doi.org/10.1007/s10107-024-02146-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A rational number is dyadic if it has a finite binary representation $$p/2^k$$ , where p is an integer and k is a nonnegative integer. Dyadic rationals are important for numerical computations because they have an exact representation in floating-point arithmetic on a computer. A vector is dyadic if all its entries are dyadic rationals. We study the problem of finding a dyadic optimal solution to a linear program, if one exists. We show how to solve dyadic linear programs in polynomial time. We give bounds on the size of the support of a solution as well as on the size of the denominators. We identify properties that make the solution of dyadic linear programs possible: closure under addition and negation, and density, and we extend the algorithmic framework beyond the dyadic case.},
  archive      = {J_MP},
  author       = {Abdi, Ahmad and Cornuéjols, Gérard and Guenin, Bertrand and Tunçel, Levent},
  doi          = {10.1007/s10107-024-02146-4},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {473-516},
  shortjournal = {Math. Program.},
  title        = {Dyadic linear programming and extensions},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive proximal algorithms for convex optimization under local lipschitz continuity of the gradient. <em>MP</em>, <em>213</em>(1), 433-471. (<a href='https://doi.org/10.1007/s10107-024-02143-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backtracking linesearch is the de facto approach for minimizing continuously differentiable functions with locally Lipschitz gradient. In recent years, it has been shown that in the convex setting it is possible to avoid linesearch altogether, and to allow the stepsize to adapt based on a local smoothness estimate without any backtracks or evaluations of the function value. In this work we propose an adaptive proximal gradient method, adaPGM, that uses novel estimates of the local smoothness modulus which leads to less conservative stepsize updates and that can additionally cope with nonsmooth terms. This idea is extended to the primal-dual setting where an adaptive three-term primal-dual algorithm, adaPDM, is proposed which can be viewed as an extension of the PDHG method. Moreover, in this setting the “essentially” fully adaptive variant $$\textsf {adaPDM}^{\textsf {+}}$$ is proposed that avoids evaluating the linear operator norm by invoking a backtracking procedure, that, remarkably, does not require extra gradient evaluations. Numerical simulations demonstrate the effectiveness of the proposed algorithms compared to the state of the art.},
  archive      = {J_MP},
  author       = {Latafat, Puya and Themelis, Andreas and Stella, Lorenzo and Patrinos, Panagiotis},
  doi          = {10.1007/s10107-024-02143-7},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {433-471},
  shortjournal = {Math. Program.},
  title        = {Adaptive proximal algorithms for convex optimization under local lipschitz continuity of the gradient},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coderivative-based semi-newton method in nonsmooth difference programming. <em>MP</em>, <em>213</em>(1), 385-432. (<a href='https://doi.org/10.1007/s10107-024-02142-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the study of a new class of nonsmooth optimization problems, where the objective is represented as a difference of two generally nonconvex functions. We propose and develop a novel Newton-type algorithm to solving such problems, which is based on the coderivative generated second-order subdifferential (generalized Hessian) and employs advanced tools of variational analysis. Well-posedness properties of the proposed algorithm are derived under fairly general requirements, while constructive convergence rates are established by using additional assumptions including the Kurdyka–Łojasiewicz condition. We provide applications of the main algorithm to solving a general class of nonsmooth nonconvex problems of structured optimization that encompasses, in particular, optimization problems with explicit constraints. Finally, applications and numerical experiments are given for solving practical problems that arise in biochemical models, supervised learning, constrained quadratic programming, etc., where advantages of our algorithms are demonstrated in comparison with some known techniques and results.},
  archive      = {J_MP},
  author       = {Aragón-Artacho, Francisco J. and Mordukhovich, Boris S. and Pérez-Aros, Pedro},
  doi          = {10.1007/s10107-024-02142-8},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {385-432},
  shortjournal = {Math. Program.},
  title        = {Coderivative-based semi-newton method in nonsmooth difference programming},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast convergence of trust-regions for non-isolated minima via analysis of CG on indefinite matrices. <em>MP</em>, <em>213</em>(1), 343-384. (<a href='https://doi.org/10.1007/s10107-024-02140-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust-region methods (TR) can converge quadratically to minima where the Hessian is positive definite. However, if the minima are not isolated, then the Hessian there cannot be positive definite. The weaker Polyak–Łojasiewicz (PŁ) condition is compatible with non-isolated minima, and it is enough for many algorithms to preserve good local behavior. Yet, TR with an exact subproblem solver lacks even basic features such as a capture theorem under PŁ. In practice, a popular inexact subproblem solver is the truncated conjugate gradient method (tCG). Empirically, TR-tCG exhibits superlinear convergence under PŁ. We confirm this theoretically. The main mathematical obstacle is that, under PŁ, at points arbitrarily close to minima, the Hessian has vanishingly small, possibly negative eigenvalues. Thus, tCG is applied to ill-conditioned, indefinite systems. Yet, the core theory underlying tCG is that of CG, which assumes a positive definite operator. Accordingly, we develop new tools to analyze the dynamics of CG in the presence of small eigenvalues of any sign, for the regime of interest to TR-tCG.},
  archive      = {J_MP},
  author       = {Rebjock, Quentin and Boumal, Nicolas},
  doi          = {10.1007/s10107-024-02140-w},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {343-384},
  shortjournal = {Math. Program.},
  title        = {Fast convergence of trust-regions for non-isolated minima via analysis of CG on indefinite matrices},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The computational complexity of finding stationary points in non-convex optimization. <em>MP</em>, <em>213</em>(1), 281-341. (<a href='https://doi.org/10.1007/s10107-024-02139-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding approximate stationary points, i.e., points where the gradient is approximately zero, of non-convex but smooth objective functions f over unrestricted d-dimensional domains is one of the most fundamental problems in classical non-convex optimization. Nevertheless, the computational and query complexity of this problem are still not well understood when the dimension d of the problem is independent of the approximation error. In this paper, we show the following computational and query complexity results:},
  archive      = {J_MP},
  author       = {Hollender, Alexandros and Zampetakis, Manolis},
  doi          = {10.1007/s10107-024-02139-3},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {281-341},
  shortjournal = {Math. Program.},
  title        = {The computational complexity of finding stationary points in non-convex optimization},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated stochastic approximation with state-dependent noise. <em>MP</em>, <em>213</em>(1), 239-280. (<a href='https://doi.org/10.1007/s10107-024-02138-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the “sub-optimality” of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size. We discuss two non-Euclidean accelerated stochastic approximation routines—stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)—which carry a particular duality relationship. We show that both SAGD and SGE, under appropriate conditions, achieve the optimal convergence rate, attaining the optimal iteration and sample complexities simultaneously. However, corresponding assumptions for the SGE algorithm are more general; they allow, for instance, for efficient application of the SGE to statistical estimation problems under heavy tail noises and discontinuous score functions. We also discuss the application of the SGE to problems satisfying quadratic growth conditions, and show how it can be used to recover sparse solutions. Finally, we report on some simulation experiments to illustrate numerical performance of our proposed algorithms in high-dimensional settings.},
  archive      = {J_MP},
  author       = {Ilandarideva, Sasila and Juditsky, Anatoli and Lan, Guanghui and Li, Tianjiao},
  doi          = {10.1007/s10107-024-02138-4},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {239-280},
  shortjournal = {Math. Program.},
  title        = {Accelerated stochastic approximation with state-dependent noise},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complexity of chordal conversion for sparse semidefinite programs with small treewidth. <em>MP</em>, <em>213</em>(1), 201-237. (<a href='https://doi.org/10.1007/s10107-024-02137-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If a sparse semidefinite program (SDP), specified over $$n\times n$$ matrices and subject to m linear constraints, has an aggregate sparsity graph G with small treewidth, then chordal conversion will sometimes allow an interior-point method to solve the SDP in just $$O(m+n)$$ time per-iteration, which is a significant speedup over the $$\varOmega (n^{3})$$ time per-iteration for a direct application of the interior-point method. Unfortunately, the speedup is not guaranteed by an O(1) treewidth in G that is independent of m and n, as a diagonal SDP would have treewidth zero but can still necessitate up to $$\varOmega (n^{3})$$ time per-iteration. Instead, we construct an extended aggregate sparsity graph $$\overline{G}\supseteq G$$ by forcing each constraint matrix $$A_{i}$$ to be its own clique in G. We prove that a small treewidth in $$\overline{G}$$ does indeed guarantee that chordal conversion will solve the SDP in $$O(m+n)$$ time per-iteration, to $$\epsilon $$ -accuracy in at most $$O(\sqrt{m+n}\log (1/\epsilon ))$$ iterations. This sufficient condition covers many successful applications of chordal conversion, including the MAX-k-CUT relaxation, the Lovász theta problem, sensor network localization, polynomial optimization, and the AC optimal power flow relaxation, thus allowing theory to match practical experience.},
  archive      = {J_MP},
  author       = {Zhang, Richard Y.},
  doi          = {10.1007/s10107-024-02137-5},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {201-237},
  shortjournal = {Math. Program.},
  title        = {Complexity of chordal conversion for sparse semidefinite programs with small treewidth},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast convergence to non-isolated minima: Four equivalent conditions for $${\textrm{C}^{2}}$$ functions. <em>MP</em>, <em>213</em>(1), 151-199. (<a href='https://doi.org/10.1007/s10107-024-02136-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization algorithms can see their local convergence rates deteriorate when the Hessian at the optimum is singular. These singularities are inescapable when the optima are non-isolated. Yet, under the right circumstances, several algorithms preserve their favorable rates even when optima form a continuum (e.g., due to over-parameterization). This has been explained under various structural assumptions, including the Polyak–Łojasiewicz condition, Quadratic Growth and the Error Bound. We show that, for cost functions which are twice continuously differentiable ( $$\textrm{C}^2$$ ), those three (local) properties are equivalent. Moreover, we show they are equivalent to the Morse–Bott property, that is, local minima form differentiable submanifolds, and the Hessian of the cost function is positive definite along its normal directions. We leverage this insight to improve local convergence guarantees for safe-guarded Newton-type methods under any (hence all) of the above assumptions. First, for adaptive cubic regularization, we secure quadratic convergence even with inexact subproblem solvers. Second, for trust-region methods, we argue capture can fail with an exact subproblem solver, then proceed to show linear convergence with an inexact one (Cauchy steps).},
  archive      = {J_MP},
  author       = {Rebjock, Quentin and Boumal, Nicolas},
  doi          = {10.1007/s10107-024-02136-6},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {151-199},
  shortjournal = {Math. Program.},
  title        = {Fast convergence to non-isolated minima: Four equivalent conditions for $${\textrm{C}^{2}}$$ functions},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convexification techniques for fractional programs. <em>MP</em>, <em>213</em>(1), 107-149. (<a href='https://doi.org/10.1007/s10107-024-02131-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a correspondence relating convex hulls of fractional functions with those of polynomial functions over the same domain. Using this result, we develop a number of new reformulations and relaxations for fractional programming problems. First, we relate $$0\mathord {-}1$$ problems involving a ratio of affine functions with the boolean quadric polytope, and use inequalities for the latter to develop tighter formulations for the former. Second, we derive a new formulation to optimize a ratio of quadratic functions over a polytope using copositive programming. Third, we show that univariate fractional functions can be convexified using moment hulls. Fourth, we develop a new hierarchy of relaxations that converges finitely to the simultaneous convex hull of a collection of ratios of affine functions of $$0\mathord {-}1$$ variables. Finally, we demonstrate theoretically and computationally that our techniques close a significant gap relative to state-of-the-art relaxations, require much less computational effort, and can solve larger problem instances.},
  archive      = {J_MP},
  author       = {He, Taotao and Liu, Siyue and Tawarmalani, Mohit},
  doi          = {10.1007/s10107-024-02131-x},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {107-149},
  shortjournal = {Math. Program.},
  title        = {Convexification techniques for fractional programs},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing distortion riskmetrics with distributional uncertainty. <em>MP</em>, <em>213</em>(1), 51-106. (<a href='https://doi.org/10.1007/s10107-024-02128-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization of distortion riskmetrics with distributional uncertainty has wide applications in finance and operations research. Distortion riskmetrics include many commonly applied risk measures and deviation measures, which are not necessarily monotone or convex. One of our central findings is a unifying result that allows to convert an optimization of a non-convex distortion riskmetric with distributional uncertainty to a convex one induced from the concave envelope of the distortion function, leading to practical tractability. A sufficient condition to the unifying equivalence result is the novel notion of closedness under concentration, a variation of which is also shown to be necessary for the equivalence. Our results include many special cases that are well studied in the optimization literature, including but not limited to optimizing probabilities, Value-at-Risk, Expected Shortfall, Yaari’s dual utility, and differences between distortion risk measures, under various forms of distributional uncertainty. We illustrate our theoretical results via applications to portfolio optimization, optimization under moment constraints, and preference robust optimization.},
  archive      = {J_MP},
  author       = {Pesenti, Silvana M. and Wang, Qiuqi and Wang, Ruodu},
  doi          = {10.1007/s10107-024-02128-6},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {51-106},
  shortjournal = {Math. Program.},
  title        = {Optimizing distortion riskmetrics with distributional uncertainty},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear conjugate gradient methods: Worst-case convergence rates via computer-assisted analyses. <em>MP</em>, <em>213</em>(1), 1-49. (<a href='https://doi.org/10.1007/s10107-024-02127-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a computer-assisted approach to the analysis of the worst-case convergence of nonlinear conjugate gradient methods (NCGMs). Those methods are known for their generally good empirical performances for large-scale optimization, while having relatively incomplete analyses. Using our computer-assisted approach, we establish novel complexity bounds for the Polak-Ribière-Polyak (PRP) and the Fletcher-Reeves (FR) NCGMs for smooth strongly convex minimization. In particular, we construct mathematical proofs that establish the first non-asymptotic convergence bound for FR (which is historically the first developed NCGM), and a much improved non-asymptotic convergence bound for PRP. Additionally, we provide simple adversarial examples on which these methods do not perform better than gradient descent with exact line search, leaving very little room for improvements on the same class of problems.},
  archive      = {J_MP},
  author       = {Das Gupta, Shuvomoy and Freund, Robert M. and Sun, Xu Andy and Taylor, Adrien},
  doi          = {10.1007/s10107-024-02127-7},
  journal      = {Mathematical Programming},
  month        = {9},
  number       = {1},
  pages        = {1-49},
  shortjournal = {Math. Program.},
  title        = {Nonlinear conjugate gradient methods: Worst-case convergence rates via computer-assisted analyses},
  volume       = {213},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="mpc">MPC - 5</h2>
<ul>
<li><details>
<summary>
(2025). Adaptive sieving: A dimension reduction technique for sparse optimization problems. <em>MPC</em>, <em>17</em>(3), 585-616. (<a href='https://doi.org/10.1007/s12532-025-00282-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an adaptive sieving (AS) strategy for solving general sparse machine learning models by effectively exploring the intrinsic sparsity of the solutions, wherein only a sequence of reduced problems with much smaller sizes need to be solved. We further apply the proposed AS strategy to generate solution paths for large-scale sparse optimization problems efficiently. We establish the theoretical guarantees for the proposed AS strategy including its finite termination property. Extensive numerical experiments are presented in this paper to demonstrate the effectiveness and flexibility of the AS strategy to solve large-scale machine learning models.},
  archive      = {J_MPC},
  author       = {Yuan, Yancheng and Lin, Meixia and Sun, Defeng and Toh, Kim-Chuan},
  doi          = {10.1007/s12532-025-00282-2},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {585-616},
  shortjournal = {Math. Program. Comput.},
  title        = {Adaptive sieving: A dimension reduction technique for sparse optimization problems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polynomial optimization relaxations for generalized semi-infinite programs. <em>MPC</em>, <em>17</em>(3), 547-583. (<a href='https://doi.org/10.1007/s12532-025-00280-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies generalized semi-infinite programs (GSIPs) given by polynomials. We propose a hierarchy of polynomial optimization relaxations to solve them. They are based on Lagrange multiplier expressions and polynomial extensions. Moment-SOS relaxations are applied to solve the polynomial optimization. The convergence of this hierarchy is shown under certain conditions. In particular, the classical semi-infinite programs can be solved as a special case of GSIPs. We also study GSIPs that have convex infinity constraints and show that they can be solved exactly by a single polynomial optimization relaxation. The computational efficiency is demonstrated by extensive numerical results.},
  archive      = {J_MPC},
  author       = {Hu, Xiaomeng and Nie, Jiawang},
  doi          = {10.1007/s12532-025-00280-4},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {547-583},
  shortjournal = {Math. Program. Comput.},
  title        = {Polynomial optimization relaxations for generalized semi-infinite programs},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MATRS: Heuristic methods for noisy derivative-free bound-constrained mixed-integer optimization. <em>MPC</em>, <em>17</em>(3), 505-546. (<a href='https://doi.org/10.1007/s12532-025-00281-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces MATRS, a novel matrix adaptation trust-region strategy designed to solve noisy derivative-free mixed-integer optimization problems with simple bounds in low dimensions. MATRS operates through a repeated cycle of five phases: mutation, selection, recombination, trust-region, and mixed-integer, executed in this sequence. But if in the mutation phase a new best point (the point with the lowest inexact function value among all evaluated points so far) is found, the selection, recombination, and trust-region phases are skipped. Similarly, if the recombination phase finds a new best point, the trust-region phase is skipped. The mixed-integer phase is always performed. To search for a new best point, the mutation and recombination phases use extrapolation whereas the mixed-integer phase performs a mixed-integer line search along directions estimated to go into a valley. Numerical results on several collections of test problems show that MATRS is competitive with state-of-the-art derivative-free mixed-integer solvers.},
  archive      = {J_MPC},
  author       = {Kimiaei, Morteza and Neumaier, Arnold},
  doi          = {10.1007/s12532-025-00281-3},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {505-546},
  shortjournal = {Math. Program. Comput.},
  title        = {MATRS: Heuristic methods for noisy derivative-free bound-constrained mixed-integer optimization},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Globally solving concave quadratic programs via doubly nonnegative relaxation. <em>MPC</em>, <em>17</em>(3), 451-503. (<a href='https://doi.org/10.1007/s12532-025-00279-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of maximizing a convex quadratic function over a bounded polyhedral set. We design a new framework based on SDP relaxations and cutting plane methods for solving the associated reference value problem. The major novelty is a new way to generate valid cuts through the doubly nonnegative (DNN) relaxation. We establish various theoretical properties of the DNN relaxation, including its equivalence with the Shor relaxation of an equivalent quadratically constrained problem, the strong duality, and the generation of valid cuts from an approximate solution of the DNN relaxation returned by an arbitrary SDP solver. Computational results on both real and synthetic data demonstrate the efficiency of the proposed method and its ability to solve high-dimensional problems with dense data. In particular, our new algorithm successfully solves in 3 days the reference value problem arising from computational biology for a dataset containing more than 300,000 instances of dimension 78. In contrast, CPLEX or Gurobi is estimated to require years of computational time for the same dataset on the same computing platform.},
  archive      = {J_MPC},
  author       = {Qu, Zheng and Zeng, Tianyou and Lou, Yuchen},
  doi          = {10.1007/s12532-025-00279-x},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {451-503},
  shortjournal = {Math. Program. Comput.},
  title        = {Globally solving concave quadratic programs via doubly nonnegative relaxation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to use local cuts. <em>MPC</em>, <em>17</em>(3), 437-450. (<a href='https://doi.org/10.1007/s12532-025-00278-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential component in modern solvers for mixed-integer (linear) programs (MIPs) is the separation of additional inequalities (cutting planes) to tighten the linear programming relaxation. Various algorithmic decisions are necessary when integrating cutting plane methods into a branch-and-bound (B&B) solver as there is always the trade-off between the efficiency of the cuts and their overhead, given that they tend to slow down the solution time of the relaxation. One of the most crucial questions is: Should cuts only be generated globally at the root or also locally at nodes of the tree? We address this question by a machine learning approach for which we train a regression forest to predict the speed-up (or slow-down) provided by using local cuts. We demonstrate with an open implementation that this helps to improve the performance of the FICO Xpress MIP solver on a public test set of general MIP instances. We further report on the impact of a practical implementation inside Xpress on a large, diverse set of real-world industry MIPs.},
  archive      = {J_MPC},
  author       = {Berthold, Timo and Francobaldi, Matteo and Hendel, Gregor},
  doi          = {10.1007/s12532-025-00278-y},
  journal      = {Mathematical Programming Computation},
  month        = {9},
  number       = {3},
  pages        = {437-450},
  shortjournal = {Math. Program. Comput.},
  title        = {Learning to use local cuts},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="mva">MVA - 6</h2>
<ul>
<li><details>
<summary>
(2025). ConcatPhys: A dual-channel path data concatenation network for robust remote heart rate estimation. <em>MVA</em>, <em>36</em>(6), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01737-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial video-based Blood Volume Pulse (BVP) signal extraction technology has demonstrated significant potential in remote health monitoring. However, most current methods are susceptible to interference from lighting changes and have limited generalization ability in dynamic or complex environments. This paper proposes a dual-channel path data concatenation network called ConcatPhys to improve the accuracy and robustness of remote heart rate (HR) estimation. First, a region-focus (RF) block is introduced to concentrate on spatial attention mechanisms, focusing on physiologically relevant regions. This approach effectively uncovers subtle local feature changes and suppresses irrelevant features, reducing sensitivity to background noise and lighting variations. Second, a dual-path framework is constructed for remote photoplethysmography (rPPG) signal prediction. By incorporating dual-path frequency-domain consistency loss and adjacent-frame similarity loss, the network’s anti-interference capability against illumination variations such as lighting changes is strengthened. Finally, leveraging the temporal correlation between adjacent video frames over short intervals, three consecutive feature image segments are concatenated. By averaging the HR values of these three adjacent segments, the video-level HR is computed. This approach enables efficient reconstruction of rPPG signals and accurate HR estimation using only a 6-s facial video segment. Experimental results demonstrate that ConcatPhys achieves state-of-the-art performance across multiple public datasets (VIPL-HR, OBF, UBFC-rPPG), highlighting its significant potential for remote health monitoring applications.},
  archive      = {J_MVA},
  author       = {Ge, Xiaorui and Xing, Jiahe and Li, Bin},
  doi          = {10.1007/s00138-025-01737-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ConcatPhys: A dual-channel path data concatenation network for robust remote heart rate estimation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the potential of deep learning techniques for analyzing athlete movements in competitive athletics sports. <em>MVA</em>, <em>36</em>(6), 1-20. (<a href='https://doi.org/10.1007/s00138-025-01733-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research work, a Deep Learning (DL) approach utilizing Spatial Transformer, Temporal Transformer, and Collaborative Movement Centric (CMC) module is presented to classify sports event based on athlete movements in competitive sports. Primarily, the Spatial Transformer utilizing the Spatial Feature extraction module (SFEM) is introduced to extract detailed spatial information from video frames. Here, the SFEM employs Modulated Moving Average Graph Convolutional Network (MMA-GCN) to extract complex spatial relationships by learning the knowledge from offset and modulation parameters. Secondly, the Temporal Transformer is designed that employs Temporal Feature extraction module (TFEM) module to extract long-range temporal dependencies and model evolution across consecutive frames. Lastly, the CMC module combines spatial and temporal information into a unified representation which is used to perform sports events categorization based on athlete movements. The proposed model is evaluated on Olympic sports dataset and University of Central Florida’s (UCF) Sports dataset across distinct evaluation measures. The model achieved 98.36% accuracy, 99.42% precision, 98.42% recall, 98.91% F1-score on the Olympic Sports dataset and 98.64% accuracy, 98.45% precision, 98.91% recall, and 98.68% F1-score on the UCF Sports dataset. Moreover, this method showed supreme results compared with existing techniques in all metrics, demonstrating its effectiveness and potential for high- applications in sports analytics and athlete monitoring.},
  archive      = {J_MVA},
  author       = {Gao, Yilun and Zou, Jie and Zhang, Yuexin},
  doi          = {10.1007/s00138-025-01733-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Exploring the potential of deep learning techniques for analyzing athlete movements in competitive athletics sports},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained 3D vehicle shape manipulation via latent space editing. <em>MVA</em>, <em>36</em>(6), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01739-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant potential of 3D object editing to impact various industries, recent research in 3D generation and editing has primarily focused on converting text and images into 3D models, often paying limited attention to the need for fine-grained control over existing 3D objects. This paper introduces a framework that uses a pre-trained regressor to enable continuous and attribute-specific modifications of both the stylistic and geometric attributes of 3D vehicle models. Here, “fine-grained control” refers to the ability to adjust specific geometric or stylistic attributes (such as roof length or perceived luxury) in a continuous and independent manner. Our method aims to preserve the identity of vehicle 3D objects and support multi-attribute editing, allowing for extensive customization while maintaining the model’s structural integrity. The framework leverages DeepSDF to obtain latent representations suitable for continuous attribute editing. Experimental results demonstrate the effectiveness of our approach in achieving detailed, controlled edits on a variety of vehicle 3D models. The code is released at https://github.com/JiangDong-miao/Vehicle_LatentEdit .},
  archive      = {J_MVA},
  author       = {Miao, JiangDong and Ikeda, Tatsuya and Raytchev, Bisser and Mizoguchi, Ryota and Hiraoka, Takenori and Nakashima, Takuji and Shimizu, Keigo and Higaki, Toru and Kaneda, Kazufumi},
  doi          = {10.1007/s00138-025-01739-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fine-grained 3D vehicle shape manipulation via latent space editing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ED-ViTTL: Ensemble vision transformer and transfer learning approach for brain tumor classification. <em>MVA</em>, <em>36</em>(6), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01741-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of brain tumors from magnetic resonance imaging (MRI) remains a challenging task due to the inherent heterogeneity of tumor morphology, class imbalance within datasets, and the limitations of individual deep learning models. To address these challenges, we propose ED-ViTTL (Ensembled Deep Vision Transformer and Transfer Learning), a hybrid framework that leverages both local and global feature representations to enhance diagnostic performance. The model integrates five advanced variants of the Vision Transformer (R50-ViT-L/16, ViT-L/16, ViT-L/32, ViT-B/16, and ViT-B/32) alongside a transfer-learned VGG19 convolutional neural network. Feature embeddings extracted from the ViT and CNN branches are fused through fully connected layers, enabling robust classification into four categories: glioma, meningioma, pituitary tumor, and healthy brain. Experiments were conducted on a publicly available dataset comprising 3264 MRI scans, partitioned into training (70%), validation (15%), and testing (15%) sets using stratified sampling. To mitigate class imbalance and improve model generalization, we employed stratified 5-fold cross-validation, class-weighted categorical cross-entropy, and extensive data augmentation. The best-performing ensemble configuration (ViT-B/32 + VGG19) achieved a classification accuracy of 98.67%, with class-specific AUC values exceeding 0.99 and ROC curves demonstrating clear inter-class separability. Performance metrics, including precision, recall, and F1-scores, remained consistently high across folds, with the confusion matrix indicating minimal misclassifications. These findings demonstrate that ED-ViTTL delivers stable and reproducible results, underscoring its potential as a reliable computer-aided diagnostic tool for assessing brain tumors in clinical practice.},
  archive      = {J_MVA},
  author       = {Thakur, Amit and Patnaik, Pawan Kumar and Kumar, Manoj and Choudhary, Chaitali},
  doi          = {10.1007/s00138-025-01741-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ED-ViTTL: Ensemble vision transformer and transfer learning approach for brain tumor classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editing implicit and explicit representations of radiance fields: A survey. <em>MVA</em>, <em>36</em>(6), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01742-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent years by offering a new volumetric representation, which is compact and provides high-quality image rendering. However, the methods to edit those radiance fields developed slower than the many improvements to other aspects of NeRF. With the recent development of alternative radiance field-based representations inspired by NeRF as well as the worldwide rise in popularity of text-to-image models, many new opportunities and strategies have emerged to provide radiance field editing. In this paper, we deliver a comprehensive survey of the different editing methods present in the literature for NeRF and other similar radiance field representations. We propose a new taxonomy for classifying existing works based on their editing methodologies, review pioneering models, reflect on current and potential new applications of radiance field editing, and compare state-of-the-art approaches in terms of editing options and performance.},
  archive      = {J_MVA},
  author       = {Hubert, Arthur and Elghazaly, Gamal and Frank, Raphaël},
  doi          = {10.1007/s00138-025-01742-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Editing implicit and explicit representations of radiance fields: A survey},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loid: Lane occlusion inpainting and detection for enhanced autonomous driving systems. <em>MVA</em>, <em>36</em>(6), 1-11. (<a href='https://doi.org/10.1007/s00138-025-01744-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate lane segmentation is essential for effective path planning and lane following in autonomous driving, especially in scenarios with significant occlusion from vehicles and pedestrians. Existing models often struggle under such conditions, leading to unreliable navigation and safety risks. We propose two innovative approaches to enhance lane detection in these challenging environments, each showing notable improvements over conventional methods. The first approach aug-Segment improves conventional lane detection models by augmenting the training dataset (e.g. CULanes) with simulated occlusions and training a segmentation model. This method achieves a 12% improvement over a number of SOTA models on the CULanes dataset. Additionally, we present a second approach, LOID: Lane Occlusion Inpainting and Detection, designed to address the issue more comprehensively and with improved robustness. LOID introduces an advanced lane segmentation network that uses an image processing pipeline to identify and mask occluded regions. An inpainting model is then applied to reconstruct the road environment in the occluded areas. The enhanced image is then processed by a lane detection algorithm, resulting in a 20% and 24% improvement over several SOTA models on the BDDK100 and CULanes datasets respectively, highlighting the effectiveness of the proposed approach.},
  archive      = {J_MVA},
  author       = {Agrawal, Aayush and Sivakumar, Ashmitha Jaysi and Kaif, Ibrahim and Banerjee, Chayan},
  doi          = {10.1007/s00138-025-01744-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Loid: Lane occlusion inpainting and detection for enhanced autonomous driving systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="naco">NACO - 27</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Distinguishing genelet circuit input pulses via a pulse detector. <em>NACO</em>, <em>24</em>(3), 797. (<a href='https://doi.org/10.1007/s11047-025-10036-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NACO},
  author       = {Yancey, Colin and Schulman, Rebecca},
  doi          = {10.1007/s11047-025-10036-7},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {797},
  shortjournal = {Nat. Comput.},
  title        = {Correction: Distinguishing genelet circuit input pulses via a pulse detector},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On recognizing graphs representing persistent perfect phylogenies. <em>NACO</em>, <em>24</em>(3), 781-795. (<a href='https://doi.org/10.1007/s11047-025-10039-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Persistent Perfect phylogeny, also known as Dollo- $$1$$ , has been introduced as a generalization of the well-known perfect phylogenetic model for binary characters to deal with the potential loss of characters. In Bonizzoni et al. (2012) it has been proved that the problem of deciding the existence of a Persistent Perfect phylogeny can be reduced to the one of recognizing a class of bipartite graphs whose nodes are species and characters. Thus an interesting question is solving directly the problem of recognizing such graphs. We present a polynomial-time algorithm for deciding Persistent Perfect phylogeny existence in maximal graphs, where no character’s species set is contained within another character’s species set. Our solution, that relies only on graph properties, narrows the gap between the linear-time simple algorithm for Perfect Phylogeny and the NP-hardness results for the Dollo-k phylogeny with $$k>1$$ .},
  archive      = {J_NACO},
  author       = {Bonizzoni, Paola and Della Vedova, Gianluca and Soto Gomez, Mauricio and Trucco, Gabriella},
  doi          = {10.1007/s11047-025-10039-4},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {781-795},
  shortjournal = {Nat. Comput.},
  title        = {On recognizing graphs representing persistent perfect phylogenies},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Palindromes in adjacent factor swap of a word. <em>NACO</em>, <em>24</em>(3), 763-780. (<a href='https://doi.org/10.1007/s11047-025-10038-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conjugate or a cyclic permutation of a word is obtained by decomposing the word into two adjacent factors and swapping them. The adjacent factor swap of a word is a generalization of the concept of cyclic permutation of a word wherein, a subword of the word is decomposed in to two non-overlapping factor parts and swapped. Since there can be multiple ways in which a word can be divided into factors, the adjacent factor swap of a word forms a set. In this paper, we study some properties of adjacent factor swap of a word. We characterize words with the minimum and the maximum number of elements in their adjacent factor swap. We then study the distribution of palindromes in the adjacent factor swap of a word. For a palindrome w, we give a tight bound on the maximum number of palindromes in the adjacent swap of w, if the number of distinct letters in w is half its length. We also provide a slack bound on the maximum number of palindromes in the adjacent swap of a given word.},
  archive      = {J_NACO},
  author       = {Mahalingam, Kalpana},
  doi          = {10.1007/s11047-025-10038-5},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {763-780},
  shortjournal = {Nat. Comput.},
  title        = {Palindromes in adjacent factor swap of a word},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-assembly of patterns in the abstract tile assembly model. <em>NACO</em>, <em>24</em>(3), 731-761. (<a href='https://doi.org/10.1007/s11047-025-10035-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the abstract Tile Assembly Model, self-assembling systems consisting of tiles of different colors can form structures on which colored patterns are “painted.” We explore the complexity, in terms of the numbers of unique tile types required, of assembling various patterns. We first demonstrate how to efficiently self-assemble a set of simple patterns, then show tight bounds on the tile type complexity of self-assembling multi-colored patterns on the surfaces of square assemblies. Finally, we demonstrate an exponential gap in tile type complexity of self-assembling an infinite series of patterns between systems restricted to one plane versus those allowed two planes. This paper is an expansion over a version in the proceedings of the 21st International Conference on Unconventional Computation and Natural Computation (UCNC 2024), with several improved results and full details of all proofs.},
  archive      = {J_NACO},
  author       = {Drake, Phillip and Patitz, Matthew J. and Summers, Scott M. and Tracy, Tyler},
  doi          = {10.1007/s11047-025-10035-8},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {731-761},
  shortjournal = {Nat. Comput.},
  title        = {Self-assembly of patterns in the abstract tile assembly model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reaction systems with nondeterministic behaviour. <em>NACO</em>, <em>24</em>(3), 719-730. (<a href='https://doi.org/10.1007/s11047-025-10034-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of a reaction system is usually achieved by applying a maximal set of reactions in a deterministic manner. In this paper, we consider reaction systems characterized by nondeterministic behaviour, where the set of applied reactions does not contain all enabled reactions, but only a subset of them based on specific constraints, particularly emphasizing on asynchronous reaction systems and restricted reaction systems. The nondeterministic approach facilitates a more realistic modelling of complex systems, where multiple potential behaviours can arise from a given set of reactions. Our aim is to explore different types of nondeterminism in reaction systems, investigating their behavioural properties, and examining the connections between the behaviour of asynchronous reaction systems and the behaviour of restricted reaction systems.},
  archive      = {J_NACO},
  author       = {Aman, Bogdan and Ciobanu, Gabriel},
  doi          = {10.1007/s11047-025-10034-9},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {719-730},
  shortjournal = {Nat. Comput.},
  title        = {Reaction systems with nondeterministic behaviour},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling equivalence and its cover relation for reaction systems. <em>NACO</em>, <em>24</em>(3), 707-718. (<a href='https://doi.org/10.1007/s11047-025-10033-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reaction system consists of a background set of entities and a set of reactions. Reactions are specified by three sets of entities: reactants, inhibitors, and products. A reaction is enabled by a state (a subset of entities), if all its reactants are present in that state and none of its inhibitors. The result of a set of reactions on a given state is a new state that consists of the products of the reactions that were enabled at the original state. In this paper, we further investigate enabling equivalence. This relation equates two sets of reactions for which the states that enable all their reactions simultaneously, are the same and, moreover, their results on those states are the same. From the point of view of enabling equivalence, sets of reactions act as if they were a single (combined) reaction. We show how combined reactions characterize enabling equivalence classes. Furthermore, enabling equivalence induces a partial order in the form of a cover relation on its equivalence classes. The resulting partially ordered set turns out to be a lattice and we demonstrate how this lattice relates to the enabling cover relation introduced earlier for single reactions.},
  archive      = {J_NACO},
  author       = {Genova, Daniela and Hoogeboom, Hendrik Jan and Kleijn, Jetty},
  doi          = {10.1007/s11047-025-10033-w},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {707-718},
  shortjournal = {Nat. Comput.},
  title        = {Enabling equivalence and its cover relation for reaction systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the convergence rate of online quantum state estimation algorithms considering disturbance and noise. <em>NACO</em>, <em>24</em>(3), 693-706. (<a href='https://doi.org/10.1007/s11047-025-10031-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convergence rate of the online quantum state fliter (OQSF) algorithm for quantum state estimation in the presence of Gaussian measurement noise and sparse disturbance is investigated in this paper. For the OQSF algorithm, by defining the average loss function of the optimization function and constraint conditions during T iterations, we derive and prove the convergence rate theorem for two loss functions. The theorem leads to the convergence rate of the normalized distance in the quantum state density matrix for the OQSF algorithm. Finally, in numerical simulations, we employ the algorithm for online estimation of a 4-qubit quantum system, using the normalized distance of the density matrix as a metric. The algorithm is compared with two existing theoretical studies on online quantum state estimation, validating the derived convergence rate performance and the superiority of the OQSF algorithm.},
  archive      = {J_NACO},
  author       = {Cong, Shuang and Qin, Weiyi},
  doi          = {10.1007/s11047-025-10031-y},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {693-706},
  shortjournal = {Nat. Comput.},
  title        = {Research on the convergence rate of online quantum state estimation algorithms considering disturbance and noise},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On decidability of problems involving insertion operations. <em>NACO</em>, <em>24</em>(3), 679-691. (<a href='https://doi.org/10.1007/s11047-025-10032-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past, there have been many different string insertion operations—often inspired by biological or physical systems—that have been defined and studied using formal language theory. For each such insertion operation, it is common to state various decision problems using important computational models, and to study whether these problems are decidable or not. Here, we generalize this approach by studying any possible insertion operation that can be defined as long as they produce every string in some lower bound language and no strings outside some upper bound language. We create two different such classes of insertion operations called general contextual insertion operations and general concatenative insertion operations. Several problems (so-called language equations) are shown to be undecidable no matter how an insertion operation is defined, so long as it belongs to one of these classes of operations. Several problems are also shown to be decidable, as long as the insertion operation satisfies some simple conditions. This general approach covers many different insertion operations that have been defined and studied in the literature, from classical operations such as concatenation and shuffle, to bio-inspired insertion operations, and we are able to demonstrate many stronger decidability results than what was previously known.},
  archive      = {J_NACO},
  author       = {Ibarra, Oscar H. and McQuillan, Ian},
  doi          = {10.1007/s11047-025-10032-x},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {679-691},
  shortjournal = {Nat. Comput.},
  title        = {On decidability of problems involving insertion operations},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on learning models of spiking neural membrane systems. <em>NACO</em>, <em>24</em>(3), 665-677. (<a href='https://doi.org/10.1007/s11047-025-10026-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural P systems (SN P systems) are a mathematical model of neural networks, abstracting the way biological neurons communicate with spikes, developed within the framework of the membrane computing theory. Recently, driven by the boom of learning neural models, SN P systems have become a rapidly emerging research front. Consequently, many different variants of the learning models of SN P system prevail among the new research results. Although large proprietary deep learning models are still based on the continuous neural network paradigm, spiking neurons are attractive because of their low-energy demands. The purpose of this paper is to provide an up-to-date overview of learning paradigms and techniques for SN P systems. After a brief introduction of the structure and function of SN P systems, we summarise recent approaches to learning and adaptation in SN P systems, including Hebbian learning, Widrow-Hoff algorithm, fuzzy approaches, nonlinear SN P systems, gated and long short-term memory inspired SN P systems, convolutional SN P systems, and more.},
  archive      = {J_NACO},
  author       = {Sosík, Petr and Paul, Prithwineel and Ciencialová, Lucie},
  doi          = {10.1007/s11047-025-10026-9},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {665-677},
  shortjournal = {Nat. Comput.},
  title        = {A survey on learning models of spiking neural membrane systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving 5 g base station placement through precise rooftop detection using super-resolution diffusion models and satellite image analysis. <em>NACO</em>, <em>24</em>(3), 651-663. (<a href='https://doi.org/10.1007/s11047-025-10030-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate deployment of 5 G base stations (BSs) in urban environments is essential for achieving optimal network performance. In these scenarios, the most common positions for installing BSs are rooftops, which, however, given the complex topography and diverse building structures, present significant challenges when identifying suitable locations. This paper proposes an enhanced method for rooftop detection, integrating diffusion models based on super-resolution with segmentation using convolutional neural networks. Starting from the input image, a super-resolution model is applied to generate sliding windows on which re-inference is performed, thereby improving both the resolution and prediction accuracy for this type of object. By refining these detections, the placement of 5 G base stations is undertaken in a practical, industrial way, thus allowing network operators to perform a more real-world network optimization. The results demonstrate a significant improvement in detection accuracy, directly contributing to more efficient 5 G base station deployment in densely populated urban areas. This methodology offers a scalable, adaptable, and effective solution based on the context of the images it applies to.},
  archive      = {J_NACO},
  author       = {García-Aguilar, Iván and Galeano-Brajones, Jesús and Luna-Valero, Francisco and Carmona-Murillo, Javier and Fernández-Rodríguez, Jose David and Luque-Baena, Rafael Marcos},
  doi          = {10.1007/s11047-025-10030-z},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {651-663},
  shortjournal = {Nat. Comput.},
  title        = {Improving 5 g base station placement through precise rooftop detection using super-resolution diffusion models and satellite image analysis},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A heuristic dataset reduction for green computing of photovoltaic power generation prediction. <em>NACO</em>, <em>24</em>(3), 637-649. (<a href='https://doi.org/10.1007/s11047-025-10029-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) has become increasingly integrated into everyday life, with the general population progressively relying on it for even routine tasks. As AI models grow in complexity, precision, and computational power, their energy consumption rises exponentially, raising serious concerns about the sustainability of their widespread adoption. The field of green learning aims to mitigate these concerns by developing energy-efficient AI solutions. In this work, we propose a method to preserve the accuracy of photovoltaic (PV) power generation forecasting while reducing the environmental impact through dataset size reduction. Our approach employs a heuristic strategy that iteratively reduces the training dataset until a cutoff point is reached, balancing predictive accuracy and environmental efficiency. Experimental results using publicly available PV generation datasets demonstrate that the proposed data reduction method decreases training time by up to 17.13%, with only a 1.47% decline in prediction accuracy. These findings highlight the potential of the method to substantially reduce the carbon footprint of AI applications with minimal performance degradation.},
  archive      = {J_NACO},
  author       = {Aravena-Cifuentes, Ana Paula and Nuñez-Gonzalez, J. David and Graña, Manuel},
  doi          = {10.1007/s11047-025-10029-6},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {637-649},
  shortjournal = {Nat. Comput.},
  title        = {A heuristic dataset reduction for green computing of photovoltaic power generation prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protein structure refinement with a memetic algorithm. <em>NACO</em>, <em>24</em>(3), 619-635. (<a href='https://doi.org/10.1007/s11047-025-10027-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A memetic approach to protein structure refinement was defined, combining Differential Evolution and the widely used Rosetta Relax refinement protocol. This refinement process can be considered as an optimization problem to optimize the positions of the amino acid atoms of the protein. The local optimization procedures of Rosetta Relax are integrated into the evolutionary algorithm. The results with different proteins show that the memetic algorithm better samples the energy landscape compared to Rosetta Relax, obtaining better energy-optimized refined conformations in the same runtime.},
  archive      = {J_NACO},
  author       = {Filgueiras, Juan Luis and Santos, José},
  doi          = {10.1007/s11047-025-10027-8},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {619-635},
  shortjournal = {Nat. Comput.},
  title        = {Protein structure refinement with a memetic algorithm},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Genetic programming policies for bin packing in the framework of deterministic markov decision process. <em>NACO</em>, <em>24</em>(3), 603-617. (<a href='https://doi.org/10.1007/s11047-025-10028-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bin Packing Problem (BPP) is a well-known NP-hard problem with numerous real-world applications. This study focuses on minimizing waste and maximum lateness in a one-dimensional version of the BPP, which is particularly relevant in industrial contexts. The goal is to develop a constructive heuristic algorithm that can be adapted to various situations. We model the BPP as a Deterministic Markov Decision Process with discrete state and action spaces, where policies are represented by arithmetic expressions involving state variables. This approach allows for a clearer explanation of the decision process, in contrast to other methods like neural networks. To evolve these policies, we use Genetic Programming (GP). Trained on a set of BPP instances, the resulting policies are effective for solving new, unseen instances. In the experimental study, we explore different GP settings, including varying sets of symbols. The results reveal valuable insights about the importance of state variables, indicating that a smaller selection of them may yield the best results. The evolved policies are compared with an exact method from the literature, achieving similar outcomes but with significantly less computational time.},
  archive      = {J_NACO},
  author       = {Quesada, Jesús and Gil-Gala, Francisco J. and Durasević, Marko and Sierra, María R. and Varela, Ramiro},
  doi          = {10.1007/s11047-025-10028-7},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {603-617},
  shortjournal = {Nat. Comput.},
  title        = {Genetic programming policies for bin packing in the framework of deterministic markov decision process},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inductive inference of lindenmayer systems: Algorithms and computational complexity. <em>NACO</em>, <em>24</em>(3), 591-601. (<a href='https://doi.org/10.1007/s11047-025-10024-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lindenmayer systems (L-systems) are string rewriting systems that can model and be used to create simulations of processes with inherent parallelism and self-similarity. Inference of L-systems involves the automated learning of these models/grammars from data; and inductive inference involves learning an L-system from a sequence of strings initially generated by an unknown L-system. This paper studies the computational complexity of inductive inference of a variety of different types of context-free L-systems (deterministic or nondeterministic, tabled or not, and allowing erasing or not). Because this inference is sometimes trivial for nondeterministic L-systems, it is more useful to find the smallest L-system that can generate the sequence of strings, in terms of either the number of rewriting rules, or (when there are tables), the number of tables. For all of the types of L-systems studied, finding an L-system with the smallest number of rewriting rules is $$\textsf {NP}$$ -complete. However, in all cases, if the number of rewriting rules is fixed, then finding an L-system of any type studied, or finding the smallest L-systems in terms of the number of rewriting rules, or the smallest in terms of the number of tables, can always be done in polynomial time.},
  archive      = {J_NACO},
  author       = {Duffy, Christopher and Hillis, Sam and Khan, Umer and McQuillan, Ian and Shan, Sonja Linghui},
  doi          = {10.1007/s11047-025-10024-x},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {591-601},
  shortjournal = {Nat. Comput.},
  title        = {Inductive inference of lindenmayer systems: Algorithms and computational complexity},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation of programmable matter systems using active tile-based self-assembly. <em>NACO</em>, <em>24</em>(3), 571-590. (<a href='https://doi.org/10.1007/s11047-025-10025-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-assembly refers to the process by which small, simple components mix and combine to form complex structures using only local interactions. Designed as a hybrid between tile assembly models and cellular automata, the Tile Automata (TA) model was recently introduced as a platform to help study connections between various models of self-assembly. However, in this paper we present a result in which we use TA to simulate arbitrary systems within the amoebot model, a theoretical model of programmable matter in which the individual components are relatively simple state machines that are able to sense the states of their neighbors and to move via series of expansions and contractions. We show that for every amoebot system, there is a TA system capable of simulating the local information transmission built into amoebot particles, and that the TA “macrotiles” used to simulate its particles are capable of simulating movement (via attachment and detachment operations) while maintaining the necessary properties of amoebot particle systems. The TA systems are able to utilize only the local interactions of state changes and binding and unbinding along tile edges, but are able to fully simulate the dynamics of these programmable matter systems. Since the signal-passing tile assembly model, a mathematical model of “active” DNA-based tile self-assembly, has been shown to be able to simulate the TA model, our result provides a bridge showing how DNA-based self-assembling tiles can simulate the behavior of the amoebot model of programmable matter.},
  archive      = {J_NACO},
  author       = {Alumbaugh, John Calvin and Daymude, Joshua J. and Demaine, Erik D. and Patitz, Matthew J. and Richa, Andréa W.},
  doi          = {10.1007/s11047-025-10025-w},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {571-590},
  shortjournal = {Nat. Comput.},
  title        = {Simulation of programmable matter systems using active tile-based self-assembly},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-aware cooperative multi-fitness evolutionary algorithm for workflow scheduling in cloud computing. <em>NACO</em>, <em>24</em>(3), 557-570. (<a href='https://doi.org/10.1007/s11047-025-10023-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing energy consumption of cloud infrastructure has attained levels that are no longer viable, necessitating the development of energy-aware scheduling algorithms. This work focuses on optimising the scheduling of scientific workflows, which requires extensive computation to achieve time-efficient results, often at the cost of excessive energy consumption. To address this challenge, a multi-fitness evolutionary algorithm that integrates multiple heuristic functions in a cooperative manner to minimise energy consumption is proposed. The approach not only facilitates the reuse of heuristics but also provides novel insights into the interplay between energy consumption and makespan, traditionally viewed as conflicting objectives. This flexible framework demonstrates its adaptability for optimising both total energy consumption and completion time, offering a robust tool for sustainable workflow scheduling.},
  archive      = {J_NACO},
  author       = {Barredo, Pablo and Puente, Jorge},
  doi          = {10.1007/s11047-025-10023-y},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {557-570},
  shortjournal = {Nat. Comput.},
  title        = {Energy-aware cooperative multi-fitness evolutionary algorithm for workflow scheduling in cloud computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Algorithms for maximal existential and universal width. <em>NACO</em>, <em>24</em>(3), 541-556. (<a href='https://doi.org/10.1007/s11047-025-10022-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximal existential width and maximal universal width provide methods of quantifying the amount of nondeterminism and parallelism, respectively, present in computations of an alternating finite automaton (AFA). In this paper, we primarily seek to understand the complexity landscape of the problems pertaining to computing these widths. One such problem involves deciding whether the maximal universal width or maximal existential width of an AFA is bounded by a given integer k. For the case of a nondeterministic finite automaton (NFA), we give a polynomial time algorithm for computing maximal existential width. We also present a polynomial time algorithm for computing the existential width of a general AFA, under the assumption that the AFA’s universal width is bounded by a fixed integer $$\ell$$ . Finally, we reconsider the problems of computing existential and universal widths for AFAs over a unary language and demonstrate that both problems can be solved in polynomial time.},
  archive      = {J_NACO},
  author       = {Alajaji, John and Salomaa, Kai},
  doi          = {10.1007/s11047-025-10022-z},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {541-556},
  shortjournal = {Nat. Comput.},
  title        = {Algorithms for maximal existential and universal width},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing languages of polymorphic p systems by parallel communicating lindenmayer systems. <em>NACO</em>, <em>24</em>(3), 529-540. (<a href='https://doi.org/10.1007/s11047-025-10021-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We continue the investigation of the computational power of non-cooperative polymorphic P systems with no ingredients (no target indicators or any special features added to the rules) in terms of parallel communicating Lindenmayer systems. We precisely characterize the language class generated by these types of P systems using a restricted class of parallel communicating ET0L systems. Our results demonstrate that the dynamically changing rewriting rules provided by the polymorphic framework in non-cooperating P systems results in a similar increase in computational power as a restricted variant of the parallel communicating framework does in the power of ET0L systems.},
  archive      = {J_NACO},
  author       = {Kuczik, Anna and Vaszil, György},
  doi          = {10.1007/s11047-025-10021-0},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {529-540},
  shortjournal = {Nat. Comput.},
  title        = {Characterizing languages of polymorphic p systems by parallel communicating lindenmayer systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connectomes inform function: From time-varying dynamics to animal behaviour. <em>NACO</em>, <em>24</em>(3), 511-528. (<a href='https://doi.org/10.1007/s11047-025-10020-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structure guides computation in biological and artificial neural networks. However, the nature of the relationship between structure and function, in this context, is unclear. For example, there is still debate on whether constraining a network with biological detail confers a non-trivial functional advantage over a network without such constraints. To shine light on this topic, we highlight five experiments which employ biological constraints onto artificial neural networks using empirically-guided wiring diagrams, or connectomes, from an adult fruit fly, a larval zebrafish, and from the Mammalian MRI (MaMI) dataset, and impose these onto reservoir-based recurrent neural networks, while studying changes in performance and prediction dynamics on synthetic and naturalistic time series data. We observe that fly-constrained networks are better at making predictions from chaotic input data, and in executing multiple mutually exclusive tasks simultaneously, all with a robustness to hyperparameter variations, some of which may lead to chaos. Separately, we find that the global clustering coefficient of the fly network improves performance and variance on time-varying predictions. We also report that an empirical functional connectome from the optomotor response circuitry of a larval zebrafish validates its own behaviour, and that this is interrupted by rewiring. Finally, using the MaMI dataset, we determine that rewiring degrades multifunctional capacity, and that more multifunctional networks have a higher mean degree centrality. Collectively, these findings suggest that biological topology constraints confer distinct advantages to arbitrarily-weighted networks.},
  archive      = {J_NACO},
  author       = {Morra, Jacob and Fouke, Kaitlyn and Naumann, Eva A. and Daley, Mark},
  doi          = {10.1007/s11047-025-10020-1},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {511-528},
  shortjournal = {Nat. Comput.},
  title        = {Connectomes inform function: From time-varying dynamics to animal behaviour},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ants on the highway. <em>NACO</em>, <em>24</em>(3), 497-509. (<a href='https://doi.org/10.1007/s11047-025-10018-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform intensive simulations of the model known as Generalised Langton’s Ants, discovering rules with a large number of highways. We depict the structure of some of them, formally proving that there is no universal bound on the number of highways which are possible for a given ant rule, and even that some ant rules have infinitely many highways. In particular we disprove the belief that the propagation speed of a given ant’s highways is uniquely determined. Our simulations show that, for some rules, different highways appear with very unequal frequencies, in some cases only a few times in a billion runs. This suggests that those highways that appear as the only possible asymptotic behaviour of some rules, might be accompanied by several very infrequent highways which are very hard to find.},
  archive      = {J_NACO},
  author       = {Gajardo, Anahí and Lutfalla, Victor H. and Rao, Michaël},
  doi          = {10.1007/s11047-025-10018-9},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {497-509},
  shortjournal = {Nat. Comput.},
  title        = {Ants on the highway},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid evolutionary approach for lexicographic green flexible jobshop with interval uncertainty. <em>NACO</em>, <em>24</em>(3), 483-496. (<a href='https://doi.org/10.1007/s11047-025-10016-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the flexible job shop problem with uncertain processing times modelled by intervals. Due to climate change and the need for energy efficiency, there is an increasing interest in sustainability in addition to traditional production-related objectives such as makespan. In this work, we tackle a lexicographical goal programming scenario minimising makespan firstly and total energy consumption lately. We propose a hybrid evolutionary algorithm based on a genetic algorithm, incorporating heuristic seeding and a post-processing step using constraint programming. The experimental study shows that the proposed approach is able to meet tighter makespan goals than previously published methods, while offering a 32% improvement in energy consumption when goals are met.},
  archive      = {J_NACO},
  author       = {Afşar, Sezin and Puente, Jorge and Palacios, Juan José and González-Rodríguez, Inés and Vela, Camino R.},
  doi          = {10.1007/s11047-025-10016-x},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {483-496},
  shortjournal = {Nat. Comput.},
  title        = {A hybrid evolutionary approach for lexicographic green flexible jobshop with interval uncertainty},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A directed graph allowing for the exploration of the set of number-conserving non-uniform one-dimensional binary cellular automata with radius one and half. <em>NACO</em>, <em>24</em>(3), 469-482. (<a href='https://doi.org/10.1007/s11047-025-10015-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main obstacle in the quest for non-uniform cellular automata that meet the often desired property of number conservation is the vast size of the search space, going far beyond the capabilities of today’s computers. In this paper, we expound the construction of a directed graph $$\Pi $$ related to the set of all number-conserving non-uniform one-dimensional binary cellular automata with radius one and half (i.e., the neighborhood of a cell consists of four cells). We show that there is a one-to-one correspondence between the set of all such cellular automata on a finite grid with n cells and the set of all length-n closed directed walks in $$\Pi $$ . This provides us with a powerful tool to investigate non-uniform cellular automata of this type.},
  archive      = {J_NACO},
  author       = {Wolnik, Barbara and Dziemiańczuk, Maciej and Makuracki, Bartosz and De Baets, Bernard},
  doi          = {10.1007/s11047-025-10015-y},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {469-482},
  shortjournal = {Nat. Comput.},
  title        = {A directed graph allowing for the exploration of the set of number-conserving non-uniform one-dimensional binary cellular automata with radius one and half},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solitaire of independence. <em>NACO</em>, <em>24</em>(3), 431-467. (<a href='https://doi.org/10.1007/s11047-025-10010-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a reversible process (more precisely, a groupoid/group action) resembling the classical 15-puzzle, where the legal moves are to “move the unique hole inside a translate of a shape S”. Such a process can be defined for any finite subset S of a group, and we refer to such a process as simply “solitaire”. We develop a general theory of solitaire, and then concentrate on the simplest possible example, solitaire for the plane $$\mathbb {Z}^2$$ , and S the triangle shape (equivalently, any three-element set in general position). In this case, we give a polynomial time algorithm that puts any finite subset of the plane in normal form using solitaire moves, and show that the solitaire orbit of a line of consecutive ones—the line orbit—is completely characterised by the notion of a so-called fill matrix. We show that the diameter of the line orbit, as a graph with edges the solitaire moves, is cubic. We show that analogous results hold for the square shape, but indicate some shapes (still on the group $$\mathbb {Z}^2$$ ) where this is less immediate. We then explain in detail the connection of the solitaire to TEP and more generally permutive subshifts. Namely, the solitaire is a closure property of various sets of subsets of the group that can be associated to such a subshift, such as the independence, spanning and filling sets.},
  archive      = {J_NACO},
  author       = {Salo, Ville and Schabanel, Juliette},
  doi          = {10.1007/s11047-025-10010-3},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {431-467},
  shortjournal = {Nat. Comput.},
  title        = {Solitaire of independence},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Networks of splicing processors: Path graph topology simulation. <em>NACO</em>, <em>24</em>(3), 419-430. (<a href='https://doi.org/10.1007/s11047-024-10008-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a direct simulation of an arbitrary network of splicing processors by a network of splicing processors having an underlying path graph. This is in line with similar simulations where the target network has other widely used graph topologies: complete graph, lattice graph, star graph, wheel graph, etc. Along with the effective construction, we provide an analysis of the size and time complexity of the obtained network. Our construction may not be the most economic conversion in terms of number of nodes, hence further investigation to find more succinct networks are of (at least) theoretical interest.},
  archive      = {J_NACO},
  author       = {Martín, José Angel Sánchez and Mitrana, Victor and Păun, Mihaela},
  doi          = {10.1007/s11047-024-10008-3},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {419-430},
  shortjournal = {Nat. Comput.},
  title        = {Networks of splicing processors: Path graph topology simulation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All optical half adder and subtractor using single 2D photonic crystal structure. <em>NACO</em>, <em>24</em>(3), 399-418. (<a href='https://doi.org/10.1007/s11047-024-10003-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work describes the design and analysis of all-optical half adder and half subtractor utilizing a two-dimensional photonic crystals-based structure. The proposal utilizes a novel approach to implement these fundamental arithmetic operations optically, aiming to enhance speed and reduce power consumptions compared to traditional electronic and optical counterparts. The structure has simulated and optimized using the Finite-Difference Time-Domain method to ensure accurate performance of the devices. The extinction ratio (ER), contrast ratio (CR), Q value (Q), amplitude modulation (AM), and transmission ratio (TR), are evaluated to assess the effectiveness of the devices. The values of these metrices are ER = 8.26 dB, CR = 12.01 dB, Q = 4.26, AM = 1.34 dB, and TR = 3.381 for the half adder Carry output ER = 10.63 dB, CR = 13.64 dB, Q = 16.8, and TR = 4.98 for the half subtractor Difference output ER = 10.24 dB, CR = 13.34 dB, Q = 5.78, AM = 0.184, and TR = 3.93, and for the half subtractor Borrow output ER = 7.45 dB, CR = 9.80 dB, Q = 7.97, and TR = 2.34. These arithmetic units may be used to design more complicated combinational and sequential devices.},
  archive      = {J_NACO},
  author       = {Maji, Kajal and Mukherjee, Kousik and Mandal, Mrinal Kanti},
  doi          = {10.1007/s11047-024-10003-8},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {399-418},
  shortjournal = {Nat. Comput.},
  title        = {All optical half adder and subtractor using single 2D photonic crystal structure},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time computing and robust memory with deterministic chemical reaction networks. <em>NACO</em>, <em>24</em>(3), 383-397. (<a href='https://doi.org/10.1007/s11047-024-09994-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research into analog computing has introduced new notions of computing real numbers. Huang, Klinge, Lathrop, Li, and Lutz defined a notion of computing real numbers in real-time with chemical reaction networks (CRNs), introducing the classes $$\mathbb {R}_\text {LCRN}$$ (the class of all Lyapunov CRN-computable real numbers) and $$\mathbb {R}_\text {RTCRN}$$ (the class of all real-time CRN-computable numbers). In their paper, they show the inclusion of the real algebraic numbers $$\text { ALG} \subseteq \mathbb {R}_\text {LCRN}\subseteq \mathbb {R}_\text {RTCRN}$$ and that $$\text { ALG} \subsetneqq \mathbb {R}_\text {RTCRN}$$ but leave open whether the inclusion is proper. In this paper, we resolve this open problem and show that $${ ALG} = \mathbb {R}_\text {LCRN}$$ and, as a consequence, $$\mathbb {R}_\text {LCRN}\subsetneqq \mathbb {R}_\text {RTCRN}$$ . However, the definition of real-time computation by Huang et al. is fragile in the sense that it is sensitive to perturbations in initial conditions. To resolve this flaw, we further require a CRN to withstand these perturbations. In doing so, we arrive at a discrete model of memory. This approach has several benefits. First, a bounded CRN may compute values approximately in finite time. Second, a CRN can tolerate small perturbations of its species’ concentrations. Third, taking a measurement of a CRN’s state only requires precision proportional to the exactness of these approximations. Lastly, if a CRN requires only finite memory, this model and Turing machines are equivalent under real-time simulations.},
  archive      = {J_NACO},
  author       = {Fletcher, Willem and Klinge, Titus H. and Lathrop, James I. and Nye, Dawn A. and Rayman, Matthew},
  doi          = {10.1007/s11047-024-09994-1},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {383-397},
  shortjournal = {Nat. Comput.},
  title        = {Real-time computing and robust memory with deterministic chemical reaction networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fractal dimension of assemblies in the abstract tile assembly model. <em>NACO</em>, <em>24</em>(3), 367-382. (<a href='https://doi.org/10.1007/s11047-023-09942-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the power of systems in the abstract Tile Assembly Model (aTAM) to self-assemble shapes having fractal dimensions between 1 and 2. We introduce the concept of sparsity as a tool for investigating such systems and demonstrate its utility by proving how it relates to fractal dimension. We then prove several results regarding the strict self-assembly of certain classes of fractal shapes in the aTAM including the construction of a universal tileset which, given the correct seed assembly, strictly self-assembles with nearly any desired fractal dimension. Additionally, we discuss a long standing conjecture in tile-assembly, that a class of fractals called discrete self-similar fractals cannot strictly self-assemble in the aTAM, and provide evidence that sparsity, rather than fractal dimension, is a more promising differentiating factor between shapes that can and cannot strictly self-assemble.},
  archive      = {J_NACO},
  author       = {Hader, Daniel and Patitz, Matthew J. and Summers, Scott M.},
  doi          = {10.1007/s11047-023-09942-5},
  journal      = {Natural Computing},
  month        = {9},
  number       = {3},
  pages        = {367-382},
  shortjournal = {Nat. Comput.},
  title        = {Fractal dimension of assemblies in the abstract tile assembly model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="nca">NCA - 28</h2>
<ul>
<li><details>
<summary>
(2025). Estimate remaining useful life for predictive railways maintenance based on LSTM autoencoder. <em>NCA</em>, <em>37</em>(27), 22967-22978. (<a href='https://doi.org/10.1007/s00521-021-06051-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, frequent maintenance and repair of mechanical equipment whose goal is to deter the suspension time of railway infrastructures are proven to be ineffectual. It also results in loss of reliability as well as consuming unnecessary means and costs, since at least half of the precautionary maintenance activities are considered redundant. Despite this spending, operators are struggling to adequately maintain their assets—resulting in unacceptably frequent delays and cancellations and low levels of satisfaction among rail users. Thanks to the increasing availability and sophistication of advanced analytics, operators have a significant opportunity to create solutions to long-standing maintenance challenges. The role of predictive maintenance and especially that of Design-Out Maintenance constitute the necessary procedure that can predict in time any hardware failures, while reducing the damage or wearing down of the overall operational equipment. This can increase the effectiveness of the railways, while significantly reducing the overall expenditure needed for the repair and maintenance of the industry’s infrastructure. This paper proposes a predictive railways maintenance strategy based on deep learning techniques. Specifically, and in order to achieve the exact remaining useful life of the railway equipment, a hybrid neural architecture of long short-term memory autoencoder network is used. The purpose of this suggested architecture is the automatic feature extraction of dynamic time series and their utilization on a prediction model, which can predict with high accuracy the remaining useful life of the railway’s mechanical equipment.},
  archive      = {J_NCA},
  author       = {Hu, Liqiang and Dai, Guoyong},
  doi          = {10.1007/s00521-021-06051-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22967-22978},
  shortjournal = {Neural Comput. Appl.},
  title        = {Estimate remaining useful life for predictive railways maintenance based on LSTM autoencoder},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I-SAMARAH, an incremental constrained clustering applied to remote sensing images. <em>NCA</em>, <em>37</em>(27), 22941-22965. (<a href='https://doi.org/10.1007/s00521-025-11161-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically extracting knowledge from diverse datasets is a valuable task that helps experts explore new types of data while reducing the time spent on manual annotations. This is particularly important for emerging fields such as emergency management and environmental monitoring. Traditional unsupervised methods often struggle to capture experts’ intuitions or integrate non-formalized knowledge. On the other hand, supervised methods typically require a substantial amount of prior knowledge to function effectively. Constrained clustering, a semi-supervised approach, addresses these challenges by allowing experts to incorporate their knowledge into the clustering process. However, it often yields suboptimal results because it is difficult for experts to provide constraints that are both informative and coherent. Building on the idea that it is easier to critique than to construct, this article introduces a novel method called I-Samarah, an incremental constrained clustering approach. This method alternates between a clustering phase, where expert-provided constraints are applied, and a critique phase, where experts provide feedback on the clustering results. Through an iterative process, the method refines the clusters, improving their alignment with expert knowledge. We empirically demonstrate the effectiveness of I-Samarah using remote sensing image time series, comparing it to other constrained clustering methods in terms of result quality and to supervised methods in terms of annotation efficiency.},
  archive      = {J_NCA},
  author       = {Lafabrègue, Baptiste and Gançarski, Pierre},
  doi          = {10.1007/s00521-025-11161-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22941-22965},
  shortjournal = {Neural Comput. Appl.},
  title        = {I-SAMARAH, an incremental constrained clustering applied to remote sensing images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online forecasting using neighbor-based incremental learning for electricity markets. <em>NCA</em>, <em>37</em>(27), 22923-22940. (<a href='https://doi.org/10.1007/s00521-024-10876-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity market forecasting is very useful for the different actors involved in the energy sector to plan both the supply chain and market operation. Nowadays, energy demand data are data coming from smart meters and have to be processed in real-time for more efficient demand management. In addition, electricity prices data can present changes over time such as new patterns and new trends. Therefore, real-time forecasting algorithms for both demand and prices have to adapt and adjust to online data in order to provide timely and accurate responses. This work presents a new algorithm for electricity demand and prices forecasting in real-time. The proposed algorithm generates a prediction model based on the k-nearest neighbors algorithm, which is incrementally updated in an online scenario considering both changes to existing patterns and adding new detected patterns to the model. Both time-frequency and error threshold based model updates have been evaluated. Results using energy demand from 2007 to 2016 and prices data for different time periods from the Spanish electricity market are reported and compared with other benchmark algorithms.},
  archive      = {J_NCA},
  author       = {Melgar-García, L. and Gutiérrez-Avilés, D. and Rubio-Escudero, C. and Troncoso, A.},
  doi          = {10.1007/s00521-024-10876-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22923-22940},
  shortjournal = {Neural Comput. Appl.},
  title        = {Online forecasting using neighbor-based incremental learning for electricity markets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-classification of brain tumors using proposed hybrid quantum–classical integrated neural network (HQCINN) models: Shallow and deep circuit approaches. <em>NCA</em>, <em>37</em>(27), 22891-22922. (<a href='https://doi.org/10.1007/s00521-025-11522-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection and accurate classification of brain tumors using MRI scans are crucial for effective diagnosis and treatment planning. However, with the growing patient population and the increasing volume of MRI data, as well as limitations like noise in image data and poor resolution, accurate and rapid diagnosis becomes challenging. To address these issues, AI systems are needed to support radiologists by offering a second opinion. Recent advancements in deep learning (DL) have significantly improved MRI-based brain tumor diagnosis. Despite these improvements, challenges such as the need for higher computational power, difficulty processing large and high-resolution datasets, and limitations of classical vector space. However, quantum computing and quantum computing-based AI methods, by leveraging properties such as superposition and entanglement, have the potential to process data in parallel, handle higher-dimensional data more efficiently, and solve certain problems that classical methods struggle with, more quickly and efficiently. In this study, we proposed four different hybrid quantum–classical integrated neural network (HQCINN) models featuring various multilayer parameterized quantum circuit architectures, which we refer to as “shallow and deep circuits,” designed based on properties such as “entanglement capability, circuit loss, and the number of trainable parameters.” These models aim to distinguish between glioma, meningioma, pituitary and non-tumor classes. The performance of these models was compared to classical DL models, revealing that quantum models provide higher accuracy and lower loss values with fewer parameters. Additionally, when the HQCINN model with the best performance was applied to a brain tumor dataset consisting of CT images, it demonstrated consistent performance across different patient data distributions and imaging modalities, thereby showing strong generalization capability. These results suggest that HQCINN approaches could provide significant advantages in medical imaging tasks, particularly in complex datasets like brain tumor classification.},
  archive      = {J_NCA},
  author       = {Akpinar, Emine and Islam, Sardar M. N. and Oduncuoglu, Murat},
  doi          = {10.1007/s00521-025-11522-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22891-22922},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-classification of brain tumors using proposed hybrid quantum–classical integrated neural network (HQCINN) models: Shallow and deep circuit approaches},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight convolutional neural network based on u shape structure and attention mechanism for anterior mediastinum segmentation. <em>NCA</em>, <em>37</em>(27), 22875-22889. (<a href='https://doi.org/10.1007/s00521-025-11515-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To automatically detect anterior mediastinum lesions (AMLs) in the anterior mediastinum (AM), an automatic segmentation model designed explicitly for AM regions in chest computed tomography (CT) scans is required. Due to the low prevalence of AML, reviewing large CT datasets retrospectively is time-consuming. Developing an artificial intelligence (AI) model to identify the AM region can help radiologists manage workloads and improve diagnostic accuracy. In this paper, we introduce a U-shaped network architecture with two attention mechanisms to maintain long-range dependencies and enhance localization. We propose a parallel multi-head self-attention (MHSA) module called wide-MHSA (W-MHSA), along with a dilated depth-wise parallel path connection (DDWPP) to support upsampling stages. Additionally, an expanding convolution block is combined with W-MHSA in the encoder to ensure a lightweight design. Our proposed model demonstrates superior segmentation performance compared to state-of-the-art networks, showing strong potential for clinical application in AM lesion detection workflows.},
  archive      = {J_NCA},
  author       = {Soleimani-Fard, Sina and Jeong, Won Gi and Ferri Ripalda, Francis and Sasani, Hasti and Choi, Younhee and Deiva, S. and Jin, Gong Yong and Ko, Seok-bum},
  doi          = {10.1007/s00521-025-11515-9},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22875-22889},
  shortjournal = {Neural Comput. Appl.},
  title        = {A lightweight convolutional neural network based on u shape structure and attention mechanism for anterior mediastinum segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demand forecasting using KAN-RNN. <em>NCA</em>, <em>37</em>(27), 22857-22874. (<a href='https://doi.org/10.1007/s00521-025-11514-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s highly competitive business environment, organizations continuously strive to maintain their competitiveness and achieve sustainable profit margins to support long-term growth and development. Accurate demand forecasting has become a critical tool for decision-makers, as it allows better resource allocation, inventory management, and strategic planning. Recurrent deep learning methods, which use gating mechanisms to maintain an internal state aligned with time series data, are among the most widely used approaches to improve forecast accuracy. Despite their success, these models still exhibit significant untapped potential that could be realized by rethinking the design of their gating mechanisms. To address this, we introduce a novel demand forecasting method inspired by Kolmogorov–Arnold networks (KANs), featuring a modified recurrent architecture with a restructured gating mechanism. This innovation leverages KAN principles to enhance the model’s capacity to capture intricate temporal dependencies and adapt to evolving demand patterns. Experimental evaluations demonstrate that the proposed method outperforms state-of-the-art approaches, highlighting its ability to provide more accurate and reliable demand forecasting results.},
  archive      = {J_NCA},
  author       = {Mejía-Muñoz, Jose-Manuel and Mederos, Boris and Avelar, Liliana and Díaz-Román, José David and Cruz-Mejia, Oliverio},
  doi          = {10.1007/s00521-025-11514-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22857-22874},
  shortjournal = {Neural Comput. Appl.},
  title        = {Demand forecasting using KAN-RNN},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient deep neural unification in symbolic processing. <em>NCA</em>, <em>37</em>(27), 22827-22855. (<a href='https://doi.org/10.1007/s00521-025-11512-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unification is indispensable for inferences in symbolic processing. The authors propose a neural network-based solution to perform efficient unification. Symbolic processing in conventional artificial intelligence has strong inference abilities but is not well suited for handling large amounts of ambiguous data. Contrastingly, neural networks can easily handle large amounts of ambiguous data but are not well suited for making complex inferences. Therefore, the authors realized the unification of the knowledge base, including ambiguous data, using a network that combines a memory network and recurrent neural network. The novelty of the proposed network is that matching, which is a unification process, is highly efficient and substitution, which is a unification process, is robust. The proposed network enables highly efficient matching by grouping multiple terms and processing them in a memory network. Furthermore, it can handle unknown words even during substitution because it uses a recurrent neural network to perform substitution. The experimental results show that the proposed network can achieve more efficient unification of ambiguous data than the baseline. This study combines symbolic processing and deep learning and suggests that it contributes to the realization of complex inferences from large amounts of ambiguous data, which has proven challenging in conventional research. Furthermore, the use of unification, which handles large amounts of ambiguous data, facilitates the development of inference systems with human-interactive interfaces. This allows humans to obtain inference results without knowing the representations of the knowledge base.},
  archive      = {J_NCA},
  author       = {Honda, Hiroshi and Hagiwara, Masafumi},
  doi          = {10.1007/s00521-025-11512-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22827-22855},
  shortjournal = {Neural Comput. Appl.},
  title        = {Efficient deep neural unification in symbolic processing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MB-ViT: MBConv vision transformer with time–frequency feature fusion for bearing fault diagnosis. <em>NCA</em>, <em>37</em>(27), 22801-22825. (<a href='https://doi.org/10.1007/s00521-025-11509-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Roller bearings play a crucial role in mechanical systems, where their operational condition directly impacts system performance and lifespan. However, detecting early-stage bearing faults during routine maintenance remains a challenge due to the cost and technical limitations of current fault diagnosis methods, often resulting in reduced accuracy. To address this issue, this paper proposes a bearing fault diagnosis method based on the MBConv Vision Transformer (MB-ViT) with frequency feature fusion. Specifically, we introduce a novel approach that transforms bearing fault signals into RGB images by combining Continuous Wavelet Transform (CWT) and Gramian Angular Summation Field (GASF) images, thereby enhancing feature representation and improving fault recognition. Additionally, recognizing the limitations of the traditional Vision Transformer (ViT) in capturing local features, we introduce the MB-Multi-Head Self-Attention (MB-MSA) module to overcome this challenge. Experimental results using data from Case Western Reserve University and Xi’an Jiaotong University show that feature fusion significantly improves fault diagnosis accuracy, while the MB-MSA module enhances both diagnostic precision and robustness. MB-ViT achieves an accuracy of 99.90 $$\%$$ in bearing fault diagnosis tasks, demonstrating its superior performance. In summary, the proposed model outperforms existing ViT-based methods, providing a promising solution for bearing fault diagnosis and supporting the advancement of next-generation industrial technologies. The code and data are available at https://github.com/viivan/MB-vit .},
  archive      = {J_NCA},
  author       = {Xiao, Gang and Yao, Junbo and Zhong, Liubing and Xiao, Zhongcheng and Lu, JiaWei},
  doi          = {10.1007/s00521-025-11509-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22801-22825},
  shortjournal = {Neural Comput. Appl.},
  title        = {MB-ViT: MBConv vision transformer with time–frequency feature fusion for bearing fault diagnosis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dependency relationship-enhanced graph convolutional network for aspect-based sentiment analysis. <em>NCA</em>, <em>37</em>(27), 22775-22800. (<a href='https://doi.org/10.1007/s00521-025-11508-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis task aims at predicting the sentiment polarity of a specific aspect in a sentence. Recent works have shown that attention-based and syntax-based approaches have gradually become mainstream methods. However, attention-based models may erroneously utilize unrelated context words as cues for prediction in sentences with long-range word dependency information. Besides, methods based on graph neural networks have been applied to model syntactic structure information, although great outcomes have been achieved, these methods are overly dependent on the precision of the syntactical dependency tree, which may lead to suboptimal dependencies between words and thus introduce noise. Effectively incorporating semantic relevance information and syntactic structure information remains a challenging task. To address the shortcomings referred to, we propose the dependency relationship-enhanced graph convolutional network (DREGCN) model, which utilizes a dual channel to integrate semantic relevance information and syntactic structure information. Specifically, in the syntactic channel, we preserve the original dependency tree to obtain global syntactic information, while introducing an aspect-oriented reconstruction tree to capture local syntactic information. Additionally, in contrast to previous studies where context words and aspect words were modeled separately, we propose cosine networks in the semantic channel to enhance information interaction between contexts and aspects. The experimental results show that our DREGCN model has a strong advantage on the three publicly available datasets.},
  archive      = {J_NCA},
  author       = {Tian, Xiaohui and Liu, Fang’ai and Zhuang, Xuqiang and Zhang, Yuling and Gao, Xuejian},
  doi          = {10.1007/s00521-025-11508-8},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22775-22800},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dependency relationship-enhanced graph convolutional network for aspect-based sentiment analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UDA-student network and the role of pixel-space relationships in pseudo-label optimization. <em>NCA</em>, <em>37</em>(27), 22755-22773. (<a href='https://doi.org/10.1007/s00521-025-11507-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the expensive and time-consuming nature of obtaining pixel-level annotations for real-world images in semantic segmentation, using readily available synthetic data to train models is practical. This allows the model to adapt to real-world images without needing additional annotations. This process has been explored extensively in the context of Unsupervised Domain Adaptation (UDA). Despite numerous studies proposing novel adaptation strategies, many have largely overlooked the role of the student network and the crucial impact of spatial relationships between pixels on pseudo-label generation. To address pseudo-label noise and enhance the quality of pseudo-labels, three simple yet crucial modification modules are employed: (1) Student Network Reverse-Guiding The Teacher Network: Replace low-confidence pseudo-labels generated by the teacher network at the current position with high-confidence pseudo-labels generated by the student network. (2) Pixel-Space Self-Modification: Leverage the spatial distribution characteristics between pixels by replacing unreliable low-probability labels within a specified range with high-probability labels identified by different categories of pixels within the same range. (3) Small Connected Component Elimination: Identify small connected domains within a specified range that are below a certain threshold and replace the labels within these domains with the label having the highest number of pixels in the surrounding area. In summary, the improvements introduced by these three modules increased the mIoU from GTA to Cityscapes to 76.8 and from Synthia to Cityscapes to 67.9, demonstrating the effectiveness of the proposed modules in enhancing model segmentation performance.},
  archive      = {J_NCA},
  author       = {Zhang, Hao and He, LingMin and Huo, WanLi},
  doi          = {10.1007/s00521-025-11507-9},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22755-22773},
  shortjournal = {Neural Comput. Appl.},
  title        = {UDA-student network and the role of pixel-space relationships in pseudo-label optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ontology-based adaptive tutoring system for learning business english idioms. <em>NCA</em>, <em>37</em>(27), 22725-22753. (<a href='https://doi.org/10.1007/s00521-025-11506-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents an intelligent tutoring system (ITS) designed for the adaptive learning of Business English idioms. It addresses the problem of limited personalization and static content delivery in traditional language learning platforms. The objective is to design a hybrid ITS that adapts to individual learner performance, knowledge level, and emotional feedback. The proposed system integrates rule-based classification, ontology-driven knowledge structuring, semantic similarity algorithms, and sentiment analysis using a BERT-based deep learning model. Learners are initially categorized using pre-assessment rules, and idioms are then recommended based on difficulty, domain relevance, and previous learning outcomes. Feedback is analyzed in real time to guide dialogue and content adaptation. The methodology was evaluated through controlled user interaction scenarios. Results indicate a 25% improvement in quiz performance and a 30% increase in learner engagement, compared to a baseline non-adaptive version. Learners received personalized recommendations, adaptive quizzes, and emotionally responsive system messages, improving motivation and retention. However, limitations include the scalability of rule-based logic, difficulties in culturally interpreting idioms, and occasional inaccuracies in sentiment detection. Future work will explore reinforcement learning for dynamic adaptation and multilingual support for broader applicability. In conclusion, the system demonstrates that combining symbolic and deep learning techniques in ITS significantly enhances the learning experience. It offers a replicable model for personalized, intelligent, and emotionally aware instruction in domain-specific language learning.},
  archive      = {J_NCA},
  author       = {Ali, Rehan S. and Abouel-Ela, Magdy and Eldakhly, Nabil M.},
  doi          = {10.1007/s00521-025-11506-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22725-22753},
  shortjournal = {Neural Comput. Appl.},
  title        = {An ontology-based adaptive tutoring system for learning business english idioms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdversarialGait: Direction invariant gait recognition with adversarial learning. <em>NCA</em>, <em>37</em>(27), 22707-22724. (<a href='https://doi.org/10.1007/s00521-025-11505-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition is an emerging biometric technology that identifies individuals based on their unique walking patterns with gait sequences and variations serving as strong biometric features. Unlike other biometric modalities, it can operate over long distances without requiring active participation from the subjects, thus having wide application in security and surveillance. The performance of gait recognition can be significantly affected by variations such as view angle, posture, clothing and occlusion. Despite the advances in deep learning, these variations still pose a challenge. Specifically, a person’s appearance differs based on walking directions, impacting the accuracy of gait recognition. Existing works primarily address appearance-level variation and do not explicitly suppress the influence of walking direction. We propose a novel approach to remove the influence of walking direction in gait recognition via an adversarial process. We introduce a three module framework—the Feature Extraction Network (FEN), Gait Recognition Network (GRN) and the Direction Estimation Network (DEN). We implement an adversarial training paradigm with walking direction as the adversarial parameter in which we simultaneously train the FEN to enhance recognition capabilities of GRN while hindering DEN’s ability to estimate the walking direction, thus learning direction irrelevant gait recognition features. We train our model on the benchmark datasets, CASIA-B and OU-MVLP. Furthermore, we provide the first experimental demonstration showing that adversarial learning actively avoids direction-related features forming more compact clusters with better inter-class separability while maintaining comparable accuracy to state-of-the-art gait recognition models.},
  archive      = {J_NCA},
  author       = {Raj, Hilton and Vishnuram, A. V. and Raman, Rahul},
  doi          = {10.1007/s00521-025-11505-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22707-22724},
  shortjournal = {Neural Comput. Appl.},
  title        = {AdversarialGait: Direction invariant gait recognition with adversarial learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate-assisted evolutionary sampling particle swarm optimization for high-dimensional expensive optimization. <em>NCA</em>, <em>37</em>(27), 22689-22705. (<a href='https://doi.org/10.1007/s00521-023-08661-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted evolutionary algorithms have been widely employed for solving expensive optimization problems. To address high-dimensional expensive optimization problems, we propose an evolutionary sampling-assisted particle swarm optimization method, termed ESPSO. ESPSO consists of some evolutionary sampling-assisted strategies. It first improves the initialized population with some elite samples by evolutionary sampling. Secondly, during the optimization process, the method builds a local radial basis function model using the personal historical optimal data of the population to approximate the objective function landscape. Finally, surrogate-assisted local search and surrogate-assisted trust region search are designed to find promising candidate solutions for replacing individuals in the population to accelerate the search process. Behavioral research experiments of ESPSO verified these strategies have led to improvements in the search efficiency of the algorithm in various aspects, such as initialization, population update, and optimal solution promotion. We compared ESPSO with five state-of-the-art SAEAs using 18 benchmark functions, which show that ESPSO outperforms the other compared SAEAs and get the best average ranking of 2.194.},
  archive      = {J_NCA},
  author       = {Huang, Kuihua and Zhen, Huixiang and Gong, Wenyin and Wang, Rui and Bian, Weiwei},
  doi          = {10.1007/s00521-023-08661-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22689-22705},
  shortjournal = {Neural Comput. Appl.},
  title        = {Surrogate-assisted evolutionary sampling particle swarm optimization for high-dimensional expensive optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEM-PSO: A lightweight evolutionary-state-driven multiple information learning particle swarm optimization algorithm. <em>NCA</em>, <em>37</em>(27), 22667-22688. (<a href='https://doi.org/10.1007/s00521-025-11083-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle swarm optimization (PSO) has been widely used, in which each particle selects its learning sample relying on fitness information. Intuitively, fitness-based selection strategy is beneficial to optimization. However, excessive reliance on fitness information may cause premature convergence of the whole population. To solve the defects of PSO, a lightweight evolutionary-state-driven multiple information learning particle swarm optimization algorithm (LEM-PSO) is proposed. In the new proposed LEM-PSO, firstly, a lightweight multiple information learning strategy is proposed. Then, adaptive evolutionary-state adjustment mechanism is proposed. Finally, local optimum warning operation is used to help the stagnant population to jump from local optimums. The comprehensive performance of LEM-PSO is compared with seven popular PSO variants on CEC2013, CEC2017 and two engineering problems, and the results confirm the firmness of LEM-PSO.},
  archive      = {J_NCA},
  author       = {Yang, Xu and Li, Hongru},
  doi          = {10.1007/s00521-025-11083-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22667-22688},
  shortjournal = {Neural Comput. Appl.},
  title        = {LEM-PSO: A lightweight evolutionary-state-driven multiple information learning particle swarm optimization algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coevolutionary artificial bee colony for training feedforword neural networks. <em>NCA</em>, <em>37</em>(27), 22649-22666. (<a href='https://doi.org/10.1007/s00521-024-10910-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A coevolutionary artificial bee colony (CoABC) trainer based on a hybrid encoding mode is proposed to optimize the network structure and connection weights of a single-hidden layer feedforward network (SLFN). In the proposed CoABC, an integrated population (or colony) with double subpopulations, one of which is responsible for evolution of the network structure encoded as a binary vector, and the other of which is in charge of evolution of the connection weights encoded as a real-number vector, is utilized to coordinate the evolution of two subpopulations. Two types of updating formulas for binary and continuous variables in employed bees phase and onlooker bees phase are developed to enhance the search capability of CoABC. The CoABC can self-organize the structure and weights of a SLFN. In the experiments, 22 benchmark classification datasets are employed to evaluate the proposed CoABC trainer. The results show that the CoABC based on the hybrid encoding mode can train the optimal SLFNs for classification tasks with average test accuracy of 85.12%. Also, the proposed CoABC trainer outperforms original ABC-based trainers and other compared metaheuristic trainers as well as gradient-based trainers. Compared with all other algorithms, the proposed algorithm ranks top 1 in average test accuracy.},
  archive      = {J_NCA},
  author       = {Zhang, Li and Li, Hong and Gao, Weifeng},
  doi          = {10.1007/s00521-024-10910-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22649-22666},
  shortjournal = {Neural Comput. Appl.},
  title        = {A coevolutionary artificial bee colony for training feedforword neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software defect prediction based on multi-filter wrapper feature selection and deep neural network with attention mechanism. <em>NCA</em>, <em>37</em>(27), 22621-22648. (<a href='https://doi.org/10.1007/s00521-024-10902-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction (SDP) models rely on various software metrics and defect data to identify potential defects in new software modules. However, the performance of these predictive models can be negatively impacted by irrelevant, redundant metrics and the imbalanced nature of defect datasets. Additionally, the previous studies mainly use conventional machine learning (ML) techniques, but their predictive performance is not superior enough. Addressing these issues is crucial to improve the accuracy and effectiveness of SDP models. This study presents a novel approach to SDP using a multi-filter wrapper feature selection technique (MFWFS). To identify a subset of relevant and informative features, we leverage the combination of filter techniques—Information gain (IG), Chi-square (CS), and Relief-F (RF) method, and a wrapper technique—Opposition-Based Whale Optimization Algorithm (OBWOA). One-dimensional-Convolutional Neural Network (CNN) with an attention mechanism is employed to enhance the classification performance of the predictive model by efficiently integrating the selected characteristics into abstract deep semantic features. We undertake experiments on seventeen open-source software datasets on four performance measures—AUC, G-mean, F-measure, and MCC and compare the obtained results with existing state-of-the-art ML and hybrid algorithms. The experimental findings demonstrate the greater efficiency of our approach, highlighting the usefulness of the multi-filter wrapper feature selection technique and 1D-CNN with attention to SDP.},
  archive      = {J_NCA},
  author       = {Malhotra, Ruchika and Chawla, Sonali and Sharma, Anjali},
  doi          = {10.1007/s00521-024-10902-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22621-22648},
  shortjournal = {Neural Comput. Appl.},
  title        = {Software defect prediction based on multi-filter wrapper feature selection and deep neural network with attention mechanism},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential evolution using multi-strategy for the improvement of optimization performance. <em>NCA</em>, <em>37</em>(27), 22593-22620. (<a href='https://doi.org/10.1007/s00521-024-10781-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential evolution (DE) is an effective population-based optimization approach that has been widely used to deal with scientific and engineering problems. However, the performance of DE method is largely dependent on its trial vector produce strategy, namely, mutation strategy, crossover operation and its corresponding control parameters. As claimed by the ‘No free Lunch theorem’, each mutation or crossover strategy has its fatal flaws; hence, the DE method having a single operation strategy cannot solve all types of optimization problems. Therefore, we propose a novel multi-strategy DE (MS-DE) in this study. First, the proposed algorithm uses combined mutation strategies including two powerful mutation strategies and selects them in a probabilistic way. Second, an improved crossover operation is introduced to tackle the stagnation problem. When a stagnation occurs, DE employs the top p-best vector to conduct crossover operation. Third, the control parameters are tuned in novel adaptation schemes. Finally, a local search is utilized in the proposed method to accelerate the convergence. The proposed MS-DE method is examined on CEC2017 test suite, and experiment results confirm its outperformance over several state-of-the-art DE methods. Furthermore, the proposed MS-DE is applied to two constrained engineering problems. The comparison results on these two problems also demonstrate the efficiency of our proposed MS-DE.},
  archive      = {J_NCA},
  author       = {Liu, Nengxian and Luo, Jianbin and Chang, Jie and Pan, Jeng-Shyang},
  doi          = {10.1007/s00521-024-10781-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22593-22620},
  shortjournal = {Neural Comput. Appl.},
  title        = {Differential evolution using multi-strategy for the improvement of optimization performance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lung infection detection and classification using the integration of the improved grasshopper and the remora optimization approaches with improved SVM. <em>NCA</em>, <em>37</em>(27), 22573-22591. (<a href='https://doi.org/10.1007/s00521-024-10624-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infectious lung diseases, such as pneumonia and COVID-19, pose significant threats to global health, with high mortality rates and substantial burdens on healthcare systems. Accurate and timely diagnosis is crucial for effective management and treatment. This study addresses the limitations of existing diagnostic methods by proposing advanced techniques based on computer-aided diagnosis systems and enhanced machine-learning algorithms. The methodology involves the development of novel algorithms for image enhancement, segmentation, feature selection, and classification. A kurtosis-based multi-thresholding grasshopper optimization algorithm is proposed for image segmentation, reducing complexity and enhancing the accuracy of lesion identification. An improved rider optimization algorithm is also introduced for feature selection, aiming to prioritize relevant features and reduce dimensionality effectively. Furthermore, an enhanced support vector machine (SVM) algorithm for lesion classification is presented, utilizing linear mapping to generate feature scores for regions of interest. This facilitates the evaluation of the loss function and improves classification results. The approach’s effectiveness is demonstrated using datasets comprising chest X-ray and CT scan images from the LIDC-IDRI and Montgomery datasets. The improved optimization algorithms were trained and tested over the chest X-ray and CT scan image datasets. An improved SVM classified the lesions with an accuracy of 99.9% for chest X-ray images and 99.8% for CT scan images. The results proved that the improved SVM adequately classifies lung diseases from the chest X-ray and CT scan images. The findings suggest that the proposed methodologies significantly enhance the accuracy and efficiency of diagnosing pneumonia and COVID-19 from medical images. By addressing the limitations of existing diagnostic techniques, this research contributes to improving healthcare practices and ultimately reducing the burden of infectious lung diseases on a global scale.},
  archive      = {J_NCA},
  author       = {Bhimavarapu, Usharani},
  doi          = {10.1007/s00521-024-10624-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22573-22591},
  shortjournal = {Neural Comput. Appl.},
  title        = {Lung infection detection and classification using the integration of the improved grasshopper and the remora optimization approaches with improved SVM},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial hummingbird algorithm with chaotic-opposition-based population initialization for solving real-world problems. <em>NCA</em>, <em>37</em>(27), 22529-22572. (<a href='https://doi.org/10.1007/s00521-024-10621-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The artificial hummingbird algorithm is a global search mechanism with many applications in engineering design, but it tends to stall in high-dimensional problems with locally optimal solutions. To address this issue, this paper enhances an artificial hummingbird algorithm by using a chaos map and opposition-based method for population initialization to combat the lack of population diversity, the imbalance between exploration and exploitation, and the algorithm’s premature convergence. The randomness of chaos maps has been leveraged to prevent solutions trapped in local optima and facilitate faster convergence. Moreover, an opposition-based population can serve as a better initial solution and accelerate convergence when compared to random initialization. Two numerical test suites are used to evaluate the efficacy of the proposed algorithm: 50 benchmark functions and the CEC 2018 benchmark test suite. The outcomes are compared to eight other cutting-edge metaheuristic algorithms. Wilcoxon rank sum test, Friedman test, and mean absolute error are used to conduct additional statistical analysis on the data. Moreover, experiments are conducted on the aforementioned 57 real-world optimization problems to demonstrate the efficacy of the proposed method. The outcomes are contrasted to the algorithms SASS, MAgES, EnMODE, and COLSHADE (which won the CEC2020 Competition on Real-World Single-Objective Constrained Optimization). All quantitative and qualitative results on benchmark functions, statistical tests, as well as real-world optimization problem results demonstrate that the proposed algorithm is competitive and preferable to the metaheuristics considered in the experiments. Hence it is concluded that the proposed algorithm balances exploration and exploitation more effectively and the population initialization technique is conducive to augmenting the search capabilities of the artificial hummingbird algorithm.},
  archive      = {J_NCA},
  author       = {Kaur, Sumandeep and Kaur, Lakhwinder and Lal, Madan},
  doi          = {10.1007/s00521-024-10621-4},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22529-22572},
  shortjournal = {Neural Comput. Appl.},
  title        = {Artificial hummingbird algorithm with chaotic-opposition-based population initialization for solving real-world problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning at the service of metaheuristics for solving numerical optimization problems. <em>NCA</em>, <em>37</em>(27), 22493-22528. (<a href='https://doi.org/10.1007/s00521-024-10610-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating deep learning methods into metaheuristic algorithms has gained attention for addressing design-related issues and enhancing performance. The primary objective is to improve solution quality and convergence speed within solution search spaces. This study investigates the use of deep learning methods as a generative model to learn historical content, including global best and worst solutions, solution sequences, function evaluation patterns, solution space characteristics, population modification trajectories, and movement between local and global search processes. An LSTM-based architecture is trained on dynamic optimization data collected during the metaheuristic optimization process. The trained model generates an initial solution space and is integrated into the optimization algorithms to intelligently monitor the search process during exploration and exploitation phases. The proposed deep learning-based methods are evaluated on 55 benchmark functions of varying complexities, including CEC 2017 and compared with 13 biology-based, evolution-based, and swarm-based metaheuristic algorithms. Experimental results demonstrate that all the deep learning-based optimization algorithms achieve high-quality solutions, faster convergence rates, and significant performance improvements. These findings highlight the critical role of deep learning in addressing design issues, enhancing solution quality, trajectory, and performance speed in metaheuristic algorithms.},
  archive      = {J_NCA},
  author       = {Oyelade, Olaide N. and Ezugwu, Absalom E. and Saha, Apu K. and Thieu, Nguyen V. and Gandomi, Amir H.},
  doi          = {10.1007/s00521-024-10610-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22493-22528},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning at the service of metaheuristics for solving numerical optimization problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M2M-net: Multi-objective neural architecture search using dynamic M2M population decomposition. <em>NCA</em>, <em>37</em>(27), 22473-22491. (<a href='https://doi.org/10.1007/s00521-024-10595-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multi-objective neural architecture search (MO-NAS) is an efficient solution for automating the design of deep neural network architectures, aiming to explore a diverse range of objectives. However, the sensitivity of objectives in MO-NAS varies over generations, leading to a search imbalance, and local search plays a crucial role in this combinatorial optimization problem with a discrete search space. In this paper, we propose M2M-Net, a dynamic self-adaptive (Multi-objective to Multi-objective) M2M population decomposition-based evolutionary algorithm for NAS. M2M-Net leverages dynamic self-adaptive M2M population decomposition to overcome the search imbalance in MO-NAS. The subpopulation-based search within M2M-Net facilitates local search through crossover and mutation. Additionally, M2M-Net incorporates a proxy model to reduce computational cost in architecture evaluation and utilizes the channel attention mechanism to improve the accuracy of proxy model evaluation. Experimental studies on CIFAR-10 and CIFAR-100 datasets validate the effectiveness and efficiency of M2M-Net. Comparisons and analysis demonstrate that M2M-Net achieves comparable performance to state-of-the-art NAS methods while utilizing fewer computational resources.},
  archive      = {J_NCA},
  author       = {Tan, Zhiwen and Guo, Daqi and Chen, Junan and Chen, Lei},
  doi          = {10.1007/s00521-024-10595-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22473-22491},
  shortjournal = {Neural Comput. Appl.},
  title        = {M2M-net: Multi-objective neural architecture search using dynamic M2M population decomposition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid algorithm considering continuous transportation for flexible job shop scheduling problem with finite transportation resources. <em>NCA</em>, <em>37</em>(27), 22451-22471. (<a href='https://doi.org/10.1007/s00521-024-10580-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional flexible job shop scheduling ignores transportation or considers production and transportation separately. With factory digitalization and the widespread use of automated guided vehicles (AGVs), the isolated scheduling of production and transportation is no longer sufficient to meet increased productivity demands. Thus, integrated scheduling has become an inevitable option. Previous research has not sufficiently explored the domain knowledge of flexible job shop scheduling problem with finite transportation resources (FJSP-T), and thus the optimal solution cannot be found in an acceptable time using meta-heuristic algorithms. This paper explores FJSP-T to enhance the efficiency of the entire production system, and the objective is to minimize the makespan. The transportation situations of FJSP-T are analyzed, and it has been identified that the key to solving the problem is considering continuous transportation of AGVs. Further, the active decoding method and the initialization method considering continuous transportation are designed. A hybrid algorithm (HA) is proposed, which incorporates local search into the genetic algorithm, and various neighborhood structures for local search are designed. Finally, the superiority of the active decoding method is proved experimentally, and the algorithm performance is tested on two sets of famous benchmark instances (including 67 instances). Compared with other state-of-the-art reported algorithms, the proposed method obtains the new best solutions for 6 instances, and all solutions are not lower than previous results. Meanwhile, the computational time is only a few seconds. As a result, the proposed HA has significantly improved in solving FJSP-T regardless of the solution accuracy and the computational time.},
  archive      = {J_NCA},
  author       = {Wang, Qingzheng and Gao, Liang and Yu, Yanbin and Xiang, Zhimou and Yao, Youjie and Li, Xinyu and Zhou, Wei},
  doi          = {10.1007/s00521-024-10580-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22451-22471},
  shortjournal = {Neural Comput. Appl.},
  title        = {A hybrid algorithm considering continuous transportation for flexible job shop scheduling problem with finite transportation resources},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering white shark optimizer for dimensionality reduction with case study of apple disease prediction. <em>NCA</em>, <em>37</em>(27), 22421-22449. (<a href='https://doi.org/10.1007/s00521-024-10577-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) plays a crucial role in developing classification models by reducing the number of features used while improving their predictive power. It is a challenging problem that can be viewed as an NP-hard optimization task. To tackle this problem, powerful wrapper-based metaheuristic algorithms are employed, as they have the ability to search for nearly optimal feature subsets in the vast search space. However, these algorithms often face challenges such as getting trapped in local optima and striking a balance between exploration and exploitation. To address these challenges, this study proposes an improved version of the white shark optimizer (WSO) called the improved WSO (IWSO). The IWSO incorporates two efficient strategies, namely opposition-based learning (OBL) and Gaussian mutation (GM), to overcome the limitations of the original method. OBL enhances exploration by considering opposite solutions, while GM prevents premature convergence and improves the exploitation capabilities of the algorithm. The effectiveness of the proposed IWSO is evaluated using various benchmark datasets and assessed using standard evaluation metrics. The results of the experiments demonstrate that the IWSO is capable of discovering new optimal solutions across different test cases. Furthermore, the proposed algorithm is applied to a real-world problem involving the identification of apple diseases, further validating its effectiveness.},
  archive      = {J_NCA},
  author       = {Sami, Aya and Barakat, Sherif I. and Mostafa, Reham R.},
  doi          = {10.1007/s00521-024-10577-5},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22421-22449},
  shortjournal = {Neural Comput. Appl.},
  title        = {Empowering white shark optimizer for dimensionality reduction with case study of apple disease prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parallel hybrid approach integrating clonal selection with artificial bee colony for logistic regression in spam email detection. <em>NCA</em>, <em>37</em>(27), 22401-22419. (<a href='https://doi.org/10.1007/s00521-024-10505-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spam emails are sent to recipients for advertisement and phishing purposes. In either case, it disturbs recipients and reduces communication quality. Addressing this issue requires classifying emails on servers as either spam or ham. Numerous methods have been proposed for this classification task. Among them, logistic regression (LR) stands out for its simplicity, speed, and ease of implementation. However, LR suffers from low detection rates caused by the gradient descent algorithm used in its training phase. To overcome this limitation, we propose a novel method based on the clonal selection algorithm (CSA), renowned for its success in optimization problems due to its local and global search capabilities. Despite CSA’s effective optimization performance, it suffers from robustness and slow training time. Therefore, the CSA and artificial bee colony (ABC) algorithms are hybridized to improve CSA’s robustness and are parallelized to reduce the training time significantly. This hybrid method is employed to optimize the weights of LR by minimizing the cost at the output of LR. The empirical results denote that the proposed method, named CSA–ABC–LR, yields better classification performance compared to state-of-the-art models reported by previous studies, demonstrating an accuracy rate of 99.13% on the Enron-1 dataset, 99.22% on the CSDMC2010 dataset, and 94.49% on the Spambase dataset.},
  archive      = {J_NCA},
  author       = {Dedeturk, Bilge Kagan and Akay, Bahriye},
  doi          = {10.1007/s00521-024-10505-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22401-22419},
  shortjournal = {Neural Comput. Appl.},
  title        = {A parallel hybrid approach integrating clonal selection with artificial bee colony for logistic regression in spam email detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolutionary swarm intelligence optimizer based on probabilistic distribution. <em>NCA</em>, <em>37</em>(27), 22387-22399. (<a href='https://doi.org/10.1007/s00521-023-09299-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel approach to balance exploitation and exploration. The proposed approach is the Evolutionary Swarm Intelligence (ESI) optimizer, which combines an exploration-biased strategy with an exploitation-biased operator. The algorithm is built based on the collective behavior of biological groups, imitating their intelligence behavior. The biological evolutionary process, inspired by genetic algorithms, is applied to every individual in the algorithm. Both swarm intelligence and genetic algorithms have been widely used in practical problems, and their reliability has been proven. ESI is characterized by both spatial group intelligence behavior and temporal biological evolution. To test the performance of ESI, we used a classic test set from IEEE CEC2017 and 22 practical problems from IEEE CEC2011. The popular training tests of the dendritic neuron model were also included in the control trials. We compared ESI with some typical swarm intelligence algorithms and classic algorithms to evaluate its performance and ability to solve practical problems. The experimental results show that ESI outperforms other algorithms in terms of basic performance and the ability to solve practical problems.},
  archive      = {J_NCA},
  author       = {Yang, Yifei and Yang, Haichuan and Li, Haotian and Tang, Zheng and Gao, Shangce},
  doi          = {10.1007/s00521-023-09299-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22387-22399},
  shortjournal = {Neural Comput. Appl.},
  title        = {An evolutionary swarm intelligence optimizer based on probabilistic distribution},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive grey wolf optimization with differential evolution operator for solving the discount {0–1} knapsack problem. <em>NCA</em>, <em>37</em>(27), 22369-22385. (<a href='https://doi.org/10.1007/s00521-023-09075-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discount {0–1} knapsack problem (D {0–1} KP) is a new variant of the knapsack problem. It is an NP-hard problem and also a binary optimization problem. As a new intelligent algorithm that imitates the leadership function of wolves, the grey wolf optimizer (GWO) can solve NP problems more effectively than accurate algorithms. At the same time, the GWO has fewer parameters, faster calculations, and easier implementation than other intelligent algorithms. This paper introduces a method of adaptively updating the prey position of wolves and a differential evolution operator with a scaling factor that adaptively changes according to the number of iterations, and selects which operator to use for iteration by the value of the search agent parameter. Finally, it combines the improved greedy repair operator based on D {0–1} KP to form the adaptive grey wolf optimization with differential evolution operator (de-AGWO). The experimental results of the standard test function prove that the algorithm in this paper has a significant improvement in function optimization performance. And the experimental results of D {0–1} KP shows that the proposed algorithm yields superior solution outcomes, except for unrelated datasets, and exhibits significant advantages when solving strongly correlated datasets. Finally, it is verified that more than 80% of the iterations utilize the grey wolf evolution operator, highlighting that the core of the algorithm remains the GWO.},
  archive      = {J_NCA},
  author       = {Wang, Zijian and Fang, Xi and Gao, Fei and Xie, Liang and Meng, Xianchen},
  doi          = {10.1007/s00521-023-09075-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22369-22385},
  shortjournal = {Neural Comput. Appl.},
  title        = {An adaptive grey wolf optimization with differential evolution operator for solving the discount {0–1} knapsack problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV path planning in mountain areas based on a hybrid parallel compact arithmetic optimization algorithm. <em>NCA</em>, <em>37</em>(27), 22353-22368. (<a href='https://doi.org/10.1007/s00521-023-08983-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV) path planning is one of the core components of its entire autonomous control system. The main challenge lies in efficiently obtaining an optimal flight route in complex environments, especially in mountain areas. To address this, we propose a novel version of arithmetic optimization algorithm (AOA), named parallel and compact AOA (PCAOA). In PCAOA, the compact technique can save the memory of UAV and shorten the calculation time, and the parallel technique can quicken the convergence speed and improve the solution accuracy. In addition, the flight path generated by PCAOA is smoothed with cubic B-spline curves, making the path suitable for a UAV. The performance of PCAOA is demonstrated on 23 benchmark functions. Experimental results show that PCAOA achieves competitive results. Finally, the simulation studies are conducted to verify that PCAOA can successfully acquire a feasible and effective route in different mountain areas.},
  archive      = {J_NCA},
  author       = {Wang, Ruo-Bin and Wang, Wei-Feng and Geng, Fang-Dong and Pan, Jeng-Shyang and Chu, Shu-Chuan and Xu, Lin},
  doi          = {10.1007/s00521-023-08983-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22353-22368},
  shortjournal = {Neural Comput. Appl.},
  title        = {UAV path planning in mountain areas based on a hybrid parallel compact arithmetic optimization algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A modified whale optimization algorithm with multi-strategy mechanism for global optimization problems. <em>NCA</em>, <em>37</em>(27), 22339-22352. (<a href='https://doi.org/10.1007/s00521-023-08287-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whale Optimization Algorithm (WOA) is an outstanding nature-inspired algorithm widely used to solve many complex engineering optimization problems. However, WOA has a poor balance in exploration and exploitation, which converges to local optimum easily. This article proposes a Modified Whale Optimization Algorithm (MWOA) with multi-strategy mechanism, which introduces the elite reverse learning strategy, nonlinear convergence factor, DE/rand/1 mutation strategy and Lévy flight disturbance strategy. MWOA can improve the convergent ability and maintain the balance of exploitation and exploration to avoid local optimum. Compared with WOA, PSO, MFO, SOA, SCA and other four WOA variants on the CEC2017 benchmark suite, MWOA has strong competitiveness and can better improve the efficiency of WOA according to the experimental results and analysis.},
  archive      = {J_NCA},
  author       = {Li, Mingyuan and Yu, Xiaobing and Fu, Bingbing and Wang, Xuming},
  doi          = {10.1007/s00521-023-08287-5},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22339-22352},
  shortjournal = {Neural Comput. Appl.},
  title        = {A modified whale optimization algorithm with multi-strategy mechanism for global optimization problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="npl">NPL - 7</h2>
<ul>
<li><details>
<summary>
(2025). Detecting bitcoin sentiment: Leveraging language model applications in sentiment analysis for bitcoin price prediction. <em>NPL</em>, <em>57</em>(5), 1-25. (<a href='https://doi.org/10.1007/s11063-025-11787-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Bitcoin continues to establish itself as a global asset and discussions around relevant regulations become more active, there is an increasing demand for a comprehensive price prediction framework. To address this necessity, this study aims to enhance the accuracy of Bitcoin price predictions by integrating sentiment information with technical indicators, on-chain data, and cryptocurrency price data. Recognizing Bitcoin’s sensitivity to market sentiment, the proposed framework incorporates sentiment features derived from both lexicon-based methods and large language models. As unsupervised sentiment tools can introduce label noise particularly in domain-specific or ambiguous financial contexts, this study combines the outputs of multiple sentiment models at the feature level to construct a more stable representation. This design improves the robustness of downstream regression performance and distinguishes the framework from previous hybrid models that relied on a single sentiment source without component-wise evaluation. Experimental results using a dataset spanning 2700 days showed that the long short-term memory (LSTM) model with a 3-day window achieves the best performance with mean absolute percentage error (MAPE) of 3.93% and R-squared value of 0.99106. Feature importance analysis further demonstrates sentiment index as the most impactful feature, as excluding it resulted in the largest decline in predictive accuracy. Additionally, the model's performance was evaluated under four major volatility periods, revealing MAPE values ranging from 1.49 to 4.03%, highlighting the framework’s practical capability in rapidly adapting to sudden market shifts. In summary, integrating sentiment information attained from multiple language models significantly enhanced prediction accuracy compared to single source approaches. These findings highlight the framework’s practical value for sentiment-informed investment strategies and risk alerts, with a modular design that enables flexible adaptation and potential integration into automated trading systems.},
  archive      = {J_NPL},
  author       = {Jung, Hae Sun and Lee, Haein and Kim, Jang Hyun},
  doi          = {10.1007/s11063-025-11787-1},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Neural Process. Lett.},
  title        = {Detecting bitcoin sentiment: Leveraging language model applications in sentiment analysis for bitcoin price prediction},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FDNet: A novel image focus discriminative network for enhancing camera autofocus. <em>NPL</em>, <em>57</em>(5), 1-20. (<a href='https://doi.org/10.1007/s11063-025-11788-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate activation and optimization of autofocus (AF) functions are essential for capturing high-quality images and minimizing camera response time. Traditional contrast detection autofocus (CDAF) methods suffer from a trade-off between accuracy and robustness, while learning-based methods often incur high spatio-temporal computational costs. To address these issues, we propose a lightweight focus discriminative network (FDNet) tailored for AF tasks. Built upon the ShuffleNet V2 backbone, FDNet leverages a genetic algorithm optimization (GAO) strategy to automatically search for efficient network structures, and incorporates coordinate attention (CA) and multi-scale feature fusion (MFF) modules to enhance spatial, directional, and contextual feature extraction. A dedicated focus stack dataset is constructed with high-quality annotations to support training and evaluation. Experimental results show that FDNet outperforms mainstream methods by up to 4% in classification accuracy while requiring only 0.2 GFLOPs, 0.5 M parameters, a model size of 2.1 MB, and an inference time of 0.06 s, achieving a superior balance between performance and efficiency. Ablation studies further confirm the effectiveness of the GAO, CA, and MFF components in improving the accuracy and robustness of focus feature classification.},
  archive      = {J_NPL},
  author       = {Kou, Chenhao and Xiao, Zhaolin and Jin, Haiyan and Guo, Qifeng and Su, Haonan},
  doi          = {10.1007/s11063-025-11788-0},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Neural Process. Lett.},
  title        = {FDNet: A novel image focus discriminative network for enhancing camera autofocus},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-band video deblurring via efficient chunked additive attention and dynamic channel adaptive module. <em>NPL</em>, <em>57</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11063-025-11798-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning-based deblurring methods perform well in the visible spectrum but suffer from high computational costs due to complex network designs. For infrared images, texture deficiency and edge degradation hinder effective feature learning. To overcome these problems, a Dynamic Channel Adaptive Network (EDCANet) is proposed, which is based on the Dynamic Channel Adaptive Module (DCAM) and Efficient Chunked Additive Attention (ECAA) integrated into a multi—scale network with an encoder—decoder architecture. The DCAM adaptively recalibrates input frame contributions through dynamic channel weighting. Additionally, the ECAA mechanism that decomposes attention operations into partitioned spatial and frequency domains is adopted to enhance contour recovery performance. Images across multiple spectral bands are used in experiments to verify the generalization ability of the proposed method and the effectiveness of structural contour and fine-grained detail recovery. Experimental results show that the EDCANet can process images in real time at a speed of 100 frames per second, with the Peak Signal-to-Noise Ratio (PSNR) exceeding 31 dB and the Structural Similarity Index (SSIM) surpassing 0.92 on infrared datasets. For visible light datasets, its PSNR reaches above 32 dB and SSIM exceeds 0.93.},
  archive      = {J_NPL},
  author       = {Ao, Yongqi and Zhu, Deyan and Li, Chengcheng and Zhang, Yufan and Xu, Jiayi},
  doi          = {10.1007/s11063-025-11798-y},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Neural Process. Lett.},
  title        = {Multi-band video deblurring via efficient chunked additive attention and dynamic channel adaptive module},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STP-Cube—Multivariate time series forecasting based on spatiotemporal correlations and periodicity. <em>NPL</em>, <em>57</em>(5), 1-19. (<a href='https://doi.org/10.1007/s11063-025-11799-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of multivariate time series (MTS) forecasting aims to utilize historical time series data to predict future trends and variations. MTS data possesses numerous latent features, with spatiotemporal correlations and periodicity being particularly significant. Most existing methods focused on either spatiotemporal correlations or periodicity, limiting their effectiveness. The paper proposes a novel STP-cube model by simultaneously capturing both, offering a more robust solution for MTS forecasting. MTS data is first transformed into a three-dimensional structure—the cube, through special stacking. Then graph convolution is utilized to capture spatiotemporal dependencies among sensors and Conv2Ds in another dimension to extract periodic features within sensors. By incorporating both features, the forecasting effectiveness is significantly enhanced. STP-cube model shows robust performance across six public MTS datasets, particularly excelling on the ETTh1 dataset where it reduces MSE by 0.177 and MAE by 0.084 comparing to the SOTA model.},
  archive      = {J_NPL},
  author       = {Zhang, Yidong and Jing, Jie and Liu, Luqi and Lan, Chengming and Shi, Peng},
  doi          = {10.1007/s11063-025-11799-x},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Neural Process. Lett.},
  title        = {STP-Cube—Multivariate time series forecasting based on spatiotemporal correlations and periodicity},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability and exponential lag-synchronization of a class of neural network with state dependent and distributed delays over a time scale. <em>NPL</em>, <em>57</em>(5), 1-24. (<a href='https://doi.org/10.1007/s11063-025-11802-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article tackles the stability and synchronization challenges of neural networks with distributed and state-dependent delays on a temporal domain, leveraging the powerful framework of time scales theory. By formulating the problem within this unified framework, we enable applications to both uniform and non-uniform time domains. Our investigation begins with a thorough analysis of the local exponential stability of the zero solution, employing a combination of pure analysis method, reduction to absurdity technique, and time scale theory. We derive a set of sufficient conditions that guarantee local exponential stability of neural networks with distributed and state-dependent delays. Furthermore, we examine exponential lag synchronization results, utilizing a range of analytical tools, including time scale theory, matrix norm theory, unified matrix-measure theory, and the Halanay inequality. To demonstrate the efficacy and broad applicability of our findings, we present a simulated example on random time scales. Specifically, the time scales theory allows us to effectively handle time scales by providing a unified framework that can seamlessly integrate both continuous and discrete time domains, thereby enabling the analysis of complex systems with varying time scales. Moreover, our approach leverages the flexibility of time scales theory to accommodate non-uniform time domains, making it an ideal tool for tackling real-world problems with intricate temporal dynamics.},
  archive      = {J_NPL},
  author       = {Abbas, Muhammad and Zada, Akbar and Popa, Ioan-Lucian and Kallekh, Afef},
  doi          = {10.1007/s11063-025-11802-5},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Neural Process. Lett.},
  title        = {Stability and exponential lag-synchronization of a class of neural network with state dependent and distributed delays over a time scale},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From task-aware to task-agnostic parameter isolation for incremental learning. <em>NPL</em>, <em>57</em>(5), 1-28. (<a href='https://doi.org/10.1007/s11063-025-11792-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mitigating catastrophic forgetting in continual learning is a long-standing challenge for artificial intelligence. Often, methods used to alleviate forgetting make use of either rehearsal buffers, pretrained backbones or task-id knowledge. However, these requirements result in severe limitations regarding scalability, privacy preservation, and efficient deployment. In this work, we explore how to eliminate the need for such requirements in incremental learning approaches based on parameter isolation. We propose Low Interference Feature Extraction Subnetworks (LIFES), a method that learns a subnetwork per task and uses all of them concurrently at inference time. This solution minimises requirements; however, it creates the need to address certain challenges. To formalize them, we break down the catastrophic forgetting problem into 4 distinct causes, and address them with a novel lateral classifiers regularization, weight standardization, and subnetwork interference connection pruning. Specifically, the use of lateral classification shows very promising results, forcing the model to learn distributions with higher inter-class distance. Using these components, LIFES achieves competitive results in standard task-agnostic scenarios, demonstrating the viability of this new perspective for parameter isolation, which has minimal requirements. Finally, we discuss how future work can improve this new paradigm further, and how the strategies defined can be complementary to other approaches.},
  archive      = {J_NPL},
  author       = {Vicente-Sola, Alex and Kirkland, Paul and Di Caterina, Gaetano and Bihl, Trevor J and Masana, Marc},
  doi          = {10.1007/s11063-025-11792-4},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Neural Process. Lett.},
  title        = {From task-aware to task-agnostic parameter isolation for incremental learning},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FEFM-YOLO11: Underwater object detection algorithm based on improved YOLO11. <em>NPL</em>, <em>57</em>(5), 1-19. (<a href='https://doi.org/10.1007/s11063-025-11805-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection technology is extensively applied in subsea resource exploration and benthic environmental monitoring; however, the underwater environment presents significant challenges due to limited visibility caused by insufficient illumination and turbid water quality, severely impeding detection tasks. To address these challenges and enhance precision, this paper proposes FEFM-YOLO11—an enhanced YOLO11-based underwater object detection algorithm. Primarily, the algorithm substitutes two C3K2 modules in the backbone with a Feature Enhancement Module (FEM). Employing a multi-branch dilated convolutional structure, the FEM increases feature diversity, expands the network’s local receptive field, and enhances semantic representation of small objects, thereby strengthening detection capabilities. Subsequently, a Context-Aware Fusion Module (CAFM) is introduced in the Neck, which effectively models global and local features through integrated local feature capture (convolutional operations) and global feature extraction (attention mechanisms), consequently improving denoising performance. Furthermore, a dedicated small-target detection layer is incorporated to specifically boost detection performance for smaller underwater objects. Finally, the Wise IoU loss function was used for comprehensive evaluation and performance optimization, and the collaborative integration of these components effectively reduced the problem of missed detections caused by occlusion in dense cluster targets. Experimental results on the URPC2020 dataset demonstrate that the improved algorithm achieves enhancements of 2.0% points in mAP@50 and 1.9% points in mAP@50:95 compared to the baseline, confirming that FEFM-YOLO11 elevates detection precision and validates the feasibility and effectiveness of the proposed methodology.},
  archive      = {J_NPL},
  author       = {Wang, Qi and Liu, Zhichuan},
  doi          = {10.1007/s11063-025-11805-2},
  journal      = {Neural Processing Letters},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Neural Process. Lett.},
  title        = {FEFM-YOLO11: Underwater object detection algorithm based on improved YOLO11},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="paaa">PAAA - 17</h2>
<ul>
<li><details>
<summary>
(2025). ViTGuard: A synergistic approach to malware detection using vision transformers and genetic algorithms optimization. <em>PAAA</em>, <em>28</em>(4), 1-20. (<a href='https://doi.org/10.1007/s10044-025-01516-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of cybersecurity, malware detection stands at the forefront of defense against malicious software. This study introduces an innovative strategy to tackle the ever-evolving cyber threats that characterize the current landscape, transcending traditional methodologies. We present a hybridized approach that combines the advanced capabilities of Vision Transformer (ViT) model, genetic algorithms, and cutting-edge deep learning techniques, marking a new era in cybersecurity. The proposed process begins by transforming complex malware source code into grayscale images, effectively bridging the gap between linear code analysis and spatial image recognition. These grayscale images are analyzed using the ViT_b16 model, renowned for its exceptional ability to uncover subtle intricacies within images. The next steps involve leveraging deep learning to scrutinize the features identified by the ViT, facilitating precise detection of malicious code. To enhance the efficiency of the proposed deep learning model, a genetic algorithm is employed for end-to-end hyperparameter optimization for both ViT and deep learning phases. this process aims at calibrating essential parameters such as the Image Size, Number of Attention Heads, Hidden Size (Embedding Dimension), MLP (Feedforward) Dimension, activation function, architectural depth, neuron count, optimizers, initializers, dropout layers, batch normalization, and learning rates of the ViT_b16 and deep learning models. After extensive training on a dataset comprising 25 diverse malware families, the proposed model exhibits remarkable performance, consistently achieving an accuracy rate exceeding 99% in differentiating among these malware variants. A comprehensive evaluation and benchmarking against both state-of-the-art malware detection methodologies and widely used baseline models, including CNNs and traditional machine learning algorithms, demonstrating superior detection performance across all metrics.},
  archive      = {J_PAAA},
  author       = {Bakır, Halit and Bakır, Rezan and Alkhaldi, Tareq and Darem, Abdulbasit A. and Alhashmi, Asma A. and Alqhatani, Abdulmajeed},
  doi          = {10.1007/s10044-025-01516-8},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ViTGuard: A synergistic approach to malware detection using vision transformers and genetic algorithms optimization},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDUA-YOLO: An advanced deep learning approach for PCB surface defect detection. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01526-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface defects of printed circuit boards (PCB) that occur during the manufacturing process seriously affect product quality. So it is important to detect PCB surface defects quickly and accurately. However, existing defect detection methods still have room to be improved for PCB surface defect detection. This paper proposes an advanced model, MDUA-YOLO, based on YOLOv5 to increase the detection accuracy of defects on PCB surface. Firstly, we introduce the C3 Mobile Vision Transformer (C3MobileViT) module in the backbone of YOLOv5, improving the feature extraction capability of the model. Secondly, the Deformable Convolutional-Receptive Field Block (DC-RFB) module is incorporated into the neck of YOLOv5 to dynamically expands the receptive field and more accurately capture the location information of small defects. Additionally, we design the Union Attention Block (UAB) module in the neck of YOLOv5 to optimize the fusion of low-level and high-level feature maps. Finally, an extra prediction head and new feature fusion layer are also added to enhance the ability of the model to detect small defects. On the benchmark PKU-PCB and DeepPCB datasets, numerous experimental results show that the MDUA-YOLO surpasses other comparison state-of-the-art models and meets the real-time detection requirement of industrial environment.},
  archive      = {J_PAAA},
  author       = {Liu, Xiaowei and Wang, Haichao},
  doi          = {10.1007/s10044-025-01526-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MDUA-YOLO: An advanced deep learning approach for PCB surface defect detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing video salient object detection via SAM-based multimodal energy prompting. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01531-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Salient Object Detection (VSOD) aims to identify the most visually conspicuous objects in videos and extract key information from complex visual scenes. Recent studies combine optical flow (OF) and depth for complementary feature extraction. However, suboptimal fusion strategies often treat these modalities merely as extensions of the RGB stream, failing to fully leverage their unique semantic contributions. To address this limitation, we propose a novel SAM-based Multimodal Energy Prompting Network (MEPNet), which utilizes implicit prompts derived from OF and depth within a pre-trained Segment Anything Model (SAM). This approach enhances VSOD by effectively integrating the complementary dynamic and structural information from these modalities. Particularly, we introduce a Spectrogram Energy Generator to extract Spectrogram Energy from OF and depth. These energy-driven prompts fine-tune SAM via the Modality Energy Adapter, effectively mitigating noise interference and improving segmentation accuracy. In addition, we propose a Circular High-frequency Filter to enhance RGB modality details using an adaptive circular mask. Extensive experiments on five VSOD benchmark datasets demonstrate that our MEPNet outperforms state-of-the-art approaches. Furthermore, our MEPNet generalizes effectively to the Video-Camouflaged Object Detection task and achieves competitive results. The module and predicted maps are publicly available at https://github.com/TOMMYWHY/MEPNet .},
  archive      = {J_PAAA},
  author       = {Jiang, Tao and Wang, Yi and Hou, Feng and Liu, Li-li},
  doi          = {10.1007/s10044-025-01531-9},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing video salient object detection via SAM-based multimodal energy prompting},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated learning with adaptive aggregation within clusters. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01533-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows clients to collaboratively train models while keeping client data local. Initially, it trains a global model to serve all clients, but when the distribution of each local dataset differs significantly from the global dataset, the local objectives of each client may diverge from the globally optimal values, leading to drift in local updates. This phenomenon greatly impacts model performance. The primary purpose of client participation in federated learning is to obtain personalized models with better local performance. In order to solve this problem, this paper proposes a new federated learning algorithm - Federated Learning with Adaptive Intra - cluster Aggregation (FedACC). This algorithm utilizes the inference correlation of client-uploaded models to the server and divides clients with similar data distributions into clusters. During the weighted aggregation of models within each cluster, we introduce an adaptive weight learning algorithm and use the obtained weights to perform the weighted aggregation of cluster models. The algorithm can cluster clients with similar data distributions and utilize adaptive weight learning within the cluster to obtain optimal aggregation weights, enabling more efficient and personalized federated learning through a weighted aggregation cluster model. Our experiments are conducted on three public image datasets, namely MNIST, Fashion - MNIST, and CIFAR − 10, and in a data - heterogeneous environment. Compared with three baseline algorithms, the Federated Averaging algorithm (FedAvg), the Federated Proximal algorithm (FedProx), and the Federated Learning with Intra - cluster Similarity algorithm (FLIS), the global model of the FedACC algorithm proposed in this paper converges faster and has a higher accuracy. On the Fashion - MNIST dataset, compared with FedAvg, FedProx, and FLIS algorithms, the accuracy of FedACC is improved by 11.6%, 10.5%, and 3.0% respectively, which proves the effectiveness of the FedACC algorithm.},
  archive      = {J_PAAA},
  author       = {Ding, Shiyuan and Liu, Yanhong and Shi, Haobin and Gao, Yingying},
  doi          = {10.1007/s10044-025-01533-7},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Personalized federated learning with adaptive aggregation within clusters},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid approach for retinal vessel segmentation with dynamic long-range dependency and multi-scale retinal edge fusion enhancement. <em>PAAA</em>, <em>28</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01534-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate retinal vessel segmentation is crucial for ophthalmic image analysis, providing key structural information for diagnosis and treatment planning. However, existing methods struggle with multi-scale vessel variability, complex curvatures, and ambiguous boundaries. CNNs, Transformer, and Mamba-based approaches have shown promise, yet still struggle to maintain vascular continuity and accurately delineate fine vessel boundaries, especially in thin or tortuous regions, leading to structural discontinuities and edge ambiguity. To address these limitations, we propose a novel hybrid framework that synergistically integrates CNNs and Mamba for high-precision retinal vessel segmentation. Our approach introduces three key innovations: (1) The proposed High-Resolution Edge Fuse Network is a high-resolution preserving hybrid segmentation framework that enhances edge features to ensure accurate and robust vessel segmentation. (2) The Dynamic Snake Visual State Space block is designed to adaptively capture vessel curvature details and long-range dependencies. An improved eight-directional 2D Snake-Selective Scan mechanism and a dynamic weighting strategy enhance the perception of complex vascular topologies. (3) The MREF module enhances boundary precision through multi-scale edge feature aggregation, suppressing noise while emphasizing critical vessel structures across scales. Experiments on DRIVE, STARE, and CHASE_DB1 demonstrate the robust and effective performance of our method. Specifically, our approach attains Dice scores of 82.14%, 76.29%, and 80.46%; clDice scores of 82.40%, 80.30%, and 82.93%; and AUC values of 98.56%, 98.05%, and 98.78%, respectively. This work provides a robust method for clinical applications requiring accurate retinal vessel analysis. The code is available at https://github.com/frank-oy/HREFNet .},
  archive      = {J_PAAA},
  author       = {Ouyang, Yihao and Kuang, Xunheng and Xiong, Mengjia and Wang, Zhida and Wang, Yuanquan},
  doi          = {10.1007/s10044-025-01534-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel hybrid approach for retinal vessel segmentation with dynamic long-range dependency and multi-scale retinal edge fusion enhancement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial translucent patch: A robust physical attack technique against object detectors. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01535-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of computer vision-based autonomous driving technology in daily life necessitates further evaluation of its safety. Current physical attack techniques, using non-transparent stickers as perturbations, lack stealth and therefore may not be effective in real-world applications. Some studies use translucent patches on camera lenses to attack deep neural networks (DNNs), but accessing a victim’s camera is impractical. Light-based attacks also struggle to achieve robust effects under varying environmental conditions. To address these issues within the domain of applied pattern recognition, we propose Adversarial Translucent Patch (AdvTP). This method utilizes translucent color patches optimized with a differential evolution algorithm to create effective physical perturbations. These patches are applied to target objects for black-box attacks on object detectors, a key area in computer vision and image processing. Extensive experiments validate the method’s effectiveness, stealth, and robustness. The proposed method achieves a 91.04% success rate in digital attacks and a 100% success rate in most physical attack cases. It demonstrates superior stealth compared to baseline methods and achieves an average attack success rate of 94.04% against advanced object detectors. We also analyze the method’s generalization capabilities across different pattern recognition tasks, including attacking image classifiers and vehicle detectors, as well as its performance in transfer attacks and against adversarial defenses. Given the significant security threats posed by this method to vision-based applications, which are critical in various applied domains, we believe this work will attract considerable attention from the pattern recognition community. The code can be found in https://github.com/kalbinur90/AdvTP.git .},
  archive      = {J_PAAA},
  author       = {Tiliwalidi, Kalibinuer and Hu, Chengyin and Shi, Weiwen and Lu, Guangxi and Wu, Hao},
  doi          = {10.1007/s10044-025-01535-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adversarial translucent patch: A robust physical attack technique against object detectors},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-capacity reversible data hiding in encrypted HDR images with multiple data hiders. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01536-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a reversible data hiding algorithm for encrypted high-dynamic-range images, with the potential to significantly impact the field. The algorithm supports high embedding capacity, high embedding rate, and allows for multiple data hiders. It begins by using a median edge detector to predict the value of each processing pixel, and determines the embedding capacity through multi-MSB prediction and leading zero count prediction strategies. Multiple label maps are generated and encoded using Huffman coding to reduce transmission overhead. Furthermore, secret sharing is employed to generate several image shares containing pre-embedded label maps, which are distributed to participants who can independently embed secret message. The proposed algorithm enhances the confidentiality of high-dynamic-range images through secret sharing and improves robustness by using multiple image shares, preventing a single image attack from compromising message recovery. Overall, the algorithm significantly increases embedding capacity. Experimental results demonstrate that this approach makes substantial contributions to reversible data hiding in encrypted high-dynamic-range images.},
  archive      = {J_PAAA},
  author       = {Lin, Alfrindo and Lin, Yun-Ting and Jao, Wen-Ting and Tsai, Yuan-Yu},
  doi          = {10.1007/s10044-025-01536-4},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {High-capacity reversible data hiding in encrypted HDR images with multiple data hiders},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boostis:boosting image semi-supervised learning through pseudo-label quality assessment. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01537-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant progress has been made in semi-supervised learning methods that leverage pseudo-labels and consistency regularization. However, two key issues with existing approaches have been identified. Firstly, these methods often focus on maintaining inter-class consistency between strong and weak augmentations, but they tend to overlook intra-class differences. This oversight results in a waste of valuable intra-class information. Secondly, the selection of a fixed high threshold for pseudo-label confidence restricts the quantity and utilization of pseudo-labels. On the contrary, using an initial low threshold introduces a large number of erroneous pseudo-labels, resulting in a degradation of the model’s performance. To address these issues, we propose a novel semi-supervised learning framework that combines contrastive learning and feature discrepancy loss. Our approach introduces a new loss function that facilitates intra-class discrimination by emphasizing inter-class differences. Additionally, we tackle the threshold problem in pseudo-label consistency loss by introducing dynamic weighting coefficients. These coefficients help balance the impact of both the quantity and quality of pseudo-labels on the model. Our experimental results demonstrate that our method effectively harnesses the feature differences among samples with different perturbations. This enhancement boosts the model’s feature generation capability while mitigating the negative effects of erroneous pseudo-labels on model’s performance. Overall, our proposed framework provides a more comprehensive and effective solution to semi-supervised learning in classification applications by addressing the issues of intra-class differences and the selection of pseudo-label thresholds.},
  archive      = {J_PAAA},
  author       = {Liu, Pingping and Chen, Pengfei and Liu, Xiaofeng and Zhou, Qiuzhan},
  doi          = {10.1007/s10044-025-01537-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Boostis:boosting image semi-supervised learning through pseudo-label quality assessment},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing multi-view deep image clustering via contrastive learning for global and local consistency. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01538-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) is a data clustering method with many applications, including but not limited to image and video analysis, text and language processing, bioinformatics, and signal processing. The objective of multi-view deep clustering is to enhance the efficacy of clustering algorithms by integrating data from disparate views. However, discrepancies and inconsistencies between different views frequently reduce the precision of the clustering outcomes. In the recent popular contrastive learning, it has been observed that the processing of positive and negative samples does not consider the multi-view consistency information, ultimately resulting in a decline in clustering accuracy. In this paper, we put forth a global and local consistency-based contrastive learning framework to enhance the efficacy of multi-view deep clustering. First, a global consistency constraint is designed to ensure that the global representations of different views can be aligned to capture the data’s main features. Secondly, we introduce a local consistency mechanism, which aims to preserve the unique local information in each view and obtain efficient, positive samples to improve the complementarity and robustness of the inter-view representations through contrastive learning. The experimental results demonstrate that the proposed method markedly enhances the clustering performance on several real benchmark datasets, mainly when dealing with multi-view data with incompleteness.},
  archive      = {J_PAAA},
  author       = {Shi, Fuhao and Lu, Hu},
  doi          = {10.1007/s10044-025-01538-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing multi-view deep image clustering via contrastive learning for global and local consistency},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-preserving image smoothing via sparse gradient enhancement. <em>PAAA</em>, <em>28</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01539-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-preserving image smoothing is fundamental in the fields of computer vision and image processing. The primary challenge is to smooth low-amplitude details while retaining critical structural information. Existing global filtering methods typically incorporate a data fidelity term and a gradient smoothness term. However, preserving the full semantic information of an image solely through data fidelity remains difficult. To address this, we propose a novel generalized smoothing model that integrates a data fidelity term, a structural fidelity term, and a sparse smoothing term to enhance edge-preserving smoothing performance. The structural fidelity term is designed to ensure that the gradient of the output image closely matches the preprocessed gradient of the input image, thereby achieving structural fidelity. Simultaneously, the smoothing term with sparse regularity constraints is employed to smooth detailed information while preserving significant structural elements. Extensive experimental validation demonstrates that our proposed method outperforms existing techniques and is applicable across various fields, including image smoothing, detail enhancement, edge extraction, HDR tone mapping, clip-art compression artifact removal, and image abstraction. The source code is available at: https://github.com/kxZhang1016/EPSGEF .},
  archive      = {J_PAAA},
  author       = {Long, Jianwu and Zhang, Kaixin and Liu, Yuanqin and Chen, Shuang and Luo, Qi},
  doi          = {10.1007/s10044-025-01539-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Edge-preserving image smoothing via sparse gradient enhancement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stain normalization of pathological images using self-attention based cycleGAN with grey scale consistency loss. <em>PAAA</em>, <em>28</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01540-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The appearance of stains in digital pathological images is prone to be affected by variations in imaging protocols, dyes, scanners, and illumination conditions. This inconsistency will prevent robustness and generalization of computer-aided diagnostic algorithms. Thus, many researchers have proposed efficient methods to normalize different stained images, among which the CycleGAN method prevails. In practice, we have found that this method can cause the background region to be mistaken for the foreground region or can result in the nucleus being considered cytoplasm, a phenomenon we refer to as Stain Region Inversion (SRI). To address the problem and improve its structure-preserving performance in stain normalization tasks, this paper proposes a novel stain normalization method called Structure-Preserving Self-Attention CycleGAN (SPSA-CycleGAN), which enhances the performance of CycleGAN in processing histological and cytological images. We demonstrate how to utilize multi-head self-attention to capture local features and use grey-scaled images to address the issue of SRI, enhancing the pixel-level structure-preserving capability of the original CycleGAN model. Our method is then verified in five experiments and compared with six other state-of-the-art stain normalization methods. The experimental results demonstrated that our SPSA-CycleGAN has better or comparable performance compared to all the other methods. Code available at: https://github.com/Smile-We/SPSA-CycleGAN},
  archive      = {J_PAAA},
  author       = {Chen, Zheng and Jiang, Peng and Duan, Wensi and Wang, Lang and Li, Cheng and Wang, Junfeng and Liu, Juan},
  doi          = {10.1007/s10044-025-01540-8},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Stain normalization of pathological images using self-attention based cycleGAN with grey scale consistency loss},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced group convolution: An improved group convolution based on approximability estimates. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01542-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of neural networks has been significantly improved by increasing the number of channels in convolutional layers. However, this increase in performance comes with a higher computational cost, resulting in numerous studies focused on reducing it. One promising approach to address this issue is group convolution, which effectively reduces the computational cost by grouping channels. However, to the best of our knowledge, there has been no theoretical analysis on how well the group convolution approximates the standard convolution. In this paper, we mathematically analyze the approximation of the group convolution to the standard convolution with respect to the number of groups. Furthermore, we propose a novel variant of the group convolution called balanced group convolution, which shows a higher approximation with a small additional computational cost. We provide experimental results that validate our theoretical findings and demonstrate the superior performance of the balanced group convolution over other variants of group convolution.},
  archive      = {J_PAAA},
  author       = {Lee, Youngkyu and Park, Jongho and Lee, Chang-Ock},
  doi          = {10.1007/s10044-025-01542-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Balanced group convolution: An improved group convolution based on approximability estimates},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-stream network with coordinate attention for multi-view micro-expression recognition using 3D face reconstruction. <em>PAAA</em>, <em>28</em>(4), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01499-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER) is a challenging task due to the subtle and local movements of facial muscles. To get rid of redundant video frames, studies have been conducted on the use of apex frames for MER. However, these studies mainly focused on 2D frontal apex frames, ignoring side face information, which can result in insufficient extraction of features?. To address this issue, we propose a novel dual-stream network with coordinate attention (DSNCA) framework for MER, which comprises a multi-view coordinate attention module (MVCAM) and an apex frame coordinate attention module (AFCAM). The MVCAM initially employs 3D face reconstruction to acquire unobstructed multi-view images that are rich in micro-expression information and enhanced with geometric details. Subsequently, it adaptively learns micro-expression features from multiple angle views with coordinate attention, thus leveraging supplementary face information. In contrast, the AFCAM aims to adaptively locate key regions where micro-expressions occur, thereby minimizing redundant information. The proposed method achieved UF1 and UAR of 76.45, 75.16, 59.21, 59.6, 62.77, 62.06, 66.74, and 69.31 on the Chinese Academy of Sciences Micro-expression DatabaseII (CASMEII), the Spontaneous Micro-expression Corpus (SMIC), the Spontaneous Actions and Micro-Movements (SAMM), and the composite databases, respectively. The code is available for research purposes at https://github.com/pennypppp/DSNCA .},
  archive      = {J_PAAA},
  author       = {Ma, Pianpian and Chen, Jingying and Liu, Xu and Liu, Xiaodi},
  doi          = {10.1007/s10044-025-01499-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dual-stream network with coordinate attention for multi-view micro-expression recognition using 3D face reconstruction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new multi-feature fusion trajectory prediction method. <em>PAAA</em>, <em>28</em>(4), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01541-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction is critical in fields such as autonomous driving and robot navigation. It enables autonomous vehicles and robots to make proactive decisions, avoid collisions with pedestrians, and ensure pedestrian safety. Some methods utilize Generative Adversarial Networks (GAN) for trajectory prediction. However, there are still two major drawbacks that hinder the improvement of trajectory prediction accuracy. 1) The complex interactions among multiple pieces of information have not been fully captured, preventing the information from being fully utilized. 2) The uncertainty of network outputs has not been fully modeled and processed. To address these challenges and enhance the model’s adaptability to unknown data, especially to provide an effective solution for managing complex tasks, a novel multi-feature fusion trajectory prediction method is introduced. Firstly, this method designs a new feature fusion submodule. It considers the correlation between multi-feature information through the Power Average (P-A) operator. It also enables the model to better learn the differences among multiple feature information and effectively capture the complex interactions among them. Then, the uncertainty of the network’s output is modeled by incorporating a conservative output state through triangular fuzzy numbers, enhancing the flexibility of the network. Evidence theory is crucial in practical applications due to its effectiveness in representing and processing uncertain information. Therefore, we adopt the belief measure of evidence theory as the uncertainty output of the network. Finally, considering that evidence distance can accurately assess the differences between uncertain information, we embed the evidence distance formula into the loss function of the model as a loss term. This loss term is used to address the discrepancy between the model’s predicted output and the actual distribution, thereby optimizing the information loss of the model and enhancing prediction accuracy. Experiments conducted on the public ETH/UCY datasets reveal that the proposed method achieves higher accuracy, particularly for trajectory prediction in complex situations. Additionally, we have not significantly reduced the model’s efficiency.},
  archive      = {J_PAAA},
  author       = {Yang, Tian and Wang, Gang and Lai, Jian and Wang, Yang},
  doi          = {10.1007/s10044-025-01541-7},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A new multi-feature fusion trajectory prediction method},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling hierarchical functional brain networks via $$\:{\mathbf{L}}_{2}$$ -normalized attention fully convolutional recurrent autoencoder for multi-task fMRI data. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01543-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling functional brain networks is essential for revealing the functional mechanisms of the human brain. Deep Neural Network (DNN) models have been widely employed for extracting multi-scale spatiotemporal features from functional magnetic resonance imaging (fMRI) data. However, current approaches face two fundamental challenges. Firstly, existing deep neural network-based approaches exhibit significant limitations in learning cross-task common representations when dealing with the variable sequence length characteristics inherent in task-fMRI multi-task data. Secondly, existing approaches often neglect the dynamic variability of neural activity across different time points in fMRI data. To overcome these challenges, a novel framework based on $$\:{\mathbf{L}}_{2}$$ -Normalized Attention Fully Convolutional Recurrent Autoencoder ( $$\:{\mathbf{L}}_{2}$$ -FCRAAE) is proposed for modeling hierarchical functional brain networks (FBNs). Specifically, the $$\:{\mathbf{L}}_{2}$$ -FCRAAE is trained in an unsupervised manner, where the autoencoder architecture guides the attention modules to focus on task-activated regions. The architecture incorporates two synergistic design principles: First, its fully convolutional recurrent structure inherently adapts to variable-length tfMRI time series while effectively capturing long-range temporal dynamics and recognizing brain state transitions. Second, the integrated $$\:{\mathbf{L}}_{2}$$ -normalized temporal-channel attention module weights task-relevant neural activation patterns, substantially enhancing representational capacity. Comprehensive experiments demonstrate that the proposed $$\:{\mathbf{L}}_{2}$$ -FCRAAE exhibits superior capability and generalizability in characterizing spatial and temporal patterns of FBNs in a hierarchical manner. Overall, this study presents a novel approach for understanding the hierarchical organization of functional brain architecture. The code for this paper is available at: https://github.com/beiweizai111/FCAAE .},
  archive      = {J_PAAA},
  author       = {Liu, Huan and Cui, Puwang and Zhang, Minye and Li, Li and Han, Fei},
  doi          = {10.1007/s10044-025-01543-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Modeling hierarchical functional brain networks via $$\:{\mathbf{L}}_{2}$$ -normalized attention fully convolutional recurrent autoencoder for multi-task fMRI data},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep hashing with prototype-aware hardness-weighted for supervised cross-modal retrieval. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01546-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing technology has gained widespread attention due to its low storage cost and high retrieval efficiency. Although existing cross-modal hashing methods have achieved good retrieval results, there is still the problem of facing modal heterogeneity leading to semantic separation. Therefore, this paper proposes a deep hashing method based on prototype-aware hardness-weighted, which uses the CLIP model after fine-tuning the large language model as a feature extractor to better obtain the feature information of the two modalities. And a set of loss functions are designed to better handle difficult samples by prototype-aware hardness-weighted loss to ensure that the data can be embedded in the appropriate location, bringing relevant data closer and pulling irrelevant data away. After conducting experiments on three datasets and comparing with some advanced cross-modal retrieval methods in recent years, it can be shown that our proposed DPHS method has excellent performance. The code and datasets used in this article can be obtained from https://github.com/chmy180/dphs-main .},
  archive      = {J_PAAA},
  author       = {Ge, Bin and Cheng, Mengyan and Xia, Chenxing},
  doi          = {10.1007/s10044-025-01546-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Deep hashing with prototype-aware hardness-weighted for supervised cross-modal retrieval},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot segmentation network based on class-aware prototype fusion. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01547-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing few-shot segmentation methods based on the support-query matching framework suffer from insufficient support information, where the limited number and coverage of annotated samples often produce prototypes that are incomplete or contaminated by background features, leading to incomplete activation of target regions in the query image and false activation of irrelevant areas. To address this issue, we propose a class-aware prototype fusion network (CAPFN) for few-shot segmentation, comprising a class-aware module (CAM) and a prototype fusion module (PFM). The CAM extracts class-specific information by jointly utilizing support features, masks, and preliminary query predictions, thereby guiding the model to attend precisely to target regions. To further mitigate semantic gap between the support and query domains, the PFM constructs a hybrid prototype by fusing support-derived and query-derived prototypes based on initial predictions. This fusion enhances prototype quality, reduces information loss, and improves the discriminative capacity of query activation. Extensive experiments on benchmark datasets (PASCAL-5 $${}^i$$ and COCO-20 $${}^i$$ ) demonstrate the superiority of our approach. Notably, with ResNet50 as the backbone, our model achieves a 2.32% improvement in mIoU over the baseline on the COCO-20 $${}^i$$ dataset. The code is available at https://github.com/ShuoWang011/CAPFN.git .},
  archive      = {J_PAAA},
  author       = {Yang, Aiping and Wang, Shuo and Sang, Zijia and Zhou, Yaran},
  doi          = {10.1007/s10044-025-01547-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Few-shot segmentation network based on class-aware prototype fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ptrf">PTRF - 12</h2>
<ul>
<li><details>
<summary>
(2025). Conservative SPDEs as fluctuating mean field limits of stochastic gradient descent. <em>PTRF</em>, <em>192</em>(3), 1447-1515. (<a href='https://doi.org/10.1007/s00440-024-01353-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convergence of stochastic interacting particle systems in the mean-field limit to solutions of conservative stochastic partial differential equations is established, with optimal rate of convergence. As a second main result, a quantitative central limit theorem for such SPDEs is derived, again, with optimal rate of convergence. The results apply, in particular, to the convergence in the mean-field scaling of stochastic gradient descent dynamics in overparametrized, shallow neural networks to solutions of SPDEs. It is shown that the inclusion of fluctuations in the limiting SPDE improves the rate of convergence, and retains information about the fluctuations of stochastic gradient descent in the continuum limit.},
  archive      = {J_PTRF},
  author       = {Gess, Benjamin and Gvalani, Rishabh S. and Konarovskyi, Vitalii},
  doi          = {10.1007/s00440-024-01353-6},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {1447-1515},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Conservative SPDEs as fluctuating mean field limits of stochastic gradient descent},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The Allen–Cahn equation with weakly critical random initial datum. <em>PTRF</em>, <em>192</em>(3), 1373-1446. (<a href='https://doi.org/10.1007/s00440-024-01312-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers the two-dimensional Allen–Cahn equation $$\begin{aligned} \partial _t u = \frac{1}{2}\Delta u + \mathfrak {m}\, u -u^3, \quad u(0,x)= \eta (x), \qquad \forall (t,x) \in [0, \infty ) \times {\textbf {R}}^{2}, \end{aligned}$$ where the initial condition $$ \eta $$ is a two-dimensional white noise, which lies in the scaling critical space of initial data to the equation. In a weak coupling scaling, we establish a Gaussian limit with nontrivial size of fluctuations, thus casting the nonlinearity as marginally relevant. The result builds on a precise analysis of the Wild expansion of the solution and an understanding of the underlying stochastic and combinatorial structure. This gives rise to a representation for the limiting variance in terms of Butcher series associated to the solution of an ordinary differential equation.},
  archive      = {J_PTRF},
  author       = {Gabriel, Simon and Rosati, Tommaso and Zygouras, Nikos},
  doi          = {10.1007/s00440-024-01312-1},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {1373-1446},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {The Allen–Cahn equation with weakly critical random initial datum},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vanishing self-diffusivity in ginibre interacting brownian motions in two dimensions. <em>PTRF</em>, <em>192</em>(3), 1325-1372. (<a href='https://doi.org/10.1007/s00440-024-01303-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that the tagged particles of infinitely many Brownian particles in $$ {\mathbb {R}} ^2$$ interacting via a logarithmic (two-dimensional Coulomb) potential with inverse temperature $$ \beta = 2 $$ are sub-diffusive. The associated unlabeled diffusion is reversible with respect to the Ginibre random point field, and the dynamics are thus referred to as the Ginibre interacting Brownian motion. If the interacting Brownian particles have interaction potential $$ \Psi $$ of Ruelle class and the total system starts in a translation invariant equilibrium state, then the tagged particles are always diffusive if the dimension $$ d$$ of the space $$ {\mathbb {R}}^{d} $$ is greater than or equal to two. That is, the tagged particles are always non-degenerate under diffusive scaling. Our result is, therefore, contrary to known results. The Ginibre random point field has various levels of geometric rigidity. Our results reveal that the geometric property of infinite particle systems affects the dynamical property of the associated stochastic dynamics.},
  archive      = {J_PTRF},
  author       = {Osada, Hirofumi},
  doi          = {10.1007/s00440-024-01303-2},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {1325-1372},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Vanishing self-diffusivity in ginibre interacting brownian motions in two dimensions},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On invariant distributions of feller markov chains with applications to dynamical systems with random switching. <em>PTRF</em>, <em>192</em>(3), 1283-1324. (<a href='https://doi.org/10.1007/s00440-024-01307-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce simple conditions ensuring that invariant distributions of a Feller Markov chain on a compact Riemannian manifold are absolutely continuous with a lower semi-continuous, continuous or smooth density with respect to the Riemannian measure. This is applied to Markov chains obtained by random composition of maps and to piecewise deterministic Markov processes obtained by random switching between flows.},
  archive      = {J_PTRF},
  author       = {Benaïm, Michel and Tough, Oliver},
  doi          = {10.1007/s00440-024-01307-y},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {1283-1324},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {On invariant distributions of feller markov chains with applications to dynamical systems with random switching},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative recurrence for $$T,T^{-1}$$ transformations. <em>PTRF</em>, <em>192</em>(3), 1245-1282. (<a href='https://doi.org/10.1007/s00440-024-01287-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in the study of the asymptotic behaviour of return times in small balls for the $$T,T^{-1}$$ -transformation. We exhibit different asymptotic behaviour (different scaling, different limit point process) depending on the respective dimensions of the measures of the two underlying dynamical systems. It behaves either as for the direct product of the underlying systems, or as for the $${\mathbb {Z}}$$ -extension of the driving system (also studied in this article), or as a more sophisticated process.},
  archive      = {J_PTRF},
  author       = {Pène, Françoise and Saussol, Benoît},
  doi          = {10.1007/s00440-024-01287-z},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {1245-1282},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Quantitative recurrence for $$T,T^{-1}$$ transformations},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KPZ exponents for the half-space log-gamma polymer. <em>PTRF</em>, <em>192</em>(3), 1113-1243. (<a href='https://doi.org/10.1007/s00440-024-01324-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the point-to-point log-gamma polymer of length 2N in a half-space with i.i.d. $${\text {Gamma}}^{-1}(2\theta )$$ distributed bulk weights and i.i.d. $${\text {Gamma}}^{-1}(\alpha +\theta )$$ distributed boundary weights for $$\theta >0$$ and $$\alpha >-\theta $$ . We establish the KPZ exponents (1/3 fluctuation and 2/3 transversal) for this model when $$\alpha =N^{-1/3}\mu $$ for $$\mu \in \mathbb {R}$$ fixed (critical regime) and when $$\alpha >0$$ is fixed (supercritical regime). In particular, in these two regimes, we show that after appropriate centering, the free energy process with spatial coordinate scaled by $$N^{2/3}$$ and fluctuations scaled by $$N^{1/3}$$ is tight. These regimes correspond to a polymer measure which is not pinned at the boundary. This is the first instance of establishing the 2/3 transversal exponent for a positive temperature half-space model, and the first instance of the 1/3 fluctuation exponent besides precisely at the boundary where recent work of Imamura et al. (Solvable models in the KPZ class: approach through periodic and free boundary Schur measures. arXiv:2204.08420 . 2022) applies and also gives the exact one-point fluctuation distribution (our methods do not access exact fluctuation distributions). Our proof relies on two inputs—the relationship between the half-space log-gamma polymer and half-space Whittaker process (facilitated by the geometric RSK correspondence as initiated in Corwin et al. (Duke Math J 163(3):513–563, 2014), O’Connell et al. (Invent Math 197(2):361–416, 2014), and an identity in Barraquand and Wang (Int Math Res Not 2023:11877, 2022) which relates the point-to-line half-space partition function to the full-space partition function for the log-gamma polymer. The primary technical contribution of our work is to construct the half-space log-gamma Gibbsian line ensemble and develop, in the spirit of work initiated in Corwin and Hammond (Invent Math 195(2):441–508, 2014), a toolbox for extracting tightness and absolute continuity results from minimal information about the top curve of such half-space line ensembles. This is the first study of half-space line ensembles.},
  archive      = {J_PTRF},
  author       = {Barraquand, Guillaume and Corwin, Ivan and Das, Sayan},
  doi          = {10.1007/s00440-024-01324-x},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {1113-1243},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {KPZ exponents for the half-space log-gamma polymer},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDEs with critical time dependent drifts: Strong solutions. <em>PTRF</em>, <em>192</em>(3), 1071-1111. (<a href='https://doi.org/10.1007/s00440-025-01390-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is a continuation of (Röckner and Zhao, Bernoulli 29(1), 821 757–784 (2023)). Based on a compactness criterion for random fields in Wiener-Sobolev spaces, in this paper, we prove the strong solvability of time-inhomogeneous stochastic differential equations with drift coefficients in critical Lebesgue spaces, which gives an affirmative answer to a longstanding open problem. As an application, we also prove a regularity criterion for solutions of a stochastic system proposed by Constantin and Iyer (Comm. Pure. Appl. Math. 61(3): 330–345, 2008), which is closely related to the Navier-Stokes equations.},
  archive      = {J_PTRF},
  author       = {Röckner, Michael and Zhao, Guohuan},
  doi          = {10.1007/s00440-025-01390-9},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {1071-1111},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {SDEs with critical time dependent drifts: Strong solutions},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uniform wasserstein convergence of penalized markov processes. <em>PTRF</em>, <em>192</em>(3), 1031-1069. (<a href='https://doi.org/10.1007/s00440-025-01385-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For general penalized Markov processes with soft killing, we propose a simple criterion ensuring uniform convergence of conditional distributions in Wasserstein distance to a unique quasi-stationary distribution. We give several examples of application where our criterion can be checked, including Bernoulli convolutions and piecewise deterministic Markov processes of the form of switched dynamical systems, for which convergence in total variation is not possible.},
  archive      = {J_PTRF},
  author       = {Champagnat, Nicolas and Strickler, Édouard and Villemonais, Denis},
  doi          = {10.1007/s00440-025-01385-6},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {1031-1069},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Uniform wasserstein convergence of penalized markov processes},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-commutative $$L^p$$ spaces and grassmann stochastic analysis. <em>PTRF</em>, <em>192</em>(3), 949-1029. (<a href='https://doi.org/10.1007/s00440-025-01379-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a theory of non-commutative $$L^p$$ spaces suitable for non-commutative probability in a non-tracial setting and use it to develop stochastic analysis of Grassmann-valued processes, including martingale inequalities, stochastic integrals with respect to Itô–Grassmann processes, Girsanov’s formula and a weak formulation of Grassmann SDEs. We apply this new setting to the construction of several unbounded random variables including a Grassmann analog of the $$\Phi ^4_2$$ Euclidean QFT in a bounded region and weak solution to singular SPDEs in the spirit of the early work of Jona-Lasinio and Mitter on the stochastic quantisation of $$\Phi ^4_2$$ .},
  archive      = {J_PTRF},
  author       = {De Vecchi, Francesco and Fresta, Luca and Gordina, Maria and Gubinelli, Massimiliano},
  doi          = {10.1007/s00440-025-01379-4},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {949-1029},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Non-commutative $$L^p$$ spaces and grassmann stochastic analysis},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local limit theorems for random walks on nilpotent lie groups. <em>PTRF</em>, <em>192</em>(3), 875-947. (<a href='https://doi.org/10.1007/s00440-025-01387-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish the (non-lattice) local limit theorem for products of i.i.d. random variables on an arbitrary simply connected nilpotent Lie group G, where the variables are allowed to be non-centered. Our result also improves on the known centered case by proving uniformity for two-sided moderate deviations and allowing measures with a moment of order $$2(\dim G)^2$$ without further regularity assumptions. As applications we establish a Ratner-type equidistribution theorem for unipotent walks on homogeneous spaces and obtain a new proof of the Choquet-Deny property in our setting.},
  archive      = {J_PTRF},
  author       = {Bénard, Timothée and Breuillard, Emmanuel},
  doi          = {10.1007/s00440-025-01387-4},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {875-947},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Local limit theorems for random walks on nilpotent lie groups},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hamilton–Jacobi equations from mean-field spin glasses. <em>PTRF</em>, <em>192</em>(3), 803-873. (<a href='https://doi.org/10.1007/s00440-025-01386-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a meaning to the Hamilton–Jacobi equation arising from mean-field spin glass models in the viscosity sense, and establish the corresponding well-posedness. Originally defined on the set of monotone probability measures, these equations can be interpreted, via an isometry, to be defined on an infinite-dimensional closed convex cone with an empty interior in a Hilbert space. We prove the comparison principle, and the convergence of finite-dimensional approximations furnishing the existence of solutions. Under additional convexity conditions, we show that the solution can be represented by a version of the Hopf–Lax formula, or the Hopf formula on cones. Previously, two notions of solutions were considered, one defined directly as the Hopf–Lax formula, and another as limits of finite-dimensional approximations. They have been proven to describe the limit free energy in a wide class of mean-field spin glass models. This work shows that these two kinds of solutions are viscosity solutions.},
  archive      = {J_PTRF},
  author       = {Chen, Hong-Bin and Xia, Jiaming},
  doi          = {10.1007/s00440-025-01386-5},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {803-873},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Hamilton–Jacobi equations from mean-field spin glasses},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metastability between the clicks of muller’s ratchet. <em>PTRF</em>, <em>192</em>(3), 721-802. (<a href='https://doi.org/10.1007/s00440-025-01377-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove the existence and uniqueness of a quasi-stationary distribution for three stochastic processes derived from the model of Muller’s ratchet. This model was invented with the aim of evaluating the limitations of an asexual reproduction mode in preventing the accumulation of deleterious mutations through natural selection alone. The main considered model is non-classical, as it is a stochastic diffusion evolving on an irregular set of infinite dimension with hard killing on a hyperplane. We are nonetheless able to prove exponential convergence in total variation to the quasi-stationary distribution even in this case. The parameters in this last convergence result are directly related to the core parameters of Muller’s ratchet. The speed of convergence to the quasi-stationary distribution is deduced both for the infinite dimensional model and for approximations with a large yet finite number of potential mutations. Likewise, we give uniform moment estimates of the empirical distribution of mutations in the population under quasi-stationarity.},
  archive      = {J_PTRF},
  author       = {Mariani, Mauro and Pardoux, Etienne and Velleret, Aurélien},
  doi          = {10.1007/s00440-025-01377-6},
  journal      = {Probability Theory and Related Fields},
  month        = {8},
  number       = {3},
  pages        = {721-802},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Metastability between the clicks of muller’s ratchet},
  volume       = {192},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="sac">SAC - 37</h2>
<ul>
<li><details>
<summary>
(2025). Improving the prediction accuracy of statistical models: A new hierarchical clustering approach. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10683-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statisticians and machine learning practitioners frequently encounter datasets originated from multiple populations but containing the same type of measurements. In such cases, predictive analytics is typically carried out by either fitting a separate model to each dataset independently or by merging the datasets and fitting a single model to the combined data. These approaches overlook the potential existence of multiple groups of datasets associated with different underlying models, and, therefore, fail to exploit the inherent similarity between datasets to improve predictions. A third alternative is to perform pairwise comparisons between the populations before fitting the models. However, this is not always feasible, can become a very challenging task with complex models, and often does not rely on predictive accuracy. To address these issues, we propose a clustering approach designed to improve predictions in general databases. The method is based on a novel type of objective function that represents the total by-group prediction error. The clustering problem is solved using a hierarchical-type algorithm of agglomerative nature that automatically obtains the resulting clustering partition in a fully data-driven manner. An additional advantage of this procedure is that the number of clusters is treated as a variable in the minimization problem, allowing it to be determined naturally in a way that optimizes the predictive accuracy of the underlying models. Furthermore, the technique is versatile and can be used with any type of model for both regression, and classification tasks. Several simulation experiments and two real-world applications involving housing prices demonstrate that the procedure outperforms benchmark approaches in terms of predictive accuracy.},
  archive      = {J_SAC},
  author       = {López-Oriona, Ángel and Sun, Ying and Vilar, José A.},
  doi          = {10.1007/s11222-025-10683-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Improving the prediction accuracy of statistical models: A new hierarchical clustering approach},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information-theoretic criteria for optimizing designs of individually randomized stepped-wedge clinical trials. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10690-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are essential for advancing medical knowledge and improving health care, with Randomized Clinical Trials (RCTs) considered the gold standard for minimizing bias and generating reliable evidence on treatment efficacy and safety. Stepped-wedge individual RCTs, which randomize participants into sequences transitioning from control to intervention at staggered time points, are increasingly adopted. To improve their design, we propose an information-theoretic framework based on D– and A–optimality criteria for participant allocation to sequences. Our approach leverages semidefinite programming for automated computation and is applicable across a range of settings, varying in: (i) number of sequences, (ii) attrition rates, (iii) optimality criteria, (iv) error correlation structures, and (v) multi-objective designs using the $$\epsilon $$ -constraint method.},
  archive      = {J_SAC},
  author       = {Duarte, Belmiro P. M. and Atkinson, Anthony C. and Moerbeek, Mirjam},
  doi          = {10.1007/s11222-025-10690-y},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Information-theoretic criteria for optimizing designs of individually randomized stepped-wedge clinical trials},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast post-process bayesian inference with variational sparse bayesian quadrature. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10695-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (vsbq), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. vsbq reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that vsbq builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.},
  archive      = {J_SAC},
  author       = {Li, Chengkun and Clarté, Grégoire and Jørgensen, Martin and Acerbi, Luigi},
  doi          = {10.1007/s11222-025-10695-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Fast post-process bayesian inference with variational sparse bayesian quadrature},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tests for simultaneous ordered alternatives in a two-way ANOVA with interaction. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10706-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many experimental designs, it is known a priori that the mean effects of the factors follow a monotone ordering. In this article, the problem of testing the homogeneity of the effects of both the factors against the alternative of their simultaneous monotone ordering is studied for a two-way crossed heteroscedastic ANOVA model. An intersection type test based on likelihood ratios of two sub-hypotheses and two multiple comparison tests are developed. Algorithms are proposed for the implementation of these tests using a parametric bootstrap approach and the asymptotic accuracy of this approach is also established. Extensive simulations are carried out to study the efficacy of these tests in controlling the type-I error rates and achieving a good power performance. It is shown that the proposed tests achieve the nominal size values regardless of the dimension of the design, number of replications in each cell and level of heterogeneity of error variances. Further, they are seen to have very good powers indicating consistency of tests. The proposed tests are further examined for their robustness under deviation from normality. An ‘R’ package is developed and shared on the open platform ‘GitHub’ for easy usage by practitioners. Finally, the applicability of our proposed test procedures is illustrated using two real data sets on mortality rates.},
  archive      = {J_SAC},
  author       = {Dey, Raju and Kumar, Somesh},
  doi          = {10.1007/s11222-025-10706-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Tests for simultaneous ordered alternatives in a two-way ANOVA with interaction},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile-based fitting for graph signals. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10689-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of monitoring tools has led to an emerging demand for analyzing data residing on graphs, referred to as graph signals. In this study, we propose a quantile-based fitting method for graph signals, which can be applicable to graph signals with a wide range of distributions. Unlike traditional data fitting methods, such as smoothing splines or quantile smoothing splines in Euclidean space, the proposed method is designed for the graph domain, considering the inherent structure of graphs. In contrast to prevalent graph signal fitting methods that rely on optimization problems with $$L_2$$ -norm fidelity, the proposed method provides robust fits for graph signals in the presence of outliers. More importantly, it identifies various distributional structures of graph signals beyond the mean feature. We further investigate the theoretical properties of the proposed solution, including its existence and uniqueness. Through a comprehensive simulation study and real data analysis, we demonstrate the promising performance of the proposed method.},
  archive      = {J_SAC},
  author       = {Kim, Kyusoon and Oh, Hee-Seok},
  doi          = {10.1007/s11222-025-10689-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Quantile-based fitting for graph signals},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical approaches to finite-horizon optimal stopping problems for the shiryaev process. <em>SAC</em>, <em>35</em>(6), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10696-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a finite-horizon optimal stopping problem the optimal stopping time is typically given by the first moment at which a sufficient statistic, namely a process containing all the relevant information on the problem, exceeds an unknown time-dependent boundary. This boundary often turns out to be the solution of a highly nonlinear integral equation involving the transition density of the sufficient statistic. When this density cannot be computed directly or easily, standard methods for solving the integral equation must be modified. This situation arises in sequential detection problems and in the pricing of certain derivative securities, where the corresponding sufficient statistics follow the so called Shiryaev process. In this context, we analyze and implement three distinct numerical methods for solving the integral equations characterizing the associated optimal stopping boundaries: two of them rely on solutions to partial differential equations, while the third is based on approximating the distribution of the sufficient statistic using a log-normal distribution. We demonstrate that these approaches return accurate results and are generally efficient.},
  archive      = {J_SAC},
  author       = {Buonaguidi, B.},
  doi          = {10.1007/s11222-025-10696-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Numerical approaches to finite-horizon optimal stopping problems for the shiryaev process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical mixtures of latent trait analyzers with concomitant variables for multivariate binary data. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10697-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing three-way data is challenging due to complex dependencies between observations, which must be accounted for to ensure reliable results. We focus on hierarchical, multivariate, binary data organized in a three-way data structure, where rows correspond to first-level units, columns to variables, and layers to second-level units within which the first-level units are nested. In this framework, model-based clustering methods can be effectively employed for dimensionality reduction purposes, facilitating a clear understanding of the phenomenon under investigation. In this work, we propose a novel modeling tool for a hierarchical clustering of first- and second-level units. We extend the Mixture of Latent Trait Analyzers (MLTA) with concomitant variables by letting prior component probabilities depend also on second-level-specific random effects. Parameter estimation is performed by means of a double EM algorithm based on a variational approximation of the model log-likelihood function, along with a nonparametric maximum likelihood estimation of the second-level-specific random effect distribution. This latter approach allows to estimate a discrete distribution which directly provides a clustering of second-level units. Within (conditional on) each of such clusters, first-level units are partitioned thanks to the MLTA specification. The proposal is applied to data from the European Social Survey to partition countries (second-level units) according to the baseline attitude of their residents (first-level units) toward digital technologies (variables). Within these clusters, residents are partitioned on the basis of their attitude toward specific digital skills. The influence of socio-economic factors on the identification of digitalization profiles is also taken into consideration via a concomitant variable approach.},
  archive      = {J_SAC},
  author       = {Failli, Dalila and Marino, Maria Francesca and Arpino, Bruno},
  doi          = {10.1007/s11222-025-10697-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Hierarchical mixtures of latent trait analyzers with concomitant variables for multivariate binary data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact computation of angular halfspace depth. <em>SAC</em>, <em>35</em>(6), 1-29. (<a href='https://doi.org/10.1007/s11222-025-10700-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The angular halfspace depth ( $$ahD$$ ) was, already in 1987, the first depth function proposed for the nonparametric analysis of directional data. Mainly due to its presumed high computational cost and lack of efficient computational algorithms, it was never widely used in directional data analysis. We address the problem of the exact computation of $$ahD$$ in any dimension d. We proceed in two steps: (i) We express $$ahD$$ as a generalized (Euclidean) halfspace depth in dimension $$d-1$$ , using a projection approach. That allows us to develop fast exact computational algorithms for $$ahD$$ in dimensions $$d=1, 2, 3$$ . (ii) In spaces of dimension 3]]d 3 we design an inductive procedure that reduces the dimensionality d in the computation of $$ahD$$ , until the algorithms for $$d \le 3$$ can be used. Using our advances we develop a family of powerful algorithms for the computation of $$ahD$$ in any dimension d. Our procedures are implemented efficiently in C++ with an interface in R. A detailed analysis of the complexity of the novel algorithms is performed. Surprisingly, we show that computing $$ahD$$ of multiple points with respect to the same dataset is substantially faster than the same task for the classical (Euclidean) halfspace depth.},
  archive      = {J_SAC},
  author       = {Dyckerhoff, Rainer and Nagy, Stanislav},
  doi          = {10.1007/s11222-025-10700-z},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {Exact computation of angular halfspace depth},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing mean independence with functional covariate. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10705-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new nonparametric conditional mean independence test for a scalar response and a functional covariate. The test statistics are built from continuous functionals over a residual marked empirical process indexed by a randomly projected functional covariate, which is less sensitive to tuning parameters and circumvents the curse of dimensionality. The asymptotic properties of the proposed test statistics under the null and the fixed alternative are established. We also show that our proposed test is able to detect a broad class of local alternatives converging to the null at the parametric rate. Due to the non-pivotal limiting null distribution, we use an easy-to-implement multiplier bootstrap procedure to estimate the critical values. Monte Carlo simulation studies demonstrate that our test outperforms other tests available in the literature due to its higher power and computational efficiency. The proposed test is further illustrated by analyzing the Tecator data set.},
  archive      = {J_SAC},
  author       = {Feng, Yongzhen and Li, Jie and Lu, Haokun and Song, Xiaojun},
  doi          = {10.1007/s11222-025-10705-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Testing mean independence with functional covariate},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random sampling of contingency tables and partitions: Two practical examples of the burnside process. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10708-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper gives new, efficient algorithms for approximate uniform sampling of contingency tables and integer partitions. The algorithms use the Burnside process, a general algorithm for sampling a uniform orbit of a finite group acting on a finite set. We show that a technique called ‘lumping’ can be used to derive efficient implementations of the Burnside process. For both contingency tables and partitions, the lumped processes have far lower per step complexity than the original Markov chains. We also define a second Markov chain for partitions called the reflected Burnside process. The reflected Burnside process maintains the computational advantages of the lumped process but empirically converges to the uniform distribution much more rapidly. By using the reflected Burnside process we can easily sample uniform partitions of size $$10^{10}$$ .},
  archive      = {J_SAC},
  author       = {Diaconis, Persi and Howes, Michael},
  doi          = {10.1007/s11222-025-10708-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Random sampling of contingency tables and partitions: Two practical examples of the burnside process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outcome-guided spike-and-slab lasso biclustering: A novel approach for enhancing biclustering techniques for gene expression analysis. <em>SAC</em>, <em>35</em>(6), 1-24. (<a href='https://doi.org/10.1007/s11222-025-10709-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering has gained interest in gene expression data analysis due to its ability to identify groups of samples that exhibit similar behaviour in specific subsets of genes (or vice versa), in contrast to traditional clustering methods that classify samples based on all genes. Despite advances, biclustering remains a challenging problem, even with cutting-edge methodologies. This paper introduces an extension of the recently proposed Spike-and-Slab Lasso Biclustering (SSLB) algorithm, termed Outcome-Guided SSLB (OG-SSLB), aimed at enhancing the identification of biclusters in gene expression analysis. Our proposed approach integrates disease outcomes into the biclustering framework through Bayesian profile regression. By leveraging additional clinical information, OG-SSLB improves the interpretability and relevance of the resulting biclusters. Comprehensive simulations and numerical experiments demonstrate that OG-SSLB achieves superior performance, with improved accuracy in estimating the number of clusters and higher consensus scores compared to the original SSLB method. Furthermore, OG-SSLB effectively identifies meaningful patterns and associations between gene expression profiles and disease states. These promising results demonstrate the effectiveness of OG-SSLB in advancing biclustering techniques, providing a powerful tool for uncovering biologically relevant insights. The OGSSLB software can be found as an R/C++ package at https://github.com/luisvargasmieles/OGSSLB .},
  archive      = {J_SAC},
  author       = {Vargas-Mieles, Luis A. and Kirk, Paul D. W. and Wallace, Chris},
  doi          = {10.1007/s11222-025-10709-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Outcome-guided spike-and-slab lasso biclustering: A novel approach for enhancing biclustering techniques for gene expression analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed estimation and inference for high-dimensional confounded models. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10710-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hidden confounded model has been widely applied in many fields. Without adjusting for the hidden confounders, the estimators from the standard methods of high-dimensional models could be biased, potentially leading to spurious scientific discoveries. Meanwhile, distributed computation has attracted wide attention in modern statistical learning. Based on high-dimensional confounded models, this paper proposes a deconfounding and debiasing approach for distributed computing, aiming to obtain accurate estimation by reducing the confounding effect and bias. Two different distributed methods are applied: one is the straightforward divide-and-conquer (DC) method and the other is the communication-efficient surrogate likelihood (CSL) method. The former is easy to use in practice, while the latter uses surrogate loss to achieve better performance than the former through multiple iterations. The estimation accuracy and asymptotic theories for both DC and CSL estimators are established. Extensive simulation experiments verify the good performance of the two estimators and two real data applications are also presented to illustrate their validity and feasibility.},
  archive      = {J_SAC},
  author       = {Liu, Jin and Fei, Yuxin and Ma, Wei and Wang, Lei},
  doi          = {10.1007/s11222-025-10710-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Distributed estimation and inference for high-dimensional confounded models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive tree ensembles for composite quantile regressions. <em>SAC</em>, <em>35</em>(6), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10711-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel approach that integrates Bayesian additive regression trees (BART) with the composite quantile regression (CQR) framework, creating a robust method for modeling complex relationships between predictors and outcomes under various error distributions. Unlike traditional quantile regression, which focuses on specific quantile levels, our proposed method, composite quantile BART, offers greater flexibility in capturing the entire conditional distribution of the response variable. By leveraging the strengths of BART and CQR, the proposed method provides enhanced predictive performance, especially in the presence of heavy-tailed errors and non-linear covariate effects. Numerical studies confirm that the proposed composite quantile BART method generally outperforms classical BART, quantile BART, and composite quantile linear regression models in terms of RMSE, especially under heavy-tailed or contaminated error distributions. Notably, under contaminated normal errors, it reduces RMSE by approximately 17% compared to composite quantile regression, and by 27% compared to classical BART.},
  archive      = {J_SAC},
  author       = {Lim, Yaeji and Lu, Ruijin and Ville, Madeleine St. and Chen, Zhen},
  doi          = {10.1007/s11222-025-10711-w},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian additive tree ensembles for composite quantile regressions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation based bayesian optimization. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10715-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. However, in complex scenarios involving optimization over categorical or mixed covariate spaces, GPs may not be ideal. This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires sampling-based access to posterior predictive distributions. SBBO allows the use of surrogate probabilistic models tailored for combinatorial spaces with discrete variables. Any Bayesian model in which posterior inference is carried out through Markov chain Monte Carlo can be selected as the surrogate model in SBBO. We demonstrate empirically the effectiveness of SBBO using various choices of surrogate models in applications involving combinatorial optimization.},
  archive      = {J_SAC},
  author       = {Naveiro, Roi and Tang, Becky},
  doi          = {10.1007/s11222-025-10715-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Simulation based bayesian optimization},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous analysis of longitudinal profiles using adaptive banded precision matrices via penalized fusion. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10716-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering profiles of longitudinal data is a prevalent method employed for analyzing heterogeneity patterns among individuals, aiming to identify clusters based on diverse patterns of the mean progression trajectories. The correlation structure plays a crucial role in longitudinal data analysis, as accurate modeling of this structure enhances the estimation efficiency of mean trajectories. In this paper, we assume that subjects are sampled from a Gaussian mixture distribution, and incorporate regularized bandable precision matrix structure information for each subgroup. In order to identify the latent group structure, we employ concave penalty functions to estimate the pairwise differences of the model parameters derived from finite mixture models. The model parameters and cluster labels are estimated simultaneously using the Expectation-Maximization (EM) algorithm in conjunction with the Alternating Direction Method of Multipliers (ADMM) algorithm. We establish computational convergence and provide a statistical guarantee through demonstrating the asymptotic rate. Numerical studies and real data analysis show improved clustering results and excellent accuracy performance.},
  archive      = {J_SAC},
  author       = {Liang, Chunhui and Ma, Wenqing},
  doi          = {10.1007/s11222-025-10716-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Heterogeneous analysis of longitudinal profiles using adaptive banded precision matrices via penalized fusion},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor train approximation of multivariate gaussian density by scaling and squaring. <em>SAC</em>, <em>35</em>(6), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10707-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor train decomposition is a promising tool for dealing with high dimensional arrays. Point mass filters utilise such arrays for representing probability density functions of the state. Proofs of concept of the application of the low rank decomposition have been provided in the literature. However, the application requires to design parameters, such as tensor train ranks. Since the parameters dictating the computational requirements are derived from the data according to more abstract hyper-parameters such as precision, an analysis of benchmark examples is needed for allocating resources. This paper studies the ranks in the case of Gaussian densities. The influence of correlation and the effect of rounding are discussed first. Efficiency of the density representation used by standard point mass filters is considered next. Aspects of series expansion of the Gaussian density evaluated over array are considered for the tensor train format. The growth of the ranks is illustrated on a four-dimensional example. An observation of the growth for a multi-dimensional case is made last. The lessons learned are valuable for designing efficient point mass filters. Namely, they show that at least the naive implementations of tensor decomposition methods do not break the curse of dimensionality.},
  archive      = {J_SAC},
  author       = {Ajgl, Jiří and Straka, Ondřej},
  doi          = {10.1007/s11222-025-10707-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Tensor train approximation of multivariate gaussian density by scaling and squaring},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new resampling method for meta gaussian distributions. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10717-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta Gaussian distributions, also known as multivariate Gaussian copula models, are a type of statistical distribution that is particularly useful in modeling dependencies among variables. The key advantage of meta Gaussian distributions is their flexibility - they can capture a wide range of dependency structures, making them a powerful tool for statistical modeling. Discrete approximations of continuous multivariate distributions, such as meta Gaussian distributions, which are of significant importance, are widely utilized across numerous disciplines. This paper introduces a new resampling method based on mean square error representative points (MSE-RPs) to construct accurate approximations for meta Gaussian distributions, thereby enhancing precision in statistical analysis. We carry out a systematic examination of the structural patterns and characteristics of MSE-RPs of these distributions. From a theoretical perspective, we analyze the invariance properties in copula-based association measures by leveraging group theory. This allows us to identify more stable invariants that are suitable for complex dependency structures. Through a simulation study, we demonstrate that MSE-RPs achieve significantly higher estimation accuracy for mean vectors and correlation matrices compared to Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods. Furthermore, MSE-RPs offer faster computation relative to the QMC method based on generalized good lattice points (GGLP) sets. Finally, we illustrate the practical advantages of our approach through empirical analysis on real-world datasets.},
  archive      = {J_SAC},
  author       = {He, Pingan and Fang, Kai-Tai and He, Ping and Ye, Huajun},
  doi          = {10.1007/s11222-025-10717-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {A new resampling method for meta gaussian distributions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive subgroup logistic regression for classification with unobserved heterogeneity. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10712-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unobserved heterogeneity refers to the variation among subjects that is not accounted for by the observed features used in a model. Its presence poses a substantial challenge to statistical modeling. This study introduces the Predictive Subgroup Logistic Regression (PSLR) model, which extends the conventional logistic regression and is specifically designed to address unobserved heterogeneity in classification problems. The PSLR model incorporates subject-specific intercepts in the log odds, fitted through a penalized likelihood approach with a concave pairwise fusion penalty. A novel two-step procedure is developed to facilitate the out-of-sample predictions for new subjects whose subgroup membership labels are unknown. This procedure allows the PSLR model to perform both inferential and predictive tasks. Through extensive simulation studies and an empirical application to a customer churn dataset in the telecommunications industry, the PSLR model not only demonstrates great performance in various aggregate accuracy metrics but also achieves a balanced effectiveness in sensitivity and specificity.},
  archive      = {J_SAC},
  author       = {Chen, Kun and Huang, Rui and Tong, Zhiwei},
  doi          = {10.1007/s11222-025-10712-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Predictive subgroup logistic regression for classification with unobserved heterogeneity},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient learning of symmetric positive-definite matrix regression. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10714-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Euclidean data are nowadays frequently encountered due to the advance in data-collection techniques. Under the Tikhonov regularization framework, this paper focuses on the gradient learning in a regression setting where the response is a symmetric positive-definite (SPD) matrix and the predictor is a Euclidean vector. We endow the SPD manifold with the Log-Euclidean metric to transform our model on the manifold to the Euclidean space and calculate the gradients by solving a linear system under the assumption that the gradient function resides in a reproducing kernel Hilbert space. We further simplify our algorithm and reduce the dimension of the linear system by singular value decomposition. Theoretical properties about the approximation error of the reducing-matrix-size algorithm and the error bound of gradient estimation are investigated as well. In numerical experiments, we show the validity of our SPD gradient learning algorithm in variable selection and sufficient dimension reduction. A real-world dataset about New York taxi networks is studied to illustrate the applicability of our algorithm.},
  archive      = {J_SAC},
  author       = {Chen, Baiyu and Fu, Xiaoyi and Li, Yunchen and Wang, Xiaozhou and Yu, Zhou},
  doi          = {10.1007/s11222-025-10714-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Gradient learning of symmetric positive-definite matrix regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing proportional hazards mixture cure models with transfer learning for interval-censored data. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10721-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a breast cancer study, we consider the analysis of interval-censored failure time data in the presence of a cure fraction. Although a great deal of literature has been established for the analysis of interval-censored data with cure fractions, there is no established method that adequately handles the limited sample size issue. Corresponding to this, we propose a transfer learning approach under the proportional hazards mixture cure models for interval-censored data with the aim to transfer the information from the informative auxiliary samples in a larger cohort to improve the performance of the target regression analysis. To assess the proposed method, an extensive simulation study is performed and suggests that it works well in practical situations. Furthermore, we apply the method to the breast cancer study with the focus on Black women by leveraging the data of other racial women and obtain the improved results.},
  archive      = {J_SAC},
  author       = {Lou, Yichen and Sun, Jianguo and Wang, Peijie and Zhao, Shishun},
  doi          = {10.1007/s11222-025-10721-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Enhancing proportional hazards mixture cure models with transfer learning for interval-censored data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust mean-shift clustering based on impartial trimming. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10718-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust version of the mean-shift algorithm is developed to cope with the presence of contaminated data when targeting a clustering problem by means of a modal approach. The goal is to protect nonparametric density-based clustering from the deleterious effect of outliers. Their occurrence affects the analysis mainly because outliers lead to the detection of spurious modes and groups. Therefore, the proposed methodology aims to recover the underlying clustered data configuration, while detecting and discarding outliers. A strategy to select the level of trimming is discussed. The finite sample behaviour of the proposed method is investigated by Monte Carlo numerical studies and empirical applications.},
  archive      = {J_SAC},
  author       = {Greco, Luca and Menardi, Giovanna},
  doi          = {10.1007/s11222-025-10718-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Robust mean-shift clustering based on impartial trimming},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-network assisted clustering using a grouped factor model. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10719-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the clustering task of large-scale panel data with assistance from multi-network information under the grouped factor model. In many real-world clustering tasks, multiple networks can be observed for the same set of cross-sectional units based on different types of interactions. Different networks are different portraits of latent group memberships, which inspired us to utilize multi-network information to improve the clustering accuracy and stability. Therefore, we propose a multi-network-assisted clustering method that encourages coherence between the clustering results and the weighted multi-network in a penalized manner. We also developed a flexible weight learning strategy to identify the clustering capacity differences of multiple networks. A computationally efficient algorithm with random initialization was developed to implement penalized estimation. Thorough simulation studies demonstrate that the proposed method is more promising than existing competitors, even with misleading network information. Finally, application to the daily returns of stocks traded on the Shanghai and Shenzhen stock exchanges demonstrates the effectiveness and efficiency of the new method. Supplementary materials for this article are available online.},
  archive      = {J_SAC},
  author       = {Liang, Wanwan and Wu, Ben and Fan, Xinyan and Zhang, Bo},
  doi          = {10.1007/s11222-025-10719-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Multi-network assisted clustering using a grouped factor model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating quantile regression for multi-source subgroup identification. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10713-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity and the multi-source nature of modern datasets present significant challenges for statistical methodologies. Motivated by the Stimulant Reduction Intervention using Dosed Exercise (STRIDE) study, we analyze primary outcomes from two complementary data sources to assess heterogeneity in treatment effects between experimental and control groups at medium and high quantiles. To address these challenges, we propose a novel approach that integrates quantile regression with weighted quantile loss and joint pairwise fusion penalties, enabling joint subgroup identification across data sources. Our method distinguishes between homogeneous and heterogeneous effects using a center regularization term and can detect sources lacking group structures. Theoretically, we establish weak Oracle properties, ensuring consistent estimation of group structures. Computationally, we employ the alternating direction method of multipliers (ADMM) and mitigate the burden of pairwise fusion through a k-nearest neighbors trimming method. The effectiveness of our approach is demonstrated through numerical simulations and an application to the STRIDE study.},
  archive      = {J_SAC},
  author       = {Wu, Jiaqi and Zhang, Weiping},
  doi          = {10.1007/s11222-025-10713-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Integrating quantile regression for multi-source subgroup identification},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite mixtures of multivariate poisson-log normal factor analyzers for clustering count data. <em>SAC</em>, <em>35</em>(6), 1-31. (<a href='https://doi.org/10.1007/s11222-025-10720-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mixture of multivariate Poisson-log normal factor analyzers is introduced by imposing constraints on the covariance matrix, which results in flexible models for clustering purposes. In particular, a class of eight parsimonious mixture models based on the mixtures of factor analyzers is introduced. The variational Gaussian approximation is used for parameter estimation, and information criteria are used for model selection. The proposed models are explored in the context of clustering discrete data arising from RNA sequencing studies. Using real and simulated data, the models are shown to give favourable clustering performance. The GitHub R package for this work is available at https://github.com/anjalisilva/mixMPLNFA and is released under the open-source MIT license.},
  archive      = {J_SAC},
  author       = {Payne, Andrea and Silva, Anjali and Rothstein, Steven J and McNicholas, Paul D. and Subedi, Sanjeena},
  doi          = {10.1007/s11222-025-10720-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Finite mixtures of multivariate poisson-log normal factor analyzers for clustering count data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized spectral clustering for large-scale multi-layer networks. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10723-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multi-layer networks with large numbers of nodes, edges, and layers arise across various domains, which poses a great computational challenge for downstream analysis. In this paper, we develop an efficient randomized spectral clustering algorithm for community detection in multi-layer networks. We first utilize the random sampling strategy to sparsify the adjacency matrix of each layer. Then we use the random projection strategy to accelerate the eigen-decomposition of the sum of squared sparsified adjacency matrices of all layers. The communities are finally obtained via the k-means of the eigenvectors. The algorithm not only has low time complexity but also saves storage space. Theoretically, we study the misclassification error rate of the proposed algorithm under the multi-layer stochastic block model, which shows that the randomization does not deteriorate the error bound under certain conditions. Numerical studies on multi-layer networks with millions of nodes show the superior efficiency of the proposed algorithm, which achieves clustering results rapidly. We develop a new R package, MLRclust, which makes the proposed methods available for both simulated and real multi-layer networks.},
  archive      = {J_SAC},
  author       = {Su, Wenqing and Guo, Xiao and Chang, Xiangyu and Yang, Ying},
  doi          = {10.1007/s11222-025-10723-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Randomized spectral clustering for large-scale multi-layer networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing quantile function estimation with beta-kernel smoothing. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10725-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a class of nonparametric quantile function estimators based on Beta kernel smoothing. We conduct a rigorous investigation into their large-sample properties, including asymptotic normality and mean squared equivalence to existing methods. Through extensive simulation studies, we demonstrate that the proposed Beta kernel estimators perform comparably to or outperform traditional empirical and symmetric location-scale kernel-based quantile estimators. Additionally, we provide two real-world applications to illustrate the practical effectiveness of our approach. The results suggest that Beta kernel smoothing offers a flexible and efficient alternative for quantile function estimation, particularly in cases where classical methods exhibit inefficiencies.},
  archive      = {J_SAC},
  author       = {Li, Juan and Yu, Ping and Shi, Jianhong and Song, Weixing},
  doi          = {10.1007/s11222-025-10725-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Enhancing quantile function estimation with beta-kernel smoothing},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive weighted composite quantile regression. <em>SAC</em>, <em>35</em>(6), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10726-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a nonparametric Bayesian estimation and model selection method for additive models based on weighted composite quantile regression. This method identifies the unknown smooth functions of the additive model as linear, nonlinear, or zero components by setting a multiplicative parameterized spike-slab prior distribution, which solves the problem of selecting predictive variable components in partially linear additive models when prior information is insufficient. In addition, it further generalizes the composite quantile regression to additive models by combining information from multiple quantiles. A Bayesian hierarchical model is established based on the mixed representation of the asymmetric Laplace distribution, and the posterior distributions of all unknown parameters are sampled by Markov chain Monte Carlo (MCMC). Finally, in the simulation and real data analysis, the variable selection results, root mean square error and other indicators are used to further prove that the method is more competitive than the existing methods.},
  archive      = {J_SAC},
  author       = {Ji, Yonggang and Wang, Mian and Zhou, Maoyuan},
  doi          = {10.1007/s11222-025-10726-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian additive weighted composite quantile regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive stratified monte carlo using decision trees. <em>SAC</em>, <em>35</em>(6), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10731-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been known for a long time that stratification is one possible strategy to obtain higher convergence rates for the Monte Carlo estimation of integrals over the hypercube $$[0, 1]^s$$ of dimension s. However, stratified estimators such as Haber’s are not practical as s grows, as they require $$\mathcal {O}(k^s)$$ evaluations for some $$k\ge 2$$ . We propose an adaptive stratification strategy, where the strata are derived from a decision tree applied to a preliminary sample. We show that this strategy leads to higher convergence rates, that is the corresponding estimators converge at rate $$\mathcal {O}(N^{-1/2-r})$$ for some $$r>0$$ for certain classes of functions. Empirically, we show through numerical experiments that the method may improve on standard Monte Carlo even when s is large.},
  archive      = {J_SAC},
  author       = {Chopin, Nicolas and Wang, Hejin and Gerber, Mathieu},
  doi          = {10.1007/s11222-025-10731-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive stratified monte carlo using decision trees},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid variational bayesian approach for spatial random effects structural equation modeling. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10730-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, structural equation modeling (SEM) has been widely applied in fields such as education, psychology, and environmental science. However, most studies overlook the spatial dependencies within the data, and there is limited research on SEM for spatial data. Spatial random effects (SRE) models, which flexibly capture spatial variation through a series of spatial basis functions (e.g., multiresolution wavelet basis functions), have become a powerful tool for spatial data analysis. This study extends the traditional SEM by incorporating SRE, resulting in a spatial random effects structural equation model (SRE-SEM) for modeling complex environmental spatial data. The model is fitted using a hybrid variational Bayes algorithm, with some model parameters estimated via the standard mean-field variational Bayes approach. To address the intractable posterior and the dimensionality of latent variables, the Metropolis-Hastings algorithm is employed to sample from the exact conditional posterior distribution of the latent variables. Additionally, a fixed-form variational Bayes approach is used to estimate the matrix on which the spatial covariance matrix depends. Simulation studies and case analyses demonstrate that the proposed model effectively captures the structure of spatial data, while the introduced estimation algorithms significantly improve computational efficiency. This study provides a robust and efficient framework for spatial data analysis, offering a promising solution for modeling complex environmental and socio-economic phenomena.},
  archive      = {J_SAC},
  author       = {Wu, Ying and Zhu, Hongyu and Zhang, Jiwei and Lu, Jing},
  doi          = {10.1007/s11222-025-10730-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A hybrid variational bayesian approach for spatial random effects structural equation modeling},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spilt conformal prediction with missing response. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10722-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction provides a distribution-free framework for constructing interval estimates of response variables with guaranteed coverage probabilities. This study introduces a missing weighted split conformalized quantile regression (M-WSCQR) method to address challenges associated with missing response variables, assuming a Missing at Random (MAR) mechanism. M-WSCQR employs covariate shift adjustment to account for the MAR mechanism, ensuring robust and reliable predictions with specified coverage guarantees. The proposed method exhibits double robustness and scalability, making it adaptable to a broad range of problem settings by modifying the weighting function or quantile regression model. Simulation studies conducted under varying missing rates, as well as both homoscedastic and heteroscedastic conditions, confirm the effectiveness and robustness of M-WSCQR. Additionally, its practical utility is demonstrated through analyses of two real-world datasets, highlighting its applicability in diverse inference contexts. In contrast to traditional methods that assume independent and identically distributed observations, M-WSCQR integrates information from both observed and missing groups. This approach accommodates covariate shifts arising from the MAR mechanism and differences in covariate distributions between observed and missing responses. By incorporating these considerations, M-WSCQR achieves enhanced robustness and versatility, offering a valuable solution for addressing missing data challenges in statistical modeling.},
  archive      = {J_SAC},
  author       = {Cao, Zhimiao and Zhang, Ce and Lian, Beibei and Jiang, Bei and Kong, Linglong and Yan, Xiaodong},
  doi          = {10.1007/s11222-025-10722-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Spilt conformal prediction with missing response},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring dependence between functional data via projection hilbert-schmidt covariance. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10727-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hilbert-Schmidt Independence Criteria is a well-known method for quantifying the dependence between two random vectors. However, it suffers from the curse of dimensionality. In this paper, we introduce a novel nonparametric independence test specifically designed for two functional random variables X and Y. The test is based on a new dependence metric, the so-called Projection Hilbert-Schmidt Covariance (PHSC), which efficiently characterizes the dependence of the random variables and improves upon the Hilbert-Schmidt Independence Criterion. The projection Hilbert-Schmidt covariance exhibits several favorable properties. It equals zero if and only if X and Y are independent, provided that the employed kernel is characteristic. It can be applied to random elements without finite moments when the employed kernel is bounded. It admits an U-statistic estimator, which facilitates the construction of our test. We construct a test based on the estimator and provide a theoretical critical value depending on the sample size and the bandwidths. Our analysis theoretically demonstrates the probabilities of the test committing two types of errors respectively, and proves its consistency under the local alternative hypothesis. Simulations and real data analysis show that the test based on the projection Hilbert-Schmidt covariance outperforms other competing tests for functional data.},
  archive      = {J_SAC},
  author       = {Tian, Zhentao and Wang, Darong and Zhang, Zhongzhan},
  doi          = {10.1007/s11222-025-10727-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Measuring dependence between functional data via projection hilbert-schmidt covariance},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Significativity indices for agreement values. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10728-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agreement measures, such as Cohen’s kappa or intraclass correlation, gauge the matching between two or more classifiers. They are used in a wide range of contexts from medicine, where they evaluate the effectiveness of medical treatments and clinical trials, to artificial intelligence, where they can quantify the approximation due to the reduction of a classifier. The consistency of different classifiers to a gold standard can be compared simply by using the order induced by their agreement measure with respect to the gold standard itself. Nevertheless, labelling an approach as good or bad exclusively by using the value of an agreement measure requires a scale or a significativity index. Some quality scales have been proposed in the literature for Cohen’s kappa, but they are mainly naïve, and their boundaries are arbitrary. We propose a general approach to evaluate the significance of any agreement value between two classifiers as the probability of randomly choosing a confusion matrix, built over the same data set, with a lower agreement value. This measure, named significativity, gauges the relevance of the observed agreement value rather than replacing the agreement measure used to calculate it. This work introduces two significativity indices: one dealing with finite data sets, the other one handling classification probability distributions. This manuscript also addresses the computational challenges of evaluating such indices and proposes some efficient algorithms for their evaluation.},
  archive      = {J_SAC},
  author       = {Casagrande, Alberto and Fabris, Francesco and Girometti, Rossano and Pagliarini, Roberto},
  doi          = {10.1007/s11222-025-10728-1},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Significativity indices for agreement values},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-inflation in the multivariate poisson lognormal family. <em>SAC</em>, <em>35</em>(6), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10729-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn’t account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to $$90\%$$ of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing $$90.6\%$$ of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.},
  archive      = {J_SAC},
  author       = {Batardière, Bastien and Chiquet, Julien and Gindraud, François and Mariadassou, Mahendra},
  doi          = {10.1007/s11222-025-10729-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Zero-inflation in the multivariate poisson lognormal family},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for matrix-vector linear regression without debiasing under kronecker covariance structure. <em>SAC</em>, <em>35</em>(6), 1-28. (<a href='https://doi.org/10.1007/s11222-025-10732-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider estimation and statistical inference for a mixed matrix-vector linear regression model by relaxing the independency entries restriction to a general Kronecker structure. A two-step estimation approach based on decorrelated score and rotation is proposed without additional debiasing procedure. In the first step, to deal with the high-dimensional nuisance matrix estimator, we construct an unbiased estimator for the vector parameter based on the decorrelated score function. In the second step, to achieve an unbiased and asymptotically normality estimator for the matrix estimator, we employ the sample-splitting and rotation-unbiasedness techniques. The Cramér-Rao lower bound of the proposed estimator for any linear function of matrix coefficient is attained. Simulation studies and an empirical analysis of Beijing air quality dataset demonstrate the superior performance of our proposed estimators.},
  archive      = {J_SAC},
  author       = {Ke, Baofang and Zhao, Weihua and Wang, Lei},
  doi          = {10.1007/s11222-025-10732-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Statistical inference for matrix-vector linear regression without debiasing under kronecker covariance structure},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiscale method for data collected from network edges via the line graph. <em>SAC</em>, <em>35</em>(6), 1-28. (<a href='https://doi.org/10.1007/s11222-025-10733-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected over networks can be modelled as noisy observations of an unknown function over the nodes of a graph or network structure, fully described by its nodes and their connections, the edges. In this context, function estimation has been proposed in the literature and typically makes use of the network topology such as relative node arrangement, often using given or artificially constructed node Euclidean coordinates. However, networks that arise in fields such as hydrology (for example, river networks) present features that challenge these established modelling setups since the target function may naturally live on edges (e.g., river flow) and/or the node-oriented modelling uses noisy edge data as weights. This work tackles these challenges and develops a novel lifting scheme along with its associated (second) generation wavelets that permit data decomposition across the network edges. The transform, which we refer to under the acronym LG-LOCAAT, makes use of a line graph construction that first maps the data in the line graph domain. We thoroughly investigate the proposed algorithm’s properties and illustrate its performance versus existing methodologies. We conclude with an application pertaining to hydrology that involves the denoising of a water quality index over the England river network, backed up by a simulation study for a river flow dataset.},
  archive      = {J_SAC},
  author       = {Cao, Dingjia and Knight, Marina I. and Nason, Guy P.},
  doi          = {10.1007/s11222-025-10733-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {A multiscale method for data collected from network edges via the line graph},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneity-aware debiased machine learning for high-dimensional partially linear models. <em>SAC</em>, <em>35</em>(6), 1-24. (<a href='https://doi.org/10.1007/s11222-025-10737-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, distributed data are often collected from different locations/times/populations /environments with non-negligible heterogeneity. In this paper, we consider inherently distributed data follow heterogeneous partially linear models with high-dimensional covariates, where each site involves a common parameter vector and site-specific nuisance functions. Three distributed estimators based on debiased machine learning are proposed to account for heterogeneity. The closed forms and asymptotic normal distributions of the proposed estimators have been explored and compared. Moreover, these estimators can be computed easily by transmitting some statistics from the local sites to the central site. The finite-sample performance is demonstrated through simulation studies and an application to Beijing multi-site air quality dataset is also provided.},
  archive      = {J_SAC},
  author       = {Wu, Yining and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-025-10737-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Heterogeneity-aware debiased machine learning for high-dimensional partially linear models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive generalized P-splines for functional data: A statistical framework via blockwise GSVD. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10734-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a novel approach for functional data approximation based on generalized P-splines with non-uniform and adaptively placed knots. The key innovation of our proposal is the integration of a conditioning-aware strategy for selecting both the number and positions of the knots, as well as the regularization parameters. By reformulating the Tikhonov regularization problem, we propose a computationally efficient criterion that controls model complexity while ensuring numerical stability. The resulting approximation framework not only improves the fit across the entire functional domain but also maintains compactness and robustness. Extensive numerical experiments conducted on both synthetic and real-world datasets demonstrate that our approach significantly outperforms traditional free knot and smoothing spline methods in terms of approximation error and conditioning.},
  archive      = {J_SAC},
  author       = {Magistris, Anna De and Romano, Elvira and Campagna, Rosanna},
  doi          = {10.1007/s11222-025-10734-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive generalized P-splines for functional data: A statistical framework via blockwise GSVD},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="si">SI - 3</h2>
<ul>
<li><details>
<summary>
(2025). Escorting drone swarm formation: A swarm intelligence and evolutionary optimisation approach. <em>SI</em>, <em>19</em>(3), 245-272. (<a href='https://doi.org/10.1007/s11721-025-00250-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot swarms provide a robust approach for performing common tasks in which the emergence of collective capabilities outperforms the addition of the individual ones. They are frequently used in surveillance or defence systems where resilience is a must. In this article we propose a swarm of drones capable of surrounding and escorting different types of targets such as a rogue drone or a ground vehicle. We use swarm intelligence and evolutionary optimisation to support the swarm self-organisation using a set of optimal parameters. Our experiments were focused on analysing the properties of the swarm of drones as well as assessing its scalability and fault tolerance. We have used computer simulations to test a variety of different initial drone positions and target trajectories. Additionally, we have validated our proposal through experiments using real-world drones. The achieved results show that our formation system has successfully built stable formations when it was properly configured. It has worked with swarms of five, ten, and twenty drones, and has also been able to recover in the majority of cases in which some drones have failed.},
  archive      = {J_SI},
  author       = {Stolfi, Daniel H. and Danoy, Grégoire},
  doi          = {10.1007/s11721-025-00250-5},
  journal      = {Swarm Intelligence},
  month        = {9},
  number       = {3},
  pages        = {245-272},
  shortjournal = {Swarm Intell.},
  title        = {Escorting drone swarm formation: A swarm intelligence and evolutionary optimisation approach},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of stochastic optimization strategies in multi-robot multi-target tracking scenarios. <em>SI</em>, <em>19</em>(3), 215-243. (<a href='https://doi.org/10.1007/s11721-025-00249-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper conducts a comparative study of stochastic optimization strategies to enable multi-robot systems to search for and track an unknown number of targets. Each robot is equipped with a noisy sensor that has a limited field of view. The robots use a distributed version of the probability hypothesis density filter to estimate both the number and states of the targets. This online target estimate is used by the different search strategies to select actions for each robot. We compare Lloyd’s algorithm, a traditional method for distributed search, with four stochastic optimization techniques: Particle Swarm Optimization, Simulated Annealing, Ant Colony Optimization, and Artificial Immune Systems. Each of the methods is adapted from the traditional case of finding a single global optimum to locate all the local maxima (i.e., targets). We demonstrate through extensive simulations that these techniques offer superior coverage of the search area and more accurate target localization compared to the baseline Lloyd’s algorithm when the ratio of robots to targets is low and that Lloyd’s algorithm performs best when the ratio of robots to targets is high. We also discuss the strengths and limitations of each method, assisting practitioners in the selection of the most appropriate strategy based on specific operational factors, such as the communication load in a multi-robot system.},
  archive      = {J_SI},
  author       = {Xin, Pujie and Dames, Philip},
  doi          = {10.1007/s11721-025-00249-y},
  journal      = {Swarm Intelligence},
  month        = {9},
  number       = {3},
  pages        = {215-243},
  shortjournal = {Swarm Intell.},
  title        = {Comparison of stochastic optimization strategies in multi-robot multi-target tracking scenarios},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consensus in the weighted voter model with noise-free and noisy observations. <em>SI</em>, <em>19</em>(3), 173-214. (<a href='https://doi.org/10.1007/s11721-025-00248-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collective decision-making is an important problem in swarm robotics arising in many different contexts and applications. The Weighted Voter Model has been proposed to collectively solve the best-of-n problem, and analysed in the thermodynamic limit. We present an exact finite-population analysis of the best-of-two model on complete as well as regular network topologies. We also present a novel analysis of this model when agent evaluations of options suffer from measurement error. Our analytical results allow us to predict the expected outcome of best-of-two decision-making on a swarm system without having to do extensive simulations or numerical computations. We show that the error probability of reaching consensus on a suboptimal solution is bounded away from 1 even if only a single agent is initialised with the better option, irrespective of the total number of agents. Moreover, the error probability tends to zero if the number of agents initialised with the best solution tends to infinity, however slowly compared to the total number of agents. Finally, we present bounds and approximations for the best-of-n problem.},
  archive      = {J_SI},
  author       = {Ganesh, Ayalvadi and Hauert, Sabine and Valla, Emma},
  doi          = {10.1007/s11721-025-00248-z},
  journal      = {Swarm Intelligence},
  month        = {9},
  number       = {3},
  pages        = {173-214},
  shortjournal = {Swarm Intell.},
  title        = {Consensus in the weighted voter model with noise-free and noisy observations},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="soco">SOCO - 10</h2>
<ul>
<li><details>
<summary>
(2025). FruCapsNet: A shuffled attention based capsule network for multi-fruit quality assessment. <em>SOCO</em>, <em>29</em>(15), 5277-5304. (<a href='https://doi.org/10.1007/s00500-025-10863-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing global consciousness in the nutritional impact of food consumption, there is a growing need to ascertain the quality standards of fruits or vegetables. Precise and timely quality assessment of fruits or vegetables offers substantial economic advantages to agronomists. It also provides agricultural support by supplying farmers with crucial assessments that enable fair compensation for harvests and higher-quality goods. A significant technological gap in monitoring the quality evolution of fruits within the food supply chain is leading to substantial fruit wastage. Several cutting-edge technologies, such as computer vision, machine learning, and deep learning, have been extensively applied in various agricultural domains, particularly in quality estimation, to address the food security issue. Recently, deep belief classifiers have facilitated promising outcomes, showing higher accuracy than conventional methods. However, the existing deep models for assessing the quality of fruits and vegetables are translation invariant and sensitive to the captured image orientation. Moreover, the existing methods for the fruit quality assessment are fruit-specific and fail to perform on multiple fruits and vegetables. To address this concern, this paper presents a novel method, FruCapsNet, which leverages the strength of capsule network with a shuffle attention mechanism to identify multiple fruit quality effectively. The proposed method utilizes the convolution layers for low-level feature extraction, a shuffled attention block for selective feature maps, and a capsule module for capturing hierarchical feature map relationships. The performance of the proposed method has been validated using the publicly available VegNet dataset, which contains four different fruits with multiple qualities. The experimental result showed that FruCapsNet overshadows the existing state-of-the-art deep network models in terms of accuracy, F1-Score, precision, recall, MSE, and specificity. The proposed method has achieved the highest testing accuracy of $$93.89\%$$ , $$93.18\%$$ , $$92.78\%$$ , and $$93.68\%$$ for bell pepper, chili pepper, new mexico chile, and tomato, respectively. Additionally, the t-distributed Symmetric Neighbor Embedding (t-SNE) framework has been employed to interpret the classification outcomes of the FruCapsNet method for validation.},
  archive      = {J_SOCO},
  author       = {Gupta, Sachin and Tripathi, Ashish Kumar and Singh, Harshit},
  doi          = {10.1007/s00500-025-10863-x},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5277-5304},
  shortjournal = {Soft Comput.},
  title        = {FruCapsNet: A shuffled attention based capsule network for multi-fruit quality assessment},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-based prediction system for diagnosis of cancer diseases: A systematic review. <em>SOCO</em>, <em>29</em>(15), 5257-5276. (<a href='https://doi.org/10.1007/s00500-025-10855-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is a diverse disease brought on by the aberrant development of a cell. Cancer prognosis has become a priority in cancer research as it helps medical experts to plan treatment strategies. As a result, Artificial Intelligence-based learning approaches have been used to model the development and treatment of cancer. In cancer research, several such techniques are being commonly used to build prediction models, leading to an efficient and reliable verdict. We examined the most recent models to highlight the role of predictive models in cancer research. Using a combination of keywords like machine learning, deep learning, cancer prognosis, cancer prediction, and cancer survivability, we found articles published in scientific databases between 2010 and 2021. Our study carries multiples investigations and presents a summary of automated learning methods used in cancer predictive modeling in this article. Although it is evident that machine/deep learning-based models enhance our ability in the early prediction of cancer diagnosis, an adequate amount of affirmation is needed to make sure that such predictive models can be introduced in routine medical practices. The cancer research field is of high impact, and investigations on cancer prediction models should be encompassed for medical decision support of both practitioners and patients.},
  archive      = {J_SOCO},
  author       = {Gupta, Surbhi and Kumar, Yogesh and Gupta, Anish},
  doi          = {10.1007/s00500-025-10855-x},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5257-5276},
  shortjournal = {Soft Comput.},
  title        = {Artificial intelligence-based prediction system for diagnosis of cancer diseases: A systematic review},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using deep learning applied in computer vision for the inclusion of individuals with tetraplegia through assistive robotics via facial features. <em>SOCO</em>, <em>29</em>(15), 5227-5255. (<a href='https://doi.org/10.1007/s00500-025-10906-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2023, the World Health Organization (WHO) conducted a global survey revealing that 16% of the world’s population lives with some form of functional limitation. The majority of these individuals reside in developing countries, where access to assistive technologies is limited due to economic and structural barriers. In this context, the development of affordable and accessible solutions is crucial to promoting autonomy and inclusion. This study proposes a facial movement-based wheelchair control system designed for individuals with tetraplegia. The system leverages real-time computer vision techniques and the YOLOv8 model to detect specific facial features - with a focus on nose position and orientation - converting these into directional commands (forward, backward, left, and right). The framework is structured into three main stages: facial detection, feature extraction, and command inference. To classify directional commands, we evaluated two machine learning methods: PCA and XGBoost. The confusion matrices demonstrated that XGBoost achieved superior accuracy across all classes, particularly in the "forward" direction (89%), outperforming PCA (78%). Interpretability analysis using a SHAP summary plot showed that the most influential features for decision-making are delta_x and delta_y, corresponding to the horizontal and vertical displacements of the nose. These features are directly linked to rotational and linear movement commands. Angular attributes such as angle_yaw and angle_pitch, which represent head rotation in the horizontal and vertical planes, further support directional inference, especially for users with limited motion amplitude. Conversely, features like angle_roll and stability_score demonstrated low relevance, as they are not directly associated with mapped commands in the current system. Overall, the performance metrics - 96.5% precision, 95.8% recall, and 95.9% mean Average Precision (mAP) - confirm the robustness of the proposed approach. The integration of advanced visual processing with interpretable models results in a responsive and intuitive control interface, effectively supporting users with severe motor impairments and enhancing their independence and quality of life.},
  archive      = {J_SOCO},
  author       = {Ferreira, Fernando Rodrigues Trindade and Couto, Loena Marins do and Saporetti, Camila Martins},
  doi          = {10.1007/s00500-025-10906-3},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5227-5255},
  shortjournal = {Soft Comput.},
  title        = {Using deep learning applied in computer vision for the inclusion of individuals with tetraplegia through assistive robotics via facial features},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the flow field around single groynes with rectangular head shapes via experimental data and intelligence methods. <em>SOCO</em>, <em>29</em>(15), 5213-5225. (<a href='https://doi.org/10.1007/s00500-025-10856-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on the 3D flow field around straight groynes using experimental datasets and kernel-based techniques. In this regard, firstly, the three-dimensional turbulent flow field around straight groynes with different lengths was investigated using laboratory experiments. Then, two kernel-based approaches including the Kernel Extreme Learning Machine (KELM) and Gaussian Process Regression (GPR) were used for modeling the three-dimensional velocity components. Finally, the modeling uncertainty was investigated using the Lower Upper Bound Estimation (LUBE) method. Estimation (LUBE) method was applied to explore modeling uncertainty. Experimental results showed that increasing the length of the groyne enhanced the maximum kinetic energy of the turbulent flow along the shear layer while also making the energy distribution in the depth more uniform. The groyne with largest length had the highest general turbulent shear stress in the downstream of the structure. The results showed that the kernel-based methods successfully simulated the 3D velocity components, and the GPR performed much better than the KELM. Based on the sensitivity analysis, the vertical coordinate of the measuring point (Z*) was the least important input variable in flow field simulation around a single straight groyne. It was found that the GPR demonstrated a desirable degree of reliability. Experimental study of flow structure around single straight groynes. Applying kernel-based methods for simulating the most significant flow features around the straight groynes. Analyzing the uncertainty of the proposed methods using Lower Upper Bound Estimation (LUBE) method and constructing Prediction Intervals (PIs) for flow features time series. The results proved desirable capability of the proposed methods in flow structure modeling.},
  archive      = {J_SOCO},
  author       = {Safarzadeh, Akbar and Ghasempour, Roghayeh and Saghebian, Seyed Mahdi},
  doi          = {10.1007/s00500-025-10856-w},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5213-5225},
  shortjournal = {Soft Comput.},
  title        = {Assessing the flow field around single groynes with rectangular head shapes via experimental data and intelligence methods},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced wind/photovoltaic power smoothing using LSTM neural networks and machine learning. <em>SOCO</em>, <em>29</em>(15), 5193-5211. (<a href='https://doi.org/10.1007/s00500-025-10782-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of stochastic renewable energy sources, like wind turbines and photovoltaic systems, into electrical grids introduces challenges to grid stability and reliability, leading to voltage and frequency deviations. This study addresses the fluctuation issue by examining hybrid energy storage systems combining batteries and supercapacitors. A novel power smoothing approach is proposed, involving two strategies: employing LSTM neural networks for short-term prediction of RES power profiles and optimizing HESS through charge/discharge cycle control using a machine learning-based algorithm. This paper also introduces the synergy of vanadium redox flow batteries and supercapacitor for efficient energy storage. The proposed approach is validated through experimental testing in a controlled microgrid setting. The evaluation demonstrates significant improvements, including a 74.2% reduction in power fluctuations and an enhanced smoothing quality evaluation index by up to 40%, surpassing conventional methods like moving average, ramp rate, and low pass filter. The contributions of this research encompass an advanced energy smoothing methodology, streamlined storage integration, and an enhanced energy quality framework for hybrid renewable energy systems.},
  archive      = {J_SOCO},
  author       = {Arévalo, Paul and Benavides, Dario and Aguado, José A. and Jurado, Francisco},
  doi          = {10.1007/s00500-025-10782-x},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5193-5211},
  shortjournal = {Soft Comput.},
  title        = {Advanced wind/photovoltaic power smoothing using LSTM neural networks and machine learning},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of cryptocurrency preferences in hungary using flexible fuzzy numbers. <em>SOCO</em>, <em>29</em>(15), 5173-5191. (<a href='https://doi.org/10.1007/s00500-025-10914-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research explores individual investors’ preferences for cryptocurrencies in Hungary using a novel analytical method with flexible fuzzy numbers, which accounts for uncertainty in responses, unlike the traditional Likert scale-based analysis. An online questionnaire yielded 116 responses, which were analyzed using descriptive and inferential statistical methods. The flexible fuzzy number-based questions assessed respondents’ knowledge of cryptocurrencies, blockchain, and trust levels. Responses were aggregated mathematically, and demographic factors such as age, gender, education, marital status, income, and cryptocurrency ownership were analyzed. Median tests were used for hypothesis testing between cryptocurrency owners and non-owners based on defuzzified values. Relationships among variables were visualized using heatmaps. Additionally, comparative analysis of cryptocurrency preferences was conducted between investors in Hungary and Germany. Insights from this study can help financial institutions tailor investment portfolios by understanding individual cryptocurrency preferences in Hungary.},
  archive      = {J_SOCO},
  author       = {Gyenes, Zoltán and Jónás, Tamás},
  doi          = {10.1007/s00500-025-10914-3},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5173-5191},
  shortjournal = {Soft Comput.},
  title        = {Analysis of cryptocurrency preferences in hungary using flexible fuzzy numbers},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ExpTODIM-VIKOR framework for multiple-attribute group decision-making with 2-tuple linguistic neutrosophic numbers and applications to quality evaluation of cultural heritage-oriented rural landscapes. <em>SOCO</em>, <em>29</em>(15), 5159-5172. (<a href='https://doi.org/10.1007/s00500-025-10672-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rural areas embody the deepest thoughts and wisdom of the Chinese nation and are the root of China’s excellent traditional culture. At present, the essence of rural construction is the revitalization of rural culture. With the development of the times, the spiritual and cultural needs of the rural population are also growing due to significant improvements in material living conditions. As an important carrier of rural culture, landscape serves as a starting point for research into rural landscape design with the concept of cultural inheritance. Reasonably and effectively exploring cultural elements and discussing the related theories and practical issues of rural landscape design from a new perspective are crucial for maintaining the vitality of rural areas and promoting sustainable rural development. The quality evaluation of cultural heritage-oriented rural landscapes within the rural revitalization framework is a task well-suited for multi-attribute group decision-making (MAGDM). Recently, the Exponential TODIM (ExpTODIM) and VIKOR methods have been introduced to address MAGDM challenges. For handling uncertain data during this evaluation, the 2-tuple linguistic neutrosophic sets (2TLNSs) have been introduced as an effective tool. In this research, the 2-tuple linguistic neutrosophic number ExpTODIM-VIKOR (2TLNN-ExpTODIM-VIKOR) method is introduced to tackle MAGDM challenges using 2TLNSs. Furthermore, a numerical study focusing on the quality evaluation of cultural heritage-oriented rural landscapes in the context of rural revitalization is conducted to validate the effectiveness of the 2TLNN-ExpTODIM-VIKOR approach. The major contributions are illustrated: (1) ExpTODIM and VIKOR approach are extended to the 2TLNNs; (2) entropy approach is expressed to obtain the weight under 2TLNNs; (3) 2TLNN-ExpTODIM-VIKOR approach is expressed forward MAGDM under 2TLNNs; (4) Finally, numerical example for quality evaluation for cultural heritage-oriented rural landscapes within the rural revitalization context and comparative analysis are expressed to validate the 2TLNN-ExpTODIM-VIKOR approach.},
  archive      = {J_SOCO},
  author       = {Zhang, Yuan and Gao, Yan},
  doi          = {10.1007/s00500-025-10672-2},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5159-5172},
  shortjournal = {Soft Comput.},
  title        = {ExpTODIM-VIKOR framework for multiple-attribute group decision-making with 2-tuple linguistic neutrosophic numbers and applications to quality evaluation of cultural heritage-oriented rural landscapes},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new optimization model for multi-project scheduling considering dynamic resource allocation. <em>SOCO</em>, <em>29</em>(15), 5143-5158. (<a href='https://doi.org/10.1007/s00500-025-10891-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an extension of the Resource-Constrained Multi-Project Scheduling Problem (RCMPSP) model, aimed at minimizing the completion time of multiple projects. The proposed model integrates dynamic resource allocation to ensure more effective utilization of resources. It considers the performance of resource allocation across various activities and permits resource reassignment based on the current project conditions. Unlike traditional static approaches, where resource allocation for each activity is predetermined and inflexible over time, the proposed model allocates resources dynamically based on the performance of each activity. The results, accompanied by sensitivity analysis, indicate that the model can successfully prioritize critical activities and adjust resource allocation dynamically. It underscores the importance of a balanced and coordinated approach to resource allocation, emphasizing that the combined effect of multiple resources is more beneficial than focusing on individual ones. Moreover, managerial insights from this study highlight the necessity for managers to consider the relative prioritization of global resources concerning their overall impact on project outcomes. The findings underscore the value of the proposed model as a powerful tool for handling resource-constrained multi-project environments, advancing theoretical understanding and influencing project management practices.},
  archive      = {J_SOCO},
  author       = {Soltan, Sajad and Ashrafi, Maryam},
  doi          = {10.1007/s00500-025-10891-7},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5143-5158},
  shortjournal = {Soft Comput.},
  title        = {A new optimization model for multi-project scheduling considering dynamic resource allocation},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calculating probabilities with LR fuzzy random variables. <em>SOCO</em>, <em>29</em>(15), 5129-5141. (<a href='https://doi.org/10.1007/s00500-025-10877-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In experimental practice, especially where the human factor plays an important role, we are faced with the need to analyze phenomena burdened with two types of uncertainty simultaneously: randomness and a lack of precision. While probability theory deals with randomness, and we cope with imprecision using the theory of fuzzy sets, combining both these descriptions is neither straightforward nor simple. Even seemingly simple tasks, such as calculating the probability of an event for a fuzzy random variable, are not obvious. In this contribution, we propose a method of calculating probabilities related to the so-called LR fuzzy random variables. Besides indicating the general method, we show how, in situations where it is difficult to obtain an analytical solution, it is possible to determine the desired probability using numerical methods, referring to the Monte Carlo simulations. The considered approach is illustrated with examples, also based on the real-life dataset.},
  archive      = {J_SOCO},
  author       = {Parchami, Abbas and Grzegorzewski, Przemysław and Romaniuk, Maciej},
  doi          = {10.1007/s00500-025-10877-5},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5129-5141},
  shortjournal = {Soft Comput.},
  title        = {Calculating probabilities with LR fuzzy random variables},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the pierce sheaf representation of de morgan residuated lattices. <em>SOCO</em>, <em>29</em>(15), 5115-5128. (<a href='https://doi.org/10.1007/s00500-025-10862-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, by characterizing the connected components of the prime spectrum (the set of all prime ideals) of a De Morgan residuated lattice, we define and investigate the Pierce sheaf representation of a De Morgan residuated lattice. Among other results, by using the Pierce stalks, we give some characterizations for Stonean residuated lattices, involution residuated lattices, and separated residuated lattices.},
  archive      = {J_SOCO},
  author       = {Rostami, Esmaeil and Ghorbani, Shokoofeh},
  doi          = {10.1007/s00500-025-10862-y},
  journal      = {Soft Computing},
  month        = {8},
  number       = {15},
  pages        = {5115-5128},
  shortjournal = {Soft Comput.},
  title        = {On the pierce sheaf representation of de morgan residuated lattices},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
