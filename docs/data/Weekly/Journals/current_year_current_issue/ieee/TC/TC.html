<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc">TC - 25</h2>
<ul>
<li><details>
<summary>
(2025). Optimizing multi-DNN parallel inference performance in MEC networks: A resource-aware and dynamic DNN deployment scheme. <em>TC</em>, <em>74</em>(11), 3938-3952. (<a href='https://doi.org/10.1109/TC.2025.3605749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Multi-access Edge Computing (MEC) has empowered Internet of Things (IoT) devices and edge servers to deploy sophisticated Deep Neural Network (DNN) applications, enabling real-time inference. Many concurrent inference requests and intricate DNN models demand efficient multi-DNN inference in MEC networks. However, the resource-limited IoT device/edge server and expanding model size force models to be dynamically deployed, resulting in significant undesired energy consumption. In addition, parallel multi-DNN inference on the same device complicates the inference process due to the resource competition among models, increasing the inference latency. In this paper, we propose a Resource-aware and Dynamic DNN Deployment (R3D) scheme with the collaboration of end-edge-cloud. To mitigate resource competition and waste during multi-DNN parallel inference, we develop a Resource Adaptive Management (RAM) algorithm based on the Roofline model, which dynamically allocates resources by accounting for the impact of device-specific performance bottlenecks on inference latency. Additionally, we design a Deep Reinforcement Learning (DRL)-based online optimization algorithm that dynamically adjusts DNN deployment strategies to achieve fast and energy-efficient inference across heterogeneous devices. Experiment results demonstrate that R3D is applicable in MEC environments and performs well in terms of inference latency, resource utilization, and energy consumption.},
  archive      = {J_TC},
  author       = {Tong Zheng and Yuanguo Bi and Guangjie Han and Xingwei Wang and Yuheng Liu and Yufei Liu and Xiangyi Chen},
  doi          = {10.1109/TC.2025.3605749},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3938-3952},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing multi-DNN parallel inference performance in MEC networks: A resource-aware and dynamic DNN deployment scheme},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCC: Synchronization congestion control for multi-tenant learning over geo-distributed clouds. <em>TC</em>, <em>74</em>(11), 3911-3924. (<a href='https://doi.org/10.1109/TC.2025.3604486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning over geo-distributed clouds enables joint training of data located in different regions, alleviating the burden of transferring large volumes of training datasets, which greatly saves bandwidth. However, the limited capacity of WAN links slows down the inter-cloud communications, which significantly decelerates the synchronization of distributed machine learning over geo-distributed clouds. Besides, the multi-tenancy in clouds results in multiple training tasks running simultaneously, whose synchronizations consistently compete for the limited WAN bandwidth with each other, which further aggravates the training performance of each task. While existing works optimize synchronizations through techniques like gradient compression, multi-resource interleaving and so on, none of them targets at the synchronization congestion especially due to multi-tenant learning, which results in inferior training performance. To solve these problems, we propose a simple but effective scheme, SCC, for fast and efficient multi-tenant learning via synchronization congestion control. SCC monitors the cross-cloud network conditions and evaluates the synchronization congestion level based on the round-trip transmission time for each synchronization. Then SCC alleviates synchronization congestion via controlling the synchronization frequency according to the synchronization congestion level in a probabilistic way. Extensive experiments are conducted within our testbeds consisted of 16 NVIDIA V100 GPUs to evaluate the performance of SCC, and comparison results show that SCC can reduce the average training completion time and makespan by up to 28.6% and 43.2% over SAP-SGD [1]. Targeted experiments are conducted to demonstrate the effectiveness and robustness of SCC.},
  archive      = {J_TC},
  author       = {Chengxi Gao and Fuliang Li and Kejiang Ye and Yang Wang and Pengfei Wang and Xingwei Wang and Chengzhong Xu},
  doi          = {10.1109/TC.2025.3604486},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3911-3924},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SCC: Synchronization congestion control for multi-tenant learning over geo-distributed clouds},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reputation-based energy-efficient transaction propagation mechanism for blockchain-enabled multi-access edge computing. <em>TC</em>, <em>74</em>(11), 3897-3910. (<a href='https://doi.org/10.1109/TC.2025.3604480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain strengthens reliable collaboration among entities through its transparency, immutability, and traceability, leading to its integration into Multi-access Edge Computing (MEC) and promoting the development of a trusted JointCloud. However, existing transaction propagation mechanisms require MEC devices to consume significant computing resources for complex transaction verification, increasing their vulnerability to malicious attacks. Adversaries can exploit this by flooding the blockchain network with spam transactions, aiming to deplete device energy and disrupt system performance. To cope with these issues, this paper proposes a reputation-based energy-efficient transaction propagation mechanism that alleviates spam transaction attacks while reducing computing resources and energy consumption. Firstly, we design a subjective logic-based reputation scheme that assesses node trust by integrating local and recommended opinions and incorporates opinion acceptance to counteract false evidence. Then, we optimize the transaction verification method by adjusting transaction discard and verification probabilities based on the proposed reputation scheme to curb the propagation of spam transactions and reduce verification consumption. Finally, we enhance the transaction transmission strategy by prioritizing nodes with higher reputations, enhancing both resilience to spam transactions and transmission reliability. A series of simulations demonstrates the effectiveness of the proposed mechanism.},
  archive      = {J_TC},
  author       = {Xijia Lu and Qiang He and Xingwei Wang and Jaime Lloret and Peichen Li and Ying Qian and Min Huang},
  doi          = {10.1109/TC.2025.3604480},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3897-3910},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A reputation-based energy-efficient transaction propagation mechanism for blockchain-enabled multi-access edge computing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-power multiplier designs by leveraging correlations of 2$\times$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>×</mml:mo></mml:math>2 encoded partial products. <em>TC</em>, <em>74</em>(11), 3888-3896. (<a href='https://doi.org/10.1109/TC.2025.3604478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multipliers, particularly those with small bit widths, are essential for modern neural network (NN) applications. In addition, multiple-precision multipliers are in high demand for efficient NN accelerators; therefore, recursive multipliers used in low-precision fusion schemes are gaining increasing attention. In this work, we design exact recursive multipliers based on customized approximate full adders (AFAs) for low-power purposes. Initially, the partial products (PPs) encoded by 2$\times$2 multiplications are analyzed, which reveals the correlations among adjacent PPs. Based on these correlations, we propose 4$\times$4 recursive multiplier architectures where certain full adders (FAs) can be simplified without affecting the correctness of the multiplication. Manually and synthesis tool-based FA simplifications are performed separately. The obtained 4$\times$4 multipliers are then used to construct 8$\times$8 multipliers based on a low-power recursive architecture. Finally, the proposed signed and unsigned 4$\times$4 and 8$\times$8 multipliers are evaluated using a 28nm CMOS technology. Compared with DesignWare (DW) multipliers, the proposed signed and unsigned 4$\times$4 multipliers achieve power reductions of 16.5% and 11.6%, respectively, without compromising area or delay; alternatively, the delay can be reduced by 20.9% and 39.4%, respectively, without compromising power or area. For signed and unsigned 8$\times$8 multipliers, the maximum power reductions are 9.7% and 13.7%, respectively, albeit with a trade-off in area.},
  archive      = {J_TC},
  author       = {Ao Liu and Siting Liu and Hui Wang and Qin Wang and Fabrizio Lombardi and Zhigang Mao and Honglan Jiang},
  doi          = {10.1109/TC.2025.3604478},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3888-3896},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Low-power multiplier designs by leveraging correlations of 2$\times$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>×</mml:mo></mml:math>2 encoded partial products},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-path bound for parallel tasks with conditional branches. <em>TC</em>, <em>74</em>(11), 3873-3887. (<a href='https://doi.org/10.1109/TC.2025.3604469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel execution and conditional execution are increasingly prevalent in modern embedded systems. In real-time scheduling, a fundamental problem is how to upper-bound the response times of a task. Recent work applied the multi-path technique to reduce the response time bound for tasks with parallel execution, but left tasks with conditional execution as an open problem. This paper focuses on upper-bounding response times for tasks with both parallel execution and conditional execution using the multi-path technique. By designing a delicate abstraction regarding the multiple paths of various conditional branches, we derive a new response time bound. We further apply this response time bound into the scheduling of multiple parallel tasks with conditional branches. Experiments demonstrate that the proposed bound significantly advances the state-of-the-art, reducing the response time bound by 9.4% and improving the schedulability by 31.2% on average.},
  archive      = {J_TC},
  author       = {Qingqiang He and Nan Guan and Zhe Jiang and Mingsong Lv},
  doi          = {10.1109/TC.2025.3604469},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3873-3887},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-path bound for parallel tasks with conditional branches},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The metric relationship between extra connectivity and extra diagnosability of multiprocessor systems. <em>TC</em>, <em>74</em>(11), 3860-3872. (<a href='https://doi.org/10.1109/TC.2025.3604468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As multiprocessor systems scale up, $h$-extra connectivity and $h$-extra diagnosability serve as two pivotal metrics for assessing the reliability of the underlying interconnection networks. To ensure that each component of the survival graph holds no fewer than $h + 1$ vertices, the $h$-extra connectivity and $h$-extra diagnosability have been proposed to characterize the fault tolerability and self-diagnosing capability of networks, respectively. Many efforts have been made to establish the quantifiable relationship between these metrics but it is less than optimal. This work addresses the flaws of the existing results and proposes a novel proof to determine the metric relationship between $h$-extra connectivity and $h$-extra diagnosability under the PMC and MM* models. Our approach overcomes the defect of previous results by abandoning the network’s regularity and independence number. Furthermore, we apply the suggested metric to establish the $h$-extra diagnosability of a new network class, named generalized exchanged X-cube-like network $GEXC(s,t)$, which takes dual-cube-like network, generalized exchanged hypercube, generalized exchanged crossed cube, and locally generalized exchanged twisted cube as special cases. Finally, we propose the $h$-extra diagnosis strategy ($h$-EDS) and design two self-diagnosis algorithms AhED-PMC and AhED-MM*, and then conduct experiments on $GEXC(s,t)$ and the real-world network DD-$g648$ to show the high accuracy and superior performance of the proposed algorithms.},
  archive      = {J_TC},
  author       = {Yifan Li and Shuming Zhou and Sun-Yuan Hsieh and Qifan Zhang},
  doi          = {10.1109/TC.2025.3604468},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3860-3872},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The metric relationship between extra connectivity and extra diagnosability of multiprocessor systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sketching for heavy item-oriented data stream mining with memory constraints. <em>TC</em>, <em>74</em>(11), 3845-3859. (<a href='https://doi.org/10.1109/TC.2025.3604467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and fast data stream mining is critical to many tasks, including real-time series analysis for mobile sensor data, big data management and machine learning. Various heavy-oriented item detection tasks, such as identifying heavy hitters, heavy changers, persistent items, and significant items, have garnered considerable attention from both industry and academia. Unfortunately, as data stream speeds continue to increase and the available memory, particularly in L1 cache, remains limited for real-time processing, existing schemes face challenges in simultaneously achieving high detection accuracy, memory efficiency, and fast update throughput, as we reveal. To tackle this conundrum, we propose a versatile and elegant sketch framework named Tight-Sketch, which supports a spectrum of heavy-based detection tasks. Recognizing that, in practice, most items are cold (non-heavy/persistent/significant), we implement distinct eviction strategies for different item types. This approach allows us to swiftly discard potentially cold items while offering enhanced protection to hot ones (heavy/persistent/significant). Additionally, we introduce an eviction method based on stochastic decay, ensuring that Tight-Sketch incurs only small one-sided errors without overestimation. To further enhance detection accuracy under extremely constrained memory allocations, we introduce Tight-Opt, a variant incorporating two optimization strategies. We conduct extensive experiments across various detection tasks to demonstrate that Tight-Sketch significantly outperforms existing methods in terms of both accuracy and update speed. Furthermore, by utilizing Single Instruction Multiple Data (SIMD) instructions, we enhance Tight-Sketch’s update throughput by up to 36%. We also implement Tight-Sketch on FPGA to validate its practicality and low resource overhead in hardware deployments.},
  archive      = {J_TC},
  author       = {Weihe Li and Paul Patras},
  doi          = {10.1109/TC.2025.3604467},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3845-3859},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient sketching for heavy item-oriented data stream mining with memory constraints},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability-aware optimization of task offloading for UAV-assisted edge computing. <em>TC</em>, <em>74</em>(11), 3832-3844. (<a href='https://doi.org/10.1109/TC.2025.3604463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed aerial vehicles (UAV) are widely used for edge computing in poor infrastructure scenarios due to their deployment flexibility and mobility. In UAV-assisted edge computing systems, multiple UAVs can cooperate with the cloud to provide superior computing capability for diverse innovative services. However, many service-related computational tasks may fail due to the unreliability of UAVs and wireless transmission channels. Diverse solutions were proposed, but most of them employ time-driven strategies which introduce unwanted decision waiting delays. To address this problem, this paper focuses on a task-driven reliability-aware cooperative offloading problem in UAV-assisted edge-enhanced networks. The issue is formulated as an optimization problem which jointly optimizes UAV trajectories, offloading decisions, and transmission power, aiming to maximize the long-term average task success rate. Considering the discrete-continuous hybrid action space of the problem, a dependence-aware latent-space representation algorithm is proposed to represent discrete-continuous hybrid actions. Furthermore, we design a novel deep reinforcement learning scheme by combining the representation algorithm and a twin delayed deep deterministic policy gradient algorithm. We compared our proposed algorithm with four alternative solutions via simulations and a realistic Kubernetes testbed-based setup. The test results show how our scheme outperforms the other methods, ensuring significant improvements in terms of task success rate.},
  archive      = {J_TC},
  author       = {Hao Hao and Changqiao Xu and Wei Zhang and Xingyan Chen and Shujie Yang and Gabriel-Miro Muntean},
  doi          = {10.1109/TC.2025.3604463},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3832-3844},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reliability-aware optimization of task offloading for UAV-assisted edge computing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A computation-quantized training framework to generate accuracy lossless QNNs for one-shot deployment in embedded systems. <em>TC</em>, <em>74</em>(11), 3818-3831. (<a href='https://doi.org/10.1109/TC.2025.3603732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantized Neural Networks (QNNs) have received increasing attention, since they can enrich intelligent applications deployed on embedded devices with limited resources, such as mobile devices and AIoT systems. Unfortunately, the numerical and computational discrepancies between training systems (i.e., servers) and deployment systems (e.g., embedded ends) may lead to large accuracy drop for QNNs in real deployments. We propose a Computation-Quantized Training Framework (CQTF), which simulates deployment-time fixed-point computation during training to enable one-shot, lossless deployment. The training procedure of CQTF is built upon a well-formulated quantization-specific numerical representation that quantifies both numerical and computational discrepancies between training and deployment. Leveraging this representation, forward propagation executes all computations in quantization mode to simulate deployment-time inference, while backward propagation identifies and mitigates gradient vanishing through an efficient floating-point gradient update scheme. Benchmark-based experiments demonstrate the efficiency of our approach, which can achieve no accuracy loss from training to deployment. Compared with existing five frameworks, the deployed accuracy of CQTF can be improved by up to 18.41%.},
  archive      = {J_TC},
  author       = {Xingzhi Zhou and Wei Jiang and Jinyu Zhan and Lingxin Jin and Lin Zuo},
  doi          = {10.1109/TC.2025.3603732},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3818-3831},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A computation-quantized training framework to generate accuracy lossless QNNs for one-shot deployment in embedded systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing in-network computing deployment via collaboration across planes. <em>TC</em>, <em>74</em>(11), 3805-3817. (<a href='https://doi.org/10.1109/TC.2025.3603730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new paradigm of In-network computing (INC) permits service computation to be executed within network paths, rather than solely on dedicated servers. Although the programmable data plane has showcased notable performance advantages for INC application deployments, its effectiveness is constrained by resource limitations, potentially impeding the expressiveness and scalability of these deployments. Conversely, delegating computational tasks to the control plane, supported by general-purpose servers with abundant resources, offers increased flexibility. Nonetheless, this strategy compromises efficiency to a considerable extent, particularly when the system operates under heavy load. To simultaneously exploit the efficiency of data plane and the flexibility of control plane, we propose Carlo, a cross-plane collaborative optimization framework to support the network-wide deployment of multiple INC applications across both the control and data plane. Carlo first analyzes resource requirements of various INC applications across different planes. It then establishes mathematical models for resource allocation in cross-plane and automatically generates solutions using proposed algorithms. We have implemented the prototype of Carlo on Intel Tofino ASIC switches and DPDK. Experimental results demonstrate that Carlo can effectively trade off between computation time and deployment performance while avoiding performance degradation.},
  archive      = {J_TC},
  author       = {Xiaoquan Zhang and Lin Cui and WaiMing Lau and Fung Po Tso and Yuhui Deng and Weijia Jia},
  doi          = {10.1109/TC.2025.3603730},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3805-3817},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enhancing in-network computing deployment via collaboration across planes},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Load balancing scheduling for batch-ordered job-store: Online vs. offline. <em>TC</em>, <em>74</em>(11), 3778-3791. (<a href='https://doi.org/10.1109/TC.2025.3603725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient resource utilization is crucial in real-world applications, especially for balancing loads across machines handling specific job types. This paper introduces a novel batch-ordered job-store scheduling model, where jobs in a batch are scheduled sequentially, with their operations allocated in a round-robin fashion across two scenarios. We establish that this problem is NP-hard and analyze it in both online and offline settings. In the online case, we first examine the exclusive scenario, where operations within the same job must be scheduled on different machines, and show that a load greedy (LG) algorithm achieves a tight competitive ratio of $2-\frac{1}{m}$, with $m$ representing the number of machines. Next, we consider the circular scenario, which requires maintaining the circular order of operations across ordered machines. In this context, we analyze potential anomalies in load distribution during local optimality achieved by the ordered load greedy (OLG) algorithm and provide bounds on the occurrence of these anomalies and the maximum load in each local scheduling round. In the offline case, we abstract each OLG scheduling process as a generalized circular sequence alignment (CSA) problem and develop a dynamic programming-based matching (DPM) algorithm to solve it. To further enhance load balancing, we develop a dynamic programming-based optimization (DPO) algorithm to schedule multiple jobs simultaneously in both scenarios. Experimental results confirm the efficiency of DPM for the CSA problem, and we validate the load balancing effectiveness of both online and offline algorithms using real traffic datasets. These theoretical findings and algorithmic implementations lay a solid groundwork for future practical advancements.},
  archive      = {J_TC},
  author       = {Mengbing Zhou and Yang Wang and Bocong Zhao and Chengzhong Xu},
  doi          = {10.1109/TC.2025.3603725},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3778-3791},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Load balancing scheduling for batch-ordered job-store: Online vs. offline},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information sharing in multi-tenant metaverse via intent-driven multicasting. <em>TC</em>, <em>74</em>(11), 3763-3777. (<a href='https://doi.org/10.1109/TC.2025.3603720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-tenant metaverse enables multiple users in a common virtual world to interact with each other online. Information sharing will occur when interactions between a user and the environment are multicast to other users by an interactive metaverse (IM) service. However, ineffective information-sharing strategies intensify competitions among users for limited resources in networks, and fail to interpret optimization intent prompts conveyed in high-level natural languages, ultimately diminishing user immersion. In this paper, we explore reliable information sharing in a multi-tenant metaverse with time-varying resource capacities and costs, where IM services are unreliable and alter the volumes of data processed by them, while the service provider dynamically adjusts global intent to minimize multicast delays and costs. To this end, we first formulate the information sharing problem as a Markov decision process and show its NP-hardness. Then, we propose a learning-based system GTP, which combines the proximal policy optimization reinforcement learning with feature extraction networks, including graph attention network and gated recurrent unit, and a Transformer encoder for multi-feature comparison to process a sequence of incoming multicast requests without the knowledge of future arrival information. The GTP operates through three modules: a deployer that allocates primary and backup IM services across the network to minimize a weighted goal of server computation costs and communication distances between users and services, an intent extractor that dynamically infers provider intent conveyed in natural language, and a router that constructs on-demand multicast routing trees adhering to users, the provider, and network constraints. We finally conduct theoretical and empirical analysis on the proposed algorithms for the system. Experimental results show that the proposed algorithms are promising, and superior to their comparison baseline algorithms.},
  archive      = {J_TC},
  author       = {Yu Qiu and Min Chen and Weifa Liang and Lejun Ai and Dusit Niyato},
  doi          = {10.1109/TC.2025.3603720},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3763-3777},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Information sharing in multi-tenant metaverse via intent-driven multicasting},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EABE-PUFPH: Efficient attribute-based encryption with reliable policy updating under full policy hiding. <em>TC</em>, <em>74</em>(11), 3750-3762. (<a href='https://doi.org/10.1109/TC.2025.3603717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ciphertext-policy attribute-based encryption (CP-ABE) has garnered significant attention for enabling fine-grained access control over encrypted data in cloud environments. However, in traditional CP-ABE schemes, access policies are transmitted in plaintext, which can lead to sensitive information leakage. To mitigate this risk, hiding access policies has become essential. Under the condition of full hidden access policies, realizing efficient and accurate decryption and dynamic policy updating has become an urgent challenge. To tackle these challenges, we present an efficient attribute-based encryption with reliable policy updating under full policy hiding (EABE-PUFPH) scheme, which effectively integrates full policy hiding with policy updating capabilities. Furthermore, we conduct a rigorous security analysis and performance evaluation of the EABE-PUFPH scheme. Evaluation results show that the EABE-PUFPH scheme achieves full hidden access policies without affecting decryption efficiency, and its efficiency surpasses other similar schemes that achieve full policy hiding.},
  archive      = {J_TC},
  author       = {Chenghao Gu and Jiguo Li and Yichen Zhang and Yang Lu and Jian Shen},
  doi          = {10.1109/TC.2025.3603717},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3750-3762},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EABE-PUFPH: Efficient attribute-based encryption with reliable policy updating under full policy hiding},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FSA-hash: Flow-size-aware sketch hashing for software switches. <em>TC</em>, <em>74</em>(11), 3736-3749. (<a href='https://doi.org/10.1109/TC.2025.3603716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern data centers and enterprise networks, software switches have become critical components for achieving flexible and efficient network management. Due to resource constraints in software switches, sketches have emerged as a promising approach for network traffic measurement. However, their accuracy is often impacted by hash collisions. Existing hash functions treat all collisions equally, failing to account for the differing impacts of collisions involving elephant flows versus mouse flows. We propose FSA-Hash, a novel flow-size-aware hashing scheme that separates elephant flows from each other and from mouse flows, minimizing the most detrimental collisions. FSA-Hash is designed based on two insights: separating elephant flows from mouse flows avoids overestimating mouse flows, while separating elephant flows from each other enables accurate heavy-hitter detection. We implement FSA-Hash using machine learning models trained on network traffic data (LFSA-Hash), and also design a lightweight online variant (OLFSA-Hash) that learns the hash model solely from sketch queries on the software switch, obviating traffic collection overheads. Evaluations across four sketches and two tasks demonstrate FSA-Hash’s superior accuracy over standard hash functions. Moreover, OLFSA-Hash closely matches LFSA-Hash’s performance, making it an attractive option for adaptively refining the hash model without monitoring traffic.},
  archive      = {J_TC},
  author       = {Fuliang Li and Kejun Guo and Yiming Lv and Jiaxing Shen and Yuting Liu and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2025.3603716},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3736-3749},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FSA-hash: Flow-size-aware sketch hashing for software switches},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NetKG: Synthesizing interpretable network router configurations with knowledge graph. <em>TC</em>, <em>74</em>(11), 3722-3735. (<a href='https://doi.org/10.1109/TC.2025.3603712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced router configuration synthesizers aim to prevent network outages by automatically synthesizing configurations that implement routing protocols. However, the lack of interpretability makes operators uncertain about how low-level configurations are synthesized and whether the automatically generated configurations correctly align with routing intents. This limitation restricts the practical deployment of synthesizers. In this paper, we present NetKG, an interpretable configuration synthesis tool. $(i)$ NetKG leverages a knowledge graph as the intermediate representation for configurations, reformulating the configuration synthesis problem as a configuration knowledge completion task; $(ii)$ NetKG regards network intents as query tasks that need to be satisfied in the current configuration space, achieving this through knowledge reasoning and completion; $(iii)$ NetKG explains the synthesis process and the consistency between configuration and intent through the configuration knowledge involved in reasoning and completion. We show that NetKG can scale to realistic networks and automatically synthesize intent-compliant configurations for static routes, OSPF, and BGP. It can explain the consistency between configuration and intent at different granularities through a visual interface. Experimental results indicate that NetKG synthesizes configurations in 2 minutes for a network with up to 197 routers, which is 7.37x faster than the SMT-based synthesizer.},
  archive      = {J_TC},
  author       = {Zhenbei Guo and Fuliang Li and Peng Zhang and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2025.3603712},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3722-3735},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NetKG: Synthesizing interpretable network router configurations with knowledge graph},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable encrypted deduplication based on location-hiding secret sharing of data keys. <em>TC</em>, <em>74</em>(11), 3710-3721. (<a href='https://doi.org/10.1109/TC.2025.3603710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encrypted deduplication is attractive because it can provide high storage efficiency while protecting data privacy. Most existing schemes achieve encrypted deduplication against brute-force attacks (BFAs) based on server-aided encryption. Unfortunately, the centralized key server in server-aided encryption can potentially become a single point of failure. To this end, distributed server-aided encryption is presented, which splits a system-level master key into multiple shares and distributes them across several key servers. However, it is hard to improve security and scalability with this method simultaneously. This paper presents a secure and scalable encrypted deduplication scheme ScalaDep. ScalaDep achieves a new design paradigm centered on location-hiding secret sharing of data keys. As the number of deployed key servers increases, the attack cost of adversaries increases while the number of requests handled by each key server decreases, enhancing both scalability and security. Furthermore, we propose a two-phase duplicate detection method for our paradigm, which utilizes short hashes and key identifiers to achieve secure duplicate detection against BFAs. Additionally, based on the allreduce algorithm, ScalaDep enables all key servers to collaboratively record the number of client requests and resist online BFAs by enforcing rate limiting. Security analysis and performance evaluation demonstrate the security and efficiency of ScalaDep.},
  archive      = {J_TC},
  author       = {Guanxiong Ha and Yuchen Chen and Chunfu Jia and Keyan Chen and Rongxi Wang and Qiaowen Jia},
  doi          = {10.1109/TC.2025.3603710},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3710-3721},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scalable encrypted deduplication based on location-hiding secret sharing of data keys},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An on-board executable pareto-based iterated local search algorithm for embedded multi-core processor task scheduling. <em>TC</em>, <em>74</em>(11), 3696-3709. (<a href='https://doi.org/10.1109/TC.2025.3603699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of wearable electronic technology has facilitated the integration of smart wearable devices into artificial intelligence (AI)-driven medical assisted diagnosis. Embedded multi-core processors (MPs) have gradually emerged as pivotal hardware components for smart wearable medical diagnostic devices due to their high performance and flexibility. However, embedded MPs face the challenge of balancing performance, power consumption, and load-balancing. In response, we introduce a Pareto-based iterated local search (PILS) algorithm for task scheduling, which systematically optimizes multiple objectives, alongside a task list model to reduce the dimension of the decision space and enhance scheduling performance. In addition, we present a two-stage discretization scheme to ensure that the proposed algorithm offers meaningful guidance throughout the scheduling process. Simulation and on-board testing results show that the proposed algorithm effectively optimizes energy consumption, task execution time, and load-balancing in embedded MPs task scheduling, indicating the potential of the proposed algorithm in enhancing the performance of smart wearable medical diagnostic devices powered by embedded MPs.},
  archive      = {J_TC},
  author       = {Qinglin Zhao and Lixin Zhang and Qi Pan and Kunbo Cui and Mingqi Zhao and Fuze Tian and Bin Hu},
  doi          = {10.1109/TC.2025.3603699},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3696-3709},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An on-board executable pareto-based iterated local search algorithm for embedded multi-core processor task scheduling},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thermal elasticity-aware host resource provision for carbon efficiency on virtualized servers. <em>TC</em>, <em>74</em>(11), 3682-3695. (<a href='https://doi.org/10.1109/TC.2025.3603698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Servers in modern data centers face increasing challenges from energy inefficiency and thermal-related outages, both of which significantly contribute to their overall carbon footprint. These challenges often arise from a lack of coordination between computational resource provisioning and thermal management capabilities. This paper introduces the concept of thermal elasticity, a system’s intrinsic ability to absorb thermal stress without requiring additional cooling, as a guiding metric for sustainable thermal management. Building on this, we propose a collaborative in-band and out-of-band resource provisioning framework that adjusts CPU allocation based on real-time thermal feedback. By leveraging a machine learning model and runtime monitoring, the framework dynamically provisions CPU clusters to virtual machines co-located on the same host. Evaluations on real servers with multiple workloads show that our method reduces peak power consumption from 5.2% to 9.6%, and lowers peak temperatures between 4${^{\boldsymbol{\circ}}}$C and 6.5${^{\boldsymbol{\circ}}}$C (up to 40${^{\boldsymbol{\circ}}}$C in extreme cases). Carbon emissions are also reduced from 7% to 37% during SPEC benchmark runs. These results highlight the framework’s potential to alleviate stress on power and cooling infrastructure, thereby enhancing energy efficiency, reducing carbon footprint, and improving service continuity during thermal challenges.},
  archive      = {J_TC},
  author       = {Da Zhang and Haojun Xia and Xiaotong Wang and Yanchang Feng and Haohao Liu and Bibo Tu},
  doi          = {10.1109/TC.2025.3603698},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3682-3695},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Thermal elasticity-aware host resource provision for carbon efficiency on virtualized servers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TeraPool: A physical design aware, 1024 RISC-V cores shared-l1-memory scaled-up cluster design with high bandwidth main memory link. <em>TC</em>, <em>74</em>(11), 3667-3681. (<a href='https://doi.org/10.1109/TC.2025.3603692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared L1-memory clusters of streamlined instruction processors (processing elements - PEs) are commonly used as building blocks in modern, massively parallel computing architectures (e.g. GP-GPUs). Scaling out these architectures by increasing the number of clusters incurs computational and power overhead, caused by the requirement to split and merge large data structures in chunks and move chunks across memory hierarchies via the high-latency global interconnect. Scaling up the cluster reduces buffering, copy, and synchronization overheads. However, the complexity of a fully connected cores-to-L1-memory crossbar grows quadratically with Processing Element (PE)-count, posing a major physical implementation challenge. We present TeraPool, a physically implementable, ${\boldsymbol &gt;} 1000$ floating-point-capable RISC-V PEs scaled-up cluster design, sharing a Multi-MegaByte ${\boldsymbol &gt;} 4000$-banked L1 memory via a low latency hierarchical interconnect (1-7/9/11 cycles, depending on target frequency). Implemented in 12 nm FinFET technology, TeraPool achieves near-gigahertz frequencies (910 MHz) typical, 0.80 V/25 $^{\boldsymbol{\circ}}$C. The energy-efficient hierarchical PE-to-L1-memory interconnect consumes only 9-13.5 pJ for memory bank accesses, just 0.74-1.1${\boldsymbol \times}$ the cost of a FP32 FMA. A high-bandwidth main memory link is designed to manage data transfers in/out of the shared L1, sustaining transfers at the full bandwidth of an HBM2E main memory. At 910 MHz, the cluster delivers up to 1.89 single precision TFLOP/s peak performance and up to 200 GFLOP/s/W energy efficiency (at a high IPC/PE of 0.8 on average) in benchmark kernels, demonstrating the feasibility of scaling a shared-L1 cluster to a thousand PEs, four times the PE count of the largest clusters reported in literature.},
  archive      = {J_TC},
  author       = {Yichao Zhang and Marco Bertuletti and Chi Zhang and Samuel Riedel and Diyou Shen and Bowen Wang and Alessandro Vanelli-Coralli and Luca Benini},
  doi          = {10.1109/TC.2025.3603692},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3667-3681},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TeraPool: A physical design aware, 1024 RISC-V cores shared-l1-memory scaled-up cluster design with high bandwidth main memory link},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRRQ: Privacy-preserving resilient RkNN query over encrypted outsourced multiattribute data. <em>TC</em>, <em>74</em>(11), 3652-3666. (<a href='https://doi.org/10.1109/TC.2025.3603688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional reverse k-nearest neighbor (RkNN) query schemes typically assume that users are available online in real-time for interactive key reception, overlooking scenarios where users might be offline. Moreover, existing privacy-preserving RkNN query schemes primarily focus on user features or spatial data, neglecting the significance of user reputation values. To address these limitations, we propose a privacy-preserving resilient RkNN query scheme over encrypted outsourced multi-attribute data (PRRQ). Specifically, to mitigate the challenges posed by resilient online presence (i.e., non-real-time online) of users for interactive key reception, we incorporate a non-interactive key exchange (NIKE) protocol and the Diffie-Hellman two-party key exchange algorithm to propose a multi-party NIKE algorithm (2K-NIKE), facilitating non-interactive key reception for multiple users. Considering the privacy leakage issues, PRRQ encodes original multi-attribute data (i.e., spatial, feature, and reputation values) alongside query requests based on formalized criteria. Additionally, we integrate the proposed 2K-NIKE and the improved symmetric homomorphic encryption (iSHE) algorithms to encrypt them. Furthermore, catering to the requirements of ciphertext-based RkNN queries, we propose a private RkNN query eligibility-checking (PREC) algorithm and a private reputation-verifying (PRRV) algorithm, which validate the compliance of encrypted outsourced multi-attribute data with query requests. Security analysis demonstrates that PRRQ achieves simulation-based security under an honest-but-curious model. Experimental results show that PRRQ offers superior computational efficiency compared to comparative schemes.},
  archive      = {J_TC},
  author       = {Jing Wang and Haiyong Bao and Na Ruan and Qinglei Kong and Cheng Huang and Hong-Ning Dai},
  doi          = {10.1109/TC.2025.3603688},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3652-3666},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PRRQ: Privacy-preserving resilient RkNN query over encrypted outsourced multiattribute data},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concurrent linguistic error detection (CLED): A new methodology for error detection in large language models. <em>TC</em>, <em>74</em>(11), 3638-3651. (<a href='https://doi.org/10.1109/TC.2025.3603682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of Large Language Models (LLMs) requires dependable operation in the presence of errors in the hardware (caused by for example radiation) as this has become a pressing concern. At the same time, the scale and complexity of LLMs limit the overhead that can be added to detect errors. Therefore, there is a need for low-cost error detection schemes. Concurrent Error Detection (CED) uses the properties of a system to detect errors, so it is an appealing approach. In this paper, we present a new methodology and scheme for error detection in LLMs: Concurrent Linguistic Error Detection (CLED). Its main principle is that the output of LLMs should be valid and generate coherent text; therefore, when the text is not valid or differs significantly from the normal text, it is likely that there is an error. Hence, errors can potentially be detected by checking the linguistic features of the text generated by LLMs. This has the following main advantages: 1) low overhead as the checks are simple and 2) general applicability, so regardless of the LLM implementation details because the text correctness is not related to the LLM algorithms or implementations. The proposed CLED has been evaluated on two LLMs: T5 and OPUS-MT. The results show that with a 1% overhead, CLED can detect more than 87% of the errors, making it suitable to improve LLM dependability at low cost.},
  archive      = {J_TC},
  author       = {Jinhua Zhu and Javier Conde and Zhen Gao and Pedro Reviriego and Shanshan Liu and Fabrizio Lombardi},
  doi          = {10.1109/TC.2025.3603682},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3638-3651},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Concurrent linguistic error detection (CLED): A new methodology for error detection in large language models},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Caravan: Incentive-driven account migration via transaction aggregation in sharded blockchain. <em>TC</em>, <em>74</em>(11), 3609-3622. (<a href='https://doi.org/10.1109/TC.2025.3603672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain sharding is a promising solution for scalability but struggles to reach the expected performance due to the high ratio of cross-shard transactions. Account migration has emerged as a critical approach to optimizing shard performance. However, existing migration solutions suffer from inefficient handling of queued withdrawal transactions from a migrating account and inadequate priority mechanism for migration transaction, resulting in prolonged transaction makespan and reduced system throughput. This paper proposes Caravan, a novel blockchain sharding system for optimizing account migration. First, Caravan proposes a transaction aggregation-based migration scheme to efficiently handle withdrawal congestion post-migration. It incorporates a multi-level Merkle tree and cross-shard synchronization protocol to ensure cross-shard security. Second, Caravan presents an economic incentive-driven priority mechanism that motivates miners to perform transaction aggregation and prioritize migration transactions by increasing the associated revenue. Furthermore, its gas recycling strategy enables users to finance migration costs without awareness or extra expenses. Finally, we develop the Caravan prototype, deploy it on Alibaba Cloud, and experiment with real Ethereum transactions. The results show that compared to the state-of-the-art account migration schemes, Caravan significantly mitigates the transaction surge caused by migration, achieving up to a 3.2× throughput improvement and a 65% reduction in transaction confirmation latency. And users share considerable migration costs without extra expenses, significantly reduce system costs.},
  archive      = {J_TC},
  author       = {Yu Tao and Shouchen Zhou and Lu Zhou and Zhe Liu},
  doi          = {10.1109/TC.2025.3603672},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3609-3622},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Caravan: Incentive-driven account migration via transaction aggregation in sharded blockchain},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic bin packing with heterogeneous dependent bins for regionless in geo-distributed clouds. <em>TC</em>, <em>74</em>(11), 3596-3608. (<a href='https://doi.org/10.1109/TC.2025.3602297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud service providers use geo-distributed datacenters to provide resources and services to clients located in different regions. However, uneven population density leads to unbalanced development of geo-distributed datacenters and cloud service providers face a shortage of land resources to further develop datacenters in densely populated regions. Thus, it is a real challenge for cloud service providers to meet the increasing demand from clients in affluent regions with saturated resources and to better utilize underutilized data centers in other regions. To address this challenge, we study an online resource allocation problem in geo-distributed clouds, whose goal is to assign each user request upon arrival to an appropriate geographic cloud region to minimize the resulting peak utilization of resource pools with different cost coefficients. To this end, we formulate the problem as a dynamic bin packing problem with heterogeneous dependent bins where user requests correspond to items to be packed and heterogeneous cloud resources are bins. To solve this online problem with high uncertainty, we propose a simulation based memetic algorithm to generate robust offline proactive policies based on historical data, which enable fast decision making for online packing. Our experiments based on realistic data show that the proposed approach leads to a reduction in total costs of up to 15% compared to the current practice, while being much faster for decision making compared to a popular online method.},
  archive      = {J_TC},
  author       = {Yinuo Li and Jin-Kao Hao and Liwei Song},
  doi          = {10.1109/TC.2025.3602297},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3596-3608},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dynamic bin packing with heterogeneous dependent bins for regionless in geo-distributed clouds},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel indirect methodology based on execution traces for grading functional test programs. <em>TC</em>, <em>74</em>(11), 3582-3595. (<a href='https://doi.org/10.1109/TC.2025.3600005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing functional test programs for hardware testing is time-consuming and experience-wise. A functional test program’s quality is usually assessed only through expensive fault simulation campaigns during early development. This paper presents indirect quality measurements of fault detection capabilities of functional test programs to reduce the total cost of fault simulation in the early development stages. We present a methodology that analyzes the instruction trace generated by running functional test programs on-chip and building its control and dataflow graph. We use the graph to identify potential flaws that affect the program’s fault detection capabilities. We present different graph-based techniques to measure the programs’ quality indirectly. By exploiting standard debugging formats, we individuate instructions in the source code that affect the graph-based measurements. We perform experiments on an automotive device manufactured by STMicroelectronics, running functional test programs of different natures. Our results show that our metric allows test engineers to develop better functional test programs without basing their development solely on fault simulation campaigns.},
  archive      = {J_TC},
  author       = {Francesco Angione and Paolo Bernardi and Andrea Calabrese and Lorenzo Cardone and Stefano Quer and Claudia Bertani and Vincenzo Tancorre},
  doi          = {10.1109/TC.2025.3600005},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3582-3595},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A novel indirect methodology based on execution traces for grading functional test programs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient delay-bounded dependent task offloading with service caching at edges. <em>TC</em>, <em>74</em>(11), 3568-3581. (<a href='https://doi.org/10.1109/TC.2025.3598749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are now embracing an era of edge computing and artificial intelligence, and the combination of the two has spawned a new field of research called edge intelligence. Massive amounts of data is generated at the edge of network, which relies on artificial intelligence to realize its potential. Meanwhile, artificial intelligence is able to flourish when processing diverse edge data. However, the computation and storage resources of edge servers are not unlimited. For some large-scale intelligent applications, it is difficult to meet their service quality requirements by directly offloading the entire application to a nearby server for processing. Due to the heterogeneity of server resources in edge environments, how to balance the workload among edge servers to provide better services also becomes complicated. The goal of this paper is to minimize the total cost of offloading large-scale applications consisting of many dependent tasks in an edge system. We formulate the Dependent task Offloading with Service Caching (DOSC) problem, which is proved to be NP-hard. A dynamic planning-based algorithm is introduced to solve fixed-DOSC, in which some services are pre-configured on the edge server, and other services can not be downloaded from the remote cloud. We also present a theoretical analysis on the performance guarantee of the dynamic planning-based algorithm. Then, we propose a near-optimal algorithm using the Gibbs sampling to solve the general DOSC problem. Testbed experiments and trace-driven simulations are conducted to verify the performance of our algorithm. Our algorithm, shown to be the most effective in terms of cost, considers both service caching and task dependencies when task offloading in comparison to other baseline algorithms.},
  archive      = {J_TC},
  author       = {Yu Liang and Sheng Zhang and Jie Wu},
  doi          = {10.1109/TC.2025.3598749},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3568-3581},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Cost-efficient delay-bounded dependent task offloading with service caching at edges},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
