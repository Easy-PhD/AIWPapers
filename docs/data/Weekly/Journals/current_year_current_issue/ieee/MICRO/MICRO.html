<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro">MICRO - 15</h2>
<ul>
<li><details>
<summary>
(2025). The scramble after breakthrough. <em>MICRO</em>, <em>45</em>(3), 108-110. (<a href='https://doi.org/10.1109/MM.2025.3567048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2025.3567048},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {108-110},
  shortjournal = {IEEE Micro},
  title        = {The scramble after breakthrough},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sipping matcha of security: A fireside chat with mengjia yan. <em>MICRO</em>, <em>45</em>(3), 103-107. (<a href='https://doi.org/10.1109/MM.2025.3572585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Jianming Tong and Zishen Wan},
  doi          = {10.1109/MM.2025.3572585},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {103-107},
  shortjournal = {IEEE Micro},
  title        = {Sipping matcha of security: A fireside chat with mengjia yan},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of wisconsin alumni research foundation v. Apple—Part IV. <em>MICRO</em>, <em>45</em>(3), 97-102. (<a href='https://doi.org/10.1109/MM.2025.3573578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is the fourth part in a series that reviews the decisions that the district judge and appellate panel made in Wisconsin Alumni Research Foundation v. Apple.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2025.3573578},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {97-102},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part IV},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three SoCs in three years: How to get agile. <em>MICRO</em>, <em>45</em>(3), 86-94. (<a href='https://doi.org/10.1109/MM.2025.3534917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We taped out three large system on chips in three years on 22-nm CMOS technology, featuring multiple RISC-V cores, and subsystems for machine learning, Ethernet, SerDes, Low-Power SDRAM, and input–output. We have covered all steps in the flow from specification to sample chips. Ballast, Tackle, and Headsail include 130 M, 12 M, and 340 M transistors and took 12, 10, and nine calendar months. Several persons from seven companies and university contributed to the three chips, and staff ranged from experts to novice master students. This article provides insight into modern fast-paced system-on-chip hardware (HW) development which is important when intellectual properties such as RISC-V processors and security accelerators are evolving rapidly. We achieved Agile development with the following guidelines: intellectual properties elaborated on the go, staff moves along the design flow, interface over instance, and schedule over features.},
  archive      = {J_MICRO},
  author       = {Antti Rautakoura and Timo Hämäläinen and Ari Kulmala},
  doi          = {10.1109/MM.2025.3534917},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {86-94},
  shortjournal = {IEEE Micro},
  title        = {Three SoCs in three years: How to get agile},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FuriosaAI RNGD: A tensor contraction processor for sustainable AI computing. <em>MICRO</em>, <em>45</em>(3), 76-85. (<a href='https://doi.org/10.1109/MM.2025.3551880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern artificial intelligence (AI) workloads require architectures capable of efficiently managing diverse tensor contraction patterns. Traditional approaches based on fixed-size matrix multiplications often fall short in scalability and flexibility. RNGD (pronounced “Renegade”), a second-generation tensor contraction processor, introduces an innovative architecture designed to exploit the parallelism and data locality inherent in tensor computations. Its coarse-grained processing elements (PEs) can operate as a unified large-scale unit or as multiple independent units, providing flexibility for various tensor shapes. Key innovations, such as a circuit switch-based fetch network, input broadcasting, and buffer-based reuse mechanisms, further enhance computational efficiency. RNGD represents a significant advancement in processor architecture, delivering optimized performance and energy efficiency for sustainable computation of next-generation AI workloads.},
  archive      = {J_MICRO},
  author       = {Younggeun Choi and Junyoung Park and Sang Min Lee and Jeseung Yeon and Minho Kim and Changjae Park and Byeongwook Bae and Hyunmin Jeong and Hanjoon Kim and June Paik and Nuno P. Lopes and Sungjoo Yoo},
  doi          = {10.1109/MM.2025.3551880},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {76-85},
  shortjournal = {IEEE Micro},
  title        = {FuriosaAI RNGD: A tensor contraction processor for sustainable AI computing},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The IBM telum II processor. <em>MICRO</em>, <em>45</em>(3), 66-75. (<a href='https://doi.org/10.1109/MM.2025.3563803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IBM Telum II is the latest processor designed specifically for IBM Z’s next-generation mainframe. Designed-for-purpose, Telum II is focused on mission-critical enterprise workloads where performance and sustainability are of the utmost importance and the demand for artificial intelligence acceleration is increasing dramatically. Innovations discussed in this article are the new on-die data processing unit for input/output acceleration, the updated cache, enhancements to the on-chip artificial intelligence accelerator, core improvements, and changes to the off-chip input/output interfaces.},
  archive      = {J_MICRO},
  author       = {Christopher Berry and Michael Becht and Tim Bubb and Howard Haynie and Robert Sonnelitter and Katie Seggerman and Jonathan Hsieh and Edward Malley and Mike Cadigan and Susan M. Eickhoff and Matthias Klein and Craig Walters and Christian G. Zoellin and Cedric Lichtenau},
  doi          = {10.1109/MM.2025.3563803},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {66-75},
  shortjournal = {IEEE Micro},
  title        = {The IBM telum II processor},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing programmable accelerators for sparse tensor algebra. <em>MICRO</em>, <em>45</em>(3), 58-65. (<a href='https://doi.org/10.1109/MM.2025.3556611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has focused on leveraging sparsity in hardware accelerators to improve the efficiency of applications spanning scientific computing to machine learning. Most such prior accelerators are fixed-function, which is insufficient for two reasons. First, applications typically include both dense and sparse components, and second, the algorithms that comprise these applications are constantly evolving. To address these challenges, we designed a programmable accelerator called Onyx for both sparse tensor algebra and dense workloads. Onyx extends a coarse-grained reconfigurable array (CGRA) optimized for dense applications with composable hardware primitives to support arbitrary sparse tensor algebra kernels. In this article, we show that we can further optimize Onyx by adding a small set of hardware features for parallelization that significantly increase both temporal and spatial utilization of the CGRA, reducing runtime by up to 6.2×.},
  archive      = {J_MICRO},
  author       = {Kalhan Koul and Zhouhua Xie and Maxwell Strange and Sai Gautham Ravipati and Bo Wun Cheng and Olivia Hsu and Po-Han Chen and Mark Horowitz and Fredrik Kjolstad and Priyanka Raina},
  doi          = {10.1109/MM.2025.3556611},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {58-65},
  shortjournal = {IEEE Micro},
  title        = {Designing programmable accelerators for sparse tensor algebra},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XiangShan: An open source project for high-performance RISC-V processors meeting industrial-grade standards. <em>MICRO</em>, <em>45</em>(3), 49-57. (<a href='https://doi.org/10.1109/MM.2025.3565285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely believed that an open source hardware ecosystem can reduce development costs and lower barriers to innovation. However, developing an open source industrial-grade high-performance processor is a challenging undertaking. For mass adoption, such an IP needs to have an advanced out-of-order microarchitecture for high performance, a robust verification infrastructure for reliable quality, and high configurability to accommodate the myriad use cases. With a best-in-class performance, XiangShan is an open source project for RISC-V processors that fully meets these requirements. To maximize overall efficiency, XiangShan adopted a collaborative hardware development model, partnering with industry on processor design, implementation, and verification. With innovations in Agile development processes and tools, the design of the processors can be evolved, optimized, and verified quickly, ensuring high quality and enabling architectural innovation and rapid commercialization.},
  archive      = {J_MICRO},
  author       = {Kaifan Wang and Jian Chen and Yinan Xu and Zihao Yu and Wei He and Dan Tang and Ninghui Sun and Yungang Bao},
  doi          = {10.1109/MM.2025.3565285},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {49-57},
  shortjournal = {IEEE Micro},
  title        = {XiangShan: An open source project for high-performance RISC-V processors meeting industrial-grade standards},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMD instinct MI300X: A generative AI accelerator and platform architecture. <em>MICRO</em>, <em>45</em>(3), 41-48. (<a href='https://doi.org/10.1109/MM.2025.3552324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMD Instinct MI300X sets a new benchmark in generative artificial intelligence (AI) acceleration, combining architectural innovation with advanced system integration to tackle the ever-growing demands of modern AI workloads. Featuring a chiplet-based architecture, the MI300X employs the fourth-generation Infinity Fabric, eight-stack HBM3 memory, and CDNA 3 compute cores to deliver unparalleled performance for both inference and training tasks. Additionally, the MI300X is central to the AMD Infinity platform, which offers industry-standard scalability through universal baseboard designs, high-bandwidth interconnectivity, and robust system management features. This article provides a detailed exploration of the MI300X architecture, its Infinity platform integration, and its impact on generative AI applications.},
  archive      = {J_MICRO},
  author       = {Alan Smith and Vamsi Krishna Alla},
  doi          = {10.1109/MM.2025.3552324},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {41-48},
  shortjournal = {IEEE Micro},
  title        = {AMD instinct MI300X: A generative AI accelerator and platform architecture},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intel xeon 6 product family. <em>MICRO</em>, <em>45</em>(3), 31-40. (<a href='https://doi.org/10.1109/MM.2025.3553756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Intel® Xeon 6 product family delivers new degrees of performance and scalability to address a wide variety of deployments across data center, enterprise, networking, and edge. The diversity of workloads, power, performance, and form factor requirements led to Intel’s most advanced modular system on chip (SoC) processor architecture. This modular construction allows the flexibility to optimize each die and build multiple SoCs using the same building blocks. For ultimate versatility, Intel Xeon 6 processors allow for the choice of two different CPU microarchitectures: performance cores and efficient cores. Both core types use a compatible x86 instruction set architecture and a common hardware platform.},
  archive      = {J_MICRO},
  author       = {Michael D. Powell and Patrick Fleming and Venkidesh Iyer Krishna and Naveen Lakkakula and Subhiksha Ravisundar and Praveen Mosur and Arijit Biswas and Pradeep Dubey and Kapil Sood and Andrew Cunningham and Smita Kumar},
  doi          = {10.1109/MM.2025.3553756},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {31-40},
  shortjournal = {IEEE Micro},
  title        = {Intel xeon 6 product family},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMD versal AI edge series gen 2. <em>MICRO</em>, <em>45</em>(3), 22-30. (<a href='https://doi.org/10.1109/MM.2025.3551319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMD’s Next-Gen Adaptive System on Chip, Versal AI Edge Series Gen 2, is a high-performance, scalable, and customizable platform for a wide array of markets, including automotive (advanced driver assistance systems and autonomous driving), robotics, audio–video broadcast, aerospace and defense, and industrial. The platform is designed to support ISO-26262 ASIL-D and IEC-61508 SIL3 for safety critical applications and was architected considering the needs of embedded vision applications, where the heterogenous adaptive architecture integrates field programmable gate array programmable logic with high-performance multicluster processors, imaging and video processing engines, and a next-generation artificial (AI) engine array with advanced data types. The advanced MX data types of the AI engine enable embedded vision applications to achieve accuracies comparable to FP32 with reduced AI engine array and memory footprint costs.},
  archive      = {J_MICRO},
  author       = {Tomai Knopp and Jeffrey Chu and Sagheer Ahmad},
  doi          = {10.1109/MM.2025.3551319},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {22-30},
  shortjournal = {IEEE Micro},
  title        = {AMD versal AI edge series gen 2},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lunar lake an intel mobile processor: SoC architecture overview (2024). <em>MICRO</em>, <em>45</em>(3), 15-21. (<a href='https://doi.org/10.1109/MM.2025.3558407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lunar Lake (LNL) is the codename for the Core Ultra Series mobile processors designed by Intel, released in September 2024. LNL took ambitious targets to cope with core and graphics performance, performance/watt, battery life, and artificial intelligence compute for an outstanding user experience. To address that, a ground-up architecture was defined. LNL enhanced the partition of cores to performance and efficient clusters with the ability to contain software load to the desired hardware at runtime, it optimized performance cluster with single threaded core, revised the idle state management, reduced frequent CPU wakes, enhanced the memory subsystem power states, and added fine-grain power delivery. LNL added premium capabilities such as memory-on-package, a power management integrated circuit, a powerful neural processing unit, memory-side cache, and total storage encryption for NVMe. LNL architecture scales from 8 W to 30 W+ and supports LPDDR5 frequencies up to 8533 Mhz.},
  archive      = {J_MICRO},
  author       = {Nadav Bonen and Arik Gihon and Leon Polishuk and Yoni Aizik and Yulia Okunev and Tsvika Kurts and Nithiyanandan Bashyam},
  doi          = {10.1109/MM.2025.3558407},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {15-21},
  shortjournal = {IEEE Micro},
  title        = {Lunar lake an intel mobile processor: SoC architecture overview (2024)},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qualcomm oryon CPU in snapdragon x elite: Micro-architecture and design. <em>MICRO</em>, <em>45</em>(3), 8-14. (<a href='https://doi.org/10.1109/MM.2025.3568807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the micro-architecture and design of the Qualcomm custom CPU, named Qualcomm Oryon CPU, that was introduced in 2024 in the Qualcomm Snapdragon X Elite system on a chip for the client computing market. It describes the micro-architecture of the CPU core and its cache and memory subsystem and is illustrative of a modern high-performance CPU with best-in-class energy efficiencies that is designed to be scalable across different product categories and price points.},
  archive      = {J_MICRO},
  author       = {Gerard Williams and Pradeep Kanapathipillai},
  doi          = {10.1109/MM.2025.3568807},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {8-14},
  shortjournal = {IEEE Micro},
  title        = {Qualcomm oryon CPU in snapdragon x elite: Micro-architecture and design},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on hot chips 2024. <em>MICRO</em>, <em>45</em>(3), 6-7. (<a href='https://doi.org/10.1109/MM.2025.3572594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Rob Aitken and Larry Yang},
  doi          = {10.1109/MM.2025.3572594},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {6-7},
  shortjournal = {IEEE Micro},
  title        = {Special issue on hot chips 2024},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward disaggregated and heterogenous AI systems. <em>MICRO</em>, <em>45</em>(3), 4-5. (<a href='https://doi.org/10.1109/MM.2025.3575180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2025.3575180},
  journal      = {IEEE Micro},
  month        = {5-6},
  number       = {3},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Toward disaggregated and heterogenous AI systems},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
