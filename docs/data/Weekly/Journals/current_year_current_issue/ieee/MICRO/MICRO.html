<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro">MICRO - 15</h2>
<ul>
<li><details>
<summary>
(2025). Prototype competition and breakthroughs. <em>MICRO</em>, <em>45</em>(4), 110-112. (<a href='https://doi.org/10.1109/MM.2025.3580075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commercial prototypes are not just products; they are the stepping stones of technological progress. They embed new features, acting as a focal point for expanding the application of cutting-edge technologies in new directions. Prototypes are not just about addressing issues that need resolution; they are about pushing the boundaries of what is possible.},
  archive      = {J_MICRO},
  author       = {Shane Greenstein},
  doi          = {10.1109/MM.2025.3580075},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {110-112},
  shortjournal = {IEEE Micro},
  title        = {Prototype competition and breakthroughs},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of wisconsin alumni research foundation v. Apple—Part v. <em>MICRO</em>, <em>45</em>(4), 104-109. (<a href='https://doi.org/10.1109/MM.2025.3599331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Part I of this series introduced the Wisconsin Alumni Research Foundation v. Apple cases and described the asserted patent (U.S. Patent Number 5,781,752). That article also summarized some recent large verdicts for patents asserted by academic institutions and provided several reasons why this series may be of interest to the readership of IEEE Micro, most notably because the inventors are well known and several well-known computer architects worked as experts on this case. Part II described the complaints, namely, it described the plaintiff, Wisconsin Alumni Research Foundation (“WARF”), the inventors, and WARF’s allegations as to how Apple’s products infringed WARF’s patent. Part III described Apple’s answer to the allegations in WARF’s complaint, Apple’s counterclaims, and WARF’s response to those counterclaims. Part IV examined Apple’s allegation of inequitable conduct by the inventors, a technical analysis of that allegation, and Judge Conley’s legal analysis of the sufficiency of Apple’s allegations.},
  archive      = {J_MICRO},
  author       = {Joshua J. Yi},
  doi          = {10.1109/MM.2025.3599331},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {104-109},
  shortjournal = {IEEE Micro},
  title        = {A review of wisconsin alumni research foundation v. Apple—Part v},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From colocation to exfiltration: Practical cache side-channel attacks in the modern public cloud. <em>MICRO</em>, <em>45</em>(4), 95-102. (<a href='https://doi.org/10.1109/MM.2025.3574715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharing resources among tenants is fundamental to public clouds, enhancing efficiency but also creating opportunities for microarchitectural side-channel attacks. However, cloud vendors remain skeptical about the practicality of these attacks, particularly regarding the ability to colocate attacker and victim, and to overcome system noise. In this work, we develop a series of techniques for each step of the attack and, for the first time, demonstrate cross-tenant information leakage on the public Google Cloud Run, refuting the belief that such attacks are impractical. Our findings highlight the need to secure public clouds against side-channel attacks.},
  archive      = {J_MICRO},
  author       = {Zirui Neil Zhao and Adam Morrison and Christopher W. Fletcher and Josep Torrellas},
  doi          = {10.1109/MM.2025.3574715},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {95-102},
  shortjournal = {IEEE Micro},
  title        = {From colocation to exfiltration: Practical cache side-channel attacks in the modern public cloud},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay-space arithmetic and architecture. <em>MICRO</em>, <em>45</em>(4), 87-94. (<a href='https://doi.org/10.1109/MM.2025.3588787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What operations can you perform efficiently when you use the “time of arrival” of a signal’s edge to represent a number? Past work has shown how linear representations can be effectively used to optimize problems expressed in max-plus algebras, but efficient general-purpose arithmetic operations have remained elusive. We present negative-logarithmic delay-space arithmetic as a completely new approach to temporal coding. Under this approach, general-purpose arithmetic is transformed to a “soft” version of the standard temporal operations in such a way that preserves all of the algebraic identities. We further show that these soft operations can be approximated by composing the original “sharp” temporal operators, resulting in simple, energy-efficient implementations. We demonstrate the effectiveness of this novel arithmetic with a near-sensor architecture for energy-efficient convolutions. Cycle-over-cycle operation is supported through temporal recurrence, dramatically limiting the need for expensive domain conversions or noise-prone temporal memories.},
  archive      = {J_MICRO},
  author       = {Rhys Gretsch and Peiyang Song and Advait Madhavan and Jeremy Lau and Timothy Sherwood},
  doi          = {10.1109/MM.2025.3588787},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {87-94},
  shortjournal = {IEEE Micro},
  title        = {Delay-space arithmetic and architecture},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BlitzCoin: A decentralized hardware solution for power management of highly heterogeneous systems on chip. <em>MICRO</em>, <em>45</em>(4), 79-86. (<a href='https://doi.org/10.1109/MM.2025.3574281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in both the number and the types of accelerators in modern systems on chip (SoCs) necessitates a rethinking of power management (PM) strategies. To overcome the scalability shortcomings of current methods, we propose BlitzCoin, a fully decentralized hardware-based PM coupled with optimized unified voltage and frequency regulation. We evaluated BlitzCoin through register transfer-level simulations of multiple SoCs targeted toward different application domains. The results are further validated through silicon measurements of a fabricated 12-nm many-accelerator SoC that includes BlitzCoin. Our evaluations show that BlitzCoin is markedly faster than state-of-the-art centralized PM strategies, with 8 × to 12 × lower response times. This results in 25%–34% throughput improvement and allows for scaling to 7 × to 13 × larger SoCs, all with a small area overhead of <1%. BlitzCoin is an addition to the open source ESP SoC platform, offering a foundation for further exploration of PM strategies.},
  archive      = {J_MICRO},
  author       = {Martin Cochet and Karthik Swaminathan and Erik Loscalzo and Joseph Zuckerman and Maico Cassel dos Santos and Davide Giri and Alper Buyuktosunoglu and Tianyu Jia and David Brooks and Gu-Yeon Wei and Kenneth Shepard and Luca P. Carloni and Pradip Bose},
  doi          = {10.1109/MM.2025.3574281},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {79-86},
  shortjournal = {IEEE Micro},
  title        = {BlitzCoin: A decentralized hardware solution for power management of highly heterogeneous systems on chip},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalar vector runahead: Removing the shackles of indirect memory chains on in-order cores. <em>MICRO</em>, <em>45</em>(4), 72-78. (<a href='https://doi.org/10.1109/MM.2025.3577524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern processors often face the memory wall as a bottleneck, an exacerbated problem for stall-on-use in-order cores. Despite this limitation, there is growing demand for energy-efficient in-order cores due to privacy and sustainability concerns. Scalar vector runahead (SVR) provides an elegant solution by extracting high memory-level parallelism through piggybacking on existing instructions executed on the processor that lead to future irregular memory accesses. SVR speculatively executes multiple transient, independent, parallel instances of memory accesses and their instruction chains, by initiating memory accesses from many different values of a predicted induction variable. This approach moves mutually independent memory accesses next to each other to hide dependent stalls. With a hardware overhead of only 2 KiB and without the need for hardware vector extensions, SVR delivers 3.2× higher performance than a baseline three-wide in-order core inspired by an Arm Cortex A510, and 1.3× higher performance than an out-of-order core, while halving energy consumption.},
  archive      = {J_MICRO},
  author       = {Jaime Roelandts and Ajeya Naithani and Sam Ainsworth and Timothy M. Jones and Lieven Eeckhout},
  doi          = {10.1109/MM.2025.3577524},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {72-78},
  shortjournal = {IEEE Micro},
  title        = {Scalar vector runahead: Removing the shackles of indirect memory chains on in-order cores},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated high-level code optimization for warehouse performance. <em>MICRO</em>, <em>45</em>(4), 60-71. (<a href='https://doi.org/10.1109/MM.2025.3590033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the twilight of Moore’s law, optimizing program performance has emerged as a central focus in computer architecture research. Yet, high-level source optimization remains challenging due to the intricate nature of understanding code semantics. Our approach unifies machine learning techniques with established insights and tools from computer architecture to tackle the inherent challenges of high-level optimization. In this work, we introduce a framework that harnesses large language models (LLMs) for high-level program optimization. We curate a dataset of competitive C++ submissions, each accompanied by extensive unit tests to capture performance-improving patterns. To mitigate the variability of performance measurements, we develop an evaluation harness using the gem5 full-system simulator. Our results show a mean speedup of 6.86, outperforming the average human optimization of 3.66×. We also give an overview of subsequent work in this space, describing how LLM-driven optimization enables autonomously applying performance-improving edits across billions of lines of code in Google data centers.},
  archive      = {J_MICRO},
  author       = {Alexander Shypula and Aman Madaan and Yimeng Zeng and Uri Alon and Jacob Gardner and Milad Hashemi and Graham Neubig and Parthasarathy Ranganathan and Osbert Bastani and Amir Yazdanbakhsh},
  doi          = {10.1109/MM.2025.3590033},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {60-71},
  shortjournal = {IEEE Micro},
  title        = {Automated high-level code optimization for warehouse performance},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Splitwise: Efficient generative LLM inference using phase splitting. <em>MICRO</em>, <em>45</em>(4), 54-59. (<a href='https://doi.org/10.1109/MM.2025.3575361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative large language model (LLM) applications are rapidly growing, leading to widespread deployment of expensive, power-hungry GPUs. Growing power demands of artificial intelligence (AI) in the cloud industry has become a global problem.5 Our analysis shows that LLM inference involves two distinct phases: a compute-intensive prefill phase and a memory-intensive decode phase, each with different resource needs. Running them together introduces inefficient scheduling. Furthermore, unlike the prefill phase, the decode phase can run on lower-cost and lower-power hardware. Building on these insights, we propose Splitwise, a scheduling technique that splits prefill and decode phases across different machines to achieve better throughput. Additionally, Splitwise allows phase-specific hardware optimization. By efficiently transferring request state between machines, Splitwise achieves up to 2.35× more throughput within the same power and cost budgets, or 1.4× higher throughput at 20% lower cost and same power.},
  archive      = {J_MICRO},
  author       = {Esha Choukse and Pratyush Patel and Chaojie Zhang and Aashaka Shah and Íñigo Goiri and Saeed Maleki and Rodrigo Fonseca and Ricardo Bianchini},
  doi          = {10.1109/MM.2025.3575361},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {54-59},
  shortjournal = {IEEE Micro},
  title        = {Splitwise: Efficient generative LLM inference using phase splitting},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From TeAAL to FuseMax: Separation of concerns for attention accelerator design. <em>MICRO</em>, <em>45</em>(4), 44-53. (<a href='https://doi.org/10.1109/MM.2025.3589955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention for transformers has recently received significant “attention” as a target for custom acceleration. Our prior work, TeAAL, proposes a new accelerator design methodology that allows architects to reason about and optimize their designs iteratively. With a focus on attention, this work makes contributions to both the theory and practice of TeAAL’s methodology. On the theory side, we propose a set of analyses that can be applied using only the algorithm specification—the cascade of Einsums summations. On the practice side, we use the new analyses to understand and taxonomize the space of attention algorithms and to iteratively build up an efficient, high-utilization accelerator. Our resulting design, FuseMax, achieves an average 6.7× speedup on attention and 5.3× speedup on end-to-end transformer inference over the prior state of the art, FLAT, while using 79% and 83% of the energy, respectively.},
  archive      = {J_MICRO},
  author       = {Nandeeka Nayak and Toluwanimi O. Odemuyiwa and Xinrui Wu and Michael Pellauer and Joel S. Emer and Christopher W. Fletcher},
  doi          = {10.1109/MM.2025.3589955},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {44-53},
  shortjournal = {IEEE Micro},
  title        = {From TeAAL to FuseMax: Separation of concerns for attention accelerator design},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Waferscale network switches. <em>MICRO</em>, <em>45</em>(4), 37-43. (<a href='https://doi.org/10.1109/MM.2025.3589927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spite of being a key determinant of latency, cost, power, space, and capability of modern computer systems, network switch radix has not seen much growth over the years due to poor scaling of off-chip input/output pitches and switch die sizes. In this work, we show that one could use waferscale integration as a way to dramatically increase the size of the switch substrate and build a network switch with 32× higher radix than state-of-the-art network switches. We identified and addressed the limitations of internal bandwidth, external bandwidth, and power density, as the vanilla design would make the benefits of waferscale network switch minimal. We show that the waferscale network switch can be used to enable new computing systems, such as single-switch data centers and massive-scale singular GPUs. It can also lead to a dramatic reduction in data center network costs.},
  archive      = {J_MICRO},
  author       = {Shuangliang David Chen and Saptadeep Pal and Rakesh Kumar},
  doi          = {10.1109/MM.2025.3589927},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {37-43},
  shortjournal = {IEEE Micro},
  title        = {Waferscale network switches},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware-assisted virtualization of neural processing units for cloud platforms. <em>MICRO</em>, <em>45</em>(4), 29-36. (<a href='https://doi.org/10.1109/MM.2025.3574630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud platforms have deployed hardware accelerators like neural processing units (NPUs) for machine learning (ML) inference services. To maximize resource utilization while ensuring quality of service, a natural approach is to virtualize NPUs for efficient resource sharing for multitenant ML services. However, virtualizing NPUs is challenging. This is not only due to the lack of system abstraction support for NPU hardware, but also due to the lack of architectural and instruction set architecture (ISA) support for fine-grained operator scheduling. This article presents Neu10, an NPU virtualization framework consisting of 1) an abstraction called vNPU for fine-grained virtualization of the heterogeneous compute units in a physical NPU (pNPU), 2) a vNPU allocator that enables pay-as-you-go pricing and flexible vNPU-to-pNPU mappings, and 3) an ISA extension of modern NPU architecture for fine-grained tensor operator scheduling for multiple vNPUs. We evaluate Neu10 with a production-level simulator to demonstrate its benefits over state-of-the-art NPU sharing approaches.},
  archive      = {J_MICRO},
  author       = {Yuqi Xue and Yiqi Liu and Lifeng Nai and Jian Huang},
  doi          = {10.1109/MM.2025.3574630},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {29-36},
  shortjournal = {IEEE Micro},
  title        = {Hardware-assisted virtualization of neural processing units for cloud platforms},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling sustainable cloud computing with low-carbon server design. <em>MICRO</em>, <em>45</em>(4), 19-28. (<a href='https://doi.org/10.1109/MM.2025.3572955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To combat climate change, we must reduce carbon emissions from hyperscale cloud computing. Compute servers cause the majority of a general-purpose cloud’s emissions. Thus, we are motivated to design carbon-efficient compute server stock keeping units (SKUs), or GreenSKUs, using recently available low-carbon components. We built three GreenSKU prototypes, integrating energy-efficient CPUs, reusing old dynamic RAM via compute express link, and reusing old solid-state drives. We reveal challenges that limit GreenSKUs’ carbon savings at scale and may prevent their adoption by cloud providers. To address these challenges, we developed a novel framework, GSF (GreenSKU Framework), that enables cloud providers to systematically evaluate GreenSKUs’ carbon savings at scale. By implementing GSF within Microsoft Azure’s production constraints, we demonstrate that GreenSKUs reduce net cloud emissions by 8%, which is globally significant. This work is the first to demonstrate and quantify how carbon-efficient server designs translate to measurable cloud-scale emissions reductions, enabling meaningful contributions to cloud sustainability goals.},
  archive      = {J_MICRO},
  author       = {Jaylen Wang and Daniel S. Berger and Fiodar Kazhamiaka and Celine Irvene and Chaojie Zhang and Esha Choukse and Kali Frost and Rodrigo Fonseca and Brijesh Warrier and Chetan Bansal and Jonathan Stern and Ricardo Bianchini and Akshitha Sriraman},
  doi          = {10.1109/MM.2025.3572955},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {19-28},
  shortjournal = {IEEE Micro},
  title        = {Enabling sustainable cloud computing with low-carbon server design},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing processor sustainability using the first-order FOCAL carbon model. <em>MICRO</em>, <em>45</em>(4), 11-18. (<a href='https://doi.org/10.1109/MM.2025.3576714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the inherent data uncertainty regarding the sustainability of computing devices in general and processors in particular, this article proposes the parameterized First-Order analytical CArbon modeL (FOCAL) to assess processor sustainability from first principles. FOCAL’s normalized carbon footprint metric guides computer architects to holistically optimize chip area, energy, and power consumption to reduce a processor’s environmental footprint. We use FOCAL to analyze and categorize a broad set of archetypal processor mechanisms into strongly, weakly, or less sustainable design choices, providing insight and intuition into how to reduce a processor’s environmental footprint with implications to both hardware and software. A case study illustrates a pathway for designing strongly sustainable multicore processors delivering high performance while at the same time reducing their environmental footprint.},
  archive      = {J_MICRO},
  author       = {Lieven Eeckhout},
  doi          = {10.1109/MM.2025.3576714},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {11-18},
  shortjournal = {IEEE Micro},
  title        = {Assessing processor sustainability using the first-order FOCAL carbon model},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on top picks from the 2024 computer architecture conferences. <em>MICRO</em>, <em>45</em>(4), 6-10. (<a href='https://doi.org/10.1109/MM.2025.3599323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is our great pleasure to present the IEEE Micro Special Issue on Top Picks From the 2024 Computer Architecture Conferences. This special issue upholds a long-standing tradition within the computer architecture community of recognizing outstanding research contributions. The articles featured herein were selected by the Top Picks Selection Committee for their exceptional novelty and strong potential for long-term impact on the field.},
  archive      = {J_MICRO},
  author       = {Jun Yang and Xulong Tang},
  doi          = {10.1109/MM.2025.3599323},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {6-10},
  shortjournal = {IEEE Micro},
  title        = {Special issue on top picks from the 2024 computer architecture conferences},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligence for sale. <em>MICRO</em>, <em>45</em>(4), 4-5. (<a href='https://doi.org/10.1109/MM.2025.3601305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MICRO},
  author       = {Hsien-Hsin S. Lee},
  doi          = {10.1109/MM.2025.3601305},
  journal      = {IEEE Micro},
  month        = {7-8},
  number       = {4},
  pages        = {4-5},
  shortjournal = {IEEE Micro},
  title        = {Intelligence for sale},
  volume       = {45},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
