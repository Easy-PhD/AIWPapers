<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmc">TMC - 95</h2>
<ul>
<li><details>
<summary>
(2025). Joint DNN model deployment, selection, and configuration for heterogeneous inference services toward edge intelligence. <em>TMC</em>, <em>24</em>(11), 12726-12741. (<a href='https://doi.org/10.1109/TMC.2025.3586793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge intelligence is an emerging paradigm in edge computing that deploys Deep Neural Network (DNN) models on edge servers with limited storage and computation capacities to provide inference services for high mobility and real-time applications, such as autonomous driving or smart surveillance, with varying accuracy and delay requirements. Adapting application configurations (e.g., image resolution or video frame rate) while selecting different DNN models and deployment locations can provide high-accuracy, low-delay inference services that meet user requirements. However, the configurations and DNN models of various inference services are highly heterogeneous. As balancing inference accuracy, resource cost, and delay is a multi-objective programming problem, it is a great challenge to obtain the optimal solution. To address this challenge, we propose a novel online framework to jointly optimize the configuration adaption, DNN model selection, and deployment for heterogeneous inference services. Specifically, we first formulate this joint optimization problem as an integer linear programming problem and prove it is NP-hard. Then, we further model the problem as a Partial Observable Markov Decision Process (POMDP) and solve it by developing a Heterogeneous-Agent Reinforcement Learning (HARL) based algorithm, named Heterogeneous Inference Service ProvidER (HISPER). It allows agents to have different action spaces corresponding to different types of configurations and DNN models. Finally, extensive experiments demonstrate that the proposed algorithm outperforms other state-of-the-art counterparts.},
  archive      = {J_TMC},
  author       = {Hebin Huang and Junbin Liang and Geyong Min},
  doi          = {10.1109/TMC.2025.3586793},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12726-12741},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint DNN model deployment, selection, and configuration for heterogeneous inference services toward edge intelligence},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing transmission cost of distributed principal components analysis in wireless networks with accuracy guaranteed. <em>TMC</em>, <em>24</em>(11), 12711-12725. (<a href='https://doi.org/10.1109/TMC.2025.3586615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a classic data processing tool, Principal Component Analysis (PCA) has been widely applied in various data analysis applications. To mitigate the high computational complexity of PCA on Big Data, distributed PCA methods have been extensively studied, which disperse the computational tasks across multiple computation units while guaranteeing the accuracy. For the scenarios of distributed PCA in wireless networks, as the data is originally dispersed across different locations, it is further required to reduce the communication cost of distributed PCA in networks, which however has been seldom studied. Reducing the communication cost of distributed PCA in wireless networks requires not only appropriately partitioning the computation of PCA, ensuring accuracy, but also effectively assigning the partitioned computations and routing strategies to the nodes. In this paper, we propose CD-PCA, a communication-efficient distributed PCA (CD-PCA) scheme. This scheme implements a transmission-benefit equipartition strategy for the network to facilitate high-accuracy distributed computation and designs novel routing strategies for nodes to execute the distributed PCA within each partitioned region. Extensive simulation results demonstrate that the proposed CD-PCA scheme can reduce transmission costs by over 30% on average compared to related methods and baseline approaches.},
  archive      = {J_TMC},
  author       = {Yiyi Zhang and Peng Guo and Xuefeng Liu and Chao Cai and Kui Zhang and Jiang Liu},
  doi          = {10.1109/TMC.2025.3586615},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12711-12725},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reducing transmission cost of distributed principal components analysis in wireless networks with accuracy guaranteed},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes: A DRL approach. <em>TMC</em>, <em>24</em>(11), 12692-12710. (<a href='https://doi.org/10.1109/TMC.2025.3586623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) has become an effective paradigm to support computation-intensive applications by providing services in close proximity to user devices (UDs). In MEC networks, computation offloading technology is devoted to balancing system load and prolonging UDs’ battery life. However, most existing studies on computation offloading take the impractical assumption of the MEC scenario with homogeneous users, ignoring security requirement from certain users. Moreover, with users mobility and task arrivals correlation, most existing computing offloading approaches suffer from inefficient or suboptimal decision making in practical MEC environments. To tackle these issues, by integrating task arrivals correlation within a time slot and environment dynamics between time slots, we propose an adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes. First, considering additional security requirement from very important people (VIP) users, we present a novel collaborative architecture by separating edge/cloud servers into public and private nodes. Then, with the architecture, we develop a dynamic computation offloading (DCO) algorithm to realize adaptive computation offloading scheme in MEC environment with mobile users. Particularly, the algorithm involves three stages. 1) By extending Poisson process into Markovian arrival process (MAP), we construct an MAP-based system model to capture the behavior of time-dependent task arrivals and then analyze the system model to derive the system delay in steady state. 2) For the purpose of minimizing the system delay in each time slot, we formulate a computation offloading problem in MEC environment with mobile users. 3) Under a deep reinforcement learning (DRL) framework, by taking the system delay as environmental feedback, we solve the formulated problem and provide offloading decisions in each time slot. We evaluate the performance of DCO algorithm by comparing it with other benchmark algorithms in various application scenarios. Results demonstrate that the proposed DCO algorithm outperforms the compared algorithms in response performance.},
  archive      = {J_TMC},
  author       = {Haixing Wu and Jiameng Zheng and Shunfu Jin},
  doi          = {10.1109/TMC.2025.3586623},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12692-12710},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes: A DRL approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSMAAC: Multi-agent reinforcement learning based flight control in partially observable multi-UAV assisted crowd sensing systems. <em>TMC</em>, <em>24</em>(11), 12672-12691. (<a href='https://doi.org/10.1109/TMC.2025.3586429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile crowd sensing systems, existing flight control methods enable uncrewed aerial vehicles (UAVs) to provide high-quality data collection services for various applications. However, due to limited communication range, UAVs typically collect data under partial observability, hindering optimal performance without global environmental information. Additionally, many methods fail to enforce critical safety constraints. This paper proposes a communication-assisted safe multi-agent actor-critic-based UAV flight control method (CSMAAC). First, we propose an independent prediction communication partner model to address the partial observability problem. Based on the UAV’s local observation, causal inference is used to obtain prior communication information between UAVs through a feed-forward neural network to help UAVs determine potential communication partners. Second, we utilize a critic-network to predict and quantify inter-UAV influence and determine the necessity of communication. By exchanging necessary information inter-UAV, UAVs can perceive global information, thereby solving the UAV’s partial observability problem and reducing communication overhead. Moreover, we propose a similarity enhancement mechanism to improve the learning efficiency of the model by enhancing the connection between UAV observations and the policies of other UAVs. Finally, we introduce a safety layer to Actor-Network to ensure safe UAV flight. The simulation results show that the proposed method outperforms the baselines.},
  archive      = {J_TMC},
  author       = {Zhen Gao and Gang Wang and Lei Yang and Chenhao Ying},
  doi          = {10.1109/TMC.2025.3586429},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12672-12691},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSMAAC: Multi-agent reinforcement learning based flight control in partially observable multi-UAV assisted crowd sensing systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARSys: An efficient and cross-platform development, deployment, and runtime system for mobile augmented reality. <em>TMC</em>, <em>24</em>(11), 12655-12671. (<a href='https://doi.org/10.1109/TMC.2025.3586797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) offers users immersive experiences to interact with digital contents in their physical space. However, practical AR applications are challenged by the tight coupling of algorithm and engineering during the development and deployment phases as well as the execution requirements of hybrid AR subtasks on heterogeneous and resource-constraint mobile devices. In this work, we build an end-to-end, cross-platform, and efficient AR system, called ARSys. The infrastructure in ARSys adopts the new principle of integrated design, unifies and refines AR fundamental capabilities, supports streaming media processing, model inference, and real-time rendering by exposing high-performance tensor compute engine to top, and constructs a Python multi-instance virtual machine as the cross-platform AR task execution container. The runtime mechanism of ARSys schedules AR tasks in a pipeline parallelism way and allocates subtasks to hardware backends by optimizing the slowest node. The development workbench and the deployment platform in ARSys allow the decoupling of algorithms written in Python from engineering components in C/C++ and further support remote debugging and quick validation of AR algorithms. We extensively evaluate ARSys in practical AR applications across high-end, mid-end, and low-end Android and iOS devices, demonstrating higher development, deployment, and runtime efficiency than existing MediaPipe-oriented framework. ARSys has been integrated into Mobile Taobao for production use.},
  archive      = {J_TMC},
  author       = {Chengfei Lv and Chaoyue Niu and Yu Cai and Xiaotang Jiang and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3586797},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12655-12671},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ARSys: An efficient and cross-platform development, deployment, and runtime system for mobile augmented reality},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSNN model: A lightweight spiking neural network-based depression classification model for wearable EEG sensors. <em>TMC</em>, <em>24</em>(11), 12640-12654. (<a href='https://doi.org/10.1109/TMC.2025.3586591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression detection via wearable Electroencephalogram (EEG) sensor-assisted diagnosis system demands computationally efficient models compatible with resource-constrained edge devices. Spiking Neural Networks (SNNs) offer inherent advantages for processing the spatio-temporal patterns of EEG through event-driven neuromorphic computing. In this study, we innovatively present LSNNet, a lightweight SNN model specifically designed for wearable EEG sensors. The model exhibits low computational complexity with 7.18 K parameters and 67.68 M Floating-Point Operations (FLOPs). It requires only 246.88 KB of Random Access Memory (RAM) and 57.33 KB of Read-Only Memory (ROM) for on-board execution, and has been validated on both the single-core STM32U535CET6 and the multi-core GAP8 microcontrollers. Despite its minimal computational and memory requirements, LSNNet achieves impressive performance metrics, with a classification accuracy of 89.2%, specificity of 92.4%, and sensitivity of 86.4% in independent tests conducted on EEG data collected from 73 depressed patients and 108 healthy controls using our three-lead EEG sensor. Especially, when running on the GAP8 microcontroller, the LSNNet model has a low power consumption of 21.43 mW and a satisfactory inference time of 0.63 s while maintaining a classification accuracy of 87.5% (only with a reduction of 1.98% ). These results underscore the potential of integrating wearable EEG sensors with the LSNNet model for depression detection in the Internet of Things (IoT) era.},
  archive      = {J_TMC},
  author       = {Qinglin Zhao and Lixin Zhang and Haojie Zhang and Hua Jiang and Kunbo Cui and Zhongqing Wu and Jingyu Liu and Mingqi Zhao and Fuze Tian and Bin Hu},
  doi          = {10.1109/TMC.2025.3586591},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12640-12654},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LSNN model: A lightweight spiking neural network-based depression classification model for wearable EEG sensors},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elevator, escalator, or neither? classifying conveyor state using smartphone under arbitrary pedestrian behavior. <em>TMC</em>, <em>24</em>(11), 12626-12639. (<a href='https://doi.org/10.1109/TMC.2025.3586618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowing a pedestrian’s conveyor state of “elevator,” “escalator,” or “neither” is fundamental to many applications such as indoor navigation and people flow management. Previous studies on classifying the conveyor state often rely on specially designed body-worn sensors or make strong assumptions on pedestrian behaviors, which greatly strangles their deployability. To overcome this, we study the classification problem under arbitrary pedestrian behaviors using the inertial navigation system (INS) of the commonly available smartphones (including accelerometer, gyroscope, and magnetometer). This problem is challenging, because the INS signals of the conveyor states are entangled by the arbitrary and diverse pedestrian behaviors. We propose ELESON, a novel and lightweight deep-learning approach that uses phone INS to classify a pedestrian to elevator, escalator, or neither. Using causal decomposition and adversarial learning, ELESON extracts the motion and magnetic features of conveyor state independent of pedestrian behavior, based on which it estimates the state confidence by means of an evidential classifier. We curate a large and diverse dataset with 36,420 instances of pedestrians randomly taking elevators and escalators under arbitrary unknown behaviors. Our extensive experiments show that ELESON is robust against pedestrian behavior, achieving a high accuracy of over 0.9 in F1 score, strong confidence discriminability of 0.81 in AUROC (Area Under the Receiver Operating Characteristics), and low computational and memory requirements fit for common smartphone deployment.},
  archive      = {J_TMC},
  author       = {Tianlang He and Zhiqiu Xia and S.-H. Gary Chan},
  doi          = {10.1109/TMC.2025.3586618},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12626-12639},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Elevator, escalator, or neither? classifying conveyor state using smartphone under arbitrary pedestrian behavior},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel information-theoretical framework for quantifying coding performance in scalable mobile video streaming. <em>TMC</em>, <em>24</em>(11), 12611-12625. (<a href='https://doi.org/10.1109/TMC.2025.3586587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, scalable video coding (SVC) has gained significant recognition in mobile video streaming because it can adapt bitstreams to time-varying transmission conditions. However, the coding performance of SVC, which is determined by its coding structure, has not been thoroughly studied. To address this issue, we propose analyzing the redundancy, reduction, distortion, and mutuality of video information within the video coding processes. This analysis facilitates the development of a novel information-theoretical framework for quantifying coding performance, which includes an information theory (IT)-based quantification method and a graphical representation system. The representation system accurately delineates the coding reference structure for encoding each video frame, while the proposed method utilizes mutual information to quantify the achievable coding performance of SVC under the delineated structure. To demonstrate the significance of our research, we apply the proposed framework to encode a basic coding unit, showcasing its effectiveness in improving SVC schemes. Consequently, our framework not only provides an efficient approach for quantifying the coding performance of SVC but also serves as an invaluable tool for optimizing SVC in various applications.},
  archive      = {J_TMC},
  author       = {Weijia Han and Chuan Huang and Yanjie Dong and Yangyingzi Zhang and Yuxiang Yue and Wei Teng},
  doi          = {10.1109/TMC.2025.3586587},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12611-12625},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel information-theoretical framework for quantifying coding performance in scalable mobile video streaming},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the robust topology recovery of UAV swarm for detection and localization of electronic signals. <em>TMC</em>, <em>24</em>(11), 12595-12610. (<a href='https://doi.org/10.1109/TMC.2025.3586447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, Unmanned Aerial Vehicle (UAV) swarm has been extensively applied in various fields. In the application of detection and localization of electronic signals, some UAVs could become disabled due to some abnormal events (e.g. electromagnetic interference and battery electricity exhaustion), and the topology connectivity of UAV swarm could be impaired, i.e., the topology of UAV swarm could be partitioned. For the topology recovery issue, we first propose Robust Topology Recovery Algorithm of UAV swarm (RTRA) to recover the topology connectivity of UAV swarm and enhance the topology robustness (reduce the number of potential topology recoveries in future) by relocating some UAVs to new positions with shortest flight distance. Furthermore, we note that the relocated UAVs are easy to exhaust the battery electricity and fail due to the extra flight movements for the topology recoveries, which affects the topology robustness. To this end, we present Cascading Robust Recovery Topology Algorithm of UAV swarm (CRTRA), which adopts a cascading movement strategy to share the flight movements among multiply relocated UAVs, thus avoiding the battery electricity exhaustion of the relocated UAVs. Extensive simulations and comparisons demonstrate that our proposed CRTRA can effectively recover the topology connectivity of UAV swarm while enhancing the topology robustness and shortening the flight distance of relocated UAVs, and CRTRA is especially suitable for some missions such as the detection and localization of electronic signals where UAVs are prone to fail.},
  archive      = {J_TMC},
  author       = {Linfeng Liu and Wenzhe Zhang and Xingyu Li and Jia Xu},
  doi          = {10.1109/TMC.2025.3586447},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12595-12610},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On the robust topology recovery of UAV swarm for detection and localization of electronic signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achievable rate maximization for multi-IRS assisted AAV-NOMA networks. <em>TMC</em>, <em>24</em>(11), 12580-12594. (<a href='https://doi.org/10.1109/TMC.2025.3586768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution towards Internet of Things (IoT) in the forthcoming sixth generation (6G) is facing massive amounts of transmitted data and harsh wireless transmission environment, which severely degrade the quality of communication. To overcome these difficulties, a novel multiple intelligent reflecting surfaces (IRSs) assisted autonomous aerial vehicle (AAV) network framework with non-orthogonal multiple access (NOMA) is proposed in this article, where the AAV applies the NOMA scheme to deliver the information to the ground users assisted by multiple IRSs. We aim to maximize the achievable rate of the considered network while guaranteeing the minimum communication rate of each user, by jointly optimizing the multi-IRS phase shifts, AAV transmit power, AAV trajectory, and NOMA decoding order. To handle the coupled variables and integer constraints, we decompose the original problem into three subproblems based on the block coordinate descent (BCD) framework. Specifically, we first obtain the multi-IRS phase shifts by applying the semidefinite relaxation (SDR) technique. Next, the AAV transmit power allocation is derived by exploiting the concave convex procedure (CCCP) method. The AAV trajectory and NOMA decoding order are finally obtained by invoking the penalty-based method and the successive convex approximation (SCA) technique. Based on these, an alternating optimization algorithm is proposed. The numerical results show that: 1) the NOMA scheme enhances the utilization of the spectrum and enhances the access capacity of the communication system; 2) the multi-IRS cooperative structure increases the reflective channels and effectively improves the air-ground transmission environment, thus enhancing the system achievable rate; 3) the proposed multi-IRS assisted AAV NOMA algorithm achieves a significant network rate improvement compared to other benchmark schemes.},
  archive      = {J_TMC},
  author       = {Dingcheng Yang and Kangqing Wu and Yu Xu and Fahui Wu and Tiankui Zhang},
  doi          = {10.1109/TMC.2025.3586768},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12580-12594},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Achievable rate maximization for multi-IRS assisted AAV-NOMA networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally robust contract theory for edge AIGC services in teleoperation. <em>TMC</em>, <em>24</em>(11), 12567-12579. (<a href='https://doi.org/10.1109/TMC.2025.3586606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced AI-Generated Content (AIGC) technologies have injected new impetus into teleoperation, enhancing its security and efficiency. Edge AIGC networks have been introduced to meet the stringent low-latency requirements of teleoperation. However, the inherent uncertainty of AIGC service quality and the need to incentivize AIGC service providers (ASPs) make the design of a robust incentive mechanism essential. This design is particularly challenging due to uncertainty and information asymmetry, as teleoperators have limited knowledge of the remaining resource capacities of ASPs. To this end, we propose a distributionally robust optimization (DRO)-based contract theory to design robust reward schemes for AIGC task offloading. Notably, our work extends the contract theory by integrating DRO, addressing the fundamental challenge of contract design under uncertainty. In this paper, we employ contract theory to model information asymmetry while utilizing DRO to capture the uncertainty in AIGC service quality. Given the inherent complexity of the original DRO-based contract theory problem, we reformulate it into an equivalent, tractable bi-level optimization problem. To efficiently solve this problem, we develop a Block Coordinate Descent (BCD)-based algorithm to derive robust reward schemes. Simulation results on our unity-based teleoperation platform demonstrate that the proposed method improves teleoperator utility by 2.7% to 10.74% under varying degrees of AIGC service quality shifts and increases ASP utility by 60.02% compared to the SOTA method, i.e., Deep Reinforcement Learning (DRL)-based contract theory.},
  archive      = {J_TMC},
  author       = {Zijun Zhan and Yaxian Dong and Daniel Mawunyo Doe and Yuqing Hu and Shuai Li and Shaohua Cao and Lei Fan and Zhu Han},
  doi          = {10.1109/TMC.2025.3586606},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12567-12579},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributionally robust contract theory for edge AIGC services in teleoperation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical optimizing UAV trajectory in wireless charging networks: An approximated approach. <em>TMC</em>, <em>24</em>(11), 12550-12566. (<a href='https://doi.org/10.1109/TMC.2025.3586457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) can be easily deployed as auxiliary base stations due to their convenience and flexibility. However, limited battery capacity becomes a bottleneck. Promising wireless power transfer (WPT) technologies can provide a continuous power supply for UAVs. Many of the recent works treat the UAV battery capacity as a constraint, which hinders the assurance of continuous UAV operation. Furthermore, most studies employ intelligent path-planning algorithms that lack explicit performance guarantees. In this paper, we study the problem of Practical Optimizing UAV Trajectory in Wireless Charging Networks (POTWCN), which involves planning the trajectory of the wireless-powered UAV in the practical environment with obstacles by selecting candidate passing positions and determining the access order in the charging network. The goal is to maximize the benefit, i.e., balancing the total task completion time and the number of charging stations visited, so as to minimize path length and flight time, and ensure energy constraints with performance bound. To solve this problem, we first formalize the problem and prove its submodularity. Then, we propose the obstacle-aware weighted graph generation algorithm (OWGGA) to deal with the obstacles in the environment, which forms an obstacle-avoidance path using tangents and arcs between two hovering positions and the blocking obstacles. Next, we propose a dynamic charging station selection algorithm (ACSA), which maximizes the UAV’s energy utilization by limiting the number of charging stations that can be included. In the algorithm, we introduce the Christofides algorithm and use the path length calculated by OWGGA as the edge weights of the graph. Subsequently, considering the UAV’s energy constraints, we iteratively solve the UAV trajectory planning problem by adding the charging station with a maximized marginal benefit to the path. We prove that the proposed algorithm achieves an approximation ratio $1 - 1/e$ as well as the path length is at most $3\pi /4$ times the optimal solution. Simulation results show that our algorithm reduces the flight distance by 38.01% and the task completion time by 34.00% on average.},
  archive      = {J_TMC},
  author       = {Yundi Wang and Xiaoyu Wang and He Huang and Haipeng Dai},
  doi          = {10.1109/TMC.2025.3586457},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12550-12566},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Practical optimizing UAV trajectory in wireless charging networks: An approximated approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive parameter-efficient federated fine-tuning on heterogeneous devices. <em>TMC</em>, <em>24</em>(11), 12533-12549. (<a href='https://doi.org/10.1109/TMC.2025.3586644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained language models in a distributed manner. However, there are two critical challenges for efficient FedFT in practical applications, i.e., resource constraints and system heterogeneity. Existing works rely on parameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA)1, but with major limitations. Herein, based on the inherent characteristics of FedFT, we observe that LoRA layers with higher ranks added close to the output help to save resource consumption while achieving comparable fine-tuning performance. Then we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces the difficulty of determining the number of LoRA layers (called, LoRA depth) and the rank of each LoRA layer (called, rank distribution). We analyze the coupled relationship between LoRA depth and rank distribution, and design an efficient LoRA configuration algorithm for heterogeneous devices, thereby promoting fine-tuning efficiency. Extensive experiments are conducted on a physical platform with 80 commercial devices. The results show that LEGEND can achieve a speedup of 1.5-2.8× and save communication costs by about 42.3% when achieving the target accuracy, compared to the advanced solutions.},
  archive      = {J_TMC},
  author       = {Jun Liu and Yunming Liao and Hongli Xu and Yang Xu and Jianchun Liu and Chen Qian},
  doi          = {10.1109/TMC.2025.3586644},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12533-12549},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive parameter-efficient federated fine-tuning on heterogeneous devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RF-DEGO: A range free localization algorithm for non uniform node distributions and obstacle environments. <em>TMC</em>, <em>24</em>(11), 12517-12532. (<a href='https://doi.org/10.1109/TMC.2025.3586636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Range-free localization algorithms have attracted considerable attention for outdoor wireless sensor network (WSN) positioning because they are less susceptible to environmental factors when estimating inter node distances and require only a few beacon nodes with known locations to rapidly determine all node positions. Among these, the connectivity based DV Hop algorithm has become widely used due to its simplicity and ease of implementation. However, its localization accuracy is limited and it is easily degraded by non uniform node distributions and obstacle environments. To address these shortcomings, this paper proposes a novel range free localization algorithm (RF-DEGO). First, a new distance estimation formula is derived from node connectivity and the probability distribution of distances. Next, the estimated distances are corrected using the local node density along communication paths, and paths identified as detouring around obstacles receive a further correction. Finally, an enhanced hierarchical Grey Wolf Optimization algorithm computes the node positions. Extensive simulation experiments under various network scenarios and parameter settings show that the proposed algorithm outperforms several existing localization methods in both accuracy and computation time, demonstrating superior overall performance and strong competitiveness.},
  archive      = {J_TMC},
  author       = {Haibin Sun and Yongzheng Zhang},
  doi          = {10.1109/TMC.2025.3586636},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12517-12532},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RF-DEGO: A range free localization algorithm for non uniform node distributions and obstacle environments},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On information collection in multi-tagged COTS RFID systems. <em>TMC</em>, <em>24</em>(11), 12505-12516. (<a href='https://doi.org/10.1109/TMC.2025.3586660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of target object information collection in multi-tagged COTS RFID systems. Unlike its single-tagged peers, the multi-tagged COTS RFID scenario poses new challenges in devising information collection algorithms: 1) Tags attached to the same object carry identical information. Hence, reusing single-tagged information collection algorithms leads to unnecessary redundancy; 2) Multi-tagged RFID systems are often deployed in applications where tags are vulnerable to damage. Such faulty tags may severely degrade the performance of information collection; 3) Most state-of-the-art information collection algorithms rely heavily on the hashing operation that is not seamlessly supported by the C1G2 standard, rendering these solutions inefficient and impractical. To tackle these technical challenges, this paper makes three contributions. First, we develop an efficient and compact tag pseudo-ID design, enabling the reader to select a single tag from each target object to collect information with only one Select command. Second, we construct a robust fault-handling mechanism capable of recognizing faulty tags without executing the entire slot. Third, armed with the above two techniques, we develop a novel information collection algorithm by leveraging the functionality offered by C1G2 to optimize the information collection sequence, thus minimizing the overall execution time. Empirical experiments on a COTS RFID system prototype demonstrate that our algorithm outperforms the best existing solution by 35–50% on average.},
  archive      = {J_TMC},
  author       = {Kanghuai Liu and Jihong Yu and Lin Chen},
  doi          = {10.1109/TMC.2025.3586660},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12505-12516},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On information collection in multi-tagged COTS RFID systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source routing for LEO mega-constellations based on bloom filter. <em>TMC</em>, <em>24</em>(11), 12487-12504. (<a href='https://doi.org/10.1109/TMC.2025.3586626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-earth-orbit (LEO) mega-constellations with inter-satellite links (ISLs) are becoming the Internet backbone in space. Satellites within LEO often need the capability to enforce data forwarding paths. For example, they may need to bypass the satellites over the untrusted areas for the data of mission-critical applications or minimize latency for the data of time-sensitive applications. However, typical source/segment routing techniques (e.g., SRv6) suffer from scalability issue, since they record source-route-style forwarding information via the list-based structure. This results in great payload and forwarding overhead. To overcome this drawback, we propose a source/segment routing architecture for LEO mega-constellations, which is named as Link-identified Routing (LiR). LiR leverages in-packet bloom filter (BF) to record source-route-style forwarding information. BF could efficiently record multiple elements via a probabilistic data structure, but overlooks the order of the encoded elements. To address this, LiR identifies each unidirectional ISL, and represents the path by encoding ISL identifiers into BF. We investigate how to optimize BF configuration and ISL encoding policy to address false positives caused by BF. We implement LiR in Linux kernel and develop a container-based emulator for performance evaluation. Results show that LiR significantly outperforms SRv6 in terms of packet forwarding and data delivery efficiency.},
  archive      = {J_TMC},
  author       = {Hefan Zhang and Zhiyuan Wang and Wenhao Lu and Shan Zhang and Hongbin Luo},
  doi          = {10.1109/TMC.2025.3586626},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12487-12504},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Source routing for LEO mega-constellations based on bloom filter},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling effective OOD detection via plug-and-play network for mobile visual applications. <em>TMC</em>, <em>24</em>(11), 12471-12486. (<a href='https://doi.org/10.1109/TMC.2025.3586625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices have increasingly integrated with numerous deep learning-based visual applications, such as object classification and recognition models. While these models perform well in controlled environments, their effectiveness declines in real-world environment due to out-of-distribution (OOD) data not seen during training. Existing methods for detecting OOD data often compromise normal data recognition and require extensive training on unattainable OOD data. To address these issues, we propose $\mathtt {POD}$, a framework designed to enhance mobile visual applications by providing high-precision OOD detection without affecting original model performance. In the offline phase, $\mathtt {POD}$ generates OOD detectors from any classification model by analyzing model’s neuron responses to various data types. In the online phase, it continuously adjusts decision boundaries by integrating results from both the original model and the detector. Evaluated on two public datasets and one self-collected dataset across various popular classification models, $\mathtt {POD}$ significantly improves OOD detection performance while maintaining the accuracy of original models.},
  archive      = {J_TMC},
  author       = {Zixiao Wang and Qi Dong and Tianzhang Xing and Zhidan Liu and Zhenjiang Li and Xiaojiang Chen},
  doi          = {10.1109/TMC.2025.3586625},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12471-12486},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling effective OOD detection via plug-and-play network for mobile visual applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HybridRDN: Delay-optimal computation offloading for autonomous vehicle fleets based on RSMA. <em>TMC</em>, <em>24</em>(11), 12456-12470. (<a href='https://doi.org/10.1109/TMC.2025.3586638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate-splitting multiple access (RSMA), space division multiple access (SDMA), and non-orthogonal multiple access (NOMA) have gained significant popularity and are extensively utilized across various domains. However, it is still unclear whether hybrid RSMA-SDMA-NOMA (HybridRDN) would seamlessly combine the advantages of RSMA, SDMA, and NOMA to contribute to the computation offloading of autonomous vehicle systems. To address the above issue, this paper introduces a novel HybridRDN-assisted computation offloading fleet (COF) scheme tailored for autonomous vehicle systems. First, we propose a stochastic-geometry-aided method to model the offloading framework. Afterwards, the task vehicles (TVs) ingeniously employ the proposed HybridRDN scheme to offload tasks to the resource vehicles (RVs) in each COF to relieve their computational burden. Diverging from the sole optimization of the task segmentation ratio or the transmission rate, a joint optimization problem involving the transmission weighting factor, the HybridRDN precoding matrix, the common rate, and the task segmentation ratio, is formulated, which aims to minimize the average delay of the COF system while approaching the rate performance of the ideal HybridRDN. Furthermore, a delay-optimal alternating optimization algorithm (DOAOA) is developed to obtain the solution for the optimization problem. Experimental results validate the plausibility and superiority of the proposed framework compared to the state-of-the-art schemes.},
  archive      = {J_TMC},
  author       = {Zhijian Lin and Yang Xiao and Yi Fang and Hongbing Chen and Xiaoqiang Lu},
  doi          = {10.1109/TMC.2025.3586638},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12456-12470},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HybridRDN: Delay-optimal computation offloading for autonomous vehicle fleets based on RSMA},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance analysis of direct acyclic graph-based ledgers in low-to-high load regime. <em>TMC</em>, <em>24</em>(11), 12441-12455. (<a href='https://doi.org/10.1109/TMC.2025.3586668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct acyclic graph (DAG)-based ledgers and distributed consensus algorithms have been proposed for use in the Internet of Things (IoT). The DAG-based ledgers have many advantages over single-chain blockchains, such as low resource consumption, low transaction fee, high transaction throughput, and short confirmation delay. However, the scalability of the DAG consensus has not been comprehensively verified on a large scale. This paper explores the scalability of DAG consensus within the low-to-high load regime (L2HR) using the tangle model, where L2HR characterizes the transition from a phase of low network load to another phase of high network load. In particular, we determine the average number of tips in the tangle in L2HR when adopting the uniform random tip selection (URTS) and rigorously prove that using the tangle model, the average number of tips at the end of L2HR converges to a constant. We also analyze the probability that a transaction in L2HR becomes an abandoned tip, the approximate average time required for the network load to transition from low load regime (LR) to high load regime (HR), and the average time required for a tip being approved for the first time in L2HR. All analytics are verified by numerical simulations.},
  archive      = {J_TMC},
  author       = {Qingwen Wei and Shuping Dang and Zhihui Ge and Xiangcheng Li and Zhenrong Zhang},
  doi          = {10.1109/TMC.2025.3586668},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12441-12455},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Performance analysis of direct acyclic graph-based ledgers in low-to-high load regime},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multipath differential routing and traffic scheduling in ultra-dense LEO satellite networks: A DRL with stackelberg game approach. <em>TMC</em>, <em>24</em>(11), 12424-12440. (<a href='https://doi.org/10.1109/TMC.2025.3586262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth orbit satellite networks (LSNs) are envisioned as key enablers of 6G by offering ubiquitous, low-latency connectivity. Their mesh topology enables multipath differential routing, which improves bandwidth utilization and reduces transmission delay. However, the growing demand for data and the dynamic, self-organizing nature of LSNs pose significant challenges for joint multipath routing and traffic scheduling under strict latency and energy constraints. To address these challenges, this paper proposes a multipath routing optimization (MRO) and traffic scheduling method tailored for multipath differential routing. Specifically, a dynamic multi-attribute graph model is developed to precisely capture the dynamic properties of LSNs. Building on this model, a MRO algorithm, integrated with a Stackelberg game framework, is introduced. The MRO algorithm employs a decomposition-based approach to identify multiple optimal paths that minimize delay and energy consumption, while the Stackelberg game framework ensures efficient traffic distribution across these paths. Numerical results demonstrate that the proposed approach significantly outperforms existing baseline methods, achieving cumulative reward improvements of 26.77% to 43.8% across four real-world network topologies and exhibiting better Pareto front coverage. Furthermore, by leveraging the rapid convergence properties of the Stackelberg game model, the proposed method enhances network throughput by 12% to 43% and reduces transmission time by 14% to 49%.},
  archive      = {J_TMC},
  author       = {Shuyang Li and Qiang Wu and Ran Wang and Long Chen and Hongke Zhang},
  doi          = {10.1109/TMC.2025.3586262},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12424-12440},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient multipath differential routing and traffic scheduling in ultra-dense LEO satellite networks: A DRL with stackelberg game approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fed$n$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math>P: Federated unlearning with multiple client set partitions. <em>TMC</em>, <em>24</em>(11), 12406-12423. (<a href='https://doi.org/10.1109/TMC.2025.3586441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has garnered increased attention in the field of distributed machine learning and privacy computing. In the FL setup, effective and efficient unlearning algorithms are required to remove the impact of specific training data from the trained model, called federated unlearning. However, traditional machine unlearning algorithms face limitations in FL systems because the client data is private and even non-IID. In this paper, we propose a new federated unlearning algorithm called Fed$n$P. Our approach involves dividing the client set into subsets using multiple different partitions. We then train constituent models for each client subset within these partitions using existing FL algorithms and aggregate the results of constituent models for predictions. With multiple partitions, Fed$n$P limits the influence of the data to be erased within its belonging subsets, while it also improves the accuracy of the aggregated prediction. Based on the multiple-partition framework, we design partition creation methods to effectively enhance the prediction accuracy. Furthermore, we propose a cost reduction method to reduce the cost of training/retraining. Our extensive experiments on various datasets and model architectures demonstrate that Fed$n$P improves prediction accuracy while well-controls the additional cost.},
  archive      = {J_TMC},
  author       = {Juncheng Jia and Weipeng Zhu and Bing Luo and Xiaodong Lin and Liuchen Ma},
  doi          = {10.1109/TMC.2025.3586441},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12406-12423},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fed$n$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math>P: Federated unlearning with multiple client set partitions},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum-driven efficient learning model for enhancing robustness of IoT topology. <em>TMC</em>, <em>24</em>(11), 12391-12405. (<a href='https://doi.org/10.1109/TMC.2025.3586749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of Internet of Things (IoT) topologies measures a network structure’s tolerance to random failures, or attacks, which is crucial for stable network communication. Research on optimizing network topology robustness has shifted from empirical rules and heuristics to machine learning, which can extract the features of robust network from topology data, thereby reducing the complexity of traditional topology optimization. However, machine learning approaches typically require a large number of parameters, resulting in high costs associated with parameter tuning and inference. To address these issues, this paper combines parameterized quantum circuits, and proposes a Quantum-Driven efficient Learning Model (QDLM) for enhancing robustness of IoT topology. This model leverages quantum exponential states to significantly reduce the number of training parameters while preserving learning performance. For inputs, QDLM integrates arithmetic encoding and quantum state encoding based on topological adjacency matrix, reducing the number of neurons. In training phase, parameterized quantum rotation gates and controlled quantum gates are used to achieve efficient training. A quantum measurement method is designed to ensure the output topology is a connected graph with the required number of edges. Compared to existing topology learning models, QDLM achieves an order-of-magnitude reduction in training parameters while maintaining topology learning effectiveness.},
  archive      = {J_TMC},
  author       = {Songwei Zhang and Tie Qiu and Xiaobo Zhou and Yusheng Ji},
  doi          = {10.1109/TMC.2025.3586749},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12391-12405},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A quantum-driven efficient learning model for enhancing robustness of IoT topology},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UltraWrite: A lightweight continuous gesture input system with ultrasonic signals on COTS devices. <em>TMC</em>, <em>24</em>(11), 12375-12390. (<a href='https://doi.org/10.1109/TMC.2025.3586259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the advantages of device ubiquity, natural interaction and privacy preservation, acoustic-based gesture input has received widespread attention. Researchers have proposed various techniques for different applications. However, the existing work has shortcomings of heavy data-collection overhead, non-continuous input, and performance degradation in cross-user scenarios. To overcome these shortcomings, we propose $\mathsf{UltraWrite}$, an acoustic-based gesture input system that only needs extremely low data-collection overhead, supports continuous input, and achieves high cross-user recognition accuracy. The key idea of our solution is to synthesize training data of continuous gestures from isolated ones, build a lightweight continuous gesture recognition model based on connectionist temporal classification (CTC) mechanism, and design a novel decoupled model training strategy to improve its cross-user recognition capability. We have implemented prototype systems on commercial devices and conducted comprehensive experiments to evaluate their performance. The results show that $\mathsf{UltraWrite}$ achieves an average top-1 word accuracy of 99.3% and top-1 word error rate of 0.34%. In addition, we have also evaluated $\mathsf{UltraWrite’s}$ robustness to the sensing distance, angle, background noise, and device. The results reveal that $\mathsf{UltraWrite}$ possesses strong robustness to these factors.},
  archive      = {J_TMC},
  author       = {Yongpan Zou and Weiyu Chen and Yunshu Wang and Canlin Zheng and Wenfeng He and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3586259},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12375-12390},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UltraWrite: A lightweight continuous gesture input system with ultrasonic signals on COTS devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RISensing: Leveraging reconfigurable intelligent surfaces to empower wi-fi sensing. <em>TMC</em>, <em>24</em>(11), 12359-12374. (<a href='https://doi.org/10.1109/TMC.2025.3583916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi technology has emerged as a promising solution for contact-free sensing owing to the pervasiveness of Wi-Fi signals in indoor environments. However, Wi-Fi sensing faces several fundamental issues, including limited sensing range and unstable orientation-dependent sensing performance, hindering the widespread adoption of Wi-Fi sensing in real-life scenarios. In this paper, we propose RISensing, a novel system that leverages Reconfigurable Intelligent Surfaces (RIS) to address these two fundamental issues of Wi-Fi sensing and bring Wi-Fi sensing one step closer to real-world adoption. Unlike prior Wi-Fi sensing works which typically rely on a single target reflection signal to capture the target movement, RISensing utilizes two target reflection signals, i.e., the direct target reflection signal and RIS-based target reflection signal, to boost the sensing capability. RISensing characterizes the RIS-based target reflection signal, and constructively combines it with the direct target reflection. We evaluate the sensing performance of RISensing in various environments, including corridor, office and lab. Extensive experiments demonstrate RISensing can improve the sensing range of Wi-Fi from 4 m to 23 m, and effectively mitigate the orientation-dependent issue.},
  archive      = {J_TMC},
  author       = {Xiaojing Wang and Binbin Xie and Guanghui Lv and Boyang Liu and Chenhao Ma and Renjie Zhao and Chao Feng and Xiaojiang Chen},
  doi          = {10.1109/TMC.2025.3583916},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12359-12374},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RISensing: Leveraging reconfigurable intelligent surfaces to empower wi-fi sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmOrbit: Micrometer-level vibration and rotor orbit measurement via synchronized dual mmWave radars. <em>TMC</em>, <em>24</em>(11), 12345-12358. (<a href='https://doi.org/10.1109/TMC.2025.3582545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce mmOrbit, a mmWave-based rotor orbit measurement system that can estimate rotor orbit by analyzing machinery surface vibration. To measure the 2D rotor orbit, two synchronized mmWave radars are deployed to build the orbit from different viewpoints. The existing literature shows that the micro-displacement measurement accuracy of mmWave radar is not enough to meet the application requirements of micron-level resolution without appropriate fine-gained processing methods. Therefore, we propose a three-step vibration displacement extraction algorithm with sliding table to extract mechanical vibration micro-displacement and increase measurement precision. Accurate mechanical vibration displacements in two vertical directions are used to estimate the 2D rotor orbit. Our extensive experiments indicate that mmOrbit can accurately measure mechanical vibration micro-displacement with an error of 4.4 $\mu \text{m}$ for the $80\text{th}$ percentile. Furthermore, mmOrbit can estimate 2D rotor orbit with high precision, showing an orbit eccentricity error of 7%, an orbit direction error of $7^\circ$, and a disjoint area proportion of 17% for the $80\text{th}$ percentile. The imbalance detection experiment verifies the accuracy and dependability of our measured rotor orbit.},
  archive      = {J_TMC},
  author       = {Changlin Mao and Haocheng Ni and Jianpin Han and Yingxiao Wu},
  doi          = {10.1109/TMC.2025.3582545},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12345-12358},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmOrbit: Micrometer-level vibration and rotor orbit measurement via synchronized dual mmWave radars},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Suite-IN++: A FlexiWear BodyNet integrating global and local motion features from apple suite for robust inertial navigation. <em>TMC</em>, <em>24</em>(11), 12329-12344. (<a href='https://doi.org/10.1109/TMC.2025.3583055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of wearable technology has established multi-device ecosystems comprising smartphones, smartwatches, and headphones as critical enablers for ubiquitous pedestrian localization. However, traditional pedestrian dead reckoning (PDR) struggles with diverse motion modes, while data-driven methods, despite improving accuracy, often lack robustness due to their reliance on a single-device setup. Therefore, a promising solution is to fully leverage existing wearable devices to form a flexiwear bodynet for robust and accurate pedestrian localization. This paper presents Suite-IN++, a deep learning framework for flexiwear bodynet-based pedestrian localization. Suite-IN++ integrates motion data from wearable devices on different body parts, using contrastive learning to separate global and local motion features. It fuses global features based on the data reliability of each device to capture overall motion trends and employs an attention mechanism to uncover cross-device correlations in local features, extracting motion details helpful for accurate localization. To evaluate our method, we construct a real-life flexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and AirPods) across diverse walking modes and device configurations. Experimental results demonstrate that Suite-IN++ achieves superior localization accuracy and robustness, significantly outperforming state-of-the-art models in real-life pedestrian tracking scenarios.},
  archive      = {J_TMC},
  author       = {Lan Sun and Songpengcheng Xia and Jiarui Yang and Ling Pei},
  doi          = {10.1109/TMC.2025.3583055},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12329-12344},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Suite-IN++: A FlexiWear BodyNet integrating global and local motion features from apple suite for robust inertial navigation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Layer-aware cost-effective container updates with edge-cloud collaboration in edge computing. <em>TMC</em>, <em>24</em>(11), 12314-12328. (<a href='https://doi.org/10.1109/TMC.2025.3583153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containers have become popular for deploying applications in Edge Computing (EC) for their seamless integration and easy deployment. Frequent container updates are essential to enhance performance and introduce new challenges for cutting-edge applications such as large language models and digital twins. However, traditional container update methods result in substantial download costs and task interruptions, which are unacceptable for latency-sensitive tasks in resource-constrained EC. Existing work has largely overlooked the layered structure of container images. By leveraging this layered structure, duplicate downloads can be reduced, and various layers can be transferred from other edges, reducing burden on the remote cloud. In this paper, we model the layer-aware container update problem with edge-cloud collaboration to minimize update and scheduling costs. We present the Layer-aware Edge-cloud collaborative Container Update (LECU) algorithm based on reinforcement learning to make container update decisions. Moreover, a task scheduling algorithm is devised to schedule tasks affected by container updates to other edges, minimizing the impact of task interruptions. We implement our LECU algorithm on an edge system with real-world data traces to demonstrate its effectiveness and conduct larger-scale simulations to evaluate its scalability. Results demonstrate that our algorithms reduce container update and task scheduling costs by 14% and 19%, respectively, compared to baselines.},
  archive      = {J_TMC},
  author       = {Hanshuai Cui and Zhiqing Tang and Yuan Wu and Weijia Jia},
  doi          = {10.1109/TMC.2025.3583153},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12314-12328},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Layer-aware cost-effective container updates with edge-cloud collaboration in edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OpenRFI: Open-set radio frequency fingerprint identification via test-time fine-tuning. <em>TMC</em>, <em>24</em>(11), 12297-12313. (<a href='https://doi.org/10.1109/TMC.2025.3582980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of low-cost mobile edge devices, security and reliability have become crucial for mobile edge computing networks, especially for high-stakes applications. To facilitate it, Radio Frequency Fingerprint Identification (RFFI) has emerged as a promising physical layer security paradigm, offering a non-cryptographic and lightweight solution. However, most existing Deep Learning (DL) based RFFI methods operate under a closed-set assumption, limiting their ability to recognize devices not seen during training and posing a risk of misclassification. Addressing the open-set RFFI problem is critical for real-world deployments, where the system must handle both known and unknown devices, ensuring robust security in dynamic environments. In this paper, we propose OpenRFI, a novel test-time fine-tuning-based RFFI framework, consisting of two sequential stages: pre-training and test-time fine-tuning. During the pre-training stage, we design a data augmentation module, a feature extraction module, and an efficient hybrid loss function to minimize intra-class feature distances and tighten decision boundaries, enhancing the model’s ability to distinguish between different classes. In the test-time fine-tuning stage, we introduce a fine-tuning dataset construction module and a full-parameter fine-tuning module to dynamically adapt to the test environment and capture information from unknown samples, further improving open-set recognition. We theoretically establish the performance boundary of the fine-tuning dataset construction method, providing insights into its robustness and scalability. Extensive numerical results based on an open source dataset demonstrate the effectiveness of the proposed OpenRFI framework in comparison with existing baselines.},
  archive      = {J_TMC},
  author       = {Jian Yang and Shuai Feng and Yatong Wang and Xinghang Wu and Mu Yan},
  doi          = {10.1109/TMC.2025.3582980},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12297-12313},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {OpenRFI: Open-set radio frequency fingerprint identification via test-time fine-tuning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video conferencing with predictive generation and collaborative computation across mobile headsets. <em>TMC</em>, <em>24</em>(11), 12282-12296. (<a href='https://doi.org/10.1109/TMC.2025.3582284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has emerged as a transformative platform for remote collaboration, but its adoption for video conferencing is hindered by challenges related to facial expression reconstruction and computational resource constraints, especially on economical mobile VR headsets. This paper introduces a novel system for VR video conferencing that addresses these challenges through two key modules: Predictive Generation and Collaborative Computation. Predictive Generation leverages multimodal inputs, including voice, head motion, and eye blinks, to synthesize realistic facial animations with low latency, eliminating the need for high-precision hardware. Collaborative Computation enhances computational efficiency by employing a game-theoretic framework for resource sharing among users. Experimental evaluations demonstrate that our system delivers immersive and realistic VR video conferencing experiences with superior facial expression reconstruction and efficient resource utilization. Our approach makes VR video conferencing more accessible and practical for a broader audience across mobile headsets.},
  archive      = {J_TMC},
  author       = {Yili Jin and Xize Duan and Kaiyuan Hu and Fangxin Wang and Xue Liu and Jiangchuan Liu},
  doi          = {10.1109/TMC.2025.3582284},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12282-12296},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Video conferencing with predictive generation and collaborative computation across mobile headsets},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pricing game for federated learning supporting lightweight local model training. <em>TMC</em>, <em>24</em>(11), 12264-12281. (<a href='https://doi.org/10.1109/TMC.2025.3583013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive distribution of data across clients with privacy concerns and heterogeneous performance in edge networks presents a significant opportunity to enhance AI model performance. Federated learning (FL) enables a model owner (MO) to recruit these clients, offering compensation for their contributions, and to improve model quality by aggregating knowledge from their locally trained models. However, several challenges arise in this process. Clients may decline participation if they do not achieve positive utility. Moreover, due to constraints in memory, computing, and communication resources, some clients can only train lightweight models that represent partial versions of the global model. Importantly, the MO’s pricing for client contributions and the proportions of local model training are interdependent, collectively influencing client utilities and participation decisions. To address these challenges, we first model the utility functions of both the MO and the clients, accommodating the support for lightweight local models. We then formulate their interactions as a Stackelberg game and theoretically prove the existence of a Nash equilibrium. Based on this equilibrium, we derive optimal collaboration strategies for both the MO and the clients. Additionally, we design an efficient approximation algorithm to enable the MO to maximize its utility by selecting suitable clients to participate in FL. Finally, extensive experiments validate our theoretical findings, demonstrating the superior performance and effectiveness of the proposed algorithms.},
  archive      = {J_TMC},
  author       = {Fengsen Tian and Mingzi Wang and Yu Zhang and Guoqiang Deng and Lingyu Liang and Xinglin Zhang},
  doi          = {10.1109/TMC.2025.3583013},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12264-12281},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A pricing game for federated learning supporting lightweight local model training},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDR-empowered environment sensing design and experimental validation using OTFS-ISAC signals. <em>TMC</em>, <em>24</em>(11), 12252-12263. (<a href='https://doi.org/10.1109/TMC.2025.3581716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the system design and experimental validation of integrated sensing and communication (ISAC) for environmental sensing, which is expected to be a critical enabler for next-generation wireless networks. We advocate exploiting orthogonal time frequency space (OTFS) modulation for its inherent sparsity and stability in delay-Doppler (DD) domain channels, facilitating a low-overhead environment sensing design. Moreover, a comprehensive environmental sensing framework is developed, encompassing DD domain channel estimation, target localization, and experimental validation. In particular, we first explore the OTFS channel estimation in the presence of fractional delay and Doppler shifts. Given the estimated parameters, we propose a three-ellipse positioning algorithm to localize the target’s position, followed by determining the mobile transmitter’s velocity. Additionally, to evaluate the performance of our proposed design, we conduct extensive simulations and experiments using a software-defined radio (SDR)-based platform with universal software radio peripheral (USRP). The experimental validations demonstrate that our proposed approach outperforms the benchmarks in terms of localization accuracy and velocity estimation, confirming its effectiveness in practical environmental sensing applications.},
  archive      = {J_TMC},
  author       = {Jun Wu and Yuye Shi and Weijie Yuan and Qingqing Cheng and Buyi Li and Xinyuan Wei},
  doi          = {10.1109/TMC.2025.3581716},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12252-12263},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SDR-empowered environment sensing design and experimental validation using OTFS-ISAC signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covert communications for intelligent reflecting surface-enabled D2D networks. <em>TMC</em>, <em>24</em>(11), 12239-12251. (<a href='https://doi.org/10.1109/TMC.2025.3583779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore covert communications in a device-to-device (D2D) network consisting of an intelligent reflecting surface (IRS), a base station, a cellular user, a D2D pair, and an adversary warden. With the help of the IRS, the D2D pair attempts to perform covert communication, while the warden also tries to detect the very existence of such a transmission. To investigate the covert performance under the scenario, we derive the detection error probability at Warden, the optimal detection threshold for minimizing the probability, and the transmission outage probabilities for D2D and cellular communications, respectively. We further jointly optimize the transmission powers of the cellular user and the D2D transmitter, the reflection phase shifts, and the amplitudes of the IRS reflecting elements to improve covert communication performance. Finally, we provide numerical results to reveal the impact of system parameters on the covert performance and also to exhibit the merits of IRS-enabled D2D networks for achieving covert communications.},
  archive      = {J_TMC},
  author       = {Yihuai Yang and Bin Yang and Shikai Shen and Yumei She and Xiaohong Jiang and Tarik Taleb},
  doi          = {10.1109/TMC.2025.3583779},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12239-12251},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Covert communications for intelligent reflecting surface-enabled D2D networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated digital twin construction via distributed sensing: A game-theoretic online optimization with overlapping coalitions. <em>TMC</em>, <em>24</em>(11), 12221-12238. (<a href='https://doi.org/10.1109/TMC.2025.3582755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel federated framework for constructing the digital twin (DT) model, referring to a living and self-evolving visualization model empowered by artificial intelligence, enabled by distributed sensing under edge-cloud collaboration. In this framework, the DT model to be built at the cloud is regarded as a global one being split into and integrating from multiple functional components, i.e., partial-DTs, created at various edge servers (ESs) using feature data collected by associated sensors. Considering time-varying DT evolutions and heterogeneities among partial-DTs, we formulate an online problem that jointly and dynamically optimizes partial-DT assignments from the cloud to ESs, ES-sensor associations for partial-DT creation, and as well as computation and communication resource allocations for global-DT integration. The problem aims to maximize the constructed DT’s model quality while minimizing all induced costs, including energy consumption and configuration costs, in long runs. To this end, we first transform the original problem into an equivalent hierarchical game with an upper-layer two-sided matching game and a lower-layer overlapping coalition formation game. After analyzing these games in detail, we apply the Gale-Shapley algorithm and particularly develop a switch rules-based overlapping coalition formation algorithm to obtain short-term equilibria of upper-layer and lower-layer subgames, respectively. Then, we design a deep reinforcement learning-based solution, called DMO, to extend the result into a long-term equilibrium of the hierarchical game, thereby producing the solution to the original problem. Simulations show the effectiveness of the introduced framework, and demonstrate the superiority of the proposed solution over counterparts.},
  archive      = {J_TMC},
  author       = {Ruoyang Chen and Changyan Yi and Fuhui Zhou and Jiawen Kang and Yuan Wu and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3582755},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12221-12238},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated digital twin construction via distributed sensing: A game-theoretic online optimization with overlapping coalitions},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaFlowLite: Scalable and non-blocking inference on asynchronous mobile data. <em>TMC</em>, <em>24</em>(11), 12206-12220. (<a href='https://doi.org/10.1109/TMC.2025.3582060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of mobile devices equipped with numerous sensors, such as LiDAR and cameras, has driven the adoption of multi-modal deep intelligence for distributed sensing tasks, such as smart cabins and driving assistance. However, the arrival time of mobile sensory data vary due to modality size and network dynamics, which can lead to delays (if waiting for slow data) or accuracy decline (if inference proceeds without waiting). Moreover, the diversity and dynamic nature of mobile systems exacerbate this challenge. In response, we present a shift to opportunistic inference for asynchronous distributed multi-modal data, enabling inference as soon as partial data arrives. While existing methods focus on optimizing modality consistency and complementarity, known as modal affinity, they lack a computational approach to control this affinity in open-world mobile environments. ${\sf AdaFlowLite}$ pioneers the formulation of structured cross-modality affinity in mobile contexts using a hierarchical analysis-based normalized matrix. This approach accommodates the diversity and dynamics of modalities, generalizing across different types and numbers of inputs. Employing an multi-modal lightweight Swin Transformer (MMLST), ${\sf AdaFlowLite}$ facilitates real-time and flexible data imputation, adapting to various modalities and downstream tasks without retraining. Experiments show that ${\sf AdaFlowLite}$ significantly reduces inference latency by up to 80.4% and enhances accuracy by up to 62.1%, while achieving nearly a 50% reduction in energy consumption, outperforming status quo approaches. Also, this method can enhance LLM performance to preprocess asynchronous data.},
  archive      = {J_TMC},
  author       = {Sicong Liu and Fengmin Wu and Yuan Gao and Bin Guo and Zimu Zhou and Hongkai Wen and Zhiwen Yu},
  doi          = {10.1109/TMC.2025.3582060},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12206-12220},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdaFlowLite: Scalable and non-blocking inference on asynchronous mobile data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward efficient verifiable data streaming without cryptographic accumulator. <em>TMC</em>, <em>24</em>(11), 12193-12205. (<a href='https://doi.org/10.1109/TMC.2025.3582087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifiable data streaming (VDS) enables the client to incrementally store a sequence of ordered data on an untrusted cloud server, and verify the validity of the retrieved data. Moreover, the client can replace a data with another value. The common security problem caused by updating operation is the cloud server may use old authentication information to make expired data pass the verification. To solve this problem, the known approaches use the cryptographic accumulator that actually influences the performance of VDS scheme. The main concerns can be generalized as how to design a VDS scheme without cryptographic accumulator, in such a way that further optimizes the performance of VDS scheme. We put forward the idea to convert the standard digital signature relevant to the updated data into chameleon digital signature whose non-transferability is the key to solve the problem. This is the first attempt to securely authenticate the dynamic data without cryptographic accumulator. In the proposed VDS scheme, the client’s local storage overhead, computation overheads of the cloud server in responding to a query and updating the data are constant. As the experimental results shown, the proposed VDS scheme outperforms the scheme in terms of the efficiency.},
  archive      = {J_TMC},
  author       = {Haining Yang and Jinlu Liu and Pingyuan Zhang and Jing Qin and Huaxiong Wang},
  doi          = {10.1109/TMC.2025.3582087},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12193-12205},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward efficient verifiable data streaming without cryptographic accumulator},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing collaborative machine learning in resource-limited networks through knowledge distillation and over-the-air computation. <em>TMC</em>, <em>24</em>(11), 12177-12192. (<a href='https://doi.org/10.1109/TMC.2025.3582683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional collaborative machine learning (CML) faces significant challenges in resource-constrained environments, such as emergency scenarios with limited power, bandwidth, and computing resources, leading to increased communication delays and energy consumption. To address these issues, this paper introduces Air-CoKD, a novel CML framework designed to reduce resource consumption and training latency while preserving model performance. Air-CoKD leverages knowledge distillation (KD) to minimize data transmission by avoiding the direct sharing of model parameters. It also integrates over-the-air computation (AirComp) to aggregate local logits, optimizing bandwidth utilization. To address the dimensional differences in local logits caused by the unbalanced device data class, Air-CoKD employs orthogonal frequency division multiplexing (OFDM) to transmitting local logits for different target classes. To handle aggregation errors introduced by AirComp, we conduct a detailed analysis of error bounds. Specifically, we convert the Kullback–Leibler (KL) divergence, used in KD loss function, into a quadratic upper bound for precise error quantification and effective optimization. Based on these insights, we propose a strategy to manage bandwidth constraints, transmission power limits, and device energy budgets within Air-CoKD. Extensive simulations demonstrate that Air-CoKD surpasses state-of-the-art methods, effectively balancing training efficiency and model performance. The framework proves to be a robust solution for CML in resource-constrained networks.},
  archive      = {J_TMC},
  author       = {Yue Zhang and Guopeng Zhang and Kun Yang and Yao Wen and Kezhi Wang},
  doi          = {10.1109/TMC.2025.3582683},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12177-12192},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing collaborative machine learning in resource-limited networks through knowledge distillation and over-the-air computation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-edge collaborative optimization of microservice caching in D2D-assisted network. <em>TMC</em>, <em>24</em>(11), 12162-12176. (<a href='https://doi.org/10.1109/TMC.2025.3582579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employing the caching resources of end users via Device-to-Device (D2D) communication to assist the edge server in microservice caching is promising to further alleviate the network congestion of the Internet of Things (IoT). However, significant extra energy consumption prevents the caching system from maximizing cache utility if all end users cache simultaneously. In this paper, we propose two novel end-edge collaborative microservice caching algorithms in D2D-assisted networks. First, we construct a D2D caching sharing link graph from the aspects of physical and social attributes of end users and introduce the Entropy-based Partitioning Around Medoid (EPAM) algorithm to identify critical users. Second, to address the challenges posed by unknown time-varying user preferences, we model the end-edge collaborative caching problem as a Multi-Agent Multi-Armed Bandit (MAMAB) problem, thus developing two caching decision schemes, i.e, Edge-Centric Scheme (ECS) and User-Centric Scheme (UCS), to accommodate different decision sequences. The simulation results show that the EPAM-ECS and EPAM-UCS have at least 29.2% and 39.3% improvement compared with other baseline algorithms.},
  archive      = {J_TMC},
  author       = {Qingyong Deng and Mengyao Li and Zhetao Li and Haolin Liu and Yong Xie},
  doi          = {10.1109/TMC.2025.3582579},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12162-12176},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {End-edge collaborative optimization of microservice caching in D2D-assisted network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cell-free massive MIMO detection: A distributed expectation propagation approach. <em>TMC</em>, <em>24</em>(11), 12149-12161. (<a href='https://doi.org/10.1109/TMC.2025.3583019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell-free massive MIMO is one of the core technologies for next-generation wireless networks. It is expected to bring enormous benefits, including ultra-high reliability, data throughput, energy efficiency, and uniform coverage. However, the radically distributed architecture of cell-free massive MIMO necessitates new paradigms for transceiver design, especially by exploiting efficient distributed processing algorithms. In this paper, we propose a distributed expectation propagation (EP) detector for cell-free massive MIMO and THz ultra-massive MIMO systems, which consists of two modules: a nonlinear module at the central processing unit (CPU) and a linear module at each access point (AP). The turbo principle in iterative channel decoding is utilized to compute and pass the extrinsic information between the two modules. An analytical framework is provided to characterize the asymptotic performance of the proposed EP detector with a large number of antennas. Furthermore, a distributed iterative channel estimation and data detection (ICD) algorithm is developed to handle the practical scenario with imperfect channel state information (CSI). Simulation results will show that the proposed method outperforms existing detectors for cell-free massive MIMO systems in terms of the bit-error rate and the developed theoretical analysis can be utilized as an asymptotic lower bound. Finally, it is shown that with imperfect CSI, the proposed ICD algorithm can significantly improve the system performance and reduce the pilot overhead.},
  archive      = {J_TMC},
  author       = {Hengtao He and Xianghao Yu and Jun Zhang and Shenghui Song and Ross D. Murch and Khaled B. Letaief},
  doi          = {10.1109/TMC.2025.3583019},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12149-12161},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cell-free massive MIMO detection: A distributed expectation propagation approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An online computation offloading approach with dual stability guarantee for heterogeneous tasks in MEC-enabled IIoT. <em>TMC</em>, <em>24</em>(11), 12137-12148. (<a href='https://doi.org/10.1109/TMC.2025.3581600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of information and the continuous expansion of applications, Industrial Internet of Things (IIoT) is facing huge data processing and storage pressure. With mobile edge computing (MEC) technology, the computing power network connects the geographically distributed computing nodes and then coordinates the allocation and scheduling of resources, transmits data, and eventually relieves the pressure of the industrial site. However, the rigorous demands of IIoT for real-time and stability pose some daunting challenges. To this end, we propose an online computation offloading approach with dual stability guarantee, named OCODSG. Specifically, the Lyapunov function is used to optimize the stability of the virtual queue, and the system stability is optimized based on the network jitter measurement. Moreover, the Dueling Double Deep Q Network (D3QN) algorithm based on deep reinforcement learning (DRL) is used for model autonomous training, while Gaussian noise is added to the network parameter space to encourage exploration and enhance algorithm robustness. Finally, experimental results on both simulated and real datasets demonstrate that OCODSG improves service efficiency and system stability.},
  archive      = {J_TMC},
  author       = {Kai Peng and Chengfang Ling and Shangguang Wang and Victor C.M. Leung},
  doi          = {10.1109/TMC.2025.3581600},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12137-12148},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An online computation offloading approach with dual stability guarantee for heterogeneous tasks in MEC-enabled IIoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional and secure spatial keyword query with arbitrary ranges in mobile cloud. <em>TMC</em>, <em>24</em>(11), 12121-12136. (<a href='https://doi.org/10.1109/TMC.2025.3581562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial keyword query has emerged as a critical service in mobile cloud, enabling cloud servers to retrieve spatio-textual objects within a mobile user’s query range that contain specified query keywords. Numerous secure spatial keyword query schemes have been developed to enable geometric range queries and keyword searches on encrypted spatial data. However, spatial keyword queries are typically designed for searching high-dimensional spatial data across arbitrary geographic ranges. Most of them fail to handle arbitrary geometric range queries and efficient spatial keyword query over high-dimensional encrypted data. To address these issues, we propose a high-dimEnsional and Privacy-preserving Spatial Keyword Query (EPSKQ) scheme with arbitrary geometric ranges over encrypted spatial data, leveraging Hilbert curve encoding and Enhanced Matrix-based Inner Product Encryption (EMIPE). In EPSKQ, spatial locations and multi-keywords are encoded into compact vectors, and arbitrary geometric range queries are transformed into range intersection tests. To reduce computational overhead, we employ vector bucketing technique to partition large-size vectors into several small-size sub-vectors. Furthermore, we design a novel index structure called Hilbert Binary tree (HB-tree) to optimize range intersection tests. Based on HB-tree, we propose an enhanced spatial keyword query scheme, named EPSKQ+, which further improves query performance. Security analysis demonstrates that both EPSKQ and EPSKQ+ achieve semantic security against indistinguishability under chosen-plaintext attack (IND-CPA). Extensive experimental evaluations show that the proposed EPSKQ and EPSKQ+ schemes significantly outperform state-of-the-art schemes in terms of computational and communication costs, with EPSKQ+ being 9× and 3× faster than the state-of-the-art schemes in the index build and query phase, respectively.},
  archive      = {J_TMC},
  author       = {Fuyuan Song and Yunlong Gao and Mingyang Zhao and Chuan Zhang and Zheng Qin and Bin Xiao},
  doi          = {10.1109/TMC.2025.3581562},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12121-12136},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {High-dimensional and secure spatial keyword query with arbitrary ranges in mobile cloud},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical and heterogeneous federated learning via a learning-on-model paradigm. <em>TMC</em>, <em>24</em>(11), 12103-12120. (<a href='https://doi.org/10.1109/TMC.2025.3581534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) collaboratively trains a shared global model without exposing clients’ private data. In practical FL systems, clients (e.g., smartphones and wearables) typically have disparate system resources. Traditional FL, however, adopts a one-size-fits-all solution, where a homogeneous large model is sent to and trained on each client. This method results in an overwhelming workload for less capable clients and starvation for others. To tackle this, we propose FedConv, a client-friendly FL framework, minimizing the system overhead on resource-constrained clients by providing heterogeneous customized sub-models. FedConv features a novel learning-on-model paradigm that learns the parameters of heterogeneous sub-models via convolutional compression. To aggregate heterogeneous sub-models, we propose transposed convolutional dilation to convert them back to large models with a unified size while retaining personalized information. The compression and dilation processes, transparent to clients, are tuned on the server using a small public dataset. We further propose a hierarchical and clustering-based local training strategy for enhanced performance. Extensive experiments on six datasets show that FedConv outperforms state-of-the-art FL systems in terms of model accuracy (by more than 35% on average), computation and communication overhead (with 33% and 25% reduction, respectively).},
  archive      = {J_TMC},
  author       = {Leming Shen and Qiang Yang and Kaiyan Cui and Yuanqing Zheng and Xiao-Yong Wei and Jianwei Liu and Jinsong Han},
  doi          = {10.1109/TMC.2025.3581534},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12103-12120},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hierarchical and heterogeneous federated learning via a learning-on-model paradigm},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NestQuant: Post-training integer-nesting quantization for on-device DNN. <em>TMC</em>, <em>24</em>(11), 12088-12102. (<a href='https://doi.org/10.1109/TMC.2025.3582583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying quantized deep neural network (DNN) models with resource adaptation capabilities on ubiquitous Internet of Things (IoT) devices to provide high-quality AI services can leverage the benefits of compression and meet multi-scenario resource requirements. However, existing dynamic/mixed precision quantization requires retraining or special hardware, whereas post-training quantization (PTQ) has two limitations for resource adaptation: (i) The state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying multiple PTQ models with diverse bitwidths consumes large storage resources and switching overheads. To this end, this paper introduces a resource-friendly post-training integer-nesting quantization, i.e., NestQuant, for on-device quantized model switching on IoT devices. The proposed NestQuant incorporates the integer weight decomposition, which bit-wise splits quantized weights into higher-bit and lower-bit weights of integer data types. It also contains a decomposed weights nesting mechanism to optimize the higher-bit weights by adaptive rounding and nest them into the original quantized weights. In deployment, we can send and store only one NestQuant model and switch between the full-bit/part-bit model by paging in/out lower-bit weights to adapt to resource changes and reduce consumption. Experimental results on the ImageNet-1 K pretrained DNNs demonstrated that the NestQuant model can achieve high performance in top-1 accuracy, and reduce in terms of data transmission, storage consumption, and switching overheads. In particular, the ResNet-101 with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and part-bit models, respectively, and reduce switching overheads by approximately 78.1% compared with diverse bitwidths PTQ models.},
  archive      = {J_TMC},
  author       = {Jianhang Xie and Chuntao Ding and Xiaqing Li and Shenyuan Ren and Yidong Li and Zhichao Lu},
  doi          = {10.1109/TMC.2025.3582583},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12088-12102},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {NestQuant: Post-training integer-nesting quantization for on-device DNN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSID: Enhancing wi-fi based gait recognition via adversarial learning. <em>TMC</em>, <em>24</em>(11), 12076-12087. (<a href='https://doi.org/10.1109/TMC.2025.3583946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Wi-Fi sensing, wireless-based gait recognition has become increasingly important as it supports a wide range of applications (person identification, disease diagnosis, etc.). However, two serious challenges limit the universal deployment of such Wi-Fi vision schemes: i) the limited bandwidth of Wi-Fi severely restricts the granularity of gait recognition, and ii) users’ non-gait behaviors (e.g., stopping and turning) interfere with the extraction of gait-related features. In this paper, we propose CSID, which can achieve robust gait recognition under the limited bandwidth conditions of commercial Wi-Fi devices. Specifically, we use a neural network to generate super-resolution spectrograms of channel state information (CSI), overcoming the limitation of insufficient Wi-Fi bandwidth. To overcome the challenge of non-gait behavior interference, considering the human-incomprehensible nature of Wi-Fi spectrograms, we adopt cross-domain adversarial training and further extract gait features that are independent of the interference behaviors by learning domain-independent representations. We conducted a large number of experiments in different indoor environments, and the average person identification rate of the CSID system reached 91.6%. These results demonstrate that the CSID system is promising and could be used as a complement to visual person identification systems in the future.},
  archive      = {J_TMC},
  author       = {Yu Liu and Jingyang Hu and Hongbo Jiang and Kehua Yang and Wei Zhang and Zheng Qin},
  doi          = {10.1109/TMC.2025.3583946},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12076-12087},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSID: Enhancing wi-fi based gait recognition via adversarial learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced stereo UWB bearing scheme via network ambiguity resolution and online phase calibration. <em>TMC</em>, <em>24</em>(11), 12061-12075. (<a href='https://doi.org/10.1109/TMC.2025.3583257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-wideband (UWB) is a prominent technology for wireless localization, mainly attributed to its superior ranging performance enabled by the large signal bandwidth. However, its bearing capability remains underdeveloped due to practical issues such as phase deviations, antenna coupling, and phase ambiguity. This paper presents a high-accuracy stereo ultra-wideband (UWB) bearing scheme through network ambiguity resolution and online phase calibration. Specifically, we propose a sparse variational Gaussian process regression-based calibration technique to eliminate phase deviations and a range-assisted network solution to resolve phase ambiguities. Building on these techniques, we present an online angle estimation scheme that performs real-time phase calibration, ambiguity resolution, and calibration model updates, significantly reducing calibration complexity in large-scale networks. Real-world experiments on 4-element stereo UWB platforms achieve root mean square errors of 2.3$^\circ$ and 1.1$^\circ$ for azimuth and elevation angles, respectively. The success rate for ambiguity resolution exceeds 96%, a 20% improvement over existing methods.},
  archive      = {J_TMC},
  author       = {Yi Li and Hanying Zhao and Yiman Liu and Yuan Shen},
  doi          = {10.1109/TMC.2025.3583257},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12061-12075},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An enhanced stereo UWB bearing scheme via network ambiguity resolution and online phase calibration},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural networks for the optimization of collaborative federated learning energy efficiency. <em>TMC</em>, <em>24</em>(11), 12049-12060. (<a href='https://doi.org/10.1109/TMC.2025.3582911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper delves into the design of an energy efficient collaborative federated learning (CFL) methodology using which mobile devices exchange their FL model with a subset of their neighbors without reliance on a parameter server based on the distributed graph neural network (GNN) method. Each device is unable to send its FL model to every neighboring device due to device mobility and wireless resource limitations. To reduce the energy consumption of FL model transmission, each device must choose a subset of devices with which to share its FL model. This problem is formulated as an optimization problem to meet the constraints of delay and training loss while minimizing the energy consumption for model transmission. However, the formulated problem is difficult to solve since the device mobility patterns, and the relationship between the device connection scheme and CFL performance are unknown. To address this challenge, we analytically characterize the relationship between dynamic device connections and the performance of CFL methodology. Based on the analysis, a GNN based algorithm is proposed to enable each device to select a subset of its neighbors and the transmit power in a decentralized method. Compared to standard optimization methods that must determine device connections in a centralized manner, the GNN based method enables each device to use its neighboring devices’ location and connection information to individually determine a subset of devices to transmit the local model. Given the device connections, the optimal transmit power of each device can be determined by convex optimization. Simulation results show that the proposed method can reduce the energy consumption for model transmission and training loss by up to 46$\%$ and 2$\%$, respectively.},
  archive      = {J_TMC},
  author       = {Nuocheng Yang and Sihua Wang and Yuchen Liu and Christopher G. Brinton and Changchuan Yin and Mingzhe Chen},
  doi          = {10.1109/TMC.2025.3582911},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12049-12060},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Graph neural networks for the optimization of collaborative federated learning energy efficiency},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-design of sensing, communications, and control for low-altitude wireless networks. <em>TMC</em>, <em>24</em>(11), 12035-12048. (<a href='https://doi.org/10.1109/TMC.2025.3581616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Internet of Things (IoT) services and the evolution toward the sixth generation (6G) have positioned aerial drones as critical enablers of low-altitude wireless networks (LAWNs). This work investigates the co-design of integrated sensing, communication, and control ($\mathbf {SC^{2}}$) for multi-aerial drone cooperative systems with finite blocklength (FBL) transmission. In particular, the aerial drones continuously monitor the state of the field robots and transmit their observations to the robot controller to ensure stable control while cooperating to localize an unknown sensing target (ST). To this end, a weighted optimization problem is first formulated by jointly considering the control and localization performance in terms of the linear quadratic regulator (LQR) cost and the determinant of the Fisher information matrix (FIM), respectively. The resultant problem, optimizing resource allocations, the aerial drones’ deployment positions, and multi-user scheduling, is non-convex. To circumvent this challenge, we first derive a closed-form expression of the LQR cost with respect to other variables. Subsequently, the non-convex optimization problem is decomposed into a series of sub-problems by leveraging the alternating optimization (AO) approach, in which the difference of convex functions (DC) programming and projected gradient descent (PGD) method are employed to obtain an efficient near-optimal solution. Furthermore, the convergence and computational complexity of the proposed algorithm are thoroughly analyzed. Extensive simulation results are presented to validate the effectiveness of our proposed approach compared to the benchmark schemes and reveal the trade-off between control and sensing performance.},
  archive      = {J_TMC},
  author       = {Haijia Jin and Jun Wu and Weijie Yuan and Fan Liu and Yuanhao Cui},
  doi          = {10.1109/TMC.2025.3581616},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12035-12048},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Co-design of sensing, communications, and control for low-altitude wireless networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crowdsensing for emergency response in unknown environments: A rapid strategic sensing approach. <em>TMC</em>, <em>24</em>(11), 12019-12034. (<a href='https://doi.org/10.1109/TMC.2025.3583816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Unmanned Aerial Vehicles (UAVs) and autonomous vehicles within the crowdsensing paradigm offers a promising approach to collecting environment-relevant data over large spatial areas, particularly in disaster-stricken or high-risk regions. However, deploying crowdsensing systems in emergency response scenarios presents substantial challenges. The lack of prior environmental knowledge complicates the selection of optimal sensing locations and strategy optimization, often relying on costly trial-and-error methods. Additionally, real-time decision-making is critical in such scenarios, requiring the rapid identification of optimal deployment strategies. Yet, the absence of prior knowledge further complicates the assessment of the optimality of these strategies. This gap remains inadequately addressed in existing research. To address this, we present the first framework that frames these challenges as a rapid online strategy optimization problem for mobile agent-based crowdsensing systems operating in unknown environments during emergency response scenarios. We propose ${\sf DGap\!-\! UCB}$, a novel approach within the multi-armed bandit (MAB) framework, which efficiently identifies the optimal sensing strategy with high-confidence guarantees. Leveraging the Upper-Confidence Bound (UCB) technique, ${\sf DGap \!-\! UCB}$ iteratively refines strategy selection based on reward feedback. To accelerate learning, we introduce a gap-confidence pair $(\Delta _{t}, \delta _{t})$-based Quick Stopping Criterion, enabling rapid and high-confidence identification of the optimal strategy. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of ${\sf DGap\!-\!UCB}$ over state-of-the-art techniques.},
  archive      = {J_TMC},
  author       = {Shan Su and Liang Wang and Zhiwen Yu and Xiaofang Xia and Lianbo Ma and Fei Xiong and Yao Zhang and Bin Guo},
  doi          = {10.1109/TMC.2025.3583816},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12019-12034},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Crowdsensing for emergency response in unknown environments: A rapid strategic sensing approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient multi-access edge computing for heterogeneous satellite-maritime networks: A hybrid harvesting-and-offloading design. <em>TMC</em>, <em>24</em>(11), 12001-12018. (<a href='https://doi.org/10.1109/TMC.2025.3581607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low earth orbit (LEO) constellation integrated maritime networks have recently attracted much interest due to the rapid development of maritime applications and services. LEO satellites have the advantages of wide coverage to provide seamless connection for maritime wireless devices. However, due to the limited battery and computing capacity of uncrewed aerial vehicles (UAVs) for ocean information perception and processing, the computing-intensive and delay-sensitive oceanic data suffer from long latency and high energy consumption, which degrades the efficiency of maritime services. In this paper, to enhance the perception and offloading endurance of UAVs in maritime networks, we propose an energy efficient multi-access edge computing scheme for heterogeneous satellite-maritime networks, with the objective of minimizing the cumulative transmitted energy for UAVs. Specifically, we first present a heterogeneous satellite-maritime network framework in which LEO satellites and uncrewed surface vehicles (USVs) equipped with edge servers can process workloads simultaneously. Next, considering the limited battery supply of UAVs, we propose a hybrid harvesting-and-offloading scheme for resource allocation, where UAVs first harvest energy from solar power and radio frequency power from USV, and then UAVs determine the offloading strategy for task processing. Moreover, a joint optimization problem is formulated to optimize the offloading decision, the time scheduling, and the transmitting power. We also exploit a vertical architecture to solve the formulated problem. Regarding each decomposed sub-problem, we propose efficient algorithms to derive the corresponding solutions. Finally, we provide numerical results to validate the performance of our proposed algorithms in comparison with several benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Minghui Dai and Shan Chang and Yixuan Wang and Zhou Su},
  doi          = {10.1109/TMC.2025.3581607},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12001-12018},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy-efficient multi-access edge computing for heterogeneous satellite-maritime networks: A hybrid harvesting-and-offloading design},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age of computing: A metric of computation freshness in communication and computation cooperative networks. <em>TMC</em>, <em>24</em>(11), 11987-12000. (<a href='https://doi.org/10.1109/TMC.2025.3582741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In communication and computation cooperative networks (3CNs), timely computation is crucial but not always guaranteed. There is a strong demand for a computational task to be completed within a given deadline. The time taken involves processing time, transmission time, and the impact of the deadline. However, a measure of such timeliness in 3CNs is lacking. To address this gap, we propose the novel concept of Age of Computing (AoC) to quantify computation freshness in 3CNs. Built on task timestamps, AoC serves as a practical metric for dynamic and complex real-world 3CNs. We evaluate AoC under two types of deadlines: (i) soft deadline, tasks can be fed back to the source if delayed beyond the deadline, but with additional latency; (ii) hard deadline, tasks delayed beyond the deadline are discarded. We investigate AoC in two distinct networks. In point-to-point, time-continuous networks, tasks are processed sequentially using a first-come, first-served discipline. We derive a general expression for the time-average AoC under both deadlines. Utilizing this expression, we obtain a closed-form solution for M/M/1-M/M/1 systems under soft deadlines and propose an accurate approximation for hard deadlines. These results are further extended to G/G/1-G/G/1 systems. Additionally, we introduce the concept of computation throughput, derive its general expression and an approximation, and explore the trade-off between freshness and throughput. In the multi-source, time-discrete networks, tasks are scheduled for offloading to a computational node. For this scenario, we develop AoC-based Max-Weight policies for real-time scheduling under both deadlines, leveraging a Lyapunov function to minimize its drift.},
  archive      = {J_TMC},
  author       = {Xingran Chen and Yi Zhuang and Kun Yang},
  doi          = {10.1109/TMC.2025.3582741},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11987-12000},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age of computing: A metric of computation freshness in communication and computation cooperative networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MobiMixer: A multi-scale spatiotemporal mixing model for mobile traffic prediction. <em>TMC</em>, <em>24</em>(11), 11972-11986. (<a href='https://doi.org/10.1109/TMC.2025.3585007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding mobile traffic data and predicting future trends are essential for wireless operators and service providers to allocate resources efficiently and manage energy effectively. Despite the strong performance of existing models, accurately forecasting mobile traffic remains a challenge due to limited spatial and temporal modeling capabilities and high computational complexity. This paper introduces MobiMixer, a lightweight and efficient multi-scale spatiotemporal mixing model. Its core concept is to integrate multi-scale information from both spatial and temporal dimensions to improve performance on mobile traffic data. We develop a hierarchical interaction module that incorporates super nodes to enable global high-level feature interactions among nodes with common patterns. Additionally, we employ a dynamic time warping strategy to decouple mobile traffic sequences into stable and seasonal components, which are then modeled at different scales using a multi-scale temporal mixing module. We conduct extensive experiments on mobile traffic datasets collected from four international cities. Compared with 21 state-of-the-art benchmark models, MobiMixer demonstrates highly competitive performance, achieving a maximum improvement of 48.49% on the Milan mobile dataset. The model achieves an improvement in training efficiency of up to 10.69 times and reduces memory usage by 33.01%.},
  archive      = {J_TMC},
  author       = {Jiaming Ma and Binwu Wang and Pengkun Wang and Zhengyang Zhou and Yudong Zhang and Xu Wang and Yang Wang},
  doi          = {10.1109/TMC.2025.3585007},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11972-11986},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MobiMixer: A multi-scale spatiotemporal mixing model for mobile traffic prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wi-GR: Wi-fi-based gait recognition using multi-part velocity profile. <em>TMC</em>, <em>24</em>(11), 11957-11971. (<a href='https://doi.org/10.1109/TMC.2025.3581549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with increasing user demands for convenience, privacy, and personalized experiences, gait recognition has been widely studied across various domains, such as indoor intrusion detection and smart homes. Although computer vision solutions are extensively researched for their visual intuitiveness, Wi-Fi sensing is emerging as a new research focus due to its ability to preserve privacy. However, previous studies have primarily relied on abstract features with limited interpretability or required multiple Wi-Fi links. To address these issues, we propose Wi-GR, which utilizes a Wi-Fi link to extract robust and highly interpretable gait features for user recognition. First, we construct a multi-path gait signal model to establish a clear relationship between Channel State Information (CSI) and gait motion. Then, we design a gait signal separation and enhancement method to mitigate the effects of external non-target reflections and internal multi-part reflections, which significantly impact the extraction and interpretability of gait features. Finally, fine-grained gait features that visualize gait patterns are generated using MUSIC-based and GAN-based multi-part velocity profile generation algorithms, tailored for single-person and multi-person scenarios, respectively. Numerous experiments have demonstrated that Wi-GR achieves single-person recognition accuracies of 95.3%, 94.0%, and 93.2% for 30 persons in the meeting room, corridor, and lobby, respectively, and an average accuracy of 88.3% for two-person recognition.},
  archive      = {J_TMC},
  author       = {Hao Chen and Penghao Wang and Jingyang Hu and Feng Li and Hongbo Jiang and Minglu Li and Chao Liu},
  doi          = {10.1109/TMC.2025.3581549},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11957-11971},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Wi-GR: Wi-fi-based gait recognition using multi-part velocity profile},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust intrusion detection system for vehicular networks: A federated learning approach based on representative client selection. <em>TMC</em>, <em>24</em>(11), 11942-11956. (<a href='https://doi.org/10.1109/TMC.2025.3582237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of network technology has allowed numerous vehicular applications to be deployed in vehicles, thereby enriching the driving experience of users. However, the openness of vehicular networks enables attackers to launch network attacks on vehicles through network ports, leading to the destruction of vehicular networks. To develop an intrusion detection system suitable for distributed vehicular networks, researchers have utilized federated learning to train detection models. Nevertheless, most federated learning-based vehicular intrusion detection systems seldom consider rapidly updating the detection model and fail to detect unknown attacks effectively. In this study, we propose a federated learning-based vehicular intrusion detection system that fully considers the traffic characteristics of multiple network regions and selects representative clients to participate in model aggregation, thereby accelerating the convergence of the global model. Furthermore, to enhance the robustness of the detection system, we utilize extreme value theory and multilayer activation vectors to construct an unknown attack discriminator that can determine whether a network flow is an unknown attack. Comprehensive experiments on three open datasets demonstrate that the proposed intrusion detection system can quickly update and effectively identify known/unknown attacks in open vehicular networks.},
  archive      = {J_TMC},
  author       = {Chunyang Fan and Jie Cui and Hulin Jin and Hong Zhong and Irina Bolodurina and Debiao He},
  doi          = {10.1109/TMC.2025.3582237},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11942-11956},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust intrusion detection system for vehicular networks: A federated learning approach based on representative client selection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic topology and resource allocation for distributed training in mobile edge computing. <em>TMC</em>, <em>24</em>(11), 11927-11941. (<a href='https://doi.org/10.1109/TMC.2025.3581510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing (MEC), edge servers and mobile terminals use federated learning distributed architecture to build a deep model, so that terminals can cooperate in training without sharing data. Distributed training requires network virtualization to provide high bandwidth and low latency characteristics to support large-scale parallel computing. Traditional virtual network embedding (VNE) relies on a static network topology, which lacks flexibility and incurs high resource costs during model training. To improve the efficiency of embedding distributed training tasks, we propose a novel Node Selection and Dynamic Topology resource allocation scheme for VNE of distributed training, NSDT-VNE, based on reconfigurable network topology. This algorithm divides the underlying network into static and dynamic topologies, enhancing low latency for small flows while providing high bandwidth for large flows as needed. Additionally, we introduce a two-phase coordinated alternating optimization algorithm that optimizes embedding decisions at both computational and topological levels, ensuring optimal node selection. Overall, NSDT-VNE follows demand-aware network design principles, allowing continuous optimization of the underlying topology. Compared to state-of-the-art heuristic and reinforcement learning-based virtual network algorithms, NSDT-VNE achieves superior performance, with request acceptance rates improving by 6.67% to 25.68% and embedding revenue increasing by approximately 7% to 32%.},
  archive      = {J_TMC},
  author       = {Weibei Fan and Donglai Wang and Fu Xiao and Yiping Zuo and Mengjie Lv and Lei Han and Sun-Yuan Hsieh},
  doi          = {10.1109/TMC.2025.3581510},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11927-11941},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic topology and resource allocation for distributed training in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FreshSpec: Sashimi freshness monitoring with low-cost multispectral devices. <em>TMC</em>, <em>24</em>(11), 11910-11926. (<a href='https://doi.org/10.1109/TMC.2025.3581714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring sashimi freshness, i.e., histamine levels, in showcases poses a critical challenge for sushi restaurants and fresh food stores. Current histamine monitoring methods involve labor-intensive chemical experiments or expensive devices, making affordable on-site monitoring difficult. This paper proposes FreshSpec, a low-cost and automatic spectral imaging system capable of precisely monitoring histamine levels in sashimi with minimal human intervention. The low concentration of histamine, combined with the potential for other ingredients to mask its spectral characteristics, complicates precise histamine level predictions using coarse or redundant spectral data from low-cost devices. To address this issue, FreshSpec employs an innovative feature-wise spectral reconstruction (SR) framework that effectively eliminates irrelevant and redundant data while preserving critical histamine-related spectral features. Specifically, we redefine the SR reconstruction target by utilizing features derived from the encoder of the spectral foundation model that is enhanced to focus on histamine-related spectral features. Furthermore, inspired by the monotonic accumulation properties of histamine over time, we propose a histamine regression model with unsupervised continual adaptation to new sashimi samples during practical deployment. Experimental results from 240 samples of salmon, tuna, and snapper demonstrate that FreshSpec achieves an R2 of 0.9319 and an RMSE of 3.101 mg/100 g, comparable to laboratory spectral imaging systems, while outperforming baseline schemes with a 46.95% RMSE reduction and a 0.1631 R2 improvement.},
  archive      = {J_TMC},
  author       = {Yinan Zhu and Haiyan Hu and Baichen Yang and Qianyi Huang and Qian Zhang and Wei Li},
  doi          = {10.1109/TMC.2025.3581714},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11910-11926},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FreshSpec: Sashimi freshness monitoring with low-cost multispectral devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AIGC-assisted federated learning for vehicular edge intelligence: Vehicle selection, resource allocation and model augmentation. <em>TMC</em>, <em>24</em>(11), 11896-11909. (<a href='https://doi.org/10.1109/TMC.2025.3581983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To leverage the vast amounts of onboard data while ensuring privacy and security, federated learning (FL) is emerging as a promising technology for supporting a wide range of vehicular applications. Although FL has great potential to improve the vehicular edge intelligence(VEI), challenges arise due to vehicle mobility, wireless channel instability, and data heterogeneity. To mitigate the issue of heterogeneous data across vehicles in FL, artificial intelligence-generated content (AIGC) can be employed as an innovative data synthesis technique to enhance FL model performance. In this paper, we propose AIGC-assisted Federated Learning for Vehicular Edge Intelligence (GenFV). We further propose a weighted policy using the Earth Mover’s Distance (EMD) to measure data distribution heterogeneity and introduce a convergence analysis for GenFV. Subsequently, we analyze system delay and formulate a mixed-integer nonlinear programming (MINLP) problem to minimize system delay. To solve this MINLP NP-hard problem, we propose a two-scale algorithm. At large communication scale, we implement label sharing and vehicle selection based on mobility and data heterogeneity. At the small computation scale, we optimally allocate bandwidth, transmission power and amount of generated data. Extensive experiments show that GenFV significantly improves the performance and robustness of FL in dynamic, resource-constrained environments, outperforming other schemes and confirming the effectiveness of our approach.},
  archive      = {J_TMC},
  author       = {Xianke Qiang and Zheng Chang and Geyong Min},
  doi          = {10.1109/TMC.2025.3581983},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11896-11909},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AIGC-assisted federated learning for vehicular edge intelligence: Vehicle selection, resource allocation and model augmentation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge computing underwater optical wireless sensor networks. <em>TMC</em>, <em>24</em>(11), 11879-11895. (<a href='https://doi.org/10.1109/TMC.2025.3581690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Optical Wireless Sensor Networks (UOWSNs) play important roles in resource exploration and maritime rescue. However, they face significant challenges in real-time data transmission due to the limited propagation range of optical signals (typically 10–100 m), frequent link disconnections caused by node mobility, and the extended distances to onshore servers. Traditional cloud computing solutions, designed for stable terrestrial networks with stationary edge servers and continuous connectivity, experience high latency (3-15 s) in UOWSNs, rendering them unsuitable for real-time applications in underwater environments. To address this issue, we propose a cloud-edge-end architecture tailored for UOWSNs, which can not only combat unique underwater environmental interference on link connection and topological changes but also guarantee robust and real-time communication. We develop a dynamic link-stability-based task offloading path selection (DLS-TOPS) algorithm for maximizing network resource profits. Afterward, we propose an online primal-dual task offloading (OPD-TO) algorithm for minimizing task completion time. Simulation results indicate that the proposed method significantly improves the real-time performance and resource profits of the network, reducing the total task completion time by more than 50% compared to baseline algorithms. We implemented a UOWSN with a cloud-edge-end architecture using commercial off-the-shelf and verified the applicability and effectiveness of the proposed scheme in emergency detection through testbed experiments.},
  archive      = {J_TMC},
  author       = {Yang Chi and Chi Lin and Jing Deng and Kaiwen Ning and Xin Fan and Guowei Wu},
  doi          = {10.1109/TMC.2025.3581690},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11879-11895},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge computing underwater optical wireless sensor networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OTFS-assisted ISAC system: Delay doppler channel estimation and SDR-based implementation. <em>TMC</em>, <em>24</em>(11), 11865-11878. (<a href='https://doi.org/10.1109/TMC.2025.3582421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal Time-Frequency Space (OTFS) modulation is an emerging technique that characterizes wireless channels and transmits information in the delay-Doppler domain. This work focuses on estimating fundamental sensing parameters, i.e., the delay and Doppler shifts of individual propagation paths, which serve as critical enablers for downstream positioning techniques, such as time-difference-of-arrival (TDOA)-based localization. Specifically, we propose a parameter-inherited (PI) channel estimation method that integrates sparse Bayesian learning (SBL) with unitary approximate message passing (UAMP), achieving low computational complexity and high estimation robustness. To accelerate the convergence of the UAMP-based iterative estimation, we explore the strategy of initializing parameters by inheriting prior estimates from adjacent OTFS transmission blocks. Furthermore, the overall computational burden is significantly reduced by employing large-scale matrix operations via two-dimensional fast Fourier transform (2D FFT). The proposed algorithms are implemented and evaluated on a software-defined radio (SDR)-based ISAC platform. Experimental results demonstrate that the proposed dual-functional system outperforms existing benchmarks in both communication quality and sensing parameter accuracy.},
  archive      = {J_TMC},
  author       = {Xinyuan Wei and Weijie Yuan and Kecheng Zhang and Fan Liu},
  doi          = {10.1109/TMC.2025.3582421},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11865-11878},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {OTFS-assisted ISAC system: Delay doppler channel estimation and SDR-based implementation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The price of forgetting: Incentive mechanism design for machine unlearning. <em>TMC</em>, <em>24</em>(11), 11852-11864. (<a href='https://doi.org/10.1109/TMC.2025.3582904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data protection policies (e.g., GDPR) enforce the right to be forgotten and require companies to perform machine unlearning once users request data removal. This process incurs costs for server and degrades model performance, impacting users’ satisfaction. In this paper, we propose the first incentive mechanism for machine unlearning, where server compensates users to retain their data. We characterize server’s major unlearning costs, accuracy degradation and consumed time, in data redemption amount through experiments on three datasets and two unlearning algorithms. We model server-users interaction as a two-stage Stackelberg game. In Stage I, server optimizes compensation unit prices to minimize costs. In Stage II, users jointly decide data redemption amounts as a non-cooperative game. By restricting the feasible set of Stage I to Nash Equilibrium of Stage II, we formulate a challenging non-convex bilevel optimization problem. We propose an iterative algorithm to compute optimal unit prices in Stage I and equilibrium data redemption amounts in Stage II by characterizing bilevel problem’s convexity. We prove the distributed convergence of best response updates to the unique Nash equilibrium by showing Stage II is a submodular game. Experimental results show that our mechanism minimizes server cost and maximizes social welfare over two practical baselines.},
  archive      = {J_TMC},
  author       = {Yue Cui and Man Hon Cheung},
  doi          = {10.1109/TMC.2025.3582904},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11852-11864},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {The price of forgetting: Incentive mechanism design for machine unlearning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV swarm-enabled collaborative post-disaster communications in low altitude economy via a two-stage optimization approach. <em>TMC</em>, <em>24</em>(11), 11833-11851. (<a href='https://doi.org/10.1109/TMC.2025.3583510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-altitude economy (LAE), as a new economic paradigm, plays an indispensable role in cargo transportation, healthcare, infrastructure inspection, and especially post-disaster communications. Specifically, uncrewed aerial vehicles (UAVs), as one of the core technologies of the LAE, can be deployed to provide communication coverage, facilitate data collection, and relay data for trapped users, thereby significantly enhancing the efficiency of post-disaster response efforts. However, conventional UAV self-organizing networks exhibit low reliability in long-range cases due to their limited onboard energy and transmit ability. Therefore, in this paper, we design an efficient and robust UAV-swarm enabled collaborative self-organizing network to facilitate post-disaster communications. Specifically, a ground device transmits data to UAV swarms, which then use collaborative beamforming (CB) technique to form virtual antenna arrays and relay the data to a remote access point (AP) efficiently. Then, we formulate a rescue-oriented post-disaster transmission rate maximization optimization problem (RPTRMOP), aimed at maximizing the transmission rate of the whole network. Given the challenges of solving the formulated RPTRMOP by using traditional algorithms, we propose a two-stage optimization approach to address it. In the first stage, the optimal multi-path traffic routing and the theoretical upper bound on the transmission rate of the network are derived. In the second stage, we transform the formulated RPTRMOP into a variant named V-RPTRMOP based on the obtained optimal multi-path traffic routing, aimed at rendering the actual transmission rate closely approaches its theoretical upper bound by optimizing the excitation current weight and the placement of each participating UAV via a diffusion model-enabled particle swarm optimization (DM-PSO) algorithm. Simulation results show the effectiveness of the proposed two-stage optimization approach in improving the transmission rate of the constructed network, which demonstrates the great potential for post-disaster communications. Moreover, the robustness of the constructed network is also validated via evaluating the impact of three unexpected situations on the system transmission rate.},
  archive      = {J_TMC},
  author       = {Xiaoya Zheng and Geng Sun and Jiahui Li and Jiacheng Wang and Qingqing Wu and Dusit Niyato and Abbas Jamalipour},
  doi          = {10.1109/TMC.2025.3583510},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11833-11851},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UAV swarm-enabled collaborative post-disaster communications in low altitude economy via a two-stage optimization approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A user-centric energy-saving method for dynamic 5G heterogeneous networks using deep reinforcement learning. <em>TMC</em>, <em>24</em>(11), 11820-11832. (<a href='https://doi.org/10.1109/TMC.2025.3582623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy consumption (EC) represents a significant challenge for 5G and 6G mobile networks, necessitating a primary focus on optimizing energy savings (ES). This paper illustrates the practical benefits of a user-centric deep reinforcement learning (DRL) models in achieving a green cellular network. The primary objective is to optimize energy usage in a heterogeneous network (HetNet). The optimization of power consumption (PC) in such networks is a non-convex and NP-hard problem. To address this challenge, we propose using reinforcement learning (RL). Due to the extensive state and action space, classical RL approaches are unsuitable. Therefore, the adoption of DRL methods, notably the deep Q-network (DQN) and deep deterministic policy gradient (DDPG) methods, is necessary. The proposed approach entails a user-centric connection establishment, whereby small base stations (SBSs) are switched to an on mode. The mode switching determined by the DRL methods is controlled by an anti-abrupt transition mechanism, which prevents unnecessary oscillations in the network. The results are benchmarked against existing approaches, specifically genetic algorithm (GA) and particle swarm optimization (PSO) for ES. The proposed methods outperform both GA and PSO optimization techniques in terms of ES and significantly reduce time consumption, enhancing its practical implementation feasibility.},
  archive      = {J_TMC},
  author       = {Mohammad Ali Arami and Erfan Rasti and Abbas Mohammadi},
  doi          = {10.1109/TMC.2025.3582623},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11820-11832},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A user-centric energy-saving method for dynamic 5G heterogeneous networks using deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint power allocation and phase shifts design for distributed RIS-assisted multiuser systems. <em>TMC</em>, <em>24</em>(11), 11808-11819. (<a href='https://doi.org/10.1109/TMC.2025.3582750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed reconfigurable intelligent surfaces (RISs) provide rich macro-diversity coverage due to different locations of the RISs, which is beneficial to combat coverage holes. However, the system performance relies on the effective coordination of multiple RISs. In particular, distributed RIS-assisted power allocation and the phase shifts of RISs should be jointly designed under nonlinear scheduling constraints. Thus, the resource allocation scheme for distributed RIS-assisted multiuser system is a crucial challenge. To tackle these issues, joint power allocation, phase shifts and communication scheduling design for distributed RIS-assisted systems is investigated in this paper, where all RISs simultaneously and cooperatively serve multiple users. To overcome the formulated nonconvex optimization problem, the original problem is decoupled into three subproblems and solved in an iterative manner. Specifically, we first consider the subproblem of power allocation, which can be solved via maximizing the ergodic achievable rate. By applying the ergodic rate, an approximate closed-form solution is formed for the power allocation. Subsequently, the phase shifts are optimized using the minimization-maximization optimization methods. Finally, a communication scheduling scheme is presented to address the scheduling variables. Numerical simulations are conducted to demonstrate that the considered solution outperforms the existing benchmark and achieves a near-optimal spectral efficiency.},
  archive      = {J_TMC},
  author       = {Zhen Chen and Gaojie Chen and Xiu Yin Zhang and Jie Tang and Shi Jin and Kai-Kit Wong and Jonathon Chambers},
  doi          = {10.1109/TMC.2025.3582750},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11808-11819},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint power allocation and phase shifts design for distributed RIS-assisted multiuser systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LOCA: Long-term optimization based on chunk-level analysis in edge-assisted massive mobile live streaming. <em>TMC</em>, <em>24</em>(11), 11793-11807. (<a href='https://doi.org/10.1109/TMC.2025.3581922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an edge-assisted massive mobile live streaming (MMLS) framework named LOCA, integrating chunk-level analysis and long-term optimization to design resource allocation, bitrate adaptation, and source selection strategies. The proposed method ensures sustained real-time video delivery while minimizing latency and communication costs. First, a chunk-level analysis of the entire process of video streaming is introduced, aiming at modeling fetch queue waiting time and rebuffering duration in each time slot. By embedding this mathamatical model into consideration, a long-term optimization is formulated to minimize rebuffering and communication overhead while maintaining high video qualities for massive users. Leveraging Lyapunov optimization, we transform this problem into a computationally tractable form. Further simplification via linearization achieves near-optimal solutions by adopting the mixed-integer linear programming method with enhanced computational efficiency. Simulation results demonstrate superior stability and long-term performance compared to the state-of-the-art and baseline methods, validating the framework’s efficacy in MMLS scenarios.},
  archive      = {J_TMC},
  author       = {Fangzheng Feng and Yu Zhang and Xinkun Zheng and Ting Bi and Tao Jiang},
  doi          = {10.1109/TMC.2025.3581922},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11793-11807},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LOCA: Long-term optimization based on chunk-level analysis in edge-assisted massive mobile live streaming},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiSec-RTF: Reinforcing RPL resilience against routing table falsification attack in 6LoWPAN. <em>TMC</em>, <em>24</em>(11), 11779-11792. (<a href='https://doi.org/10.1109/TMC.2025.3581561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routing Protocol for Low-Power and Lossy Networks (RPL) is an energy-efficient routing solution for IPv6 over Low-Power Wireless Personal Area Networks (6LoWPAN), recommended for resource-constrained devices. While RPL offers significant benefits, its security vulnerabilities pose challenges, particularly due to unauthenticated control messages used to establish and maintain routing information. These messages are susceptible to manipulation, enabling malicious nodes to inject false routing data. A notable security concern is the Routing Table Falsification (RTF) attack, where attackers forge Destination Advertisement Object (DAO) messages to promote fake routes via a parent node’s routing table. Experimental results indicate that RTF attacks significantly reduce packet delivery ratio, increase end-to-end delay, and leverage power consumption. Currently, no effective countermeasures exist in the literature, reinforcing the need for a security solution to prevent network disruption and protect user applications. This paper introduces a Lightweight Security Solution against Routing Table Falsification Attack (LiSec-RTF), leveraging Physical Unclonable Functions (PUFs) to generate unique authentication codes, termed “Licenses.” LiSec-RTF mitigates RTF attack impact while considering the resource limitations of 6LoWPAN devices in both static and mobile scenarios. Our testbed experiments indicate that LiSec-RTF significantly improves network performance compared to standard RPL under RTF attacks, thereby ensuring reliable and efficient operation.},
  archive      = {J_TMC},
  author       = {Shefali Goel and Vinod Kumar Jain and Abhishek Verma},
  doi          = {10.1109/TMC.2025.3581561},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11779-11792},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LiSec-RTF: Reinforcing RPL resilience against routing table falsification attack in 6LoWPAN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint dynamic VNF placement and delay and jitter aware multicast routing in NFV-enabled SDNs. <em>TMC</em>, <em>24</em>(11), 11764-11778. (<a href='https://doi.org/10.1109/TMC.2025.3581905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For reliability, security and scalability, Multicast Request flows (MRs) need to traverse a Service Function Chain (SFC) that consists of series of Virtual Network Functions (VNFs) such as firewalls, encoder-decoder in Network Function Virtualization-enabled Software-Defined Networks (NFV-enabled SDNs). There are typically multiple identical VNF-instances in the network, it brings significant challenges when dynamically choosing or placing the requisite VNF-instances to construct a Service Function Tree (SFT) consisting of SFCs for fulfilling the MRs’s routing. This paper investigates the Delay and Jitter Aware Dynamic SFT Embedding and Routing Problem (DJA-DSERP) considering VNF placement, network resources, delay and jitter constraints as well as network load balance in NFV-Enabled SDNs. First, we formulate DJA-DSERP as an integer linear programming model and prove it to be NP-hard. Then, an auxiliary edge-weighted graph and an Optimal Link Selection Function (OLSF) are devised, and SFT Embedding Algorithm (SFT-EA) is proposed to address the problem aiming at minimizing the resource consumption costs while satisfying multiple QoS constraints and network load balance. Furthermore, we theoretically prove the effectiveness of the OLSF and the SFT-EA. Simulation results demonstrate that the SFT-EA exhibits superior performance compared to existing algorithms in terms of throughput, traffic acceptance rate, and network load balance.},
  archive      = {J_TMC},
  author       = {Liang Liu and Siyuan Tan and Songtao Guo and Guiyan Liu and Hao Feng},
  doi          = {10.1109/TMC.2025.3581905},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11764-11778},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint dynamic VNF placement and delay and jitter aware multicast routing in NFV-enabled SDNs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile phone-based digital biomarkers empowered by knowledge distillation for diagnosis of parkinson’s disease. <em>TMC</em>, <em>24</em>(11), 11748-11763. (<a href='https://doi.org/10.1109/TMC.2025.3581525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile phones have evolved from basic communication tools to feature-rich mobile devices. These ubiquitous and portable devices, equipped with inertial sensors and high-speed network access, create opportunities for remote health monitoring, especially for movement disorders such as Parkinson’s disease (PD). Inertial sensors (gyroscopes and accelerometers) endow smartphones with a natural ability to monitor movement disorders. Based on this, we develop a novel vision-based time-series feature augmentation framework for remote diagnosis and severity grading of PD using mobile phone walking records. Specifically, preprocessed time-series data is encoded into RGB images for the teacher model, while the time-series data is input into the student model, with the teacher guiding the student’s learning. The teacher model is based on MobileNetV2 and incorporates spatial and channel relation-aware attention mechanisms to capture important features and filter out irrelevant information. The inter-modal feature fusion module combines attention and CNN to emphasize both global and local features. The student model utilizes a simple CNN to directly extract features from time-series data and perform classification. For the three-level classification task, the teacher model achieves accuracies of 0.887, 0.886, and 0.896 across the three datasets, while the distillation student model reaches 0.779, 0.828, and 0.827, generally surpassing state-of-the-art algorithms.},
  archive      = {J_TMC},
  author       = {Tongyue He and Junxin Chen and Chi Lin and Wei Wang},
  doi          = {10.1109/TMC.2025.3581525},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11748-11763},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobile phone-based digital biomarkers empowered by knowledge distillation for diagnosis of parkinson’s disease},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-enhanced healthcare monitoring service refreshment in human digital twin-assisted fabric metaverse. <em>TMC</em>, <em>24</em>(11), 11731-11747. (<a href='https://doi.org/10.1109/TMC.2025.3582084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human digital twin bridges humans with digital avatars in the fabric metaverse, assisting users and healthcare professionals with real-time visualization, analysis, and prediction of personal data sensed by fabric sensors. The human digital twin-assisted healthcare monitoring (HHM) service refreshment refers to sending personal health data to corresponding services hosted on nearby edge servers and receiving the results to update local digital avatars continuously. However, the malicious nature and resource limitations of edge servers may lead to user privacy leaks and refreshment timeout, thereby impacting diagnostics. In this paper, we investigate a novel privacy-enhanced HHM service refreshment maximization problem in the fabric metaverse by considering privacy data encryption, model compression, and personalized user requirements. To this end, we first formulate the above issue as an Integer Linear Programming (ILP) problem, and prove its NP-hardness. Then, a resource scheduler named Wiper is designed, consisting of a shallow-deep distiller and an agile refresher library. To enable efficient inference while preserving user privacy, the former replaces violation modules in existing models with approximations and conducts shallow distillation on model layers to meet operation type and depth limits of homomorphic encryption, and then deep distillation on model parameters to decrease end-to-end refreshment delay. Finally, to satisfy user requirements on accuracy and delay during encrypted refreshments while maximizing the throughput of HHM services in offline and online situations with different problem scales, a series of HHM service refreshment algorithms are merged into the latter, including exact, performance-guaranteed approximation, and residual diffusion reinforcement learning algorithms. Theoretical analyses and experiments demonstrate that our algorithms are promising compared with baseline algorithms.},
  archive      = {J_TMC},
  author       = {Yu Qiu and Min Chen and Weifa Liang and Lejun Ai and Dusit Niyato and Gang Wei},
  doi          = {10.1109/TMC.2025.3582084},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11731-11747},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-enhanced healthcare monitoring service refreshment in human digital twin-assisted fabric metaverse},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From non-IID to IID: Mobility-aware hierarchical federated learning with client-edge association control. <em>TMC</em>, <em>24</em>(11), 11717-11730. (<a href='https://doi.org/10.1109/TMC.2025.3585538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying federated learning (FL) in wireless network with hierarchical client-edge-cloud architecture enables large-scale distribution collaboration without long-distance communication latency. However, the ongoing edge dynamics with uncertain client mobility and imbalanced data distributions, poses great challenge for collaboration efficiency of FL. In this work, we first model the client mobility with a Markov chain, and formulate the minimization of performance degradation as a client-edge association control problem. With the analysis of client mobility patterns, we propose ALPHA, a new client-edge association control framework for mobility-aware FL, to reshape the edge-level data distributions close to i.i.d in both offline and online mobility scenarios. In the offline scenario with deterministic client mobility trajectories, we leverage alternating optimization theory to transform the client-edge association control problem into a weighted bipartite b-matching problem, and derive an efficient solution with linear relaxation and dependent rounding techniques. As for the online scenario, where each client arrives at different edge access points (APs) in an online manner, we design a fast and simple online subgradient projection algorithm with a bounded regret to make an online decision on client-edge association. Extensive experiment results on three public datasets and a real-world mobility trajectory dataset show that ALPHA has a superior learning performance with 1.40× – 2.89× convergence speedup compared to state-of-the-art solutions.},
  archive      = {J_TMC},
  author       = {Haibo Liu and Zhenzhe Zheng and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3585538},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11717-11730},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {From non-IID to IID: Mobility-aware hierarchical federated learning with client-edge association control},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IFresher: Information freshening for mobile augmented reality with multi-agent reinforcement learning in edge computing. <em>TMC</em>, <em>24</em>(11), 11703-11716. (<a href='https://doi.org/10.1109/TMC.2025.3581523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the IFresher framework to improve the timeliness of multi-agent mobile augmented reality (MAR) systems. Existing works have made strides in accuracy-latency trade-offs, but fail to directly address real-time task responsiveness and multi-agent contention challenges. To bridge this gap, we introduce the concept of the age of analytics information (AoAI), which quantifies the combined impact of video analytics (VA) accuracy, transmission delay, and computational efficiency. By deriving a closed-form expression for AoAI, IFresher establishes a central control mechanism that jointly optimizes bandwidth allocation and video configuration to minimize AoAI while ensuring accuracy. Due to the mixed-integer nonlinear characteristics of the problem and the fact that each agent only has local observations, the problem is reformulated into a decentralized partially observable Markov decision process (Dec-POMDP). We propose a multi-agent reinforcement learning (MARL) algorithm, named convex-embedded transformer QMIX (CTQMIX), using the centralized training and decentralized execution (CTDE) framework for agent collaboration. Specifically, the convex optimization ensures optimal bandwidth distribution, and the transformer captures temporal dependencies between observations and actions across time steps to improve decision-making in dynamic environments. Evaluations with real-world experiments show that the CTQMIX outperforms state-of-the-art (SOTA) algorithms.},
  archive      = {J_TMC},
  author       = {Shuang Cheng and Zhaoyang Wang and Fangzheng Feng and Yu Zhang and Ting Bi and Tao Jiang},
  doi          = {10.1109/TMC.2025.3581523},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11703-11716},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {IFresher: Information freshening for mobile augmented reality with multi-agent reinforcement learning in edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CADTR: Context-aware trust routing algorithm based on priority sampling DDPG for UASNs. <em>TMC</em>, <em>24</em>(11), 11688-11702. (<a href='https://doi.org/10.1109/TMC.2025.3581512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The underwater acoustic sensor network (UASN) is a pivotal paradigm within the underwater Internet of Things, where multi-hop forwarding-based underwater data routing is essential for information acquisition. However, the dynamic nature of underwater network topology and the instability of underwater acoustic communication pose significant challenges to achieving efficient and reliable data transmission. In light of unreliable underwater environments and potential malicious attacks, studying trusted routing strategies for UASNs is crucial. This study introduces a context-aware trust routing scheme (CADTR) based on deep reinforcement learning (DRL), which integrates real-time environmental state perception with AI-driven routing decisions, thereby enhancing the reliability and robustness of data routing in dynamic and potentially hostile underwater scenarios. First, a unified trust evidence framework is developed to strengthen the support of evidence experience for subsequent trust decisions by mapping multi-dimensional trust evidence to a unified scale. This framework is tightly coupled with the DRL agent, allowing the agent to evaluate and update trust levels based on real-time evidence. Second, a dynamic topology perception model and an underwater acoustic communication perception model are constructed to enable real-time perception of the interactive experience context. These models provide continuous input to the DRL agent, enabling it to adapt to topological changes and communication conditions dynamically. This facilitates priority experience sampling during the training process of the routing decision model, indirectly boosting model training efficiency and decision accuracy. Finally, the DRL agent learns optimal routing policies by interacting with the environment, leveraging the trust evidence and perception models to make informed decisions. Experimental results demonstrate that the proposed CADTR algorithm significantly improves the overall performance of the routing strategy in terms of packet delivery rate, energy utilization efficiency, and data transmission delay compared to the benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Yu He and Guangjie Han and Jinfang Jiang and Xin Cheng and Pengfei Xu},
  doi          = {10.1109/TMC.2025.3581512},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11688-11702},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CADTR: Context-aware trust routing algorithm based on priority sampling DDPG for UASNs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AoI-aware air-ground mobile crowdsensing by multi-agent curriculum learning with collaborative observation augmentation. <em>TMC</em>, <em>24</em>(11), 11675-11687. (<a href='https://doi.org/10.1109/TMC.2025.3583499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By harnessing the capabilities of unmanned aerial and ground vehicles (UAVs and UGVs), equipped with high-precision sensors, air-ground mobile crowdsensing (AG-MCS) has proven to be effective for data collection in urban environments. In this paper, by optimizing the metric of age-of-information (AoI) that measures the freshness of collected data, we consider the problem of AoI-Aware AG-MCS (A3G-MCS), where UGVs dispatch UAVs from multiple UGV stops to collect data from point-of-interests (PoIs). We propose a novel multi-agent curriculum learning framework called “MACL(MCS)”, that explicitly balances the individual and team goals of both UAV/UGV controllers to facilitate the exploration of policy towards globally-optimal performance. It is further enhanced by a UAV/UGV collaborative observation augmentation (COA) module for improved inter-controller communication. Extensive results reveal that MACL(MCS) consistently outperforms five baselines, and achieves comparable performance to exact method with better scalability and efficiency. It also showcases strong generalization capability towards real-world scenarios on both TSPLIB and Purdue, KAIST and NCSU datasets.},
  archive      = {J_TMC},
  author       = {Yuxiao Ye and Yuxuan Tian and Chi Harold Liu and Linkang Dong and Guangpeng Qi and Dapeng Wu},
  doi          = {10.1109/TMC.2025.3583499},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11675-11687},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AoI-aware air-ground mobile crowdsensing by multi-agent curriculum learning with collaborative observation augmentation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly detection and localization in NFV systems by utilizing masked-autoencoder and XAI. <em>TMC</em>, <em>24</em>(11), 11657-11674. (<a href='https://doi.org/10.1109/TMC.2025.3582195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Network Functions Virtualization (NFV) systems into mobile edge and core networks has heightened the need for effective anomaly detection and localization methods. The complexity of NFV demands robust mechanisms for network resilience, security, and performance. Machine Learning approaches have demonstrated promising solutions in crafting adaptive and efficient mechanisms for detecting and localizing potential anomalies within NFV systems. Particularly, Unsupervised Learning (UL) methods have garnered significant attention for their potential to detect anomalies without the need for labeled data. However, UL methods are susceptible to even minor levels of anomalous samples in the training data, termed contamination, which can severely compromise their performance. This paper proposes a novel approach using the Noisy-Student technique for anomaly detection. It addresses data contamination by combining a density-estimation teacher model for pseudo-labeling with a weakly-supervised student model based on a Masked Autoencoder trained on the pseudo-labeled data. For anomaly localization, we introduce a heuristic tailored for our anomaly detection model and two Explainable Artificial Intelligence (XAI)-based approaches applicable to any detection model. Extensive experiments on three NFV datasets demonstrate superior performance, with up to a 20% improvement in anomaly detection and up to a 22% improvement in localization, in terms of F1-score.},
  archive      = {J_TMC},
  author       = {Seyed Soheil Johari and Nashid Shahriar and Massimo Tornatore and Raouf Boutaba and Aladdin Saleh},
  doi          = {10.1109/TMC.2025.3582195},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11657-11674},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Anomaly detection and localization in NFV systems by utilizing masked-autoencoder and XAI},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative edge and cloud computing: Optimal configuration and computation management. <em>TMC</em>, <em>24</em>(11), 11641-11656. (<a href='https://doi.org/10.1109/TMC.2025.3584524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) plays an increasingly important role in the rapidly increasing mobile applications by providing high-quality computing services. The majority of current research has focused on designing efficient computing task offloading schemes to ensure the effectiveness of the MEC system. However, the configuration and resource management of the MEC system, which are crucial for its scattered features, have not received due attention. This paper investigates the configuration and computation resource management problem for the MEC system by formulating a profit maximization problem. To address this problem, we first analyze the relationship among mobile users’ offloading decisions, the configuration and computation management of the MEC system, and the service quality. Then, we design an optimal configuration and computation management scheme for the MEC system, which can not only maintain the efficiency of computing processes but also make a good trade-off between profitability and service quality. In such a way, the total expected profit of the MEC system can be maximized. Numerical evaluations show that the proposed optimal configuration and computation management scheme can efficiently improve the total profit of the MEC system.},
  archive      = {J_TMC},
  author       = {Yongmin Zhang and Wei Wang and Rui Huang and Junfan Zhou and Yang Xu and Ju Ren and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3584524},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11641-11656},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative edge and cloud computing: Optimal configuration and computation management},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized AR content and POI recommendation in mobile cultural heritage systems using semantic relatedness. <em>TMC</em>, <em>24</em>(11), 11628-11640. (<a href='https://doi.org/10.1109/TMC.2025.3587655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a mobile augmented reality (AR) system designed to enhance user experiences at cultural heritage (CH) sites by providing personalized content and points of interest (POI) recommendations. Applying semantic relatedness, we address the heterogeneity of CH POIs to improve personalization accuracy. Our system constructs a POI-based co-occurrence graph to model semantic relationships, enriching users’ visited AR content items for better recommendations. The proposed method integrates content-filtering-based recommendations with this graph and recommends personalized POIs and AR content items. A user study demonstrated that our system outperforms conventional methods by 9.4% in recommendation precision and recall, significantly enhancing user engagement and attention focus during CH tours.},
  archive      = {J_TMC},
  author       = {Maryam Shakeri and Abolghasem Sadeghi-Niaraki and Jens Grubert and Soo-Mi Choi},
  doi          = {10.1109/TMC.2025.3587655},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11628-11640},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Personalized AR content and POI recommendation in mobile cultural heritage systems using semantic relatedness},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling state shifting via local-global distillation for event-frame gaze tracking. <em>TMC</em>, <em>24</em>(11), 11614-11627. (<a href='https://doi.org/10.1109/TMC.2025.3581317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the problem of passive gaze estimation using both event and frame (or 2D image) data. Considering the inherently different physiological structures, it is intractable to accurately estimate gaze purely based on a given state. Thus, we reformulate gaze estimation as the quantification of the state shifting from the current state to several prior registered anchor states. Specifically, we propose a two-stage learning-based gaze estimation framework that divides the whole gaze estimation process into a coarse-to-fine approach involving anchor state selection and final gaze location. Moreover, to improve the generalization ability, instead of learning a large gaze estimation network directly, we align a group of local experts with a student network, where a novel denoising distillation algorithm is introduced to utilize denoising diffusion techniques to iteratively remove inherent noise in event data. Extensive experiments demonstrate the effectiveness of the proposed method, which surpasses state-of-the-art methods by a large margin of 15$\%$.},
  archive      = {J_TMC},
  author       = {Zhiyu Zhu and Jinhui Hou and Jiading Li and Jinjian Wu and Junhui Hou},
  doi          = {10.1109/TMC.2025.3581317},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11614-11627},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Modeling state shifting via local-global distillation for event-frame gaze tracking},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-knowledge neighbor discovery for underwater optical wireless sensor networks. <em>TMC</em>, <em>24</em>(11), 11596-11613. (<a href='https://doi.org/10.1109/TMC.2025.3581371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighbor discovery poses significant challenges in Underwater Optical Wireless Sensor Networks (UOWSNs) due to the unique characteristics of directional transceivers, line-of-sight communication, and mobility induced by water currents. Traditional methods typically rely on prerequisites and prior knowledge, such as centralized coordination, time synchronization, and neighbor-related information, which are often unavailable or impractical in underwater environments. In this paper, we make the first attempt to address the issue of Robust and Efficient Neighbor Discovery (termed the REND problem) in UOWSNs with zero-knowledge. Here, zero-knowledge refers to the capability that enables sensors to identify neighbors in dynamic underwater optical channel conditions without prerequisites or prior knowledge. We design a zero-knowledge distributed directional neighbor discovery scheme inspired by gear meshing. We then propose a deterministic algorithm for the REND problem based on theoretical analysis. Additionally, to further reduce the discovery delay for the periodic REND problem, we develop a greedy-based approximation algorithm with a performance guarantee. Finally, extensive simulations demonstrate that the proposed scheme reduces the discovery delay by 34.9% on average and achieves an additional 54.4% reduction for periodic neighbor discovery. Furthermore, test-bed experiments are carried out to verify the applicability of our zero-knowledge scheme in real-world scenarios.},
  archive      = {J_TMC},
  author       = {Yu Tian and Lei Wang and Chi Lin and Bin Han and Lupeng Zhang and Zhiyi Zhou and Yu Sun and Bingxian Lu},
  doi          = {10.1109/TMC.2025.3581371},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11596-11613},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Zero-knowledge neighbor discovery for underwater optical wireless sensor networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and asynchronous multi-node cooperative vehicular fog computing enhanced IoV. <em>TMC</em>, <em>24</em>(11), 11582-11595. (<a href='https://doi.org/10.1109/TMC.2025.3581397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vehicular Fog Computing (VFC) provides low-latency computing service to support emerging intelligent transportation applications in Internet of Vehicles (IoV). Multi-node cooperative VFC can utilize Connected and Autonomous Vehicles (CAVs) to implement cooperative intelligence. Due to the mobility of vehicles, service migration is necessary when task offloading service providers change. This paper proposes an Asynchronous Task Offloading Scheme (ATO-S) that allows each CAV to choose an independent optimization period and provides robust task offloading services under unknown vehicle mobility probability distribution. To the best of our knowledge, this is the first work to investigate asynchronous and robust multi-node cooperative task offloading in dynamic VFC-enhanced IoV scenarios. Furthermore, we formulate the long-term energy consumption minimization problem of VFC and transfer it into each time slot problem by Lyapunov optimization. Then we design Asynchronous Task Offloading Algorithm (ATO-A) to jointly optimizing CAVs matching, communication and computation resource allocation, and transmission power based on multiple mathematical techniques and hybrid heuristic algorithm. Extensive simulations based on real-world traffic scenario are conducted by varying multiple crucial parameters. Simulation results demonstrate the energy efficiency and task queue stability achieved by ATO-A, and service robustness achieved by ATO-S, in comparison with benchmark solutions.},
  archive      = {J_TMC},
  author       = {Yifeng Zhao and Chenyi Liang and Zhibin Gao and Lianfen Huang},
  doi          = {10.1109/TMC.2025.3581397},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11582-11595},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust and asynchronous multi-node cooperative vehicular fog computing enhanced IoV},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RAPOO: An efficient privacy-preserving facial expression recognition via mobile crowdsensing. <em>TMC</em>, <em>24</em>(11), 11568-11581. (<a href='https://doi.org/10.1109/TMC.2025.3581687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition is a technology that involves analyzing and interpreting human facial expressions to determine individual expressions or states. Mobile crowdsensing (MCS), a promising sensing paradigm, makes it easy to capture facial images and benefits facial expression recognition. Existing inference models for facial expression recognition usually rely on facial feature vectors or facial images, increasing privacy concerns about expression. For this reason, this paper proposes a privacy-preserving facial expression recognition scheme through MCS, named RAPOO, which falls in a client-server architecture. Roughly speaking, a user captures facial images using mobile devices and requests a recognition service provided by a cloud computing center. To protect the privacy of expressions, our approach focuses on designing secure computation protocols required by facial expression recognition necessarily, such as secure vector distance calculation and secure top-$k$ query. These protocols enable facial expression recognition over encrypted data directly. To speed up the recognition and store encrypted feature vectors, a $k$-D tree data structure is introduced. The security analysis confirms that RAPOO effectively preserves the confidentiality of personal expressions. Extensive experimental evaluations show that our solution obtains a three-order-of-magnitude speedup in terms of computational overhead compared with the state-of-the-art.},
  archive      = {J_TMC},
  author       = {Bo Tian and Bowen Zhao and Yang Xiao and Yang Liu and Qingqi Pei and Yulong Shen},
  doi          = {10.1109/TMC.2025.3581687},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11568-11581},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RAPOO: An efficient privacy-preserving facial expression recognition via mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-CoSen: Revisiting collaborative sensing with large language models (LLMs). <em>TMC</em>, <em>24</em>(11), 11555-11567. (<a href='https://doi.org/10.1109/TMC.2025.3583345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative sensing has emerged as a novel sensing paradigm, entailing multi-sensor data sharing and multimodal modeling to collaboratively understand sensing behaviors. However, current solutions, i.e., data-level and decision-level fusion methods, fall short of generality, expert knowledge, and holistic/chronic perspective. In this paper, we propose LLM-CoSen to revisit collaborative sensing with Large Language Models (LLMs). Specifically, LLM-CoSen designs a semantic-level fusion approach for inference results for collaborative sensing. Such an approach is characterized by its generality, making it applicable to any heterogeneous devices, and its expert knowledge incorporation, which provides chronic, holistic, and insightful perspectives on the inference results. Regarding inference absence challenges, we propose a personalized model design method to constrain inference time, and a voting-based two-pass prompt engineering strategy for token completion. Regarding inference error challenges, we propose an accuracy restoration strategy for personalized models, and a two-level error estimator coupled with self-correction. Experimental results of human digital system use case on four corresponding benchmark datasets show LLM-CoSen can decrease inference absence by 72.83% and inference errors by 7.65% on average.},
  archive      = {J_TMC},
  author       = {Xingyu Feng and Zehua Sun and Zhuangzhuang Chen and Chengwen Luo and Zhangbing Zhou and Victor C.M. Leung and Weitao Xu},
  doi          = {10.1109/TMC.2025.3583345},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11555-11567},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LLM-CoSen: Revisiting collaborative sensing with large language models (LLMs)},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHandover: Parallel handover in mobile satellite network. <em>TMC</em>, <em>24</em>(11), 11541-11554. (<a href='https://doi.org/10.1109/TMC.2025.3582245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of Low Earth Orbit satellite constellations has recently spurred tremendous attention from both academia and industry. 5G and 6G standards have specified the LEO satellite network as a key component of the mobile network. However, due to the satellites’ fast traveling speed, ground terminals usually experience frequent and high-latency handover, which significantly deteriorates the performance of latency-sensitive applications. To address this challenge, we propose a parallel handover mechanism for the mobile satellite network which can considerably reduce the handover latency. The main idea is to use plan-based handovers instead of measurement-based handovers to avoid interactions between the access and core networks, hence eliminating the significant time overhead in the traditional handover procedure. Specifically, we introduce a novel network function named Satellite Synchronized Function (SSF), which is designed for being compliant with the standard 5G core network. Moreover, we propose a machine learning model for signal strength prediction, coupled with an efficient handover scheduling algorithm. We have conducted extensive experiments and results demonstrate that our proposed handover scheme can considerably reduce the handover latency by 21× compared to the standard NTN handover scheme and two other existing handover schemes, along with significant improvements in network stability and user-level performance.},
  archive      = {J_TMC},
  author       = {Jiasheng Wu and Shaojie Su and Wenjun Zhu and Xiong Wang and Jingjing Zhang and Xingqiu He and Yue Gao},
  doi          = {10.1109/TMC.2025.3582245},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11541-11554},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PHandover: Parallel handover in mobile satellite network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parasitic communication: Opportunistic utilization of interference using asymmetric demodulation. <em>TMC</em>, <em>24</em>(11), 11527-11540. (<a href='https://doi.org/10.1109/TMC.2025.3581798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of wireless communication technologies, interference has become a key impediment to the improvement of wireless data transmission performance. Traditional interference management (IM) suppresses or adjusts interference at the cost of additional communication resources without exploiting interference effectively. Moreover, wirelessly transmitted data is susceptible to eavesdropping. To address these issues cooperatively, we propose Opportunistic Parasitic Communication with Asymmetric Demodulation(OPC-AD). In particular, we consider the interference experienced by the intended/target communication (i.e., parasitic) receiver (Rx) as the host signal. The target communication constructs a selection signal carrying parasitic indication information based on the data it intends to send and the data decoded by its Rx using asymmetric demodulation from the host signal, and then sends it to its Rx. This signal is used to instruct the parasitic Rx to extract the desired information from the host signal. OPC-AD allows for the exploitation of the interference (i.e., host signal) for data transmission to an interfered Rx. Using AD can also ensure the privacy of the host communication. Since the parasitic communication is concealed within the host signal, eavesdroppers cannot compromise the confidentiality of the parasitic transmission without precisely decoding the selection signal. Furthermore, considering more practical situations, we extend the OPC-AD design to cover a broader range of realistic scenarios. Our experimental results validate the applicability of OPC-AD, while our in-depth simulations demonstrate that parasitic communication can effectively thwart eavesdropping and achieve higher spectral efficiency (SE) than other existing IM methods, particularly in strong interference environments.},
  archive      = {J_TMC},
  author       = {Zhao Li and Lijuan Zhang and Kang G. Shin and Jia Liu and Yicheng Liu and Pintian Lyu and Zheng Yan},
  doi          = {10.1109/TMC.2025.3581798},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11527-11540},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Parasitic communication: Opportunistic utilization of interference using asymmetric demodulation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconsidering sparse sensing techniques for channel sounding using splicing. <em>TMC</em>, <em>24</em>(11), 11511-11526. (<a href='https://doi.org/10.1109/TMC.2025.3581446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-band splicing offers a promising solution to extend existing band-limited communication systems to support high-precision sensing applications. This technique involves performing narrow-band measurements at multiple center frequencies, which are then combined to effectively increase the bandwidth without changing the sampling rate. In this paper, we introduce a mmWave channel sounder based on multi-band splicing, leveraging the sparse nature of wireless channels through compressed sensing and sparse recovery techniques for channel reconstruction. We focus on three sparse recovery methods: the widely used grid-based orthogonal matching pursuit (OMP) algorithm as a baseline, our newly developed two-stage mmSplicer algorithm, which extends the OMP method by introducing an additional stage for improving its performance for our application, and our adaptation of sparse reconstruction by separable approximation (SpaRSA), named Net-SpaRSA, optimized for wireless applications. All three algorithms are integrated into an experimental OFDM-based IEEE 802.11ac system. Our analysis centers on evaluating the performance of these algorithms under limited number of narrow-band measurements, demonstrating that accurate CIR estimation is achievable even using only 50% of the full wideband spectrum. Additionally, we analyze and compare the computational complexity of these algorithms to assess their practical feasibility.},
  archive      = {J_TMC},
  author       = {Sigrid Dimce and Anatolij Zubow and Alireza Bayesteh and Giuseppe Caire and Falko Dressler},
  doi          = {10.1109/TMC.2025.3581446},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11511-11526},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reconsidering sparse sensing techniques for channel sounding using splicing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embodied AI-enhanced vehicular networks: An integrated vision language models and reinforcement learning method. <em>TMC</em>, <em>24</em>(11), 11494-11510. (<a href='https://doi.org/10.1109/TMC.2025.3582864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates adaptive transmission strategies in embodied AI-enhanced vehicular networks by integrating vision language models (VLMs) for semantic information extraction and deep reinforcement learning (DRL) for decision-making. The proposed framework aims to optimize both data transmission efficiency and decision accuracy by formulating an optimization problem that incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth utilization and quality of experience (QoE). Specifically, we employ the large language and vision assistant (LLAVA) model to extract critical semantic information from raw image data captured by embodied AI agents (i.e., vehicles), reducing transmission data size by approximately more than 90% while retaining essential content for vehicular communication and decision-making. In the dynamic vehicular environment, we employ a generalized advantage estimation-based proximal policy optimization (GAE-PPO) method to stabilize decision-making under uncertainty. Simulation results show that attention maps from LLAVA highlight the model’s focus on relevant image regions, enhancing semantic representation accuracy. Additionally, our proposed transmission strategy improves QoE by up to 36% compared to DDPG and accelerates convergence by reducing required steps by up to 47% compared to pure PPO. Further analysis indicates that adapting semantic symbol length provides an effective trade-off between transmission quality and bandwidth, achieving up to a 61.4% improvement in QoE when scaling from 4 to 8 vehicles.},
  archive      = {J_TMC},
  author       = {Ruichen Zhang and Changyuan Zhao and Hongyang Du and Dusit Niyato and Jiacheng Wang and Suttinee Sawadsitang and Xuemin Shen and Dong In Kim},
  doi          = {10.1109/TMC.2025.3582864},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11494-11510},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Embodied AI-enhanced vehicular networks: An integrated vision language models and reinforcement learning method},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient on-device federated learning system through the interplay of client selection and batch size with watermarked data. <em>TMC</em>, <em>24</em>(11), 11480-11493. (<a href='https://doi.org/10.1109/TMC.2025.3585033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables edge devices to collaboratively train a global model using local data. However, the increasing prevalence of watermarks in datasets presents a new challenge to efficient FL. While watermarks assert data ownership and copyright, they introduce complexities that can lead to shortcut learning problems and mislead utility measurements for client selection. These issues are further exacerbated by batch size variations in efficient FL frameworks, ultimately undermining their time-to-accuracy performance. We introduce LotusFL, an FL system designed to address the challenges posed by watermarked datasets in efficient FL. Specifically, it tackles the increased time-to-accuracy due to erroneous client selection and the accuracy degradation observed with larger batch sizes. LotusFL first estimates the characteristics of watermarks through statistical estimation and then adjusts the batch size using this estimated watermark information to balance the negative impact of the watermark against device idle waiting time. Additionally, its client selection mechanism, based on historical information, avoids the misleading utility signals from watermarks. This mechanism, working in conjunction with batch size adjustment, aims to accurately predict device runtime and identify potentially valuable devices. We evaluated LotusFL through a real-world deployment on 40 edge devices. Compared to state-of-the-art efficient FL frameworks, LotusFL achieves superior performance, enhancing accuracy by up to 8.2% and reducing training time by 1.97×.},
  archive      = {J_TMC},
  author       = {Tao Ling and Siping Shi and Hao Wang and Chuang Hu and Dan Wang},
  doi          = {10.1109/TMC.2025.3585033},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11480-11493},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient on-device federated learning system through the interplay of client selection and batch size with watermarked data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive-XLight: Proactive traffic signal control with pluggable and reliable traffic prediction. <em>TMC</em>, <em>24</em>(11), 11465-11479. (<a href='https://doi.org/10.1109/TMC.2025.3581938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic signal control (TSC) plays a crucial role in the intelligent transportation system. Among existing TSC approaches, Proactive TSC (PTSC) predicts future traffic states at intersections and proactively adjusts control policies. It is evident that PTSC methods are highly effective in alleviating both current and future traffic congestion at intersections. However, existing PTSC methods focus on point estimation prediction while neglecting prediction reliability. Additionally, they fail to adaptively coordinate between current and future traffic states for optimal control. To address these limitations, we propose an innovative Proactive-Plugin that can be combined with existing TSC methods to enhance the accuracy and robustness of traffic signal control policies. This plugin enhances two critical aspects: 1) Prediction reliability is achieved through Fine-grained Traffic Uncertainty Quantification. This module generates probabilistic forecasts along with confidence intervals to explicitly indicate the credibility of the predictions. 2) Coordination adaptiveness is enabled by a Current-Future Tradeoff Integration mechanism. This mechanism dynamically adjusts the relative influence of current traffic states and probabilistic forecasts on control policies. To further ensure robustness, we design a multi-task joint optimization to reduce the negative impact of inaccurate predictions during training. Experimental results on six real-world datasets demonstrate consistent improvements in traffic efficiency, validating the effectiveness of our approach.},
  archive      = {J_TMC},
  author       = {Yang Jiang and Shengnan Guo and Hanyang Chen and Xiaowei Mao and Youfang Lin and Huaiyu Wan},
  doi          = {10.1109/TMC.2025.3581938},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11465-11479},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Proactive-XLight: Proactive traffic signal control with pluggable and reliable traffic prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-assisted lightweight cross-domain authentication for multi-UAV wireless networks. <em>TMC</em>, <em>24</em>(11), 11449-11464. (<a href='https://doi.org/10.1109/TMC.2025.3582833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of future network and control technologies has enabled unmanned aerial vehicles (UAVs) to collaborate across diverse geographical areas and task domains, enhancing task execution efficiency through data and resource sharing. In response to the increasing demand for cross-domain task allocation and operations for UAVs, establishing robust authentication mechanisms within trusted domains has become a critical foundation for ensuring secure cross-domain access. Despite significant progress in UAV identity authentication and cross-domain access, challenges persist, such as cumbersome and inefficient processes, UAV resource limitations, and establishing trust relationships across different domains. To address these challenges, this paper introduces a dual blockchain-assisted trusted authentication scheme for UAVs’ cross-domain access. Our approach utilizes a certificateless signcryption algorithm for lightweight UAV authentication, thereby eliminating the need for certificate management. Then, an efficient credit-based trust model is designed to measure the trustworthiness of data-in-transit and cross-domain entities. Furthermore, blockchain technology is introduced to store the relevant information of UAVs and credibility to assist cross-domain authentication. Theoretical security analysis and extensive simulations have been conducted, demonstrating the effectiveness and efficiency of our proposed scheme.},
  archive      = {J_TMC},
  author       = {Mingyue Xie and Zheng Chang and Li Wang and Geyong Min},
  doi          = {10.1109/TMC.2025.3582833},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11449-11464},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-assisted lightweight cross-domain authentication for multi-UAV wireless networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online collaborative resource allocation and task offloading for multi-access edge computing. <em>TMC</em>, <em>24</em>(11), 11430-11448. (<a href='https://doi.org/10.1109/TMC.2025.3580365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) is emerging as a promising paradigm to provide flexible computing services close to user devices (UDs). However, meeting the computation-hungry and delay-sensitive demands of UDs faces several challenges, including the resource constraints of MEC servers, inherent dynamic and complex features in the MEC system, and difficulty in dealing with the time-coupled and decision-coupled optimization. In this work, we first present an edge-cloud collaborative MEC architecture, where the MEC servers and cloud collaboratively provide offloading services for UDs. Moreover, we formulate an energy-efficient and delay-aware optimization problem (EEDAOP) to minimize the energy consumption of UDs under the constraints of task deadlines and long-term queuing delays. Since the problem is proved to be non-convex mixed integer nonlinear programming (MINLP), we propose an online joint communication resource allocation and task offloading approach (OJCTA). Specifically, we transform EEDAOP into a real-time optimization problem by employing the Lyapunov optimization framework. Then, to solve the real-time optimization problem, we propose a communication resource allocation and task offloading optimization method by employing the Tammer decomposition mechanism, convex optimization method, bilateral matching mechanism, and dependent rounding method. Simulation results demonstrate that the proposed OJCTA can achieve superior system performance compared to the benchmark approaches.},
  archive      = {J_TMC},
  author       = {Geng Sun and Minghua Yuan and Zemin Sun and Jiacheng Wang and Hongyang Du and Dusit Niyato and Zhu Han and Dong In Kim},
  doi          = {10.1109/TMC.2025.3580365},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11430-11448},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Online collaborative resource allocation and task offloading for multi-access edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge graph fusion based semantic communication framework. <em>TMC</em>, <em>24</em>(11), 11416-11429. (<a href='https://doi.org/10.1109/TMC.2025.3583605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic communication (SemCom), a paradigm that emphasizes conveying the meaning of information, faces challenges in precise reasoning in semantic coding models. Knowledge graphs (KGs) offer a potential solution by providing structured triples (entities and relations), enabling inference via entity attributes and relational logic. Several key challenges exist in leveraging KGs within SemCom. The first challenge lies in developing methods to create semantic representations aligning and integrating source data and KG information. Second, reconstructing the original data using KGs becomes challenging particularly under poor communication conditions. Moreover, integrating KGs with source data inevitably increases the transmission overhead. In this paper, we propose a novel SemCom framework named KG-SemCom with sophisticated KG-based semantic encoding and decoding designs to solve these challenges. This framework aligns KG entities with message tokens, and then encodes messages into a semantic fusion of contextual and knowledge-based information. Furthermore, KG-SemCom can utilize the KG and contextual relationships to assist in predicting incomplete or distorted messages during the decoding process. Finally, simulation results demonstrate that KG-SemCom achieves higher accuracy and greater robustness compared to existing benchmarks without incorporating KGs, especially in challenging communication environments.},
  archive      = {J_TMC},
  author       = {Chengsi Liang and Yao Sun and Dusit Niyato and Muhammad Ali Imran},
  doi          = {10.1109/TMC.2025.3583605},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11416-11429},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Knowledge graph fusion based semantic communication framework},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-aware neural radio-frequency radiance fields. <em>TMC</em>, <em>24</em>(11), 11401-11415. (<a href='https://doi.org/10.1109/TMC.2025.3583580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Maxwell discovered the physical laws of electromagnetic waves about 160 years ago, accurately modeling the propagation of RF signals in large and complex electrical environments remains a persistent challenge. This complexity arises from the interactions between the RF signal and various obstacles, including reflection and diffraction. Inspired by the success of neural networks in mapping the optical field in computer vision, we introduce the neural radio-frequency radiance field, or NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>. This represents a continuous volumetric scene function that effectively models RF signal propagation. Remarkably, after only a sparse amount of training with signal measurements, NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math> can accurately predict the nature and origin of signals received at any location, assuming the transmitter’s position is known. Additionally, we propose the frequency-aware NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math> to enhance channel prediction performance for wideband signals using an RF prism module. Compared to the vanilla NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>, the frequency-aware NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math> achieves a 4 dB improvement in SNR for FDD OFDM channel estimation and is nearly 3.5 × faster. Functioning as a physical-layer neural network, NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math> also supports application-layer artificial neural networks (ANNs) by generating synthetic training datasets. Our empirical results demonstrate that augmented sensing enhances the accuracy of AoA estimation, achieving an approximate 50% improvement.},
  archive      = {J_TMC},
  author       = {Xiaopeng Zhao and Zhenlin An and Qingrui Pan and Lei Yang},
  doi          = {10.1109/TMC.2025.3583580},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11401-11415},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Frequency-aware neural radio-frequency radiance fields},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cellular infrastructure sharing for network robustness: A citywide empirical study. <em>TMC</em>, <em>24</em>(11), 11386-11400. (<a href='https://doi.org/10.1109/TMC.2025.3580605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual cellular networks have been very robust to random cell tower failures due to redundant cell tower deployments. However, a large-scale clustered failure with multiple cell towers can lead to the loss of services of a cellular network. Recently, off-the-shelf smartphones can support multiple network standards, so cellular network infrastructure sharing is a promising direction to improve the service robustness under potential large-scale clustered tower failures. The existing work on cellular network robustness is usually limited to large-scale studies of individual networks or small-scale studies of multiple networks. In this work, we conduct the first investigation, to our knowledge, on cross-network infrastructure sharing benefits for enhancing robustness with a full cellular penetration rate. Our work is based on all cellular networks in Shenzhen, China, covering over 10 million cellular users. Specifically, we design a new metric to quantify cellular network robustness with or without cross-network sharing under both random and clustered cell tower failures. We further study the impact of different factors on robustness, including the number of networks, spatiotemporal dynamics, contextual factors, and a case study at two key transportation hubs. We provide a set of lessons learned based on our study, along with discussions of the results.},
  archive      = {J_TMC},
  author       = {Zhihan Fang and Guang Yang and Wenjun Lyu and Zhiqing Hong and Shuxin Zhong and Weijian Zuo and Yuelei Xie and Yu Yang and Guang Wang and Yunhuai Liu and Desheng Zhang},
  doi          = {10.1109/TMC.2025.3580605},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11386-11400},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cellular infrastructure sharing for network robustness: A citywide empirical study},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seamless physical-layer cross-technology communication from ZigBee to LoRa via neural networks. <em>TMC</em>, <em>24</em>(11), 11369-11385. (<a href='https://doi.org/10.1109/TMC.2025.3580396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRa, designed for Low-Power, Wide-Area Networks (LPWANs), is widely used in the Internet of Things (IoT). In contrast, Wireless Personal Area Network (WPAN) technologies like ZigBee struggle to connect directly to LPWANs due to their limited communication range and differing modulation schemes. ZigBee uses Offset Quadrature Phase-Shift Keying (OQPSK) modulation, while LoRa employs Chirp Spread Spectrum (CSS) modulation, complicating cross-technology communication. To address this challenge, we propose a novel approach for seamless physical-layer cross-technology communication between ZigBee and LoRa networks, bridging the gap between short-range and long-range communication technologies. We introduce ZigRa, a communication method that leverages neural networks for efficient modulation translation between ZigBee’s IEEE 802.15.4 standard and LoRa’s CSS modulation. The core of ZigRa is a deep learning model that adapts and optimizes the transformation of ZigBee signals into ultra-narrowband single-tone sinusoidal signals, which can be reliably detected by LoRaWAN base stations. Our solution enables ZigBee devices to seamlessly connect to LoRa-based LPWANs, overcoming modulation mismatches and providing long-range connectivity. Extensive evaluations with both USRP hardware and commercial devices demonstrate that ZigRa achieves a frame reception rate exceeding 85% at distances up to 500 meters, significantly enhancing the interoperability and coverage of heterogeneous IoT networks.},
  archive      = {J_TMC},
  author       = {Demin Gao and Yongrui Chen and Ye Liu and Honggang Wang},
  doi          = {10.1109/TMC.2025.3580396},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11369-11385},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Seamless physical-layer cross-technology communication from ZigBee to LoRa via neural networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASSUME: An optimal algorithm to minimize UAV energy by altitude and speed scheduling. <em>TMC</em>, <em>24</em>(11), 11351-11368. (<a href='https://doi.org/10.1109/TMC.2025.3581929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed aerial vehicles (UAVs) are being widely employed in wireless communication applications, e.g., collecting data from ground nodes (GNs). Minimizing UAV energy in these applications is crucial due to the limited energy supply onboard. Unlike previous studies that assume UAVs fly at a fixed altitude and simplify the energy consumption model of UAVs, we consider the impact of varying UAV altitudes on the ground-to-air communication and utilize a general communication model for GN. Furthermore, we conduct real-world flight tests and introduce a practical speed-related flight energy consumption model of UAVs. This paper focuses on the UAV altitude-speed scheduling and GN transmission switching (UASS-GTS) problem, specifically in scenarios where the UAV flies straight for monitoring applications such as power transmission lines, roads, and water/oil/gas pipes. However, minimizing energy consumption presents challenges due to the tight coupling of altitude scheduling and speed scheduling. To tackle this, first, we develop the looking before crossing algorithm for speed scheduling. We then extend this algorithm by integrating altitude scheduling to propose the Altitude-Speed Scheduling of UAV for Minimizing Energy (ASSUME) algorithm, using a dynamic programming method. The ASSUME algorithm is theoretically proven to be optimal. Additionally, based on ASSUME, we propose an offline-inspired online heuristic algorithm to handle agnostic situations where GN information is not available unless flies close. Simulations indicate that the ASSUME algorithm saves an average of 26.1%–62.7% energy compared to the baseline methods, and the performance gap between the online algorithm and the offline optimal algorithm ASSUME is 22.8%.},
  archive      = {J_TMC},
  author       = {Jianping Huang and Feng Shan and Junzhou Luo and Runqun Xiong and Wenjia Wu},
  doi          = {10.1109/TMC.2025.3581929},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11351-11368},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ASSUME: An optimal algorithm to minimize UAV energy by altitude and speed scheduling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast multimodal edge inference via selective feature distillation. <em>TMC</em>, <em>24</em>(11), 11337-11350. (<a href='https://doi.org/10.1109/TMC.2025.3580102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring user status at the edge is essential for delivering personalized services, such as detecting emotional states. However, deploying large-scale models directly on user devices is impractical due to substantial computational overhead and the scarcity of labeled data. Conversely, uploading raw data to the cloud for processing raises significant privacy concerns and incurs prohibitive communication costs. To address this challenge, we propose a privacy-preserving multimodal inference framework that leverages large-scale public data while safeguarding sensitive information and optimizing computational efficiency. Specifically, we first train a teacher model in the cloud using publicly available data. Through a feature distillation process, the knowledge from this teacher model is transferred to a lightweight encoder deployed at the user end. This transfer is tailored to the user’s data, ensuring that only relevant knowledge is distilled. To accommodate varying communication constraints, we introduce a feature compression mechanism that significantly reduces communication overhead without compromising inference accuracy. Extensive experiments on emotion recognition tasks demonstrate that the proposed framework effectively balances privacy preservation, resource efficiency, and inference accuracy, facilitating seamless collaboration between cloud and edge devices.},
  archive      = {J_TMC},
  author       = {Jinyu Chen and Wenchao Xu and Yunfeng Fan and Haozhao Wang and Quan Chen and Jing Li},
  doi          = {10.1109/TMC.2025.3580102},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11337-11350},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fast multimodal edge inference via selective feature distillation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling cross-band backscatter communication with twaltz. <em>TMC</em>, <em>24</em>(11), 11323-11336. (<a href='https://doi.org/10.1109/TMC.2025.3581900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency switching is a fundamental capability for wireless communication systems. However, this capability is significantly constrained in backscatter systems. The difficulty is to generate tunable high-frequency modulation signals on a backscatter tag at an acceptable power budget. In this paper, we present Twaltz, a new design paradigm for backscatter communication that enables frequency switching across large frequency bands. By exploiting a low-power semiconductor device, i.e., tunnel diode, and carefully addressing its physical features, Twaltz generates oscillation signals up to 1.2 GHz while maintaining micro-watt level power consumption. Twaltz further facilitates on-tag oscillation signal stabilization and programmable oscillation frequency tuning. We prototype Twaltz on a PCB board, demonstrating its efficiency in cross-band communication for LoRa backscatter, and verifying its performance in concurrent transmission, channel hopping, and data transmission.},
  archive      = {J_TMC},
  author       = {Xiuzhen Guo and Boya Liu and Nan Jing and Chaojie Gu and Yuanchao Shu and Jiming Chen},
  doi          = {10.1109/TMC.2025.3581900},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11323-11336},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling cross-band backscatter communication with twaltz},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). X5G: An open, programmable, multi-vendor, end-to-end, private 5G O-RAN testbed with NVIDIA ARC and OpenAirInterface. <em>TMC</em>, <em>24</em>(11), 11305-11322. (<a href='https://doi.org/10.1109/TMC.2025.3580764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Fifth generation (5G) cellular systems transition to softwarized, programmable, and intelligent networks, it becomes fundamental to enable public and private 5G deployments that are (i) primarily based on software components while (ii) maintaining or exceeding the performance of traditional monolithic systems and (iii) enabling programmability through bespoke configurations and optimized deployments. This requires hardware acceleration to scale the Physical (PHY) layer performance, programmable elements in the Radio Access Network (RAN) and intelligent controllers at the edge, careful planning of the Radio Frequency (RF) environment, as well as end-to-end integration and testing. In this paper, we describe how we developed the programmable X5G testbed, addressing these challenges through the deployment of the first 8-node network based on the integration of NVIDIA Aerial RAN CoLab Over-the-Air (ARC-OTA), OpenAirInterface (OAI), and a near-real-time RAN Intelligent Controller (RIC). The Aerial Software Development Kit (SDK) provides the PHY layer, accelerated on Graphics Processing Unit (GPU), with the higher layers from the OAI open-source project interfaced with the PHY through the Small Cell Forum (SCF) Functional Application Platform Interface (FAPI). An E2 agent provides connectivity to the O-RAN Software Community (OSC) near-real-time RIC. We discuss software integration, network infrastructure, and a digital twin framework for RF planning. We then profile the performance with up to 4 Commercial Off-the-Shelf (COTS) smartphones for each base station with iPerf and video streaming applications, as well as up to 25 emulated User Equipments (UEs), measuring a cell rate higher than 1.65 Gbps in downlink and 143 Mbps in uplink.},
  archive      = {J_TMC},
  author       = {Davide Villa and Imran Khan and Florian Kaltenberger and Nicholas Hedberg and Rúben Soares da Silva and Stefano Maxenti and Leonardo Bonati and Anupa Kelkar and Chris Dick and Eduardo Baena and Josep M. Jornet and Tommaso Melodia and Michele Polese and Dimitrios Koutsonikolas},
  doi          = {10.1109/TMC.2025.3580764},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11305-11322},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {X5G: An open, programmable, multi-vendor, end-to-end, private 5G O-RAN testbed with NVIDIA ARC and OpenAirInterface},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age of information-aware multi-objective optimization for heterogeneous UAV-USV-UUV networks in underwater target hunting. <em>TMC</em>, <em>24</em>(11), 11292-11304. (<a href='https://doi.org/10.1109/TMC.2025.3581836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater target hunting (UTH) is a critical and complex mission involving the search, tracking, and hunting of targets in an underwater environment. However, the unpredictable trajectories and flexibility of these targets, along with complex underwater environments, significantly impede the efficiency and success of traditional methods that depend solely on unmanned underwater vehicles (UUVs). Consequently, this paper presents the “3U network”, a novel heterogeneous framework integrating unmanned aerial vehicles (UAVs), unmanned surface vehicles (USVs), and UUVs for UTH. Within this framework, a UAV identifies the target, a USV acts as a communication relay, and a swarm of UUVs hunts the target. Moreover, to improve the timeliness of target detection, we incorporate the age of information (AoI) concept into the UAV’s search strategy. Additionally, we develop a constrained multi-objective optimization problem to minimize energy consumption and mission duration by optimizing vehicles’ trajectories, considering mobility limitations, safety, and connectivity constraints. Furthermore, to tackle this problem, we design an AoI- and energy-aware multi-vehicle twin-delayed deep deterministic policy gradient algorithm (AE-MVTD3) to optimize control policies for heterogeneous vehicles. The experimental results show that the proposed method performs effectively across diverse complex scenarios.},
  archive      = {J_TMC},
  author       = {Xiangwang Hou and Tianyu Xing and Jingjing Wang and Jun Du and Chunxiao Jiang and Yong Ren and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3581836},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11292-11304},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age of information-aware multi-objective optimization for heterogeneous UAV-USV-UUV networks in underwater target hunting},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
