<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm">TMM - 386</h2>
<ul>
<li><details>
<summary>
(2025). XMusic: Towards a generalized and controllable symbolic music generation framework. <em>TMM</em>, <em>27</em>, 6857-6871. (<a href='https://doi.org/10.1109/TMM.2025.3590912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is: https://xmusic-project.github.io.},
  archive      = {J_TMM},
  author       = {Sida Tian and Can Zhang and Wei Yuan and Wei Tan and Wenjie Zhu},
  doi          = {10.1109/TMM.2025.3590912},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6857-6871},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {XMusic: Towards a generalized and controllable symbolic music generation framework},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFAN: An attention-driven forgery adversarial network for blind image inpainting. <em>TMM</em>, <em>27</em>, 6845-6856. (<a href='https://doi.org/10.1109/TMM.2025.3590914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image inpainting is a challenging task aimed at reconstructing corrupted regions without relying on mask information. Due to the lack of mask priors, previous methods usually integrate a mask prediction network in the initial phase, followed by an inpainting backbone. However, this multi-stage generation process may result in feature misalignment. While recent end-to-end generative methods bypass the mask prediction step, they typically struggle with weak perception of contaminated regions and introduce structural distortions. This study presents a novel mask region perception strategy for blind image inpainting by combining adversarial training with forgery detection. To implement this strategy, we propose an attention-driven forgery adversarial network (AFAN), which leverages adaptive contextual attention (ACA) blocks for effective feature modulation. Specifically, within the generator, ACA employs self-attention to enhance content reconstruction by utilizing the rich contextual information of adjacent tokens. In the discriminator, ACA utilizes cross-attention with noise priors to guide adversarial learning for forgery detection. Moreover, we design a high-frequency omni-dimensional dynamic convolution (HODC) based on edge feature enhancement to improve detail representation. Extensive evaluations across multiple datasets demonstrate that the proposed AFAN model outperforms existing generative methods in blind image inpainting, particularly in terms of quality and texture fidelity.},
  archive      = {J_TMM},
  author       = {Jiahao Wang and Gang Pan and Di Sun and Jinyuan Li and Jiawan Zhang},
  doi          = {10.1109/TMM.2025.3590914},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6845-6856},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AFAN: An attention-driven forgery adversarial network for blind image inpainting},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards student actions in classroom scenes: New dataset and baseline. <em>TMM</em>, <em>27</em>, 6831-6844. (<a href='https://doi.org/10.1109/TMM.2025.3590899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing student actions is an important and challenging task in educational research. Existing efforts have been hampered by the lack of accessible datasets to capture the nuanced action dynamics in classrooms. In this paper, we present a new multi-label Student Action Video (SAV) dataset, specifically designed for action detection in classroom settings. The SAV dataset consists of 4,324 carefully trimmed video clips from 758 different classrooms, annotated with 15 distinct student actions. Compared to existing action detection datasets, the SAV dataset stands out by providing a wide range of real classroom scenarios, high-quality video data, and unique challenges, including subtle movement differences, dense object engagement, significant scale differences, varied shooting angles, and visual occlusion. These complexities introduce new opportunities and challenges to advance action detection methods. To benchmark this, we propose a novel baseline method based on a visual transformer, designed to enhance attention to key local details within small and dense object regions. Our method demonstrates excellent performance with a mean Average Precision (mAP) of 67.9% and 27.4% on the SAV and AVA datasets, respectively. This paper not only provides the dataset but also calls for further research into AI-driven educational tools that may transform teaching methodologies and learning outcomes.},
  archive      = {J_TMM},
  author       = {Zhuolin Tan and Chenqiang Gao and Anyong Qin and Ruixin Chen and Tiecheng Song and Feng Yang and Deyu Meng},
  doi          = {10.1109/TMM.2025.3590899},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6831-6844},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards student actions in classroom scenes: New dataset and baseline},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image super-resolution with taylor expansion approximation and large field reception. <em>TMM</em>, <em>27</em>, 6819-6830. (<a href='https://doi.org/10.1109/TMM.2025.3590917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-similarity techniques are booming in no-reference super-resolution (SR) due to accurate estimation of the degradation types involved in low-resolution images. However, high-dimensional matrix multiplication within self-similarity computation prohibitively consumes massive computational costs. We find that the high-dimensional attention map is derived from the matrix multiplication between query and key, followed by a softmax function. This softmax makes the matrix multiplication inseparable, posing a great challenge in simplifying computational complexity. To address this issue, we first propose a second-order Taylor expansion approximation (STEA) to separate the matrix multiplication of query and key, resulting in the complexity reduction from $\mathcal {O}(N^{2})$ to $\mathcal {O}(N)$. Then, we design a multi-scale large field reception (MLFR) to compensate for the performance degradation caused by STEA. Finally, we apply these two core designs to laboratory and real-world scenarios by constructing LabNet and RealNet, respectively. Extensive experimental results tested on five synthetic datasets demonstrate that our LabNet sets a new benchmark in qualitative and quantitative evaluations. Tested on the real-world dataset, our RealNet achieves superior visual quality over existing methods. Ablation studies further verify the contributions of STEA and MLFR towards both LabNet and RealNet frameworks.},
  archive      = {J_TMM},
  author       = {Jiancong Feng and Yuan-Gen Wang and Mingjie Li and Fengchuang Xing},
  doi          = {10.1109/TMM.2025.3590917},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6819-6830},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image super-resolution with taylor expansion approximation and large field reception},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-domain generalized object detection with frequency whitening and contrastive learning. <em>TMM</em>, <em>27</em>, 6805-6818. (<a href='https://doi.org/10.1109/TMM.2025.3590915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-Domain Generalization Object Detection (Single-DGOD) refers to training a model with only one source domain, enabling the model to generalize to any unseen domain. For instance, a detector trained on a sunny daytime dataset should also perform well in scenarios such as rainy nighttime. The main challenge is to improve the detector’s ability to learn the domain-invariant representation (DIR) while removing domain-specific information. Recent progress in Single-DGOD has demonstrated the efficacy of removing domain-specific information by adjusting feature distributions. Nonetheless, simply adjusting the global feature distribution in Single-DGOD task is insufficient to learn the potential relationship from sunny to adverse weather, as these ignore the significant domain gaps between instances across different weathers. In this paper, we propose a novel object detection method for more robust single-domain generalization. In particular, it mainly consists of a frequency-aware selective whitening module (FSW) for removing redundant domain-specific information and a contrastive feature alignment module (CFA) for enhancing domain-invariant information among instances. Specially, FSW extracts the magnitude spectrum of the feature and uses a group whitening loss to selectively eliminate redundant domain-specific information in the magnitude. To further eliminate domain differences among instances, we apply the style transfer method for data augmentation and use the augmented data in the CFA module. CFA formulates both the original and the augmentd RoI features into a series of groups with different categories, and utilizes contrastive learning across them to facilitate the learning of DIR in various categories. Experiments show that our method achieves favorable performance on existing standard benchmarks.},
  archive      = {J_TMM},
  author       = {Xiaolong Guo and Chengxu Liu and Xueming Qian and Zhixiao Wang and Xubin Feng and Yao Xue},
  doi          = {10.1109/TMM.2025.3590915},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6805-6818},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Single-domain generalized object detection with frequency whitening and contrastive learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting modal-specific representations for sentiment analysis with incomplete modalities. <em>TMM</em>, <em>27</em>, 6793-6804. (<a href='https://doi.org/10.1109/TMM.2025.3590909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis aims at exploiting complementary information from multiple modalities or data sources to enhance the understanding and interpretation of sentiment. While existing multi-modal fusion techniques offer significant improvements in sentiment analysis, real-world scenarios often involve missing modalities, introducing complexity due to uncertainty of which modalities may be absent. To tackle the challenge of incomplete modality-specific feature extraction caused by missing modalities, this paper proposes a Cosine Margin-Aware Network (CMANet) which centers on the Cosine Margin-Aware Distillation (CMAD) module. The core module measures distance between samples and the classification boundary, enabling CMANet to focus on samples near the boundary. So, it effectively captures the unique features of different modal combinations. To address the issue of modality imbalance during modality-specific feature extraction, this paper proposes a Weak Modality Regularization (WMR) strategy, which aligns the feature distributions between strong and weak modalities at the dataset-level, while also enhancing the prediction loss of samples at the sample-level. This dual mechanism improves the recognition robustness of weak modality combination. Extensive experiments demonstrate that the proposed method outperforms the previous best model, MMIN, with a 3.82% improvement in unweighted accuracy. These results underscore the robustness of the approach under conditions of uncertain and missing modalities.},
  archive      = {J_TMM},
  author       = {Xin Jiang and Lihuo He and Fei Gao and Kaifan Zhang and Jie Li and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3590909},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6793-6804},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boosting modal-specific representations for sentiment analysis with incomplete modalities},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-grained vision-and-language model for medical image and text alignment. <em>TMM</em>, <em>27</em>, 6780-6792. (<a href='https://doi.org/10.1109/TMM.2025.3590930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing interest in learning from paired medical images and textual reports highlights the need for methods that can achieve multi-grained alignment between these two modalities. However, most existing approaches overlook fine-grained semantic alignment, which can constrain the quality of the generated representations. To tackle this problem, we propose the Multi-Grained Vision-and-Language Alignment (MGVLA) model, which effectively leverages multi-grained correspondences between medical images and texts at different levels, including disease, instance, and token levels. For disease-level alignment, our approach adopts the concept of contrastive learning and uses medical terminologies detected from textual reports as soft labels to guide the alignment process. At the instance level, we propose a strategy for sampling hard negatives, where images and texts with the same disease type but differing in details such as disease locations and severity are considered as hard negatives. This strategy helps our approach to better distinguish between positive and negative image-text pairs, ultimately enhancing the quality of our learned representations. For token-level alignment, we employ a masking and recovery technique to achieve fine-grained semantic alignment between patches and sub-words. This approach effectively aligns the different levels of granularity between the image and language modalities. To assess the efficacy of our MGVLA model, we conduct comprehensive experiments on the image-text retrieval and phrase grounding tasks.},
  archive      = {J_TMM},
  author       = {Huimin Yan and Xian Yang and Liang Bai and Jiamin Li and Jiye Liang},
  doi          = {10.1109/TMM.2025.3590930},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6780-6792},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-grained vision-and-language model for medical image and text alignment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROSA: A robust self-adaptive model for multimodal emotion recognition with uncertain missing modalities. <em>TMM</em>, <em>27</em>, 6766-6779. (<a href='https://doi.org/10.1109/TMM.2025.3590929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of online media has heightened the importance of multimodal emotion recognition (MER) in video analysis. However, practical applications often encounter challenges due to missing modalities caused by various interferences. It is difficult to predict the specific missing situations, such as the number and types of missing modalities. Current approaches to modality missing typically apply a uniform method to address various missing cases, which are insufficiently adaptive to dynamic conditions. For example, translation-based methods can efficiently complete missing text from audio, but generating audio or video features that retain the original emotional information from other modalities is challenging and may introduce additional noise. In this paper, we introduce ROSA, a novel robust self-adaptive model designed to address various missing cases with tailored approaches, leveraging available modalities effectively and reducing the introduction of additional noise. Specifically, the A-T Completion module based on the encoder-decoder architecture enables ROSA to generate missing raw text from audio rather than mere embedding representations, capturing more nuanced modal features. Additionally, we design the T-V Fusion module based on a vision-language large model for deep extraction and fusion of textual and visual features. Comprehensive experiments conducted on three widely used public datasets demonstrate the superiority and effectiveness of our model. ROSA outperforms other models in both fixed missing rate and fixed missing modality cases. The ablation studies further highlights the contribution of each designed module.},
  archive      = {J_TMM},
  author       = {Ziming Li and Yaxin Liu and Chuanpeng Yang and Yan Zhou and Songlin Hu},
  doi          = {10.1109/TMM.2025.3590929},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6766-6779},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ROSA: A robust self-adaptive model for multimodal emotion recognition with uncertain missing modalities},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-projection distilling knowledge for omnidirectional image quality assessment. <em>TMM</em>, <em>27</em>, 6752-6765. (<a href='https://doi.org/10.1109/TMM.2025.3590920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, virtual reality technology is advancing rapidly and becoming increasingly matured. Omnidirectional images have integrated into the daily lives of many individuals. However, these images are susceptible to irreversible distortion during the encoding and transmission processes. Given the unique characteristics of deformation and distortion in omnidirectional images, the development of a quality assessment method is crucial. To ensure that our network not only delivers efficient and stable performance but also maintains a minimal parameter count, we have integrated the concept of knowledge distillation into our network. This involves utilizing a full-reference (FR) teacher network to guide the training of a no-reference (NR) student network by cross-projection distilling knowledge. To specifically implement this method, a Dual Projection Format Fusion (DPFF) module is specifically designed to complement and integrate the mutual fusion of the two projection formats of omnidirectional images. In the design of our knowledge distillation process and loss function, we have introduced a review mechanism to enhance the performance and efficiency of response-based knowledge, as well as utilized intermediate fusion features to improve the effectiveness of feature-based knowledge. These components are combined to formulate the final loss function. Experimental results validate the superiority of our proposed model over existing FR and NR methods when evaluated on four omnidirectional image databases. This highlights the effectiveness of our proposed model in elevating the quality assessment of omnidirectional images.},
  archive      = {J_TMM},
  author       = {Huixin Hu and Feng Shao and Hangwei Chen and Xiongli Chai and Qiuping Jiang},
  doi          = {10.1109/TMM.2025.3590920},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6752-6765},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-projection distilling knowledge for omnidirectional image quality assessment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient partially relevant video retrieval with active moment discovering. <em>TMM</em>, <em>27</em>, 6740-6751. (<a href='https://doi.org/10.1109/TMM.2025.3590937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially relevant video retrieval (PRVR) is a practical yet challenging task in text-to-video retrieval, where videos are untrimmed and contain much background content. The pursuit here is of both effective and efficient solutions to capture the partial correspondence between text queries and untrimmed videos. Existing PRVR methods, which typically focus on modeling multi-scale clip representations, however, suffer from content independence and information redundancy, impairing retrieval performance. To overcome these limitations, we propose a simple yet effective approach with active moment discovering (AMDNet). We are committed to discovering video moments that are semantically consistent with their queries. By using learnable span anchors to capture distinct moments and applying masked multi-moment attention to emphasize salient moments while suppressing redundant backgrounds, we achieve more compact and informative video representations. To further enhance moment modeling, we introduce a moment diversity loss to encourage different moments of distinct regions and a moment relevance loss to promote semantically query-relevant moments, which cooperate with a partially relevant retrieval loss for end-to-end optimization. Extensive experiments on two large-scale video datasets (i.e., TVR and ActivityNet Captions) demonstrate the superiority and efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller (#parameters) while 6.0 points higher (SumR) than the up-to-date method GMMFormer on TVR.},
  archive      = {J_TMM},
  author       = {Peipei Song and Long Zhang and Long Lan and Weidong Chen and Dan Guo and Xun Yang and Meng Wang},
  doi          = {10.1109/TMM.2025.3590937},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6740-6751},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards efficient partially relevant video retrieval with active moment discovering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PIMG: Progressive image-to-music generation with contrastive diffusion models. <em>TMM</em>, <em>27</em>, 6732-6739. (<a href='https://doi.org/10.1109/TMM.2025.3586119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of Image-to-Music Generation is to create pure music according to the given image. Unlike existing tasks such as text-to-image generation, there is no explicit connection between image content and musical melody. Some existing studies attempt to generate music by directly mapping image features (such as color, edges, etc.) into musical notes, which may result in the melodic incoherence. Inspired by neuroscience, it is desirable to employ emotion to bridge these two modalities. However, the continuity and complexity of emotions make it difficult to capture the cross-modal correlation. Drawing from human perception mechanisms of emotions, a Progressive Image-to-Music Generation (PIMG) framework is proposed. The framework designs a mean-teacher based association network to guide the music generation process progressively, starting from highly correlated image-music pairs. The generation network receives more challenging sample pairs gradually, eventually capturing complex cross-modal emotional correspondences. Additionally, a contrastive learning strategy is introduced into the diffusion models to better capture the consistency between pieces of music with the similar emotions. Extensive experimental results demonstrate that the proposed framework is able to generate high-quality and emotionally consistent music from images.},
  archive      = {J_TMM},
  author       = {Mulin Chen and Yajie Wang and Xuelong Li},
  doi          = {10.1109/TMM.2025.3586119},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6732-6739},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PIMG: Progressive image-to-music generation with contrastive diffusion models},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral discrepancy and cross-modal semantic consistency learning for object detection in hyperspectral images. <em>TMM</em>, <em>27</em>, 6719-6731. (<a href='https://doi.org/10.1109/TMM.2025.3586155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images with high spectral resolution provide new insights into recognizing subtle differences in similar substances. However, object detection in hyperspectral images faces significant challenges in intra- and inter-class similarity due to the spatial differences in hyperspectral inter-bands and unavoidable interferences, e.g., sensor noises and illumination. To alleviate the hyperspectral inter-bands inconsistencies and redundancy, we propose a novel network termed Spectral Discrepancy and Cross-Modal semantic consistency learning (SDCM), which facilitates the extraction of consistent information across a wide range of hyperspectral bands while utilizing the spectral dimension to pinpoint regions of interest. Specifically, we leverage a semantic consistency learning (SCL) module that utilizes inter-band contextual cues to diminish the heterogeneity of information among bands, yielding highly coherent spectral dimension representations. On the other hand, we incorporate a spectral gated generator (SGG) into the framework that filters out the redundant data inherent in hyperspectral information based on the importance of the bands. Then, we design the spectral discrepancy aware (SDA) module to enrich the semantic representation of high-level information by extracting pixel-level spectral features. Extensive experiments on two hyperspectral datasets demonstrate that our proposed method achieves state-of-the-art performance when compared with other ones.},
  archive      = {J_TMM},
  author       = {Xiao He and Chang Tang and Xinwang Liu and Wei Zhang and Zhimin Gao and Chuankun Li and Shaohua Qiu and Jiangfeng Xu},
  doi          = {10.1109/TMM.2025.3586155},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6719-6731},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spectral discrepancy and cross-modal semantic consistency learning for object detection in hyperspectral images},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-prototype discrimination: Boosting support-query matching for few-shot segmentation. <em>TMM</em>, <em>27</em>, 6705-6718. (<a href='https://doi.org/10.1109/TMM.2025.3586125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation (FSS) aims at training a model on base classes with sufficient annotations and then tasking the model with predicting a binary mask to identify novel class pixels with limited labeled images. Mainstream FSS methods adopt a support-query matching paradigm that activates target regions of the query image according to their similarity with a single support class prototype. However, this prototype vector is inclined to overfit the support images, leading to potential under-matching in latent query object regions and incorrect mismatches with base class features in the query image. To address these issues, this study reformulates conventional single foreground prototype matching to a multi-prototype matching paradigm. In this paradigm, query features exhibiting high confidence with non-target prototypes will be categorized as background. Specifically, the target query features are drawn closer to the novel class prototype through a Masked Cross-Image Encoding (MCE) module and a Semantic Multi-prototype Matching (SMM) module is incorporated to collaboratively filter unexpected base class regions on multi-scale features. Furthermore, we devise an adaptive class activation map, termed target-aware class activation map (TCAM) to preserve semantically coherent regions that might be inadvertently suppressed under pixel-wise matching guidance. Experimental results on PASCAL-5$^{i}$ and COCO-20$^{i}$ datasets demonstrate the advantage of the proposed novel modules, with the holistic approach outperforming compared state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Wenbo Xu and Huaxi Huang and Yongshun Gong and Litao Yu and Qiang Wu and Jian Zhang},
  doi          = {10.1109/TMM.2025.3586125},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6705-6718},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical multi-prototype discrimination: Boosting support-query matching for few-shot segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MossVLN: Memory-observation synergistic system for continuous vision-language navigation. <em>TMM</em>, <em>27</em>, 6690-6704. (<a href='https://doi.org/10.1109/TMM.2025.3586105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigating in continuous environments with vision-language cues presents critical challenges, particularly in the accuracy of waypoint prediction and the quality of navigation decision-making. Traditional methods, which predominantly rely on spatial data from depth images or straightforward RGB-depth integrations, frequently encounter difficulties in environments where waypoints share similar spatial characteristics, leading to erroneous navigational outcomes. Additionally, the capacity for effective navigation decisions is often hindered by the inadequacies of traditional topological maps and the issue of uneven data sampling. In response, this paper introduces a robust memory-observation synergistic vision-language navigation framework to substantially enhance the navigation capabilities of agents operating in continuous environments. We present an advanced observation-driven waypoint predictor that effectively utilizes spatial data and integrates aligned visual and textual cues to significantly improve the accuracy of waypoint predictions within complex real-world scenarios. Additionally, we develop a strategic memory-observation planning approach that leverages memory panoramic environmental data and detailed current observation information, enabling more informed and precise navigation decisions. Our framework sets new performance benchmarks on the VLN-CE dataset, achieving a 60.25% Success Rate (SR) and a 50.89% Path Length Score (SPL) on the R2R-CE dataset’s unseen validation splits. Furthermore, when adapted to a discrete environment, our model also shows exceptional performance on the R2R dataset, achieving a 74% SR and a 64% SPL on the unseen validation split.},
  archive      = {J_TMM},
  author       = {Ting Yu and Yifei Wu and Qiongjie Cui and Qingming Huang and Jun Yu},
  doi          = {10.1109/TMM.2025.3586105},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6690-6704},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MossVLN: Memory-observation synergistic system for continuous vision-language navigation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S4R: Rethinking point cloud sampling via guiding upsampling-aware perception. <em>TMM</em>, <em>27</em>, 6677-6689. (<a href='https://doi.org/10.1109/TMM.2025.3586148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud sampling aims to derive a sparse point cloud from a relatively dense point cloud, which is essential for efficient data transmission and storage. While existing deep sampling methods prioritize preserving the perception of sampled point clouds for downstream networks, few studies have critically examined the rationale behind this goal. Specifically, we observe that sampling can lead to a perceptual degradation phenomenon in many influential downstream networks, impairing their ability to effectively process sampled point clouds. We theoretically reveal the nature of the phenomenon and attempt to construct a novel sampling target by uniting upsampling and perceptual reconstruction. Accordingly, we propose a Maximum A Posteriori (MAP) sampling framework named Sample for Reconstruct (S4R), which impels the sampling stage to infer upsampling-guided perception. In S4R, we design very simple but effective sampling and upsampling networks using residual-based graph convolutions and incorporate a pseudo-residual connection to introduce prior knowledge. This architecture takes advantage of reconstruction properties and allows the sampling network to be trained in an unsupervised manner. Extensive experiments on classical networks demonstrates the excellent performance of S4R compared with the previous sampling schemes and reveals its advantages on different point cloud downstream tasks, i.e., classification, reconstruction and segmentation.},
  archive      = {J_TMM},
  author       = {Zhuangzi Li and Shan Liu and Wei Gao and Guanbin Li and Ge Li},
  doi          = {10.1109/TMM.2025.3586148},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6677-6689},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {S4R: Rethinking point cloud sampling via guiding upsampling-aware perception},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathology-preserving transformer based on multicolor space for low-quality medical image enhancement. <em>TMM</em>, <em>27</em>, 6661-6676. (<a href='https://doi.org/10.1109/TMM.2025.3586133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images acquired under suboptimal conditions often suffer from quality degradation, such as low-light, blurring, and artifacts. Such degradations obscure the lesions and anatomical structures in medical images, making it difficult to distinguish key pathological regions. This significantly increases the risk of misdiagnosis by automated medical diagnostic systems or clinicians. To address this challenge, we propose a multi-Color space-based quality enhancement network (MSQNet) that effectively eliminates global low-quality factors while preserving pathology-related characteristics for improved clinical observation and analysis. We first revisit the properties of image quality enhancement in different color spaces, where the V-channel in the HSV space can better represent the contrast and brightness enhancement process, whereas the A/B-channel in the LAB space is more focused on the color change of low-quality images. The proposed framework harnesses the unique properties of different color spaces to optimize the image enhancement process. Specifically, we propose a pathology-preserving transformer, designed to selectively aggregate features across different color spaces and enable comprehensive multiscale feature fusion. Leveraging these capabilities, MSQNet effectively enhances low-quality RGB medical images while preserving key pathological features, thereby establishing a new paradigm in medical image enhancement. Extensive experiments on three public medical image datasets demonstrate that MSQNet outperforms traditional enhancement techniques and state-of-the-art methods, in terms of both quantitative metrics and qualitative visual assessment. MSQNet successfully improves image quality while preserving pathological features and anatomical structures, facilitating accurate diagnosis and analysis by medical professionals and automated systems.},
  archive      = {J_TMM},
  author       = {Qingshan Hou and Yaqi Wang and Peng Cao and Jianguo Ju and Huijuan Tu and Xiaoli Liu and Jinzhu Yang and Huazhu Fu and Yih Chung Tham and Osmar R. Zaiane},
  doi          = {10.1109/TMM.2025.3586133},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6661-6676},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Pathology-preserving transformer based on multicolor space for low-quality medical image enhancement},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DomainVerse: A benchmark towards real-world distribution shifts for training-free adaptive domain generalization. <em>TMM</em>, <em>27</em>, 6648-6660. (<a href='https://doi.org/10.1109/TMM.2025.3586108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional cross-domain tasks, including unsupervised domain adaptation (UDA), domain generalization (DG) and test-time adaptation (TTA), rely heavily on the training model by source domain data whether for specific or arbitrary target domains. With the recent advance of vision-language models (VLMs), recognized as natural source models that can be transferred to various downstream tasks without any parameter training, we propose a novel cross-domain task directly combining the strengths of both UDA and DG, named Training-Free Adaptive Domain Generalization (TF-ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which hinder the real-world application of current cross-domain models due to the lack of accurate and fair evaluation of fine-grained realistic domains. These insights motivate us to establish a novel realistic benchmark for TF-ADG. Benefiting from the introduced hierarchical definition of domain shifts, our proposed dataset DomainVerse addresses these issues by providing about 0.5 million images from 390 realistic, hierarchical, and balanced domains, allowing for decomposition across multiple domains within each image. With the help of the constructed DomainVerse and VLMs, we further propose two algorithms called Domain CLIP and Domain++ CLIP for training-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.},
  archive      = {J_TMM},
  author       = {Feng Hou and Jin Yuan and Ying Yang and Yao Zhang and Yang Liu and Yang Zhang and Cheng Zhong and Zhongchao Shi and Jianping Fan and Zhiqiang He and Yong Rui},
  doi          = {10.1109/TMM.2025.3586108},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6648-6660},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DomainVerse: A benchmark towards real-world distribution shifts for training-free adaptive domain generalization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A blockchain and improved perception hash based copyright protection scheme for purely chromatic background images. <em>TMM</em>, <em>27</em>, 6635-6647. (<a href='https://doi.org/10.1109/TMM.2025.3586150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Purely chromatic background images are widely used in computer wallpapers and advertisements, leading to issues such as copyright infringement and the loss of interest of holders. Image hashing is a technique used for comparing the similarity between images, and is often used for image verification, search, and copy detection due to its insensitivity to subtle changes in the original image. In a purely chromatic background image, the central detail of the image is the primary part and the key for copyright authentication. As the perception hash (pHash) algorithm only retains the low-frequency portion of the discrete cosine transform (DCT) matrix, it is unsuitable for purely chromatic background images. To deal with this issue, we propose an improved perception hash (ipHash) algorithm to enhance the universality of the algorithm by extracting purely chromatic background image features. Meanwhile, the development of image hashing is restricted due to the requirement of a trusted third party. To solve this issue, a secure blockchain-based image copyright protection scheme is designed. It realizes the copyright authentication and traceability, and overcomes the issue of a lack of trusted third parties. Experimental results show that the proposed method outperforms the state-of-the-art image copyright protection schemes.},
  archive      = {J_TMM},
  author       = {Guangyong Gao and Tongchao Feng and Chongtao Guo and Zhihua Xia and Yun-Qing Shi},
  doi          = {10.1109/TMM.2025.3586150},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6635-6647},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A blockchain and improved perception hash based copyright protection scheme for purely chromatic background images},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive prompt-driven low-light image enhancement with frequency aware learning. <em>TMM</em>, <em>27</em>, 6620-6634. (<a href='https://doi.org/10.1109/TMM.2025.3586101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light Image Enhancement (LLIE) aims to rectify inadequate illumination conditions and achieve superior visual quality in images, which plays a pivotal role in the domain of low-level computer vision. Due to poor illumination in images, many high-frequency details are obscured, which leads to an uneven distribution of low- and high-frequency information. However, most existing LLIE methods do not pay special attention to the restoration of high-frequency detail information and some challenging-to-recover areas in images. To address this issue, we propose a novel progressive prompt-driven LLIE framework with frequency aware learning, through a two-stage coarse-to-fine learning mechanism. Specifically, the proposed method fully utilizes both the specially designed brightness-aware prompt and detail-aware prompt on the prior trained model, to achieve an excellent enhanced image that exhibits more natural brightness and richer detail information. Furthermore, the proposed frequency aware learning objective can adaptively adjust the contribution of individual pixels for image reconstruction based on the statistics of high- and low-frequency features, which enables the network to focus on learning intricate details and other challenging areas in low-light images. Extensive experimental results demonstrate the effectiveness of the proposed method, achieving superior performances to state-of-the-art methods on representative real-world and synthetic datasets.},
  archive      = {J_TMM},
  author       = {Xiaoyan Sun and De Cheng and Yan Li and Nannan Wang and Dingwen Zhang and Xinbo Gao and Jiande Sun},
  doi          = {10.1109/TMM.2025.3586101},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6620-6634},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive prompt-driven low-light image enhancement with frequency aware learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint distribution weighted alignment for multi-source domain adaptation via kernel relative entropy estimation. <em>TMM</em>, <em>27</em>, 6606-6619. (<a href='https://doi.org/10.1109/TMM.2025.3586109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of Multi-Source Domain Adaptation (MSDA) is to train a neural network on labeled data from multiple joint source distributions (source domains) and unlabeled data from a joint target distribution (target domain), and use the trained network to estimate the target data labels. The challenge in this MSDA problem is that the multiple joint source distributions are relevant but distinct from the joint target distribution. To address this challenge, we propose a Joint Distribution Weighted Alignment (JDWA) approach to align a weighted joint source distribution to the joint target distribution under the relative entropy. Specifically, the weighted joint source distribution is defined as the weighted sum of the multiple joint source distributions, and is parameterized by the relevance weights. Since the relative entropy is unknown in practice, we propose a Kernel Relative Entropy Estimation (KREE) method to estimate it from data. Our KREE method first reformulates relative entropy as the negative of the minimal value of a functional, then exploits a function from the Reproducing Kernel Hilbert Space (RKHS) as the functional’s input, and finally solves the resultant convex problem with a global optimal solution. We also incorporate entropy regularization to enhance the network’s performance. Together, we minimize cross entropy, relative entropy, and entropy to learn both the relevance weights and the neural network. Experimental results on benchmark image classification datasets demonstrate that our JDWA approach performs better than the comparison methods. Intro video and Pytorch code are available at https://github.com/sentaochen/Joint-Distribution-Weighted-Alignment. Interested readers are also welcome to visit https://github.com/sentaochen for more source codes of the domain adaptation, partial domain adaptation, multi-source domain adaptation, and domain generalization approaches.},
  archive      = {J_TMM},
  author       = {Sentao Chen and Ping Xuan and Zhifeng Hao},
  doi          = {10.1109/TMM.2025.3586109},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6606-6619},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Joint distribution weighted alignment for multi-source domain adaptation via kernel relative entropy estimation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MVP-shot: Multi-velocity progressive-alignment framework for few-shot action recognition. <em>TMM</em>, <em>27</em>, 6593-6605. (<a href='https://doi.org/10.1109/TMM.2025.3586118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent few-shot action recognition (FSAR) methods typically perform semantic matching on learned discriminative features to achieve promising performance. However, most FSAR methods focus on single-scale (e.g., frame-level, segment-level, etc.) feature alignment, which ignores that human actions with the same semantic may appear at different velocities. To this end, we develop a novel Multi-Velocity Progressive-alignment (MVP-Shot) framework to progressively learn and align semantic-related action features at multi-velocity levels. Concretely, a Multi-Velocity Feature Alignment (MVFA) module is designed to measure the similarity between features from support and query videos with different velocity scales and then merge all similarity scores in a residual fashion. To avoid the multiple velocity features deviating from the underlying motion semantic, our proposed Progressive Semantic-Tailored Interaction (PSTI) module injects velocity-tailored text information into the video feature via feature interaction on channel and temporal domains at different velocities. The above two modules compensate for each other to make more accurate query sample predictions under the few-shot settings. Experimental results show our method outperforms current state-of-the-art methods on multiple standard few-shot benchmarks (i.e., HMDB51, UCF101, Kinetics, SSv2-full, and SSv2-small).},
  archive      = {J_TMM},
  author       = {Hongyu Qu and Rui Yan and Xiangbo Shu and Hailiang Gao and Peng Huang and Guosen Xie},
  doi          = {10.1109/TMM.2025.3586118},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6593-6605},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MVP-shot: Multi-velocity progressive-alignment framework for few-shot action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UBTransformer: Uncertainty-based transformer model for complex scenarios detection in autonomous driving. <em>TMM</em>, <em>27</em>, 6581-6592. (<a href='https://doi.org/10.1109/TMM.2025.3586103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional object detection algorithm in the intelligent vehicle perception system cannot maintain stable recognition performance in the unknown and changing road environment. We find that uncertainty quantification is of great significance in detecting unknown complex environments and helps to improve the robustness and safety of autonomous driving systems. Therefore, this paper proposes an Uncertainty-based Transformer (UBT) object detection algorithm. Firstly, the double Gaussian feature map network (DGF) is designed to quantify and utilize the uncertainty of the features derived from the backbone network. Secondly, we propose a RBF-based query filtering model(RBQF), which takes uncertainty sum as the index of query vector screening. At the same time, this paper proposes an uncertainty detection head (UDH); the final model output results are quantitative uncertainty, improved detection performance and enhanced algorithm reliability. To further prove the detection performance of the proposed method in real driving scenes, we use COCO, Cityscapes, FoggyCityscapes, RainCityscapes and self-made traffic scene datasets for verification, which shows that our algorithm is well applicable to large datasets and complex road scenes.},
  archive      = {J_TMM},
  author       = {Ke Wang and Qi Ma and Xingcan Li and Chongqiang Shen and Rui Leng and Jianbo Lu},
  doi          = {10.1109/TMM.2025.3586103},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6581-6592},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {UBTransformer: Uncertainty-based transformer model for complex scenarios detection in autonomous driving},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthesizing multi-person and rare pose images for human pose estimation. <em>TMM</em>, <em>27</em>, 6568-6580. (<a href='https://doi.org/10.1109/TMM.2025.3586122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) models underperform in recognizing rare poses because they suffer from data imbalance problems (i.e., there are few image samples for rare poses) in their training datasets. From a data perspective, the most intuitive solution is to synthesize data for rare poses. Specifically, the rule-based methods apply manual manipulations (such as Cutout and GridMask) to the existing data, so the limited diversity of the data constrains the model. An alternative method is to learn the underlying data distribution via deep generative models (such as ControlNet and HumanSD) and then sample “new data” from the distribution. This works well for generating frequent poses in common scenes, but suffers when applied to rare poses or complex scenes (such as multiple persons with overlapping limbs). In this paper, we aim to address the above two issues, i.e., rare poses and complex scenes, for person image generation. We propose a two-stage method. In the first stage, we design a controllable pose generator named PoseFactory to synthesize rare poses. This generator is specifically trained on augmented pose data, and each pose is labelled with its level of difficulty and rarity. In the second stage, we introduce a multi-person image generator named MultipGenerator. It is conditioned on multiple human poses and textual descriptions of complex scenes. Both stages are controllable in terms of the diversity of poses and the complexity of scenes. For evaluation, we conduct extensive experiments on three widely used datasets: MS-COCO, HumanArt, and OCHuman. We compare our method against traditional pose data augmentation and person image generation methods, and it demonstrates its superior performance both quantitatively and qualitatively.},
  archive      = {J_TMM},
  author       = {Liuqing Zhao and Zichen Tian and Peng Zou and Richang Hong and Qianru Sun},
  doi          = {10.1109/TMM.2025.3586122},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6568-6580},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Synthesizing multi-person and rare pose images for human pose estimation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing light field salient object detection with variance-maximized key focal slice selection. <em>TMM</em>, <em>27</em>, 6555-6567. (<a href='https://doi.org/10.1109/TMM.2025.3586131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field saliency object detection (LF SOD) methods have made significant progress recently. Most of them explore abundant multi-modal information from the all-focus image and the focal stacks at all focal planes to enrich scene details and depth perception. However, in light-field images, the spatial and depth information varies slightly across different slices, raising redundancy within focal stacks. Besides, the noise can appear repeatedly in multiple images of the focal stacks, which brings interference. To address these issues, in this work, we propose VMKNet, an effective approach that leverages innovative variance-maximized key slice selection and interacts with the all-focus image, to improve LF SOD. Specifically, we measure consistency differences between the all-focus image and each focal slice in the salient region as saliency scores. Then, we randomly assemble sets of them, where each score corresponds to a certain slice. The one exhibiting the highest variance is singled out to determine key focal slices as they reveal the diversity of salient objects. Then, the bidirectional guidance module (BGM) is presented to learn attentive features of all-focus and selected key slices in a mutual guidance manner, thus producing enhanced and holistic features. With hierarchical BGMs, our model can progressively aggregate common salient semantics and meaningful contextual details, generating more discriminative representations. Moreover, we introduce the edge enhancement module in conjunction with BGM to improve the sharpness of saliency maps. Extensive experiments on common light field datasets demonstrate that our method, termed VMKNet, outperforms recent state-of-the-art LF, RGB-D, and RGB methods.},
  archive      = {J_TMM},
  author       = {Jiaxin Han and Feng Li and Anqi Li and Mengmeng Zhang and Huihui Bai and Jimin Xiao and Yao Zhao},
  doi          = {10.1109/TMM.2025.3586131},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6555-6567},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing light field salient object detection with variance-maximized key focal slice selection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeMaskFormer: Adapting mask transformer for semantic edge detection. <em>TMM</em>, <em>27</em>, 6543-6554. (<a href='https://doi.org/10.1109/TMM.2025.3586134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic Edge Segmentation (SED) is crucial for intelligent agents to understand and interact with their environments, as it enables them to locate and recognize semantic boundaries. The prevailing framework in the field of SED is multi-label learning, which identifies edges and their semantics by learning to assign multiple labels that indicate the categories of the objects forming the edges. However, this framework has demonstrated limited performance when dealing with complex scenarios. In this paper, we propose a mask classification framework specifically tailored for the SED task, termed EdgeMaskFormer. Within this framework, we develop a query-based edge semantic extractor to learn semantic embeddings for edge mask classification with assistance from regional semantic supervision. Additionally, we design a context-aware hierarchical edge extractor to serve as an edge mask head, which can capture multi-scale edges of different categories under the guidance from the semantic embeddings via dynamic convolution. Furthermore, we develop matching and supervision mechanisms specifically for edge mask classification in order to reduce edge noise and address the imbalance between edge and non-edge samples. Our extensive experiments on three public datasets demonstrate that the proposed approach achieves outstanding performance in semantic edge detection, particularly on those datasets with complex scenarios.},
  archive      = {J_TMM},
  author       = {Lijun Dong and Wei Ma and Hongbin Zha},
  doi          = {10.1109/TMM.2025.3586134},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6543-6554},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EdgeMaskFormer: Adapting mask transformer for semantic edge detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRSOT: Cross-resolution object tracking using unaligned frame and event cameras. <em>TMM</em>, <em>27</em>, 6529-6542. (<a href='https://doi.org/10.1109/TMM.2025.3586135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing datasets for RGB-DVS tracking are collected with DVS346 camera and their resolution ($346 \times 260$) is low for practical applications. Actually, only visible cameras are deployed in many practical systems, and the newly designed neuromorphic cameras may have different resolutions. The latest neuromorphic sensors can output high-definition event streams, but it is very difficult to achieve strict alignment between events and frames on both spatial and temporal views. Therefore, how to achieve accurate tracking with unaligned neuromorphic and visible sensors is a valuable but unresearched problem. In this work, we formally propose the task of object tracking using unaligned neuromorphic and visible cameras. We build the first unaligned frame-event dataset CRSOT collected with a specially built data acquisition system, which contains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In addition, we propose a novel unaligned object tracking framework that can realize robust tracking even using the loosely aligned RGB-Event data. This proposed method utilizes uncertainty perception techniques, which can effectively reduce the negative impact of noise (especially noise in event data) on tracking performance. Specifically, we extract the template and search regions of RGB and Event data and feed them into a unified ViT backbone for feature embedding. Next, we propose uncertainty perception modules to encode the RGB and Event features, respectively, then, we propose a modality uncertainty fusion module to aggregate the two modalities. These three branches are jointly optimized in the training phase. Extensive experiments demonstrate that our tracker can collaborate the dual modalities for high-performance tracking even without strictly temporal and spatial alignment.},
  archive      = {J_TMM},
  author       = {Yabin Zhu and Xiao Wang and Chenglong Li and Bo Jiang and Lin Zhu and Zhixiang Huang and Yonghong Tian and Jin Tang},
  doi          = {10.1109/TMM.2025.3586135},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6529-6542},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CRSOT: Cross-resolution object tracking using unaligned frame and event cameras},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond 3D: Generic IoU for 3D object detection. <em>TMM</em>, <em>27</em>, 6516-6528. (<a href='https://doi.org/10.1109/TMM.2025.3586127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection from point clouds is a fundamental task for 3D scene understanding and has a wide range of applications in the field of multimedia data processing and analysis, such as autonomous driving and virtual interaction. The IoU evaluates the overlap between the two bounding boxes to ensure consistency across network optimization and testing, becoming a recognized regression loss in the field of 3D object detection. However, there is a kind of error coupling between the IoU and the angle, i.e., the IoU does not decrease as the angle error increases and vice versa. This problem leads to sub-optimal solutions for the neural network model, which severely hampers the improvement of 3D object detection accuracy. In this paper, a novel 4DIoU method is introduced for detecting 3D objects from point clouds, which provides a comprehensive rethinking of IoU computation by integrating angular information as an additional dimension. 4DIoU not only solves the problem of error coupling between IoU and angular but also facilitates neural network optimization using angle information. Furthermore, to solve the different impacts of various object shapes on IoU variations, a special 4DIoU called TV4DIoU is proposed to fuse shape information based on three orthogonal projection views, which can adaptively learn the information of objects with different shapes. In addition, to enhance the generalization of the 4DIoU method, a high-flexibility anchor encoding method and a cyclic consistent computation formula for angular errors are designed to make 4DIoU a plug-and-play module for both anchor-based and anchor-free frameworks. Extensive evaluations conducted on the nuScenes, Waymo, and KITTI datasets have confirmed the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Hengsheng Lun and Ke Lu and Liping Hou and Shuhua Wang and Jian Xue},
  doi          = {10.1109/TMM.2025.3586127},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6516-6528},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Beyond 3D: Generic IoU for 3D object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and controllable privacy-preserving image retrieval in multi-user settings. <em>TMM</em>, <em>27</em>, 6503-6515. (<a href='https://doi.org/10.1109/TMM.2025.3586149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud environments, privacy-preserving content-based image retrieval (PPCBIR) enables users to retrieve images, while protecting image privacy. Existing PPCBIR systems often use a single image key, which causes low efficiency and makes it difficult to achieve fine-grained access control over images. This paper proposes a lightweight and controllable privacy-preserving image retrieval in multi-user settings (named LCPIRM) to improve time efficiency and access control performance. A one-time image encryption method based on reversible embedding is proposed to balance the contradiction between complexity and security without increasing the difficulty of key management. A robust hash generation method is designed by combining piece-wise mean quantization and encryption image features, which can effectively improve retrieval efficiency because the robust hashes embedded in the encrypted images can be extracted and establish inverted indexing in the cloud. When dealing with authorized encrypted images, the cloud server uses proxy re-encryption to convert the image keys embedded within themselves from the owner’s public key protection to the authorized user’s public key protection, achieving fine-grained access control over images in a multi-user setting. Theoretical analysis and experimental results show that LCPIRM has better performance in terms of retrieval accuracy, consumption, and search efficiency while meeting security requirements. In the real datasets Caltech256 and Caltech101, the search efficiency has increased by 74% and 58% respectively compared to the existing schemes.},
  archive      = {J_TMM},
  author       = {Zhuo Feng and Hongjie He and Fan Chen and Jie Bai},
  doi          = {10.1109/TMM.2025.3586149},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6503-6515},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Lightweight and controllable privacy-preserving image retrieval in multi-user settings},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video segmentation and tokenization for model-based video scene classification. <em>TMM</em>, <em>27</em>, 6489-6502. (<a href='https://doi.org/10.1109/TMM.2025.3595019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel approach for segmenting and tokenizing a video scene recording into a sequence of cascade units, known as visual segment units and modeled with visual segment models (VSMs) for video scene classification (VSC). Specifically, the proposed VSM framework takes deep visual features extracted from pre-trained encoders as inputs and models the temporal interactions between segment units by hidden Markov models. Next, we use unit co-occurrence statistics to introduce relationships between VSM units within a video scene recording. Furthermore, the VSM approach is extended to an acoustic-visual variant, subsequently integrating itself into a deep learning-based multi-modal scene classification system. This combination serves to further exploit the complementary nature of audio and video data. By incorporating a set of visual segment units into modeling a video scene class, it captures both inter-class similarity and intra-class diversity, facilitating improved scene classification, especially within categories prone to confusion. Extensive experimental results on a benchmark published by the DCASE (Detection and Classification of Acoustic Scenes and Events) 2021 Challenge show that the proposed framework can effectively handle the confusion issue among similar video scenes. In addition, our multi-modal integration system achieves state-of-the-art performance in the audio-visual scene classification task in the DCASE 2021 Challenge, thereby demonstrating the effectiveness of our proposed approach.},
  archive      = {J_TMM},
  author       = {Qing Wang and Yajian Wang and Hang Chen and Shuxian Wang and Jun Du and Chin-Hui Lee},
  doi          = {10.1109/TMM.2025.3595019},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6489-6502},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video segmentation and tokenization for model-based video scene classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCIGeo: Cross-view and cross-day-night image geo-localization using daytime image supervision. <em>TMM</em>, <em>27</em>, 6475-6488. (<a href='https://doi.org/10.1109/TMM.2025.3586124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-view image geo-localization is a technique to determine the geographic location of the query image by matching it with geo-tagged aerial images. However, when the query image is captured at nighttime, the existing methods could not extract geographic-related information from low and uneven illumination areas effectively, thus geo-localizing the nighttime ground image with poor performance. In this work, we propose a cross-view and cross-day-night image geo-localization method (CCIGeo), which contains three branches, taking the query nighttime ground image, the supervision daytime ground image, and the reference satellite image as inputs, respectively. Inspired by knowledge distillation, the proposed method takes daytime ground image branch as the teacher model, which would supervise the nighttime ground image branch to overcome the interference of the uneven and low illumination, and pay more attention to the areas containing rich geographic-related information. And to better adapt to the cross-day-night environment, a dual-constraint loss function is designed inspired by the concept of knowledge distillation. Extensive experimental results show that CCIGeo significantly improves the performance on nighttime image geo-localization, exceeding the state-of-the-art (SOTA) methods by 1.83%, 3.84%, and 1.64% on three datasets.},
  archive      = {J_TMM},
  author       = {Nan Wu and Chunfang Yang and Baojun Qi and Ma Zhu and Jiangshan Li and Xiangyang Luo},
  doi          = {10.1109/TMM.2025.3586124},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6475-6488},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CCIGeo: Cross-view and cross-day-night image geo-localization using daytime image supervision},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCFMN: Hierarchical cross-modal fine-grained mining network for temporal sentence grounding. <em>TMM</em>, <em>27</em>, 6462-6474. (<a href='https://doi.org/10.1109/TMM.2025.3586156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Sentence Grounding (TSG) requires a thorough understanding of the complex cross-modal semantic relationships between videos and text. However, existing methods fail to accurately capture content at diverse granularity levels with distinct semantics, making it difficult to achieve fine alignment of visuals and text. To overcome this issue, we attempt to mine for rich semantic clues by utilizing the hierarchical correspondence structure and multi-granularity visual-to-text reconstruction, achieving fine-grained reasoning. Specifically, for the TSG task, we propose a novel Hierarchical Cross-modal Fine-grained Mining Network (HCFMN), which utilizes an attention mechanism based on temporal hierarchical relationships to extract temporal features corresponding to the text of different granularities. We leverage the reconstructability of visual-to-text, recovering multi-granularity textual content from coarse to fine by focusing on temporal features at different layers, hierarchically extracting temporal features and the dependencies related to the text, and implementing fine-grained cross-modal semantic alignment. Furthermore, HCFMN introduces a novel partitioned efficient attention mechanism, which significantly enhances the model’s efficiency through a two-stage attention based on sequence and channel compression. Extensive experimental results on three public datasets (ActivityNet-Captions, TACoS, and Charades-STA) demonstrate that the proposed method achieves state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Ran Ran and Jiwei Wei and Yuyang Zhou and Xiang Guan and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3586156},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6462-6474},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HCFMN: Hierarchical cross-modal fine-grained mining network for temporal sentence grounding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised semantic segmentation with multi-constraint consistency learning. <em>TMM</em>, <em>27</em>, 6449-6461. (<a href='https://doi.org/10.1109/TMM.2025.3586111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistency regularization has prevailed in semi-supervised semantic segmentation and achieved promising performance. However, existing methods typically concentrate on enhancing the Image-augmentation based Prediction consistency and optimizing the segmentation network as a whole, resulting in insufficient utilization of potential supervisory information. In this paper, we propose a Multi-Constraint Consistency Learning (MCCL) approach to facilitate the staged enhancement of the encoder and decoder. Specifically, we first design a feature knowledge alignment (FKA) strategy to promote the feature consistency learning of the encoder from image-augmentation. Our FKA encourages the encoder to derive consistent features for strongly and weakly augmented views from the perspectives of point-to-point alignment and prototype-based intra-class compactness. Moreover, we propose a self-adaptive intervention (SAI) module to increase the discrepancy of aligned intermediate feature representations, promoting Feature-perturbation based Prediction consistency learning. Self-adaptive feature masking and noise injection are designed in an instance-specific manner to perturb the features for robust learning of the decoder. Experimental results on Pascal VOC2012 and Cityscapes datasets demonstrate that our proposed MCCL achieves new state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Jianjian Yin and Tao Chen and Gensheng Pei and Huafeng Liu and Yazhou Yao and Liqiang Nie and Xiansheng Hua},
  doi          = {10.1109/TMM.2025.3586111},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6449-6461},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semi-supervised semantic segmentation with multi-constraint consistency learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust coverless video steganography based on two-level DCT features against video attacks. <em>TMM</em>, <em>27</em>, 6434-6448. (<a href='https://doi.org/10.1109/TMM.2025.3586104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with traditional video steganography, coverless video steganography (CVS) can completely avoid being detected by steganalysis algorithms. Recently, the study of CVS has developed rapidly. However, it is still far from the theoretical maximum values in capacity, i.e., the theoretical limit is $2^\ell$ for a hash sequence length of $\ell$. Besides, most existing CVS methods have only considered limited types of video attacks in robustness. In this paper, a novel coverless video steganography based on two-level discrete cosine transform (DCT) features is proposed. First, pre-processing is accomplished on the public video datasets. Then, two-level DCT features are calculated and the Coverless Video Database (CVD) is constructed by the K-means++ clustering algorithm. After that, the mapping table is established to map the secret segments to the CVD. Finally, each secret segment corresponds to a video sequence in the CVD by the mapping table to complete the process of information embedding and extraction. The proposed method first evaluates the robustness against the frame swapping attack, which is a common video attack. Experimental results show that the proposed method can achieve the theoretical maximum value in effective capacity and better robustness compared to the state-of-the-art works.},
  archive      = {J_TMM},
  author       = {Laijin Meng and Xinghao Jiang and Qiang Xu and Tanfeng Sun},
  doi          = {10.1109/TMM.2025.3586104},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6434-6448},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A robust coverless video steganography based on two-level DCT features against video attacks},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LCNet: Lightweight cycle network driven by physical and deep prior for compressed sensing. <em>TMM</em>, <em>27</em>, 6422-6433. (<a href='https://doi.org/10.1109/TMM.2025.3586145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) networks have recently achieved excellent performance on image compressed sensing. However, most existing methods rely on burdened and complex network structures, resulting in significant computational and storage requirements that defeat the purpose of compressed sensing. This severely hinders their applicability in real-world resource-limited devices. In this paper, a lightweight cycle network driven by physical and deep priors for image compressed sensing is proposed which integrates the learning of the sensing matrix and compressive image reconstruction. Specifically, the regularization terms and a likelihood term derived from the physical observation model are learned in an end-to-end cycle network, simultaneously estimating the reconstructed image and sensing matrix in the image and feature domains. Moreover, a dual-domain fusion reconstruction module is proposed. It creates simulated measurement residuals for enhancing reconstruction in the compressed domain, which leads to high reconstruction performance and reduces computational load by bonding together the compressed image domains in the cyclic network. Extensive experiments demonstrate that our model delivers superior performance and alleviates model complexity, which is of great importance in low-budget applications.},
  archive      = {J_TMM},
  author       = {Shuowen Yang and Fernando Pérez-Bueno and Hanlin Qin and Rafael Molina and Aggelos K. Katsaggelos},
  doi          = {10.1109/TMM.2025.3586145},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6422-6433},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LCNet: Lightweight cycle network driven by physical and deep prior for compressed sensing},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring transferability of multimodal adversarial samples for vision-language pre-training models with contrastive learning. <em>TMM</em>, <em>27</em>, 6410-6421. (<a href='https://doi.org/10.1109/TMM.2025.3581811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of visual and textual data in Vision- Language Pre-training (VLP) models is crucial forenhancing vision-language understanding. However, the adversarial robustness of these models, especially in the alignment of image-text features, has not yet been sufficiently explored. In this paper, we introduce a novel gradient-based multimodal adversarial attack method, underpinned by contrastive learning, to improve the transferability of multimodal adversarial samples in VLP models. This method concurrently generates adversarial texts and images within imperceptive perturbation, employing both image-text and intra-modal contrastive loss. We evaluate the effectiveness of our approach on image-text retrieval and visual entailment tasks, using publicly available datasets in a black-box setting. Extensive experiments indicate a significant advancement over existing single-modal transfer-based adversarial attack methods and current multimodal adversarial attack approaches.},
  archive      = {J_TMM},
  author       = {Youze Wang and Wenbo Hu and Yinpeng Dong and Hanwang Zhang and Hang Su and Richang Hong},
  doi          = {10.1109/TMM.2025.3581811},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6410-6421},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring transferability of multimodal adversarial samples for vision-language pre-training models with contrastive learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible optimal transport with contrastive graphical modeling for multimodal hate detection. <em>TMM</em>, <em>27</em>, 6397-6409. (<a href='https://doi.org/10.1109/TMM.2025.3581795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal hate detection plays a crucial role in maintaining harmonious online environments by identifying harmful content, such as hateful memes. Although previous research has made significant progress in detecting explicit hate speech, there remains a critical gap in analyzing implicit hate, which is particularly challenging due to the absence of explicit harmful text claims or demographic visual cues. Despite the promising results based on cross-modal attention, previous methods may suffer from the distributional modality gap caused by the non-literal associations between multimodal elements, which lacks apparent alignment in implicit hateful contents. In this work, we propose a novel framework: Flexible Optimal Transport (FLOT) to capture the non-literal cross-modal alignment for multimodal hate in the context of memes. FLOT formulates the problem of cross-modal alignment as finding optimal transportation plans, which leverages a kernel method to capture complementary information from multiple modalities. The kernel embeddings reproduce a kernel Hilbert space (RKHS) to serve as a non-linear transformation of alignment, which effectively reduces the distributional modality gap with more interpretability. Moreover, we established topological structures with contrastive modeling for the aligned representations, which are optimized to achieve comprehensive alignment between different modalities, and facilitate local reasoning based on multimodal elements. Experimental results have demonstrated that our FLOT achieved state-of-the-art performance on three publicly available benchmark datasets. Furthermore, extensive qualitative analysis confirms the superior ability of FLOT in capturing implicit cross-modal alignment.},
  archive      = {J_TMM},
  author       = {Linhao Zhang and Li Jin and Xiaoyu Li and Xian Sun and Xin Wang and Zequn Zhang and Jian Liu and Zhicong Lu and Guangluan Xu},
  doi          = {10.1109/TMM.2025.3581795},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6397-6409},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Flexible optimal transport with contrastive graphical modeling for multimodal hate detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointMT: Efficient point cloud analysis with hybrid MLP-transformer architecture. <em>TMM</em>, <em>27</em>, 6382-6396. (<a href='https://doi.org/10.1109/TMM.2025.3581747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, point cloud analysis methods based on the Transformer architecture have made significant progress, particularly in the context of multimedia applications such as 3D modeling, virtual reality, and autonomous systems. However, the Transformer architecture’s high computational demands limit its scalability and deployment on resource-constrained platforms, hindering its practical applications in on-device multimedia processing. To address this challenge, we propose an efficient point cloud analysis architecture, Point MLP-Transformer (PointMT). This study tackles the quadratic complexity of the self-attention mechanism by introducing a linear complexity local attention mechanism for effective feature aggregation. Additionally, to counter the Transformer’s focus on token differences while neglecting channel differences, we introduce a parameter-free channel temperature adaptation mechanism that adaptively adjusts the attention weight distribution in each channel, enhancing the precision of feature aggregation. To improve the Transformer’s slow convergence speed due to the limited scale of point cloud datasets, we propose an MLP-Transformer hybrid module, which significantly enhances the model’s convergence speed. Furthermore, to boost the feature representation capability of point tokens, we refine the classification head, enabling point tokens to directly participate in prediction. Experimental results on multiple evaluation benchmarks demonstrate that PointMT achieves performance comparable to state-of-the-art methods while maintaining an optimal balance between performance and accuracy. This research provides an innovative solution for efficient point cloud analysis, offering significant potential for multimedia applications and other domains.},
  archive      = {J_TMM},
  author       = {Qiang Zheng and Chao Zhang and Jian Sun},
  doi          = {10.1109/TMM.2025.3581747},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6382-6396},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PointMT: Efficient point cloud analysis with hybrid MLP-transformer architecture},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust multi-stage tracking via multi-scale and multi-level representation learning. <em>TMM</em>, <em>27</em>, 6369-6381. (<a href='https://doi.org/10.1109/TMM.2025.3581755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to learn multi-scale and multi-level representations is crucial for robust tracking. However, most current one-stream structure based trackers with visual transformers (dubbed ViTs) cannot effectively capture multi-scale representations due to the structure of their adopted ViTs is non-hierarchical. Meanwhile, they often only use the output features from the final layer for predicting results (i.e., ignoring the utilization of low-level features from the shallow layers) which may result in a certain degree of lacking multi-level representation learning ability. To address these issues, we propose a robust multi-stage tracker that effectively combines the advantages of both hierarchical and one-stream structured ViT as a tracking backbone to improve the multi-scale and multi-level representation learning abilities. Specifically, first of all, we design a hierarchical tracker with a three-stage backbone. In the first two stages of our tracker, we utilize a dual-branch structure to obtain multi-scale features of the template and search region separately. Especially, We design the local scale awareness modules based on simple MLP layers to capture multi-scale features. These modules remove complex operations such as convolutions or shifted window attentions, thus avoiding the performance degradation caused by traditional hierarchical ViTs. In the third stage (i.e. the main stage), we construct a global encoder based on the one-stream ViT to achieve efficient feature extraction and feature interaction for our tracker. Then, we design a multi-level feature integration module in the main stage to explicitly utilize the representation information learned from the shallow layers and fuse them with the features of the final layer to obtain multi-level representation information. Lastly, benefit from the these designs, our tracker can effectively capture more multi-scale and multi-level representations for robust tracking. Comprehensive experiments on GOT-10 k, LaSOT, LaSOT$_{ext}$, TNL2K, UAV123, TrackingNet and VOT2020 benchmarks validate the effectiveness and robustness of our method.},
  archive      = {J_TMM},
  author       = {Ning Li and Bineng Zhong and Qihua Liang and Zhiyi Mo and Shuxiang Song},
  doi          = {10.1109/TMM.2025.3581755},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6369-6381},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust multi-stage tracking via multi-scale and multi-level representation learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Programmable-room: Interactive textured 3D room meshes generation empowered by large language models. <em>TMM</em>, <em>27</em>, 6358-6368. (<a href='https://doi.org/10.1109/TMM.2025.3581748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Programmable-Room, a framework which interactively generates and edits a 3D room mesh, given natural language instructions. For precise control of a room’s each attribute, we decompose the challenging task into simpler steps such as creating plausible 3D coordinates for room meshes, generating panorama images for the texture, constructing 3D meshes by integrating the coordinates and panorama texture images, and arranging furniture. To support the various decomposed tasks with a unified framework, we incorporate visual programming (VP). VP is a method that utilizes a large language model (LLM) to write a Python-like program which is an ordered list of necessary modules for the various tasks given in natural language. We develop most of the modules. Especially, for the texture generating module, we utilize a pretrained large-scale diffusion model to generate panorama images conditioned on text and visual prompts (i.e., layout, depth, and semantic map) simultaneously. Specifically, we enhance the panorama image generation quality by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM. We demonstrate Programmable-Room’s flexibility in generating and editing 3D room meshes, and prove our framework’s superiority to an existing model quantitatively and qualitatively.},
  archive      = {J_TMM},
  author       = {Jihyun Kim and Junho Park and Kyeongbo Kong and Suk-Ju Kang},
  doi          = {10.1109/TMM.2025.3581748},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6358-6368},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Programmable-room: Interactive textured 3D room meshes generation empowered by large language models},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTR: A unified deep tensor representation framework for multimedia data recovery. <em>TMM</em>, <em>27</em>, 6347-6357. (<a href='https://doi.org/10.1109/TMM.2025.3581777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the transform-based tensor representation has attracted increasing attention in multimedia data (e.g., images and videos) recovery problems, which consists of two indispensable components, i.e., the transform and the characterization. Previously, the development of transform-based tensor representation has focused mainly on the transform perspective. Although several attempts have considered shallow matrix factorization (e.g., singular value decomposition and nonnegative matrix factorization) for characterizing the frontal slices of the transformed tensor (termed the latent tensor), the faithful characterization perspective has been underexplored. To address this issue, we propose a unified Deep Tensor Representation (DTR) framework by synergistically combining the deep latent generative module and the deep transform module. Especially, the deep latent generative module can faithfully generate the latent tensor as compared with shallow matrix factorization. The new DTR framework not only allows us to better understand the classical shallow representations but also leads us to explore new representations. To examine the representation capability of the proposed DTR, we consider the representative multidimensional data recovery task and suggest an unsupervised DTR-based multidimensional data recovery model. Extensive experiments demonstrate that DTR achieves superior performance compared to the state-of-the-art methods from both quantitative and qualitative aspects, especially for fine detail recovery.},
  archive      = {J_TMM},
  author       = {Ting-Wei Zhou and Xi-Le Zhao and Jian-Li Wang and Yi-Si Luo and Min Wang and Xiao-Xuan Bai and Hong Yan},
  doi          = {10.1109/TMM.2025.3581777},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6347-6357},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DTR: A unified deep tensor representation framework for multimedia data recovery},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TalkCLIP: Talking head generation with text-guided expressive speaking styles. <em>TMM</em>, <em>27</em>, 6335-6346. (<a href='https://doi.org/10.1109/TMM.2025.3581808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-driven talking head generation has drawn growing attention. To produce talking head videos with desired facial expressions, previous methods rely on extra reference videos to provide expression information, which may be difficult to find and hence limits their usage. In this work, we propose TalkCLIP, a framework that can generate talking heads where the expressions are specified by natural language, hence allowing for specifying expressions more conveniently. To model the mapping from text to expressions, we first construct a text-video paired talking head dataset where each video has diverse text descriptions that depict both coarse-grained emotions and fine-grained facial movements. Leveraging the proposed dataset, we introduce a CLIP-based style encoder that projects natural language-based descriptions to the representations of expressions. TalkCLIP can even infer expressions for descriptions unseen during training. TalkCLIP can also use text to modulate expression intensity and edit expressions. Extensive experiments demonstrate that TalkCLIP achieves the advanced capability of generating photo-realistic talking heads with vivid facial expressions guided by text descriptions.},
  archive      = {J_TMM},
  author       = {Yifeng Ma and Suzhen Wang and Yu Ding and Bowen Ma and Tangjie Lv and Changjie Fan and Zhipeng Hu and Zhidong Deng and Xin Yu},
  doi          = {10.1109/TMM.2025.3581808},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6335-6346},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TalkCLIP: Talking head generation with text-guided expressive speaking styles},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plug-in open-set cross-modal hashing. <em>TMM</em>, <em>27</em>, 6319-6334. (<a href='https://doi.org/10.1109/TMM.2025.3581813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Cross-Modal Hashing (UCMH) models the intrinsic semantic correlations across different modalities to generate binary hash codes, facilitating efficient cross-modal retrieval. This technology offers notable advantages, such as independence from labeled data and superior generalization capabilities compared to supervised methods. However, most UCMH methods are designed for closed-set retrieval scenarios and have difficulty generalizing to open multi-modal data, which is common in real-world retrieval settings. This limitation hampers their performance in open retrieval tasks, particularly when these tasks involve novel categories. To address the above issue, we propose an Open-set Cross-Modal Hashing (OCMH) method, which enhances the generalization capability of trained UCMH models in an efficient plug-in manner for open cross-modal retrieval. Our method enables the model to learn from novel categories in open-set scenarios by increasing the pre-defined hash code length, while simultaneously preventing the catastrophic forgetting of trained knowledge from the closed-set domain using basic hash codes. Additionally, we introduce a historical-category detection module and an asymmetric optimization strategy to support the joint learning of basic and increased hash codes by replaying detected samples related to historical categories. By plugging our proposed method into several representative UCMH methods on three widely used datasets, experimental results show that the enhanced UCMH methods achieve superior retrieval performance in both open-set and closed-set scenarios.},
  archive      = {J_TMM},
  author       = {Bowen Wang and Lei Zhu and Fengling Li and Hui Cui and Jingjing Li},
  doi          = {10.1109/TMM.2025.3581813},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6319-6334},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Plug-in open-set cross-modal hashing},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlexVLN: Flexible adaptation for diverse vision-and-language navigation tasks. <em>TMM</em>, <em>27</em>, 6307-6318. (<a href='https://doi.org/10.1109/TMM.2025.3581809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aspiration of the Vision-and-Language Navigation (VLN) task has long been to develop an embodied agent with robust adaptability, capable of seamlessly transferring its navigation capabilities across various tasks. Despite remarkable advancements in recent years, most methods necessitate dataset-specific training, thereby lacking the capability to generalize across diverse datasets encompassing distinct types of instructions. Large language models (LLMs) have demonstrated exceptional reasoning and generalization abilities, exhibiting immense potential in robot action planning. In this paper, we propose FlexVLN, an innovative hierarchical approach to VLN that integrates the fundamental navigation ability of a supervised-learning-based Instruction Follower with the robust generalization ability of the LLM Planner, enabling effective generalization across diverse VLN datasets. Moreover, a verification mechanism and a multi-model integration mechanism are proposed to mitigate potential hallucinations by the LLM Planner and enhance execution accuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as out-of-domain datasets for assessing generalization ability. The generalization performance of FlexVLN surpasses that of all the previous methods to a large extent.},
  archive      = {J_TMM},
  author       = {Siqi Zhang and Yanyuan Qiao and Qunbo Wang and Longteng Guo and Zhihua Wei and Jing Liu},
  doi          = {10.1109/TMM.2025.3581809},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6307-6318},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FlexVLN: Flexible adaptation for diverse vision-and-language navigation tasks},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-enhanced video moment retrieval with large language models. <em>TMM</em>, <em>27</em>, 6296-6306. (<a href='https://doi.org/10.1109/TMM.2025.3581797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methods for Video Moment Retrieval (VMR) struggle to align complex situations involving specific environmental details, character descriptions, and action narratives. To tackle this issue, we propose a Large Language Model-guided Moment Retrieval (LMR) approach that employs the extensive knowledge of Large Language Models (LLMs) to improve video context representation as well as cross-modal alignment, facilitating accurate localization of target moments. Specifically, LMR introduces a context enhancement technique with LLMs to generate crucial target-related context semantics. These semantics are integrated with visual features for producing discriminative video representations. Finally, a language-conditioned transformer is designed to decode free-form language queries, on the fly, using aligned video representations for moment retrieval. Extensive experiments demonstrate that LMR achieves state-of-the-art results, outperforming the nearest competitor by up to 3.28% and 4.06% on the challenging QVHighlights and Charades-STA benchmarks, respectively. More importantly, the performance gains are significantly higher for localization of complex queries.},
  archive      = {J_TMM},
  author       = {Weijia Liu and Bo Miao and Jiuxin Cao and Xuelin Zhu and Jiawei Ge and Bo Liu and Mehwish Nasim and Ajmal Mian},
  doi          = {10.1109/TMM.2025.3581797},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6296-6306},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Context-enhanced video moment retrieval with large language models},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEHand: Deformable encoding for photo-realistic free-view and free-pose hand rendering. <em>TMM</em>, <em>27</em>, 6284-6295. (<a href='https://doi.org/10.1109/TMM.2025.3581749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Input encoding has proven crucial in the success of methods based on neural radiance field. Compared to the literature on general static scene modeling, input encoding for dynamic hand modeling has been less explored. However, this aspect is critical to the modeling of deformation and rendering, as it maps a sampled point in space to the representation containing all the information associated with dynamic hand for inferring the geometry and appearance property of this point. The design of input encoding determines how well the neural network can learn for photo-realistic hand rendering. We offer an in-depth examination of this key component and introduce DEHand, a new representation utilizing Deformable Encoding for photo-realistic free-view and free-pose Hand rendering. DEHand leverages deformable encoding with a latent code map to achieve high-quality, pose-controlled rendering. Deformable encoding is achieved by adapting static input encoding techniques for the view synthesis of dynamic hands, using parametric hand mesh model as a proxy to construct encodings that map sampled points into a space capable of integrating over different poses and providing rich information for hand modeling. Our findings demonstrate that with our deformable encoding, a single Multilayer Perceptron (MLP) can achieve high-quality dynamic hand rendering, learning solely from images. Extensive experiments on InterHand2.6 M validate the superior rendering quality of our method and the effectiveness of each component in our design.},
  archive      = {J_TMM},
  author       = {Yunzhi Teng and Xiaoke Huang and Kejie Li and Xiao-Ping Zhang and Yansong Tang},
  doi          = {10.1109/TMM.2025.3581749},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6284-6295},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DEHand: Deformable encoding for photo-realistic free-view and free-pose hand rendering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WI3D: Weakly incremental 3D detection via vision foundation models. <em>TMM</em>, <em>27</em>, 6273-6283. (<a href='https://doi.org/10.1109/TMM.2025.3581776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental 3D object detection demands a 3D detector to locate and recognize novel categories in a stream fashion while preserving its base detection ability. However, existing methods require delicate 3D annotations for learning novel categories, resulting in significant labeling costs. To this end, we explore a label-efficient approach called Weakly Incremental 3D Detection (WI3D), which teaches a 3D detector to learn incrementally with off-the-shelf vision foundation models. We propose a novel dual-teaching framework incorporating both intra-modal and inter-modal knowledge from pseudo labels and feature space. Specifically, our framework features a class-agnostic pseudo-label refinement module, designed for the generation of high-quality 3D pseudo labels. This module is built on a lightweight transformer that models the spatial relationships between pseudo labels and their interactions with rich contextual information in point clouds. Additionally, we introduce a cross-modal knowledge transfer module to enhance the representation learning of novel classes, along with a reweighting knowledge distillation strategy that dynamically assesses and distills knowledge from previously learned categories. Extensive experiments show that our approach can efficiently learn novel concepts while preserving knowledge of base classes in WI3D scenarios, and surpass baseline approaches on both SUN-RGBD and ScanNet.},
  archive      = {J_TMM},
  author       = {Mingsheng Li and Sijin Chen and Shengji Tang and Hongyuan Zhu and Yanyan Fang and Xin Chen and Zhuoyuan Li and Fukun Yin and Tao Chen},
  doi          = {10.1109/TMM.2025.3581776},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6273-6283},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WI3D: Weakly incremental 3D detection via vision foundation models},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFCM-AEG: Source-free cross-modal adversarial example generation. <em>TMM</em>, <em>27</em>, 6262-6272. (<a href='https://doi.org/10.1109/TMM.2025.3581781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel task of source-free cross-modal adversarial example generation, which generates adversarial examples based on textual descriptions of attackers. This task has two challenges as follows. First, how to generate adversarial examples when the clean examples are missing or inaccessible. Second, how to achieve fine-grained custom adversarial example generation according to the semantic descriptions of the attackers. Existing adversarial example generation methods can not effectively deal with these two challenges. To address these challenges, we propose a Source-Free Cross-Modal Adversarial Example Generation framework, abbreviated as SFCM-AEG. Within the SFCM-AEG model, we firstly leverage a pre-trained GPT as a simulator to construct textual descriptions of attackers by labels. Following this, we employ a diffusion model to synthesize an image that aligns with the generated textual description. Finally, the generated images are converted into adversarial examples using an adversarial example generation method. Experimental results demonstrate that our proposed SFCM-AEG method can generate adversarial examples with customized semantic descriptions, without relying on clean examples, while achieving strong attack performance in a white-box setting.},
  archive      = {J_TMM},
  author       = {Yan Gan and Xinyao Xiao and Tao Xiang and Chengqian Wu and Deqiang Ouyang},
  doi          = {10.1109/TMM.2025.3581781},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6262-6272},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SFCM-AEG: Source-free cross-modal adversarial example generation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical uncertainty-aware salient object detection for $360 ^{\circ }$ images via bi-projection collaborative learning. <em>TMM</em>, <em>27</em>, 6248-6261. (<a href='https://doi.org/10.1109/TMM.2025.3581812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a hierarchical uncertainty-aware $360 ^{\circ }$ image salient object detection methodology that explicitly explores the geometric and spatial complementary coherence of Tangent projection (TP) and Equirectangular projection (ERP) by a collaborative learning strategy. Concretely, to mitigate spherical distortion, we first intend to learn saliency-related features from less-distorted tangent images, in which a deformation-aware attention block is introduced to mitigate the geometric distortion caused by projecting a $360 ^{\circ }$ image onto a 2D plane. However, the discrepancies among tangent images pose a new challenge to $360 ^{\circ }$ image salient object detection. To tackle this issue and achieve accurate localization for salient objects of all sizes, we design a spatial-frequency saliency feature aggregation module to leverage fast Fourier convolution to capture global contextual information from ERP images, such that obtaining more representative saliency features. Moreover, a hierarchical uncertainty-aware bi-projection consistency learning module with strong local-global information embedding capabilities is constructed, which learns the geometric and spatial correlations between tangent images and ERP images via a collaborative learning strategy. Ultimately, salient object maps are produced for $360 ^{\circ }$ images on the basis of the merged saliency features driven by the uncertainty. Extensive experiments show that our developed method improves ${\mathrm{F}}_\beta ^{\sigma }$ by an average of 31.67% compared to twenty existing advanced methods on the publicly available 360-SOD dataset.},
  archive      = {J_TMM},
  author       = {Qiudan Zhang and Kaiyu Ji and Jie Zhang and Xu Wang and Zhaoqing Pan and Jianmin Jiang},
  doi          = {10.1109/TMM.2025.3581812},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6248-6261},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical uncertainty-aware salient object detection for $360 ^{\circ }$ images via bi-projection collaborative learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DGeoDet: General-purpose geometry-aware image-based 3D object detection. <em>TMM</em>, <em>27</em>, 6235-6247. (<a href='https://doi.org/10.1109/TMM.2025.3581780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection approach that effectively handles single- and multi-view RGB images in indoor and outdoor environments, showcasing its general-purpose applicability. The key challenge for image-based 3D object detection tasks is the lack of 3D geometric cues, which leads to ambiguity in establishing correspondences between images and 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D geometric representations in both explicit and implicit manners based on predicted depth information. Specifically, we utilize the predicted depth to learn voxel occupancy and optimize the voxelized 3D feature volume explicitly through the proposed voxel occupancy attention. To further enhance 3D awareness, the feature volume is integrated with an implicit 3D representation, the truncated signed distance function (TSDF). Without requiring supervision from 3D signals, we significantly improve the model’s comprehension of 3D geometry by leveraging intermediate 3D representations and achieve end-to-end training. Our approach surpasses the performance of state-of-the-art image-based methods on both single- and multi-view benchmark datasets across diverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D dataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19 $\text{AP}_{\text{3D}}$@0.7 improvement on the KITTI dataset.},
  archive      = {J_TMM},
  author       = {Yi Zhang and Yi Wang and Yawen Cui and Lap-Pui Chau},
  doi          = {10.1109/TMM.2025.3581780},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6235-6247},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3DGeoDet: General-purpose geometry-aware image-based 3D object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized skewed histogram shifting based reversible data hiding by differential evolution. <em>TMM</em>, <em>27</em>, 6221-6234. (<a href='https://doi.org/10.1109/TMM.2025.3581804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skewed histogram shifting (SHS) is an efficient scheme in reversible data hiding (RDH) research. By employing a pair of symmetric predictors which averages part of sorted pixels around the to-be-predicted pixel, two skewed histograms are generated. With the embedding and shifting directions toward the short tail of the two histograms, SHS reduces many invalid modifications. However, the design of the symmetric predictors pair is strictly constrained, which seriously degrades the performance on both embedding capacity and distortion of this SHS scheme. In this work, we propose a generalized SHS model to remove the weight and symmetry constraints. With the help of differential evolution algorithm, the optimized parameters are obtained in a short period of time, avoiding wasting time using exhaustive search. What is more, adaptive pairwise mapping and embedding bin selection are also realized by adding parameters into the evolutionary process, which greatly improve the embedding performance without increasing too much computational complexity. Experiments demonstrate the superiority of our method by comparing it with state-of-the-art RDH schemes.},
  archive      = {J_TMM},
  author       = {Guojun Fan and Lei Lu and Zijing Li and Ping Li and Quan Zhou and Zhibin Pan},
  doi          = {10.1109/TMM.2025.3581804},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6221-6234},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Generalized skewed histogram shifting based reversible data hiding by differential evolution},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MarkPlugger: Generalizable watermark framework for latent diffusion models without retraining. <em>TMM</em>, <em>27</em>, 6211-6220. (<a href='https://doi.org/10.1109/TMM.2025.3565960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, the family of latent diffusion models (LDMs) has gained prominence for its high quality outputs and scalability. This has also raised security concerns on social media, as malicious users can create and disseminate harmful content. Existing approaches typically involve training specific components or entire generative models to embed a watermark in generated images for traceability and responsibility. However, in the fast-evolving era of AI-generated content (AIGC), the rapid iteration and modification of LDMs makes retraining with watermark models costly. To address the problem, we propose MarkPlugger, a generalizable plug-and-play watermark framework without LDM retraining. In particular, to reduce the disturbance of the watermark on the semantics of the generated image, we try to identify a watermark representation that is approaching orthogonal to the semantic in latent space, and apply an additive fusion strategy for the watermark and the semantic. Without modifying any components of the LDMs, we embed diverse watermarks in latent space, adapting to the denoising process. Our experimental findings reveal that our method effectively harmonizes image quality and watermark recovery rate. We also have validated that our method is generalized to multiple official versions and modified variants of LDMs, even without retraining the watermark model. Furthermore, it performs robustly under various attacks of different intensities.},
  archive      = {J_TMM},
  author       = {Guokai Zhang and Lanjun Wang and Yuting Su and An-An Liu},
  doi          = {10.1109/TMM.2025.3565960},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6211-6220},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MarkPlugger: Generalizable watermark framework for latent diffusion models without retraining},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAT-net: Structure-aware transformer-based attention fusion network for low-quality retinal FunduImages enhancement. <em>TMM</em>, <em>27</em>, 6198-6210. (<a href='https://doi.org/10.1109/TMM.2025.3565935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ophthalmology diagnosis, high-fidelity fundus images are essential for disease diagnosis and intervention. However, many real-world clinical conditions may degrade the quality of the acquired images and thus affect clinical diagnostic accuracy. Traditional convolutional neural network-based retinal fundus image enhancement methods cannot always capture long-range dependencies, which reduces the overall visual quality of images, especially for real retinal fundus images. Furthermore, existing enhancement methods often fail to fully utilize low-resolution structural detail information, which potentially leads to inaccurate pivotal fundus vessel topology or capillary details. In this paper, we propose a novel Structure-Aware Transformer-based attention fusion Network (SAT-Net) for low-quality retinal fundus image enhancement. First, we introduce a Transformer-based attention fusion module which incorporates window-based self-attention and channel self-attention to capture global spatial dependencies and emphasize important feature channels simultaneously. This fusion significantly improves the overall perceptual quality of the image by enhancing both the local details and the uniformity of the non-vessel background regions. Second, we introduce a cross-quality knowledge distillation technique, which bridges the quality gap between high-quality and low-quality fundus images. By designing a high-performing teacher network to guide a lightweight student network, the student network enables to capture detailed features from low-quality fundus images, further preserving critical diagnostic information and fine topology structures. Moreover, we design a structure-aware multi-scale loss function by using a trainable subnetwork to obtain the edge structure from different scales to better constrain pivotal fundus vessel structure and capillary details. Comprehensive quantitative and qualitative experiments on both synthetic and real fundus image datasets robustly validate that our proposed SAT-Net outperforms other state-of-the-art methods for fundus image enhancement. In addition, extensive comparative experiments on both the vessel segmentation and Optic Disc/Cup detection tasks further validate the effectiveness and superiority of our proposed method.},
  archive      = {J_TMM},
  author       = {Yang Wen and Bin Luo and Wuzhen Shi and Jianhua Ji and Wenming Cao and Xiaokang Yang and Bin Sheng},
  doi          = {10.1109/TMM.2025.3565935},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6198-6210},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SAT-net: Structure-aware transformer-based attention fusion network for low-quality retinal FunduImages enhancement},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracking like human: Dynamic scene learning reasoning tracker in satellite videos. <em>TMM</em>, <em>27</em>, 6182-6197. (<a href='https://doi.org/10.1109/TMM.2025.3565976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In satellite video object tracking, the individual frame analysis method is usually used for target localization, ignoring informative cues of the dynamic scene. Temporal information could contribute to identifying the target from distractors. In this work, a novel dynamic scene learning reasoning tracker is proposed for satellite videos, which reasons over temporal dynamic information to derive the target location. It is inspired by the tracking pattern through human perception and reasoning. First, static-dynamic united analysis is designed to construct dynamic scenes by concatenating the static searching results along the temporal dimension. Second, the information of each response object is aggregated by wavelet transforms. Meanwhile, these scenes are projected into low-frequency and high-frequency subspaces, which could imitate different levels of perceptions of humans for scenes. Third, an object-aware reasoning transformer is proposed to utilize the temporal dynamics of input response objects. In each subspace, it models the mutual interactions between dynamic objects and further learns the intrinsic property of each object for target reasoning. Finally, to obtain the current reasoning result, inverse wavelet transforms are utilized to integrate the results of low-frequency and high-frequency subspaces. The effectiveness of the proposed method is validated on three public satellite video datasets, including SV248S, SkySat, and VISO. Qualitative and quantitative experimental results show that the proposed tracker outperforms 22 popular approaches in seven challenging tracking satellite scenarios.},
  archive      = {J_TMM},
  author       = {Xiaoyan Yang and Licheng Jiao and Yangyang Li and Xu Liu and Lingling Li and Puhua Chen and Fang Liu and Wenping Ma and Shuyuan Yang},
  doi          = {10.1109/TMM.2025.3565976},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6182-6197},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tracking like human: Dynamic scene learning reasoning tracker in satellite videos},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High fidelity face swapping via facial texture and structure consistency mining. <em>TMM</em>, <em>27</em>, 6168-6181. (<a href='https://doi.org/10.1109/TMM.2025.3565975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The face swapping task has always been attractive for its wide range of applications. However, existing face swapping methods suffer from two main challenges: a) degraded generation fidelity due to insufficient facial texture information; b) inconsistent synthesized face structure due to the lack of effective forms of face structure supervision. To address the above issues, we propose a novel Texture and Structure Consistency Mining (TSCM) framework to achieve high-fidelity face swapping with rich textures and consistent facial structure. For one thing, a Dual-Scale Oriented Identity Transfer module is devised to globally transfer source identity to the well-disentangled target identity features at dual-level feature spaces, which achieves more efficient identity transfer and promotes target attribute texture preservation. Then, to compensate for the local facial textures, a Semantic-Guided Texture Enhancing module is developed by exploiting disentangled identity and attribute semantics to ensure local texture consistency. For another thing, different from previous methods that directly apply abstract 3D coefficients, a Structure-Aware Head Modeling module is designed to provide intuitive face structure supervision, which is adaptively integrated with local facial texture information in a self-learning manner. Moreover, a structure-consistency discriminator is introduced to effectively restrict the synthesized face structure consistency. Comprehensive experiments demonstrate that our TSCM yields a substantial advantage over the state-of-the-art methods in synthesizing texture- and structure-consistent swapped faces.},
  archive      = {J_TMM},
  author       = {Fengyuan Liu and Lingyun Yu and Quanwei Yang and Meng Shao and Hongtao Xie},
  doi          = {10.1109/TMM.2025.3565975},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6168-6181},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {High fidelity face swapping via facial texture and structure consistency mining},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive frame patching for FoV-based point cloud video streaming. <em>TMM</em>, <em>27</em>, 6154-6167. (<a href='https://doi.org/10.1109/TMM.2025.3565928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many XR applications require the delivery of volumetric video to users. Point Cloud has become a popular volumetric video format. A dense point cloud consumes much higher bandwidth than a 2D/360 $^{\circ }$ video frame. User Field of View (FoV) is more dynamic with 6-DoF movement than 3-DoF movement. To save bandwidth, FoV-adaptive streaming predicts a user's FoV and only downloads point cloud data falling in the predicted FoV. However, it is vulnerable to FoV prediction errors, which can be significant when a long buffer is utilized for smoothed streaming. In this work, we propose a multi-round progressive refinement framework for point cloud video streaming. Instead of sequentially downloading point cloud frames, our solution simultaneously downloads/patches multiple frames falling into a sliding time-window, leveraging the inherent scalability of octree-based point-cloud coding. The optimal rate allocation among all tiles of active frames are solved numerically using the heterogeneous tile rate-quality functions calibrated by the predicted user FoV. Multi-frame downloading/patching simultaneously takes advantage of the streaming smoothness resulting from long buffer and the FoV prediction accuracy at short buffer length. We evaluate our streaming solution using simulations driven by real point cloud videos, real bandwidth traces, and 6-DoF FoV traces of real users. Our solution is robust against the bandwidth/FoV prediction errors, and can deliver high and smooth view quality in the face of bandwidth variations and dynamic user and point cloud movements.},
  archive      = {J_TMM},
  author       = {Tongyu Zong and Yixiang Mao and Chen Li and Yong Liu and Yao Wang},
  doi          = {10.1109/TMM.2025.3565928},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6154-6167},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive frame patching for FoV-based point cloud video streaming},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instruction-driven 3D facial expression generation and transition. <em>TMM</em>, <em>27</em>, 6140-6153. (<a href='https://doi.org/10.1109/TMM.2025.3565929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications.},
  archive      = {J_TMM},
  author       = {Anh H. Vo and Tae-Seok Kim and Hulin Jin and Soo-Mi Choi and Yong-Guk Kim},
  doi          = {10.1109/TMM.2025.3565929},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6140-6153},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Instruction-driven 3D facial expression generation and transition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward robust semi-supervised distribution alignment against label distribution shift with noisy annotations. <em>TMM</em>, <em>27</em>, 6127-6139. (<a href='https://doi.org/10.1109/TMM.2025.3565967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based AI models typically require a large amount of high-quality annotated data to achieve optimal performance. However, the label distribution shift caused by noisy annotations can lead to perturbations in the classification boundary, reducing the robustness and generalization capabilities of deep learning models. To mitigate this issue, we transform the problem of learning from noisy labels into a semi-supervised learning problem, and propose a novel Semi-Supervised Distribution Alignment (SSDA) framework that strategically integrates noise-robust distribution alignment within a unified semi-supervised learning paradigm for combating noisy labels. By leveraging the similarity distribution between historical predictions, the proposed SSDA approach benefits from a flexible multi-historical regression modeling strategy, which aims to identify high-confidence samples/pairs and recalibrate the label shift through pseudo-labels. Furthermore, our approach employs a comprehensive multi-granularity distribution adaptation strategy, incorporating both instance-wise and class-aware distribution alignment to quantitatively minimize semantic discrepancies across different mixed feature domains. In this way, our SSDA approach ultimately achieves more resilient and generalizable performance against label noise, even in the presence of substantial noise. Extensive experiments conducted on multiple simulated and real-world noisy benchmark datasets consistently demonstrate the superiority and effectiveness of our SSDA method compared to existing state-of-the-art baselines.},
  archive      = {J_TMM},
  author       = {Bingzhi Chen and Zhanhao Ye and Yishu Liu and Xiaozhao Fang and Guangming Lu and Shengli Xie and Xuelong Li},
  doi          = {10.1109/TMM.2025.3565967},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6127-6139},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Toward robust semi-supervised distribution alignment against label distribution shift with noisy annotations},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MGDefect: A mask-guided high-quality defect image generation method for improving defect inspection. <em>TMM</em>, <em>27</em>, 6113-6126. (<a href='https://doi.org/10.1109/TMM.2025.3565978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based defect inspection methods have achieved promising performance, which usually relies on a large number of well-labeled training samples. However, it requires much effort to obtain enough annotated samples especially pixel-level annotations in practical production. Generative adversarial networks (GANs) can be utilized to generate defect samples. However, training GANs typically requires a large amount of defect data and most of them cannot generate defect samples with pixel-level annotations. In this paper, we present a Mask-Guided Defect image generation method, called MGDefect, which can generate high-quality defect samples with pixel-level annotations and effectively improves the performance of downstream tasks. Specifically, MGDefect consists of a Mask-Guided Defect Generation GAN (MGDG-GAN) and a Defect Mask GAN (DM-GAN). MGDG-GAN generates images containing defects with specific locations, shapes, and sizes via mask guidance and the dual discrimination for defects at the region level and image level. DM-GAN aims to generate diverse and rational masks for MGDG-GAN. It also adopts region-level and image-level dual discrimination for masks to generate compatible masks with the target objects. MGDG-GAN mainly focuses on generating local defect regions and DM-GAN specializes in generating masks, which are both trained on limited defect samples and abundant normal samples. Experiments conducted on the MVTec AD, DAGM 2007, and KolektorSDD2 benchmark datasets demonstrate that our method achieves promising results compared with other state-of-the-art approaches. Meanwhile, the generated defect samples significantly improve the performance of defect inspection tasks including classification and segmentation. Specifically, our method achieves KID$\times 10^{3}$/IS scores of 48.35/2.27 on MVTec AD, 15.37/2.44 on DAGM 2007, and 19.70/2.01 on KolektorSDD2. Furthermore, our method improves mIoU by 10.59%, 2.20%, and 2.17% on these datasets, respectively, using U-Net as the segmentation model.},
  archive      = {J_TMM},
  author       = {Xiaoheng Jiang and Yingjie Li and Feng Yan and Yang Lu and Changsheng Xu and Mingliang Xu},
  doi          = {10.1109/TMM.2025.3565978},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6113-6126},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MGDefect: A mask-guided high-quality defect image generation method for improving defect inspection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor-based late fusion incomplete multiview clustering. <em>TMM</em>, <em>27</em>, 6102-6112. (<a href='https://doi.org/10.1109/TMM.2025.3565971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Late fusion-based algorithms have attracted extensive attention because of their low time and space complexity for handling in-complete multiview data. However, these methods have certain limitations. First, the basic clustering indicator matrices generated by incomplete views are susceptible to low-quality imputation. Second, traditional methods often fail to adequately consider the high-order correlations between these basic clustering indicator matrices, leading to suboptimal performance. Third, conventional methods focus primarily on improving speed, with less emphasis on enhancing clustering performance. To address these issues, we propose two novel models. The first is called tensor-based late fusion incomplete multiview clustering (TLF-IMVC-1). Specifical-ly, TLF-IMVC-1 first seeks a consensus clustering matrix from the basic clustering indicator matrices and subsequently imputes the incomplete portions of these matrices via the learned consen-sus matrix. This approach seamlessly integrates the clustering process with the imputation of missing elements into a unified framework. Furthermore, we construct a third-order tensor from these basic clustering matrices, constrained by the tensor nuclear norm, to capture their high-order correlations. Although this model is effective, it lacks proper guidance in the learning process of the basic clustering indicator matrices, making them suscepti-ble to low-quality imputation. Therefore, we introduce the second novel model, i.e., TLF-IMVC-2, to address this issue. Specifically, TLF-IMVC-2 uses the learned consensus representation matrix as a new component to construct the third-order tensor. This strategy leverages the robust clustering structure inherent in the consensus matrix to guide the learning process of the basic clus-tering matrices. The experimental results demonstrate that both models outperform state-of-the-art methods in clustering.},
  archive      = {J_TMM},
  author       = {Xiaoxing Guo and Ming Yang and Gui-Fu Lu},
  doi          = {10.1109/TMM.2025.3565971},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6102-6112},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Tensor-based late fusion incomplete multiview clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scale-shift attention in polarization domain for fine-grained classification of satellite ISAR images. <em>TMM</em>, <em>27</em>, 6092-6101. (<a href='https://doi.org/10.1109/TMM.2025.3565957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional fine-grained classification focuses on visible light domains, such as animals and cars. However, these methods often perform poorly when applied to radar images and images of satellites because of challenges such as distinguishing between noise and objects and the significant scale differences among object components. To address these unique scenarios, we propose the scale-shift attention in polarization domain (SAPD) method for fine-grained classification in satellite ISAR images. Specifically, radar emits different types of waves, each with distinct imaging effects. We utilize multipolarization inputs and introduce a polarization domain query module to integrate complementary features from various radar wave types captured from the same viewpoint. This multipolarization learning helps distinguish noise and leverages complementary features from different inputs. Moreover, to handle the substantial scale differences between centimeter-level payloads and the overall meter-level structure of satellites, we propose a scale-shift attention mechanism based on shift kernels. This mechanism extends attention in the direction specified by the shift kernel by incorporating adjacent pixels, allowing for the diffusion of attention. This is beneficial for capturing features of satellite components with varying scales and shapes. Extensive experiments on a novel satellite ISAR image dataset validate the effectiveness and superiority of the SAPD.},
  archive      = {J_TMM},
  author       = {Zewei Xin and Qinya Li and Bowen Sheng and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMM.2025.3565957},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6092-6101},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Scale-shift attention in polarization domain for fine-grained classification of satellite ISAR images},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment-enhanced graph-based sarcasm explanation in dialogue. <em>TMM</em>, <em>27</em>, 6080-6091. (<a href='https://doi.org/10.1109/TMM.2025.3565959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which aims to generate a natural language explanation for the given sarcastic dialogue that involves multiple modalities (i.e., utterance, video, and audio). Although existing studies have achieved great success based on the generative pretrained language model BART, they overlook exploiting the sentiments residing in the utterance, video and audio, which play important roles in reflecting sarcasm that essentially involves subtle sentiment contrasts. Nevertheless, it is non-trivial to incorporate sentiments for boosting SED performance, due to three main challenges: 1) diverse effects of utterance tokens on sentiments; 2) gap between video-audio sentiment signals and the embedding space of BART; and 3) various relations among utterances, utterance sentiments, and video-audio sentiments. To tackle these challenges, we propose a novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation framework, named EDGE. In particular, we first propose a lexicon-guided utterance sentiment inference module, where a heuristic utterance sentiment refinement strategy is devised. We then develop a module named Joint Cross Attention-based Sentiment Inference (JCA-SI) by extending the multimodal sentiment analysis model JCA to derive the joint sentiment label for each video-audio clip. Thereafter, we devise a context-sentiment graph to comprehensively model the semantic relations among the utterances, utterance sentiments, and video-audio sentiments, to facilitate sarcasm explanation generation. Extensive experiments on the publicly released dataset WITS verify the superiority of our model over cutting-edge methods.},
  archive      = {J_TMM},
  author       = {Kun Ouyang and Liqiang Jing and Xuemeng Song and Meng Liu and Yupeng Hu and Liqiang Nie},
  doi          = {10.1109/TMM.2025.3565959},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6080-6091},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Sentiment-enhanced graph-based sarcasm explanation in dialogue},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transition-aware point cloud completion by a progressive refinement generative adversarial network. <em>TMM</em>, <em>27</em>, 6070-6079. (<a href='https://doi.org/10.1109/TMM.2025.3565974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional reconstruction can help robots and vehicles understand their surroundings for subsequent navigation and manipulation tasks. However, in the case of target occlusion, it is difficult for visual sensors to acquire complete information about objects. In this work, we propose a progressive refinement generative adversarial network (PR-GAN) to recover object shapes guided by transition-awareness. This method directly predicts the missing point cloud from the partial point cloud. Our PR-GAN contains a progressive generation module (PGM) and a discriminator. A self-attention-based encoder is proposed in PGM to capture contextual information between local and global features. To guide encoders in generating accurate point clouds, we further propose a progressive fusion module (PFM) that extracts transition information between point clouds of different scales. Moreover, a part-whole correlation module (PWCM) is designed to extract the transition-awareness between the partial and the whole point clouds to further preserve the details. With the above modules, we enhance the spatial logic perception capability of the network so that PR-GAN can fully extract point cloud features and predict the high-fidelity point cloud. Experimental results show that PR-GAN performs better compared to other methods, evaluated on three public datasets.},
  archive      = {J_TMM},
  author       = {Feng Luan and Jiarui Hu and Zhipeng Wang and Jiguang Yue and Yanmin Zhou and Bin He},
  doi          = {10.1109/TMM.2025.3565974},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6070-6079},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Transition-aware point cloud completion by a progressive refinement generative adversarial network},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bring adaptive binding prototypes to generalized referring expression segmentation. <em>TMM</em>, <em>27</em>, 6059-6069. (<a href='https://doi.org/10.1109/TMM.2025.3565964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring Expression Segmentation (RES), which aims to identify and segment objects based on natural language expressions is garnering increased research attention. While substantial progress has been made in RES, the emergence of Generalized Referring Expression Segmentation (GRES) introduces new challenges by allowing the expressions to describe multiple objects or lack specific object references. Existing RES methods usually rely on sophisticated encoder-decoder and feature fusion modules, and have difficulty generating class prototypes that match each instance individually when confronted with the complex referent and binary labels of GRES. In this paper, reevaluating the differences between RES and GRES, we propose a novel Model with Adaptive Binding Prototypes (MABP) that adaptively binds queries to object features in the corresponding region. It enables different query vectors to match instances of different categories, or different parts of the same instance, significantly expanding the decoder's flexibility, dispersing global pressure across all the queries, and easing the demands on the encoder. The experimental results demonstrate that MABP significantly outperforms the state-of-the-art methods in all three splits on the gRefCOCO dataset. Moreover, MABP outperforms the state-of-the-art methods on the RefCOCO+ and G-Ref datasets, and achieves very competitive results on RefCOCO.},
  archive      = {J_TMM},
  author       = {Weize Li and Zhicheng Zhao and Haochen Bai and Fei Su},
  doi          = {10.1109/TMM.2025.3565964},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6059-6069},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bring adaptive binding prototypes to generalized referring expression segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nutrition estimation for dietary management: A transformer approach with depth sensing. <em>TMM</em>, <em>27</em>, 6047-6058. (<a href='https://doi.org/10.1109/TMM.2025.3565966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nutrition estimation is crucial for effective dietary management and overall health and well-being. Existing methods often struggle with sub-optimal accuracy and can be time-consuming. In this paper, we propose NuNet, a transformer-based network designed for nutrition estimation that utilizes both RGB and depth information from food images. We have designed and implemented a multi-scale encoder and decoder, along with two types of feature fusion modules, specialized for estimating five nutritional factors. These modules effectively balance the efficiency and effectiveness of feature extraction with flexible usage of our customized attention mechanisms and fusion strategies. Our experimental study shows that NuNet significantly outperforms its variants and existing solutions for nutrition estimation. It achieves an error rate of 15.65%, the lowest known to us, largely due to our multi-scale architecture and fusion modules. This research holds practical values for dietary management with huge potential for transnational research and deployment and could inspire other applications involving multiple data types with varying degrees of importance.},
  archive      = {J_TMM},
  author       = {Zhengyi Kwan and Wei Zhang and Zhengkui Wang and Aik Beng Ng and Simon See},
  doi          = {10.1109/TMM.2025.3565966},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6047-6058},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Nutrition estimation for dietary management: A transformer approach with depth sensing},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical distortion learning for fast lossy compression of point clouds. <em>TMM</em>, <em>27</em>, 6031-6046. (<a href='https://doi.org/10.1109/TMM.2025.3565958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of 3D point cloud applications requires efficient compression techniques for high-quality and low-latency services. Recently, learning-based point cloud compression models have made significant progress. However, geometric distortion resulting from downsampling limits the feature depth within large-scale point clouds, thereby constraining the receptive field and suppressing the redundant removal. Moreover, the issues of computational efficiency and reconstruction quality still persist in the compression of large-scale point clouds. To address these challenges, we propose a hierarchical distortion learning framework for end-to-end lossy compression of point clouds. First, we design a feature residual compression module to efficiently transmit shallow semantics between the encoder and the decoder, which enables a lightweight design of our framework. Second, we introduce a geometry residual compression module to progressively complement the reconstruction distortion, avoiding the accumulation of geometric distortion. By integrating these two modules and employing sufficient downsampling processes, we develop a high-performance framework with a significantly enlarged receptive field and low computational cost. Extensive experiments demonstrate that our method achieves state-of-the-art performance in geometry lossy compression, while delivering competitive performance in joint geometry and color lossy compression with fast running speed.},
  archive      = {J_TMM},
  author       = {Pengpeng Yu and Ye Zhang and Fan Liang and Haoran Li and Yulan Guo},
  doi          = {10.1109/TMM.2025.3565958},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6031-6046},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical distortion learning for fast lossy compression of point clouds},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OpenSlot: Mixed open-set recognition with object-centric learning. <em>TMM</em>, <em>27</em>, 6019-6030. (<a href='https://doi.org/10.1109/TMM.2025.3565972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing open-set recognition (OSR) studies typically assume that each image contains only one class label,with the unknown test set (negative) having a disjoint label space from the known test set (positive), a scenario referred to as full-label shift. This paper introduces the mixed OSR problem, where test images contain multiple class semantics, with both known and unknown classes co-occurring in the negatives, leading to a more complex super-label shift that better reflects real-world scenarios. To tackle this challenge, we propose the OpenSlot framework, based on object-centric learning, which uses slot features to represent diverse class semantics and generate class predictions. The proposed anti-noise slot (ANS) technique helps mitigate the impact of noise (invalid or background) slots during classification training, addressing the semantic misalignment between class predictions and ground truth. We evaluate OpenSlot on both mixed and conventional OSR benchmarks. Without elaborate designs, our method not only excels existing approaches in detecting super-label shifts across OSR tasks, but also achieves state-of-the-art performance on conventional benchmarks. Meanwhile, OpenSlot can localize class objects without using bounding boxes during training, demonstrating competitive performance in open-set object detection and potential for generalization.},
  archive      = {J_TMM},
  author       = {Xu Yin and Fei Pan and Guoyuan An and Yuchi Huo and Zixuan Xie and Sung-Eui Yoon},
  doi          = {10.1109/TMM.2025.3565972},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6019-6030},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {OpenSlot: Mixed open-set recognition with object-centric learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of HEVC double compression based on deep representations of in-loop filtering and CU depth maps. <em>TMM</em>, <em>27</em>, 6003-6018. (<a href='https://doi.org/10.1109/TMM.2025.3565961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of HEVC (High Efficiency Video Coding) double compression detection, relocated I-frame (RI frame) detection and original GOP size estimation are two significant problems for video forensics. However, little research explores the interconnection between the two problems, and effective methods to resolve them are still lacking. In this paper, a novel feature model called In-loop Filtering and CU Depth Map (IFCDM) is proposed to accurately detect RI frames, and the intrinsic correlation between RI frames and GOP structure is explored, which can be used for original GOP size estimation. Theoretical and statistical analysis of HEVC recompression process is first carried out. Then, sub-features of HEVC in-loop filtering modes and CU partition depth are extracted, and transformed into grey-scale maps to construct IFCDM. A neural network, consisting of tiny Vision Transformer and LSTM, is trained to learn spatial and temporal representations of input features, and further derive the RI frame detection results. Finally, an adaptive periodic analysis algorithm is designed, to integrate the RI frame detection results and estimate the original GOP size of recompressed videos. Experiments show that our method can outperform the existing state-of-the-art methods in both frame level and video level.},
  archive      = {J_TMM},
  author       = {Xing Yan and Tanfeng Sun and Qiang Xu and Ke Xu and Xinghao Jiang},
  doi          = {10.1109/TMM.2025.3565961},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {6003-6018},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Detection of HEVC double compression based on deep representations of in-loop filtering and CU depth maps},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FrDiff: Framelet-based conditional diffusion model for multispectral and panchromatic image fusion. <em>TMM</em>, <em>27</em>, 5989-6002. (<a href='https://doi.org/10.1109/TMM.2025.3565985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of fusing low-resolution multispectral (LRMS) and high-resolution panchromatic (PAN) imagery, commonly referred to as pansharpening, is intended to generate high-resolution multispectral (HRMS) imagery. Typically, most pre-existing pansharpening frameworks mainly emphasize the straightforward learning of the mapping relationship among PAN and LRMS images to HRMS images. However, a key limitation of these frameworks is their potential overemphasis on spatial information, particularly the enhancement of low-frequency components. As a result, such an oversight potentially hinders the model's ability to simultaneously restore both spectral and spatial details. To address this issue, we propose a novel pansharpening model based on the denoising diffusion probabilistic model (DDPM), dubbed FrDiff. Specifically, we build a framelet-based conditional diffusion model that leverages the generative power of diffusion models to produce more refine results. Different from conventional methods directly inferring HRMS images, our strategy is designed to project their framelet coefficients, utilizing the available PAN and LRMS images as resources. This approach enables the separation of high-frequency and low-frequency components through framelet transformation, which are subsequently recombined to create a novel set of conditional embeddings that feed into the diffusion process. At the same time, the powerful predictive power of the diffusion model is exploited to simultaneously recover the high-frequency and low-frequency components of the HRMS. Moreover, we introduce a framelet-oriented cross-attention module dedicated to honing spectral fidelity. This module is crucial for improving the spectral precision of the HRMS images, ensuring a balanced emphasis on both spatial and spectral enhancements. Quantitative and qualitative experiments on multiple benchmark datasets demonstrate that the proposed method achieves more robustness and high-quality results than other state-of-the-art pansharpening methods.},
  archive      = {J_TMM},
  author       = {Junkang Zhang and Faming Fang and Tingting Wang and Guixu Zhang and Haichuan Song},
  doi          = {10.1109/TMM.2025.3565985},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5989-6002},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FrDiff: Framelet-based conditional diffusion model for multispectral and panchromatic image fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PID controller-driven network for image fusion. <em>TMM</em>, <em>27</em>, 5977-5988. (<a href='https://doi.org/10.1109/TMM.2025.3565970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With its well-designed network architecture, the deep learning-based infrared and visible image fusion (IVIF) method shows its efficiency and effectiveness by realizing a fine feature extraction and fusion mechanism. However, disparities in cross-modal features often result in an imbalance between texture details and contextual information, causing detailed features to be overshadowed by prevailing contextual information. To tackle this issue, this study introduces PIDFusion, a fusion model driven by a PID controller, designed to dynamically optimize cross-modal feature fusion deviations. The core of PIDFusion is the dynamic adaptation capability of the PID controller, which facilitates real-time corrections for deviations encountered during the fusion process, thereby maintaining a harmonious balance between texture details and contextual information. Additionally, we introduced the Cyclic Self-Supervised Feature Refinement (CSSFR), which under the constraint of self-supervised loss, minimizes redundant information within the feature flow and ensures the preservation of salient feature through the cyclic input of decoupled features. Concurrently, we developed the Iterative Attention Module (IAM), utilizing the unique gating mechanism of LSTM to capture feature changes across successive iterations, thereby driving the model to cultivate more discriminative feature representations. Extensive experiments revealed that PIDFusion outperforms SOTA methods in terms of both efficiency and cost-effectiveness, through static statistics and high-level vision tasks.},
  archive      = {J_TMM},
  author       = {Xue Wang and Zheng Guan and Wenhua Qian and Jinde Cao and Runzhuo Ma},
  doi          = {10.1109/TMM.2025.3565970},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5977-5988},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PID controller-driven network for image fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motion direction awareness: A biomimetic dynamic capture mechanism for video prediction. <em>TMM</em>, <em>27</em>, 5946-5960. (<a href='https://doi.org/10.1109/TMM.2025.3565988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video prediction is an important yet challenging task that generates future frames based on previous observations. Despite recent progress, existing methods still suffer from motion blur, due to weak motion perception capabilities leading to uncertainty in motion direction. To address this, we propose a Motion Direction Awareness (MDA) mechanism inspired by the direction-selective mechanism in animal visual systems. Specifically, MDA can decompose complex motions into horizontal and vertical components, allowing dimension reduction and independent processing, thereby effectively enhancing motion perception and reducing uncertainty in predicted motion directions. Based on MDA, we design a multi-scale feature fusion network named MDANet for video prediction, which incorporates different scales of spatially encoded features in conjunction with MDA mechanism to extract the temporal evolution information of global and local spatial features. Extensive experiments on representative datasets demonstrate that MDANet can alleviate motion blurring, improving prediction accuracy and temporal consistency over state-of-the-art models. Furthermore, we validate the generalizability and effectiveness of our MDA mechanism by integrating it into other advanced models. The code is available at supplementary.},
  archive      = {J_TMM},
  author       = {Lianqiang Gan and Junyu Lai and Junhong Zhu and Huashuo Liu and Lianli Gao},
  doi          = {10.1109/TMM.2025.3565988},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5946-5960},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Motion direction awareness: A biomimetic dynamic capture mechanism for video prediction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring kernel transformations for implicit neural representations. <em>TMM</em>, <em>27</em>, 5936-5945. (<a href='https://doi.org/10.1109/TMM.2025.3565979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit neural representations (INRs), which leverage neural networks to represent signals by mapping coordinates to their corresponding attributes, have garnered significant attention. They are extensively utilized for image representation, with pixel coordinates as input and pixel values as output. In contrast to prior works focusing on investigating the effect of the model's inside components (activation function, for instance), this work pioneers the exploration of the effect of kernel transformation of input/output while keeping the model itself unchanged. A byproduct of our findings is a simple yet effective method that combines scale and shift to significantly boost INR with negligible computation overhead. Moreover, we present two perspectives, depth and normalization, to interpret the performance benefits caused by scale and shift transformation. Overall, our work provides a new avenue for future works to understand and improve INR through the lens of kernel transformation.},
  archive      = {J_TMM},
  author       = {Sheng Zheng and Chaoning Zhang and Dongshen Han and Fachrina Dewi Puspitasari and Xinhong Hao and Yang Yang and Heng Tao Shen},
  doi          = {10.1109/TMM.2025.3565979},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5936-5945},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploring kernel transformations for implicit neural representations},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EraW-net: Enhance-refine-align W-net for scene-associated driver attention estimation. <em>TMM</em>, <em>27</em>, 5922-5935. (<a href='https://doi.org/10.1109/TMM.2025.3565934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Associating driver attention with driving scene across two fields of view is a challenging cross-domain perception problem, which requires comprehensive consideration of cross-view mapping, dynamic driving scene analysis and driver status tracking. Previous methods typically analyze a single view or map attention to the scene through a two-step projection, failing to exploit their implicit connections and establish accurate associations. Moreover, simple fusion modules are inadequate for modeling the complex relationships between the two views, making information integration complicated. To address these issues, we propose EraW-Net, a novel end-to-end framework for scene-associated driver attention estimation by aggregating information from dual views. This method enhances the most discriminative dynamic cues, refines feature representations, and facilitates semantically aligned cross-domain integration through a W-shaped architecture, termed W-Net. Specifically, a Dynamic Adaptive Filter Module (DAF-Module) is proposed to address the challenges of frequently changing driving environments by extracting vital regions. It suppresses the indiscriminately recorded dynamics and highlights crucial ones by innovative joint frequency-spatial analysis, enhancing the model's ability to parse complex dynamics. Additionally, to track driver states during non-fixed facial poses, we propose a Global Context Sharing Module (GCS-Module) to construct refined feature representations by capturing hierarchical features that adapt to various scales of head and eye movements. Finally, W-Net achieves systematic cross-view information integration through its unique two-stage decoding strategy, addressing semantic misalignment in heterogeneous data integration. Experiments demonstrate that the proposed method robustly and accurately estimates scene-associated driver attention on large public datasets.},
  archive      = {J_TMM},
  author       = {Jun Zhou and Chunsheng Liu and Faliang Chang and Wenqian Wang and Penghui Hao and Yiming Huang and Zhiqiang Yang},
  doi          = {10.1109/TMM.2025.3565934},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5922-5935},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EraW-net: Enhance-refine-align W-net for scene-associated driver attention estimation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Black-box adversarial defense based on image decomposition and reconstruction. <em>TMM</em>, <em>27</em>, 5909-5921. (<a href='https://doi.org/10.1109/TMM.2025.3565987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks have challenged the security of deep neural networks (DNNs) recently. The most prominent adversarial attack methods include backdoor attacks, adversarial examples, etc. These attack methods inject triggers or perturbations into images, leading to extremely dangerous security vulnerability in deep learning domain. The various forms of adversarial attacks can contaminate DNNs with their distinct characteristics. The complexity of adversarial attack poses a great challenge to designing a general defense strategy. In this paper, we propose a novel defense method against most of adversarial attacks through Image Decomposition and Reconstruction (IDR). Our method can be applied to poisoned images without the need for internal information about the model or any prior knowledge of the clean/poisoned images. We apply a linear transformation on the poisoned image to destroy the perturbations or triggers and deploy a pre-trained diffusion model to reconstruct the original information. In particular, we propose a novel reverse process that utilizes the consistency of range-null space decomposition to guide the generation of purified images. The decomposition of the range-null space can guarantee the retrieval of image information, which enhances the robustness of our method and contributes to the reliable purification of poisoned images. We assess the effectiveness of our proposed IDR against various prevalent backdoor attacks, adversarial examples and Image-Scaling attack methods. The experimental results highlight the outstanding defensive capabilities of our proposed IDR, demonstrating an exceptionally high defense success rate.},
  archive      = {J_TMM},
  author       = {Jimiao Yu and Honglong Chen and Junjian Li and Linghan Chen and Yudong Gao and Weifeng Liu and Lei Zhang},
  doi          = {10.1109/TMM.2025.3565987},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5909-5921},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Black-box adversarial defense based on image decomposition and reconstruction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot text-driven dynamic neural radiance fields stylization. <em>TMM</em>, <em>27</em>, 5895-5908. (<a href='https://doi.org/10.1109/TMM.2025.3565983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-driven style transfer for Neural Radiance Fields (NeRFs) is an emerging research topic that leverages text descriptions instead of reference style images to apply style transfer. However, existing methods for stylizing NeRFs predominantly struggle to extend to 4D dynamic scenes, due to NeRFs’ inherent limitation to static environments. Moreover, these current methods require training for each specific text input, which limits them to a single style description and significantly hampers generalizability and applications. In this paper, we introduce a novel approach to zero-shot text-driven 4D style transfer that adopts text inputs into the CLIP’s style space with a canonical feature volume. Specifically, using geometric priors from pre-trained dynamic Neural Radiance Fields, we train a canonical feature volume by rendering feature maps under the supervision of a pre-trained VGG encoder. Then we utilize CLIP’s multi-modal embedding to connect the text descriptions with style images and learn a canonical style transformation matrix in CLIP’s feature space. Experiments show that our method achieves zero-shot text-driven style transfer for dynamic neural radiance fields and maintains good multi-view and cross-time consistency.},
  archive      = {J_TMM},
  author       = {Wanlin Liang and Hongbin Xu and Wanshui Gan and Wenxiong Kang},
  doi          = {10.1109/TMM.2025.3565983},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5895-5908},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Zero-shot text-driven dynamic neural radiance fields stylization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-task mutual reinforcing embedded joint video paragraph retrieval and grounding. <em>TMM</em>, <em>27</em>, 5879-5894. (<a href='https://doi.org/10.1109/TMM.2025.3565981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Paragraph Grounding (VPG) aims to precisely locate the most appropriate moments within a video that are relevant to a given textual paragraph query. However, existing methods typically rely on large-scale annotated temporal labels and assume that the correspondence between videos and paragraphs is known. This is impractical in real-world applications, as constructing temporal labels requires significant labor costs, and the correspondence is often unknown. To address this issue, we propose a Dual-task Mutual Reinforcing Embedded Joint Video Paragraph Retrieval and Grounding method (DMR-JRG). In this method, retrieval and grounding tasks are mutually reinforced rather than being treated as separate issues. DMR-JRG mainly consists of two branches: a retrieval branch and a grounding branch. The retrieval branch uses inter-video contrastive learning to roughly align the global features of paragraphs and videos, reducing modality differences and constructing a coarse-grained feature space to break free from the need for correspondence between paragraphs and videos. Additionally, this coarse-grained feature space further facilitates the grounding branch in extracting fine-grained contextual representations. In the grounding branch, we achieve precise cross-modal matching and grounding by exploring the consistency between local, global, and temporal dimensions of video segments and textual paragraphs. By synergizing these dimensions, we construct a fine-grained feature space for video and textual features, greatly reducing the need for large-scale annotated temporal labels. Meanwhile, we design a grounding reinforcement retrieval module (GRRM) that brings the coarse-grained feature space of the retrieval branch closer to the fine-grained feature space of the grounding branch, thereby reinforcing retrieval branch through grounding branch, and finally achieving mutual reinforcement between tasks. Extensive experiments on three challenging datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_TMM},
  author       = {Mengzhao Wang and Huafeng Li and Yafei Zhang and Jinxing Li and Minghong Xie and Dapeng Tao},
  doi          = {10.1109/TMM.2025.3565981},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5879-5894},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-task mutual reinforcing embedded joint video paragraph retrieval and grounding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive semi-decoupled detector for accurate object detection. <em>TMM</em>, <em>27</em>, 5866-5878. (<a href='https://doi.org/10.1109/TMM.2025.3565933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inconsistent accuracy between classification and localization tasks is a common challenge in modern object detection. Task decoupling, which employs distinct features or labeling strategies for each task, is a widely used approach to address this issue. Although it has led to noteworthy advancements, this approach is insufficient as it neglects task interdependence and lacks an explicit consistency constraint. To bridge this gap, this paper proposes the Progressive Semi-Decoupled Detector (ProSDD) to enhance both classification and localization accuracy. Specifically, a new detection head is designed that incorporates feature suppression and enhancement mechanism (FSEM) and bidirectional interaction module (BIM). Compared with the decoupled head, it not only filters out task-irrelevant information and enhances task-related information, but also avoids excessive decoupling at the feature level. Moreover, both FSEM and BIM are used multiple times, thus forming a progressive semi-decoupled head. Then, a novel consistency loss is proposed and integrated into the loss function of object detection, ensuring harmonic performance in classification and localization. Experimental results demonstrate that the proposed ProSDD effectively alleviates inconsistent accuracy and achieves high-quality object detection. Taking the pretrained ResNet-50 as the backbone, ProSDD achieves a remarkable 43.3 AP on the MS COCO dataset, surpassing contemporary state-of-the-art detectors by a substantial margin under the equivalent configurations.},
  archive      = {J_TMM},
  author       = {Bo Han and Lihuo He and Junjie Ke and Jinjian Wu and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3565933},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5866-5878},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive semi-decoupled detector for accurate object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal hybrid interaction vision-language tracking. <em>TMM</em>, <em>27</em>, 5857-5865. (<a href='https://doi.org/10.1109/TMM.2025.3565984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language tracking is a crucial branch of multi-modal object tracking, aiming to jointly locate an object by utilizing visual information and language descriptions. Typically, existing vision-language trackers employ language and visual encoders to extract features from language descriptions and visual information, respectively. Based on these extracted visual and language features, a cross-modal interaction module is used to extract multi-modal features to locate the targets. However, they ignore the differences between visual and language modalities. Due to the lack of pixel-level position information in language descriptions, the positional information of the multi-modal features is greatly weakened by the cross-modal interaction modules. As a result, the vision-language trackers cannot effectively capture subtle changes in the target's positions. To address this problem, we propose a multi-modal hybrid interaction vision-language tracking method (named MHITrack), in which a multi-modal hybrid interaction decoder is designed to enhance the positional information of multi-modal features. The proposed multi-modal hybrid interaction decoder consists of a visual-language interaction module, a multi-level position interaction module, and a hybrid interaction module. Firstly, the multi-level position interaction module is utilized to capture fine-grained position information of the target from multi-level features. Meanwhile, the visual-language interaction module performs cross-modal interaction between visual and language features to obtain multi-modal features. Furthermore, the hybrid interaction module is employed to integrate the multi-modal features with target position information, enhancing the positional information of the multi-modal features. Finally, the proposed tracker can effectively capture subtle changes in the target's positions. Through extensive experiments on four benchmark datasets, namely TNL2k, LaSOT, OTB-Lang, and LaSOText, we demonstrate that the proposed vision-language tracker achieves promising performance compared to existing state-of-the-art vision-language trackers.},
  archive      = {J_TMM},
  author       = {Lei Lei and Xianxian Li},
  doi          = {10.1109/TMM.2025.3565984},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5857-5865},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-modal hybrid interaction vision-language tracking},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compact-yet-separate: Proto-centric multi-modal hashing with pronounced category differences for multi-modal retrieval. <em>TMM</em>, <em>27</em>, 5843-5856. (<a href='https://doi.org/10.1109/TMM.2025.3565973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal hashing achieves low storage costs and high retrieval speeds by using compact hash codes to represent complex and heterogeneous multi-modal data, effectively addressing the inefficiency and resource intensiveness challenges faced by the traditional multi-modal retrieval methods. However, balancing intraclass compactness and interclass separability remains a struggle in existing works due to coarse-grained feature limitations, simplified fusion strategies that overlook semantic complementarity, and neglect of the structural information within the multi-modal data. To address these limitations comprehensively, we propose a Proto-centric Multi-modal Hashing with Pronounced Category Differences (PMH-PCD) model. Specifically, PMH-PCD first learns modality-specific prototypes by deeply exploring within-modality class information, ensuring effective fusion of each modality's unique characteristics. Furthermore, it learns multi-modal integrated class prototypes that seamlessly incorporate semantic information across modalities to effectively capture and represent the intricate relationships and complementary semantic content embedded within the multi-modal data. Additionally, to generate more discriminative and representative binary hash codes, PMH-PCD integrates multifaceted semantic information, encompassing both low-level pairwise relations and high-level structural patterns, holistically capturing intricate data details and leveraging underlying structures. The experimental results demonstrate that, compared with existing advanced methods, PMH-PCD achieves superior and consistent performances in multi-modal retrieval tasks.},
  archive      = {J_TMM},
  author       = {Ruifan Zuo and Chaoqun Zheng and Lei Zhu and Wenpeng Lu and Jiasheng Si and Weiyu Zhang},
  doi          = {10.1109/TMM.2025.3565973},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5843-5856},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Compact-yet-separate: Proto-centric multi-modal hashing with pronounced category differences for multi-modal retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning the scale in reference picture resampling for versatile video coding. <em>TMM</em>, <em>27</em>, 5831-5842. (<a href='https://doi.org/10.1109/TMM.2025.3543098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressing high-resolution videos under low bitrate constraints is a challenging task. Resampling-based compression, which reduces the resolution before encoding and restores it after decoding, has great potential to improve the rate-distortion performance in such scenarios. In this paper, we propose a learning-based frame-level coding scale control scheme that enhances the coding performance by adjusting the coding scale for each frame. The scheme cooperates with the Reference Picture Resampling of the latest video coding standard Versatile Video Coding (VVC), which allows coding scale variations on each frame. More specifically, a dataset with 5200 videos is created by a greedy rate-distortion optimization algorithm employed to select the optimal coding scale for each frame. A neural network-based decision model is further incorporated into VVC, learning to predict the coding scale for each frame in one pass. The scheme is implemented into the Fraunhofer Versatile Video Encoder (VVenC), a fast and efficient VVC encoder, and evaluated on 4 K contents. Experimental results show that the proposed scheme outperforms GOP-based coding scale adaptation methods, achieving average bitrate savings of 3.06% and 4.14% in terms of PSNR and MS-SSIM.},
  archive      = {J_TMM},
  author       = {Riyu Lu and Yingwen Zhang and Hengyu Man and Meng Wang and Shiqi Wang and Xiaopeng Fan},
  doi          = {10.1109/TMM.2025.3543098},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5831-5842},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning the scale in reference picture resampling for versatile video coding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DLS-HCAN: Duplex label smoothing based hierarchical context-aware network for fine-grained 3D shape classification. <em>TMM</em>, <em>27</em>, 5815-5830. (<a href='https://doi.org/10.1109/TMM.2025.3543077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained 3D shape classification (FGSC) has garnered significant attention recently and has made notable advancements. However, due to high inter-class similarity and intra-class diversity, it is still a challenge for existing methods to capture subtle differences between different subcategories for FGSC. On the one hand, one-hot labels in loss function are too hard to describe the above data characteristics, and on the other hand, local details are submerged in the global features extraction process and final network constraints, impacting classification results. In this paper, we propose a duplex label smoothing-based hierarchical context-aware network for fine-grained 3D shape classification, named DLS-HCAN. Specifically, DLS-HCAN firstly employs a hierarchical context-aware network (HCAN), in which the intra-view context attention mechanism (intra-ATT) and the inter-view context multilayer perceptron (inter-MLP) are designed to focus on and discern the beneficial local details. Subsequently, we propose a novel duplex label smoothing (DLS) regularization in which shape-level and view-level smooth labels are separately applied in two improved loss functions, adapting to the fine-grained data characteristics and considering the varying uniqueness of different views. Notably, our approach does not require additional annotation information. Experimental results and comparison with state-of-the-art methods demonstrate the superiority of our proposed DLS-HCAN for FGSC. In addition, our approach also achieves comparable performance for the coarse-grained dataset on ModelNet40.},
  archive      = {J_TMM},
  author       = {Shaojin Bai and Liang Zheng and Jing Bai and Xiangyu Ma},
  doi          = {10.1109/TMM.2025.3543077},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5815-5830},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DLS-HCAN: Duplex label smoothing based hierarchical context-aware network for fine-grained 3D shape classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). You only need clear images: Self-supervised single image dehazing. <em>TMM</em>, <em>27</em>, 5800-5814. (<a href='https://doi.org/10.1109/TMM.2025.3542999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hazing refers to adding haze to a clear image, which is important for improving the data amount and diversity of synthetic hazy images that are required to train deep image dehazing models. However, existing image hazing works generate hazy images from a given clear image with a single transmission map. This violates the fact that hazy images are diverse for a natural scene at different times. The domain shift issue between synthetic and real-world hazy images constrains the robustness of deep dehazing models when dealing with real-world hazy images. In this work, we propose an unsupervised haze generation work to synthesize multiple hazy images with diverse haze distributions from a clear image, which requires only an atmospheric scattering model without extra labeling information. Instead of estimating a transmission map from a clear image, we propose to customize the transmission maps by redefining the transmission function. In such a controllable way, hazy images with diverse haze distributions are generated, which avoids the labor-intensive collection of paired data and alleviates the common domain-shift issue of deep image dehazing. Incorporating the unsupervised hazy images generator, we also construct a generalizable self-supervised image dehazing (SSID) framework, where deep image dehazing models can be trained without any human annotations. Extensive experiments on real-world hazy images show that the proposed approach is superior to state-of-the-art unsupervised dehazing works, and achieves competitive performance with the supervised works. Moreover, the proposed SSID framework can be easily generalized to the existing deep dehazing models, greatly improving dehazing robustness on real-world hazy images.},
  archive      = {J_TMM},
  author       = {Jiyou Chen and Wenqi Ren and Huihuang Zhao and Qunbing Xia and Gaobo Yang},
  doi          = {10.1109/TMM.2025.3542999},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5800-5814},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {You only need clear images: Self-supervised single image dehazing},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Language knowledge-assisted representation learning for skeleton-based action recognition. <em>TMM</em>, <em>27</em>, 5784-5799. (<a href='https://doi.org/10.1109/TMM.2025.3543034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How humans understand and recognize the actions of others is a complex neuroscientific problem that involves a combination of cognitive mechanisms and neural networks. Research has shown that humans have brain areas that recognize actions that process top-down attentional information, such as the temporoparietal association area. Also, humans have brain regions dedicated to understanding the minds of others and analyzing their intentions, such as the medial prefrontal cortex of the temporal lobe. Skeleton-based action recognition creates mappings for the complex connections between the human skeleton movement patterns and behaviors. Although existing studies encoded meaningful node relationships and synthesized action representations for classification with good results, few of them considered incorporating a priori knowledge to aid potential representation learning for better performance. LA-GCN proposes a graph convolution network using large-scale language models (LLM) knowledge assistance. First, the LLM knowledge is mapped into a priori global relationship (GPR) topology and a priori category relationship (CPR) topology between nodes. The GPR guides the generation of new “bone” representations, aiming to emphasize essential node information from the data level. The CPR mapping simulates category prior knowledge in human brain regions, encoded by the PC-AC module and used to add additional supervision—forcing the model to learn class-distinguishable features. In addition, to improve information transfer efficiency in topology modeling, we propose multi-hop attention graph convolution. It aggregates each node's k-order neighbor simultaneously to speed up model convergence. LA-GCN reaches state-of-the-art on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.},
  archive      = {J_TMM},
  author       = {Haojun Xu and Yan Gao and Zheng Hui and Jie Li and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3543034},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5784-5799},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Language knowledge-assisted representation learning for skeleton-based action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geo6D: Geometric-constraints-guided direct object 6D pose estimation network. <em>TMM</em>, <em>27</em>, 5770-5783. (<a href='https://doi.org/10.1109/TMM.2025.3543083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct pose estimation networks aim to directly regress the 6D poses of target objects in the scene image using a neural network. These direct methods offer efficiency and an optimal optimization target, presenting significant potential for practical applications. However, due to the complex and implicit mappings between input features and target pose parameters, direct methods are challenging to train and prone to overfitting on mappings seen during training, resulting in limited effectiveness and generalization capability on unseen mappings. Existing methods focus primarily on improvements of the network architecture and training strategies, with less attention given to mappings. In this work, we propose a geometric constraints learning approach, which enables networks to explicitly capture and utilize the geometric mappings between inputs and optimization targets for pose estimation. Specifically, we introduce a residual pose transformation formula that preserves pose transformation constraints within both the 2D image plane and the 3D space while decoupling the absolute pose distribution, thereby addressing the pose distribution gap issue. We further design a Geo6D mechanism based on the formula, which enables the network to explicitly utilize geometric constraints for pose estimation by reconstructing the inputs and outputs. We select two different methods as our baseline and extensive experiments show that Geo6D enhances the performance and reduces the dependence on extensive training data, remaining effective even with only 10% of the typical data volume.},
  archive      = {J_TMM},
  author       = {Jianqiu Chen and Mingshan Sun and Ye Zheng and Tianpeng Bao and Zhenyu He and Donghai Li and Guoqiang Jin and Zhao Rui and Liwei Wu and Xiaoke Jiang},
  doi          = {10.1109/TMM.2025.3543083},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5770-5783},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Geo6D: Geometric-constraints-guided direct object 6D pose estimation network},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal dual-graph collaborative network with serial attentive aggregation mechanism for micro-video multi-label classification. <em>TMM</em>, <em>27</em>, 5758-5769. (<a href='https://doi.org/10.1109/TMM.2025.3542895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing commercial value of micro-videos has spurred a rising demand for grasping their contents. The abundant multimodal cues in micro-videos exhibit substantial potential in enhancing content comprehension. However, effectively harnessing the collaborative characteristics across different modalities remains a significant challenge, especially in multi-label scenarios due to inconsistent behaviors regarding label correlations. To better tackle this issue, in this paper, we first introduce a multimodal dual-graph collaborative network with serial attentive aggregation mechanism (MDGCN) for micro-video multi-label classification. In MDGCN, we exploit an asymmetric encoder-decoder framework, which incorporates multiple parallel encoders with complementary representations and a decoder to ensure the completeness of encoded results. Meanwhile, an adversarial constraint is used to ensure individual differences prominently featured within each modality. Furthermore, considering the inconsistency of label correlations across various modalities, we then construct a serial attentive graph convolutional network that employs an interactive dual-graph attention paradigm to sequentially integrate multimodal representations and dynamically explore label correlations. The experiments conducted on two datasets demonstrate that our proposed method outperforms state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Yu Qiao and Wei Lu and Peiguang Jing and Weiming Wang and Yuting Su},
  doi          = {10.1109/TMM.2025.3542895},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5758-5769},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multimodal dual-graph collaborative network with serial attentive aggregation mechanism for micro-video multi-label classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HGFormer: Topology-aware vision transformer with HyperGraph learning. <em>TMM</em>, <em>27</em>, 5746-5757. (<a href='https://doi.org/10.1109/TMM.2025.3542958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computer vision community has witnessed an extensive exploration of vision transformers in the past two years. Drawing inspiration from traditional schemes, numerous works focus on introducing vision-specific inductive biases. However, the implicit modeling of permutation invariance and fully-connected interaction with individual tokens disrupts the regional context and spatial topology, further hindering higher-order modeling. This deviates from the principle of perceptual organization that emphasizes the local groups and overall topology of visual elements. Thus, we introduce the concept of hypergraph for perceptual exploration. Specifically, we propose a topology-aware vision transformer called HyperGraph Transformer (HGFormer). Firstly, we present a Center Sampling K-Nearest Neighbors (CS-KNN) algorithm for semantic guidance during hypergraph construction. Secondly, we present a topology-aware HyperGraph Attention (HGA) mechanism that integrates hypergraph topology as perceptual indications to guide the aggregation of global and unbiased information during hypergraph messaging. Using HGFormer as visual backbone, we develop an effective and unitive representation, achieving distinct and detailed scene depictions. Empirical experiments show that the proposed HGFormer achieves competitive performance compared to the recent SoTA counterparts on various visual benchmarks. Extensive ablation and visualization studies provide comprehensive explanations of our ideas and contributions.},
  archive      = {J_TMM},
  author       = {Hao Wang and Shuo Zhang and Biao Leng},
  doi          = {10.1109/TMM.2025.3542958},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5746-5757},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HGFormer: Topology-aware vision transformer with HyperGraph learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple adaptation network for multi-source and multi-target domain adaptation. <em>TMM</em>, <em>27</em>, 5731-5745. (<a href='https://doi.org/10.1109/TMM.2025.3543094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source domain adaptation (MSDA) has garnered significant attention due to its emphasis on transferring knowledge from multiple labeled source domains to a single unlabeled target domain. MSDA requires sufficient labeled data from multiple source domains, but in practice, massive unlabeled data exist instead of well-labeled data. Multiple target domains also provide plenty of information, which is useful for domain adaptation. However, most MSDA studies overlook the critical scenario of multi-source and multi-target domain adaptation (MMDA). To address these problems, we propose a Multiple Adaptation Network (MAN) approach for MMDA, which utilizes multiple alignment strategies for each source-target domain pair-group to align relevant specific feature spaces. MAN also aligns multiple classifiers for the relevant feature spaces to optimize the decision boundaries of multiple target domains. Moreover, to consider the task relations of multiple classifiers, we minimize the semantic differences between the target-conditioned classifiers and utilize a weight learning category to optimize this process. To fully utilize the information from multiple target domains, we transfer the style information of the target data to the source data, aiding in the training of multiple classifiers. Extensive experiments in challenge domain adaptation benchmarks, including the ImageCLEF-DA, Office-Home, DomainNet, and RGB-to-thermal datasets, demonstrate the superiority of our method over the state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Yuwu Lu and Haoyu Huang and Xue Hu and Zhihui Lai},
  doi          = {10.1109/TMM.2025.3543094},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5731-5745},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multiple adaptation network for multi-source and multi-target domain adaptation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frefusion: Frequency domain transformer for infrared and visible image fusion. <em>TMM</em>, <em>27</em>, 5722-5730. (<a href='https://doi.org/10.1109/TMM.2025.3543019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible and infrared image fusion(VIF) provides more comprehensive understanding of a scene and can facilitate subsequent processing. Although frequency domain contains valuable global information in low frequency and rapid pixel intensity variation data in high frequency of images, existing fusion methods mainly focus on spatial domain. To close this gap, a novel VIF method in frequency domain is proposed. First, a frequency-domain feature extraction module is developed for source images. Then, a frequency-domain transformer fusion method is designed to merge the extracted features. Finally, a residual reconstruction module is introduced to obtain final fused images. To the best of our knowledge, it is the first time that image fusion study is conducted from frequency domain perspective. Comprehensive experiments on three datasets, i.e., MSRS, TNO, and Roadscene, demonstrate that the proposed approach obtains superior fusion performance over several state-of-the-art fusion methods, indicating its great potential as a generic backbone for VIF tasks.},
  archive      = {J_TMM},
  author       = {Junjie Shi and Puhong Duan and Xiaoguang Ma and Jianning Chi and Yong Dai},
  doi          = {10.1109/TMM.2025.3543019},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5722-5730},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frefusion: Frequency domain transformer for infrared and visible image fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale retinex unfolding network for low-light image enhancement. <em>TMM</em>, <em>27</em>, 5709-5721. (<a href='https://doi.org/10.1109/TMM.2025.3543015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinex theory-based low-light image enhancement methods have received increasing attention and achieved tremendous advancements. However, there still exist two seldom-explored issues: 1) The above methods only formally simulate the Retinex decomposition, resulting in lacking explicit interpretability. 2) They usually are performed in single-scale space, leading to suboptimal enhancement results. In this paper, we propose an interpretable Multi-scale Retinex Unfolding Network (MRUNet) for low-light image enhancement, which can tackle both of the aforementioned issues simultaneously. Specifically, we formulate low-light image enhancement as a multi-scale Retinex optimization problem and design an iteration minimization solution to solve it. The optimization solution is further unfolded to fabricate MRUNet, which is empowered with clear physical significance and multi-scale prior knowledge in favor of image enhancement. However, it will aggravate model size and efficiency when exploiting multiple proximal mapping networks to extract multi-scale prior from multi-scale inputs. To surmount the issue, we propose a Scale-Aware Proximal mapping Module (SAPM), which efficiently collect multi-scale prior knowledge via the weight sharing strategy. In SAPM, we tailor a scale-aware transformer to model the specific scale-similarity among different scales. Extensive experiments manifest that MRUNet surpasses other Retinex-based low-light image enhancement methods on multiple benchmarks.},
  archive      = {J_TMM},
  author       = {Huake Wang and Xingsong Hou and Jutao Li and Yadi Yan and Wenke Sun and Xin Zeng and Kaibing Zhang and Xiangyong Cao},
  doi          = {10.1109/TMM.2025.3543015},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5709-5721},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale retinex unfolding network for low-light image enhancement},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous bijection supervised pyramid diffeomorphic deformation for learning tooth meshes from CBCT images. <em>TMM</em>, <em>27</em>, 5696-5708. (<a href='https://doi.org/10.1109/TMM.2025.3543091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and high-quality tooth mesh generation from cone-beam computerized tomography (CBCT) is an essential computer-aided technology for digital dentistry. However, existing segmentation-based methods require complicated post-processing and significant manual correction to generate regular tooth meshes. In this paper, we propose a method of continuous bijection supervised pyramid diffeomorphic deformation (PDD) for learning tooth meshes, which could be used to directly generate high-quality tooth meshes from CBCT Images. Overall, we adopt a classic two-stage framework. In the first stage, we devise an enhanced detector to accurately locate and crop every tooth. In the second stage, a PDD network is designed to deform a sphere mesh from low resolution to high one according to pyramid flows based on diffeomorphic mesh deformations, so that the generated mesh approximates the ground truth infinitely and efficiently. To achieve that, a novel continuous bijection distance loss on the diffeomorphic sphere is also designed to supervise the deformation learning, which overcomes the shortcoming of loss based on nearest-neighbour mapping and improves the fitting precision. Experiments show that our method outperforms the state-of-the-art methods in terms of both different evaluation metrics and the geometry quality of reconstructed tooth surfaces.},
  archive      = {J_TMM},
  author       = {Zechu Zhang and Weilong Peng and Jinyu Wen and Keke Tang and Meie Fang and David Dagan Feng and Ping Li},
  doi          = {10.1109/TMM.2025.3543091},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5696-5708},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Continuous bijection supervised pyramid diffeomorphic deformation for learning tooth meshes from CBCT images},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse visible-to-thermal image translation via controllable temperature encoding. <em>TMM</em>, <em>27</em>, 5685-5695. (<a href='https://doi.org/10.1109/TMM.2025.3543053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Translating readily available visible (VIS) images into thermal infrared (TIR) images effectively alleviates the shortage of TIR data. While current methods have yielded commendable results, they fall short in generating diverse and realistic thermal infrared images, primarily due to insufficient consideration of temperature variations. In this paper, we propose a Thermally Controlled GAN (TC-GAN) that leverages VIS images to generate diverse TIR images, with the ability to control the relative temperatures of multiple objects, particularly those with temperature variations. Firstly, we introduce the physical coding module, which employs a conditional variational autoencoder GAN to learn the distributions of relative temperature information for the objects and environmental state information. Then, the physical information can be obtained by sampling the distribution. When this information is fused with the visible image, it facilitates the generation of diverse TIR images. To ensure authenticity and strengthen the physical constraints across different regions of the image, we introduce a self-attention mechanism in the generator that prioritizes the relative temperature relationships within the image. Additionally, we utilize a local discriminator that focuses on objects with actively changing temperatures and their interactions with the surrounding environment, thereby reducing the discontinuity between the target and the background. Experiments on the Drone Vehicle and AVIID datasets show that our approach outperforms mainstream diversity generation methods in terms of authenticity and diversity.},
  archive      = {J_TMM},
  author       = {Lei Zhao and Mengwei Li and Bo Li and Xingxing Wei},
  doi          = {10.1109/TMM.2025.3543053},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5685-5695},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Diverse visible-to-thermal image translation via controllable temperature encoding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust coverless audio steganography based on differential privacy clustering. <em>TMM</em>, <em>27</em>, 5669-5684. (<a href='https://doi.org/10.1109/TMM.2025.3543107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional audio steganography methods typically require embedding secret information into the carrier, making them vulnerable to steganalysis. To address this issue, we propose a novel coverless audio steganography method that hides information by generating carriers and establishing mapping rules rather than embedding data directly. Our approach leverages a differential privacy clustering algorithm to cluster audio data and select representative audio files, thereby enhancing the security of the steganography. Additionally, we introduce an improved audio feature extraction method that combines traditional Mel-frequency cepstral coefficients (MFCC) with global statistical information, significantly boosting the robustness of the secret information against common audio attacks, particularly time-stretching attacks. Experimental results show that our method achieves a robustness rate of up to 95% against time-stretching and maintains an average security accuracy rate exceeding 97% across various attack scenarios. The proposed method ensures that the audio carrier remains unaltered, thus effectively resisting detection by steganalysis tools. This innovative approach provides a practical and efficient solution for the secure transmission of information in the digital era.},
  archive      = {J_TMM},
  author       = {Yan Feng and Longting Xu and Xiaochen Lu and Guanglin Zhang and Wei Rao},
  doi          = {10.1109/TMM.2025.3543107},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5669-5684},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A robust coverless audio steganography based on differential privacy clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CR-famba: A frequency-domain assisted mamba for thin cloud removal in optical remote sensing imagery. <em>TMM</em>, <em>27</em>, 5659-5668. (<a href='https://doi.org/10.1109/TMM.2025.3542976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical remote sensing images are inevitably affected by cloud cover. To remove clouds from optical remote sensing images, a series of deep learning-based thin cloud removal methods have been developed. However, these methods have not explored the long-range modeling ability of state space models in optical remote sensing image thin cloud removal. In this paper, we propose a frequency-domain assisted Mamba for thin cloud removal, which is called CR-Famba. In CR-Famba, to better extract global and local features of images, we design a frequency-domain assisted state space layer (FDA-SSL). The FDA-SSL consists of two core components: residual state space block (RSSB) and frequency domain detail enhancement block (FDDEB). The RSSB utilizes the visual state space module (VSSM) to extract long-range dependencies of images from a spatial perspective while adding convolutional layers to overcome local pixel forgetting. Due to the rich detailed information of remote sensing images, we present FDDEB equipped with discrete wavelet transform (DWT) to supplement the extracted local information from the frequency domain perspective. We conduct experiments on different types of cloud-containing datasets, and the results show that our method can recover images with clearer texture details compared to other methods.},
  archive      = {J_TMM},
  author       = {Jiao Liu and Bin Pan and Zhenwei Shi},
  doi          = {10.1109/TMM.2025.3542976},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5659-5668},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CR-famba: A frequency-domain assisted mamba for thin cloud removal in optical remote sensing imagery},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking depth guided reflection removal. <em>TMM</em>, <em>27</em>, 5647-5658. (<a href='https://doi.org/10.1109/TMM.2025.3543096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When photographing through glass, reflections are often observed, which negatively impact the quality of the captured images or videos. In this article, we summarize and rethink depth guided reflection removal methods and, inspired by the human binocular vision system, investigate how to utilize depth for effective binocular video reflection removal. We propose an end-to-end learning-based reflection removal method that learns the transmission depth and designs a unified structure to achieve depth guided, cross-view, and cross-frame feature enhancement in a cascaded manner. Within the unified structure, different gating controllers are custom-designed to emphasize the direction of feature interaction. A dataset containing synthetic and real binocular mixture video dataset is built for network training and testing. Experimental results on both synthetic and real data from the proposed dataset demonstrate that the proposed method achieves superior performance in binocular video reflection removal.},
  archive      = {J_TMM},
  author       = {Lingzhi He and Yakun Chang and Runmin Cong and Hongyu Liu and Shujuan Huang and Renshuai Tao and Yao Zhao},
  doi          = {10.1109/TMM.2025.3543096},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5647-5658},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rethinking depth guided reflection removal},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-directional deep contextual video compression. <em>TMM</em>, <em>27</em>, 5632-5646. (<a href='https://doi.org/10.1109/TMM.2025.3543061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep video compression has made impressive process in recent years, with the majority of advancements concentrated on P-frame coding. Although efforts to enhance B-frame coding are ongoing, their compression performance is still far behind that of traditional bi-directional video codecs. In this article, we introduce a bi-directional deep contextual video compression scheme tailored for B-frames, termed DCVC-B, to improve the compression performance of deep B-frame coding. Our scheme mainly has three key innovations. First, we develop a bi-directional motion difference context propagation method for effective motion difference coding, which significantly reduces the bit cost of bi-directional motions. Second, we propose a bi-directional contextual compression model and a corresponding bi-directional temporal entropy model, to make better use of the multi-scale temporal contexts. Third, we propose a hierarchical quality structure-based training strategy, leading to an effective bit allocation across large groups of pictures (GOP). Experimental results show that our DCVC-B achieves an average reduction of 26.6% in BD-Rate compared to the reference software for H.265/HEVC under random access conditions. Remarkably, it surpasses the performance of the H.266/VVC reference software on certain test datasets under the same configuration. We anticipate our work can provide valuable insights and bring up deep B-frame coding to the next level.},
  archive      = {J_TMM},
  author       = {Xihua Sheng and Li Li and Dong Liu and Shiqi Wang},
  doi          = {10.1109/TMM.2025.3543061},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5632-5646},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bi-directional deep contextual video compression},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with noisy low-cost MOS for image quality assessment via dual-bias calibration. <em>TMM</em>, <em>27</em>, 5617-5631. (<a href='https://doi.org/10.1109/TMM.2025.3543014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based Image Quality Assessment (IQA) models have obtained impressive performance with the help of reliable subjective quality labels, where Mean Opinion Score (MOS) is the most popular choice. However, in view of the subjective bias of individual annotators, the Labor-Abundant MOS (LA-MOS) typically requires large collections of opinion scores from multiple annotators for each image, which significantly increases the learning cost. In this paper, we aim to learn robust IQA models from Low-Cost MOS (LC-MOS), which only requires very few opinion scores or even a single opinion score for each image. More specifically, we consider the LC-MOS as the noisy observation of LA-MOS and enforce the IQA model learned from LC-MOS to approach the unbiased estimation of LA-MOS. Thus, we represent the subjective bias between LC-MOS and LA-MOS, and the model bias between IQA predictions learned from LC-MOS and LA-MOS (i.e., dual-bias) as two latent variables with unknown parameters. By means of the expectation-maximization-based alternating optimization, we can jointly estimate the parameters of the dual-bias, which suppresses the misleading of LC-MOS via a gated dual-bias calibration (GDBC) module. To the best of our knowledge, this is the first exploration of robust IQA model learning from noisy low-cost labels. Theoretical analysis and extensive experiments on four popular IQA datasets show that the proposed method is robust toward different bias rates and annotation numbers and significantly outperforms the other Learning-based IQA models when only LC-MOS is available. Furthermore, we also achieve comparable performance with respect to the other models learned with LA-MOS.},
  archive      = {J_TMM},
  author       = {Lei Wang and Qingbo Wu and Desen Yuan and King Ngi Ngan and Hongliang Li and Fanman Meng and Linfeng Xu},
  doi          = {10.1109/TMM.2025.3543014},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5617-5631},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning with noisy low-cost MOS for image quality assessment via dual-bias calibration},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond correlation: Evaluating multimedia quality models with the constrained concordance index. <em>TMM</em>, <em>27</em>, 5604-5616. (<a href='https://doi.org/10.1109/TMM.2025.3542991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the evaluation of multimedia quality models, focusing on the inherent uncertainties in subjective Mean Opinion Score (MOS) ratings due to factors like rater inconsistency and bias. Traditional statistical measures such as Pearson's Correlation Coefficient (PCC), Spearman's Rank Correlation Coefficient (SRCC), and Kendall's Tau (KTAU) often fail to account for these uncertainties, leading to inaccuracies in model performance assessment. We introduce the Constrained Concordance Index (CCI), a novel metric designed to overcome the limitations of existing metrics by considering the statistical significance of MOS differences and excluding comparisons where MOS confidence intervals overlap. Through comprehensive experiments across various domains including speech and image quality assessment, we demonstrate that CCI provides a more robust and accurate evaluation of instrumental quality models, especially in scenarios of low sample sizes, rater group variability, and restriction of range. Our findings suggest that incorporating rater subjectivity and focusing on statistically significant pairs can significantly enhance the evaluation framework for multimedia quality prediction models. This work not only sheds light on the overlooked aspects of subjective rating uncertainties but also proposes a methodological advancement for more reliable and accurate quality model evaluation.},
  archive      = {J_TMM},
  author       = {Alessandro Ragano and Helard Becerra Martinez and Andrew Hines},
  doi          = {10.1109/TMM.2025.3542991},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5604-5616},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Beyond correlation: Evaluating multimedia quality models with the constrained concordance index},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSAF: Dual space alignment framework for visible-infrared person re-identification. <em>TMM</em>, <em>27</em>, 5591-5603. (<a href='https://doi.org/10.1109/TMM.2025.3542988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is a cross-modality retrieval task that aims to match visible and infrared pedestrian images across non-overlapped cameras. However, we observe that three crucial challenges remain inadequately addressed by existing methods: (i) limited discriminative capacity for modality-shared representation, (ii) modality misalignment, and (iii) neglect of identity consistency knowledge. To solve the above issues, we propose a novel dual space alignment framework (DSAF) to constrain the modality in two specific spaces. Specifically, for (i), we design a lightweight and plug-and-play modality invariant enhancement (MIE) module to capture fine-grained semantic information and render identity discriminative. This facilitates the establishment of correlations between visible and infrared modalities, enabling the model to learn robust modality-shared features. To tackle (ii), a dual space alignment (DSA) is introduced to conduct the pixel-level alignment in both Euclidean space and Hilbert space. DSA establishes an elastic relationship between these two spaces, remaining invariant knowledge across two spaces. To solve (iii), we propose an adaptive identity-consistent learning (AIL) to discover identity-consistent knowledge between visible and infrared modalities in a dynamic manner. Extensive experiments on mainstream VI-ReID benchmarks show the superiority and flexibility of our proposed method, achieving competitive performance on mainstream datasets.},
  archive      = {J_TMM},
  author       = {Yan Jiang and Xu Cheng and Hao Yu and Xingyu Liu and Haoyu Chen and Guoying Zhao},
  doi          = {10.1109/TMM.2025.3542988},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5591-5603},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DSAF: Dual space alignment framework for visible-infrared person re-identification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCDL: Dual causal disentangled learning for zero-shot sketch-based image retrieval. <em>TMM</em>, <em>27</em>, 5575-5590. (<a href='https://doi.org/10.1109/TMM.2025.3543035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot sketch-based image retrieval (ZS-SBIR) is a challenging task that hinges on overcoming the cross-domain differences between sketches and images. Previous methods primarily address cross-domain differences by creating a common embedding space, improving final retrieval results. However, most previous approaches have overlooked a critical aspect: sketch-based image retrieval task actually requires only the cross-domain invariant information relevant to the retrieval. Irrelevant information (such as posture, expression, background, and specificity) may detract from retrieval accuracy. In addition, most previous methods perform well on traditional SBIR datasets but lack corresponding research on generalization and extensibility in the face of more diverse and complex data. To address these issues, we propose a Dual Causal Disentangled Learning (DCDL) for ZS-SBIR. This approach can mitigate the negative impact of irrelevant features by separating retrieval-relevant features in the latent variable space. Specifically, we constructed a causal disentanglement model using two Variational Autoencoders (VAE), each applied to the sketch and image domains, to obtain disentangled variables with exchangeable attributes. Our framework effectively integrates causal intervention with disentangled representation learning, enabling a clearer separation of cross-domain retrieval-relevant and intra-class irrelevant features, which can be recombined into new reconstructed samples. Concurrently, we designed a Dual Alignment Module (DAM), leveraging the accurate and comprehensive semantic features provided by a text encoder pre-trained on large-scale datasets to supplement semantic associations and align disentangled retrieval-relevant features. The Dual Alignment Module enhances the model's ability to generalize across diverse datasets by effectively aligning retrieval-relevant information from different domains. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) performance on the Sketchy and TU–Berlin datasets. Additionally, more experiments on larger scale dataset QuickDraw, fine-grained datasets, Shoe-V2 and Chair-V2, as well as an inter-dataset further validate the generalization and extensibility of DCDL.},
  archive      = {J_TMM},
  author       = {Qiang Li and Shihao Wang and Wei Zhang and Shaojin Bai and Weizhi Nie and Anan Liu},
  doi          = {10.1109/TMM.2025.3543035},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5575-5590},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DCDL: Dual causal disentangled learning for zero-shot sketch-based image retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure neural network watermarking protocol against evidence exposure attack. <em>TMM</em>, <em>27</em>, 5563-5574. (<a href='https://doi.org/10.1109/TMM.2025.3542975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trigger-based backdoor watermarking is an extensively utilized and effective method to safeguard the copyright of deep neural networks (DNNs), in which the trigger set could be taken as the key of the watermark. However, during the verification stage, there is a risk that the trigger set could be leaked and exposed to adversaries. If this occurs, the adversaries might apply this leaked trigger set to claim ownership of the model, posing significant copyright issues for the watermarked DNN. To address such an evidence exposure problem, a secure neural network watermarking protocol is put forward in this paper. In the proposed protocol, the trigger set is not fixed, once the trigger is utilized for verification, it is invalid and cannot be used for verification in the future. As a result, even if the trigger set is leaked during the verification process and obtained by the attacker, they cannot use it for copyright verification since it is invalid. To assist the protocol, a trigger set generation method is designed, in which the auxiliary classifier generative adversarial network (ACGAN) and the target classification model are trained together. The special logits distribution and the labels of the generated trigger samples can be ensured and verified effectively in this way. The performance of the trigger generation methods regarding effectiveness, fidelity, and robustness is verified by experiments, and the security analysis of the designed watermarking protocol is conducted.},
  archive      = {J_TMM},
  author       = {Huixin Luo and Li Li and Xinpeng Zhang},
  doi          = {10.1109/TMM.2025.3542975},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5563-5574},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Secure neural network watermarking protocol against evidence exposure attack},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-aware semi-supervised learning segmentation for remote sensing images. <em>TMM</em>, <em>27</em>, 5548-5562. (<a href='https://doi.org/10.1109/TMM.2025.3543026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based remote sensing (RS) image segmentation significantly impacts several real application scenarios. Behind its success, massive labeled data plays an important role. However, annotating high-resolution RS images requires time-consuming and relevant expertise efforts. To address it, many works dive into semi-supervised learning which utilizes raw information embedded in unlabeled data to improve the segmentation model. Nevertheless, previous studies ignore the integrity and effectiveness of the potential context information hidden in RS data. In this work, we propose an uncertainty-aware masked consistency learning (U-MCL) framework that contains an uncertainty-aware masked denoising (U-MD) module and an uncertainty-aware masked image consistency (U-MIC) module. U-MCL initially generates a patch-wise uncertainty map for each unlabeled image during each training iteration, which is then used to derive an adaptive mask ratio for pseudo-label denoising in U-MD. Simultaneously, the uncertainty map is adopted to model a masked unlabeled image for reasoning unseen areas in U-MIC. Consequently, U-MCL is capable of enhancing model performance by engaging in accurate and stable consistency learning while preserving the integrity of the context and employing the context to infer the predictions of the masked regions safely. Extensive experiments on six RS datasets, i.e., ISPRS Vaihingen, FloodNet, MiniFrance, LoveDA, MER, and MSL, demonstrate the superiority of our U-MCL over recent most advanced methods, achieving new state-of-the-art performance under all benchmarks.},
  archive      = {J_TMM},
  author       = {Xiaoqiang Lu and Lingling Li and Licheng Jiao and Xu Liu and Fang Liu and Wenping Ma and Shuyuan Yang},
  doi          = {10.1109/TMM.2025.3543026},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5548-5562},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty-aware semi-supervised learning segmentation for remote sensing images},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMoA: Contrastive mixture of adapters for generalized few-shot continual learning. <em>TMM</em>, <em>27</em>, 5533-5547. (<a href='https://doi.org/10.1109/TMM.2025.3543038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of Few-Shot Continual Learning (FSCL) is to incrementally learn novel tasks with limited labeled samples and preserve previous capabilities simultaneously. However, current FSCL works lack research on domain increment and domain generalization ability, which cannot cope with changes in the visual perception environment. In this paper, we set up a Generalized FSCL (GFSCL) protocol involving both class- and domain-incremental scenarios together with domain generalization assessment. Firstly, two benchmark datasets and protocols are newly arranged, and detailed baselines are provided for this unexplored configuration. Furthermore, we find that common continual learning methods have poor generalization ability on unseen domains and cannot better tackle catastrophic forgetting issue in cross-incremental tasks. Hence, we propose a rehearsal-free framework based on Vision Transformer (ViT) named Contrastive Mixture of Adapters (CMoA). It contains two non-conflicting parts: (1) By applying the fast-adaptation characteristic of adapter-embedded ViT, the mixture of Adapters (MoA) module is incorporated into ViT. For stability purpose, cosine similarity regularization and dynamic weighting are designed to make each adapter learn specific knowledge and concentrate on particular classes. (2) To further enhance domain generalization ability, we alleviate the intra-class variation by prototype-calibrated contrastive learning to improve domain-invariant representation learning. Finally, six evaluation indicators showing the overall performance and forgetting are compared by comprehensive experiments on two benchmark datasets to validate the efficacy of CMoA, and the results illustrate that CMoA can achieve comparative performance with rehearsal-based continual learning methods.},
  archive      = {J_TMM},
  author       = {Yawen Cui and Jian Zhao and Zitong Yu and Rizhao Cai and Xun Wang and Lei Jin and Alex C. Kot and Li Liu and Xuelong Li},
  doi          = {10.1109/TMM.2025.3543038},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5533-5547},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CMoA: Contrastive mixture of adapters for generalized few-shot continual learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph contrastive learning for fusion of graph structure and attribute information. <em>TMM</em>, <em>27</em>, 5521-5532. (<a href='https://doi.org/10.1109/TMM.2025.3542984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) plays a crucial role in multimedia applications due to its effectiveness in analyzing graph-structured data. Existing GCL methods focus on maximizing the agreement of node representations across different augmentations, which leads to the neglect of unique and complementary information in each augmentation. In this paper, we propose a fusion-based GCL model (FB-GCL) that learns fused representations to effectively capture complementary information from both the graph structure and node attributes. Our model consists of two modules: a graph fusion encoder and a graph contrastive module. The graph fusion encoder adaptively fuses the representations learned from the topology graph and the attribute graph. The graph contrastive module extracts supervision signals from the raw graph by leveraging both the pairwise relationships within the graph structure and the multi-label information from the attributes. Extensive experiments on seven benchmark datasets demonstrate that FB-GCL enhances performance in node classification and link prediction tasks. This improvement is especially valuable for multimedia data analysis, as integrating graph structure and attribute information is crucial for effectively understanding and processing complex datasets.},
  archive      = {J_TMM},
  author       = {Zhuomin Liang and Liang Bai and Xian Yang and Jiye Liang},
  doi          = {10.1109/TMM.2025.3542984},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5521-5532},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Graph contrastive learning for fusion of graph structure and attribute information},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class incremental learning for image classification with out-of-distribution task identification. <em>TMM</em>, <em>27</em>, 5507-5520. (<a href='https://doi.org/10.1109/TMM.2025.3543088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class Incremental Learning (CIL) for image classification aims to address real-world scenarios by allowing a model to learn new categories while retaining the knowledge of old categories. It is more challenging than Task Incremental Learning (TIL) as task ID is not provided during testing. Therefore, transitioning from CIL to TIL is an intuitive approach to handling CIL problems for image classification. Currently, the main challenge of this approach lies in improving the accuracy of task identification. To address this issue, we propose to use a large-scale image-text pre-training model (i.e. CLIP) as the backbone, training and saving different classifiers for different tasks. Each classifier not only includes the classes of the current task, but also an Out-of-distribution (OOD) class corresponding to the classes encountered in all previous tasks. At test time, we iterate through classifiers from the last task to find the correct task ID of the test image, and perform classification in a TIL way. In addition, to tackle the issue of early-stop termination in iterative prediction due to model bias toward later tasks, we propose using CLIP zero-shot ability to assist learned OOD detection. Experiments show that our method achieves state-of-the-art performance on the traditional many-shot and the more challenging few-shot settings of CIFAR-100 and ImageNet-Subset datasets.},
  archive      = {J_TMM},
  author       = {Xusheng Cao and Haori Lu and Xialei Liu and Ming-Ming Cheng},
  doi          = {10.1109/TMM.2025.3543088},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5507-5520},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Class incremental learning for image classification with out-of-distribution task identification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real scene single image dehazing network with multi-prior guidance and domain transfer. <em>TMM</em>, <em>27</em>, 5492-5506. (<a href='https://doi.org/10.1109/TMM.2025.3543063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is essential to boost the visual quality of images captured in hazy conditions. Recently, many learning-based methods were proposed to achieve single image dehazing with the training of tremendous paired synthetic hazy/ real clean images. Due to the domain gap between real and synthetic scenes, these models cannot generalize well to various real hazy scenes, leading to under-dehazed results. To overcome this problem, we propose a real scene image Dehazing Network with Multi-prior Guidance and Domain Transfer (DNMGDT). Our DNMGDT is based on a parameter shared architecture trained by synthetic hazy images and real hazy images simultaneously. For real hazy images, multiple prior-based dehazed images are adopted as pseudo clean images. An Image Quality Guided Adaptive Weighting (IQGAW) scheme is proposed to form the supervision by automatically weighting different parts of these prior-based dehazed images and suppressing negative information of them. Moreover, to reduce the domain gap between real and synthetic hazy scenes, a Physical Model Guided image level Domain Transfer (PMGDT) mechanism is proposed to regularize the learning process with consistency constraint. Experiments on various datasets demonstrated the effectiveness of our proposed method especially for real hazy scenes.},
  archive      = {J_TMM},
  author       = {Yanzhao Su and Nian Wang and Zhigao Cui and Yanping Cai and Chuan He and Aihua Li},
  doi          = {10.1109/TMM.2025.3543063},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5492-5506},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Real scene single image dehazing network with multi-prior guidance and domain transfer},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RDVC: Efficient deep video compression with regulable rate and complexity optimization. <em>TMM</em>, <em>27</em>, 5480-5491. (<a href='https://doi.org/10.1109/TMM.2025.3543005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep video coding has paved a way to break through the performance bottleneck of reigning hybrid video coding. However, unlike hybrid video codecs, existing deep video codecs cannot offer both flexible rates and regulable complexities within one single codec, which limits their applications. In this article, we propose a Regulable Deep Video Codec (RDVC) to address the above issue. First, we propose an Adaptive Feature Compression (AFC) network that generates variable rates while ensuring Rate-Distortion (RD) performance. The network introduces a two-stage coarse-to-fine rate adjustment that can be controlled by a user-specified rate level. Second, we propose a Spatio-Temporal Feature Propagation (STFP) mechanism to provide high-quality reference information for AFC process. Third, we also utilize slimmable convolutional components in our framework to adjust decoding complexity constrained by user configuration. Experimental results demonstrate that RDVC can adjust the codec structure flexibly according to different user configurations while maintaining advanced performance. On average, it reduces the bit-per-pixel (bpp) by 9.35%$/$58.12% while maintaining the same PSNR/MS-SSIM as the reference software VTM-13.2.},
  archive      = {J_TMM},
  author       = {Xiaojie Wei and Jielian Lin and Jiawei Xu and Wei Gao and Tiesong Zhao},
  doi          = {10.1109/TMM.2025.3543005},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5480-5491},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RDVC: Efficient deep video compression with regulable rate and complexity optimization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imperceptible backdoor attacks on text-guided 3D scene grounding. <em>TMM</em>, <em>27</em>, 5466-5479. (<a href='https://doi.org/10.1109/TMM.2025.3543050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the maturity of depth sensors, the vulnerability of 3D point cloud models has received increasing attention in various applications such as autonomous driving and robot navigation. Previous 3D adversarial attackers mainly focus on attacking naive 3D classification models by perturbing 3D objects. However, since real-world 3D applications generally rely on more complicated scene-based point cloud data, these attack methods are impractical to deploy in realistic scenarios. Therefore, in this paper, we attempt to introduce the adversarial attacks into a more practical yet challenging large-scale scene-based 3D task, i.e., text-guided 3D scene grounding. To make perturbations both effective and imperceptible in scene cases, we investigate the vulnerability of 3D grounding models to backdoor attacks, which implant backdoor triggers into 3D models via data poisoning so as to control the models' predictions at test time. Specifically, we propose a novel Joint Scene-Text Backdoor Attack (JSTBA) method to embed triggers in each of the input modalities and activate the malicious behavior only when both triggers are present. We further design a visual trigger optimization strategy to place the visual trigger appropriately in the 3D scene, aiming to make it natural and imperceptible. Extensive experiments are conducted on seven classic 3D grounding models and three datasets, showing that our JSTBA attack significantly degrades the performance of 3D models on the poisoned data while gaining comparable performance with the benign models on the clean data.},
  archive      = {J_TMM},
  author       = {Daizong Liu and Wei Hu},
  doi          = {10.1109/TMM.2025.3543050},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5466-5479},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Imperceptible backdoor attacks on text-guided 3D scene grounding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Copy-move forgery image detection based on cross-scale modeling and alternating refinement. <em>TMM</em>, <em>27</em>, 5452-5465. (<a href='https://doi.org/10.1109/TMM.2025.3543057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of image tampering, specifically copy detection, is an important problem in many domains such as military, media, and public opinion outlets. Effective means to detect such tampering is crucial in controlling the dissemination of false information. However, a major challenge in achieving high detection accuracy lies in the variability of the scale of the copied targets. To tackle this problem, we introduce an all-encompassing methodology called Cross-Scale Modeling and Alternating Refinement (CANet) to detect the genuine source and tampered region at the pixel level. CANet consists of three modules: the Cross-Scale Similar Region Detection (CS) module, the Edge-Supervised Tamper Region Detection (ET) module, and the Alternating Refinement (AR) module. The CS module extracts coarse similar region features by cross-scale correlation modeling, which can alleviate the scale gap between the source and tampered region. The obtained coarse similar region feature is refined by the AR module, in which we introduce the source and the tampered region as the auxiliary information and employ a two-stage process that sequentially models their global feature representations. The tampered region used in the AR module is obtained from the ET module using edge supervision with a salient edge selection scheme, and the source region is generated by the implicit modeling. We conducted experiments on the USC-ISI, CASIA v2.0, CoMoFoD, and MICC-F220 datasets separately. Results show that our method outperforms the state-of-the-art.},
  archive      = {J_TMM},
  author       = {Jingyu Wang and Jie Nie and Niantai Jing and Xinyue Liang and Xiaodong Wang and Chi-Hung Chi and Zhiqiang Wei},
  doi          = {10.1109/TMM.2025.3543057},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5452-5465},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Copy-move forgery image detection based on cross-scale modeling and alternating refinement},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lossless LiDAR point cloud reflectance compression with a deep hierarchical KNN context model. <em>TMM</em>, <em>27</em>, 5439-5451. (<a href='https://doi.org/10.1109/TMM.2025.3542987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, numerous learning-based point cloud compression methods with outstanding performance have been developed. The majority of them concentrate on point cloud geometry compression, and several works have demonstrated advances in the color attribute compression for dense point clouds. However, compression of the reflectance attribute attached to the point captured by the light detection and ranging (LiDAR) sensors remains a major challenge. In this article, we present a lossless reflectance compression method for LiDAR point clouds (LPCs) that learns reflectance probability distributions with a deep hierarchical k-nearest-neighbors (KNN) context model, namely, the HK-PCRC. We first represent the original LPC with a series of hierarchical layers. Relying on the hierarchical structure, points in the same layer are coded in parallel by referencing the points in the previously coded layers. The approach balances the coding efficiency and time complexity while also supporting the progressive coding functionality. By introducing the KNN context, the context size is significantly reduced, which eases the computational burden while maintaining the coding performance. To enrich the context information, we further search for enhanced neighbors for each point in the context window. For each enhanced neighbor, in addition to its reflectance value, the relative distance, elevation angle, and local density are further collected. Then, a transformer-style sequential model is applied to construct an accurate deep context model. Furthermore, to efficiently fuse context features from different sources, a cross-feature fusion attention mechanism is designed for the transformer network. The comprehensive experimental results on SemanticKITTI, a large scale LiDAR benchmark, and Ford, an MPEG-specified dataset, demonstrate that our proposed framework achieves a state-of-the-art reflectance lossless compression performance, with average bit savings of 11.3% and 9.6% when compared to the state-of-the-art hand-crafted methods.},
  archive      = {J_TMM},
  author       = {Lizhi Hou and Tingyu Fan and Yiling Xu and Zhu Li},
  doi          = {10.1109/TMM.2025.3542987},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5439-5451},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Lossless LiDAR point cloud reflectance compression with a deep hierarchical KNN context model},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual class incremental learning with textual priors guidance based on an adapted vision-language model. <em>TMM</em>, <em>27</em>, 5426-5438. (<a href='https://doi.org/10.1109/TMM.2025.3543109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ideal artificial intelligence (AI) system should have the capability to continually learn like humans. However, when learning new knowledge, AI systems often suffer from catastrophic forgetting of old knowledge. Although many continual learning methods have been proposed, they often ignore the issue of misclassifying similar classes and make insufficient use of textual priors of visual classes to improve continual learning performance. In this study, we propose a continual learning framework based on a pre-trained vision-language model (VLM) that does not require storing old class data. This framework utilizes parameter-efficient fine-tuning of the VLM's text encoder for constructing a shared and consistent semantic textual space throughout the continual learning process. The textual priors of visual classes are encoded by the adapted VLM's text encoder to generate discriminative semantic representations, which are then used to guide the learning of visual classes. Additionally, fake out-of-distribution (OOD) images constructed from each training image further assist in the learning of visual classes. Extensive empirical evaluations on three natural datasets and one medical dataset demonstrate the superiority of the proposed framework.},
  archive      = {J_TMM},
  author       = {Wentao Zhang and Tong Yu and Ruixuan Wang and Jianhui Xie and Emanuele Trucco and Wei-Shi Zheng and Xiaobo Yang},
  doi          = {10.1109/TMM.2025.3543109},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5426-5438},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual class incremental learning with textual priors guidance based on an adapted vision-language model},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AVS-mamba: Exploring temporal and multi-modal mamba for audio-visual segmentation. <em>TMM</em>, <em>27</em>, 5413-5425. (<a href='https://doi.org/10.1109/TMM.2025.3542995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The essence of audio-visual segmentation (AVS) lies in locating and delineating sound-emitting objects within a video stream. While Transformer-based methods have shown promise, their handling of long-range dependencies struggles due to quadratic computational costs, presenting a bottleneck in complex scenarios. To overcome this limitation and facilitate complex multi-modal comprehension with linear complexity, we introduce AVS-Mamba, a selective state space model to address the AVS task. Our framework incorporates two key components for video understanding and cross-modal learning: Temporal Mamba Block for sequential video processing and Vision-to-Audio Fusion Block for advanced audio-vision integration. Building on this, we develop the Multi-scale Temporal Encoder, aimed at enhancing the learning of visual features across scales, facilitating the perception of intra- and inter-frame information. To perform multi-modal fusion, we propose the Modality Aggregation Decoder, leveraging the Vision-to-Audio Fusion Block to integrate visual features into audio features across both frame and temporal levels. Further, we adopt the Contextual Integration Pyramid to perform audio-to-vision spatial-temporal context collaboration. Through these innovative contributions, our approach achieves new state-of-the-art results on the AVSBench-object and AVSBench-semantic datasets.},
  archive      = {J_TMM},
  author       = {Sitong Gong and Yunzhi Zhuge and Lu Zhang and Yifan Wang and Pingping Zhang and Lijun Wang and Huchuan Lu},
  doi          = {10.1109/TMM.2025.3542995},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5413-5425},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AVS-mamba: Exploring temporal and multi-modal mamba for audio-visual segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All-in-focus imaging from events with occlusions. <em>TMM</em>, <em>27</em>, 5398-5412. (<a href='https://doi.org/10.1109/TMM.2025.3542978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-based Synthetic Aperture Imaging (E-SAI) extends the SAI technique to observe targets behind extremely dense occlusions. Existing approaches remain confined to the de-occlusion of a specific depth plane, i.e., single depth in focus, unable to be applied to observe occluded targets with varying depths due to the decreased focus range. To achieve All-in-Focus E-SAI, i.e., recovering the occlusion-free image of all depth planes, the depth information behind the occlusions should be given to ensure accurate event refocusing. In this paper, we first prove the feasibility of predicting the depth map from captured events in the presence of dense occlusions. Then, we propose the ESAI-AF network, which consists of a Depth Estimation Module (DEM) designed to estimate the depth information from multi-view events and an Image Enhancement Module (IEM) designed to reconstruct high-quality occlusion-free images from the refocused events. We employ only multi-view occlusion-free images as supervised signals for end-to-end training of the above modules. Extensive experiments have shown that the proposed method can effectively perform All-in-Focus image reconstruction of occluded multi-depth targets and achieves superior performance to existing methods.},
  archive      = {J_TMM},
  author       = {Yichen Liu and Lixuan Wei and Yufei Guo and Lei Yu},
  doi          = {10.1109/TMM.2025.3542978},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5398-5412},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {All-in-focus imaging from events with occlusions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ArbiTrack: A novel multi-object tracking framework for a moving AAV to detect and track arbitrarily oriented targets. <em>TMM</em>, <em>27</em>, 5387-5397. (<a href='https://doi.org/10.1109/TMM.2025.3543018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The operation of traditional multi-object trackers on a moving autonomous aerial vehicle (AAV) faces many difficulties due to the irregular motion of AAV, the occlusion problem, and in particular arbitrarily oriented targets that are densely distributed with complex backgrounds. To solve these difficulties, this paper proposes a novel multi-object tracking framework, namely ArbiTrack, for a moving AAV to effectively detect and track arbitrarily oriented targets on the grounds. The proposed framework consists of an oriented object detection module to capture ground objects, a multi-scale context aggregation (MCA) module to improve the detection accuracy of small objects, and an adaptive motion switching (AMS) module to deal with the nonlinear complexity among AAV and ground objects. Historical information from multiple moments is used in this framework to learn the spatio-temporal characteristics so that the occlusion problem can be solved effectively. Experiments are conducted by using our OriDrone dataset and the public dataset UAVDT dataset. Results demonstrate that the proposed method achieves state-of-the-art tracking performance.},
  archive      = {J_TMM},
  author       = {Yuqing Chen and Jiayu Wang and Qianchen Zhou and Huosheng Hu},
  doi          = {10.1109/TMM.2025.3543018},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5387-5397},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ArbiTrack: A novel multi-object tracking framework for a moving AAV to detect and track arbitrarily oriented targets},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing semantic awareness by sentimental constraint with automatic outlier masking for multimodal sarcasm detection. <em>TMM</em>, <em>27</em>, 5376-5386. (<a href='https://doi.org/10.1109/TMM.2025.3543074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sarcasm detection, aiming to uncover sarcastic sentiment behind multimodal data, has gained substantial attention in multimodal communities. Recent advancements in multimodal sarcasm detection (MSD) methods have primarily focused on modality alignment with pre-trained vision-language (V-L) model. However, text-image pairs often exhibit weak or even opposite semantic correlations in MSD tasks. Consequently, directly aligning these modalities can potentially result in feature shift and inter-class confusion, ultimately hindering the model's ability. To alleviate this issue, we propose the Enhancing Semantic Awareness Model (ESAM) for multimodal sarcasm detection. Specifically, we first devise a Modality-decoupled Framework (MDF) to separate the textual and visual features from the fused multimodal representation. This decoupling enables the parallel integration of the Sentimental Congruity Constraint (SCC) within both visual and textual latent spaces, thereby enhancing the semantic awareness of different modalities. Furthermore, given that certain outlier samples with ambiguous sentiments can mislead the training and weaken the performance of SCC, we further incorporate Automatic Outlier Masking. This mechanism automatically detects and masks the outliers, guiding the model to focus on more informative samples during training. Experimental results on two public MSD datasets validate the robustness and superiority of our proposed ESAM model.},
  archive      = {J_TMM},
  author       = {Shaozu Yuan and Yiwei Wei and Hengyang Zhou and Qinfu Xu and Meng Chen and Xiaodong He},
  doi          = {10.1109/TMM.2025.3543074},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5376-5386},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing semantic awareness by sentimental constraint with automatic outlier masking for multimodal sarcasm detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TBag: Three recipes for building up a lightweight hybrid network for real-time SISR. <em>TMM</em>, <em>27</em>, 5363-5375. (<a href='https://doi.org/10.1109/TMM.2025.3542966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalent convolution neural network (CNN) and Transformer have revolutionized the area of single-image super-resolution (SISR). Though these models have significantly improved performance, they often struggle with real-time applications or on resource-constrained platforms due to their complexity. In this paper, we propose TBag, a lightweight hybrid network that combines the strengths of CNN and Transformer to address these challenges. Our method simplifies the Transformer block with three key optimizations: 1) No projection layer is applied to the value in the original self-attention operation; 2) The number of tokens is rescaled before the self-attention operation and then rescaled back for easing of computation; 3) The expansion factor of the original feed-forward network (FFN) is adjusted. These optimizations enable the development of an efficient hybrid network tailored for real-time SISR. Notably, the hybrid design of CNN and Transformer further enhances both local detail recovery and global feature modeling. Extensive experiments show that TBag achieves a competitive trade-off between effectiveness and efficiency compared to previous lightweight SISR methods (e.g., +0.42 dB PSNR with an 86.7% reduction in latency). Moreover, TBag's real-time capabilities make it highly suitable for practical applications, with the TBag-Tiny version achieving up to 59 FPS on hardware devices. Future work will explore the potential of this hybrid approach in other image restoration tasks, such as denoising and deblurring.},
  archive      = {J_TMM},
  author       = {Ruoyi Xue and Cheng Cheng and Hang Wang and Hongbin Sun},
  doi          = {10.1109/TMM.2025.3542966},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5363-5375},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TBag: Three recipes for building up a lightweight hybrid network for real-time SISR},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN prior-enhanced novel view synthesis from monocular degraded images. <em>TMM</em>, <em>27</em>, 5352-5362. (<a href='https://doi.org/10.1109/TMM.2025.3542963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the escalating demand for three-dimensional visual applications such as gaming, virtual reality, and autonomous driving, novel view synthesis has become a critical area of research. Current methods mainly depend on multiple views of the same subject to achieve satisfactory results, but there is often a significant lack of available data. Typically, only a single degraded image is available for reconstruction, which may be affected by occlusion, low resolution, or absence of color information. To overcome this limitation, we propose a two-stage feature matching approach designed specifically for single degraded images, leading to the synthesis of high-quality novel perspective images. This method involves the sequential use of an encoder for feature extraction followed by the fine-tuning of a generator for feature matching. Additionally, the integration of an information filtering module proposed by us during the GAN inversion process helps eliminate misleading information present in degraded images, thereby correcting the inversion direction. Extensive experimental results show that our method outperforms existing state-of-the-art single-view novel view synthesis techniques in handling challenges like occluded, grayscale, and low-resolution images. Moreover, the efficacy of our method remains unparalleled even when aforementioned method integrated with image restoration algorithms.},
  archive      = {J_TMM},
  author       = {Kehua Guo and Zheng Wu and Xianhong Wen and Shaojun Guo and Zhipeng Xi and Tianyu Chen},
  doi          = {10.1109/TMM.2025.3542963},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5352-5362},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GAN prior-enhanced novel view synthesis from monocular degraded images},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging content and context cues for low-light image enhancement. <em>TMM</em>, <em>27</em>, 5337-5351. (<a href='https://doi.org/10.1109/TMM.2025.3543047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light conditions have an adverse impact on machine cognition, limiting the performance of computer vision systems in real life. Since low-light data is limited and difficult to annotate, we focus on image processing to enhance low-light images and improve the performance of any downstream task model, instead of fine-tuning each of the models which can be prohibitively expensive. We propose to improve the existing zero-reference low-light enhancement by leveraging the CLIP model to capture image prior and for semantic guidance. Specifically, we propose a data augmentation strategy to learn an image prior via prompt learning, based on image sampling, to learn the image prior without any need for paired or unpaired normal-light data. Next, we propose a semantic guidance strategy that maximally takes advantage of existing low-light annotation by introducing both content and context cues about the image training patches. We experimentally show, in a qualitative study, that the proposed prior and semantic guidance help to improve the overall image contrast and hue, as well as improve background-foreground discrimination, resulting in reduced over-saturation and noise over-amplification, common in related zero-reference methods. As we target machine cognition, rather than rely on assuming the correlation between human perception and downstream task performance, we conduct and present an ablation study and comparison with related zero-reference methods in terms of task-based performance across many low-light datasets, including image classification, object and face detection, showing the effectiveness of our proposed method.},
  archive      = {J_TMM},
  author       = {Igor Morawski and Kai He and Shusil Dangi and Winston H. Hsu},
  doi          = {10.1109/TMM.2025.3543047},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5337-5351},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Leveraging content and context cues for low-light image enhancement},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated user preference modeling for privacy-preserving cross-domain recommendation. <em>TMM</em>, <em>27</em>, 5324-5336. (<a href='https://doi.org/10.1109/TMM.2025.3543106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain recommendation (CDR) aims to address the data-sparsity problem by transferring knowledge across domains. Existing CDR methods generally assume that the user-item interaction data is shareable between domains, which leads to privacy leakage. Recently, some privacy-preserving CDR (PPCDR) models have been proposed to solve this problem. However, they primarily transfer simple representations learned only from user-item interaction histories, overlooking other useful side information, leading to inaccurate user preferences. Additionally, they transfer differentially private user-item interaction matrices or embeddings across domains to protect privacy. However, these methods offer limited privacy protection, as attackers may exploit external information to infer the original data. To address these challenges, we propose a novel Federated User Preference Modeling (FUPM) framework. In FUPM, first, a novel comprehensive preference exploration module is proposed to learn users' comprehensive preferences from both interaction data and additional data including review texts and potentially positive items. Next, a private preference transfer module is designed to first learn differentially private local and global prototypes, and then privately transfer the global prototypes using a federated learning strategy. These prototypes are generalized representations of user groups, making it difficult for attackers to infer individual information. Extensive experiments on four CDR tasks conducted on the Amazon and Douban datasets validate the superiority of FUPM over SOTA baselines.},
  archive      = {J_TMM},
  author       = {Li Wang and Shoujin Wang and Quangui Zhang and Qiang Wu and Min Xu},
  doi          = {10.1109/TMM.2025.3543106},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5324-5336},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Federated user preference modeling for privacy-preserving cross-domain recommendation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multi-scale language reinforcement for multimodal named entity recognition. <em>TMM</em>, <em>27</em>, 5312-5323. (<a href='https://doi.org/10.1109/TMM.2025.3543105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the recent years, multimodal named entity recognition has gained increasing attentions due to its wide applications in social media. The key factor of multimodal named entity recognition is to effectively fuse information of different modalities. Existing works mainly focus on reinforcing textual representations by fusing image features via the cross-modal attention mechanism. However, these works are limited in reinforcing the text modality at the token level. As a named entity usually contains several tokens, modeling token-level inter-modal interactions is suboptimal for the multimodal named entity recognition problem. In this work, we propose a multimodal named entity recognition approach dubbed Adaptive Multi-scale Language Reinforcement (AMLR) to implement entity-level language reinforcement. To this end, our model first expands token-level textual representations into multi-scale textual representations which are composed of language units of different lengths. After that, the visual information reinforces the language modality by modeling the cross-modal attention between images and expanded multi-scale textual representations. Unlike existing token-level language reinforcement methods, the word sequences of named entities can be directly interacted with the visual features as a whole, making the modeled cross-modal correlations more reasonable. Although the underlying entity is not given, the training procedure can encourage the relevant image contents to adaptively attend to the appropriate language units, making our approach not rely on the pipeline design. Comprehensive evaluation results on two public Twitter datasets clearly demonstrate the superiority of our proposed model.},
  archive      = {J_TMM},
  author       = {Enping Li and Tianrui Li and Huaishao Luo and Jielei Chu and Lixin Duan and Fengmao Lv},
  doi          = {10.1109/TMM.2025.3543105},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5312-5323},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive multi-scale language reinforcement for multimodal named entity recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mask-aware light field de-occlusion with gated feature aggregation and texture-semantic attention. <em>TMM</em>, <em>27</em>, 5296-5311. (<a href='https://doi.org/10.1109/TMM.2025.3543048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A light field image records rich information of a scene from multiple views, thereby providing complementary information for occlusion removal. However, current occlusion removal methods have several issues: 1) inefficient exploitation of spatial and angular complementary information among views; 2) indistinguishable treatment of pixels from foreground occlusion and background; and 3) insufficient exploration of spatial detail supplementation. Therefore, in this article, we propose a mask-aware de-occlusion network (MANet). Specifically, MANet is a joint training network that integrates the occlusion mask predictor (OMP) and the occlusion remover (OR). First, OMP is proposed to provide the location of occluded regions for OR, as the occlusion removal task is ill-posed without occluded region localization. In OR, we introduce gated spatial-angular feature aggregation, which uses a soft gating mechanism to focus on spatial-angular interaction features in non-occluded regions, extracting effective aggregated features specific to the de-occlusion. Then, we design a complementary strategy to fully utilize spatial-angular information among views. Finally, we propose texture-semantic attention to improve the performance of detail generation. Experimental results demonstrate the superiority of MANet, with substantial improvements in both PSNR and SSIM metrics. Moreover, MANet stands out with an efficient parameter count of 2.4 M, making it a promising solution for real-world applications in public safety and security surveillance.},
  archive      = {J_TMM},
  author       = {Jieyu Chen and Ping An and Xinpeng Huang and Yilei Chen and Chao Yang and Liquan Shen},
  doi          = {10.1109/TMM.2025.3543048},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5296-5311},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mask-aware light field de-occlusion with gated feature aggregation and texture-semantic attention},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous detection and interaction reasoning for object-centric action recognition. <em>TMM</em>, <em>27</em>, 5283-5295. (<a href='https://doi.org/10.1109/TMM.2025.3543033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interactions between human and objects are important for recognizing object-centric actions. Existing methods usually adopt a two-stage pipeline, where object proposals are first detected using a pretrained detector, and then are fed to an action recognition model for extracting video features and learning the object relations for action recognition. However, since the action prior is unknown in the object detection stage, important objects could be easily overlooked, leading to inferior action recognition performance. In this paper, we propose an end-to-end object-centric action recognition framework that simultaneously performs Detection And Interaction Reasoning (dubbed DAIR) in one stage. Particularly, after extracting video features using a base network, we design three consecutive modules for simultaneously learning object detection and interaction reasoning. Firstly, we build a Patch-based Object Decoder (PatchDec) to generate object proposals from video patch tokens. Then, we design an Interactive Object Refining and Aggregation (IRA) to identify the interactive objects that are important for action recognition. The IRA module adjusts the interactiveness scores of proposals based on their relative position and appearance, and aggregates the object-level information into global video representation. Finally, we build an Object Relation Modeling (ORM) module to encode the object relations. These three modules together with the video feature extractor can be trained jointly in an end-to-end fashion, thus avoiding the heavy reliance on an off-the-shelf object detector, and reducing the multi-stage training burden. We conduct experiments on two datasets, Something-Else and Ikea-Assembly, to evaluate the performance of our proposed approach on conventional, compositional, and few-shot action recognition tasks. Through in-depth experimental analysis, we show the crucial role of interactive objects in learning for action recognition, and we can outperform state-of-the-art methods on both datasets. We hope our DAIR can provide a new perspective for object-centric action recognition.},
  archive      = {J_TMM},
  author       = {Xunsong Li and Pengzhan Sun and Yangcen Liu and Lixin Duan and Wen Li},
  doi          = {10.1109/TMM.2025.3543033},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5283-5295},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Simultaneous detection and interaction reasoning for object-centric action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive learning with multiple prototypes for unsupervised domain adaptive semantic segmentation. <em>TMM</em>, <em>27</em>, 5267-5282. (<a href='https://doi.org/10.1109/TMM.2025.3543115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptive semantic segmentation aims to transfer knowledge from the annotated source domain to the unlabeled target domain. Recently, self-training methods have gained substantial attention, which leverage high-confidence predictions in the target domain as pseudo labels for supervision. However, limited exploration of intra-class variations across domains, including significant visual differences within each category, has led to misalignment between feature distribution across domains. In this article, we present a unified non-parametric distance-based online clustering method to efficiently maintain multiple centroid-based prototypes within each category subspace instead of one prototype for each category subspace, which enables prototypes to possess the capacity for richer feature representation. Then, considering the variance across different dimensions of a feature representation, we then extend the prototypes from centroid-based ones to distribution-based ones. Specifically, each subspace is modeled using a Gaussian mixture model which includes several anisotropic Gaussian distributions, aimed at prioritizing discriminative dimensions and obtaining a finer measurement of the pixel-to-prototype similarity. Meanwhile, a category-aware feature space is achieved through pixel-to-prototype contrastive learning to ensure the compactness of pixel features in the same subcategory and drive the separation between pixel features of different subcategories. What's more, multi-resolution features are utilized to promote diversity and robustness among intra-class prototypes. Experiments validate the competitiveness of our two prototype-based methods against existing state-of-the-art methods, with a mIoU of 76.8% on GTA $\rightarrow$ Cityscapes, 68.4% on Synthia $\rightarrow$ Cityscapes, 54.5% on Cityscapes $\rightarrow$ DarkZurich and 56.4% on Cityscapes $\rightarrow$ ACDC. Notably, our method is able to seamlessly integrate with existing UDA methods.},
  archive      = {J_TMM},
  author       = {Jun Yu and Guochen Xie and Quansheng Liu and Zhen Kan and Lei Wang and Tianyu Liu and Qiang Ling and Wei Xu and Fang Gao},
  doi          = {10.1109/TMM.2025.3543115},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5267-5282},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Contrastive learning with multiple prototypes for unsupervised domain adaptive semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient hierarchical feature collaboration transformer for image inpainting. <em>TMM</em>, <em>27</em>, 5255-5266. (<a href='https://doi.org/10.1109/TMM.2025.3543039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image inpainting methods face limitations in detail restoration. Although transformer-based models have made certain progress recently, the lack of hierarchical feature interaction and insufficient consideration of the importance of features at different network levels lead to semantic ambiguity in image reconstruction. To enhance the visual quality and accuracy of image inpainting, we adopt a multi-level feature fusion approach and propose a novel, efficient hierarchical feature collaboration transformer (HFCT). Our approach comprises two modules: dual stream gated feature fusion (DSGF) and region-separated attention module (RSAM), effectively capturing features at different levels of the network and enhancing inter-level information exchange. The DSGF module uses soft gating to fuse primary and advanced features, strengthening the connection from local to global consistency and reducing artifacts. The RSAM module resolves attention isolation issues in feature fusion through region-separated attention, strengthening the understanding of feature relationships, capturing more image semantics, and improving restoration accuracy. Extensive experiments on the Paris StreetView, CelebA-HQ, and Places2 benchmark datasets demonstrate that our proposed method achieves superior image inpainting quality compared to several state-of-the-art inpainting algorithms.},
  archive      = {J_TMM},
  author       = {Dengyong Zhang and Nuo Fu and Xin Liao and Jiaxin Chen and Hengfu Yang and Gaobo Yang},
  doi          = {10.1109/TMM.2025.3543039},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5255-5266},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient hierarchical feature collaboration transformer for image inpainting},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic camera movement generation with enhanced immersion for virtual cinematography. <em>TMM</em>, <em>27</em>, 5241-5254. (<a href='https://doi.org/10.1109/TMM.2025.3542956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-generated cinematic creations are gaining popularity as our daily entertainment, yet it is a challenge to master cinematography for producing immersive contents. Many existing automatic methods focus on roughly controlling predefined shot types or movement patterns, which struggle to engage viewers with the actor's circumstances. Real-world cinematographic rules show that directors can create immersion by comprehensively synchronizing the camera with the actor. Inspired by this strategy, we propose a deep camera control framework that enables actor-camera synchronization in three aspects, considering frame aesthetics, spatial action, and emotional status in the 3D virtual stage. Following rule-of-thirds, our framework first modifies the initial camera placement to position the actor aesthetically. This adjustment is facilitated by a weakly-supervised adjustor that analyzes frame composition via camera projection. We then design a GAN model that can adversarially synthesize fine-grained camera movement based on the actor's action and psychological state, using an encoder-decoder generator to map kinematics and emotional variables into camera trajectories. Moreover, we incorporate a regularizer to align the generated stylistic variances with specific emotional categories and intensities. The experimental results show that our proposed method yields immersive cinematic videos of high quality, both quantitatively and qualitatively. Live examples can be found in the supplementary video.},
  archive      = {J_TMM},
  author       = {Xinyi Wu and Haohong Wang and Aggelos K. Katsaggelos},
  doi          = {10.1109/TMM.2025.3542956},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5241-5254},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Automatic camera movement generation with enhanced immersion for virtual cinematography},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiViPose: A video-aided wi-fi framework for environment-independent 3D human pose estimation. <em>TMM</em>, <em>27</em>, 5225-5240. (<a href='https://doi.org/10.1109/TMM.2025.3543090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent complexity of Wi-Fi signals makes video-aided Wi-Fi 3D pose estimation difficult. The challenges include the limited generalizability of the task across diverse environments, its significant signal heterogeneity, and its inadequate ability to analyze local and geometric information. To overcome these challenges, we introduce WiViPose, a video-aided Wi-Fi framework for 3D pose estimation, which attains enhanced cross-environment generalization through cross-layer optimization. Bilinear temporal-spectral fusion (BTSF) is initially used to fuse the time-domain and frequency-domain features derived from Wi-Fi. Video features are derived from a multiresolution convolutional pose machine and enhanced by local self-attention. Cross-modality data fusion is facilitated through an attention-based transformer, with the process further refined under a supervisory mechanism. WiViPose demonstrates effectiveness by achieving an average percentage of correct keypoints (PCK)@50 of 91.01% across three typical indoor environments.},
  archive      = {J_TMM},
  author       = {Lei Zhang and Haoran Ning and Jiaxin Tang and Zhenxiang Chen and Yaping Zhong and Yahong Han},
  doi          = {10.1109/TMM.2025.3543090},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5225-5240},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WiViPose: A video-aided wi-fi framework for environment-independent 3D human pose estimation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global spatial-temporal information-based residual ConvLSTM for video space-time super-resolution. <em>TMM</em>, <em>27</em>, 5212-5224. (<a href='https://doi.org/10.1109/TMM.2025.3542970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By converting low-frame-rate, low-resolution videos into high-frame-rate, high-resolution ones, space-time video super-resolution techniques can enhance visual experiences and facilitate more efficient information dissemination. We propose a convolutional neural network (CNN) for space-time video super-resolution, namely GIRNet. Our method combines long-term global information and short-term local information from the video to better extract complete and accurate spatial-temporal information. To generate highly accurate features and thus improve performance, the proposed network integrates a feature-level temporal interpolation module with deformable convolutions and a global spatial-temporal information-based residual convolutional long short-term memory (convLSTM) module. In the feature-level temporal interpolation module, we leverage deformable convolution, which adapts to deformations and scale variations of objects across different scene locations. This provides a more efficient solution than conventional convolution for extracting features from moving objects. Our network effectively uses forward and backward feature information to determine inter-frame offsets, leading to the direct generation of interpolated frame features. In the global spatial-temporal information-based residual convLSTM module, the first convLSTM is used to derive global spatial-temporal information from the input features, and the second convLSTM uses the previously computed global spatial-temporal information feature as its initial cell state. This second convLSTM adopts residual connections to preserve spatial information, thereby enhancing the output features. Experiments on the Vimeo90 K dataset show that the proposed method outperforms open source state-of-the-art techniques in peak signal-to-noise-ratio (by 1.45 dB, 1.14 dB, and 0.2 dB over STARnet, TMNet, and 3DAttGAN, respectively), structural similarity index(by 0.027, 0.023, and 0.006 over STARnet, TMNet, and 3DAttGAN, respectively), and visual quality.},
  archive      = {J_TMM},
  author       = {Congrui Fu and Hui Yuan and Shiqi Jiang and Guanghui Zhang and Liquan Shen and Raouf Hamzaoui},
  doi          = {10.1109/TMM.2025.3542970},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5212-5224},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Global spatial-temporal information-based residual ConvLSTM for video space-time super-resolution},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boundary discretization and reliable classification network for temporal action detection. <em>TMM</em>, <em>27</em>, 5198-5211. (<a href='https://doi.org/10.1109/TMM.2025.3543108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action detection aims to recognize the action category and determine each action instance's starting and ending time in untrimmed videos. The mixed method has demonstrated notable performance by integrating both anchor-based and anchor-free approaches. However, while it leverages the strengths of each method, it also retains their respective limitations. For instance, the anchor-based approach depends on manually crafted anchors tailored to specific datasets, while the anchor-free approach predicts potential action instances at each temporal position, resulting in a significant number of false positives in category prediction. The inclusion of these limitations undermines the potential benefits of the mixed method. In this paper, we propose a novel Boundary Discretization and Reliable Classification Network (BDRC-Net) that addresses the issues above by introducing boundary discretization and reliable classification modules. Specifically, the boundary discretization module (BDM) elegantly merges anchor-based and anchor-free approaches in the form of boundary discretization, eliminating the need for the traditional handcrafted anchor design. Furthermore, the reliable classification module (RCM) predicts reliable global action categories to reduce false positives. Extensive experiments conducted on different benchmarks demonstrate that our proposed method achieves competitive detection performance.},
  archive      = {J_TMM},
  author       = {Zhenying Fang and Jun Yu and Richang Hong},
  doi          = {10.1109/TMM.2025.3543108},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5198-5211},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boundary discretization and reliable classification network for temporal action detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased meta reinforcement learning for interactive recommender systems. <em>TMM</em>, <em>27</em>, 5185-5197. (<a href='https://doi.org/10.1109/TMM.2025.3543045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive recommender systems have garnered widespread attention due to their ability to dynamically update recommendation strategies based on user feedback, enhancing the user's interactive experience. To maximize long-term user satisfaction, existing research has incorporated reinforcement learning into interactive recommender systems and combined it with meta-learning to form a meta-reinforcement learning framework that further addresses the cold-start problem in interactive recommendation. However, on one hand, there are latent confounders affecting user feedback; on the other hand, since training samples are observed rather than experimentally obtained, selection bias and exposure bias exist in the interactive data. Most existing studies remove biases using the method of Inverse Propensity Score, which often utilizes fixed propensity scores and neglects the latent confounders affecting user feedback. In this paper, we propose an unbiased interactive recommender system (UIRS) based on a meta-reinforcement learning framework. To eliminate the impact of latent confounders in the state encoding process, we design a user preference representer consisting of three interconnected gated recurrent units. Additionally, we use the item recommendation probabilities output from the policy network as propensity scores and design the objective functions based on these scores, to eliminate biases while addressing latent confounders. Extensive experiments conducted on three benchmark datasets demonstrate that our proposed UIRS model achieves significant improvements over existing state-of-the-art baseline models.},
  archive      = {J_TMM},
  author       = {Huiting Liu and Xinlong Lv and Peng Zhao and Peipei Li and Xindong Wu},
  doi          = {10.1109/TMM.2025.3543045},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5185-5197},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unbiased meta reinforcement learning for interactive recommender systems},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SimVPv2: Towards simple yet powerful spatiotemporal predictive learning. <em>TMM</em>, <em>27</em>, 5170-5184. (<a href='https://doi.org/10.1109/TMM.2025.3543051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed remarkable advances in spatiotemporal predictive learning, with methods incorporating auxiliary inputs, complex neural architectures, and sophisticated training strategies. While SimVP has introduced a simpler, CNN-based baseline for this task, it still relies on heavy Unet-like architectures for spatial and temporal modeling, which still suffers from high complexity and computational overhead. In this paper, we propose SimVPv2, a streamlined model that eliminates the need for Unet architectures and demonstrates that plain stacks of convolutional layers, enhanced with an efficient Gated Spatiotemporal Attention mechanism, can deliver state-of-the-art performance. SimVPv2 not only simplifies the model architecture but also improves both performance and computational efficiency. On the standard Moving MNIST benchmark, SimVPv2 achieves superior performance compared to SimVP, with fewer FLOPs, about half the training time, and 60% faster inference efficiency. Extensive experiments across eight diverse datasets, including real-world tasks such as traffic forecasting and climate prediction, further demonstrate that SimVPv2 offers a powerful yet straightforward solution, achieving robust generalization across various spatiotemporal learning scenarios. We believe the proposed SimVPv2 can serve as a solid baseline to benefit the spatiotemporal predictive learning community.},
  archive      = {J_TMM},
  author       = {Cheng Tan and Zhangyang Gao and Siyuan Li and Stan Z. Li},
  doi          = {10.1109/TMM.2025.3543051},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5170-5184},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SimVPv2: Towards simple yet powerful spatiotemporal predictive learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust watermarking based on multi-layer watermark feature fusion. <em>TMM</em>, <em>27</em>, 5156-5169. (<a href='https://doi.org/10.1109/TMM.2025.3543079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of robust image watermarking is to embed a watermark into a carrier image in an invisible form and extract the watermark successfully even under noise interference conditions to achieve copyright confirmation and traceability. Although watermarking methods based on deep learning can improve the robustness by adding a noise simulation layer, few theoretical analyses of the codec structure have been conducted. Theoretical explainability is the theoretical basis for developing a network architecture, which plays a guiding role in network development. On the basis of the interpretability of convolutional networks, this paper analyzes the mathematical process of embedding and extracting watermarks in codecs and proposes a novel watermarking framework based on multi-layer watermark feature fusion. Specifically, the encoder can be a convolutional network structure of arbitrary depth, whereas the decoder needs only to adopt its corresponding deconvolution structure. To improve the quality and robustness of the generated watermarked image, the watermark is associated with an arbitrary layer feature space in the decoder. In the decoder, the network quickly converges to each original encoding feature space through the deconvolution structure, thus decoupling the watermark features. Finally, the watermark is extracted via the automatic fusion of multi-layer watermark features. The experimental results show that the proposed method is suitable for few-shot learning, and its invisibility, robustness and generalization performance on multiple datasets are significantly better than those of other advanced methods.},
  archive      = {J_TMM},
  author       = {Shaowu Wu and Wei Lu and Xiangyang Luo},
  doi          = {10.1109/TMM.2025.3543079},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5156-5169},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust watermarking based on multi-layer watermark feature fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GFTLS-SLT: Gloss-free transformer based lexical and semantic awareness framework for multimodal sign language translation. <em>TMM</em>, <em>27</em>, 5144-5155. (<a href='https://doi.org/10.1109/TMM.2025.3542990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language provides communication support for deaf and severely hearing-impaired people. Sign language translation (SLT) bridges the hearing-impaired and hearing communities. Existing SLT methods use gloss as intermediate supervisory information to help the model sense gesture boundaries and understand global semantics. However, annotating gloss requires great cost, especially in multimodal SLT task. This paper proposes GFTLS-SLT: gloss-free Transformer based lexical and semantic awareness framework for SLT. The multimodal alignment and fusion module in GFTLS-SLT utilizes cross-attention to align multimodal features, and fuses them using the improved statistical and contrastive attention. To replace the role of gloss, GFTLS-SLT designs gesture lexical awareness (GLA) and global semantic awareness (GSA) modules. The GLA module utilizes the defined observation matrix to obtain the lexical meaning matrix, and makes the model sense gesture boundaries by the designed dynamic step-size lexical matching algorithm. The multimodal semantic header is used by GSA module to represent the sign language global semantic and is aligned with the spoken semantic on semantic space. In addition, the experiment results of GFTLS-SLT on publicly available multimodal SLT datasets show that its performance reaches that of SLT methods with gloss supervision.},
  archive      = {J_TMM},
  author       = {Jiangtao Zhang and Qingshan Wang and Qi Wang},
  doi          = {10.1109/TMM.2025.3542990},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5144-5155},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GFTLS-SLT: Gloss-free transformer based lexical and semantic awareness framework for multimodal sign language translation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constructing balanced training samples: A new perspective on long-tailed classification. <em>TMM</em>, <em>27</em>, 5130-5143. (<a href='https://doi.org/10.1109/TMM.2025.3543084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most significant characteristic of long-tailed classification is that severe sample imbalance causes the model to be biased towards the head category. While the long-tailed distribution of multimedia dataset remains a constant, we can enhance the acquisition of balanced training samples and corresponding features during the learning process. This paper innovatively designs a sample provider to construct balanced training samples to enhance the acquisition of comprehensive features, and proposes a Siamese-based parameter-sharing framework to handle data with long-tailed distributions. Specifically, one branch of the Siamese network is introduced to classify samples with conventional random cropping sampling, another branch integrates the advantages of constructed balanced samples and hybrid optimization to capture the balanced features to identify more precise category boundaries. This combination not only facilitates the learning of long-tailed distribution but also strengthens the model's extraction of balanced features through the incorporation of contrastive learning. Most significantly, extensive experiments on CIFAR10-LT, CIFAR100-LT, ImageNet-LT and iNaturalist 2018 datasets demonstrate our model not only achieves superior performance but also retains the benefits of end-to-end training. Specifically, our method achieves 60.7% accuracy on ImageNet-LT with an end-to-end ResNeXt-50 backbone.},
  archive      = {J_TMM},
  author       = {Wenyi Zhao and Wei Li and Yuhan Li and Lu Yang and Zhenhao Liang and Enwen Hu and Weidong Zhang and Huihua Yang},
  doi          = {10.1109/TMM.2025.3543084},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5130-5143},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Constructing balanced training samples: A new perspective on long-tailed classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Completed interaction networks for pedestrian trajectory prediction. <em>TMM</em>, <em>27</em>, 5119-5129. (<a href='https://doi.org/10.1109/TMM.2025.3542967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social and environmental interactions, as well as the pedestrian goal are crucial for pedestrian trajectory prediction. This is because they could learn both complex interactions in the scenes and the intentions of the pedestrians. However, most existing methods either learn the one-moment social interactions, or supervise the pedestrian trajectories using long-term goal, resulting in suboptimal prediction performances. In this paper, we propose a novel network named Completed Interaction Network (CINet) to simultaneously consider the social interactions in all moments, the environmental interactions and the short-term goal of pedestrians in a unified framework for pedestrian trajectory prediction. Specifically, we propose the Spatio-Temporal Transformer Layer (STTL) to fully mine the spatio-temporal information among historical trajectories of all pedestrians in order to obtain the social interactions in all moments. Additionally, we present the Gradual Goal Module (GGM) to capture the environmental interactions under the supervision of the short-term goal, which is beneficial to understanding the intentions of the pedestrian. Afterwards, we employ the cross-attention to effectively integrate the all-moment social and environmental interactions. The experimental results on three standard pedestrian datasets, i.e., ETH/UCY, SDD and inD demonstrate that our method achieves a new state-of-the-art performance. Furthermore, the visualization results indicate that our method could predict trajectories more reasonably in complex scenarios such as sharp turns, infeasible areas and so on.},
  archive      = {J_TMM},
  author       = {Zhong Zhang and Jianglin Zhou and Shuang Liu and Baihua Xiao},
  doi          = {10.1109/TMM.2025.3542967},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5119-5129},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Completed interaction networks for pedestrian trajectory prediction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moiré-watermark: Robust watermarking against screen-shooting using moiré patterns. <em>TMM</em>, <em>27</em>, 5103-5118. (<a href='https://doi.org/10.1109/TMM.2025.3543008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of digital content leakage via screen capture highlights the urgent need for robust watermarking solutions capable of withstanding cross-media transmission. Current approaches primarily focus on developing watermarking techniques resilient to screen-shooting distortions, where distinguishing the watermark signal from these distortions is paramount. In contrast, our study addresses an inverse problem by investigating the generation patterns of noise during screen-shooting and considering them as feasible representations of watermark signals. Leveraging Moiré patterns as one of the distortion signals naturally generated by the interaction between electronic screens and camera sensors, we propose Moiré-watermark, presenting watermark information encoded into meticulously crafted Moiré patterns within images. To enhance the naturalness of Moiré-watermark amidst the irregularities of screen-shooting Moiré patterns, we encode watermark signals using gratings at different angles. A corresponding angle-based decoding method facilitates effective blind extraction of watermarks. Comprehensive experimental evaluations under diverse conditions of distance, angle, lighting, and across various capturing and display devices, alongside comparisons with existing methods, validate the superior performance of Moiré-watermark.},
  archive      = {J_TMM},
  author       = {Heng Wang and Hongxia Wang and Fei Zhang and Zhenhao Shi and Xinyi Huang},
  doi          = {10.1109/TMM.2025.3543008},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5103-5118},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Moiré-watermark: Robust watermarking against screen-shooting using moiré patterns},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cross-modal generation algorithm for temporal force tactile data for multidimensional haptic rendering. <em>TMM</em>, <em>27</em>, 5092-5102. (<a href='https://doi.org/10.1109/TMM.2025.3590907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting the correlation between multimodal data to generate tactile data has become a preferred approach to enhance tactile rendering fidelity. Nevertheless, existing studies have often overlooked the temporal dynamics of force tactile data. To fill this gap in the literature, this paper introduces a joint visual-audio approach to generate a temporal tactile data (VA2T) algorithm, focusing on the temporal and long-term dependencies of force tactile data. VA2T uses a feature extraction network to extract audio and image features and then uses an attention mechanism and decoder to fuse these features. The tactile reconstructor generates temporal friction and a normal force, with dilated causal convolution securing the temporal dependencies in the force tactile data. Simulation experiments on the LMT dataset demonstrate that compared with the transformer and audio-visual-aided haptic signal reconstruction (AVHR) algorithms, the VA2T algorithm reduces the RMSE for generated friction by 29.44% and 32.37%, respectively, and for normal forces by 23.30% and 35.43%, respectively. In addition, we developed a haptic rendering approach that combines electrovibration and mechanical vibration to render the generated friction and normal force. The subjective experimental results showed that the rendering fidelity of the data generated using the VA2T method was significantly higher than that of the data generated using the transformer and AVHR methods.},
  archive      = {J_TMM},
  author       = {Rui Song and Guohong Liu and Yan Zhang and Xiaoying Sun},
  doi          = {10.1109/TMM.2025.3590907},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5092-5102},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A cross-modal generation algorithm for temporal force tactile data for multidimensional haptic rendering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking temporal context in video-QA: A comprehensive study of single-frame static bias. <em>TMM</em>, <em>27</em>, 5077-5091. (<a href='https://doi.org/10.1109/TMM.2025.3543031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video question answering (Video-QA) has emerged as a core task in the vision-language domain, which requires the models to understand a given video and answer textual questions related to the video. Compared to conventional image-language tasks, Video-QA is designed for improving the models' capacity of memorizing and integrating multi-frame temporal cues associated with the questions. While significant performance improvements have recently been witnessed on public benchmarks, in this work, we rethink whether these improvements truly stem from better understanding of video temporal context as expected. To this end, we accomplish a strong single-frame baseline model trained with knowledge distillation. With this model, we surprisingly find that visiting only one single frame, without incorporating multi-frame and temporal information, is sufficient to achieve state-of-the-art (SOTA) performance on multiple mainstream benchmarks. This finding reveals the prevalence of single-frame bias in current benchmarks for the first time. Around the single-frame bias, we conduct an in-depth analysis on multiple popular benchmarks, which demonstrate that: (i) merely relying on one frame is able to achieve comparable performance with SOTA temporal Video-QA models; (ii) simply ensembling the prediction scores of only 3 separate frames is able to surpass temporal SOTAs. Furthermore, we observe that most of the benchmarks are biased towards central segments, and even the latest benchmarks tailored for temporal reasoning still suffer from severe single-frame bias. In case study, we find two key properties of low-bias instances: the question emphasizes temporal dependency and contextual understanding, and the associated video content presents significant variability in scenes, actions or interactions. Through further analysis on compositional reasoning datasets, we find that constructing explicit object/event interactions upon videos to fill in well-designed temporal question templates can effectively reduce the single-frame bias during annotation. We hope our analysis helps facilitate future efforts in the field towards mitigating static bias and highlighting temporal reasoning.},
  archive      = {J_TMM},
  author       = {Tianming Liang and Linhui Li and Jian-Fang Hu and Xiangyang Yu and Wei-Shi Zheng and Jianhuang Lai},
  doi          = {10.1109/TMM.2025.3543031},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5077-5091},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rethinking temporal context in video-QA: A comprehensive study of single-frame static bias},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SGG-nets: Generic rotation-invariant plugin networks for point cloud analysis. <em>TMM</em>, <em>27</em>, 5062-5076. (<a href='https://doi.org/10.1109/TMM.2025.3543001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotation invariance is a crucial requirement for the analysis of 3D point clouds. However, current methods often achieve rotation invariance by employing specific network designs. These networks, though perform well on rotation-aware tasks, is inferior in general tasks such as classification and segmentation. On the other hand, many powerful point processing networks, such as PointNet++, DGCNN, etc., have general point processing abilities, but do not own the property of rotation invariance. In this paper, we propose a standalone rotation-invariant convolution operator called SGGConv (Spherical Geometric Graph-based Convolution) and two ways integrating it with common point-based networks. The networks equipped with SGGConvs are called SGG-Nets which promote the rotation-invariance ability of regular point networks without modifying their network architectures much. Our contributions are three-fold. First, we propose a rotation-invariant feature descriptor, namely Spherical Geometry Descriptor (SGD), which captures point-pair features in a Local Spherical Coordinate System (LSCS). Second, we propose the SGGConv based on SGD and LSCS with an efficient Graph-based Spherical Feature Passing (GSFP) mechanism. Thirdly, we define two modules S-SGGConvMdl and M-SGGConvMdl, which are used to integrate SGGConv into baseline point nets. We test SGG-Nets, such as SGG-PointNet++, SGG-DGCNN, SGG-RIConv++, on representative point cloud datasets. These models, equipped with our SGGConvs, not only enhance the rotation-invariance of the baseline network but also improve its performance on point cloud analysis tasks such as classification and part segmentation, without incurring too much computational overhead.},
  archive      = {J_TMM},
  author       = {Jian Zhu and Jianrong Yan and Jiebin Huang and Yongwei Nie and Bin Sheng and Tong-Yee Lee},
  doi          = {10.1109/TMM.2025.3543001},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5062-5076},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SGG-nets: Generic rotation-invariant plugin networks for point cloud analysis},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive medical topic learning for enhanced fine-grained cross-modal alignment in medical report generation. <em>TMM</em>, <em>27</em>, 5050-5061. (<a href='https://doi.org/10.1109/TMM.2025.3543101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical report generation refers to the automatic creation of accurate and coherent diagnostic reports for medical images. This task can alleviate the workload of radiologists, enhance the efficiency of disease diagnosis, and therefore holds significant value and challenges. Considering the feature differences between different modalities, existing methods primarily focus on facilitating medical report generation through cross-modal alignment of images and texts. However, since medical images are very similar to each other, it is difficult to tag obvious objects, making most methods limited to coarse-grained image-text global alignment. In this paper, we propose a medical report generation model based on adaptive topic learning and fine-grained cross-modal alignment, which aligns images and texts from medical topic perspective and token perspective. From the medical topic perspective, a global-local contrastive loss is introduced to adaptively learn efficient medical topic features, and medical topics are utilized to map images and texts to the same semantic space for fine-grained alignment. From the token perspective, a token prediction module is designed to enable the model to focus on important local information by predicting the key tokens contained in the report. Experimental results on the two public datasets (i.e. IU-Xray and MIMIC-CXR) demonstrate that our proposed model outperforms state-of-the-art baselines.},
  archive      = {J_TMM},
  author       = {Xin Mei and Libin Yang and Dehong Gao and Xiaoyan Cai and Junwei Han and Tianming Liu},
  doi          = {10.1109/TMM.2025.3543101},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5050-5061},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive medical topic learning for enhanced fine-grained cross-modal alignment in medical report generation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling interactions between autonomous agents in a multi-agent self-awareness architecture. <em>TMM</em>, <em>27</em>, 5035-5049. (<a href='https://doi.org/10.1109/TMM.2025.3543110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from experience is a fundamental capability of intelligent agents. Autonomous systems rely on sensors that provide data about the environment and internal situations to their perception systems for learning and inference mechanisms. These systems can also learn Self-Aware and Situation-Aware generative modules from these data to localize themselves and interact with the environment. In this paper, we propose a self-aware cognitive architecture capable to perform tasks where the interactions between the self-state of an agent and the surrounding environment are explicitly and dynamically represented. We specifically develop a Deep Learning (DL) based Self-Aware interaction model, empowered by learning from Multi-Modal Perception (MMP) and World Models using multi-sensory data in a novel Multi-Agent Self-Awareness Architecture (MASAA). Two sub-modules are developed, the Situation Model (SM) and the First-Person model (FPM), that address different and interrelated aspects of the World Model (WM). The MMP model, instead, aims at learning the mapping of different sensory perceptions into Exteroceptive (EI) and Proprioceptive (PI) latent information. The WM then uses the learned MMP model as experience to predict dynamic self-behaviors and interaction patterns within the experienced environment. WM and MMP Models are learned in a data-driven way, starting from the lower-dimensional odometry data used to guide the learning of higher-dimensional video data, thus generating coupled Generalized State Hierarchical Dynamic Bayesian Networks (GS-HDBNs). We test our model on KITTI, CARLA, and iCab datasets, achieving high performance and a low average localization error (RMSE) of 2.897%, when considering two interacting agents.},
  archive      = {J_TMM},
  author       = {Abrham Shiferaw Alemaw and Giulia Slavic and Pamela Zontone and Lucio Marcenaro and David Martin Gomez and Carlo Regazzoni},
  doi          = {10.1109/TMM.2025.3543110},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5035-5049},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Modeling interactions between autonomous agents in a multi-agent self-awareness architecture},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CACP: Covariance-aware cross-domain prototypes for domain adaptive semantic segmentation. <em>TMM</em>, <em>27</em>, 5023-5034. (<a href='https://doi.org/10.1109/TMM.2025.3543016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptive semantic segmentation aims to reduce domain shifts / discrepancies between source and target domains, improving the source domain model's generalization ability to the target domain. Recently, prototypical methods, which primarily use single-source or single-target domain prototypes as category centers to aggregate features from both domains, have achieved competitive performance in this task. However, due to large domain shifts, single-source domain prototypes have finite generalization ability and not all source domain knowledge is conducive to model generalization. Single-target domain prototypes are noisy because they are prematurely initialized with all features filtered by pseudo labels, which causes error accumulation in the prototypes. To address these issues, we propose a covariance-aware cross-domain prototypes method (CACP) to achieve robust domain adaptation. We propose to use both domain prototypes to dynamically rectify pseudo labels in the target domain, effectively reducing the recognition difficulty of hard target domain samples and narrowing the gap between features of the same category in both domains. In addition, to further generalize the model to the target domain, we propose two modules based on covariance correlation, FSPC (Features Selection by Prototypes Covariances) and WSPC (Weighting Source by Prototypes Coefficients), to learn discriminative characteristics. FSPC selects highly correlated features to update target domain prototypes online, denoising and enhancing discriminativeness between categories. WSPC utilizes the correlation coefficients between target domain prototypes and source domain features to weight each point in the source domain, eliminating the information interference from the source domain. In particular, CACP achieves excellent performance on the GTA5 $\to$ Cityscapes and SYNTHIA $\to$ Cityscapes tasks with minimal computational resources and time.},
  archive      = {J_TMM},
  author       = {Yanbing Xue and Xinyu Tian and Feifei Zhang and Xianbin Wen and Zan Gao and Shengyong Chen},
  doi          = {10.1109/TMM.2025.3543016},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5023-5034},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CACP: Covariance-aware cross-domain prototypes for domain adaptive semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal reference learning for fine-grained text-to-image retrieval. <em>TMM</em>, <em>27</em>, 5009-5022. (<a href='https://doi.org/10.1109/TMM.2025.3543066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained text-to-image retrieval aims to retrieve a fine-grained target image with a given text query. Existing methods typically assume that each training image is accurately depicted by its textual descriptions. However, textual descriptions can be ambiguous and fail to depict discriminative visual details in images, leading to inaccurate representation learning. To alleviate the effects of text ambiguity, we propose a Multi-Modal Reference learning framework to learn robust representations. We first propose a multi-modal reference construction module to aggregate all visual and textual details of the same object into a comprehensive multi-modal reference. The multi-modal reference hence facilitates the subsequent representation learning and retrieval similarity computation. Specifically, a reference-guided representation learning module is proposed to use multi-modal references to learn more accurate visual and textual representations. Additionally, we introduce a reference-based refinement method that employs the object references to compute a reference-based similarity that refines the initial retrieval results. Extensive experiments are conducted on five fine-grained text-to-image retrieval datasets for different text-to-image retrieval tasks. The proposed method has achieved superior performance over state-of-the-art methods. For instance, on the text-to-person image retrieval dataset RSTPReid, our method achieves the Rank1 accuracy of 56.2%, surpassing the recent CFine by 5.6%.},
  archive      = {J_TMM},
  author       = {Zehong Ma and Hao Chen and Wei Zeng and Limin Su and Shiliang Zhang},
  doi          = {10.1109/TMM.2025.3543066},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {5009-5022},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-modal reference learning for fine-grained text-to-image retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal haptic compression inspired by embodied AI for haptic communications. <em>TMM</em>, <em>27</em>, 4996-5008. (<a href='https://doi.org/10.1109/TMM.2025.3542997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haptic data compression has gradually become a key issue for emerging real-time haptic communications in Tactile Internet (TI). However, it is challenging to achieve a trade-off between high perceptual quality and compression ratio in haptic data compression scheme. Inspired by the perspective of embodied AI, we propose a cross-modal haptic compression scheme for haptic communications to improve the perception quality on TI devices in this paper. Since multimodal fusion is routinely employed to improve the ability of system in cognition, we assume that haptic codec is guided by visual semantics to optimize parameter settings in the coding process. We first design a multi-dimensional tactile feature fusion network (MTFFN) relying on multi-head attention mechanism. The MTFFN extracts the multi-dimensional features from the material surface and maps them to infer the coding parameters. Secondly, we provide second-order difference and linear interpolation to establish an criterion for the determination of optimal codec parameters, which are customized by the material categories so as to give high robustness. Finally, the simulation results reveal that our compression scheme can efficiently make a personalized codec procedure for different materials, obtaining more than 17% improvement in terms of compression ratio with high perceptual quality at the same time.},
  archive      = {J_TMM},
  author       = {Hang Lu and Xinmeng Tan and Mingkai Chen and Zhe Zhang and Xuguang Zhang and Jianxin Chen and Xin Wei and Tiesong Zhao},
  doi          = {10.1109/TMM.2025.3542997},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4996-5008},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal haptic compression inspired by embodied AI for haptic communications},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical transformer with mix mask generation for thorax disease classification. <em>TMM</em>, <em>27</em>, 4984-4995. (<a href='https://doi.org/10.1109/TMM.2025.3543003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest X-ray images have been highly involved in clinical diagnosis and treatment planning for thoracic disease. The process of medical images has attracted great attention in the machine learning community. However, the labeled medical images are limited and the regions of lesions are usually much smaller in the image. Most of the existing methods are prone to learning the spurious correlation for classification, resulting in poor generalization. In this paper, we propose a medical generation transformer network based on self-supervised learning and the adversarial strategy to capture the discriminative label-relevant regions with lesions in the images by extending the Chest X-ray images. In the proposed method, we first localize the label-relevant regions in each transformer layer. Then we keep the label-relevant regions to mask the image and construct the masked image with self-supervised learning. Thus we can generate more images to fine-tune the classification network with masked images that keep the label-relevant regions. Since the generated images are usually noisy to fine-tune the classification network, we adopt the adversarial probabilities to weight the importance of each generated image for training. Experimental results on two large-scale and popular chest X-ray datasets show that the proposed method can efficiently leverage the location of lesions to improve the performance of classification.},
  archive      = {J_TMM},
  author       = {Ziyi Liu and Zengmao Wang and Bo Du},
  doi          = {10.1109/TMM.2025.3543003},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4984-4995},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Medical transformer with mix mask generation for thorax disease classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised semantic soft label learning network for deep multi-view clustering. <em>TMM</em>, <em>27</em>, 4971-4983. (<a href='https://doi.org/10.1109/TMM.2025.3543075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering, which identifies shared semantics from different perspectives and classifies data samples into distinct categories using unsupervised methods, is gaining increasing interest. This task primarily focuses on learning consistent multi-view feature representations and clustering labels. Current approaches for achieving consistent multi-view feature representations often use techniques such as cascading, weight fusion, and attention mechanism fusion. These methods reconstruct features based on original low-level features via encoder-decoder, which often contain visual private information, leading to misleading feature representations. Furthermore, in the clustering label learning process, many methods use a two-stage approach: first, they achieve consistent feature representations, and then they apply hard labeling methods like K-means or spectral clustering to obtain clustering labels. Single-stage methods typically derive consistent labels through a linear coding layer based on consistent representation learning. These methods do not fully utilize the multi-view view semantic information, and consistent representation learning may be impaired when some low-quality views are present, leading to the generation of inaccurate semantic labels. To address these issues, we propose a Self-supervised Semantic Soft Label Learning Network for Deep Multi-view Clustering. Specifically, we introduce a consensus high-level feature learning module that uses a shared MLP layer to transform low-level features into a high-level feature space. To enhance the consistency between high-level features from different views, we maximize mutual information between these features and introduce the U-Projection module, which improves the expressive power of the consensus feature via resampling the features and concatenating the fused features before and after sampling operations. Additionally, we propose a self-supervised semantic label learning module that employs a dual-branch approach to independently learn consistent view-specific semantic labels through contrastive learning, while deriving view-consensus semantic labels from shared high-level features extracted from multiple views. Finally, KL divergence is used to align the view-consensus labels with the view-specific labels. A series of extensive experiments have shown that our approach yields superior clustering results compared to existing techniques.},
  archive      = {J_TMM},
  author       = {Weiqing Yan and Tingyu Yang and Chang Tang},
  doi          = {10.1109/TMM.2025.3543075},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4971-4983},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-supervised semantic soft label learning network for deep multi-view clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-granularity relation graph aggregation framework with multimodal clues for social relation reasoning. <em>TMM</em>, <em>27</em>, 4961-4970. (<a href='https://doi.org/10.1109/TMM.2025.3543054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social relation is a fundamental attribute of human beings in daily life. The ability of humans to form large organizations and institutions stems directly from our complex social networks. Therefore, understanding social relationships in the context of multimedia is crucial for building domain-specific or general artificial intelligence systems. The key to reason social relations lies in understanding the human interactions between individuals through multimodal representations such as action and utterance. However, due to video editing techniques and various narrative sequences in videos, two individuals with social relationships may not appear together in the same frame or clip. Additionally, social relations may manifest in different levels of granularity in video expressions. Previous research has not effectively addressed these challenges. Therefore, this paper proposes a Multi-Granularity Relation Graph Aggregation Framework (MGRG) to enhance the inference ability for social relation reasoning in multimedia content, like video. Different from existing methods, our method considers the paradigm of jointly inferring the relations by constructing a social relation graph. We design a hierarchical multimodal relation graph illustrating the exchange of information between individuals' roles, capturing the complex interactions at multi-levels of granularity from fine to coarse. In MGRG, we propose two aggregation modules to cluster multimodal features in different granularity layer relation graph, considering temporal aspects and importance. Experimental results show that our method generates a logical and coherent social relation graph and improves the performance in accuracy.},
  archive      = {J_TMM},
  author       = {Cong Xu and Feiyu Chen and Qi Jia and Yihua Wang and Liang Jin and Yunji Li and Yaqian Zhao and Changming Zhao},
  doi          = {10.1109/TMM.2025.3543054},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4961-4970},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A multi-granularity relation graph aggregation framework with multimodal clues for social relation reasoning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SyNet: A synergistic network for 3D object detection through geometric-semantic-based multi-interaction fusion. <em>TMM</em>, <em>27</em>, 4950-4960. (<a href='https://doi.org/10.1109/TMM.2025.3542993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by rising demands in autonomous driving, robotics, etc., 3D object detection has recently achieved great advancement by fusing optical images and LiDAR point data. On the other hand, most existing optical-LiDAR fusion methods straightly overlay RGB images and point clouds without adequately exploiting the synergy between them, leading to suboptimal fusion and 3D detection performance. Additionally, they often suffer from limited localization accuracy without proper balancing of global and local object information. To address this issue, we design a synergistic network (SyNet) that fuses geometric information, semantic information, as well as global and local information of objects for robust and accurate 3D detection. The SyNet captures synergies between optical images and LiDAR point clouds from three perspectives. The first is geometric, which derives high-quality depth by projecting point clouds onto multi-view images, enriching optical RGB images with 3D spatial information for a more accurate interpretation of image semantics. The second is semantic, which voxelizes point clouds and establishes correspondences between the derived voxels and image pixels, enriching 3D point clouds with semantic information for more accurate 3D detection. The third is balancing local and global object information, which introduces deformable self-attention and cross-attention to process the two types of complementary information in parallel for more accurate object localization. Extensive experiments show that SyNet achieves 70.7% mAP and 73.5% NDS on the nuScenes test set, demonstrating its effectiveness and superiority as compared with the state-of-the-art.},
  archive      = {J_TMM},
  author       = {Xiaoqin Zhang and Kenan Bi and Sixian Chan and Shijian Lu and Xiaolong Zhou},
  doi          = {10.1109/TMM.2025.3542993},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4950-4960},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SyNet: A synergistic network for 3D object detection through geometric-semantic-based multi-interaction fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ContextualCoder: Adaptive in-context prompting for programmatic visual question answering. <em>TMM</em>, <em>27</em>, 4936-4949. (<a href='https://doi.org/10.1109/TMM.2025.3543043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) presents a challenging task at the intersection of computer vision and natural language processing, aiming to bridge the semantic gap between visual perception and linguistic comprehension. Traditional VQA approaches do not distinguish between data processing and reasoning, limiting their interpretability and generalizability in complex and diverse scenarios. Conversely, Programmatic Visual Question Answering (PVQA) models leverage large language models (LLMs) to generate executable codes, providing answers with detailed and interpretable reasoning processes. However, existing PVQA models typically rely on simplistic input-output prompting, which struggles to elicit domain-specific knowledge from LLMs and often produces unclear or extraneous outputs. Furthermore, PVQA models typically rely on a basic in-context example (ICE) selection methodology that is heavily influenced by individual word similarity rather than the overall sentence context. This leads to suboptimal ICE selection and a reliance on dataset-specific ICE candidates. In this paper, we propose ContextualCoder, a novel prompting framework tailored for PVQA models. ContextualCoder leverages frozen LLMs for code generation and pre-trained visual models for code execution, eliminating the need for extensive training and enhancing model flexibility. By incorporating an innovative prompting methodology and a novel ICE selection strategy, ContextualCoder facilitates the use of diverse in-context information for code generation, thereby improving the performance of PVQA models. Our approach surpasses state-of-the-art models, as evidenced by comprehensive experiments across diverse VQA datasets, including multilingual scenarios.},
  archive      = {J_TMM},
  author       = {Ruoyue Shen and Nakamasa Inoue and Dayan Guan and Rizhao Cai and Alex C. Kot and Koichi Shinoda},
  doi          = {10.1109/TMM.2025.3543043},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4936-4949},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ContextualCoder: Adaptive in-context prompting for programmatic visual question answering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced generative framework with LLMs for multimodal emotion-cause pair extraction in conversations. <em>TMM</em>, <em>27</em>, 4924-4935. (<a href='https://doi.org/10.1109/TMM.2025.3543080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion-Cause Pair Extraction (ECPE) in conversations aims to identify the emotional utterances (even their categories) along with their corresponding causal utterances, which is crucial in understanding the cause-effect relationship in dialogues. While prior studies of ECPE have predominantly focused on purely textual dialogues and neglected the exploration on the natural scenario of the dialogues with multimodal features, i.e., Multimodal Emotion-Cause Pair Extraction (MECPE) in conversations. To attempt this scenario, we propose a Generative approach for Multimodal Emotion-Cause pair extraction (GMEC) with a single stage, thus effectively reducing errors associated with the propagation and accumulation for MECPE. This approach can not only uniformly handle the information of diverse modalities, but also address all emotion and cause analysis tasks uniformly. Additionally, instead of utilizing the fixed commonsense knowledge base as previously, we resort to the Large Language Models (LLMs), which possess a powerful ability to emerge new knowledge, thereby acting as implicit knowledge engines for MECPE. We refer to this approach as enhanced GMEC. Extensive experimental results and detailed analysis demonstrate a notable improvement in the generative approach. Moreover, the integration of external knowledge from LLMs optimizes the efficiency of data utilization, particularly in few-shot scenarios. The integration of the generative model with LLMs has resulted in a cumulative enhancement of 4.94%, 10.90% on MECPE and MECPE-C (with emotion Category).},
  archive      = {J_TMM},
  author       = {Xincheng Ju and Dong Zhang and Junhui Li and Shoushan Li and Guodong Zhou},
  doi          = {10.1109/TMM.2025.3543080},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4924-4935},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced generative framework with LLMs for multimodal emotion-cause pair extraction in conversations},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $\mathrm{Tri^{2}plane}$: Advancing neural implicit surface reconstruction for indoor scenes. <em>TMM</em>, <em>27</em>, 4910-4923. (<a href='https://doi.org/10.1109/TMM.2025.3565989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing 3D indoor scenes presents significant challenges, requiring models capable of inferring both planar surfaces and intricate details. Although recent methods can generate complete surfaces, they often struggle to simultaneously reconstruct low-texture regions and high-frequency details due to non-local effects. In this paper, we introduce a novel triangle-based triplane representation, named (tri$^{2}$plane), specifically designed to account for the diverse spatial feature distribution and information density of indoor environments. Our method begins by projecting point clouds onto three orthogonal planes, followed by 2D Delaunay triangulation. This representation enables adaptive encoding of low-texture and high-frequency regions by employing triangles of variable sizes. Moreover, we develop a dual tri$^{2}$ plane framework that incorporates both geometric and semantic information, significantly enhancing the reconstruction quality. We combine these key modules and evaluate our method on benchmark indoor scene datasets. The results unequivocally demonstrate the superiority of our proposed method over the state-of-the-art Occ-SDF. Specifically, our method achieves significant improvements over Occ-SDF, with margins of 1.3, 1.7, and 2.3 in F-score on the ScanNet, Tanks & Temples, and Replica datasets, respectively. To facilitate further research, we will make our code publicly available.},
  archive      = {J_TMM},
  author       = {Yiping Xie and Haihong Xiao and Wenxiong Kang},
  doi          = {10.1109/TMM.2025.3565989},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4910-4923},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {$\mathrm{Tri^{2}plane}$: Advancing neural implicit surface reconstruction for indoor scenes},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal aware-based unsupervised network for infrared small target detection. <em>TMM</em>, <em>27</em>, 4895-4909. (<a href='https://doi.org/10.1109/TMM.2025.3543002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advantages of deep learning (DL) techniques, various infrared (IR) small target detection networks have been proposed. While many networks aim at single-frame detection through supervised learning, ignoring abundant spatial-temporal information and causing heavy labeling costs. In this paper, we develop a 3-D spatial-temporal knowledge aware-based unsupervised network for IR target detection (STUTD). Specifically, we transform IR sequences into 3-D spatial-temporal tensors as data foundation. Based on the designed spatial-temporal Swin Transformer block (ST-STB), we introduce a multiscale feature extraction and aggregation (MFEA) module for effective feature extraction. And a Variational Autoencoder (VAE)-style background reconstruction module with a multihead gating mechanism is designed for background reconstruction. Besides, a designed sparse cardinality selection of residuals performs element-wise filtering on the residuals between the original tensors and the reconstructed background to obtain a pure target tensor. By an unsupervised learning approach, STUTD can achieve IR small target detection. Comprehensive experiments illustrate the superiority of STUTD among state-of-the-art methods. It can be concluded that STUTD has satisfactory overall performance and real-time performance.},
  archive      = {J_TMM},
  author       = {Yuan Luo and Xiaorun Li and Shuhan Chen},
  doi          = {10.1109/TMM.2025.3543002},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4895-4909},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial-temporal aware-based unsupervised network for infrared small target detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An information compensation framework for zero-shot skeleton-based action recognition. <em>TMM</em>, <em>27</em>, 4882-4894. (<a href='https://doi.org/10.1109/TMM.2025.3543004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot human skeleton-based action recognition aims to construct a model that can recognize actions outside the categories seen during training. Previous research has focused on aligning sequences' visual and semantic spatial distributions. However, these methods extract semantic features simply. They ignore that proper prompt design for rich and fine-grained action cues can provide robust representation space clustering. In order to alleviate the problem of insufficient information available for skeleton sequences, we design an information compensation learning framework from an information-theoretic perspective to improve zero-shot action recognition accuracy with a multi-granularity semantic interaction mechanism. Inspired by ensemble learning, we propose a multi-level alignment (MLA) approach to compensate information for action classes. MLA aligns multi-granularity embeddings with visual embedding through a multi-head scoring mechanism to distinguish semantically similar action names and visually similar actions. Furthermore, we introduce a new loss function sampling method to obtain a tight and robust representation. Finally, these multi-granularity semantic embeddings are synthesized to form a proper decision surface for classification. Significant action recognition performance is achieved when evaluated on the challenging NTU RGB+D, NTU RGB+D 120, and PKU-MMD benchmarks and validate that multi-granularity semantic features facilitate the differentiation of action clusters with similar visual features.},
  archive      = {J_TMM},
  author       = {Haojun Xu and Yan Gao and Jie Li and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3543004},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4882-4894},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An information compensation framework for zero-shot skeleton-based action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SVSRD: Spatial visual and statistical relation distillation for class-incremental semantic segmentation. <em>TMM</em>, <em>27</em>, 4869-4881. (<a href='https://doi.org/10.1109/TMM.2025.3543102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental semantic segmentation (CISS) aims to incrementally learn novel classes while retaining the ability to segment old classes, and suffers catastrophic forgetting since the old-class labels are unavailable. Most existing methods typically impose strict constraints on the consistency between the extracted features or output logits of each pixel from old and current models in an attempt to prevent forgetting through knowledge distillation (KD), which 1) results in a significant transfer of redundant knowledge while limiting the restoration of old classes (rigidity) due to potentially overlooking essential knowledge extraction, and 2) imposes strong constraints at the pixel level making it challenging for the model to learn novel classes (plasticity). To solve the above limitations, we propose a novel Spatial Visual and Statistical Relation Distillation (SVSRD) by applying multi-scale visual and statistical position relation distillation for CISS, which enjoys several merits. First, we introduce a region-based similarity matrix and impose a consistency constraint between current and old models, which preserves the essential visual knowledge to enhance the rigidity. Second, we propose a novel statistical feature calculation algorithm to investigate the distribution of the data and further preserve the rules of statistics through statistical consistency, which also promotes the model on the novel-class learning for improving the plasticity. Finally, the aforementioned constraints are jointly applied in multiple scales to alleviate old-class forgetting and enhance novel-class learning. Extensive experiments on Pascal-VOC 2012 and ADE20 K demonstrate that the proposed approach performs favorably against the state-of-the-art CISS methods.},
  archive      = {J_TMM},
  author       = {Yuyang Chang and Yifan Jiao and Bing-Kun Bao},
  doi          = {10.1109/TMM.2025.3543102},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4869-4881},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SVSRD: Spatial visual and statistical relation distillation for class-incremental semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep semantic segmentation network with semantic and contextual refinements. <em>TMM</em>, <em>27</em>, 4856-4868. (<a href='https://doi.org/10.1109/TMM.2025.3543037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a fundamental task in multimedia processing, which can be used for analyzing, understanding, editing contents of images and videos, among others. To accelerate the analysis of multimedia data, existing segmentation researches tend to extract semantic information by progressively reducing the spatial resolutions of feature maps. However, this approach introduces a misalignment problem when restoring the resolution of high-level feature maps. In this paper, we design a Semantic Refinement Module (SRM) to address this issue within the segmentation network. Specifically, SRM is designed to learn a transformation offset for each pixel in the upsampled feature maps, guided by high-resolution feature maps and neighboring offsets. By applying these offsets to the upsampled feature maps, SRM enhances the semantic representation of the segmentation network, particularly for pixels around object boundaries. Furthermore, a Contextual Refinement Module (CRM) is presented to capture global context information across both spatial and channel dimensions. To balance dimensions between channel and space, we aggregate the semantic maps from all four stages of the backbone to enrich channel context information. The efficacy of these proposed modules is validated on three widely used datasets—Cityscapes, Bdd100 K, and ADE20K—demonstrating superior performance compared to state-of-the-art methods. Additionally, this paper extends these modules to a lightweight segmentation network, achieving an mIoU of 82.5% on the Cityscapes validation set with only 137.9 GFLOPs.},
  archive      = {J_TMM},
  author       = {Zhiyan Wang and Deyin Liu and Lin Yuanbo Wu and Song Wang and Xin Guo and Lin Qi},
  doi          = {10.1109/TMM.2025.3543037},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4856-4868},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A deep semantic segmentation network with semantic and contextual refinements},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error-aware generative reasoning for zero-shot visual grounding. <em>TMM</em>, <em>27</em>, 4844-4855. (<a href='https://doi.org/10.1109/TMM.2025.3543062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot visual grounding is the task of identifying and localizing an object in an image based on a referring expression without task-specific training. Existing methods employ heuristic rules to step-by-step perform visual perception for visual grounding. Despite their remarkable performance, there are still two limitations. First, such a rule-based manner struggles with expressions that are not covered by predefined rules. Second, existing methods lack a mechanism for identifying and correcting visual perceptual errors of incomplete information, resulting in cascading errors caused by reasoning based on incomplete visual perception results. In this article, we propose an Error-Aware Generative Reasoning (EAGR) method for zero-shot visual grounding. To address the limited adaptability of existing methods, a reasoning chain generator is presented, which prompts LLMs to dynamically generate reasoning chains for specific referring expressions. This generative manner eliminates the reliance on human-written heuristic rules. To mitigate visual perceptual errors of incomplete information, an error-aware mechanism is presented to elicit LLMs to identify these errors and explore correction strategies. Experimental results on four benchmarks show that EAGR outperforms state-of-the-art zero-shot methods by up to 10% and an average of 7%.},
  archive      = {J_TMM},
  author       = {Yuqi Bu and Xin Wu and Yi Cai and Qiong Liu and Tao Wang and Qingbao Huang},
  doi          = {10.1109/TMM.2025.3543062},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4844-4855},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Error-aware generative reasoning for zero-shot visual grounding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing event-based video reconstruction with bidirectional temporal information. <em>TMM</em>, <em>27</em>, 4831-4843. (<a href='https://doi.org/10.1109/TMM.2025.3543010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-based video reconstruction has emerged as an appealing research direction to break through the limitations of traditional cameras to better record dynamic scenes. Most existing methods reconstruct each frame from its corresponding event subset in chronological order. Since the temporal information contained in the whole event sequence is not fully exploited, these methods suffer inferior reconstruction quality. In this paper, we propose to enhance event-based video reconstruction by leveraging the bidirectional temporal information in event sequences. The proposed model processes event sequences in a bidirectional fashion, allowing for exploiting bidirectional information in the whole sequence. Furthermore, a transformer-based temporal information fusion module is introduced to aggregate long-range information in both temporal and spatial dimensions. Additionally, we propose a new dataset for the event-based video reconstruction task which contains a variety of objects and movement patterns. Extensive experiments demonstrate that the proposed model outperforms existing state-of-the-art event-based video reconstruction methods both quantitatively and qualitatively.},
  archive      = {J_TMM},
  author       = {Pinghai Gao and Longguang Wang and Sheng Ao and Ye Zhang and Yulan Guo},
  doi          = {10.1109/TMM.2025.3543010},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4831-4843},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing event-based video reconstruction with bidirectional temporal information},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aggregate and discriminate: Pseudo clips-guided boundary perception for video moment retrieval. <em>TMM</em>, <em>27</em>, 4819-4830. (<a href='https://doi.org/10.1109/TMM.2025.3542894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video moment retrieval (VMR) aims to localize a video segment in an untrimmed video that is semantically relevant to a language query. The challenge of this task lies in effectively aligning the intricate and information-dense video modality with the succinctly summarized textual modality, and further localizing the starting and ending timestamps of the target moments. Previous works have attempted to achieve multi-granularity alignment of video and query in a coarse-to-fine manner, yet these efforts still fall short in addressing the inherent disparities in representation and information density between videos and queries, leading to modal misalignments. In this paper, we propose a progressive video moment retrieval framework, initially retrieving the most relevant and irrelevant video clips to the query as semantic guidance, thereby bridging the semantic gap between video modality and language modality. Futhermore, we introduce a pseudo clips guided aggregation module to aggregate densely relevant moment clips closer together and propose a discriminative boundary-enhanced decoder with the guidance of pseudo clips to push the semantically confusing proposals away. Extensive experiments on the Charades-STA, ActivityNet Captions and TACoS datasets demonstrate that our method outperforms existing methods.},
  archive      = {J_TMM},
  author       = {Jing Liu and Zongbing Zhang and Yuting Su and Bing Yang and Xiongkuo Min and Guangtao Zhai},
  doi          = {10.1109/TMM.2025.3542894},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4819-4830},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Aggregate and discriminate: Pseudo clips-guided boundary perception for video moment retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UW-adapter: Adapting monocular depth estimation model in underwater scenes. <em>TMM</em>, <em>27</em>, 4808-4818. (<a href='https://doi.org/10.1109/TMM.2025.3543089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating depth maps from monocular underwater images poses one of the most challenging problems in underwater applications. Due to the lack of large-scale paired underwater color-depth datasets for effective training, existing style transfer-based and self-supervision-based approaches can improve the performance of depth estimation to some extent, but they remain unsatisfactory. Leveraging the power of massive training datasets, foundation models designed for terrestrial monocular depth estimation have demonstrated superior performance across various scenes. These models provide rich prior knowledge of 3D perception, which can be valuable for underwater depth estimation. Upon this, we introduce tunable adapters (UW-Adapter) that tailor a pre-trained foundation model specifically for underwater depth estimation, customizing it to the unique characteristics of underwater imagery. Our approach involves freezing the parameters of the pre-trained model and updating only the adapters through self-supervision. To address the complex degradation of underwater images, we propose two adapters: the transmission adapter and the high-frequency adapter. These adapters incorporate depth clues and high-frequency information as prior knowledge, thereby enhancing the performance of pre-trained model in underwater depth estimation. Experimental results demonstrate that by integrating lightweight adapters into off-the-shelf depth estimation foundation models, our method achieves superior performance across multiple datasets.},
  archive      = {J_TMM},
  author       = {Xinchen Ye and Yue Chang and Rui Xu and Haojie Li},
  doi          = {10.1109/TMM.2025.3543089},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4808-4818},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {UW-adapter: Adapting monocular depth estimation model in underwater scenes},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition with heatmap neighbor contrastive learning. <em>TMM</em>, <em>27</em>, 4795-4807. (<a href='https://doi.org/10.1109/TMM.2025.3543029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many supervised learning-based facial expression recognition (FER) methods achieve good performance with the assistance of expression labels and a complex framework. However, there are inconsistent annotations in different expression datasets, making the above methods disadvantageous for new expression datasets or datasets with limited training data. The objective of this paper is to learn self-supervised facial expression features that enable the FER model not to rely on the annotation consistency of the different datasets. Most current self-supervised learning algorithms based on contrastive learning learn the representation by forcing different augmented views of the same image close in the embedding space, but they cannot cover all variances within a semantic class. We propose a heatmap neighbor contrastive learning (HNCL) method for FER. It treats the images corresponding to the heatmap nearest neighbors of expressions as other positives, providing more semantic variations than pre-defined augmented transformations. Therefore, our HNCL can learn better expression features covering more intra-class variances, improving the performance of the FER model based on self-supervised learning. After fine-tuning, HNCL with a simple framework achieves top-three performance on the in-the-lab datasets and even matches the performance of state-of-the-art supervised learning methods on the in-the-wild datasets.},
  archive      = {J_TMM},
  author       = {Tong Liu and Jing Li and Jia Wu and Bo Du and Yibing Zhan and Dapeng Tao and Jun Wan},
  doi          = {10.1109/TMM.2025.3543029},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4795-4807},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Facial expression recognition with heatmap neighbor contrastive learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preemptive defense algorithm based on generalizable black-box feedback regulation strategy against face-swapping deepfake models. <em>TMM</em>, <em>27</em>, 4780-4794. (<a href='https://doi.org/10.1109/TMM.2025.3543059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the previous efforts to counteract Deepfake, detection methods were most adopted, but they could only function after-effect and could not undo the harm. Preemptive defense has recently gained attention as an alternative, but such defense works have either limited their scenario to facial-reenactment Deepfake models or only targeted specific face-swapping Deepfake model. Motivated to fill this gap, we start by establishing the Deepfake scenario modeling and finding the scenario difference among categories, then move on to the face-swapping scenario setting overlooked by previous works. Based on this scenario, we first propose a novel Black-Box Penetrating Defense Process that enables defense against face-swapping models without prior model knowledge. Then we propose a novel Double-Blind Feedback Regulation Strategy to solve the reality problem of avoiding alarming distortions after defense that had previously been ignored, which helps conduct valid preemptive defense against face-swapping Deepfake models in reality. Experimental results in comparison with state-of-the-art defense methods are conducted against popular face-swapping Deepfake models, proving our proposed method valid under practical circumstances.},
  archive      = {J_TMM},
  author       = {Zhongjie Mi and Xinghao Jiang and Tanfeng Sun and Ke Xu and Qiang Xu},
  doi          = {10.1109/TMM.2025.3543059},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4780-4794},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Preemptive defense algorithm based on generalizable black-box feedback regulation strategy against face-swapping deepfake models},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rain2Avoid: Learning deraining by self-supervision. <em>TMM</em>, <em>27</em>, 4765-4779. (<a href='https://doi.org/10.1109/TMM.2025.3542981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured on rainy days often contain rain streaks that can obscure important scenery and degrade the performance of high-level vision tasks, such as image segmentation in autonomous vehicles. As a result, image deraining, a low-level vision task focused on removing rain streaks from images, has gained popularity over the past decade. Recent advancements have primarily concentrated on supervised image deraining methods, which rely on paired rain-clean image datasets to train deep neural network models. However, collecting such paired real data is challenging and time-consuming. To address this, our method introduces a novel self-supervised approach that leverages the proposed locally dominant gradient prior and non-local self-similarity stochastic sampling. This approach extracts potential rain streaks and generates stochastic derained references for image deraining. Experimental results on public benchmark image-deraining datasets show that our proposed method performs favorably against state-of-the-art few-shot and self-supervised image deraining methods.},
  archive      = {J_TMM},
  author       = {Yan-Tsung Peng and Wei-Hua Li and Zihao Chen},
  doi          = {10.1109/TMM.2025.3542981},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4765-4779},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rain2Avoid: Learning deraining by self-supervision},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Viewport prediction with unsupervised multiscale causal representation learning for virtual reality video streaming. <em>TMM</em>, <em>27</em>, 4752-4764. (<a href='https://doi.org/10.1109/TMM.2025.3543087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of the metaverse has driven the rapid development of various applications, such as Virtual Reality (VR) and Augmented Reality (AR). As a form of multimedia in the metaverse, VR video streaming (a.k.a., VR spherical video streaming and 360$^{\circ }$ video streaming) can provide users with a 360$^{\circ }$ immersive experience. Generally, transmitting VR video requires far more bandwidth than regular videos, which greatly strains existing network transmission. Predicting and selectively streaming VR video in the users' viewports in advance can reduce bandwidth consumption and system latency. However, existing methods either consider only historical viewport-based prediction methods or predict viewports by correlations between visual features of video frames, making it hard to adapt to the dynamics of users and video content. In the meantime, spurious correlations between visual features lead to inaccurate and unreliable prediction results. Hence, we propose an unsupervised multiscale causal representation learning (UMCRL)-based method to predict viewports in VR video streaming, including user preference-based and video content-based viewport prediction models. The former is designed by a position predictor to predict the future users' viewports based on their historical viewports in multiple video frames to adapt to users' dynamic preferences. The latter achieves unsupervised multiscale causal representation learning through an asymmetric causal regressor, used to infer the causalities between local and global-local visual features in video frames, thereby helping the model understand the contextual information in the videos. We embed the causalities in the transformer decoder via causal self-attention for predicting the users' viewports, adapting to the dynamic changes of video content. Finally, combining the results of the two aforementioned models yields the final prediction of the users' viewports. In addition, the QoE of users is satisfied by assigning different bitrates to the tiles in the viewport through a pyramid-based bitrate allocation. The experimental results verify the effectiveness of the method.},
  archive      = {J_TMM},
  author       = {Yingjie Liu and Dan Wang and Bin Song},
  doi          = {10.1109/TMM.2025.3543087},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4752-4764},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Viewport prediction with unsupervised multiscale causal representation learning for virtual reality video streaming},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MVL-net: Pairwise learning for multi-view multiple people labelling. <em>TMM</em>, <em>27</em>, 4736-4751. (<a href='https://doi.org/10.1109/TMM.2025.3535383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multi-view domain, it is challenging to correctly label multiple people across viewpoints because of occlusions, visual ambiguities, appearance variation, etc. Deep learning, although having witnessed remarkable success in computer vision tasks, still remains underexplored for the multi-view labelling task, due to the lack of labelled multi-view datasets. In this paper, we propose a novel end-to-end deep neural network named Multi-View Labelling network (MVL-net) that addresses this issue. To overcome the dataset shortage, a large-scale multi-view dataset is generated by combining 3D human models and panoramic backgrounds, along with human poses and realistic rendering. In the proposed MVL-net, we first incorporate Transformer blocks to capture the non-local information for multi-view feature extraction. A matching net is then introduced to achieve multiple people labelling, by predicting matching confidence scores for pairwise instances from two views, thus addressing the problem of the unknown number of people when labelling across views. An additional geometry feature obtained from the epipolar geometry is integrated to leverage multi-view cues during training. To the best of our knowledge, the MVL-net is the first work using deep learning to train a multi-view labelling network. Comprehensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed method, which outperforms the existing state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Yue Zhang and Akin Caliskan and Mai Xu and Adrian Hilton and Jean-Yves Guillemaut},
  doi          = {10.1109/TMM.2025.3535383},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4736-4751},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MVL-net: Pairwise learning for multi-view multiple people labelling},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TITFormer: Combining textual modality and simulating infrared modality based on transformer for image enhancement. <em>TMM</em>, <em>27</em>, 4725-4735. (<a href='https://doi.org/10.1109/TMM.2025.3535318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image enhancement task requires a complex balance between extracting high-level contextual information and optimizing spatial details in the image to improve the visual quality. Most of existing methods have limited capability in capturing contextual features and optimizing spatial details when they only rely on a single modality. To address the above issues, this paper introduces a novel multi-modal image enhancement network based on Transformer, named as TITFormer, which combines textual and simulating infrared modalities firstly for this important task. TITFormer comprises a text channel attention fusion (TCF) network block and an infrared-guided spatial detail optimization (SDO) network block. The TCF extracts contextual features from the high-dimensional features compressed after spatial channel transformation of the textual feature and image feature. The SDO module uses simulating infrared images characterized by pixel intensity to guide the optimization of spatial details with contextual features adaptively. Experimental results demonstrate that TITFormer achieves state-of-the-art performance on two publicly available benchmark datasets.},
  archive      = {J_TMM},
  author       = {Kaijiang Li and Haining Li and Miduo Cui and Junxin Li and Pei Lv and Bing Zhou and Mingliang Xu},
  doi          = {10.1109/TMM.2025.3535318},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4725-4735},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TITFormer: Combining textual modality and simulating infrared modality based on transformer for image enhancement},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient chroma intra prediction via exemplar colorization network for versatile video coding. <em>TMM</em>, <em>27</em>, 4713-4724. (<a href='https://doi.org/10.1109/TMM.2025.3535312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chroma intra prediction aims to reduce chroma redundancies within a frame, which plays an important role in improving the coding efficiency of intra coding. Existing chroma intra prediction methods typically utilize the spatial relationship between the current luma block and its neighboring reference luma blocks to predict its chroma samples. However, the spatial properties of luma components differ from those of chroma components, which limits the accuracy of chroma intra prediction. To tackle this issue, an efficient Exemplar Colorization Network (ECNet)-based chroma intra prediction method is proposed in this paper, in which the colorization relationship between reference luma and chroma components is exploited to predict the chroma components for the current luma component. Inspired by the principle that semantic information in an image exhibits short-range continuity, a Spatial-consistency-based Colorization Transfer Network (SCTNet) is proposed, which builds and transfers colorization representations of neighboring reference blocks for chroma prediction. To improve the chroma prediction capability of SCTNet, a colorization learning module is developed to learn the robust mapping relationship from the luma component to the chroma component in a region-to-pixel manner, and a weight-adaptive reconstruction module is designed to adaptively utilize reference information from neighboring blocks to generate an initial prediction result. In addition, to further improve the accuracy of chroma intra prediction, a multi-reference-based chroma refinement network is proposed, which simultaneously uses the spatial information of neighboring reference chroma blocks and the current luma block to eliminate blocking and color-bleeding artifacts in the initial prediction result. Experimental results demonstrate that our proposed ECNet outperforms the state-of-the-art chroma intra prediction methods in terms of coding performance.},
  archive      = {J_TMM},
  author       = {Zhaoqing Pan and Jixing Chen and Bo Peng and Jianjun Lei and Fu Lee Wang and Nam Ling and Sam Kwong},
  doi          = {10.1109/TMM.2025.3535312},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4713-4724},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient chroma intra prediction via exemplar colorization network for versatile video coding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Centra-net: A centralized network for visual localization spanning multiple scenes. <em>TMM</em>, <em>27</em>, 4698-4712. (<a href='https://doi.org/10.1109/TMM.2025.3535388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Centra-Net, a centralized network that concurrently optimizes visual localization over numerous scenes under heterogeneous dataset domains. Centra-Net exemplifies storage efficiency by amalgamating multiple models with task-shared parameters into a singular cohesive structure. Technically, we develop a basic feature extraction unit (BFEU) with two parallel branches: one dedicated to local feature extraction and the other adept at adaptively generating a task-specific attention mask for feature calibration, thus bolstering its feature extraction capability across diverse scenes. Based on the BFEU, we introduce a filter-wise sharing mechanism (FSM) that adaptively determines parameter sharing within the unit, thus facilitating fine-grained parameter allocation. The key insight of FSM resides in reconceptualizing the parameter sharing of the unit as a learnable paradigm, enabling the determination of shared parameters to be made post-training. Finally, we suggest a complexity-prioritized gradient algorithm (CPGA) that capitalizes on task complexity to attain a harmonious learning space for various tasks, thus safeguarding optimal performances across all tasks. Through rigorous experiments on numerous benchmarks, Centra-Net demonstrates a notable edge over existing state-of-the-art works while operating with a significantly reduced parameter footprint.},
  archive      = {J_TMM},
  author       = {Zhiqiang Jiang and Kun Dai and Ke Wang and Tao Xie and Zhendong Fan and Ruifeng Li and Peng Kang and Lijun Zhao},
  doi          = {10.1109/TMM.2025.3535388},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4698-4712},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Centra-net: A centralized network for visual localization spanning multiple scenes},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning pyramid-structured long-range dependencies for 3D human pose estimation. <em>TMM</em>, <em>27</em>, 4684-4697. (<a href='https://doi.org/10.1109/TMM.2025.3535349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action coordination in human structure is indispensable for the spatial constraints of 2D joints to recover 3D pose. Usually, action coordination is represented as a long-range dependence among body parts. However, there are two main challenges in modeling long-range dependencies. First, joints should not only be constrained by other individual joints but also be modulated by the body parts. Second, existing methods make networks deeper to learn dependencies between non-linked parts. They introduce uncorrelated noise and increase the model size. In this paper, we utilize a pyramid structure to better learn potential long-range dependencies. It can capture the correlation across joints and groups, which complements the context of the human sub-structure. In an effective cross-scale way, it captures the pyramid-structured long-range dependence. Specifically, we propose a novel Pyramid Graph Attention (PGA) module to capture long-range cross-scale dependencies. It concatenates information from various scales into a compact sequence, and then computes the correlation between scales in parallel. Combining PGA with graph convolution modules, we develop a Pyramid Graph Transformer (PGFormer) for 3D human pose estimation, which is a lightweight multi-scale transformer architecture. It encapsulates human sub-structures into self-attention by pooling. Extensive experiments show that our approach achieves lower error and smaller model size than state-of-the-art methods on Human3.6 M and MPI-INF-3DHP datasets.},
  archive      = {J_TMM},
  author       = {Mingjie Wei and Xuemei Xie and Yutong Zhong and Guangming Shi},
  doi          = {10.1109/TMM.2025.3535349},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4684-4697},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning pyramid-structured long-range dependencies for 3D human pose estimation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prune and merge: Efficient token compression for vision transformer with spatial information preserved. <em>TMM</em>, <em>27</em>, 4670-4683. (<a href='https://doi.org/10.1109/TMM.2025.3535405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Token compression is essential for reducing the computational and memory requirements of transformer models, enabling their deployment in resource-constrained environments. In this work, we propose an efficient and hardware-compatible token compression method called Prune and Merge. Our approach integrates token pruning and merging operations within transformer models to achieve layer-wise token compression. By introducing trainable merge and reconstruct matrices and utilizing shortcut connections, we efficiently merge tokens while preserving important information and enabling the restoration of pruned tokens. Additionally, we introduce a novel gradient-weighted attention scoring mechanism that computes token importance scores during the training phase, eliminating the need for separate computations during inference and enhancing compression efficiency. We also leverage gradient information to capture the global impact of tokens and automatically identify optimal compression structures. Extensive experiments on the ImageNet-1 k and ADE20 K datasets validate the effectiveness of our approach, achieving significant speed-ups with minimal accuracy degradation compared to state-of-the-art methods. For instance, on DeiT-Small, we achieve a 1.64× speed-up with only a 0.2% drop in accuracy on ImageNet-1k. Moreover, by compressing segmenter models and comparing with existing methods, we demonstrate the superior performance of our approach in terms of efficiency and effectiveness.},
  archive      = {J_TMM},
  author       = {Junzhu Mao and Yang Shen and Jinyang Guo and Yazhou Yao and Xiansheng Hua and Hengtao Shen},
  doi          = {10.1109/TMM.2025.3535405},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4670-4683},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Prune and merge: Efficient token compression for vision transformer with spatial information preserved},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-guided diffusion model for camouflaged object detection. <em>TMM</em>, <em>27</em>, 4656-4669. (<a href='https://doi.org/10.1109/TMM.2025.3535290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, diffusion models have significantly improved the performance of Camouflaged Object Detection (COD) by adding noise to a mask and iteratively denoising it to match the target distributions. Due to the direct extraction of features from noisy masks and the lack of conditional constraints on a prediction area, the diffusion model may deviate from a correct prediction range and produces mispredictions in regions with high uncertainty. To address this issue, we propose an uncertainty-guided diffusion model (UGDNet) for COD, which explicitly quantifies uncertainty and integrates it as an anchor condition into the diffusion models to provide an initialization of the diffusion regions. The core idea is first to utilize a probability representation and transformer to explicitly model uncertainty, aiming to identify areas where a model may generate overconfident mispredictions. Then, we use the uncertainty as an anchor condition to provide a reference prediction range for the diffusion model, guiding each step of the diffusion process. Furthermore, we use uncertainty to guide feature aggregation, prompting the model to pay extra attention to the semantic features of regions with high uncertainty to refine the segmentation results further. The experimental results indicate that our proposed UGDNet achieves higher accuracy than existing state-of-the-art models on five COD benchmarks, including COD10K, NC4K, CAMO, CHAMELEON, and CDS2K.},
  archive      = {J_TMM},
  author       = {Jinsheng Yang and Bineng Zhong and Qihua Liang and Zhiyi Mo and Shengping Zhang and Shuxiang Song},
  doi          = {10.1109/TMM.2025.3535290},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4656-4669},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty-guided diffusion model for camouflaged object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DISD-net: A dynamic interactive network with self-distillation for cross-subject multi-modal emotion recognition. <em>TMM</em>, <em>27</em>, 4643-4655. (<a href='https://doi.org/10.1109/TMM.2025.3535344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal Emotion Recognition (MER) has demonstrated competitive performance in affective computing, owing to synthesizing information from diverse modalities. However, many existing approaches still face unresolved challenges, such as: (i) how to learn compact yet representative features from multi-modal data simultaneously and (ii) how to address differences among subjects and enhance the generalization of the emotion recognition model, given the diverse nature of individual biological signals. To this end, we propose a Dynamic Interactive Network with Self-Distillation (DISD-Net) for cross-subject MER. The DISD-Net incorporates a dynamin interactive module to capture the intra- and inter-modal interactions from multi-modal data. Additionally, to enhance compactness in modal representations, we leverage the soft labels generated by the DISD-Net model as supplemental training guidance. This involves incorporating self-distillation, aiming to transfer the knowledge that the DISD-Net model contains hard and soft labels to each modality. Finally, domain adaptation (DA) is seamlessly integrated into the dynamic interactive and self-distillation components, forming a unified framework to extract subject-invariant multi-modal emotional features. Experimental results indicate that the proposed model achieves a mean accuracy of 75.00% with a standard deviation of 7.68% for the DEAP dataset and a mean accuracy of 65.65% with a standard deviation of 5.08% for the SEED-IV dataset.},
  archive      = {J_TMM},
  author       = {Cheng Cheng and Wenzhe Liu and Xinying Wang and Lin Feng and Ziyu Jia},
  doi          = {10.1109/TMM.2025.3535344},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4643-4655},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DISD-net: A dynamic interactive network with self-distillation for cross-subject multi-modal emotion recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph-based remaining prototype alignment for open-set cross-domain image retrieval. <em>TMM</em>, <em>27</em>, 4627-4642. (<a href='https://doi.org/10.1109/TMM.2025.3535298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing cross-domain image retrieval (CDIR) methods exhibit a strong dependency on prior knowledge of training categories, which leads to problems of class confusion and domain shift when encountering unseen categories in open-set environments. In this paper, we explore the CDIR task towards open-set environments and introduce the Hypergraph-Based Remaining Prototype Alignment (RePro) framework for this task. Specifically, to address the problem of unseen class confusion caused by the category differences, we utilize the Remaining Prototype Embedding (RPE) module to generate the remaining embeddings of images and treat these embeddings as domain noise, rather than directly mapping them to the explicit domain-unified prototypes. To overcome the problem of domain shift, our method leverages the high-order correlations among both domains and categories through the Heterogeneous Structure Alignment (HSA) module, by constructing a heterogeneous hypergraph based on intra-domain and inter-category correlations. Besides, we build two multi-domain datasets for open-set cross-domain image retrieval, i.e., OCD-PACS and OCD-VLCS. Each dataset is divided into seen and unseen categories for training and testing, and each class has four different domains of images. Extensive experiments and ablation studies on these two datasets demonstrate the superiority of our method over current state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yang Xu and Yifan Feng and Xiaopin Zhong and Yue Gao and Zongze Wu},
  doi          = {10.1109/TMM.2025.3535298},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4627-4642},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hypergraph-based remaining prototype alignment for open-set cross-domain image retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep semantic-consistent penalizing hashing for cross-modal retrieval. <em>TMM</em>, <em>27</em>, 4613-4626. (<a href='https://doi.org/10.1109/TMM.2025.3535306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the advantages of low storage cost and high retrieval efficiency, hash learning could significantly speed up large-scale cross-modal retrieval. Based on the prior annotations, most of the available cross-modal hashing usually introduces the margin-based constraint to generate different boundaries for each class in the inference phase, optimizing the model. However, these obtained label-guided penalty boundaries may differ from the primitive semantic relationships between heterogeneous modalities, impairing retrieval performance. Besides, the margin-based constraint is too weak to penalize the classes with low intra-class variances or inter-class correlations, which struggle to learn high-quality embeddings. In this paper, we propose a novel Deep Semantic-consistent Penalizing Hashing framework (DScPH) to learn the consistent penalizing fields for all classes, achieving accurate and efficient cross-modal retrieval. Specifically, by exploring unbalanced intra-class and inter-class correlations, the consistent penalizing loss is introduced into cross-modal retrieval to learn the consistency decision boundaries across classes. During training, the dice-like optimization strategy is developed to balance the pulling penalizing elements and pushing penalizing elements, facilitating the model convergence. Besides, based on the invariance of similarity measures under orthogonal transformations, the alternative quantization is proposed to minimize the errors between the learned continuous embeddings and binary discretization, maintaining the consistency of semantic relationships after performing binary projection. Extensive experiments are conducted on three benchmark datasets, and the comprehensive results validate the efficacy of our proposed DScPH framework, which outperforms the current mainstream deep cross-modal hashing algorithms.},
  archive      = {J_TMM},
  author       = {Qibing Qin and Lei Wu and Wenfeng Zhang and Lei Huang and Jie Nie},
  doi          = {10.1109/TMM.2025.3535306},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4613-4626},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep semantic-consistent penalizing hashing for cross-modal retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting adversarial attacks based on tracking differences in frequency bands. <em>TMM</em>, <em>27</em>, 4597-4612. (<a href='https://doi.org/10.1109/TMM.2025.3535376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Like deep neural network (DNN)-based classifiers, DNN-based trackers are also vulnerable to adversarial attacks that degrade the tracking performance by adding adversarial perturbations to the input videos. This paper proposes a detection method for the first time to assist the tracker in detecting adversarial attacks. The adversarial perturbations in the visual object tracking task are invisible but are effective at attacking trackers. This naturally creates challenges in detecting attacks in the spatial pixel domain. To this end, we innovatively transfer the detection of adversarial attacks from the spatial domain to the frequency domain. Specifically, we first theoretically prove that the perturbations are added mainly to the high-frequency band of the video. Then, from the empirical studies, we conclude that the low-frequency band contributes most to the tracking performance and is most robust against adversarial attacks. According to the theoretical proof and empirical conclusion, we finally design an unsupervised adversarial detection framework, which mainly contains a frequency decomposition module (FDM), a target tracker (TT) with its mirror tracker (MT), and a discriminant module (DM). For an input video, the TT is fed the full-frequency video, whereas the MT takes as input the low-frequency video that is decomposed by the FDM. The DM discriminates the input video as adversarial or natural by comparing the racking performance differences between the two trackers. The whole detection process is performed along with the tracking phase, and all the modules in the framework require no training on adversarial examples. Extensive experiments demonstrate that our adversarial detection framework can effectively detect mainstream adversarial attacks in the tracking field. It can also be flexibly integrated with many trackers, including anchor-based and anchor-free trackers. More importantly, the trackers integrated with the detection framework can still maintain near-original tracking performance.},
  archive      = {J_TMM},
  author       = {Chaobo Li and Hongjun Li and Guoan Zhang},
  doi          = {10.1109/TMM.2025.3535376},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4597-4612},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Detecting adversarial attacks based on tracking differences in frequency bands},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-language relational transformer for video-to-text generation. <em>TMM</em>, <em>27</em>, 4584-4596. (<a href='https://doi.org/10.1109/TMM.2025.3535394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-to-text generation is a challenging task that involves translating video contents into accurate and expressive sentences. Existing methods often ignore the importance of establishing fine-grained semantics within visual representations and exploring textual knowledge implied by video contents, leading to difficulty in generating satisfactory sentences. To address these problems, a vision-language relational transformer model is proposed for video-to-text generation. Three key novel aspects are investigated. First, a visual relation modeling block is designed to obtain higher-order feature representations and establish semantic relationships between regional and global features. Second, a knowledge attention block is developed to explore hierarchical textual information and capture cross-modal dependencies. Third, a video-centric conversation system is constructed to complete multi-round dialogues by incorporating the proposed modules including visual relation modeling, knowledge attention and text generation. Extensive experiments on five benchmark datasets including MSVD, MSRVTT, ActivityNet, Charades and EMVPC demonstrate that the proposed scheme achieves remarkable performance compared with the state-of-the-art methods. Besides, the qualitative experiment reveals the system's favorable conversation capability and provides a valuable exemplar for future video understanding works.},
  archive      = {J_TMM},
  author       = {Tengpeng Li and Hanli Wang and Qinyu Li and Zhangkai Ni},
  doi          = {10.1109/TMM.2025.3535394},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4584-4596},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Vision-language relational transformer for video-to-text generation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view clustering via multi-stage fusion. <em>TMM</em>, <em>27</em>, 4571-4583. (<a href='https://doi.org/10.1109/TMM.2025.3535360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) exploits the information captured from diverse views to partition data into different groups and attracts much attention recently. Despite significant progress, most MVC methods fuse multi-view information via one-stage fusion while neglecting the merits of multi-stage fusion which causes insufficient in utilizing rich information within data and therefore degrades the clustering performance. To this end, designing a functional framework that can fully exploit multi-view information becomes a key challenge in multi-view clustering research. In this paper, we propose a novel multi-stage fusion method, which elegantly unifies the late and early fusion into one unified framework, to capture sufficient information underlying the multi-view data and to effectively reduce the effect of low-quality views. Specifically, we construct a low dimensional latent representation from multi-view data by learning proper correlation among multi-view data in the early fusion stage. The late fusion establishes a new optimal combinational data partition from base partitions constructed by spectral clustering, which suppresses the influence of low-quality basic partitions. Then we couple the low dimensional latent representation with the learned combinational data partition to share the same cluster structure by $k$-means and maximization alignment. As a result, we collaboratively learn an accurate and robust partition representation for the following clustering task. Besides, the late fusion and early fusion are jointly learned to achieve mutual collaboration for better performance. Finally, an alternating optimization algorithm is designed to solve the resultant optimization problem. Extensive experiments conducted on eight datasets show the superiority of our method in terms of effectiveness and efficiency.},
  archive      = {J_TMM},
  author       = {Yu Gan and Yunning You and Junjie Huang and Sen Xiang and Chang Tang and Wei Hu and Shan An},
  doi          = {10.1109/TMM.2025.3535360},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4571-4583},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-view clustering via multi-stage fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking the role of panchromatic images in pan-sharpening. <em>TMM</em>, <em>27</em>, 4558-4570. (<a href='https://doi.org/10.1109/TMM.2025.3535309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent pan-sharpening methods have predominantly utilized techniques tailored for natural image scenes, often overlooking the unique features arising from non-overlapping spectral responses. In light of this, we have reevaluated the utility of panchromatic (PAN) images and introduced a theory anchored in the spectral response of satellite sensors. This posits that a PAN image is effectively a linear weighted summation of individual bands from its corresponding multi-spectral (MS) image, offset by an error map. We developed a deep unmixing network termed “DUN” that integrates an unmixing network, a fusion mechanism, and a distinctive mutual information contrastive loss function. Notably, the unmixing network is adept at decomposing a PAN image into its MS counterpart and error map. Further, the demixed image alongside the low-resolution MS image is channeled into the fusion network for pan-sharpening. Recognizing the challenges of achieving robust supervised learning directly from the unmixing phase, we have innovated a mutual information contrastive learning loss function, ensuring enhanced separation and minimizing overlap during the unmixing process. Preliminary experiments underscore both the quantitative and qualitative prowess of the proposed method.},
  archive      = {J_TMM},
  author       = {Jiaming Wang and Xitong Chen and Xiao Huang and Ruiqian Zhang and Yu Wang and Tao Lu},
  doi          = {10.1109/TMM.2025.3535309},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4558-4570},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rethinking the role of panchromatic images in pan-sharpening},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rectangling for stitched image via pixel-wise deformation learning. <em>TMM</em>, <em>27</em>, 4544-4557. (<a href='https://doi.org/10.1109/TMM.2025.3535285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image rectangling involves filling in the blanks created during image stitching through deformation techniques. However, existing methods still struggle with incomplete filling and distortion of content, ultimately affecting the overall visual impression and potentially hindering subsequent tasks such as recognition. In this work, we design a pixel-wise deformation framework that utilizes explicit edge guidance to maintain consistency of texture and structure, yielding rectangular images with natural structure. Specifically, we decouple motion into region-level and pixel-level components through uniform mesh warping and pixel-wise deformation to precisely rearrange the spatial distribution of all pixels. Uniform deformation preserves local structure within divided patches, while pixel-wise motion coordinates the consistency between patches. Their combination provides robust and accurate pixel-wise offsets for structure-preserved rectangling. To further bolster the consistency of structure and texture, we leverage edge information to establish structural constraints and design an edge-guided enhancement module to aid in restoring fine texture details. Additionally, stitched images encompass both meaningful content and blank spaces, we innovatively incorporate a mask predictor, which acts as a guiding beacon, directing the network's attention solely towards content-rich regions to facilitate precise pixel-wise motion estimation. Experimental results demonstrate that our approach achieves state-of-the-art performance in rectifying irregular boundaries while contributing to downstream visual perception tasks.},
  archive      = {J_TMM},
  author       = {Xiaomei Feng and Qi Jia and Yu Liu and Weimin Wang and Yuqing Liu and Xinwei Xue},
  doi          = {10.1109/TMM.2025.3535285},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4544-4557},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rectangling for stitched image via pixel-wise deformation learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust one-stop multi-modality image registration-fusion-segmentation framework against misalignments and adversarial attacks. <em>TMM</em>, <em>27</em>, 4531-4543. (<a href='https://doi.org/10.1109/TMM.2025.3535291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex open scenes, multi-modality image fusion and segmentation encounter two challenges: i) Imaging misalignments, manifested as pixel shifts and structural distortions, are perceptible. ii) Human-crafted adversarial attacks, reflected in pixel distribution variations, are imperceptible. They not only degrade the visual quality of fused images, e.g., noticeable edge ghosts but more critically undermine semantic perception. However, none of the existing works considered the coupled effect of these degradations. This paper proposes a One-Stop framework incorporating sequential task flows of “Registration-Fusion-Segmentation”, termed OS-RFS. Registration aims to mitigate the chained impact of misalignment on fusion and segmentation. We follow a coarse-to-fine registration paradigm and develop a Global-Local Incremental Registration (GLoIR) model, where the global shift registration (GSR) is performed initially for long-range pixel shifts, followed by incremental local deformation registration (LDR) for subtle local deformations. To improve segmentation robustness, we innovatively introduce auxiliary positive attacks and build a Cancellation Defense Strategy (CDS) in the fusion model. The CDS constrains the fusion model to fit fused images to the distribution of positive attacks, endowing fused images with a robust defense ability against adversarial attacks. This significantly mitigates the impact of adversarial attacks on semantic segmentation. Extensive experimental results reveal that our OS-RFS performs remarkable robustness on multi-modality image fusion and semantic segmentation against imaging misalignments and adversarial attacks.},
  archive      = {J_TMM},
  author       = {Di Wang and Xianghao Jiao and Jinyuan Liu and Xin Fan},
  doi          = {10.1109/TMM.2025.3535291},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4531-4543},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust one-stop multi-modality image registration-fusion-segmentation framework against misalignments and adversarial attacks},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eliminating moiré patterns across diverse image resolutions via DMMNet. <em>TMM</em>, <em>27</em>, 4520-4530. (<a href='https://doi.org/10.1109/TMM.2025.3535324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The occurrence of frequency aliasing between the camera and high-frequency scene elements causes moiré patterns in images, leading to color distortions and a loss of fine details, thereby reducing image quality. The intricate frequency characteristics and diverse appearances inherent in moiré patterns render their removal, commonly referred to as demoiréing, particularly challenging. Recent advancements in deep learning-based demoiréing methods have showcased notable efficacy. However, prevailing techniques often specialize in mitigating moiré patterns exclusively within either the frequency or spatial domains. Additionally, these methods generally perform well at specific image resolutions, but struggle to maintain effectiveness across different resolutions due to less generalized architectures. To address these issues, we propose a Dual-domain Multi-level Multi-scale Network DMMNet, working in both spatial and frequency domains sequentially. The Multi-scale Multi-level Demoire Stage (MMDS) in our framework focuses on moiré patterns removal in the spatial domain. To adeptly integrate features from various semantic levels, we introduce a pioneering plug-and-play Adjacent Cross Attention (ACA) module within the MMDS. Subsequently, the Frequency Separation and Reconstruction Stage (FSRS) restores high-frequency texture details, reconstructs color information, and eliminates residual moiré patterns in the wavelet frequency domain. Ultimately, the clean image is obtained by converting it back to the spatial domain. Extensive experimental assessments, spanning both quantitative metrics and qualitative visual evaluations, attest to the superior efficacy of DMMNet to State-Of-The-Art (SOTA) demoiréing methods, concurrently exhibiting enhanced generalization for demoiréing across diverse image resolutions. We posit that the proposed methodology presents a viable solution for broader applications in the realm of demoiréing. Code will be available on https://github.com/Mr-Ma-yikun/DMMNet.},
  archive      = {J_TMM},
  author       = {Yikun Ma and Haoran Qi and Zhi Jin},
  doi          = {10.1109/TMM.2025.3535324},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4520-4530},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Eliminating moiré patterns across diverse image resolutions via DMMNet},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive PUPM-based HEVC video steganography balancing embedding performance and security. <em>TMM</em>, <em>27</em>, 4508-4519. (<a href='https://doi.org/10.1109/TMM.2025.3535283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the prediction unit partition modes (PUPM)-based steganography, a mainstream branch of high efficiency video coding (HEVC) video steganography, striking a balance between embedding performance and security is very challenging. Including the $2\mathcal {N} \times 2\mathcal {N}$ PUPMs having the maximum number of PUPMs into data embedding is indeed an effective way of enlarging the embedding capacity, but it necessarily causes a significant decline in security. Therefore, a multi-factor-involved cost function (MFICF) is proposed in this paper to evaluate the embedding cost for modifying each PUPM by comprehensively considering four different aspects affecting the embedding performance and security. With the assistance of MFICF, the 7-ary notational system is combined to use all the 7 types of PUPMs containing $2\mathcal {N} \times 2\mathcal {N}$ for data embedding, thus enlarging the embedding capacity as well as enhancing the embedding efficiency. The syndrome-trellis code driven by MFICF, named CFSTC, is designed to preferentially select PUPMs with low embedding costs for data embedding, so that the embedding efficiency is largely enhanced. The security is effectively guaranteed by allocating a large embedding cost for modifying $2\mathcal {N} \times 2\mathcal {N}$ to another type of PUPM. Finally, a lightweight convolutional neural network in combination with gated channel transformation, called GSCNet, is proposed to replace the in-loop filter in HEVC, further optimizing the visual distortion and bitrate increase caused by data embedding. Combining these components above, we design a PUPM-based steganography algorithm, GSAPM. Experimental results show that GSAPM effectively enhances the embedding performance while maintaining high security.},
  archive      = {J_TMM},
  author       = {Lifang Yu and Zichao Yu and Shaowei Weng and Dewang Chen},
  doi          = {10.1109/TMM.2025.3535283},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4508-4519},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive PUPM-based HEVC video steganography balancing embedding performance and security},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot image harmonization with generative model prior. <em>TMM</em>, <em>27</em>, 4494-4507. (<a href='https://doi.org/10.1109/TMM.2025.3535343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a zero-shot approach to image harmonization, aiming to overcome the reliance on large amounts of synthetic composite images in existing methods. These methods, while showing promising results, involve significant training expenses and often struggle with generalization to unseen images. To this end, we introduce a fully modularized framework inspired by human behavior. Leveraging the reasoning capabilities of recent foundation models in language and vision, our approach comprises three main stages. Initially, we employ a pretrained vision-language model (VLM) to generate descriptions for the composite image. Subsequently, these descriptions guide the foreground harmonization direction of a text-to-image generative model (T2I). We refine text embeddings for enhanced representation of imaging conditions and employ self-attention and edge maps for structure preservation. Following each harmonization iteration, an evaluator determines whether to conclude or modify the harmonization direction. The resulting framework, mirroring human behavior, achieves harmonious results without the need for extensive training. We present compelling visual results across diverse scenes and objects, along with quantitative comparisons validating the effectiveness of our approach.},
  archive      = {J_TMM},
  author       = {Jianqi Chen and Yilan Zhang and Zhengxia Zou and Keyan Chen and Zhenwei Shi},
  doi          = {10.1109/TMM.2025.3535343},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4494-4507},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Zero-shot image harmonization with generative model prior},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strumming in the metaverse: A deep-learning-enabled virtual air guitar system in VR with enhanced chord recognition and simulated pedal effects. <em>TMM</em>, <em>27</em>, 4480-4493. (<a href='https://doi.org/10.1109/TMM.2025.3535282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) is increasingly capable and inexpensive, and VR devices have become indispensable in many domains, such as gaming, videoconferencing, education, and healthcare. VR has also been applied to music performance and learning. Virtual instruments, such as virtual pianos and drums, free users from the need to own physical forms of these (often bulky and expensive) instruments. VR devices enable users to enjoy music anytime and anywhere without constraints. Virtual concerts, including spatial audio simulations and reconstructions of historical performances, are becoming increasingly common. Previous studies have primarily examined virtual guitars in non-VR environments and air guitar chord recognition. However, systematic research on virtual air guitar systems in VR remains scarce. Virtual guitar games that are available on the market cannot recognize hand gestures accurately and thus cannot accurately identify the strumming patterns and chords played by the player. To overcome this problem, we propose a VR-based virtual air guitar system that can recognize 30 chords and various strumming techniques through deep learning and visual feedback. Employing a black-box approach, we combine WaveNet and FiLM to simulate electric guitar pedal effects with a knob difference loss mechanism, which simulates the turning of knobs on a guitar effects pedal, for enhanced accuracy.},
  archive      = {J_TMM},
  author       = {Yi-Zeng Hsieh and Ji-Jie Lin and Mu-Chun Su and Wei-Jen Lin},
  doi          = {10.1109/TMM.2025.3535282},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4480-4493},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Strumming in the metaverse: A deep-learning-enabled virtual air guitar system in VR with enhanced chord recognition and simulated pedal effects},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AplusN: Progressively integrating attention and normalization in wavelet domain for pose transfer. <em>TMM</em>, <em>27</em>, 4467-4479. (<a href='https://doi.org/10.1109/TMM.2025.3535296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose-guided person image generation aims to synthesize images of human in various poses, often encountering issues such as occlusions and texture transfers. Previous methods have utilized attention mechanisms, flow field, normalization techniques, and diffusion model. Among them, flow field and attention are the two most commonly used methods. Flow fields are good at preserving detailed textures, while attention is better at generating reasonable semantic structures. Previous networks often used only one of the two and failed to make full use of their advantages. At the same time, the flow field and attention also showed complementary functions in the frequency domain. The flow field was good at preserving the high frequency information of the image with the detailed texture, while the semantic structure of attention was good at generating the image with the low frequency information, and few networks used this to improve the generation effect. Based on these facts, this paper introduces the AplusN network, which innovatively addresses the image generation problem by processing from low to high frequencies. For low-frequency information, a conditional large-kernel convolutional attention mechanism (CLA) is employed to capture the global information of the human body. High-frequency information is refined using a spatial-channel normalization module (SCN) to enhance the body's detailed textures. Additionally, we propose a wavelet loss function to align the frequency domain information of the generated images with the target images. Both qualitative and quantitative experiments demonstrate the superiority of our method over state-of-the-art (SOTA) methods, yielding better-defined overall body contours, local details, and higher-quality image generation.},
  archive      = {J_TMM},
  author       = {Wei Yu and Rui Wang and Weizhi Yang and Wenjian Hu and Wei Xiang},
  doi          = {10.1109/TMM.2025.3535296},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4467-4479},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AplusN: Progressively integrating attention and normalization in wavelet domain for pose transfer},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical cross-attention network for virtual try-on. <em>TMM</em>, <em>27</em>, 4454-4466. (<a href='https://doi.org/10.1109/TMM.2025.3548437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present an innovative solution tailored for the intricate challenges of the virtual try-on task—our novel Hierarchical Cross-Attention Network, HCANet. HCANet is meticulously crafted with two primary stages: geometric matching and try-on, each playing a crucial role in delivering realistic and visually convincing virtual try-on outcomes. A distinctive feature of HCANet is the incorporation of a novel Hierarchical Cross-Attention (HCA) block into both stages, enabling the effective capture of long-range correlations between individual and clothing modalities. The HCA block functions as a cornerstone, enhancing the depth and robustness of the network. By adopting a hierarchical approach, it facilitates a nuanced representation of the interaction between the person and clothing, capturing intricate details essential for an authentic virtual try-on experience. Our extensive set of experiments establishes the prowess of HCANet. The results showcase its cutting-edge performance across both objective quantitative metrics and subjective evaluations of visual realism. HCANet stands out as a state-of-the-art solution, demonstrating its capability to generate virtual try-on results that not only excel in accuracy but also satisfy subjective criteria of realism. This marks a significant step forward in advancing the field of virtual try-on technologies.},
  archive      = {J_TMM},
  author       = {Hao Tang and Bin Ren and Pingping Wu and Nicu Sebe},
  doi          = {10.1109/TMM.2025.3548437},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4454-4466},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical cross-attention network for virtual try-on},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WV-LUT: Wide vision lookup tables for real-time low-light image enhancement. <em>TMM</em>, <em>27</em>, 4441-4453. (<a href='https://doi.org/10.1109/TMM.2025.3535342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the lookup tables (LUTs) with deep learning for image enhancement have achieved remarkable results with extremely high inference efficiency. However, when dealing with severely degraded low-light images, lookup-table-based methods tend to exhibit poor enhancement results due to the lack of contextual and global information. To address the limitations of current lookup-table-based methods in the low-light image enhancement task, we propose the novel Wide Vision Lookup Tables (WV-LUT) by introducing Complementary-Hierarchical 4D-LUTs into 3D-LUT, which allows 3D-LUT to have a wider range of vision. Specifically, the 4D-LUTs are used to expand the receptive field and process local information on a single channel, while a 3D-LUT is used for sRGB channel post-processing. Additionally, we propose a lightweight Global Adjustment Module that further enhances the performance and generalization of WV-LUT by obtaining global adjustment parameters for gamma and color correction matrix to adaptively process images. Experimental results demonstrate that our method outperforms other state-of-the-art methods in low-light image enhancement with the highest average ranking and superior inference efficiency. Furthermore, deployment experiments on mobile devices demonstrate that our WV-LUT achieves superior results and inference efficiency, showcasing promising application prospects for edge devices.},
  archive      = {J_TMM},
  author       = {Canlin Li and Haowen Su and Xin Tan and Xiangfei Zhang and Lizhuang Ma},
  doi          = {10.1109/TMM.2025.3535342},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4441-4453},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WV-LUT: Wide vision lookup tables for real-time low-light image enhancement},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCPTalk: Speech-driven 3D face animation with personalized facial dynamic coupling properties. <em>TMM</em>, <em>27</em>, 4427-4440. (<a href='https://doi.org/10.1109/TMM.2025.3535399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech-driven 3D facial animation has emerged as a hot topic. During this process, movements in different facial regions are interdependent, influenced by the intricate interactions among facial muscles, and manifest personalized differences. The existing methods typically simplify the facial animation generation task to an infinitely thin surface skin deformation without an underlying structure, thereby ignoring the intricate and personalized dynamics of facial muscle activity. These methods tend to produce static or weak upper-face animations with an average facial movement style. In this work, we propose a novel framework, called DCPTalk, to mimic the intricate dynamics of facial muscle activity and portray personalized facial animations. Based on facial dynamic coupling properties, we propose Mouth2Face to simulate the facial muscle control system, yielding realistic and coordinated facial animations evoked by mouth movements. Mouth movements are easily synthesized from speech signals due to their direct correlation with phonetic articulation and vocal tract dynamics. To further enhance the detail of facial movements, we employ surface skin deformation to refine the facial animation derived from Mouth2Face. Furthermore, personal factors, including inherent physical traits and acquired speaking styles, directly determine the uniqueness and realism of facial animations. Inherent physical traits are embedded into Mouth2Face for constructing personalized facial muscle control system, while acquired speaking styles are employed to modulate external driving signals. Extensive qualitative and quantitative experiments as well as a user study indicate that DCPTalk outperforms the existing state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Zhaojie Chu and Kailing Guo and Xiaofen Xing and Pengsheng Liu and Bolun Cai and Xiangmin Xu},
  doi          = {10.1109/TMM.2025.3535399},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4427-4440},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DCPTalk: Speech-driven 3D face animation with personalized facial dynamic coupling properties},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural volumetric video coding with hierarchical coded representation of dynamic volume. <em>TMM</em>, <em>27</em>, 4412-4426. (<a href='https://doi.org/10.1109/TMM.2025.3544415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel multi-view (MV) video coding technique that leverages a four-dimensional (4D) voxel-grid representation to enhance coding efficiency, particularly in novel view synthesis. Although the voxel grid approximation provides a continuous representation for dynamic scenes, its volumetric nature requires substantial storage. The compression of MV videos can be interpreted as the compression of dense features. However, the substantial size of these features poses a significant problem relative to the generation of dynamic scenes at arbitrary viewpoints. To address this challenge, this study introduces a hierarchical coded representation of dynamic volumes based on low-rank tensor decomposition of volumetric features and develops effective coding techniques based on this representation. The proposed method employs a two-level coding strategy to capture the temporal characteristics of the decomposed features. At a higher level, spatial features are encoded, representing 3D structural information, with time-invariant components over short intervals of an MV video sequence. At a lower level, temporal features are encoded to capture the dynamics of current scenes. The spatial features are shared in a group, and temporal features are encoded at each time step. The experimental results demonstrate that the proposed technique outperforms existing MV video coding standards and current state-of-the-art methods, providing superior rate-distortion performance in the novel view synthesis of MV video compression.},
  archive      = {J_TMM},
  author       = {Ju-Yeon Shin and Jung-Kyung Lee and Gun Bang and Jun-Sik Kim and Je-Won Kang},
  doi          = {10.1109/TMM.2025.3544415},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4412-4426},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Neural volumetric video coding with hierarchical coded representation of dynamic volume},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deformable cross-attention transformer for weakly aligned RGB–T pedestrian detection. <em>TMM</em>, <em>27</em>, 4400-4411. (<a href='https://doi.org/10.1109/TMM.2025.3543056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection plays a crucial role in autonomous driving systems. To ensure reliable and effective detection in challenging conditions, researchers have proposed RGB–T (RGB–thermal) detectors that integrate thermal images with color images for more complementary feature representations. However, existing methods face challenges in capturing the spatial and geometric correlations between different modalities, as well as in assuming perfect synchronization of the two modalities, which is unrealistic in real-world scenarios. In response to these challenges, we present a new deformable-attention-based approach for weakly aligned RGB–T pedestrian detection. The proposed method uses a dual-branch cross-attention mechanism to capture the inherent spatial and geometric correlations between color and thermal images. Furthermore, it incorporates positional information for each image pixel into the sampling offset generation to enhance robustness in scenarios where modalities are not precisely aligned or registered. To reduce computational complexity, we introduce a local attention mechanism that samples only a small set of keys and values within a limited region in the feature maps for each query. Extensive experiments and ablation studies conducted on multiple public datasets confirm the effectiveness of the proposed framework.},
  archive      = {J_TMM},
  author       = {Yu Hu and Xiaobo Chen and Sheng Wang and Luyang Liu and Hengyang Shi and Lihong Fan and Jing Tian and Jun Liang},
  doi          = {10.1109/TMM.2025.3543056},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4400-4411},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deformable cross-attention transformer for weakly aligned RGB–T pedestrian detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end deep video compression based on hierarchical temporal context learning. <em>TMM</em>, <em>27</em>, 4386-4399. (<a href='https://doi.org/10.1109/TMM.2025.3535367'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging learning-based video compression suffers from error propagation in long group of pictures (GOP), yielding limited coding performance. To address this problem, a novel end-to-end Deep Video Compression method based on Hierarchical Temporal Context Learning (DVCH) is proposed in this paper. DVCH aims to fully exploit temporal contexts and suppress error propagation for better coding performance. It first divides video frames into several hierarchies with different compression qualities. The frames in lower hierarchies have high compression quality, and serve as reference frames. To mine high-quality reference information, we propose a Hierarchical Temporal Context Learning (HTCL) network as the fundamental module of our DVCH. The informative temporal context features from hierarchical prediction structure can be extracted by the network. Motion vectors (MVs) between the to-be-coded frame and its reference frames are estimated by the MV Learning module and used to align the extracted contexts. The contexts are fed into Context Coding module to generate the prediction of the decoded frame. Moreover, a multi-stage training strategy is developed to solve the imbalanced training challenge. Experimental results demonstrate that the proposed DVCH exceeds x264 and other end-to-end video compression methods, regardless of objective, subjective, error propagation suppression, GOP sizes, and sequence length evaluations. As much as 49.27% bitrate savings and 2.52 dB PSNR gains can be achieved in large GOP.},
  archive      = {J_TMM},
  author       = {Kejun Wu and Zhenxing Li and You Yang and Qiong Liu and Xiao-Ping Zhang},
  doi          = {10.1109/TMM.2025.3535367},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4386-4399},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {End-to-end deep video compression based on hierarchical temporal context learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive feedback vision-language for 3D skeleton-based action recognition. <em>TMM</em>, <em>27</em>, 4372-4385. (<a href='https://doi.org/10.1109/TMM.2025.3535393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large contrastive vision-language models (VLMs) have recently shown promise in skeleton-based action recognition. However, given the lack of skeleton frame-text training datasets for VLMs, aligning the representations between the skeleton frames and labels remains challenging. Specifically, two key limitations must be addressed. First, VLMs struggle to align abstract action labels' language representations with sequential skeleton frames containing primary action semantics, impeding the ability of language representations to represent primary action information effectively. Second, vision representations with high-order action information are difficult to align with labels' language representations because of the risk of homogenizing discriminative features from different data streams. To address these challenges, we propose a Contrastive Feedback Vision-Language (CFVL) model for 3D skeleton-based action recognition that consists of a language representations' feedback decoder and a data stream-adaptive projection module. The feedback decoder aligns the decoded language representations with the original skeleton inputs to help the model comprehend primary action vision information. The projection module employs adaptive structures to further extract spatiotemporal information from various data streams. Additionally, the data stream-adaptive projection module projects vision and text language representations into a unified high-latency semantic space. Discriminative action vision representations, along with consistent representation spaces, support the effective alignment of vision-language representations with high-order action information. The experimental results demonstrate the superior performance of the proposed CFVL model on the Northwestern-UCLA, PKU MMD, NTU RGB+D 60/120, and FSD-10 datasets.},
  archive      = {J_TMM},
  author       = {Qinyang Zeng and Ronghao Dang and Xun Zhou and Chengju Liu and Qijun Chen},
  doi          = {10.1109/TMM.2025.3535393},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4372-4385},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Contrastive feedback vision-language for 3D skeleton-based action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identity and modality attributes driven multimodal fusion networks for emotion recognition in conversations. <em>TMM</em>, <em>27</em>, 4361-4371. (<a href='https://doi.org/10.1109/TMM.2025.3535347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in conversations (ERC) is a crucial aspect of human-computer interaction and plays an important role in various domains, including healthcare, entertainment, and education. Since the conversation data in the form of multimodal sequences is well suited to be constructed into graphs, the methods based on graph convolutional network (GCN) show incomparable advantages. However, existing methods attempt to model the highly uncertain emotional relationships between different speakers, which is not an easy task and may even introduce interference information. Therefore, we propose an identity and modality attributes driven multimodality fusion network (dubbed IMDNet) for emotion recognition in conversations. Specifically, we construct a speaker-centric graph that only connects nodes of the same speaker within modalities to each other, reducing the interference between the emotions of different speakers. We also introduce the attribute embedding mechanism, which facilitates the correct calculation of correlations between nodes for better multimodal feature fusion. Considering that the emotional correlation between utterances will decrease over time, we present an utterance distance attention to make the fusion network pay more attention to the adjacent utterances. Furthermore, we explore the solution to the data imbalance problem suitable for conversation scenarios. Given the presence of possible anomalous samples in the dataset, we opt for the BoundaryFocalLoss. Experiments on the IEMOCAP and MELD datasets show that our IMDNet outperforms the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Wuzhen Shi and Xuping Chen and Biyun Yao and Yang Wen and Bin Sheng},
  doi          = {10.1109/TMM.2025.3535347},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4361-4371},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Identity and modality attributes driven multimodal fusion networks for emotion recognition in conversations},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Replay-based incremental object detection with local response exploration. <em>TMM</em>, <em>27</em>, 4348-4360. (<a href='https://doi.org/10.1109/TMM.2025.3535403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental object detection (IOD) aims to train an object detector on non-stationary data streams without forgetting previous knowledge. Prevalent replay-based methods keep a buffer composed of carefully selected instances towards this goal. However, due to the limited storage space and uniform feature distribution, existing methods are prone to overfit on replayed instances, leading to poor generalization on diverse test data. Additionally, the imbalance in data quantity makes the detector fail to distinguish old and new classes that are visually similar, introducing bias toward new classes. To enhance the diversity of stored instances and eliminate bias, we propose a Local Response Exploration (LRE) framework, which comprises three modules. First, Region-Entropy Instance Selector (REIS) introduces a novel metric to assess instance diversity based on the entropy of local responses. Second, Confusion-Guided Instance Replay (CGIR) replaces the previous random replay approach by replaying specific old class instances based on class similarity, ensuring that parameters for similar new and old classes are updated together, thereby mitigating bias and helping mining discriminative patterns. Third, Confusion-Aware Region Segregation (CARS) adaptively differentiates biased regions from other regions based on local responses, reducing bias toward new classes while preserving relationships between new and old classes. Extensive evaluations on Pascal-VOC and MS COCO datasets demonstrate that our approach outperforms State-of-the-Art methods in incremental object detection.},
  archive      = {J_TMM},
  author       = {Jian Zhong and Yifan Jiao and Bing-Kun Bao},
  doi          = {10.1109/TMM.2025.3535403},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4348-4360},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Replay-based incremental object detection with local response exploration},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image compressive sensing with scale-variable adaptive sampling and hybrid-attention transformer reconstruction. <em>TMM</em>, <em>27</em>, 4333-4347. (<a href='https://doi.org/10.1109/TMM.2025.3535114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a large number of image compressive sensing (CS) methods with deep unfolding networks (DUNs) have been proposed. However, existing methods either use fixed-scale blocks for sampling that leads to limited insights into the image content or employ a plain convolutional neural network (CNN) in each iteration that weakens the perception of broader contextual prior. In this paper, we propose a novel DUN (dubbed SVASNet) for image compressive sensing, which achieves scale-variable adaptive sampling and hybrid-attention Transformer reconstruction with a single model. Specifically, for scale-variable sampling, a sampling matrix-based calculator is first employed to evaluate the reconstruction distortion, which only requires measurements without access to the ground truth image. Then, a Block Scale Aggregation (BSA) strategy is presented to compute the reconstruction distortion under block divisions at different scales and select the optimal division scale for sampling. To realize hybrid-attention reconstruction, a dual Cross Attention (CA) submodule in the gradient descent step and a Spatial Attention (SA) submodule in the proximal mapping step are developed. The CA submodule introduces inter-phase inertial forces in the gradient descent, which improves the memory effect between adjacent iterations. The SA submodule integrates local and global prior representations of CNN and Transformer, and explores local and global affinities between dense feature representations. Extensive experimental results show that the proposed SVASNet achieves significant improvements over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Chen Hui and Debin Zhao and Weisi Lin and Shaohui Liu and Feng Jiang},
  doi          = {10.1109/TMM.2025.3535114},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4333-4347},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image compressive sensing with scale-variable adaptive sampling and hybrid-attention transformer reconstruction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S-HR-VQVAE: Sequential hierarchical residual learning vector quantized variational autoencoder for video prediction. <em>TMM</em>, <em>27</em>, 4321-4332. (<a href='https://doi.org/10.1109/TMM.2025.3535370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the video prediction task by putting forth a novel model that combines (i) a novel hierarchical residual learning vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel autoregressive spatiotemporal predictive model (AST-PM). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the AST-PM's ability to handle spatiotemporal information, S-HR-VQVAE can better deal with major challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on four challenging tasks, namely KTH Human Action, TrafficBJ, Human3.6 M, and Kitti, demonstrate that our model compares favorably against state-of-the-art video prediction techniques both in quantitative and qualitative evaluations despite a much smaller model size. Finally, we boost S-HR-VQVAE by proposing a novel training method to jointly estimate the HR-VQVAE and AST-PM parameters.},
  archive      = {J_TMM},
  author       = {Mohammad Adiban and Kalin Stefanov and Sabato Marco Siniscalchi and Giampiero Salvi},
  doi          = {10.1109/TMM.2025.3535370},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4321-4332},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {S-HR-VQVAE: Sequential hierarchical residual learning vector quantized variational autoencoder for video prediction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive confidence multi-view learning. <em>TMM</em>, <em>27</em>, 4309-4320. (<a href='https://doi.org/10.1109/TMM.2025.3535391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view hashing is a crucial technology for multimedia retrieval because it transforms heterogeneous data from many viewpoints into binary hash codes. However, the existing approaches focus mostly on the complementarity among multiple views while being without confidence fusion. Furthermore, redundant noise is present in the single-view data in real-world application contexts. We present an innovative Adaptive Confidence Multi-View Learning (ACMVL) method to perform confidence fusion and remove extraneous noise. Initially, a confidence network is constructed to eliminate noise data and extract useful information from various single-view features. Moreover, an adaptive confidence multi-view network is utilized to quantify the confidence of each view and further fuse multiple view features using a weighted summation. Here, we propose an Automatic View Confidence Metric (AVCM) as a score for evaluating the confidence of views. Finally, to improve the semantic representation of the fused feature, a dilation network is created. Based on ACMVL, we introduce a novel Adaptive Confidence Multi-View Hashing (ACMVH) method. To our knowledge, we are the pioneers in using confidence learning for multimedia retrieval. Comprehensive experiments on three publicly available datasets demonstrate that our ACMVH outperforms the state-of-the-art methods (maximum improvement of 3.24% on mAP).},
  archive      = {J_TMM},
  author       = {Jian Zhu and Lei Liu and Yu Zhang and Chang Tang and Li-Rong Dai},
  doi          = {10.1109/TMM.2025.3535391},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4309-4320},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive confidence multi-view learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compact latent primitive space learning for compositional zero-shot learning. <em>TMM</em>, <em>27</em>, 4297-4308. (<a href='https://doi.org/10.1109/TMM.2025.3535315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional zero-shot learning (CZSL) aims to recognize novel compositions formed by known primitives (attribute and object). The key challenge of CZSL is the visual diversity of the primitive caused by the dependencies of attributes and objects. To solve this problem, most existing methods attempt to mine primitive-invariant features shared in all compositions or learn primitive-variant features specialized for each composition. However, these methods overlook that the primitives have inherent similarities and differences in different compositions, i.e., one primitive may exhibit a common visual appearance under some compositions, but have different expressions in other partial compositions. To sufficiently explore the partial similarity and visual diversity of primitives, we propose a compact latent primitive space learning framework, which explicitly leverages various codewords to encode the primitive features to make a balance between generality and diversity. Specifically, we borrow the idea from discriminative sparse coding to learn these representative codewords to build the latent primitive space. Through the sparse reconstruction loss, contrastive loss and orthogonal constraint, our model can adaptively reconstruct the primitive features according to the similarity weights between the primitive features and codewords. Comprehensive experiments on four benchmarks demonstrate that the proposed method achieves better performance than previous methods.},
  archive      = {J_TMM},
  author       = {Han Jiang and Chaofan Chen and Xiaoshan Yang and Changsheng Xu},
  doi          = {10.1109/TMM.2025.3535315},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4297-4308},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Compact latent primitive space learning for compositional zero-shot learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain generalization study of empirical risk minimization from causal perspectives. <em>TMM</em>, <em>27</em>, 4284-4296. (<a href='https://doi.org/10.1109/TMM.2025.3535276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical risk minimization (ERM) is a celebrated induction principle for developing data-driven models. However, ERM has received both pros and cons for its capability on domain generalization (DG). To this end, this paper attempts to study the success and failure of ERM at supervised DG classification tasks, both theoretically and empirically, with causal perspectives. In the theoretical aspect, we first explore different properties of a causal metric termed information flow, followed with discussing relationships between the information flow and the mutual information in the proposed causal graph. Next, we analyze the roles of the transformed causal feature and the transformed spurious feature on modeling performances. It reveals that the interaction between the spurious influencer and the transformed causal feature is the key determining the failure or success of ERM on DG. In the empirical study, we first simulate various DG settings based on the MNIST, Fashion MNIST, and CIFAR10 datasets. Next, we verify developed theories by testing three different neural network configurations in designed experiments. In addition, experiments based on real-world datasets are conducted to further consolidate key points of the proposed theories. To extend application benefits of the theoretical discoveries, a new risk minimization framework with a novel feature intervention for regulating ERM is proposed. It achieves DG improvements over ERM on real-world datasets of image segmentation, image classification, and text classification.},
  archive      = {J_TMM},
  author       = {Zhenling Mo and Zijun Zhang and Kwok-Leung Tsui},
  doi          = {10.1109/TMM.2025.3535276},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4284-4296},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Domain generalization study of empirical risk minimization from causal perspectives},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLIP-AE: A multi-modal unsupervised images enhancement method based on high-order adaptive curve for visual disbalance defects. <em>TMM</em>, <em>27</em>, 4269-4283. (<a href='https://doi.org/10.1109/TMM.2025.3535333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For visual disbalance defects (VDDs) in low-light images, such as brightness unevenness and color imbalance, existing enhancement methods struggle to extract defect features from local regions and apply adaptive enhancement based on varying degrees of these defects. To address these challenges, we propose an unsupervised multi-modal enhancement method based on a high-order adaptive curve, named CLIP-AE. Specifically, we introduce a multi-modal recurrent optimization approach utilizing contrastive language-image pre-training (CLIP). This method iteratively optimizes variable embedded prompts and an Adaptive Enhancement Module (AEM) to establish dependencies between the prompts and detailed style features in the images, guiding the AEM to perform adaptive image enhancement. Additionally, we implement a progressive feature alignment strategy to enhance the model's ability to perceive style features and improve optimization efficiency by using multiple enhanced images with identical content features and incremental style features. In the AEM, the optimized Hyperparameters Generative Network (HGN) generates the optimal hyperparameters, which drive a High-Dimensional Nested Gamma correction (HDN-Gamma) to perform pixel-wise adaptive enhancement for VDDs. HDN-Gamma further maps pixel values using specific enhancement curves to avoid artifacts. Extensive experiments demonstrate that our method effectively improves visual disbalance defects and reduces artifacts. Compared to seven state-of-the-art algorithms, our method shows significant improvements (PSNR: 16.46%, 16.89%, and 15.14%; SSIM: 9.26%, 8.02%, and 9.85%; MUSIQ: 6.37%, 6.54%, and 7.45%) on the LOL, SICE, and MIT-Adobe FiveK datasets. Our approach offers a novel solution for applying multimedia technology in low-light image enhancement tasks.},
  archive      = {J_TMM},
  author       = {Jiaqi Wu and Shihao Zhang and Mingshuo Hou and Zehua Wang and Wei Chen and Zijian Tian and F. Richard Yu and Victor C. M. Leung},
  doi          = {10.1109/TMM.2025.3535333},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4269-4283},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CLIP-AE: A multi-modal unsupervised images enhancement method based on high-order adaptive curve for visual disbalance defects},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPPNet: A depth pixel-wise potential-aware network for RGB-D salient object detection. <em>TMM</em>, <em>27</em>, 4256-4268. (<a href='https://doi.org/10.1109/TMM.2025.3535386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth cues are essential for visual perception tasks like Salient Object Detection (SOD). Due to varying depth reliability across scenes, some researchers propose evaluating the overall quality of the depth maps and discarding the less reliable ones to avoid contamination. However, these methods often fail to fully utilize valuable information in depth maps, leading to sub-optimal performance particularly when depth quality is unreliable. Since low-quality depth maps still contain useful information that potentially improves model performance, we propose a Depth Pixel-wise Potential-aware Network to leverage these depth cues effectively. This network includes two novel components designed: 1) A learning strategy for explicitly modeling the confidence of each depth pixel to assist the model in locating valid information in the depth map. 2) A cross-modal adaptive multiple fusion module that fuses features from both RGB and depth modalities. It aims to mitigate the contamination effect of unreliable depth maps and fully exploit the benefits of multiple fusion strategies. Experimental results show that on four publicly available datasets, our method outperforms 17 mainstream methods on various evaluation metrics.},
  archive      = {J_TMM},
  author       = {Junbin Yuan and Yiqi Wang and Zhoutao Wang and Qingzhen Xu and Bharadwaj Veeravalli and Xulei Yang},
  doi          = {10.1109/TMM.2025.3535386},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4256-4268},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DPPNet: A depth pixel-wise potential-aware network for RGB-D salient object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video compressed sensing via wavelet residual sampling and dual-domain fusion. <em>TMM</em>, <em>27</em>, 4240-4255. (<a href='https://doi.org/10.1109/TMM.2025.3535326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based compressed sensing (CS) technology attracts widespread attention owing to its remarkable reconstruction with only a few sampling measurements and low computational complexity. However, the existing video compressive sampling approaches cannot fully exploit the inherent interframe and intraframe correlations and sparsity of video sequences. To address this limitation, a novel sampling and reconstruction method for video CS (called WRDD) is proposed, which exploits the advantages of wavelet residual sampling and dual-domain fusion optimization. Specifically, in order to capture high-frequency details and achieve efficient and high-quality measurements, we propose a wavelet residual (WR) sampling strategy for the nonkeyframe sampling, which is achieved by the wavelet residuals between nonkeyframes and keyframes. Furthermore, a dual-domain (DD) fusion strategy is proposed, which fully combine intraframe and interframe to improve the reconstruction quality of nonkeyframes both in the pixel domain and multilevel feature domains. Extensive experiments demonstrate that our WRDD surpasses the state-of-the-art video and image CS methods in both subjective and objective evaluations. Besides, it exhibits outstanding antinoise capability and computational efficiency.},
  archive      = {J_TMM},
  author       = {Zhu Yin and Zhongcheng Wu and Wuzhen Shi and Guyue Hu and Weisi Lin},
  doi          = {10.1109/TMM.2025.3535326},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4240-4255},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video compressed sensing via wavelet residual sampling and dual-domain fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free semantic regularization learning for semi-supervised domain adaptation. <em>TMM</em>, <em>27</em>, 4227-4239. (<a href='https://doi.org/10.1109/TMM.2025.3535364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised domain adaptation (SSDA) has been extensively researched due to its ability to improve classification performance and generalization ability of models by using a small amount of labeled data on the target domain. However, existing methods cannot effectively adapt to the target domain due to difficulty in fully learning rich and complex target semantic information and relationships. In this paper, we propose a novel SSDA learning framework called semantic regularization learning (SERL), which captures the target semantic information from multiple perspectives of regularization learning to achieve adaptive fine-tuning of the source pre-trained model on the target domain. SERL includes three robust semantic regularization techniques. Firstly, semantic probability contrastive regularization (SPCR) helps the model learn more discriminative feature representations from a probabilistic perspective, using semantic information on the target domain to understand the similarities and differences between samples. Additionally, adaptive weights in SPCR can help the model learn the semantic distribution correctly through the probabilities of different samples. To further comprehensively understand the target semantic distribution, we introduce hard-sample mixup regularization (HMR), which uses easy samples as guidance to mine the latent target knowledge contained in hard samples, thereby learning more complete and complex target semantic knowledge. Finally, target prediction regularization (TPR) regularizes the target predictions of the model by maximizing the correlation between the current prediction and the past learned objective, thereby mitigating the misleading of semantic information caused by erroneous pseudo-labels. Extensive experiments on three benchmark datasets demonstrate that our SERL method achieves state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Xinyang Huang and Chuang Zhu and Ruiying Ren and Shengjie Liu and Tiejun Huang},
  doi          = {10.1109/TMM.2025.3535364},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4227-4239},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Source-free semantic regularization learning for semi-supervised domain adaptation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-domain teacher for unsupervised domain adaptation detection. <em>TMM</em>, <em>27</em>, 4217-4226. (<a href='https://doi.org/10.1109/TMM.2025.3535362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation for object detection aims to bridge the domain gap by transferring knowledge from a labeled source domain to an unlabeled target domain, thus improving the performance of detection models. Common strategies focus on aligning the feature distributions between source and target domains to reduce their discrepancies. However, achieving complete alignment is often not feasible in real-world situations due to a lack of annotations in the target domain. Recently, TeacherStudent approaches achieve feature alignment by generating reliable target pseudo-labels and become the dominant solution for addressing this issue. However, due to the domain shift, the teacher model bias to source domain, making it challenging to enhance the quality of target pseudo-labels. Some methods within this framework attempt to overcome the domain shift by incorporating distribution alignment components, yet these approaches also face challenges in achieving perfect alignment between domains. In this paper, we propose the Dual-Domain Teacher (DDT) method to address the domain adaptation detection problem by simultaneously detecting objects in both domains, thereby decreasing the need for perfect alignment. To address the issue of duplicate detection results produced by the Dual-Domain detection process, a candidate set refinement strategy is proposed to eliminate these duplicates across domains. Moreover, when teachers generate pseudo-labels by selecting reliable predictions with fixed confidence thresholds, valuable predictions may be overlooked in mutual learning. In our approach, a minimum variance-based dynamic threshold module is designed to mine valuable pseudo-labels by adaptively adjusting to the optimal threshold. Extensive experiments show that the DDT achieve a 56.7$\%$ mAP on the CityScapes-to-Foggy CityScapes task, marking a 4.8 point improvement over the latest methods. On the PASCAL VOC-to-Clipart1k task, our method reaches 51.2$\%$ mAP, outperforming previous state-of-the-art.},
  archive      = {J_TMM},
  author       = {Fei Wang and Luhui Zhao and Shijie Hong and Zhe Wang and Chen Liu and Changxin Gao and Jinsheng Li and Xin Li and Dapeng Luo},
  doi          = {10.1109/TMM.2025.3535362},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4217-4226},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-domain teacher for unsupervised domain adaptation detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cluster assumption-guided timestamp-supervised temporal action segmentation. <em>TMM</em>, <em>27</em>, 4206-4216. (<a href='https://doi.org/10.1109/TMM.2025.3535331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current timestamp-supervised temporal action segmentation (TS-TAS) methods typically follow a two-phase pipeline: initializing the model with timestamp labels and refining it with pseudo-labels. However, limited by the sparsity of timestamp annotations, current methods' performance is sub-optimal. Specifically, initializing the model with only timestamp annotations may cause overfitting to labeled frames. Additionally, sparse timestamp annotations cannot capture the diverse action representations throughout the whole instance, especially those near the ambiguous action boundaries, leading to pseudo-label noise. Inspired by the cluster assumption of semi-supervised learning (SSL) that points within the same manifold likely share the same label, we here model TS-TAS as an SSL problem. Specifically, we propose a Temporal Embedding Consistency (TEC) strategy to mitigate the excessive focus on annotated frames. The TEC strategy encourages frames with similar representations within the video to have similar classification probability distributions, thereby propagating labeled frames' information to implicit ones. Besides, we design a TS-Mix strategy to further leverage unlabeled data to mitigate the influence of pseudo-label noise in a consistency regularization manner. The TS-Mix strategy includes intra-mix, which adds linear interpolation of two adjacent timestamps to every frame between them, and inter-mix, which mixes frames from two different untrimmed videos frame-by-frame. Then the mixed video is trained with the correspondingly mixed pseudo-labels. Comprehensive experimental results on different benchmarks show that we achieve new state-of-the-art performances. Furthermore, the proposed method can seamlessly enhance existing methods, significantly improving their performances.},
  archive      = {J_TMM},
  author       = {Ziyou Ren and Guozhang Li and Nan Cheng and Anqi Wu and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3535331},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4206-4216},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cluster assumption-guided timestamp-supervised temporal action segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-centered financial signal analysis based on visual patterns in stock charts. <em>TMM</em>, <em>27</em>, 4193-4205. (<a href='https://doi.org/10.1109/TMM.2025.3535278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study adopted a human-centered perspective to research the financial markets, focusing on identifying variations in eye movement patterns between professional and non-professional traders as they analyze a series of stock charts. Eye movement data was selected as the analysis target based on the hypothesis that it represents a behavioral phenotype indicative of stock analysts' cognitive processes during market analysis. Disparities were identified by conducting variance analysis and the Wilcoxon signed-rank test on statistical metrics derived from eye fixations and saccades. Psychological and behavioral economic interpretations were provided to understand the underlying reasons for these observed patterns. To showcase the practical application potential of the human-centered perspective, eye movement data and human visual characteristics were used to construct visual saliency prediction models of professional stock analysts. Leveraging this human-centered model, we developed two practical application demonstrations specifically designed to support and instruct novice traders. Based on the above demonstrations, a training program was designed that demonstrates how, with ongoing training, the non-professional traders' ability to observe stock charts improves progressively.},
  archive      = {J_TMM},
  author       = {Ji-Feng Luo and Yuzhen Chen and Kaixun Zhang and Xudong An and Menghan Hu and Guangtao Zhai and Xiao-Ping Zhang},
  doi          = {10.1109/TMM.2025.3535278},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4193-4205},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Human-centered financial signal analysis based on visual patterns in stock charts},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hand gesture recognition from an open-set perspective. <em>TMM</em>, <em>27</em>, 4181-4192. (<a href='https://doi.org/10.1109/TMM.2025.3535363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing hand gesture recognition methods predominantly rely on a close-set assumption, which in essence limits the viewpoints, gesture categories, and hand shapes at test time to closely resemble those seen during training. This requirement is however rarely met in practice, as images are often captured from unconstrained viewpoints, with novel gestures and unseen hand shapes that can differ significantly from the training data. This motivates us to investigate an open-set hand gesture recognition problem, where hand gestures are still recognizable from unconstrained viewpoints, and novel gesture classes and hand shapes can be incrementally learned with just a few examples. To address this, we propose a viewpoint influence elimination network that extracts view-independent features, significantly improving performance in scenarios with unconstrained viewpoints. Moreover, a joint-weighted classification scheme is introduced to augment the cosine similarity metric for evaluating few-shot incremental learning of novel gestures and shapes. Finally, as existing hand gesture recognition datasets primarily adhere to the close-set assumption, a new hand gesture recognition dataset, OHG, is introduced in this paper, that includes a wide range of viewpoints, diverse gesture classes, and distinct hand shapes. Experimental hand gesture recognition results demonstrate the superior performance of our approach in both unconstrained viewpoint and few-shot incremental learning scenarios.},
  archive      = {J_TMM},
  author       = {Jun Zhou and Chi Xu and Li Cheng},
  doi          = {10.1109/TMM.2025.3535363},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4181-4192},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hand gesture recognition from an open-set perspective},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RAFDet: Range view augmented fusion network for point-based 3D object detection. <em>TMM</em>, <em>27</em>, 4167-4180. (<a href='https://doi.org/10.1109/TMM.2025.3535289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, point-based methods have achieved promising performance on 3D object detection task. Although effective, they still suffer from the inherent sparsity of point cloud, which makes it challenging to distinguish objects with backgrounds only relying on the view of raw point. To this end, we propose a straightforward yet effective multi-view fusion network termed RAFDet to alleviate this issue. The core idea of our method lies in combining the merits of raw point and its range view to enhance the representation learning for sparse point cloud, thus mitigating the sparsity problem and boosting the detection performance. In particular, we introduce a novel bidirectional attentive fusion module to equip sparse point with interacted fine-grained semantic clues during feature learning process. Then, we devise the range-view augmented fusion module to fully exploit the supplementary relationship between different perspectives with the aim of enhancing original point-view features. In the end, a single-stage detection head is utilized to predict final 3D bounding boxes based on the enhanced semantics. We have evaluated our method on the popular KITTI Dataset, DAIR-V2X Dataset and Waymo Open Dataset. Experimental results on the above three datasets demonstrate the effectiveness and robustness of our approach in terms of detection performance and model complexity.},
  archive      = {J_TMM},
  author       = {Zhijie Zheng and Zhicong Huang and Jingwen Zhao and Kang Lin and Haifeng Hu and Dihu Chen},
  doi          = {10.1109/TMM.2025.3535289},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4167-4180},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RAFDet: Range view augmented fusion network for point-based 3D object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution-level memory recall for continual learning: Preserving knowledge and avoiding confusion. <em>TMM</em>, <em>27</em>, 4151-4166. (<a href='https://doi.org/10.1109/TMM.2025.3535341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning (CL) aims to enable deep neural networks (DNNs) to learn new data without forgetting previously learned knowledge. The key to achieving this goal is to avoid confusion at the feature level, i.e., to avoid confusion within old tasks and between new and old tasks. Existing prototype-based CL methods generate pseudo features for old knowledge replay by adding Gaussian noise to the centroids of old classes. However, the distribution in the feature space exhibits anisotropy during the incremental process, which prevents the pseudo features from faithfully reproducing the distribution of old knowledge in the feature space, leading to confusion at the classification boundaries within old tasks. To address this issue, we propose the distribution-level memory recall (DMR) method, which uses a Gaussian mixture model to precisely fit the feature distribution of old knowledge at the distribution level and generate pseudo features in the next stage. Furthermore, resistance to confusion at the distribution level is crucial for multimodal learning. Multimodal imbalance, which refers to uneven optimization processes among encoders of different modalities, results in significant differences in feature responses between modalities; this exacerbates confusion within old tasks in prototype-based CL methods. Therefore, we mitigate the multimodal imbalance problem by using the intermodal guidance and intramodal mining (IGIM) method to guide weaker modalities with prior information from dominant modalities and further explore useful information within modalities. To avoid confusion between new and old tasks, we propose using the confusion index to quantitatively describe a model's ability to distinguish between new and old tasks, and we use the incremental mixup feature enhancement (IMFE) method to enhance pseudo features with new sample features, alleviating classification confusion between new and old knowledge. We conduct extensive experiments on the CIFAR100, ImageNet100, TinyImageNet, ImageNet-1K and UESTC-MMEA-CL datasets and achieve state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Shaoxu Cheng and Kanglei Geng and Chiyuan He and Zihuan Qiu and Linfeng Xu and Heqian Qiu and Lanxiao Wang and Qingbo Wu and Fanman Meng and Hongliang Li},
  doi          = {10.1109/TMM.2025.3535341},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4151-4166},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Distribution-level memory recall for continual learning: Preserving knowledge and avoiding confusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLIP-driven semantic discovery network for visible-infrared person re-identification. <em>TMM</em>, <em>27</em>, 4137-4150. (<a href='https://doi.org/10.1109/TMM.2025.3535353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VIReID) primarily deals with matching identities across person images from different modalities. Due to the modality gap between visible and infrared images, cross-modality identity matching poses significant challenges. Recognizing that high-level semantics of pedestrian appearance, such as gender, shape, and clothing style, remain consistent across modalities, this paper intends to bridge the modality gap by infusing visual features with high-level semantics. Given the capability of Contrastive Language-Image Pre-training (CLIP) to sense high-level semantic information corresponding to visual representations, we explore the application of CLIP within the domain of VIReID. Consequently, we propose a CLIP-Driven Semantic Discovery Network (CSDN) that consists of Modality-specific Prompt Learner, Semantic Information Integration (SII), and High-level Semantic Embedding (HSE). Specifically, considering the diversity stemming from modality discrepancies in language descriptions, we devise bimodal learnable text tokens to capture modality-private semantic information for visible and infrared images, respectively. Additionally, acknowledging the complementary nature of semantic details across different modalities, we integrate text features from the bimodal language descriptions to achieve comprehensive semantics. Finally, we establish a connection between the integrated text features and the visual features across modalities. This process embed rich high-level semantic information into visual representations, thereby promoting the modality invariance of visual representations. The effectiveness and superiority of our proposed CSDN over existing methods have been substantiated through experimental evaluations on multiple widely used benchmarks.},
  archive      = {J_TMM},
  author       = {Xiaoyan Yu and Neng Dong and Liehuang Zhu and Hao Peng and Dapeng Tao},
  doi          = {10.1109/TMM.2025.3535353},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4137-4150},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CLIP-driven semantic discovery network for visible-infrared person re-identification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised LiDAR semantic segmentation via scatter image annotation. <em>TMM</em>, <em>27</em>, 4121-4136. (<a href='https://doi.org/10.1109/TMM.2025.3535350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised LiDAR semantic segmentation has made significant strides with limited labeled data. However, most existing methods focus on the network training under weak supervision, while efficient annotation strategies remain largely unexplored. To tackle this gap, we implement LiDAR semantic segmentation using scatter image annotation, effectively integrating an efficient annotation strategy with network training. Specifically, we propose employing scatter images to annotate LiDAR point clouds, combining a pre-trained optical flow estimation network with a foundational image segmentation model to rapidly propagate manual annotations into dense labels for both images and point clouds. Moreover, we propose ScatterNet, a network that includes three pivotal strategies to reduce the performance gap caused by such annotations. First, it utilizes dense semantic labels as supervision for the image branch, alleviating the modality imbalance between point clouds and images. Second, an intermediate fusion branch is proposed to obtain multimodal texture and structural features. Finally, a perception consistency loss is introduced to determine which information needs to be fused and which needs to be discarded during the fusion process. Extensive experiments on the nuScenes and SemanticKITTI datasets demonstrate that our method requires less than 0.02% of the labeled points to achieve over 95% of the performance of fully-supervised methods. Notably, our labeled points are only 5% of those used in the most advanced weakly supervised methods.},
  archive      = {J_TMM},
  author       = {Yilong Chen and Zongyi Xu and Xiaoshui Huang and Shanshan Zhao and Xinqi Jiang and Xinyu Gao and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3535350},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4121-4136},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weakly supervised LiDAR semantic segmentation via scatter image annotation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ShapeGPT: 3D shape generation with a unified multi-modal language model. <em>TMM</em>, <em>27</em>, 4107-4120. (<a href='https://doi.org/10.1109/TMM.2025.3535389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of large language models, which enable flexibility through instruction-driven approaches, has revolutionized many traditional generative tasks, but large models for 3D data, particularly in comprehensively handling 3D shapes with other modalities, are still under-explored. By achieving instruction-based shape generation, versatile multi-modal generative shape models can significantly benefit various fields, such as 3D virtual construction and network-aided design. In this article, we present ShapeGPT, a shape-included multi-modal framework to leverage strong pre-trained language models to address multiple shape-relevant tasks. Specifically, ShapeGPT employs a “word-sentence-paragraph” framework to discretize continuous shapes into shape words, further assembles these words into shape sentences, and integrates shape with instructional text for multi-modal paragraphs. To learn this shape-language model, we use a three-stage training scheme, including shape representation, multi-modal alignment, and instruction-based generation, to align shape-language codebooks and learn the intricate correlations among these modalities. Extensive experiments demonstrate that ShapeGPT achieves comparable performance across shape-relevant tasks, including text-to-shape, shape-to-text, shape completion, and shape editing.},
  archive      = {J_TMM},
  author       = {Fukun Yin and Xin Chen and Chi Zhang and Biao Jiang and Zibo Zhao and Wen Liu and Gang Yu and Tao Chen},
  doi          = {10.1109/TMM.2025.3535389},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4107-4120},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ShapeGPT: 3D shape generation with a unified multi-modal language model},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reversible consistency learning for cross-modal retrieval. <em>TMM</em>, <em>27</em>, 4095-4106. (<a href='https://doi.org/10.1109/TMM.2025.3535313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval (CMR) typically involves learning common representations to directly measure similarities between multimodal samples. Most existing CMR methods commonly assume multimodal samples in pairs and employ joint training to learn common representations, limiting the flexibility of CMR. Although some methods adopt independent training strategies for each modality to improve flexibility in CMR, they utilize the randomly initialized orthogonal matrices to guide representation learning, which is suboptimal since they assume inter-class samples are independent of each other, limiting the potential of semantic alignments between sample representations and ground-truth labels. To address these issues, we propose a novel method termed Deep Reversible Consistency Learning (DRCL) for cross-modal retrieval. DRCL includes two core modules, i.e., Selective Prior Learning (SPL) and Reversible Semantic Consistency learning (RSC). More specifically, SPL first learns a transformation weight matrix on each modality and selects the best one based on the quality score as the Prior, which greatly avoids indiscriminateselection of priors learned from low-quality modalities. Then, RSC employs a Modality-invariant Representation Recasting mechanism (MRR) to recast the potential modality-invariant representations from sample semantic labels by the generalized inverse matrix of the prior. Since labels are devoid of modal-specific information, we utilize the recast features to guide the representation learning, thus maintaining semantic consistency to the fullest extent possible. In addition, a feature augmentation mechanism (FA) is introduced in RSC to encourage the model to learn over a wider data distribution for diversity. Finally, extensive experiments conducted on five widely used datasets and comparisons with 15 state-of-the-art baselines demonstrate the effectiveness and superiority of our DRCL.},
  archive      = {J_TMM},
  author       = {Ruitao Pu and Yang Qin and Dezhong Peng and Xiaomin Song and Huiming Zheng},
  doi          = {10.1109/TMM.2025.3535313},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4095-4106},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deep reversible consistency learning for cross-modal retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A copy-move forgery detection network based on selective sampling attention and low-cost two-step self-correlation calculation. <em>TMM</em>, <em>27</em>, 4084-4094. (<a href='https://doi.org/10.1109/TMM.2025.3535369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The commonly used standard convolutional layers cannot adaptively adjust the number and locations of sampling points according to the scales and shapes of tampered regions, which increases the difficulty of detecting images containing tampered regions of different sizes. Therefore, the selective sampling attention (SSA) is proposed to automatically learn the number and locations of sampling points as well as the weight of each sampling point within a certain context range of the input feature map through backpropagation, which can help the network better adapt to tampered regions of different scales and shapes. In addition, the self-correlation calculation (SCC), aiming at calculating the similarity between every two feature points in a feature map, necessarily incurs an expensive computational burden when used for high-resolution feature maps. To remedy the problem, the two-step SCC (TS-SCC) with low computation burden is proposed to pick out highly similar regions by means of the feature similarity obtained from low-resolution version of the input feature map, so that the high-resolution version merely needs to calculate the similarity between every two feature points within its high-similarity regions. Finally, to predict the edges and interiors of copy-move tampered regions more precisely, adaptive dual-branch feature fusion module is proposed to employ a lightweight multi-scale atrous convolutional module to adaptively fuse multi-level features before TS-SCC and the correlation features after TS-SCC, thereby improving the detection performance. Combining these three structures, a lightweight, fast, low-cost and high-precision CMFD network, ST-Net, is designed in this paper. Experimental results on four publicly available datasets verify that ST-Net outperforms several related CMFD networks in terms of detection accuracy, number of parameters, computational cost and inference time.},
  archive      = {J_TMM},
  author       = {Yuxuan Shi and Shaowei Weng and Lifang Yu and Li Li},
  doi          = {10.1109/TMM.2025.3535369},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4084-4094},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A copy-move forgery detection network based on selective sampling attention and low-cost two-step self-correlation calculation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HotMoE: Exploring sparse mixture-of-experts for hyperspectral object tracking. <em>TMM</em>, <em>27</em>, 4072-4083. (<a href='https://doi.org/10.1109/TMM.2025.3535339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral videos contain richer spectral and physical features than RGB videos and thus have greater potential for use in object tracking. The mainstream hyperspectral object tracking approach involves the integration of multiple RGB-based video tracking models. Although ensembles of multiple models can effectively utilize spectral information and improve tracker performance, this approach has high computational complexity, making it difficult to meet the real-time requirements of video object tracking. To bridge the gap, we propose a new hyperspectral object tracking framework (HotMoE) based on Mixture-of-Experts (MoE). HotMoE leverages a divide-and-conquer strategy, where only a subset of expert models is computed for each input, reducing computational complexity while maintaining performance. In this paper, we first design a splitter to group multiple spectral bands into multiple false-color images based on spectral correlations. Then, we design a hyperspectral MoE router that can adaptively learn to aggregate spectral image feature information and route it to suitable experts. Different experts can handle various scenarios, and HotMoE effectively utilizes the capabilities of different experts to obtain better overall performance. Compared with previous state-of-the-art hyperspectral object tracking networks, our model has significantly reduced inference time and performs well, with a processing speed of 43.7 FPS and an AUC of 0.704 with the HOT2022 dataset.},
  archive      = {J_TMM},
  author       = {Wenfang Sun and Yuedong Tan and Jingyuan Li and Shuwei Hou and Xiaobo Li and Yingzhao Shao and Zhe Wang and Beibei Song},
  doi          = {10.1109/TMM.2025.3535339},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4072-4083},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HotMoE: Exploring sparse mixture-of-experts for hyperspectral object tracking},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dataset and metric for quality assessment of HDR tone mapping: Detail visibility, color naturalness, and overall quality. <em>TMM</em>, <em>27</em>, 4058-4071. (<a href='https://doi.org/10.1109/TMM.2025.3535338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tone-Mapping Operators (TMOs) aim at converting high dynamic range (HDR) images into standard dynamic range (SDR) ones that are suitable for being displayed on standard screens. As the visual quality of tone-mapped image (TMI) is paramount, conducting quality assessment of TMIs becomes crucial. Despite the growing body of research on TMI quality assessment, the existing metrics are often limited to a narrow selection of hand-picked examples generated by a restricted range of TMOs. Consequently, their ability of generalizing to the wide array of TMIs encountered in practical scenarios remains unclear. Moreover, the quality degradation in practical TMIs can be intricate, diverse, and complex. To overcome these limitations, we construct so far the largest subjective-annotated TMI quality assessment dataset which comprises a total number of 14,000 TMIs generated by applying 20 representative TMOs to 700 HDR images. The dataset is accompanied by subjective scores that encompass multiple quality dimensions, i.e., TMI quality dataset in terms of Detail visibility, Color naturalness, and overall Quality (TDCQ). In addition, we also design a multi-branch deep neural network tailored to characterize the multi-dimensional quality perception of TMIs, i.e., Color naturalness-, Detail visibility-aware TMI Quality (CDTIQ) metric, allowing for a comprehensive and multifaceted quality assessment of TMIs. Through extensive experiments, we demonstrate the superiority of our proposed metric, showcasing a higher correlation with subjective rating results compared to other relevant no-reference image quality metrics.},
  archive      = {J_TMM},
  author       = {Qiuping Jiang and Xiwen Li and Xinyi Wang and Zhihua Wang and Guangtao Zhai},
  doi          = {10.1109/TMM.2025.3535338},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4058-4071},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dataset and metric for quality assessment of HDR tone mapping: Detail visibility, color naturalness, and overall quality},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long and recent preference learning with recent-K items distribution for recommender system. <em>TMM</em>, <em>27</em>, 4043-4057. (<a href='https://doi.org/10.1109/TMM.2025.3535337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) aims to formulate the recommendation task as a Markov decision process (MDP) and trains an agent to automatically learn the optimal recommendation policy from interaction trajectories through trial-and-error and reward mechanisms. However, most existing RL-based approaches overlook the correlation between items and the dynamics of user interests implied in temporally close interactions. Therefore, in this paper, we propose a reinforcement learning method that incorporates a “recent-k items” distribution to capture users' local preferences. Specifically, we model the output layer as two distinct branches. The “recent-k items” branch, formulated with a Kullback-Leibler divergence loss, learns the recent interests of users, whereas the other branch utilizes a one-step temporal difference error to capture long-term preferences. The proposed structure is integrated into deep Q-learning and actor-critics, resulting in two enhanced methods named R$k$Q and R$k$AC, respectively. Furthermore, a novel soft inter-reward is carefully designed to enhance the proposed method, and we theoretically prove the convergence of the proposed algorithm. We perform extensive experiments on two large real-world datasets and conduct further analysis of the influences of different action sequences, time intervals, and enhancement capabilities for state-of-the-art models. The experimental results demonstrate the efficacy of our proposed methods.},
  archive      = {J_TMM},
  author       = {Yongbiao Gao and Sijie Niu and Guohua Lv and Miaogen Ling and Xin Geng},
  doi          = {10.1109/TMM.2025.3535337},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4043-4057},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Long and recent preference learning with recent-K items distribution for recommender system},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-level masked semantic inference for semi-supervised semantic segmentation. <em>TMM</em>, <em>27</em>, 4029-4042. (<a href='https://doi.org/10.1109/TMM.2025.3535294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised semantic segmentation pursues a holistic pixel-wise understanding of unseen images with limited annotation. To this end, existing methods focus on regularizing per-pixel prediction consistency within unlabeled data, while rarely modeling contextual relationships. But in fact, contextual semantics can provide valuable clues for scene understanding like inner-object continuity and spatial relationships' causality. Thus, in this paper, we propose a Dual-level Masked Semantics Inference (DMSI) that takes the initiative to explicitly learn contextual relationships via enforcing our model to infer the semantics of a pixel according to its surrounding contexts. This allows our model to exhaust accurate semantics by incorporating inter-pixel context clues, further leading to comprehensive segmentation. Specifically, DMSI comprises two main components. 1) Dual-level mask consistency regularization (DMCR) that learns the ability of semantics inference by aligning the predictions of masked views with the prediction of the complete view. The masked views here come from both the image level and feature level, where our model captures low-level attributes and high-level representations respectively. 2) AdaMask that provides a proper mask position and ratio for each image, guiding our model to focus on semantic-rich regions while providing balanced training between hard and easy samples. Through learning the ability of semantic inferring, DMSI remarkably enhances the interaction between pixels, further progressively intensifying the understanding of semantics. Extensive experiments under various settings on Cityscapes and Pascal VOC 2012 show that DMSI achieves new state-of-the-art performances. Furthermore, analysis indicates that our method has superiority in mining inter-pixel semantic relationships and improving robustness facing noise corruption.},
  archive      = {J_TMM},
  author       = {Qiankun Ma and Ziyao Zhang and Pengchong Qiao and Yu Wang and Rongrong Ji and Chang Liu and Jie Chen},
  doi          = {10.1109/TMM.2025.3535294},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4029-4042},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dual-level masked semantic inference for semi-supervised semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFANet: Adaptive frequency-aware network for weakly-supervised few-shot semantic segmentation. <em>TMM</em>, <em>27</em>, 4018-4028. (<a href='https://doi.org/10.1109/TMM.2025.3535348'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5i and COCO-20i datasets demonstrate that AFANet has achieved state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Jiaqi Ma and Guo-Sen Xie and Fang Zhao and Zechao Li},
  doi          = {10.1109/TMM.2025.3535348},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4018-4028},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AFANet: Adaptive frequency-aware network for weakly-supervised few-shot semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A category-driven contrastive recovery network for double incomplete multi-view multi-label classification. <em>TMM</em>, <em>27</em>, 4008-4017. (<a href='https://doi.org/10.1109/TMM.2025.3535286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-view multi-label learning, the challenges of incomplete views and missing labels are prevalent due to the complexity of manual labeling and data acquisition errors. These challenges significantly reduce the quality of latent representations and hinder prediction by multi-label classification. To address this issue, we propose a novel Category-driven Semi-supervised Contrastive Recovery (CSCR) framework in this study. Our framework aims to fully integrate existing label information into incomplete representation learning and classification. Specifically, to address the limitations posed by incomplete views and labels, we construct a label coincidence matrix based on existing labels, which serves as a similarity matrix in subsequent semi-supervised contrastive learning and multi-view classification. By leveraging this matrix, we design a semi-supervised multi-view contrastive learning module, which constructs sample pairs on the basis of inter-view correspondences and label similarity. It learns discriminative latent representations without the need for data augmentation. A weighted multi-label classification module is subsequently employed to integrate the predictions from each view to obtain the final classification result. Experimental evaluations on five challenging datasets demonstrate the superiority of our model over existing state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yiming Wang and Qun Li and Dongxia Chang and Jie Wen and Fu Xiao and Yao Zhao},
  doi          = {10.1109/TMM.2025.3535286},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {4008-4017},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A category-driven contrastive recovery network for double incomplete multi-view multi-label classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One is all: A unified rate-distortion-complexity framework for learned image compression under energy concentration criteria. <em>TMM</em>, <em>27</em>, 3992-4007. (<a href='https://doi.org/10.1109/TMM.2025.3535279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The learned image compression (LIC) technique has surpassed the state-of-the-art traditional codecs (H.266/VVC) in case of rate-distortion (R-D) performance. Its real-time deployments are far advanced. In order to achieve more flexible deployments, an LIC technique should be flexible in adjusting its computational complexity and rate as demanded by a situation and its environment. In this paper, we propose a unified Rate-Distortion-Complexity (R-D-C) framework for LIC under channel energy concentration criteria. Specifically, we first introduce an Energy Asymptotic Nonlinear Transformation (EANT) designed to directly concentrate on the channel energy of latent representations, thus laying the groundwork for a scalable entropy coding. Next, leveraging this energy concentration characteristic, we propose a corresponding Heterogeneous Scalable Entropy Model (HSEM) for flexibly scaling bitstreams as needed. Finally, utilizing the proposed EANT, we construct a fine-grained scalable codec for formulating, in combination with HSEM, a comprehensive scalable R-D-C framework under the energy concentration criteria. The obtained experimental results demonstrate that the proposed method could enable seamless transitions between 13 different widths of sub-models within a single network, allowing for fine-grained control over the model bitrate, complexity, and hardware inference time. Additionally, the proposed method exhibits competitive R-D performance compared to many existing methods.},
  archive      = {J_TMM},
  author       = {Chao Li and Tianyi Li and Fanyang Meng and Qingyu Mao and Youneng Bao and Yonghong Tian and Yongsheng Liang},
  doi          = {10.1109/TMM.2025.3535279},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3992-4007},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {One is all: A unified rate-distortion-complexity framework for learned image compression under energy concentration criteria},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phrase decoupling cross-modal hierarchical matching and progressive position correction for visual grounding. <em>TMM</em>, <em>27</em>, 3979-3991. (<a href='https://doi.org/10.1109/TMM.2025.3535345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual grounding has attracted wide attention thanks to its broad application in various visual language tasks. Although visual grounding has made significant research progress, existing methods ignore the promotion effect of the association between text and image features at different hierarchies on cross-modal matching. This paper proposes a Phrase Decoupling Cross-Modal Hierarchical Matching and Progressive Position Correction Visual Grounding method. It first generates a mask through decoupled sentence phrases, and a text and image hierarchical matching mechanism is constructed, highlighting the role of association between different hierarchies in cross-modal matching. In addition, a corresponding target object position progressive correction strategy is defined based on the hierarchical matching mechanism to achieve accurate positioning for the target object described in the text. This method can continuously optimize and adjust the bounding box position of the target object as the certainty of the text description of the target object improves. This design explores the association between features at different hierarchies and highlights the role of features related to the target object and its position in target positioning. The proposed method is validated on different datasets through experiments, and its superiority is verified by the performance comparison with the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Minghong Xie and Mengzhao Wang and Huafeng Li and Yafei Zhang and Dapeng Tao and Zhengtao Yu},
  doi          = {10.1109/TMM.2025.3535345},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3979-3991},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Phrase decoupling cross-modal hierarchical matching and progressive position correction for visual grounding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal progressive perspective matching network for remote sensing image-text retrieval. <em>TMM</em>, <em>27</em>, 3966-3978. (<a href='https://doi.org/10.1109/TMM.2025.3535365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality based on remote sensing (RS) text-image retrieval has gained increasing attention in recent years due to its ability to leverage the rich semantics of images and the understandability of text to provide a more comprehensive description. Existing cross-modal retrieval methods typically apply self-attention or cross-attention mechanisms to identify important information in RS data, but they ignore the multi-view perception characteristic of geographical space in RS images. As a result, these retrieval models fail to locate the correct perspective in images according to the query text, ultimately leading to incorrect matching. In this work, a Cross-modal Progressive Perspective Matching Network (CPPMN) is proposed for remote sensing image-text retrieval by establishing a progressive perspective matching mechanism and semantic alignment to further improve the performance of the retrieval model. Specifically, the CPPMN framework consists of three core modules: the Compensation Network for Full Perspective Modeling (CN_FPM), the Graph Transformation for Individual Perspective Modeling (GT_IPM), and the Cascaded Transformer for Cross-modal Semantic Alignment (CT_CSA). The CN_FPM module utilizes all positive text samples as supervision signals to guide the feature extraction training process, aiming to capture full perspective information from images. Subsequently, the GT_IPM module transforms implicit-perspective feature representations into explicit-perspective cross-modal relationship graphs. This transformation enables the identification of specific perspective locations within the image according to the query sentence by analyzing graph density and connectivity. Finally, the CT_CSA module comprises a cascaded Transformer network that aligns features at the semantic level between cross-modal data The quantitative and qualitative experiments are conducted on four large-scale remote sensing cross-modal retrieval datasets to demonstrate the significant performance of adopting the progressive perspective matching mechanism and semantic alignment strategy.},
  archive      = {J_TMM},
  author       = {Chengyu Zheng and Xiu Li and Xinyue Liang and Lei Huang and Shan Du and Jie Nie and Junyu Dong},
  doi          = {10.1109/TMM.2025.3535365},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3966-3978},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal progressive perspective matching network for remote sensing image-text retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free elastic model adaptation for vision-and-language navigation. <em>TMM</em>, <em>27</em>, 3953-3965. (<a href='https://doi.org/10.1109/TMM.2025.3535356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-and-Language Navigation (VLN) requires an agent to follow given instructions to navigate. Despite the significant progress, the model trained on seen environments has a performance drop on unseen environments due to distribution shift. To improve the generalization, existing method attempts to apply test-time adaptation to VLN. However, it needs to access the training data and all testing data for updating the model before inference. The setting is not suitable for the real application because it is hard for the agent to access training data and all testing data when the agent is applied in a new environment. In this paper, we consider a more practical setting with source-free and online-inference test-time adaption. In other words, the model can only access one testing sample for test-time adaptation. In this setting, the model may suffer from catastrophic forgetting of the learned knowledge and unstable parameter update issues. To solve these challenges, we propose an elastic adaptation model (EAM) that consists of an auxiliary decision model and a sample replay mechanism. We use the online testing samples to adapt the auxiliary decision model to new environments, which cooperates with the frozen original model to make better action decisions. The sample replay mechanism stores the historical testing samples to make the adaptation process more stable. Our method is model-agnostic and is effortless to be applied to most existing methods. Experimental results show that our method achieves stable performance improvement based on three existing methods on three VLN benchmark datasets.},
  archive      = {J_TMM},
  author       = {Mingkui Tan and Peihao Chen and Hongyan Zhi and Jiajie Mai and Benjamin Rosman and Dongyu Ji and Runhao Zeng},
  doi          = {10.1109/TMM.2025.3535356},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3953-3965},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Source-free elastic model adaptation for vision-and-language navigation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-shot human motion transfer via occlusion-robust flow prediction and neural texturing. <em>TMM</em>, <em>27</em>, 3939-3952. (<a href='https://doi.org/10.1109/TMM.2025.3535368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion transfer aims at animating a static source image with a driving video. While recent advances in one-shot human motion transfer have led to significant improvement in results, it remains challenging for methods with 2D body landmarks, skeleton and semantic mask to accurately capture correspondences between source and driving poses due to the large variation in motion and articulation complexity. In addition, the accuracy and precision of DensePose degrade the image quality for neural-rendering-based methods. To address the limitations and by both considering the importance of appearance and geometry for motion transfer, in this work, we proposed a unified framework that combines multi-scale feature warping and neural texture mapping to recover better 2D appearance and 2.5D geometry, partly by exploiting the information from DensePose, yet adapting to its inherent limited accuracy. Our model takes advantage of multiple modalities by jointly training and fusing them, which allows it to robust neural texture features that cope with geometric errors as well as multi-scale dense motion flow that better preserves appearance. Experimental results with full and half-view body video datasets demonstrate that our model can generalize well and achieve competitive results, and that it is particularly effective in handling challenging cases such as those with substantial self-occlusions.},
  archive      = {J_TMM},
  author       = {Yuzhu Ji and Chuanxia Zheng and Tat-Jen Cham},
  doi          = {10.1109/TMM.2025.3535368},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3939-3952},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {One-shot human motion transfer via occlusion-robust flow prediction and neural texturing},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain expansion and boundary growth for open-set single-source domain generalization. <em>TMM</em>, <em>27</em>, 3925-3938. (<a href='https://doi.org/10.1109/TMM.2025.3535334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-set single-source domain generalization aims to use a single-source domain to learn a robust model that can be generalized to unknown target domains with both domain shifts and label shifts. The scarcity of the source domain and the unknown data distribution of the target domain pose a great challenge for domain-invariant feature learning and unknown class recognition. In this article, we propose a novel learning approach based on domain expansion and boundary growth to expand the scarce source samples and enlarge the boundaries across the known classes that indirectly broaden the boundary between the known and unknown classes. Specifically, we achieve domain expansion by employing both background suppression and style augmentation on the source data to synthesize new samples. Then we force the model to distill consistent knowledge from the synthesized samples so that the model can learn domain-invariant information. Furthermore, we realize boundary growth across classes by using edge maps as an additional modality of samples when training multi-binary classifiers. In this way, it enlarges the boundary between the inliers and outliers, and consequently improves the unknown class recognition during open-set generalization. Extensive experiments show that our approach can achieve significant improvements and reach state-of-the-art performance on several cross-domain image classification datasets.},
  archive      = {J_TMM},
  author       = {Pengkun Jiao and Na Zhao and Jingjing Chen and Yu-Gang Jiang},
  doi          = {10.1109/TMM.2025.3535334},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3925-3938},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Domain expansion and boundary growth for open-set single-source domain generalization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Purifier$^{+}$: Plug-and-play backdoor mitigation for pre-trained models via activation alignment. <em>TMM</em>, <em>27</em>, 3910-3924. (<a href='https://doi.org/10.1109/TMM.2025.3535277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained models are extensively embraced in deep learning, facilitating efficient fine-tuning for downstream user-specific tasks and yielding substantial computational savings. However, backdoor attacks present a significant security threat to downstream models constructed on corrupted pre-trained models, necessitating the implementation of effective countermeasures to mitigate this threat prior to deploying the models in safety-critical applications. This paper introduces Purifier and its advanced version Purifier$^{+}$, the former of which mitigates backdoors in pre-trained models by aligning anomaly activation to normal activation, and the latter builds on this by making importance rating about activation patterns, boosting important activation patterns and suppressing unimportant activation patterns. Purifier and Purifier$^{+}$ draw inspiration from the observation that anomaly activation patterns for backdoor triggers manifest across various perspectives such as channel-wise, cube-wise, and feature-wise, each exhibiting distinct levels of granularity. Crucially, the choice of alignment granularity plays a pivotal role in ensuring robustness and accuracy. In addressing this challenge, Purifier and Purifier$^{+}$ demonstrate the ability to effectively thwart various categories of backdoor triggers devoid of requiring prior information about the specific backdoor attacks. Additionally, it offers a convenient and flexible deployment feature, namely, plug-and-play capability. The comprehensive experimental results demonstrate that Purifier and Purifier$^{+}$ outperform current methodologies regarding defense efficacy and accuracy in model inference with uncontaminated samples when subjected to a series of State-of-the-Art mainstream attacks.},
  archive      = {J_TMM},
  author       = {Xiaoyu Zhang and Yulin Jin and Haoyu Tong and Jian Lou and Kai Wu and Xiaofeng Chen},
  doi          = {10.1109/TMM.2025.3535277},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3910-3924},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Purifier$^{+}$: Plug-and-play backdoor mitigation for pre-trained models via activation alignment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delving into quaternion wavelet transformer for facial expression recognition in the wild. <em>TMM</em>, <em>27</em>, 3895-3909. (<a href='https://doi.org/10.1109/TMM.2025.3535361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Facial Expression Recognition (FER) technique has increasingly matured over time. However, recognizing facial expressions in wild environments poses great challenges in achieving promising performance. The main obstacles arise from various factors, such as illumination changes, head pose variations, and occlusions. To overcome interferences from external environments and improve recognition accuracy, we propose a novel Quaternion Wavelet TRansformer (QWTR) model for FER in the wild. Specifically, we present a Quaternion Value Transformer (QVT) network that combines quaternion multi-head attention with quaternion CNN to capture emotional cues from global and local perception. To preserve the color structure while enhancing image contrast and brightness, we introduce a Quaternion Histogram Equalization (QHE) representation to transform color images into quaternion matrices representation. After that, to alleviate the impact of head pose and occlusion together with feature redundancy, a Quaternion Wavelet Feature Selection (QWFS) scheme is designed to decompose quaternion features and select the most correlated signals. Extensive experiments have been conducted on four in-the-wild FER datasets and several specific FER benchmarks under various conditions. The qualitative and quantitative results demonstrate that QWTR outperforms other state-of-the-art methods in FER benchmarks, e.g., 68.37% vs. 66.31% accuracy on the AffectNet dataset.},
  archive      = {J_TMM},
  author       = {Yu Zhou and Jialun Pei and Weixin Si and Jing Qin and Pheng-Ann Heng},
  doi          = {10.1109/TMM.2025.3535361},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3895-3909},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Delving into quaternion wavelet transformer for facial expression recognition in the wild},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLFuse: Multi-scenario feature joint learning for multi-modality image fusion. <em>TMM</em>, <em>27</em>, 3880-3894. (<a href='https://doi.org/10.1109/TMM.2025.3535355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modality image fusion (MMIF) entails synthesizing images with detailed textures and prominent objects. Existing methods tend to use general feature extraction to handle different fusion tasks. However, these methods have difficulty breaking fusion barriers across various modalities owing to the lack of targeted learning routes. In this work, we propose a multi-scenario feature joint learning architecture, MLFuse, that employs the commonalities of multi-modality images to deconstruct the fusion progress. Specifically, we construct a cross-modal knowledge reinforcing network that adopts a multipath calibration strategy to promote information communication between different images. In addition, two professional networks are developed to maintain the salient and textural information of fusion results. The spatial-spectral domain optimizing network can learn the vital relationship of the source image context with the help of spatial attention and spectral attention. The edge-guided learning network utilizes the convolution operations of various receptive fields to capture image texture information. The desired fusion results are obtained by aggregating the outputs from the three networks. Extensive experiments demonstrate the superiority of MLFuse for infrared-visible image fusion and medical image fusion. The excellent results of downstream tasks (i.e., object detection and semantic segmentation) further verify the high-quality fusion performance of our method.},
  archive      = {J_TMM},
  author       = {Jia Lei and Jiawei Li and Jinyuan Liu and Bin Wang and Shihua Zhou and Qiang Zhang and Xiaopeng Wei and Nikola K. Kasabov},
  doi          = {10.1109/TMM.2025.3535355},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3880-3894},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MLFuse: Multi-scenario feature joint learning for multi-modality image fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Language-assisted 3D scene understanding. <em>TMM</em>, <em>27</em>, 3869-3879. (<a href='https://doi.org/10.1109/TMM.2025.3535305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scale and quality of point cloud datasets constrain the advancement of point cloud learning. Recently, with the development of multi-modal learning, the incorporation of domain-agnostic prior knowledge from other modalities, such as images and text, to assist in point cloud feature learning has been considered a promising avenue. Existing methods have demonstrated the effectiveness of multi-modal contrastive training and feature distillation on point clouds. However, challenges remain, including the requirement for paired triplet data, redundancy and ambiguity in supervised features, and the disruption of the original priors. In this paper, we propose a language-assisted approach to point cloud feature learning (LAST-PCL), enriching semantic concepts through large language model-based text enrichment. We achieve de-redundancy and feature dimensionality reduction without compromising textual priors by statistical-based and training-free significant feature selection. Furthermore, we also delve into an in-depth analysis of the impact of text contrastive training on the point cloud. Extensive experiments validate that the proposed method learns semantically meaningful point cloud features and achieves state-of-the-art or comparable performance in 3D semantic segmentation, 3D object detection, and 3D scene classification tasks.},
  archive      = {J_TMM},
  author       = {Yanmin Wu and Qiankun Gao and Renrui Zhang and Haijie Li and Jian Zhang},
  doi          = {10.1109/TMM.2025.3535305},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3869-3879},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Language-assisted 3D scene understanding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video motion blur attack via grad-weighted and discrete-fusion based perturbation generation. <em>TMM</em>, <em>27</em>, 3856-3868. (<a href='https://doi.org/10.1109/TMM.2025.3535307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has shown that deep learning networks are vulnerable to adversarial samples. Although there has been great progress in the study of adversarial attacks on images, there is relatively little research on adversarial attacks in the video domain, especially on intrinsic factors of videos, such as motion blur. In this paper, we devise a novel Grad-Weighted based One-step Motion Blur Attack (GWO-MBA) and a Discrete-Fusion based Progressive Motion Blur Attack (DFP-MBA) for video recognition, starting from the idea of integrating global adversarial attacks and adversarial patch attacks. Concretely, we use gradient maps to filter and weighted fusion motion blur (termed GWO-MBA) to achieve the attack that matches the motion information in the context of the video. In order to make the generated motion blur attack perturbations more natural and improve the attack success rate, we further introduce a progressive decomposition motion blur strategy (termed DFP-MBA) to progressively fuse more realistic discrete motion blurs. Besides, we propose an Aggressive Motion Blur Generation (AMBG), which generates natural motion blur based on the video context and has a better attack effect. The extensive experiments, on the HMDB-51 and UCF-101 datasets, demonstrate the effectiveness and superiority of our proposed attack method. In addition, the attack effectiveness of the mainstream denoising defense model and the deblur model further validates the robustness of our attack method.},
  archive      = {J_TMM},
  author       = {Guoming Wu and Jun Li and Yangfan Xu and Zhiping Shi and Xianglong Liu},
  doi          = {10.1109/TMM.2025.3535307},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3856-3868},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video motion blur attack via grad-weighted and discrete-fusion based perturbation generation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Repetitive action counting with hybrid temporal relation modeling. <em>TMM</em>, <em>27</em>, 3844-3855. (<a href='https://doi.org/10.1109/TMM.2025.3535385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repetitive Action Counting (RAC) aims to count the number of repetitive actions occurring in videos. In the real world, repetitive actions have great diversity and bring numerous challenges (e.g., viewpoint changes, non-uniform periods, and action interruptions). Existing methods based on the temporal self-similarity matrix (TSSM) for RAC are trapped in the bottleneck of insufficient capturing action periods when applied to complicated daily videos. To tackle this issue, we propose a novel method named Hybrid Temporal Relation Modeling Network (HTRM-Net) to build diverse TSSM for RAC. The HTRM-Net mainly consists of three key components: bi-modal temporal self-similarity matrix modeling, random matrix dropping, and local temporal context modeling. Specifically, we construct temporal self-similarity matrices by bi-modal (self-attention and dual-softmax) operations, yielding diverse matrix representations from the combination of row-wise and column-wise correlations. To further enhance matrix representations, we propose incorporating a random matrix dropping module to guide channel-wise learning of the matrix explicitly. After that, we inject the local temporal context of video frames and the learned matrix into temporal correlation modeling, which can make the model robust enough to cope with error-prone situations, such as action interruption. Finally, a multi-scale matrix fusion module is designed to aggregate temporal correlations adaptively in multi-scale matrices. Extensive experiments across intra- and cross-datasets demonstrate that the proposed method not only outperforms current state-of-the-art methods and but also exhibits robust capabilities in accurately counting repetitive actions in unseen action categories. Notably, our method surpasses the classical TransRAC method by 20.04% in MAE and 22.76% in OBO.},
  archive      = {J_TMM},
  author       = {Kun Li and Xinge Peng and Dan Guo and Xun Yang and Meng Wang},
  doi          = {10.1109/TMM.2025.3535385},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3844-3855},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Repetitive action counting with hybrid temporal relation modeling},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling intra- and inter-modal correlations for incomplete multi-modal 3D shape clustering. <em>TMM</em>, <em>27</em>, 3833-3843. (<a href='https://doi.org/10.1109/TMM.2025.3535317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The investigation for incomplete multi-modal 3D shape clustering is evolving as a promising task for the field of recognizing massive unlabeled 3D shapes. As two widely adopted 3D shape modalities, point clouds and multiple views not only exhibit rich intra-modal correlations but also encompass complementary structures and appearances of 3D shapes. By effectively modeling the intra-modal and inter-modal correlations, this paper proposes a novel incomplete multi-modal 3D shape clustering method to reveal the underlying clustering associations from incomplete multi-modal 3D shapes. In detail, a similarity-transferred feature prediction module is presented to recover the features of missing instances within one modality with the assistance of similarity exploring from another modality. Then, an intra-to-inter progressive feature fusion module is designed to mine the correlations within the modality as well as between different modalities, thereby obtaining comprehensive 3D shape features for clustering. Extensive experiments on two public 3D shape datasets have demonstrated that the proposed method has achieved promising clustering results under different missing rates.},
  archive      = {J_TMM},
  author       = {Tianyi Qin and Bo Peng and Jianjun Lei and Yuxuan Yao and Qingming Huang},
  doi          = {10.1109/TMM.2025.3535317},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3833-3843},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Modeling intra- and inter-modal correlations for incomplete multi-modal 3D shape clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to generate realistic images for bit-depth enhancement via camera imaging processing. <em>TMM</em>, <em>27</em>, 3821-3832. (<a href='https://doi.org/10.1109/TMM.2025.3535322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of advanced displays devices, many attempts have been successfully made in bit-depth enhancement (BDE) to restore the low bit-depth (LBD) images to visually pleasant high bit-depth (HBD) images. However, most methods are still far from satisfactory when addressing real-world LBD images owing to their heavy dependence on LBD-HBD data pairs through direct pixel quantization. Therefore, in this paper, we propose a novel network dubbed RealGAN to generate real-world LBD images by simulating the complex quantization procedure in camera imaging process. Particularly, we design a two-mode differentiable quantization block embedded in the synthesis network facilitating adaptively simulation of the complicated quantization distortions. Furthermore, a simple residual group network is proposed in order to learn the distribution of degradation and non-linear processing in the Image Signal Processing (ISP) pipeline. In the absence of paired HBD and LBD data, the synthesis model is trained end-to-end within the generative adversarial framework using non-paired LBD and HBD images. Finally, we demonstrate that a series of BDE models can benefit from the proposed synthetic dataset and exhibit improved visual quality with sharper edges and finer textures on real-world scenes compared with the original versions trained on directly quantized LBD-HBD pairs.},
  archive      = {J_TMM},
  author       = {Jing Liu and Qingying Li and Huiyu Duan and Zhiwei Fan and Yuting Su and Guangtao Zhai},
  doi          = {10.1109/TMM.2025.3535322},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3821-3832},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning to generate realistic images for bit-depth enhancement via camera imaging processing},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning intrinsic invariance within intra-class for domain generalization. <em>TMM</em>, <em>27</em>, 3807-3820. (<a href='https://doi.org/10.1109/TMM.2025.3535297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods often struggle with the domain shift problem, leading to poor generalization on out-of-domain (OOD) data. To address the problem, domain generalization (DG) has been proposed to leverage the source domains to train a model that can generalize to OOD data. Existing domain generalization methods primarily focus on learning domain invariance, but they fail to ensure proximity among samples within the same category when domains are aligned for domain-invariant learning. Consequently, their generalization performance remains suboptimal. In this paper, we propose a novel approach to address this issue by iteratively approximating the category domain-invariant distribution from all domains. Our method involves an iterative loop where we initially estimate the domain-invariant distribution for each category by averaging the statistical characteristics across all domains. Then the adversarial perturbation alignment is adopted to keep each sample close to its corresponding category domain-invariant distribution. With the iterative loop, the deep network is optimized for robust domain invariance learning. Extensive experiments demonstrate that our proposed method consistently outperforms state-of-the-art approaches across various scenarios.},
  archive      = {J_TMM},
  author       = {Chaoyang Zhou and Zengmao Wang and Bo Du},
  doi          = {10.1109/TMM.2025.3535297},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3807-3820},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning intrinsic invariance within intra-class for domain generalization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text2Avatar: Articulated 3D avatar creation with text instructions. <em>TMM</em>, <em>27</em>, 3797-3806. (<a href='https://doi.org/10.1109/TMM.2025.3535293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework for creating articulated human avatars, editing their styles, and animating the human avatars from three different types of text instructions. The three types of instructions, identity, edit, and action, are fed into three models that generate, edit, and animate human avatars. Specifically, the proposed framework takes identity instruction and multi-view pose condition images to generate the images of a human using the avatar generation model. Then, the avatar can be edited with text instructions by changing the style of the images generated. We apply the Neural Radiance Field (NeRF) and Poisson reconstruction to extract a human mesh model from images and assign linear blend skinning (LBS) weights to the vertices. Finally, the action instructions can animate human avatars, where we use the off-the-shelf method to generate the motions from text instructions. Notably, our proposed method adapts the appearance of hundreds of different individuals to construct a conditionally editable avatar-generated model, allowing easy creation of 3D avatars using text instructions. We demonstrate high-fidelity 3D animatable avatar creation with text instructions on various datasets and highlight a superior performance of the proposed method compared to the previous studies.},
  archive      = {J_TMM},
  author       = {Yong-Hoon Kwon and Ju Hong Yoon and Min-Gyu Park},
  doi          = {10.1109/TMM.2025.3535293},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3797-3806},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Text2Avatar: Articulated 3D avatar creation with text instructions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic visual semantic sub-embeddings and fast re-ranking for image-text retrieval. <em>TMM</em>, <em>27</em>, 3781-3796. (<a href='https://doi.org/10.1109/TMM.2025.3535373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core of image-text retrieval is to accurately measure the similarity between different modalities in a unified representation space. However, compared to textual descriptions of a certain perspective, the visual modality has more semantic variations. Therefore, images are usually associated with multiple textual captions in databases. Although popular symmetric embedding methods have explored numerous modal interaction approaches, they often learn toward outputting the average representation of multiple semantic variations within image embeddings. Consequently, information entropy in embeddings is increased, resulting in redundancy and decreased accuracy. In this work, we propose a Dynamic Visual Semantic Sub-Embeddings framework (DVSE) to reduce the information entropy. Specifically, we obtain a set of heterogeneous visual sub-embeddings through dynamic orthogonal constraint loss. To encourage the generated candidate image embeddings to capture various semantic variations, we construct a mixed distribution and employ a variance-aware weighting loss to assign different weights to the optimization process. In addition, we develop a Fast Re-ranking strategy (FR) to efficiently evaluate the retrieval results and enhance the performance. We compare the performance with existing set-based method using five image feature encoders and three text feature encoders on three benchmark datasets: MSCOCO, Flickr30K and CUB Captions. We also show the role of different components by ablation studies and perform a sensitivity analysis of the hyperparameters. The qualitative analysis of visualized bidirectional retrieval and attention maps further demonstrates the ability of our method to encode semantic variations.},
  archive      = {J_TMM},
  author       = {Wenzhang Wei and Zhipeng Gui and Changguang Wu and Anqi Zhao and Dehua Peng and Huayi Wu},
  doi          = {10.1109/TMM.2025.3535373},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3781-3796},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Dynamic visual semantic sub-embeddings and fast re-ranking for image-text retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion-oriented cross-modal prompting and alignment for human-centric emotional video captioning. <em>TMM</em>, <em>27</em>, 3766-3780. (<a href='https://doi.org/10.1109/TMM.2025.3535292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-centric Emotional Video Captioning (H-EVC) aims to generate fine-grained, emotion-related sentences for human-based videos, enhancing the understanding of human emotions and facilitating human-computer emotional interaction. However, existing video captioning methods often overlook subtle emotional clues and interactions in videos. As a result, the generated captions frequently lack emotional information. To address this, we propose Emotion-oriented Cross-modal Prompting and Alignment (ECPA), which improves HEVC accuracy by modeling fine-grained visual-textual emotion clues. Using large foundation models, ECPA introduces two learnable prompting strategies: visual emotion prompting (VEP) and textual emotion prompting (TEP), along with an emotion-oriented cross-modal alignment (ECA) module. VEP uses two levels of visual prompts, i.e., emotion recognition (ER) and action unit (AU), to focus on both coarse and fine visual emotional features. TEP devise two-level learnable textual prompts, i.e., sentence-level emotional tokens and word-level masked tokens to capture global and local textual emotion representations. ECA introduces another two levels of emotion-oriented prompt alignment learning mechanisms: the ER-sentence level and the AU-word level alignment losses. Both enhance the model's ability to capture and integrate both global and local cross-modal emotion semantics, thereby enabling the generation of fine-grained emotional linguistic descriptions in video captioning. Experiments show ECPA significantly outperforms state-of-the-art methods on various H-EVC datasets (relative improvements of 9.98%, 5.72%, 4.46%, 24.52% on MAFW, and 12.82%, 20.27%, 4.22%, 5.01% on EmVidCap across four evaluation metrics) and supports zero-shot tasks on MSVD and MSRVTT, demonstrating strong applicability and generalization.},
  archive      = {J_TMM},
  author       = {Yu Wang and Yuanyuan Liu and Shunping Zhou and Yuxuan Huang and Chang Tang and Wujie Zhou and Zhe Chen},
  doi          = {10.1109/TMM.2025.3535292},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3766-3780},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Emotion-oriented cross-modal prompting and alignment for human-centric emotional video captioning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stream-ViT: Learning streamlined convolutions in vision transformer. <em>TMM</em>, <em>27</em>, 3755-3765. (<a href='https://doi.org/10.1109/TMM.2025.3535321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently Vision Transformer (ViT) and Convolution Neural Network (CNN) start to emerge as a hybrid deep architecture with better model capacity, generalization, and latency trade-off. Most of these hybrid architectures often directly stack self-attention module with static convolution or fuse their outputs through two pathways within each block. Instead, we present a new Transformer architecture (namely Stream-ViT) to novelly integrate ViT with streamlined convolutions, i.e., a series of high-to-low resolution convolutions. The kernels of each convolution are dynamically learnt on a basis of current input features plus pre-learnt kernels throughout the whole network. The new architecture incorporates a critical pathway to streamline kernel generation that triggers the interactions between dynamically learnt convolutions across different layers. Moreover, the introduction of a layer-wise streamlined convolution is functionally equivalent to a squeezed version of multi-branch convolution structure, thereby improving the capacity of self-attention module with enlarged cardinality in a cost-efficient manner. We validate the superiority of Stream-ViT over multiple vision tasks, and its performances surpass state-of-the-art ViT and CNN backbones with comparable FLOPs.},
  archive      = {J_TMM},
  author       = {Yingwei Pan and Yehao Li and Ting Yao and Chong-Wah Ngo and Tao Mei},
  doi          = {10.1109/TMM.2025.3535321},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3755-3765},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Stream-ViT: Learning streamlined convolutions in vision transformer},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EAT: Multi-exposure image fusion with adversarial learning and focal transformer. <em>TMM</em>, <em>27</em>, 3744-3754. (<a href='https://doi.org/10.1109/TMM.2025.3535390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, different from previous traditional multi-exposure image fusion (MEF) algorithms that use hand-designed feature extraction approaches or deep learning-based algorithms that utilize convolutional neural networks for information preservation, we propose a novel multi-Exposure image fusion method via Adversarial learning and focal Transformer, named EAT. In our framework, a Focal Transformer is proposed to focus on more remarkable regions and construct long-range multi-exposure relationships, with which the fusion model can simultaneously extract local and global multi-exposure properties and therefore generate promising fusion results. To further improve the fusion performance, we introduce adversarial learning to train the proposed method in an adversarial manner with the guidance of ground truth. By doing so, the fused images exhibit better visual perception and color fidelity. Extensive experiments conducted on publicly available databases provide compelling evidence that EAT surpasses other state-of-the-art approaches on both quantitative and qualitative evaluations. Furthermore, we directly employ our trained model to address another benchmark MEF dataset. The impressive fusion performance serves as evidence of the credible generalization ability of EAT.},
  archive      = {J_TMM},
  author       = {Wei Tang and Fazhi He},
  doi          = {10.1109/TMM.2025.3535390},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3744-3754},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {EAT: Multi-exposure image fusion with adversarial learning and focal transformer},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JPEG reversible data hiding via block sorting optimization and dynamic iterative histogram modification. <em>TMM</em>, <em>27</em>, 3729-3743. (<a href='https://doi.org/10.1109/TMM.2025.3535320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {JPEG reversible data hiding (RDH) refers to covert communication technology to accurately extract secret data while also perfectly recovering the original JPEG image. With the development of cloud services, a large number of private JPEG images can be efficiently managed in cloud platforms by embedding user ID or authentication labels. Nevertheless, data embedding operations may inadvertently disrupt the encoding sequence of the original JPEG image, resulting in severe distortion of the host image when it is re-compressed to JPEG format. To address this problem, this paper proposes a new JPEG RDH scheme based on block sorting optimization and dynamic iterative histogram modification. We firstly design a block ordering optimization strategy by combining the number of zero coefficients and the quantization table values of non-zero coefficients in a DCT block. Subsequently, a dynamic iterative histogram modification scheme is proposed by considering the local features and embedding capability of histograms generated from different texture images. According to the given payloads, we introduce different parameters to control the iterations of two-dimensional histogram and then adaptively generate the optimal histogram modification mapping, which can realize low JPEG file size increments by guaranteeing most of the AC coefficients unchanged as much as possible. Numerous experiments have shown that our scheme can achieve an effective balance among embedding capacity, visual quality, file size increment, computational complexity, and outperforms the state-of-the-arts in terms of the above metrics.},
  archive      = {J_TMM},
  author       = {Fengyong Li and Qiankuan Wang and Hang Cheng and Xinpeng Zhang and Chuan Qin},
  doi          = {10.1109/TMM.2025.3535320},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3729-3743},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {JPEG reversible data hiding via block sorting optimization and dynamic iterative histogram modification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Air pollution monitoring by integrating local and global information in self-adaptive multiscale transform domain. <em>TMM</em>, <em>27</em>, 3716-3728. (<a href='https://doi.org/10.1109/TMM.2025.3535351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed a novel image-based air pollution monitor (IAPM) by incorporating local and global information in the self-adaptive multiscale transform domain, so as to achieve the timely and effective leakage detection of typical air pollutants from a single image. To be specific, this paper first developed a screen-shaped module according to two significant findings in visual neuroscience, which include the high sensitivity of human eyes to horizontal and vertical stimuli and the center-surround inhibition, by designing and fusing the square module, horizontal strip module and vertical strip module parallelly for simulating the behaviour of human eyes to extract local features. Second, the learnable weights and proportional mapping were applied to incorporate the screen-shaped module and lightweight vision transformer as backbone, towards more richly exploiting and fusing local and global information just as the way a brain perceives external stimuli. Third, a new self-adaptive multiscale transform domain method was devised based on two motivations from the visual characteristics of multiscale perception and the brain characteristics of self-adaptive domain transform to modify the backbone by using the operations of pooling and pointwise convolution. Extensive experiments implemented on the datasets of carbon particulate matters and ethylene leakage confirmed the superior monitoring performance of the proposed IAPM model beyond the state-of-the-art (SOTA) peers by an accuracy gain of about 4%. Furthermore, the proposed IAPM model only required 0.089 GFLOPs and 0.15 million model parameters, remarkably outperforming SOTA competitors in computational efficiency and storage resources.},
  archive      = {J_TMM},
  author       = {Ke Gu and Yuchen Liu and Hongyan Liu and Bo Liu and Junfei Qiao and Weisi Lin and Wenjun Zhang},
  doi          = {10.1109/TMM.2025.3535351},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3716-3728},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Air pollution monitoring by integrating local and global information in self-adaptive multiscale transform domain},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLIP-GAN: Stacking CLIPs and GAN for efficient and controllable text-to-image synthesis. <em>TMM</em>, <em>27</em>, 3702-3715. (<a href='https://doi.org/10.1109/TMM.2025.3535304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in text-to-image synthesis have captivated audiences worldwide, drawing considerable attention. Although significant progress in generating photo-realistic images through large pre-trained autoregressive and diffusion models, these models face three critical constraints: (1) The requirement for extensive training data and numerous model parameters; (2) Inefficient, multi-step image generation process; and (3) Difficulties in controlling the output visual features, requiring complexly designed prompts to ensure text-image alignment. Addressing these challenges, we introduce the CLIP-GAN model, which innovatively integrates the pretrained CLIP model into both the generator and discriminator of the GAN. Our architecture includes a CLIP-based generator that employs visual concepts derived from CLIP through text prompts in a feature adapter module. We also propose a CLIP-based discriminator, utilizing CLIP's advanced scene understanding capabilities for more precise image quality evaluation. Additionally, our generator applies visual concepts from CLIP via the Text-based Generator Block (TG-Block) and the Polarized Feature Fusion Module (PFFM) enabling better fusion of text and image semantic information. This integration within the generator and discriminator enhances training efficiency, enabling our model to achieve evaluation results not inferior to large pre-trained autoregressive and diffusion models, but with a 94% reduction in learnable parameters. CLIP-GAN aims to achieve the best efficiency-accuracy trade-off in image generation given the limited resource budget. Extensive evaluations validate the superior performance of the model, demonstrating faster image generation speed and the potential for greater stylistic diversity within the GAN model, while still preserving its smooth latent space.},
  archive      = {J_TMM},
  author       = {Yingli Hou and Wei Zhang and Zhiliang Zhu and Hai Yu},
  doi          = {10.1109/TMM.2025.3535304},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3702-3715},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CLIP-GAN: Stacking CLIPs and GAN for efficient and controllable text-to-image synthesis},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving image inpainting using markov random field modeling. <em>TMM</em>, <em>27</em>, 3688-3701. (<a href='https://doi.org/10.1109/TMM.2025.3535382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud services have attracted extensive attention due to low cost, agility and mobility. However, when processing data on cloud servers, users may worry about semi-honest third parties stealing private information from them, hence, data encryption is applied for privacy protection. Inpainting is a technique that reconstructs certain undesirable regions in an image through an imperceptible manner, which can be accomplished by searching for well-matching candidate patches and copying them to to-be-inpainted locations. However, when the image is encrypted, the matched candidate patch searching is a challenging dilemma. Therefore, tackling these data-privacy issues for image inpainting over a cloud infrastructure, we propose an image inpainting scheme using Markov random field (MRF) modeling in encrypted domain. In this scheme, the sender encrypts the to-be-inapinted image by using a homomorphic cryptosystem that supports homomorphic ciphertext comparison. Then, the cloud realizes the MRF-based inpainting for encrypted images through some specific homomorphic operations. In addition, secure context descriptors are utilized to improve the inpainting of textures and structures. Finally, the receiver obtains the inpainted result through image decryption. The proposed scheme is proved to be secure through various cryptographic attacks. Qualitative and quantitative results demonstrate our scheme achieves better inpainted results in structure compared with state-of-the-art schemes in encrypted domain.},
  archive      = {J_TMM},
  author       = {Ping Kong and An Li and Daidou Guo and Liang Zhou and Chuan Qin and Xinpeng Zhang},
  doi          = {10.1109/TMM.2025.3535382},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3688-3701},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Privacy-preserving image inpainting using markov random field modeling},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BMB: Balanced memory bank for long-tailed semi-supervised learning. <em>TMM</em>, <em>27</em>, 3677-3687. (<a href='https://doi.org/10.1109/TMM.2025.3535115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring a substantial amount of unlabeled data, semi-supervised learning boosts the recognition performance when only a limited number of labels are provided. However, conventional methods assume a class-balanced data distribution, which is difficult to realize in practice due to the long-tailed nature of real-world data. While addressing the data imbalance is a well-explored area in supervised learning paradigms, directly transferring existing approaches to SSL is nontrivial, as prior knowledge about unlabeled data distribution remains unknown in SSL. In light of this, we introduce the Balanced Memory Bank (BMB), a framework for long-tailed semi-supervised learning. The core of BMB is an online-updated memory bank that caches historical features alongside their corresponding pseudo-labels, and the memory is also carefully maintained to ensure the data therein are class-rebalanced. Furthermore, an adaptive weighting module is incorporated to work jointly with the memory bank to further re-calibrate the biased training process. Experimental results across various datasets demonstrate the superior performance of BMB compared with state-of-the-art approaches. For instance, an improvement of 8.2% on the 1% labeled subset of ImageNet127 and 4.3% on the 50% labeled subset of ImageNet-LT.},
  archive      = {J_TMM},
  author       = {Wujian Peng and Zejia Weng and Hengduo Li and Zuxuan Wu and Yu-Gang Jiang},
  doi          = {10.1109/TMM.2025.3535115},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3677-3687},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {BMB: Balanced memory bank for long-tailed semi-supervised learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMU-assisted gray pixel shift for video white balance stabilization. <em>TMM</em>, <em>27</em>, 3664-3676. (<a href='https://doi.org/10.1109/TMM.2025.3535396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video white balance is to correct the scene color of video frames to the color under the standard white illumination. Due to the camera movement, video white balance usually suffers temporal instability with unnatural color change between frames. This paper presents a video white balance stabilization method for spatially correct and temporally stable color correction. It exploits the color invariance at the position of the same object to obtain the consistent illumination color estimation through frames. Specifically, it detects gray pixels that inherit the potential illumination color, and their inter-frame motion calculated with the assistance of inertial measurement unit (IMU) is used to carry gray pixels for establishing their correspondence and color fusion between adjacent frames. Because the IMU has more robust and accurate motion cues against large camera movement and texture-less regions in the scene, our method can generate better gray pixel correspondences and illumination color estimation for the white balance stabilization. Besides, our method is computationally efficient to be deployed on mobile phones. Experimental results show that our method can significantly improve the temporal stability as well as maintain the spatial correctness of white balance for videos recorded by cameras equipped with IMU sensors.},
  archive      = {J_TMM},
  author       = {Lei Zhang and Xin Chen and Zichen Wang},
  doi          = {10.1109/TMM.2025.3535396},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3664-3676},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {IMU-assisted gray pixel shift for video white balance stabilization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotics-aware multi-view subspace clustering. <em>TMM</em>, <em>27</em>, 3650-3663. (<a href='https://doi.org/10.1109/TMM.2025.3535402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multi-view subspace clustering has attracted extensive attention due to the rapid increase of multi-view data in many real-world applications. The main goal of this task is to learn a common representation of multiple subspaces from the given multi-view data, and most existing methods usually directly merge multiple groups of features by the single-step integration. However, there may exist large disparities among different views of the data, and thus the conventional single-step practice can hardly obtain a generally consistent feature representation for the multi-view data. To overcome this challenge, we present a novel approach dubbed “Asymptotics-Aware Multi-view Subspace Clustering (A$^{2}$MSC)” to pursue a consistent feature representation in a multi-step way, which iteratively conducts the data recovery to gradually reduce the differences between pairwise views. Specifically, we construct an asymptotic learning rule to update the feature representation, and the iteration result converges to a consistent feature vector for characterizing each instance of the original multi-view data. After that, we utilize such a new feature representation to learn a clustering-oriented similarity matrix via minimizing a self-expressive objective, and we also design the corresponding optimization algorithm to solve it with convergence guarantees. Theoretically, we prove that the learned asymptotic representation effectively integrates multiple views, thereby ensuring the effective handling of multi-view data. Empirically, extensive experimental results demonstrate the superiority of our proposed A$^{2}$MSC over the state-of-the-art multi-view subspace clustering approaches.},
  archive      = {J_TMM},
  author       = {Yesong Xu and Shuo Chen and Jun Li and Jian Yang},
  doi          = {10.1109/TMM.2025.3535402},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3650-3663},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Asymptotics-aware multi-view subspace clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards depth-continuous scene representation with a displacement field for robust light field depth estimation. <em>TMM</em>, <em>27</em>, 3637-3649. (<a href='https://doi.org/10.1109/TMM.2025.3535352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) captures both spatial and angular information of scenes, enabling accurate depth estimation. However, previous deep learning methods have typically model surface depth only, while ignoring the continuous nature of depth in 3D scenes. In this paper, we use displacement field (DF) to describe this continuous property, and propose a novel depth-continuous scene representation for robust LF depth estimation. Experiments demonstrate that our representation enables the network to generate highly detailed depth maps with fewer parameters and faster speed. Specifically, inspired by signed distance field in 3D object description, we aim to exploit the intrinsic depth-continuous property of 3D scenes using DF, and define a novel depth-continuous scene representation. Then, we introduce a simple yet general learning framework for depth-continuous scene embedding, and the proposed network, DepthDF, achieves state-of-the-art performance on both synthetic and real-world LF datasets, ranking 1st on the HCI 4D Light Field benchmark. Furthermore, previous LF depth estimation methods can also be seamlessly integrated into this framework. Finally, we extend this framework beyond LF depth estimation to various tasks, including multi-view stereo depth inference, LF super-resolution, and LF salient object detection. Experiments demonstrate improved performance when the continuous scene representation is applied, suggesting that our framework can potentially bring insights to more fields.},
  archive      = {J_TMM},
  author       = {Rongshan Chen and Hao Sheng and Da Yang and Ruixuan Cong and Zhenglong Cui and Sizhe Wang and Tun Wang and Mingyuan Zhao},
  doi          = {10.1109/TMM.2025.3535352},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3637-3649},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards depth-continuous scene representation with a displacement field for robust light field depth estimation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning auxiliary representations with inconsistency-guided detail regularization for mask-guided matting. <em>TMM</em>, <em>27</em>, 3625-3636. (<a href='https://doi.org/10.1109/TMM.2025.3535381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mask-guided matting networks have achieved significant improvements and have shown great potential in practical applications in recent years. However, simply learning matting representation from synthetic and lack-of-real-world-diversity matting data, these approaches tend to overfit low-level details in wrong regions, lack generalization to objects with complex structures and real-world scenes such as shadows, as well as suffer from interference of background lines or textures. To address these challenges, in this paper, we propose a novel auxiliary learning framework for mask-guided matting models, incorporating three auxiliary tasks: semantic segmentation, edge detection, and background line detection besides matting, to learn different and effective auxiliary representations from different types of data and annotations. Our framework and model introduce the following key aspects: 1) to learn real-world adaptive semantic representation for objects with diverse and complex structures under real-world scenes, we introduce extra semantic segmentation and edge detection tasks on more diverse real-world data with segmentation annotations; 2) to avoid overfitting on low-level details, we propose a module to utilize the inconsistency between learned segmentation and matting representations to regularize detail refinement; 3) we propose a novel background line detection task into our auxiliary learning framework, to suppress interference of background lines or textures. In addition, we propose a high-quality matting benchmark, Plant-Mat, to evaluate matting methods on complex structures. Extensively quantitative and qualitative results show that our approach outperforms state-of-the-art mask-guided methods.},
  archive      = {J_TMM},
  author       = {Weihao Jiang and Zhaozhi Xie and Yuxiang Lu and Longjie Qi and Jingyong Cai and Hiroyuki Uchiyama and Bin Chen and Yue Ding and Hongtao Lu},
  doi          = {10.1109/TMM.2025.3535381},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3625-3636},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning auxiliary representations with inconsistency-guided detail regularization for mask-guided matting},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Snippet-inter difference attention network for weakly-supervised temporal action localization. <em>TMM</em>, <em>27</em>, 3610-3624. (<a href='https://doi.org/10.1109/TMM.2025.3535336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of weakly-supervised temporal action localization (WTAL) task is to simultaneously classify and localize action instances in untrimmed videos with only video-level labels. Previous works fail to extract multi-scale temporal features to identify action instances with different durations, and they do not fully use the temporal cues of action video to learn discriminative features. In addition, the classifiers trained by current methods usually focus on easy-to-distinguish snippets while ignoring other semantically ambiguous features, which leads to incomplete and over-complete localization. To address these issues, we introduce a new Snippet-inter Difference Attention Network (SDANet) for WTAL, which can be trained end-to-end. Specifically, our model presents three modules, with primary contributions lying in the snippet-inter difference attention (SDA) module and potential feature mining (PFM) module. Firstly, we construct a simple multi-scale temporal feature fusion (MTFF) module to generate multi-scale temporal feature representation, so as to help the model better detect short action instances. Secondly, we consider the temporal cues of video features and design SDA module based on the Transformer to capture global discriminative features for each modality based on multi-scale features. It calculates the differences between temporal neighbor snippets in each modality to explore salient-difference features, and then utilizes them to guide correlation modeling. Thirdly, after learning discriminative features, we devise PFM module to excavate potential action and background snippets from ambiguous features. By contrastive learning, potential actions are forced closer to discriminative actions and away from the background, thereby learning more accurate action boundaries. Finally, two losses (i.e., similarity loss and reconstruction loss) are further developed to constrain the consistency between two modalities and help the model retain original feature information for better localization results. Extensive experiments show that our model achieves better performance against current WTAL methods on three datasets, i.e., THUMOS14, ActivityNet1.2 and ActivityNet1.3.},
  archive      = {J_TMM},
  author       = {Wei Zhou and Kang Lin and Weipeng Hu and Chao Xie and Tao Su and Haifeng Hu and Yap-Peng Tan},
  doi          = {10.1109/TMM.2025.3535336},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3610-3624},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Snippet-inter difference attention network for weakly-supervised temporal action localization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaMesh: Personalized facial expressions and head poses for adaptive speech-driven 3D facial animation. <em>TMM</em>, <em>27</em>, 3598-3609. (<a href='https://doi.org/10.1109/TMM.2025.3535287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech-driven 3D facial animation aims at generating facial movements that are synchronized with the driving speech, which has been widely explored recently. Existing works mostly neglect the person-specific talking style in generation, including facial expression and head pose styles. Several works intend to capture the personalities by fine-tuning modules. However, limited training data leads to the lack of vividness. In this work, we propose AdaMesh, a novel adaptive speech-driven facial animation approach, which learns the personalized talking style from a reference video of about 10 seconds and generates vivid facial expressions and head poses. Specifically, we propose mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter, which efficiently captures the facial expression style. For the personalized pose style, we propose a pose adapter by building a discrete pose prior and retrieving the appropriate style embedding with a semantic-aware pose style matrix without fine-tuning. Extensive experimental results show that our approach outperforms state-of-the-art methods, preserves the talking style in the reference video, and generates vivid facial animation.},
  archive      = {J_TMM},
  author       = {Liyang Chen and Weihong Bao and Shun Lei and Boshi Tang and Zhiyong Wu and Shiyin Kang and Haozhi Huang and Helen Meng},
  doi          = {10.1109/TMM.2025.3535287},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3598-3609},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AdaMesh: Personalized facial expressions and head poses for adaptive speech-driven 3D facial animation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupled prototype learning for reliable test-time adaptation. <em>TMM</em>, <em>27</em>, 3585-3597. (<a href='https://doi.org/10.1109/TMM.2025.3535400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class's pseudo-feature from a memory in a momentum manner and insert an additional DPL loss. Finally, we introduce a consistency regularization-based approach to leverage samples with unconfident pseudo-labels. This approach transfers feature styles of samples with unconfident pseudo-labels to those with confident pseudo-labels. Thus, more reliable samples for TTA are created. The experimental results demonstrate that our methods achieve state-of-the-art performance on domain generalization benchmarks, and reliably improve the performance of self-training-based methods on image corruption benchmarks.},
  archive      = {J_TMM},
  author       = {Guowei Wang and Changxing Ding and Wentao Tan and Mingkui Tan},
  doi          = {10.1109/TMM.2025.3535400},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3585-3597},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Decoupled prototype learning for reliable test-time adaptation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GITANet: Group interactive threshold-based attention network for hyperspectral image classification. <em>TMM</em>, <em>27</em>, 3571-3584. (<a href='https://doi.org/10.1109/TMM.2025.3535398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group convolution networks have shown great potential in hyperspectral image (HSI) classification because of their ability to divide total spectral bands into multiple groups and focus on fine discrimination within different spectral ranges. Most group convolution networks process parallel spectral groups independently; however, they neglect the important relevance of nearby spectral ranges. Moreover, the feature maps from different spectral groups are not considered for recalibration in the existing attention. To address these issues, we propose a novel group interactive threshold attention network (GITANet). In the network, a stratified-split-concatenation strategy, which not only splits all bands into multiple groups for intragroup convolution but also propagates the intergroup information via the stratified concatenation operation between different groups, is designed for bandwise group convolution. Relying on the high dependencies among nearby spectra, the cross-group interactive attention block is designed to encourage significant spectral features. Subsequently, from different spectral ranges, a learnable threshold generation block is built to estimate the information validity of each pixel. On the basis of this threshold, soft threshold spatial attention is developed in the bandwise encoder-decoder architecture, which emphasizes high-value spatial areas during the fusion of group convolutional features. Therefore, complementary and discriminative spectral-spatial features are obtained to improve the performance of HSI classification. The experimental results on three HSI datasets illustrate that GITANet is superior to several state-of-the-art networks.},
  archive      = {J_TMM},
  author       = {Yule Duan and Chuang Chen and Maixia Fu and Xiuwen Gong and Yingying Niu and Fulin Luo},
  doi          = {10.1109/TMM.2025.3535398},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3571-3584},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GITANet: Group interactive threshold-based attention network for hyperspectral image classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-based privacy-preserving medical images storage scheme with low consumption. <em>TMM</em>, <em>27</em>, 3556-3570. (<a href='https://doi.org/10.1109/TMM.2025.3535335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the security risks and high transmission/storage consumption in cloud-based medical images storage systems (CMISS), reversible data hiding in encrypted images (RDHEI) provide an effective solution. Nevertheless, challenges persist concerning the security risks cause by key transmission and the large file size of encrypted medical images. Consequently, a cloud-based privacy-preserving medical images storage scheme with low consumption is proposed in this paper. First, RDHEI is applied to CMISS, where image encryption achieves privacy protection, reversible data hiding eliminates extra space consumption by index data self-hiding, and the reversibility enables lossless recovery and extraction of medical images and index data. Then, hybrid encryption is designed to achieve high security. The security of encrypted images is guaranteed by combining a one-time cryptosystem with symmetric XOR encryption, which makes our scheme can resist various attacks. Time-varying key used in XOR is encrypted by asymmetric RSA, and only public key is used in RSA, avoiding the risk of private key transmission. Finally, to reduce the file size of encrypted images and achieve low consumption, context Huffman coding is proposed to adaptively selects the block coding method by context and thresholds, and has at most 98 056 bits shorter than Huffman coding in encoded stream length. Experimental results show that the proposed scheme has better performance in terms on security, consumption, and reversibility. The minimum compression ratio in databases is 32.46%, which is 2.63% lower than the existing schemes. And the medical image and index data can be restored lossless.},
  archive      = {J_TMM},
  author       = {Yaolin Yang and Hongjie He and Zhuo Feng and Fan Chen and Yuan Yuan},
  doi          = {10.1109/TMM.2025.3535335},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3556-3570},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cloud-based privacy-preserving medical images storage scheme with low consumption},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RSUIA: Dynamic no-reference underwater image assessment via reinforcement sequences. <em>TMM</em>, <em>27</em>, 3542-3555. (<a href='https://doi.org/10.1109/TMM.2025.3535308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image quality assessment (UIQA) is a challenging task due to the complexities of underwater environments. Traditional UIQA methods primarily rely on fitting mean opinion scores (MOS), which are limited by human visual biases. To address the above limitation, we propose a no-reference underwater image quality assessment paradigm using reinforcement sequences. Our paradigm leverages reinforcement learning to iteratively merge the input image with the corresponding ground truth, generating an optimized sequence of images. A classifier generates probability arrays for the optimized sequence, which are converted into objective scores by a regression model. Unlike existing methods that focus solely on the final quality score, our paradigm emphasizes dynamic quality changes throughout the image-enhancement process. By employing objective mixing ratio labels, our reinforcement sequence dataset reduces subjective bias. The multiscale classifier captures local and global information differences between the input and ground truth images, effectively preserving the contrast and detail in diverse lighting conditions. Our paradigm combines multi-source data classification with support vector regression, optimizing the mapping of feature vectors to quality scores through fine-tuning libsvm kernel parameters. Experimental results on multiple benchmark datasets demonstrate that our paradigm outperforms the state-of-the-art UIQA methods, providing an effective solution for Underwater Image quality Assessment via Reinforcement Sequences (RSUIA).},
  archive      = {J_TMM},
  author       = {Jingchun Zhou and Chunjiang Liu and Dehuan Zhang and Zongxin He and Ferdous Sohel and Qiuping Jiang},
  doi          = {10.1109/TMM.2025.3535308},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3542-3555},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RSUIA: Dynamic no-reference underwater image assessment via reinforcement sequences},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group-PTP: A pedestrian trajectory prediction method based on group features. <em>TMM</em>, <em>27</em>, 3527-3541. (<a href='https://doi.org/10.1109/TMM.2025.3535380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group features have significant effects on pedestrian movement and constitute a focal point in pedestrian trajectory prediction research. In reality, pedestrians within a group exhibit notable consistency features due to their compact spatial positions, close destinations, and factors such as coordination within the group. In contrast, owing to the dispersed destinations among groups and the lack of coordination, there are significant differences in velocity and direction between the groups, leading to strong conflicts. However, existing pedestrian trajectory prediction models based on group features lack sufficient quantification of both within-group and between-group features. To address this problem, we propose Group-PTP, a novel pedestrian trajectory prediction model based on group features. Specifically, we first propose a group graph attention network-based group features aggregation method (Group-GAT). By quantifying and aggregating the intra-consistency and inter-conflict features exhibited by the groups, our method can better capture the features and interactions both within and between groups. Second, we propose a group multi-feature information representation model that fuses captured group aggregate features, pedestrian coordinates, surrounding pedestrian features, and obstacle features through fusion concatenation. Finally, we propose a multi-feature temporal convolutional network (MF-TCN) that embeds the impact weights of multi-feature information into pedestrian coordinates to obtain feature outputs and conducts temporal operations on feature outputs to predict future trajectories. The experimental results demonstrate that our proposed Group-PTP achieves state-of-the-art performance on several different trajectory prediction benchmarks.},
  archive      = {J_TMM},
  author       = {Chuanyang Zhang and Guijuan Zhang and Zhuoran Zheng and Dianjie Lu},
  doi          = {10.1109/TMM.2025.3535380},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3527-3541},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Group-PTP: A pedestrian trajectory prediction method based on group features},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive complex wavelet informed transformer operator. <em>TMM</em>, <em>27</em>, 3513-3526. (<a href='https://doi.org/10.1109/TMM.2025.3535392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual transformers have achieved great success in representation learning. This is mainly due to efficient token dependency modeling via self-attention. However, the computational burden increases sharply as the input pixels increase. Although recent Fourier-based global frequency-domain mixing methods attempt to improve the efficiency of transformers for high-resolution image inputs, the Fourier operator has limited ability to capture the local geometric structure. Complex wavelets can perform local attention in both the spatial domain and the frequency domain. Therefore, we propose the complex wavelet informed transformer operator that uses the real and imaginary wavelets of the dual-tree complex wavelet transform to simulate the interaction in the attention kernel. In order to further reduce the computational burden of operators, we introduce an adaptive local block shared attention mechanism in the channel domain for our wavelet informed operators. Further, we construct the deep multi-head operator network consisting of a hybrid stack of complex wavelet informed transformer operators and self-attention layers. This enables the Transformer to more sparsely capture multi-scale and multi-directional structured features in the process of learning dependencies. Extensive experimental results show that our adaptive complex wavelet informed transformer operator under the Transformer architecture achieves highly competitive accuracy performance on multiple image classification benchmark datasets. And the proposed operators can be flexibly and effectively migrated to vision tasks in dynamic video scenarios.},
  archive      = {J_TMM},
  author       = {Xiaotong Li and Licheng Jiao and Fang Liu and Shuyuan Yang and Hao Zhu and Xu Liu and Lingling Li and Wenping Ma},
  doi          = {10.1109/TMM.2025.3535392},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3513-3526},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive complex wavelet informed transformer operator},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GEM: Boost simple network for glass surface segmentation via vision foundation models. <em>TMM</em>, <em>27</em>, 3501-3512. (<a href='https://doi.org/10.1109/TMM.2025.3535404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting glass regions is a challenging task due to the inherent ambiguity in their transparency and reflective characteristics. Current solutions in this field remain rooted in conventional deep learning paradigms, requiring the construction of annotated datasets and the design of network architectures. However, the evident drawback with these mainstream solutions lies in the time-consuming and labor-intensive process of curating datasets, alongside the increasing complexity of model structures. In this paper, we propose to address these issues by fully harnessing the capabilities of two existing vision foundation models (VFMs): Stable Diffusion and Segment Anything Model (SAM). Firstly, we construct a Synthetic but photorealistic large-scale Glass Surface Detection dataset, dubbed S-GSD, without any labour cost via Stable Diffusion. This dataset consists of four different scales, consisting of 168 k images totally with precise masks. Besides, based on the powerful segmentation ability of SAM, we devise a simple Glass surface sEgMentor named GEM, which follows the simple query-based encoder-decoder architecture. Comprehensive experiments are conducted on the large-scale glass segmentation dataset GSD-S. Our GEM establishes a new state-of-the-art performance with the help of these two VFMs, surpassing the best-reported method GlassSemNet with an IoU improvement of 2.1%. Additionally, extensive experiments demonstrate that our synthetic dataset S-GSD exhibits remarkable performance in zero-shot and transfer learning settings.},
  archive      = {J_TMM},
  author       = {Jing Hao and Moyun Liu and Jinrong Yang and Kuo Feng Hung},
  doi          = {10.1109/TMM.2025.3535404},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3501-3512},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GEM: Boost simple network for glass surface segmentation via vision foundation models},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards gradient equalization and feature diversification for long-tailed multi-label image recognition. <em>TMM</em>, <em>27</em>, 3489-3500. (<a href='https://doi.org/10.1109/TMM.2025.3535395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image recognition with convolutional neural networks has achieved remarkable progress in the past few years. However, most existing multi-label image recognition methods suffer from the long-tailed data distribution problem, i.e., head categories occupy most training samples, while tailed classes have few samples. This work firstly studies the influence of long-tailed data distribution on existing multi-label image recognition methods. Based on this, two crucial issues of the existing methods are identified: 1) severe gradient imbalance between head and tailed categories, even though re-balancing strategies are adopted; 2) the lack of diversity of tail category training samples. To tackle the first issue, this paper proposes a group sampling strategy to create group-wise balanced data distribution. Meanwhile, a dynamic gradient balancing loss is proposed to equalize the gradient for all categories. To tackle the second issue, this paper proposes a diversity enhancement module to fuse the information across all categories, preventing the network from overfitting tail classes. Furthermore, it also balances the gradient, promoting the discriminability of learned classifiers. Our method significantly outperforms the baseline method and achieves competitive performance with state-of-the-art methods on VOC-LT and COCO-LT datasets. Extensive ablation studies are conducted to verify the effectiveness of the essential proposals.},
  archive      = {J_TMM},
  author       = {Zhao-Min Chen and Quan Cui and Xiaoqin Zhang and Ruoxi Deng and Chaoqun Xia and Shijian Lu},
  doi          = {10.1109/TMM.2025.3535395},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3489-3500},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards gradient equalization and feature diversification for long-tailed multi-label image recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble prototype networks for unsupervised cross-modal hashing with cross-task consistency. <em>TMM</em>, <em>27</em>, 3476-3488. (<a href='https://doi.org/10.1109/TMM.2025.3535378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the swiftly advancing realm of information retrieval, unsupervised cross-modal hashing has emerged as a focal point of research, taking advantage of the inherent advantages of the multifaceted and dynamism inherent in multimedia data. Existing unsupervised cross-modal hashing methods rely mainly on initial pre-trained correlations among cross-modal features, and the inaccurate neighborhood correlations impacts the presentation of common semantics throughout the optimization. To address the aforementioned issues, we propose Ensemble Prototype Networks (EPNet), which delineates class attributes of cross-modal instances through an ensemble clustering methodology. EPNet seeks to extract correlation information between instances by leveraging local correlation aggregation and ensemble clustering from multiple perspectives, aiming to reduce initialization effects and enhance cross-modal representations. Specifically, the local correlation aggregation is first proposed within a batch of semantic affinity relationships to generate a precise and compact hash code among cross-modal instances. Secondly, the ensemble prototype module is employed to discern the class attributes of deep features, thereby aiding the model in extracting more universally applicable feature representations. Thirdly, an early attempt to constrict the representational congruity of local semantic affinity relationships and deep feature ensemble prototype correlations using cross-task consistency loss aims to enhance the representation of cross-modal common semantic features. Finally, EPNet outperforms several state-of-the-art cross-modal retrieval methods on three real-world image-text datasets in extensive experiments.},
  archive      = {J_TMM},
  author       = {Xiaoqing Liu and Huanqiang Zeng and Yifan Shi and Jianqing Zhu and Kaixiang Yang and Zhiwen Yu},
  doi          = {10.1109/TMM.2025.3535378},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3476-3488},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Ensemble prototype networks for unsupervised cross-modal hashing with cross-task consistency},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A 3D self-awareness diffusion network for multimodal classification. <em>TMM</em>, <em>27</em>, 3462-3475. (<a href='https://doi.org/10.1109/TMM.2025.3535295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As imaging sensor technology in remote sensing has advanced quickly, multimodal fusion classification has become an important research direction in land cover and urban planning classification tasks. While generative models and image classification have greatly benefited from diffusion models, the present ones primarily concentrate on single-modality-driven diffusion processes. Therefore, this paper presents a 3D self-awareness diffusion network (3DSA-DiffNet) for multispectral (MS) and panchromatic (PAN) image fusion classification, which would make it easier to classify heterogeneous data from various sensors. First, in order to model the relationship between multi-channel spectra and multi-pixel spatial distributions as well as samples, respectively, a spatial-spectral joint denoising network (S$^{2}$JD-Net) is proposed. It can incorporate the diffusion process into the neural network to enhance the quality of diffusion features. Secondly, to imitate the brain's spatial-spectral coexistence learning mechanism, this work offers a 3D self-awareness module (3DSA-Module) that can learn the weight of each pixel in 3D space, resulting in extraordinarily high feature representation capabilities. Finally, experimental verification demonstrates that the 3D self-awareness diffusion fusion network driven by brain inspiration outperforms more sophisticated approaches on the Xi'an, Huhhot, and Muufl datasets.},
  archive      = {J_TMM},
  author       = {Mengru Ma and Wenping Ma and Licheng Jiao and Lingling Li and Xu Liu and Fang Liu and Shuyuan Yang and Yuwei Guo},
  doi          = {10.1109/TMM.2025.3535295},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3462-3475},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A 3D self-awareness diffusion network for multimodal classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous domain adaptation via correlative and discriminative feature learning. <em>TMM</em>, <em>27</em>, 3447-3461. (<a href='https://doi.org/10.1109/TMM.2025.3535346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous domain adaptation seeks to learn an effective classifier or regression model for unlabeled target samples by using the well-labeled source samples but residing in different feature spaces and lying different distributions. Most recent works have concentrated on learning domain-invariant feature representations to minimize the distribution divergence via target pseudo-labels. However, two critical issues need to be further explored: 1) new feature representations should be not only domain-invariant but also category-correlative and discriminative and 2) alleviating the negative transfer caused by the incorrect pseudo-labeling target samples could boost the adaptation performance during the iterative learning process. To address these issues, in this paper, we put forward a novel heterogeneous domain adaptation method to learn category-correlative and discriminative representations, referred to as correlative and discriminative feature learning (CDFL). Specifically, CDFL aims to learn a feature space where class-specific feature correlations between the source and target domains are maximized, the divergences of marginal and conditional distribution between the source and target domains are minimized, and the distances of inter-class distribution are forced to be maximized to ensure the discriminative ability. Meanwhile, a selective pseudo-labeling procedure based on the correlation coefficient and classifier prediction is introduced to boost class-specific feature correlation and discriminative distribution alignment in an iteration way. Extensive experiments certify that CDFL outperforms the State-of-the-Art algorithms on five standard benchmarks.},
  archive      = {J_TMM},
  author       = {Yuwu Lu and Dewei Lin and Linlin Shen and Yicong Zhou and Jiahui Pan},
  doi          = {10.1109/TMM.2025.3535346},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3447-3461},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Heterogeneous domain adaptation via correlative and discriminative feature learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S3GAAR: Segmented spatiotemporal skeleton graph-attention for action recognition. <em>TMM</em>, <em>27</em>, 3437-3446. (<a href='https://doi.org/10.1109/TMM.2025.3535284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion recognition is extremely important for many practical applications in several disciplines, such as surveillance, medicine, sports, gait analysis, and computer graphics. Graph convolutional networks (GCNs) enhance the accuracy and performance of skeleton-based action recognition. However, this approach has difficulties in modeling long-term temporal dependencies. In Addition, the fixed topology of the skeleton graph is not sufficiently robust to extract features for skeleton motions. Although transformers that rely entirely on self-attention have demonstrated great success in modeling global correlations between inputs and outputs, they ignore the local correlations between joints. In this study, we propose a novel segmented spatiotemporal skeleton graph-attention network (S3GAAR) to effectively learn different human actions and concentrate on the most operative part of the human body for each action. The proposed S3GAAR models spatial-temporal features through spatiotemporal attention for each segment to capture short-term temporal dependencies. Owing to several human actions that focus on one or more body parts such as mutual actions, our novel method divides the human skeleton into three segments: superior, inferior, and extremity joints. Our proposed method is designed to extract the features of each segment individually because human actions focus on one or more segments. Moreover, our segmented spatiotemporal graph introduces additional edges between important distant joints in the same segment. The experimental results show that our novel method outperforms state-of-the-art methods up to 1.1% on two large-scale benchmark datasets, NTU-RGB+D 60 and NTU-RGB+D 120.},
  archive      = {J_TMM},
  author       = {Musrea Abdo Ghaseb and Ahmed Elhayek and Fawaz Alsolami and Abdullah Marish Ali},
  doi          = {10.1109/TMM.2025.3535284},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3437-3446},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {S3GAAR: Segmented spatiotemporal skeleton graph-attention for action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local fine-grained visual tracking. <em>TMM</em>, <em>27</em>, 3426-3436. (<a href='https://doi.org/10.1109/TMM.2025.3535329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel local fine-grained visual tracking task, aiming to precisely locate arbitrary local parts of objects. This task is motivated by our observation that in many realistic scenarios, the user demands to track a local part instead of a holistic object. However, the absence of an evaluation dataset and the distinctive characteristics of local fine-grained targets present extra challenges in conducting this research. To tackle these issues, first, this paper constructs a local fine-grained tracking (LFT) dataset to evaluate the tracking performance for local fine-grained targets. Second, this paper designs a cutting-edge solution to handle the challenges posed by properties of local objects, including ambiguity and high-proportion backgrounds. It consists of a hierarchical adaptive mask mechanism and foreground-background differentiated learning. The former adaptively searches for and masks ambiguity, which drives the network to concentrate on the local target instead of the holistic objects. The latter is constructed to distinguish foreground and background in an unsupervised manner, which is beneficial to mitigate the impacts of high-proportion backgrounds. Extensive analytic experiments are performed to verify the effectiveness of each submodule in the proposed fine-grained tracker.},
  archive      = {J_TMM},
  author       = {Jingjing Wu and Yifan Sun and Richang Hong},
  doi          = {10.1109/TMM.2025.3535329},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3426-3436},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Local fine-grained visual tracking},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SeaCap: Multi-sight embedding and alignment for one-stage image captioner. <em>TMM</em>, <em>27</em>, 3411-3425. (<a href='https://doi.org/10.1109/TMM.2025.3535303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent mainstream image captioning methods usually adopt two-stage captioners, i.e., calculating the object features of the given image by a pre-trained detector and then feeding them into a language model to generate the descriptive sentences. However, such a two-stage procedure will lead to a task-based information gap that decreases the performance of the captioners, because the object features learned from the detection task are suboptimal representations and cannot provide all the necessary information for subsequent sentence generation. Besides, the object features are usually represented by the last pooling features of the detector that lose the local details of images. In this paper, we propose a novel One-Stage Image Captioner using dynamic multi-sight embedding and alignment, called SeaCap, which directly transforms input images into descriptive sentences in one stage to eliminate the information gap. Specifically, to obtain rich features, we use the Swin Transformer to capture multi-level features, followed by a sights alignment module to alleviate the vision confusion, and then feed them into a novel dynamic multi-sight embedding module to exploit both the global structure and local texture of input images. To enhance the global modeling capacity of the visual encoder, we propose a new dual-dimensional refining module to non-locally model the interaction of the embedded features. As a result, SeaCap can obtain rich and useful information to improve the performance of the captioner. Extensive comparisons on the benchmark MS-COCO, Flickr8K and Flickr30 K datasets verified the superior performance of our method.},
  archive      = {J_TMM},
  author       = {Bo Wang and Zhao Zhang and Mingbo Zhao and Xiaojie Jin and Mingliang Xu and Meng Wang},
  doi          = {10.1109/TMM.2025.3535303},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3411-3425},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SeaCap: Multi-sight embedding and alignment for one-stage image captioner},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent watermark: Inject and detect watermarks in latent diffusion space. <em>TMM</em>, <em>27</em>, 3399-3410. (<a href='https://doi.org/10.1109/TMM.2025.3535300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Watermarking is a tool for actively identifying and attributing the images generated by latent diffusion models. Existing methods face the dilemma of image quality and watermark robustness. Watermarks with superior image quality usually have inferior robustness against attacks such as blurring and JPEG compression, while watermarks with superior robustness usually significantly damage image quality. This dilemma stems from the traditional paradigm where watermarks are injected and detected in pixel space, relying on pixel perturbation for watermark detection and resilience against attacks. In this paper, we highlight that an effective solution to the problem is to both inject and detect watermarks in the latent diffusion space, and propose Latent Watermark with a progressive training strategy. It weakens the direct connection between quality and robustness and thus alleviates their contradiction. We conduct evaluations on two datasets and against 10 watermark attacks. Six metrics measure the image quality and watermark robustness. Results show that compared to the recently proposed methods such as StableSignature, StegaStamp, RoSteALS, LaWa, TreeRing, and DiffuseTrace, LW not only surpasses them in terms of robustness but also offers superior image quality.},
  archive      = {J_TMM},
  author       = {Zheling Meng and Bo Peng and Jing Dong},
  doi          = {10.1109/TMM.2025.3535300},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3399-3410},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Latent watermark: Inject and detect watermarks in latent diffusion space},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Universal infrared image nonuniformity correction via stripe-aware attention network. <em>TMM</em>, <em>27</em>, 3383-3398. (<a href='https://doi.org/10.1109/TMM.2025.3535366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared image nonuniformity correction aims to remove the column-wise stripe noise. Most existing methods just consider stripe noise whereas failing to handle real captured nonuniformity, as directional characteristic of stripe is severely disrupted by random Gaussian noise. Moreover, deep learning-based methods proposed in recent years are blocked by limited receptive field thus cannot accurately distinguish vertical structure and vertical stripes. To address these issues, we propose a universal infrared image nonuniformity correction method based on stripe-aware attention network. We seek to improve the performance of our algorithm by first restoring the damaged stripe directional characteristics, then maximizing the utilization of the prior characteristics. On the one hand, we construct the two-stage framework, in which denoising network is firstly applied to eliminate Gaussian noise and preserve stripes as scene information. As a result, the prior directional characteristics are restored, thereby enhancing the ability of subsequent sub-network to perceive stripe noise. On the other hand, due to the distinct long-range pixel correlations of vertical structures and vertical textures, we introduce a column-wise stripe attention mechanism (CSA) that can capture long-range dependencies of target pixels in the vertical direction. This significantly improves the discriminative ability of algorithm towards vertical structures and stripes, with minimal computational cost. Extensive experiments show that the proposed method can achieve promising results and has better universality for different infrared scenarios.},
  archive      = {J_TMM},
  author       = {Kangle Wu and Jun Huang and Yong Ma and Fan Fan and Jiayi Ma},
  doi          = {10.1109/TMM.2025.3535366},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3383-3398},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Universal infrared image nonuniformity correction via stripe-aware attention network},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analogical augmentation and significance analysis for online task-free continual learning. <em>TMM</em>, <em>27</em>, 3370-3382. (<a href='https://doi.org/10.1109/TMM.2025.3535384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online task-free continual learning (OTFCL) is a more challenging variant of continual learning that emphasizes the gradual shift of task boundaries and learning in an online mode. Existing methods rely on a memory buffer of old samples to prevent forgetting. However, the use of memory buffers not only raises privacy concerns but also hinders the efficient learning of new samples. To address this problem, we propose a novel framework called I$^{2}$CANSAY that gets rid of the dependence on memory buffers and efficiently learns the knowledge of new data from one-shot samples. Concretely, our framework comprises two main modules. Firstly, the Inter-Class Analogical Augmentation (ICAN) module generates diverse pseudo-features for old classes based on the inter-class analogy of feature distributions for different new classes, serving as a substitute for the memory buffer. Secondly, the Intra-Class Significance Analysis (ISAY) module analyzes the significance of attributes for each class via its distribution standard deviation, and generates an importance vector as a correction bias for the linear classifier, thereby enhancing the capability of learning from new samples. We run our experiments on four popular image classification datasets: CoRe50, CIFAR-10, CIFAR-100, and CUB-200, our approach outperforms the prior state-of-the-art by a large margin.},
  archive      = {J_TMM},
  author       = {Songlin Dong and Yingjie Chen and Yuhang He and Yuhan Jin and Alex C. Kot and Yihong Gong},
  doi          = {10.1109/TMM.2025.3535384},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3370-3382},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Analogical augmentation and significance analysis for online task-free continual learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QRNet: Quaternion-based refinement network for surface normal estimation. <em>TMM</em>, <em>27</em>, 3356-3369. (<a href='https://doi.org/10.1109/TMM.2025.3535299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a notable increase in interest in image-based surface normal estimation. These approaches are capable of predicting the surface normal of real scenes using only an image, thereby facilitating a more profound comprehension of the actual scene and providing assistance with other perceptual tasks. However, dense regression predictions are susceptible to misdirection when encountering intricate details, which presents a paradoxical challenge for image-based surface normal estimation in reconciling detail and density. By introducing quaternion rotations as fusion module with geometric property, we propose a quaternion-based refined network structure that fuses detailed and structural information. Specifically, we design a high-resolution surface normal baseline with a streamlined structure, to extract fine-grained features while reducing the angular error in surface normal regression values caused by downsampling. Additionally, we propose a subtle angle loss function that prevents subtle changes from being overlooked without extra information, further enhancing the model's ability to learn detailed information. The proposed method demonstrates state-of-the-art performance compared to existing techniques on three real-world datasets comprising indoor and outdoor scenes. The results demonstrate the robust effectiveness of our deep learning approach that incorporates geometric prior guidance, highlighting improved robustness in applying deep learning methods. The source code will be released upon acceptance.},
  archive      = {J_TMM},
  author       = {Hanlin Bai and Xin Gao and Wei Deng and Jianwang Gan and Yijin Xiong and Kangkang Kou and Guoying Zhang},
  doi          = {10.1109/TMM.2025.3535299},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3356-3369},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {QRNet: Quaternion-based refinement network for surface normal estimation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All-in-one weather-degraded image restoration via adaptive degradation-aware self-prompting model. <em>TMM</em>, <em>27</em>, 3343-3355. (<a href='https://doi.org/10.1109/TMM.2025.3535316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing approaches for all-in-one weather-degraded image restoration suffer from inefficiencies in leveraging degradation-aware priors, resulting in sub-optimal performance in adapting to different weather conditions. To this end, we develop an adaptive degradation-aware self-prompting model (ADSM) for all-in-one weather-degraded image restoration. Specifically, our model employs the contrastive language-image pre-training model (CLIP) to facilitate the training of our proposed latent prompt generators (LPGs), which represent three types of latent prompts to characterize the degradation type, degradation property and image caption. Moreover, we integrate the acquired degradation-aware prompts into the time embedding of diffusion model to improve degradation perception. Meanwhile, we employ the latent caption prompt to guide the reverse sampling process using the cross-attention mechanism, thereby guiding the accurate image reconstruction. Furthermore, to accelerate the reverse sampling procedure of diffusion model and address the limitations of frequency perception, we introduce a wavelet-oriented noise estimating network (WNE-Net). Extensive experiments conducted on eight publicly available datasets demonstrate the effectiveness of our proposed approach in both task-specific and all-in-one applications.},
  archive      = {J_TMM},
  author       = {Yuanbo Wen and Tao Gao and Ziqi Li and Jing Zhang and Kaihao Zhang and Ting Chen},
  doi          = {10.1109/TMM.2025.3535316},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3343-3355},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {All-in-one weather-degraded image restoration via adaptive degradation-aware self-prompting model},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial action units as a joint dataset training bridge for facial expression recognition. <em>TMM</em>, <em>27</em>, 3331-3342. (<a href='https://doi.org/10.1109/TMM.2025.3535327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label biases in facial expression recognition (FER) datasets, caused by annotators' subjectivity, pose challenges in improving the performance of target datasets when auxiliary labeled data are used. Moreover, training with multiple datasets can lead to visible degradations in the target dataset. To address these issues, we propose a novel framework called the AU-aware Vision Transformer (AU-ViT), which leverages unified action unit (AU) information and discards expression annotations of auxiliary data. AU-ViT integrates an elaborately designed AU branch in the middle part of a master ViT to enhance representation learning during training. Through qualitative and quantitative analyses, we demonstrate that AU-ViT effectively captures expression regions and is robust to real-world occlusions. Additionally, we observe that AU-ViT also yields performance improvements on the target dataset, even without auxiliary data, by utilizing pseudo AU labels. Our AU-ViT achieves performances superior to, or comparable to, that of the state-of-the-art methods on FERPlus, RAFDB, AffectNet, LSD and the other three occlusion test datasets.},
  archive      = {J_TMM},
  author       = {Shuyi Mao and Xinpeng Li and Fan Zhang and Xiaojiang Peng and Yang Yang},
  doi          = {10.1109/TMM.2025.3535327},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3331-3342},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Facial action units as a joint dataset training bridge for facial expression recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSCC: Cross-scene crowd counting via learning to diversify for domain generalization. <em>TMM</em>, <em>27</em>, 3320-3330. (<a href='https://doi.org/10.1109/TMM.2025.3535302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging for crowd counting models to generalize to new scenes due to domain shifts in training and test data. Although domain adaptation approaches have made notable progress in bridging the domain gap, they require target domain data. In this paper, we propose a novel framework for cross-scene crowd counting, which unifies domain generalization and adaptation. For domain generalization, we train a model only using single-domain data and the model can be generalized to any scene with satisfying performance. Regarding domain adaptation, we use both source and target domain data to further improve the performance. We first design a generation network that diversifies the generated samples to cover the unseen target domains as much as possible by minimizing mutual information. This approach simulates training data in various domains, thereby enhancing the model's generalization ability. Then we develop a pixel-wise supervised contrastive loss function that pulls the human heads in the source images and generated images closer to each other and pushes them further away from the background. This loss helps extract a domain-invariant feature representation, thus improving the model's generalization ability. Moreover, if information about the target domain is available, our generalization method can be easily applied as an adaptation method by replacing the mutual information minimization loss with the mutual information maximization loss. This can further improve cross-scene crowd counting performance. The experimental results demonstrate the strong generalizability of our method across different datasets.},
  archive      = {J_TMM},
  author       = {Yuehai Chen and Qingzhong Wang and Jing Yang and Badong Chen and Haoyi Xiong and Shaoyi Du},
  doi          = {10.1109/TMM.2025.3535302},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3320-3330},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CSCC: Cross-scene crowd counting via learning to diversify for domain generalization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clothes-changing person re-identification with feasibility-aware intermediary matching. <em>TMM</em>, <em>27</em>, 3307-3319. (<a href='https://doi.org/10.1109/TMM.2025.3535357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current clothes-changing person re-identification (re-id) approaches usually perform retrieval based on clothes-irrelevant features, while neglecting the potential of clothes-relevant features. However, we observe that relying solely on clothes-irrelevant features for clothes-changing re-id is limited, since they often lack adequate identity information and suffer from large intra-class variations. On the contrary, clothes-relevant features can be used to discover same-clothes intermediaries that possess informative identity clues. Based on this observation, we propose a Feasibility-Aware Intermediary Matching (FAIM) framework to additionally utilize clothes-relevant features for retrieval. First, an Intermediary Matching (IM) module is designed to perform an intermediary-assisted matching process. This process involves using clothes-relevant features to find informative intermediates, and then using clothes-irrelevant features of these intermediates to complete the matching. Second, in order to reduce the negative effect of low-quality intermediaries, an Intermediary-Based Feasibility Weighting (IBFW) module is designed to evaluate the feasibility of intermediary matching process by assessing the quality of intermediaries. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on several widely-used clothes-changing re-id benchmarks.},
  archive      = {J_TMM},
  author       = {Jiahe Zhao and Ruibing Hou and Hong Chang and Xinqian Gu and Bingpeng Ma and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TMM.2025.3535357},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3307-3319},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Clothes-changing person re-identification with feasibility-aware intermediary matching},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning shape-color diffusion priors for text-guided 3D object generation. <em>TMM</em>, <em>27</em>, 3294-3306. (<a href='https://doi.org/10.1109/TMM.2025.3535325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating 3D shapes according to specific textual input is a crucial topic in the multimedia application, with its potential enhancement to the VR/AR/XR usage that enables more diverse virtual scenes. Due to the recent success of diffusion models, text-guided 3D object generation has drawn a lot of attention recently. However, current latent diffusion-based methods are restricted to shape-only generation, requiring time-consuming and computationally expensive post-processing to obtain colored objects. In this paper, we propose an end-to-end Shape-Color Diffusion Prior framework (SCDiff) to achieve colored text-to-3D object generation. Given a general text description as input, our SCDiff is able to distinguish shape and color-related priors in the text and generate a shape latent and a color latent for a pre-trained 3D object auto-encoder to derive colored 3D objects. Our SCDiff contains two 3D latent diffusion models (LDM), where one generates the shape latent from the input text and the other generates the color latent. To help the two LDMs focus on shape/color-related information, we further adopt a Large Language Model (LLM) to separate the input text into a shape phrase and a color phrase via an in-context learning technique so that our shape/color LDM would not be influenced by irrelevant information. Due to the separation of shape and color latent, we are able to manipulate the color of an object by giving different color phrases while maintaining the original shape. Experiments on a benchmark dataset would quantitatively and qualitatively verify the effectiveness and practicality of our proposed model. As an extension, we show the capability of our SCDiff on 3D object generation and manipulation based on various modality conditions, which further confirms the scalability and applications in multimedia of our proposed framework.},
  archive      = {J_TMM},
  author       = {Sheng-Yu Huang and Chi-Pin Huang and Kai-Po Chang and Zi-Ting Chou and I-Jieh Liu and Yu-Chiang Frank Wang},
  doi          = {10.1109/TMM.2025.3535325},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3294-3306},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning shape-color diffusion priors for text-guided 3D object generation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ATM-NeRF: Accelerating training for NeRF rendering on mobile devices via geometric regularization. <em>TMM</em>, <em>27</em>, 3279-3293. (<a href='https://doi.org/10.1109/TMM.2025.3535288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, an increasing number of researchers have been dedicated to transferring the impressive novel view synthesis capability of Neural Radiance Fields (NeRF) to resource-constrained mobile devices. One common solution is to pre-train NeRF and bake it into textured meshes which are well supported by mobile graphics hardware. However, the training process of existing methods often requires several hours even with multiple high-end NVIDIA V100 GPUs. The underlying reason is that these schemes mainly rely on photometric rendering loss, neglecting the geometric relationship between the pre-trained NeRF and the baked results. Standing on this point, we present ATM-NeRF (Accelerating Training for Mobile rendering based on NeRF), which is the first to apply effective geometric regularization constraints during both the pre-training and the baking training stages for faster convergence. Specifically, in the initial NeRF pre-training stage, we enforce consistency of the multi-resolution density grids representing the scene geometry to mitigate the shape-radiance ambiguity problem to some extent, achieving a coarse mesh with smoothness. In the second stage, we utilize the positions and geometric features of 3D points projected from the pre-trained posed depths to provide geometric supervision for joint refinement of geometry and appearance of the coarse mesh. As a result, our ATM-NeRF achieves comparable rendering quality to MobileNeRF with a training speed that is about $30\times \sim 70\times$ faster while maintaining finer structure details of the exported mesh.},
  archive      = {J_TMM},
  author       = {Yang Chen and Lin Zhang and Shengjie Zhao and Yicong Zhou},
  doi          = {10.1109/TMM.2025.3535288},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3279-3293},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ATM-NeRF: Accelerating training for NeRF rendering on mobile devices via geometric regularization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DetailRecon: Focusing on detailed regions for online monocular 3D reconstruction. <em>TMM</em>, <em>27</em>, 3266-3278. (<a href='https://doi.org/10.1109/TMM.2025.3535311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based online monocular 3D reconstruction has emerged with great potential recently. Most state-of-the-art methods focus on two key questions, namely 1) how to exploit accurate voxel features and 2) how to preserve detailed voxels in the sparsification process. However, 1) most methods adopt the same receptive field to extract features for both informative and uninformative regions, which struggle to capture geometric details. Furthermore, 2) they mainly utilize a fixed threshold or a straightforward ray-based algorithm to discard voxels in the sparsification process. However, some detailed regions (especially thin regions) may be discarded incorrectly. To tackle these challenges, we present a novel method named DetailRecon to focus on detailed regions that contain more geometric information. Specifically, we first propose an Adaptive Hybrid Fusion (AHF) module and a Connectivity-Aware Sparsification (CAS) module for voxel feature learning and voxel sparsification, respectively. 1) The AHF receives multiple feature maps with different receptive fields as input, and adaptively adopts a smaller receptive field for regions with fine structures to exploit accurate geometric details. 2) The CAS updates the occupancy value of voxels based on the connected voxels within its neighbor space, which could expand the radiation range of reliable voxels in detailed regions and eventually reduce their probability of being discarded. Moreover, 3) we introduce a lightweight yet effective pipeline named Focus On Fine (FOF) to accelerate our DetailRecon. In addition, 4) we propose a Hierarchical Consistency Loss (HCL) to align multi-level volume features, which assists in exploring accurate volume features for recovering more details. Extensive experiments conducted on the ScanNet (V2) and 7-Scenes datasets demonstrate the superiority of our DetailRecon.},
  archive      = {J_TMM},
  author       = {Fupeng Chu and Yang Cong and Yanmei Wang and Ronghan Chen},
  doi          = {10.1109/TMM.2025.3535311},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3266-3278},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DetailRecon: Focusing on detailed regions for online monocular 3D reconstruction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpliceMix: A cross-scale and semantic blending augmentation strategy for multi-label image classification. <em>TMM</em>, <em>27</em>, 3251-3265. (<a href='https://doi.org/10.1109/TMM.2025.3535387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Mix-style data augmentation methods (e.g., Mixup and CutMix) have shown promising performance in various visual tasks. However, these methods are primarily designed for single-label images, ignoring the considerable discrepancies between single- and multi-label images, i.e., a multi-label image involves multiple co-occurred categories and fickle object scales. On the other hand, previous multi-label image classification (MLIC) methods tend to design elaborate models, bringing expensive computation. In this article, we introduce a simple but effective augmentation strategy for multi-label image classification, namely SpliceMix. The “splice” in our method is two-fold: 1) Each mixed image is a splice of several downsampled images in the form of a grid, where the semantics of images attending to mixing are blended without object deficiencies for alleviating co-occurred bias; 2) We splice mixed images and the original mini-batch to form a new SpliceMixed mini-batch, which allows an image with different scales to contribute to training together. Furthermore, such splice in our SpliceMixed mini-batch enables interactions between mixed images and original regular images. We also provide a simple and non-parametric extension based on consistency learning (SpliceMix-CL) to show the potential of extending our SpliceMix. Extensive experiments on various tasks demonstrate that only using SpliceMix with a baseline model (e.g., ResNet) achieves better performance than state-of-the-art methods. Moreover, the generalizability of our SpliceMix is further validated by the improvements in current MLIC methods when married with our SpliceMix.},
  archive      = {J_TMM},
  author       = {Lei Wang and Yibing Zhan and Leilei Ma and Dapeng Tao and Liang Ding and Chen Gong},
  doi          = {10.1109/TMM.2025.3535387},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3251-3265},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SpliceMix: A cross-scale and semantic blending augmentation strategy for multi-label image classification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous pairwise-semantic enhancement hashing for large-scale cross-modal retrieval. <em>TMM</em>, <em>27</em>, 3238-3250. (<a href='https://doi.org/10.1109/TMM.2025.3535401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hash learning has drawn widespread attention for large-scale multimodal retrieval because of its stability and efficiency in approximate similarity searches. However, most existing cross-modal hashing approaches employ discrete label-guided information to coarsely reflect intra- and intermodality correlations, making them less effective to measuring the semantic similarity of data with multiple modalities. In this paper, we propose a new heterogeneous pairwise-semantic enhancement hashing (HPsEH) for large-scale cross-modal retrieval by distilling higher-level pairwise-semantic similarity from supervision information. First, we adopt a supervised self-expression to learn a data-specific quantified semantic matrix, which uses real values to measure both the similarity and dissimilarity ranks of paired instances, such that the intrinsic semantics of the data can be well captured. Then, we fuse the label-based information and quantified semantic similarity to collaboratively learn the hash codes of multimodal data, such that both the intermodality consistency and modality-specific features can be simultaneously obtained during hash code learning. Moreover, we employ effective iterative optimization to address the discrete binary solution and massive pairwise matrix calculation, making the HPsEH scalable to large-scale datasets. Extensive experimental results on three widely used datasets demonstrate the superiority of our proposed HPsEH method over most state-of-the art approaches.},
  archive      = {J_TMM},
  author       = {Wai Keung Wong and Lunke Fei and Jianyang Qin and Shuping Zhao and Jie Wen and Zhihao He},
  doi          = {10.1109/TMM.2025.3535401},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3238-3250},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Heterogeneous pairwise-semantic enhancement hashing for large-scale cross-modal retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometry-aware self-supervised indoor 360$^{\circ }$ depth estimation via asymmetric dual-domain collaborative learning. <em>TMM</em>, <em>27</em>, 3224-3237. (<a href='https://doi.org/10.1109/TMM.2025.3535340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to estimate monocular depth for spherical panoramas is of fundamental importance in 3D scene perception. However, spherical distortion severely limits the effectiveness of vanilla convolutions. To push the envelope of accuracy, recent approaches attempt to utilize Tangent projection (TP) to estimate the depth of $360 ^{\circ }$ images. Yet, these methods still suffer from discrepancies and inconsistencies among patch-wise tangent images, as well as the lack of accurate ground truth depth maps under a supervised fashion. In this paper, we propose a geometry-aware self-supervised $360 ^{\circ }$ image depth estimation methodology that explores the complementary advantages of TP and Equirectangular projection (ERP) by an asymmetric dual-domain collaborative learning strategy. Especially, we first develop a lightweight asymmetric dual-domain depth estimation network, which enables to aggregate depth-related features from a single TP domain, and then produce depth distributions of the TP and ERP domains via collaborative learning. This effectively mitigates stitching artifacts and preserves fine details in depth inference without overspending model parameters. In addition, a frequent-spatial feature concentration module is devised to simultaneously capture non-local Fourier features and local spatial features, such that facilitating the efficient exploration of monocular depth cues. Moreover, we introduce a geometric structural alignment module to further improve geometric structural consistency among tangent images. Extensive experiments illustrate that our designed approach outperforms existing self-supervised $360 ^{\circ }$ depth estimation methods on three publicly available benchmark datasets.},
  archive      = {J_TMM},
  author       = {Xu Wang and Ziyan He and Qiudan Zhang and You Yang and Tiesong Zhao and Jianmin Jiang},
  doi          = {10.1109/TMM.2025.3535340},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3224-3237},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Geometry-aware self-supervised indoor 360$^{\circ }$ depth estimation via asymmetric dual-domain collaborative learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ICE: Interactive 3D game character facial editing via dialogue. <em>TMM</em>, <em>27</em>, 3210-3223. (<a href='https://doi.org/10.1109/TMM.2025.3557611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recent popular Role-Playing Games (RPGs) allow players to create in-game characters with hundreds of adjustable parameters, including bone positions and various makeup options. Although text-driven auto-customization systems have been developed to simplify the complex process of adjusting these intricate character parameters, they are limited by their single-round generation and lack the capability for further editing and fine-tuning. In this paper, we propose an Interactive Character Editing framework (ICE) to achieve a multi-round dialogue-based refinement process. In a nutshell, our ICE offers a more user-friendly way to enable players to convey creative ideas iteratively while ensuring that created characters align with the expectations of players. Specifically, we propose an Instruction Parsing Module (IPM) that utilizes large language models (LLMs) to parse multi-round dialogues into clear editing instruction prompts in each round. To reliably and swiftly modify character control parameters at a fine-grained level, we propose a Semantic-guided Low-dimension Parameter Solver (SLPS) that edits character control parameters according to prompts in a zero-shot manner. Our SLPS first localizes the character control parameters related to the fine-grained modification, and then optimizes the corresponding parameters in a low-dimension space to avoid unrealistic results. Extensive experimental results demonstrate the effectiveness of our proposed ICE for in-game character creation and the superior editing performance of ICE.},
  archive      = {J_TMM},
  author       = {Haoqian Wu and Minda Zhao and Zhipeng Hu and Changjie Fan and Lincheng Li and Weijie Chen and Rui Zhao and Xin Yu},
  doi          = {10.1109/TMM.2025.3557611},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3210-3223},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ICE: Interactive 3D game character facial editing via dialogue},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-target pose estimation and behavior analysis based on symmetric cascaded AdderNet. <em>TMM</em>, <em>27</em>, 3197-3209. (<a href='https://doi.org/10.1109/TMM.2025.3557614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the tasks of pose estimation and behavior analysis in computer vision, conventional models are often constrained by various factors or complex environments (such as multiple targets, small targets, occluded targets, etc.). To address this problem, this paper proposes a symmetric cascaded additive network (MulAG) to improve the accuracy of posture estimation and behavior analysis in complex environments. MulAG consists of two modules, MulA and MulG. The MulA module is designed based on a cascaded symmetric network structure and incorporates the addition operation. MulA extracts the posture spatial features of the target from a single frame image. And, the MulG module is designed based on three continuous GRUs (gated recurrent unit). Based on the MulA, MulG extracts the posture temporal features from the posture spatial features of the moving target and predicts the posture temporal features of the moving target. The paper firstly demonstrates the feasibility of addition operations in pose estimation tasks by comparing with MobileNet-v3 in ablation experiments. Secondly, on the HiEve and CrowdPose datasets, MulA achieves accuracy of 79.6% and 80.4%, respectively, outperforming the PTM model by 12.0% and 21.2%. Detection speed of MulA achieves the best value at 8.6 ms, which is 1 times higher than HDGCN. The result demonstrates the effectiveness of MulA in multi-target pose estimation in complex scenes. Finally, on the HDMB-51 and UCF-101 datasets, MulAG achieves accuracy of 74.8% and 86.3%, respectively, outperforming HDGCN by 9.6% and 9.5%. Compared with SKP and GIST, the fps of MulAG (44.8 s−1) is improved by 8.2% and 8.9%. These experiments highlight the generalizability and superiority of MulAG in behavior analysis and pose estimation tasks.},
  archive      = {J_TMM},
  author       = {Xiaoshuo Jia and Qingzhen Xu and Aiqing Zhu and Xiaomei Kuang},
  doi          = {10.1109/TMM.2025.3557614},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3197-3209},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-target pose estimation and behavior analysis based on symmetric cascaded AdderNet},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-lingual adaptation for vision-language model via multimodal semantic distillation. <em>TMM</em>, <em>27</em>, 3184-3196. (<a href='https://doi.org/10.1109/TMM.2025.3557678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Multimodal Models (LMMs) excel in English multimedia tasks but face challenges in adapting to other languages due to linguistic diversity, limited non-English multimodal data, and high training costs. Existing approaches rely on machine-translated multimodal corpora or multilingual large language models, yet they demand substantial resources and achieve only modest zero-shot cross-lingual transfer performance, as shown in the IGLUE benchmark. In this work, we propose SMSA, a Syntax-aware Multimodal Semantic Adaptation approach, which efficiently extends vision-language models (VLMs) to multiple languages via a lightweight adaptation module. Instead of learning from scratch, SMSA transfers multimodal knowledge from English-trained models using two key components: (1) a Syntax-aware Adapter (SAA), which restructures multilingual text representations to align better with English syntax, reducing cross-lingual misalignment; (2) a Multimodal Semantic Distillation (MSD) method, which enables the model to mimic English sequence processing and retain multimodal associations across languages. This allows efficient adaptation to new languages while preserving the original model's strong multimodal capabilities. We extend an MoE-based VLM to 8 languages using a small translation dataset. Evaluations on the IGLUE benchmark show that SMSA achieves strong zero-shot transfer, outperforming some multilingual LMMs and demonstrating its effectiveness in cross-lingual vision-language adaptation.},
  archive      = {J_TMM},
  author       = {Yu Weng and Wenbin He and Jun Dong and Chaomurilige and Xuan Liu and Zheng Liu},
  doi          = {10.1109/TMM.2025.3557678},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3184-3196},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-lingual adaptation for vision-language model via multimodal semantic distillation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HA-FGOVD: Highlighting fine-grained attributes via explicit linear composition for open-vocabulary object detection. <em>TMM</em>, <em>27</em>, 3171-3183. (<a href='https://doi.org/10.1109/TMM.2025.3557624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary object detection (OVD) models are considered to be Large Multi-modal Models (LMM), due to their extensive training data and a large number of parameters. Mainstream OVD models prioritize object coarse-grained category rather than focus on their fine-grained attributes, e.g., colors or materials, thus failed to identify objects specified with certain attributes. Despite being pretrained on large-scale image-text pairs with rich attribute information, their latent feature space does not highlight these fine-grained attributes. In this paper, we introduce HA-FGOVD, a universal and explicit method that enhances the attribute-level detection capabilities of frozen OVD models by highlighting fine-grained attributes in explicit linear space. Our approach uses a LLM to extract attribute words in input text as a zero-shot task. Then, token attention masks are adjusted to guide text encoders in extracting both global and attribute-specific features, which are explicitly composited as two vectors in linear space to form a new attribute-highlighted feature for detection tasks. The composition weight scalars can be learned or transferred across different OVD models, showcasing the universality of our method. Experimental results show that HA-FGOVD achieves state-of-the-art performance on the FG-OVD benchmark and demonstrates promising generalization on the OVDEval benchmark, suggesting that our method addresses significant limitations in fine-grained attribute detection and has potential for broader fine-grained detection applications.},
  archive      = {J_TMM},
  author       = {Yuqi Ma and Mengyin Liu and Chao Zhu and Xu-Cheng Yin},
  doi          = {10.1109/TMM.2025.3557624},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3171-3183},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HA-FGOVD: Highlighting fine-grained attributes via explicit linear composition for open-vocabulary object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot 3D point cloud segmentation via relation consistency-guided heterogeneous prototypes. <em>TMM</em>, <em>27</em>, 3158-3170. (<a href='https://doi.org/10.1109/TMM.2025.3557699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot 3D point cloud semantic segmentation is a challenging task due to the lack of labeled point clouds (support set). To segment unlabeled query point clouds, existing prototype-based methods learn 3D prototypes from point features of the support set and then measure their distances to the query points. However, such homogeneous 3D prototypes are often of low quality because they overlook the valuable heterogeneous information buried in the support set, such as semantic labels and projected 2D depth maps. To address this issue, in this paper, we propose a novel Relation Consistency-guided Heterogeneous Prototype learning framework (RCHP), which improves prototype quality by integrating heterogeneous information using large multi-modal models (e.g. CLIP). RCHP achieves this through two core components: Heterogeneous Prototype Generation module which collaborates with 3D networks and CLIP to generate heterogeneous prototypes, and Heterogeneous Prototype Fusion module which effectively fuses heterogeneous prototypes to obtain high-quality prototypes. Furthermore, to bridge the gap between heterogeneous prototypes, we introduce a Heterogeneous Relation Consistency loss, which transfers more reliable inter-class relations (i.e., inter-prototype relations) from refined prototypes to heterogeneous ones. Extensive experiments conducted on five point cloud segmentation datasets, including four indoor datasets (S3DIS, ScanNet, SceneNN, NYU Depth V2) and one outdoor dataset (Semantic3D), demonstrate the superiority and generalization capability of our method, outperforming state-of-the-art approaches across all datasets.},
  archive      = {J_TMM},
  author       = {Lili Wei and Congyan Lang and Zheming Xu and Liqian Liang and Jun Liu},
  doi          = {10.1109/TMM.2025.3557699},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3158-3170},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Few-shot 3D point cloud segmentation via relation consistency-guided heterogeneous prototypes},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial geometric attacks for 3D point cloud object tracking. <em>TMM</em>, <em>27</em>, 3144-3157. (<a href='https://doi.org/10.1109/TMM.2025.3557613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud object tracking (3D PCOT) plays a vital role in applications such as autonomous driving and robotics. Adversarial attacks offer a promising approach to enhance the robustness and security of tracking models. However, existing adversarial attack methods for 3D PCOT seldom leverage the geometric structure of point clouds and often overlook the transferability of attack strategies. To address these limitations, this paper proposes an adversarial geometric attack method tailored for 3D PCOT, which includes a point perturbation attack module (non-isometric transformation) and a rotation attack module (isometric transformation). First, we introduce a curvature-aware point perturbation attack module that enhances local transformations by applying normal perturbations to critical points identified through geometric features such as curvature and entropy. Second, we design a Thompson sampling-based rotation attack module that applies subtle global rotations to the point cloud, introducing tracking errors while maintaining imperceptibility. Additionally, we design a fused loss function to iteratively optimize the point cloud within the search region, generating adversarially perturbed samples. The proposed method is evaluated on multiple 3D PCOT models and validated through black-box tracking experiments on benchmarks. For P2B, white-box attacks on KITTI reduce the success rate from 53.3% to 29.6% and precision from 68.4% to 37.1%. On NuScenes, the success rate drops from 39.0% to 27.6%, and precision from 39.9 to 26.8%. Black-box attacks show a transferability, with BAT showing a maximum 47.0% drop in success rate and 47.2% in precision on KITTI, and a maximum 22.5% and 27.0% on NuScenes.},
  archive      = {J_TMM},
  author       = {Rui Yao and Anqi Zhang and Yong Zhou and Jiaqi Zhao and Bing Liu and Abdulmotaleb El Saddik},
  doi          = {10.1109/TMM.2025.3557613},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3144-3157},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adversarial geometric attacks for 3D point cloud object tracking},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal evidential learning for open-world weakly-supervised video anomaly detection. <em>TMM</em>, <em>27</em>, 3132-3143. (<a href='https://doi.org/10.1109/TMM.2025.3557682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efforts in weakly-supervised video anomaly detection center on detecting abnormal events within videos by coarse-grained labels, which has been successfully applied to many real-world applications. However, a significant limitation of most existing methods is that they are only effective for specific objects in specific scenarios, which makes them prone to misclassification or omission when confronted with previously unseen anomalies. Relative to conventional anomaly detection tasks, Open-world Weakly-supervised Video Anomaly Detection (OWVAD) poses greater challenges due to the absence of labels and fine-grained annotations for unknown anomalies. To address the above problem, we propose a multi-scale evidential vision-language model to achieve open-world video anomaly detection. Specifically, we leverage generalized visual-language associations derived from CLIP to harness the full potential of large pre-trained models in addressing the OWVAD task. Subsequently, we integrate a multi-scale temporal modeling module with a multimodal evidence collector to achieve precise frame-level detection of both seen and unseen anomalies. Extensive experiments on two widely-utilized benchmarks have conclusively validated the effectiveness of our method. The code will be made publicly available.},
  archive      = {J_TMM},
  author       = {Chao Huang and Weiliang Huang and Qiuping Jiang and Wei Wang and Jie Wen and Bob Zhang},
  doi          = {10.1109/TMM.2025.3557682},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3132-3143},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multimodal evidential learning for open-world weakly-supervised video anomaly detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging concise concepts with probabilistic modeling for interpretable visual recognition. <em>TMM</em>, <em>27</em>, 3117-3131. (<a href='https://doi.org/10.1109/TMM.2025.3557677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretable visual recognition is essential for decision-making in high-stakes situations. Recent advancements have automated the construction of interpretable models by leveraging Visual Language Models (VLMs) and Large Language Models (LLMs) with Concept Bottleneck Models (CBMs), which process a bottleneck layer associated with human-understandable concepts. However, existing methods suffer from two main problems: a) the collected concepts from LLMs could be redundant with task-irrelevant descriptions, resulting in an inferior concept space with potential mismatch. b) VLMs directly map the global deterministic image embeddings with fine-grained concepts results in an ambiguous process with imprecise mapping results. To address the above two issues, we propose a novel solution for CBMs with Concise Concept and Probabilistic Modeling (CCPM) that can achieve superior classification performance via high-quality concepts and precise mapping strategy. First, we leverage in-context examples as category-related clues to guide LLM concept generation process. To mitigate redundancy in the concept space, we propose a Relation-Aware Selection (RAS) module to obtain a concise concept set that is discriminative and relevant based on image-concept and inter-concept relationships. Second, for precise mapping, we employ a Probabilistic Distribution Adapter (PDA) that estimates the inherent ambiguity of the image embeddings of pre-trained VLMs to capture the complex relationships with concepts. Extensive experiments indicate that our model achieves state-of-the-art results with a 6.18% improvement in classification accuracy on eight mainstream recognition benchmarks as well as reliable explainability through interpretable analysis.},
  archive      = {J_TMM},
  author       = {Yixuan Zhang and Chuanbin Liu and Yizhi Liu and Yifan Gao and Zhiying Lu and Hongtao Xie and Yongdong Zhang},
  doi          = {10.1109/TMM.2025.3557677},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3117-3131},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Leveraging concise concepts with probabilistic modeling for interpretable visual recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoGeo: Automating geometric image dataset creation for enhanced geometry understanding. <em>TMM</em>, <em>27</em>, 3105-3116. (<a href='https://doi.org/10.1109/TMM.2025.3557720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of large language models, there has been a growing interest in their capabilities in mathematical reasoning. However, existing research has primarily focused on text-based algebra problems, neglecting the study of geometry due to the lack of high-quality geometric datasets. To address this gap, this paper introduces AutoGeo, a novel approach for automatically generating mathematical geometric images to fulfill the demand for large-scale and diverse geometric datasets. AutoGeo facilitates the creation of AutoGeo-100 k, an extensive repository comprising 100 k high-quality geometry image-text pairs. By leveraging precisely defined geometric clauses, AutoGeo-100 k contains a wide variety of geometric shapes, including lines, polygons, circles, and complex spatial relationships, etc. Furthermore, this paper demonstrates the efficacy of AutoGeo-100 k in enhancing the performance of multimodal large language models through fine-tuning. Experimental results indicate significant improvements in the model's ability in handling geometric images, as evidenced by enhanced accuracy in tasks such as geometric captioning and mathematical reasoning. This research not only fills a critical gap in the availability of geometric datasets but also paves the way for the advancement of sophisticated AI-driven tools in education and research.},
  archive      = {J_TMM},
  author       = {Zihan Huang and Tao Wu and Wang Lin and Shengyu Zhang and Jingyuan Chen and Fei Wu},
  doi          = {10.1109/TMM.2025.3557720},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3105-3116},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AutoGeo: Automating geometric image dataset creation for enhanced geometry understanding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal saliency guided unbiased contrastive learning for video scene graph generation. <em>TMM</em>, <em>27</em>, 3092-3104. (<a href='https://doi.org/10.1109/TMM.2025.3557688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately detecting objects and their interrelationships for Video Scene Graph Generation (VidSGG) confronts two primary challenges. The first involves the identification of active objects interacting with humans from the numerous background objects, while the second challenge is long-tailed distribution among predicate classes. To tackle these challenges, we propose STABILE, a novel framework with a spatial-temporal saliency-guided contrastive learning scheme. For the first challenge, STABILE features an active object retriever that includes an object saliency fusion block for enhancing object embeddings with motion cues alongside an object temporal encoder to capture temporal dependencies. For the second challenge, STABILE introduces an unbiased relationship representation learning module with an Unbiased Multi-Label (UML) contrastive loss to mitigate the effect of long-tailed distribution. With the enhancements in both aspects, STABILE substantially boosts the accuracy of scene graph generation. Extensive experiments demonstrate the superiority of STABILE, setting new benchmarks in the field by offering enhanced accuracy and unbiased scene graph generation.},
  archive      = {J_TMM},
  author       = {Weijun Zhuang and Bowen Dong and Zhilin Zhu and Zhijun Li and Jie Liu and Yaowei Wang and Xiaopeng Hong and Xin Li and Wangmeng Zuo},
  doi          = {10.1109/TMM.2025.3557688},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3092-3104},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial-temporal saliency guided unbiased contrastive learning for video scene graph generation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view user preference modeling for personalized text-to-image generation. <em>TMM</em>, <em>27</em>, 3082-3091. (<a href='https://doi.org/10.1109/TMM.2025.3557683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized text-to-image generation aims to synthesize images tailored to individual user preferences. Existing methods primarily generate customized content using a few reference images, which often struggle to mine user preferences from historical records, and thus fail to synthesize truly personalized content. In addition, it is difficult to directly incorporate the extracted feature of user preferences into the feature space of the generation model, since there exists a considerable gap between them. In this paper, we propose a novel multi-view personalized text-to-image generation method based on the diffusion model, named MVP-Diffusion, which learns instance- and user-level preferences from historical records and integrates them into the generation model. For instance-level user preference modeling, we employ a chain-of-thought prompting strategy to deduce preference keywords and integrate them into input prompts with the aid of a large language model. For user-level preference modeling, we construct a learnable embedding for each user to capture more comprehensive preferences by analyzing their historical records. An adaptive user preference fusion module is proposed to inject user preferences into the generation model via a set of learnable parameters. Experimental results demonstrate that the proposed method significantly enhances the personalization of the generated images compared to the other personalized text-to-image generation methods.},
  archive      = {J_TMM},
  author       = {Huaiwen Zhang and Tianci Wu and Yinwei Wei},
  doi          = {10.1109/TMM.2025.3557683},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3082-3091},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-view user preference modeling for personalized text-to-image generation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ExpLLM: Towards chain of thought for facial expression recognition. <em>TMM</em>, <em>27</em>, 3069-3081. (<a href='https://doi.org/10.1109/TMM.2025.3557704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is a critical task in multimedia with significant implications across various domains. However, analyzing the causes of facial expressions is essential for accurately recognizing them. Current approaches, such as those based on facial action units (AUs), typically provide AU names and intensities but lack insight into the interactions and relationships between AUs and the overall expression. In this paper, we propose a novel method called ExpLLM, which leverages large language models to generate an accurate chain of thought (CoT) for facial expression recognition. Specifically, we have designed the CoT mechanism from three key perspectives: key observations, overall emotional interpretation, and conclusion. The key observations describe the AU's name, intensity, and associated emotions. The overall emotional interpretation provides an analysis based on multiple AUs and their interactions, identifying the dominant emotions and their relationships. Finally, the conclusion presents the final expression label derived from the preceding analysis. Furthermore, we also introduce the Exp-CoT Engine, designed to construct this expression CoT and generate instruction-description data for training our ExpLLM. Extensive experiments on the RAF-DB and AffectNet datasets demonstrate that ExpLLM outperforms current state-of-the-art FER methods. ExpLLM also surpasses the latest GPT-4o in expression CoT generation, particularly in recognizing micro-expressions where GPT-4o frequently fails.},
  archive      = {J_TMM},
  author       = {Xing Lan and Jian Xue and Ji Qi and Dongmei Jiang and Ke Lu and Tat-Seng Chua},
  doi          = {10.1109/TMM.2025.3557704},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3069-3081},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ExpLLM: Towards chain of thought for facial expression recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RoG-SAM: A language-driven framework for instance-level robotic grasping detection. <em>TMM</em>, <em>27</em>, 3057-3068. (<a href='https://doi.org/10.1109/TMM.2025.3557685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic grasping is a crucial topic in robotics and computer vision, with broad applications in industrial production and intelligent manufacturing. Although some methods have begun addressing instance-level grasping, most remain limited to predefined instances and categories, lacking flexibility for open-vocabulary grasp prediction based on user-specified instructions. To address this, we propose RoG-SAM, a language-driven, instance-level grasp detection framework built on Segment Anything Model (SAM). RoG-SAM utilizes open-vocabulary prompts for object localization and grasp pose prediction, adapting SAM through transfer learning with encoder adapters and multi-head decoders to extend its segmentation capabilities to grasp pose estimation. Experimental results show that RoG-SAM achieves competitive performance on single-object datasets (Cornell and Jacquard) and cluttered datasets (GraspNet-1Billion and OCID), with instance-level accuracies of 91.2% and 90.1%, respectively, while using only 28.3% of SAM's trainable parameters. The effectiveness of RoG-SAM was also validated in real-world environments.},
  archive      = {J_TMM},
  author       = {Yunpeng Mei and Jian Sun and Zhihong Peng and Fang Deng and Gang Wang and Jie Chen},
  doi          = {10.1109/TMM.2025.3557685},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3057-3068},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RoG-SAM: A language-driven framework for instance-level robotic grasping detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient transfer from image-based large multimodal models to video tasks. <em>TMM</em>, <em>27</em>, 3045-3056. (<a href='https://doi.org/10.1109/TMM.2025.3557692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extending image-based Large Multimodal Models (LMMs) to video-based LMMs always requires temporal modeling in the pre-training. However, training the temporal modules gradually erases the knowledge of visual features learned from various image-text-based scenarios, leading to degradation in some downstream tasks. To address this issue, in this paper, we introduce a novel, efficient transfer approach termed MTransLLAMA, which employs transfer learning from pre-trained image LMMs for fine-grained video tasks with only small-scale training sets. Our method enables fewer trainable parameters and achieves faster adaptation and higher accuracy than pre-training video-based LMM models. Specifically, our method adopts early fusion between textual and visual features to capture fine-grained information, reuses spatial attention weights in temporal attentions for cyclical spatial-temporal reasoning, and introduces dynamic attention routing to capture both global and local information in spatial-temporal attentions. Experiments demonstrate that across multiple datasets and tasks, without relying on video pre-training, our model achieves state-of-the-art performance, enabling lightweight and efficient transfer from image-based LMMs to fine-grained video tasks.},
  archive      = {J_TMM},
  author       = {Shidong Cao and Zhonghan Zhao and Shengyu Hao and Wenhao Chai and Jenq-Neng Hwang and Hongwei Wang and Gaoang Wang},
  doi          = {10.1109/TMM.2025.3557692},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3045-3056},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient transfer from image-based large multimodal models to video tasks},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual-linguistic feature alignment with semantic and kinematic guidance for referring multi-object tracking. <em>TMM</em>, <em>27</em>, 3034-3044. (<a href='https://doi.org/10.1109/TMM.2025.3557710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring Multi-Object Tracking (RMOT) aims to dynamically track an arbitrary number of referred targets in a video sequence according to the language expression. Previous methods mainly focus on cross-modal fusion at the feature level with designed structures. However, the insufficient visual-linguistic alignment is prone to causing visual-linguistic mismatches, leading to some targets being tracked but not correctly referred especially when facing the language expression with complex semantics or motion descriptions. To this end, we propose to conduct visual-linguistic alignment with semantic and kinematic guidance to effectively align the visual features with more diverse language expressions. In this paper, we put forward a novel end-to-end RMOT framework SKTrack, which follows the transformer-based architecture with a Language-Guided Decoder (LGD) and a Motion-Aware Aggregator (MAA). In particular, the LGD performs deep semantic interaction layer-by-layer in a single frame to enhance the alignment ability of the model, while the MAA conducts temporal feature fusion and alignment across multiple frames to enable the alignment between visual targets and language expression with motion descriptions. Extensive experiments on the Refer-KITTI and Refer-KITTI-v2 demonstrate that SKTrack achieves state-of-the-art performance and verify the effectiveness of our framework and its components.},
  archive      = {J_TMM},
  author       = {Yizhe Li and Sanping Zhou and Zheng Qin and Le Wang},
  doi          = {10.1109/TMM.2025.3557710},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3034-3044},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual-linguistic feature alignment with semantic and kinematic guidance for referring multi-object tracking},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modality prompts: Few-shot multi-label recognition with single-label training. <em>TMM</em>, <em>27</em>, 3023-3033. (<a href='https://doi.org/10.1109/TMM.2025.3557700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot multi-label recognition (FS-MLR) presents a significant challenge due to the need to assign multiple labels to images with limited examples. Existing methods often struggle to balance the learning of novel classes and the retention of knowledge from base classes. To address this issue, we propose a novel Cross-Modality Prompts (CMP) approach. Unlike conventional methods that rely on additional semantic information to mitigate the impact of limited samples, our approach leverages multimodal prompts to adaptively tune the feature extraction network. A new FS-MLR benchmark is also proposed, which includes single-label training and multi-label testing, accompanied by benchmark datasets constructed from MS-COCO and NUS-WIDE. Extensive experiments on these datasets demonstrate the superior performance of our CMP approach, highlighting its effectiveness and adaptability. Our results show that CMP outperforms CoOp on the MS-COCO dataset with a maximal improvement of 19.47% and 23.94% in mAPharmonic for 5-way 1-shot and 5-way 5-shot settings, respectively.},
  archive      = {J_TMM},
  author       = {Zixuan Ding and Zihan Zhou and Hui Chen and Tianxiang Hao and Yizhe Xiong and Sicheng Zhao and Qiang Zhang and Jungong Han},
  doi          = {10.1109/TMM.2025.3557700},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3023-3033},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modality prompts: Few-shot multi-label recognition with single-label training},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open-vocabulary multi-object tracking with domain generalized and temporally adaptive features. <em>TMM</em>, <em>27</em>, 3009-3022. (<a href='https://doi.org/10.1109/TMM.2025.3557619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary multi-object tracking (OVMOT) is a cutting research direction within the multi-object tracking field. It employs large multi-modal models to effectively address the challenge of tracking unseen objects within dynamic visual scenes. While models require robust domain generalization and temporal adaptability, OVTrack, the only existing open-vocabulary multi-object tracker, relies solely on static appearance information and lacks these crucial adaptive capabilities. In this paper, we propose OVSORT, a new framework designed to improve domain generalization and temporal information processing. Specifically, we first propose the Adaptive Contextual Normalization (ACN) technique in OVSORT, which dynamically adjusts the feature maps based on the dataset's statistical properties, thereby fine-tuning our model's to improve domain generalization. Then, we introduce motion cues for the first time. Using our Joint Motion and Appearance Tracking (JMAT) strategy, we obtain a joint similarity measure and subsequently apply the Hungarian algorithm for data association. Finally, our Hierarchical Adaptive Feature Update (HAFU) strategy adaptively adjusts feature updates according to the current state of each trajectory, which greatly improves the utilization of temporal information. Extensive experiments on the TAO validation set and test set confirm the superiority of OVSORT, which significantly improves the handling of novel and base classes. It surpasses existing methods in terms of accuracy and generalization, setting a new state-of-the-art for OVMOT.},
  archive      = {J_TMM},
  author       = {Run Li and Dawei Zhang and Yanchao Wang and Yunliang Jiang and Zhonglong Zheng and Sang-Woon Jeon and Hua Wang},
  doi          = {10.1109/TMM.2025.3557619},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {3009-3022},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Open-vocabulary multi-object tracking with domain generalized and temporally adaptive features},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting EfficientSAM and temporal coherence for audio-visual segmentation. <em>TMM</em>, <em>27</em>, 2999-3008. (<a href='https://doi.org/10.1109/TMM.2025.3557637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-Visual Segmentation (AVS) aims to accurately identify and segment sound sources within video content at the pixel level and requires a fine-grained semantic understanding of both visual and audio cues. While the Segment Anything Model (SAM) has demonstrated outstanding results across various segmentation tasks, its design is primarily focused on single-image segmentation with points, boxes, and mask prompts. As a result, when SAM is applied directly to AVS, it struggles to effectively leverage contextual information from audio data and capture temporal correlations across video frames. Additionally, its high computational requirements pose challenges to its practical applicability in AVS applications. In this paper, we introduce ESAM-AVS, a new framework built on EfficientSAM, aimed at transferring SAM's prior knowledge to the AVS domain. Specifically, we utilize the EfficientSAM as the backbone to maintain model adaptability while significantly lowering computational and processing costs. To tackle the challenges posed by temporal and audio-visual correlations, we designed the Inter-Frame Coherence module, which independently integrates the temporal information from both visual and audio modalities. Furthermore, we incorporate an audio-guided prompt encoder that generates audio prompts to provide guidance, effectively integrating audio cues into the segmentation process. By combining these components, our model maximizes the potential of SAM's prior knowledge, and adapts it to the more complex AVS task. Extensive experiments on the AVSBench dataset demonstrate that ESAM-AVS outperforms existing state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yue Zhu and Kun Li and Zongxin Yang},
  doi          = {10.1109/TMM.2025.3557637},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2999-3008},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Exploiting EfficientSAM and temporal coherence for audio-visual segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantics alternating enhancement and bidirectional aggregation for referring video object segmentation. <em>TMM</em>, <em>27</em>, 2987-2998. (<a href='https://doi.org/10.1109/TMM.2025.3557689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring Video Object Segmentation (RVOS) aims at segmenting out the described object in a video clip according to given expression. The task requires methods to effectively fuse cross-modality features, communicate temporal information, and delineate referent appearance. However, existing solutions bias their focus to mainly mining one or two clues, causing their performance inferior. In this paper, we propose Semantics Alternating Enhancement (SAE) to achieve cross-modality fusion and temporal-spatial semantics mining in an alternate way that makes comprehensive exploit of three cues possible. During each update, SAE will generate a cross-modality and temporal-aware vector that guides vision feature to amplify its referent semantics while filtering out irrelevant contents. In return, the purified feature will provide the contextual soil to produce a more refined guider. Overall, cross-modality interaction and temporal communication are together interleaved into axial semantics enhancement steps. Moreover, we design a simplified SAE by dropping spatial semantics enhancement steps, and employ the variant in the early stages of vision encoder to further enhance usability. To integrate features of different scales, we propose Bidirectional Semantic Aggregation decoder (BSA) to obtain referent mask. The BSA arranges the comprehensively-enhanced features into two groups, and then employs difference awareness step to achieve intra-group feature aggregation bidirectionally and consistency constraint step to realize inter-group integration of semantics-dense and appearance-rich features. Extensive results on challenging benchmarks show that our method performs favorably against the state-of-the-art competitors.},
  archive      = {J_TMM},
  author       = {Jiaxing Yang and Lihe Zhang and Huchuan Lu},
  doi          = {10.1109/TMM.2025.3557689},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2987-2998},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Semantics alternating enhancement and bidirectional aggregation for referring video object segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segmenting anything in the dark via depth perception. <em>TMM</em>, <em>27</em>, 2975-2986. (<a href='https://doi.org/10.1109/TMM.2025.3557612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation under low-light conditions is essential in real-world applications, such as autonomous driving and video surveillance systems. The recent Segment Anything Model (SAM) exhibits strong segmentation capability in various vision applications. However, its performance could be severely degraded under low-light conditions. On the other hand, multimodal information has been exploited to help models construct more comprehensive understanding of scenes under low-light conditions by providing complementary information (e.g., depth). Therefore, in this work, we present a pioneer attempt that elevates a unimodal vision foundation model (e.g., SAM) to a multimodal one, by efficiently integrating additional depth information under low-light conditions. To achieve that, we propose a novel method called Depth Perception SAM (DPSAM) based on the SAM framework. Specifically, we design a modality encoder to extract the depth information and the Depth Perception Layers (DPLs) for mutual feature refinement between RGB and depth features. The DPLs employ the cross-modal attention mechanism to mutually query effective information from both RGB and depth for the subsequent feature refinement. Thus, DPLs can effectively leverage the complementary information from depth to enrich the RGB representations and obtain comprehensive multimodal visual representations for segmenting anything in the dark. To this end, our DPSAM maximally maintains the instinct expertise of SAM for RGB image segmentation and further leverages on the strength of depth for enhanced segmenting anything capability, especially for cases that are likely to fail with RGB only (e.g., low-light or complex textures). As demonstrated by extensive experiments on four RGBD benchmark datasets, DPSAM clearly improves the performance for the segmenting anything performance in the dark, e.g., +12.90% mIoU and +16.23% mIoU on LLRGBD and DeLiVER, respectively.},
  archive      = {J_TMM},
  author       = {Peng Liu and Jinhong Deng and Lixin Duan and Wen Li and Fengmao Lv},
  doi          = {10.1109/TMM.2025.3557612},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2975-2986},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Segmenting anything in the dark via depth perception},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imp: Highly capable large multimodal models for mobile devices. <em>TMM</em>, <em>27</em>, 2961-2974. (<a href='https://doi.org/10.1109/TMM.2025.3557680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By harnessing the capabilities of large language models (LLMs), recent large multimodal models (LMMs) have shown remarkable versatility in open-world multimodal understanding. Nevertheless, they are usually parameter-heavy and computation-intensive, thus hindering their applicability in resource-constrained scenarios. To this end, several lightweight LMMs have been proposed successively to maximize the capabilities under constrained scale (e.g., 3B). Despite the encouraging results achieved by these methods, most of them only focus on one or two aspects of the design space, and the key design choices that influence model capability have not yet been thoroughly investigated. In this paper, we conduct a systematic study for lightweight LMMs from the aspects of model architecture, training strategy, and training data. Based on our findings, we obtain Imp—a family of highly capable LMMs at the 2B$\sim$4B scales. Notably, our Imp-3B model steadily outperforms all the existing lightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs at the 13B scale. With low-bit quantization and resolution reduction techniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile chip with a high inference speed of about 13 tokens/s.},
  archive      = {J_TMM},
  author       = {Zhenwei Shao and Zhou Yu and Jun Yu and Xuecheng Ouyang and Lihao Zheng and Zhenbiao Gai and Mingyang Wang and Zhenzhong Kuang and Jiajun Ding},
  doi          = {10.1109/TMM.2025.3557680},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2961-2974},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Imp: Highly capable large multimodal models for mobile devices},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal large models are effective action anticipators. <em>TMM</em>, <em>27</em>, 2949-2960. (<a href='https://doi.org/10.1109/TMM.2025.3557615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of long-term action anticipation demands solutions that can effectively model temporal dynamics over extended periods while deeply understanding the inherent semantics of actions. Traditional approaches, which primarily rely on recurrent units or Transformer layers to capture long-term dependencies, often fall short in addressing these challenges. Large Language Models (LLMs), with their robust sequential modeling capabilities and extensive commonsense knowledge, present new opportunities for long-term action anticipation. In this work, we introduce the ActionLLM framework, a novel approach that treats video sequences as successive tokens, leveraging LLMs to anticipate future actions. Our baseline model simplifies the LLM architecture by setting future tokens, incorporating an action tuning module, and reducing the textual decoder layer to a linear layer, enabling straightforward action prediction without the need for complex instructions or redundant descriptions. To further harness the commonsense reasoning of LLMs, we predict action categories for observed frames and use sequential textual clues to guide semantic understanding. In addition, we introduce a Cross-Modality Interaction Block, designed to explore the specificity within each modality and capture interactions between vision and textual modalities, thereby enhancing multimodal tuning. Extensive experiments on benchmark datasets demonstrate the superiority of the proposed ActionLLM framework, encouraging a promising direction to explore LLMs in the context of action anticipation.},
  archive      = {J_TMM},
  author       = {Binglu Wang and Yao Tian and Shunzhou Wang and Le Yang},
  doi          = {10.1109/TMM.2025.3557615},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2949-2960},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multimodal large models are effective action anticipators},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal self-perception enhanced large language model for 3D region-of-interest captioning with limited data. <em>TMM</em>, <em>27</em>, 2935-2948. (<a href='https://doi.org/10.1109/TMM.2025.3557703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Region-of-Interest (RoI) Captioning involves translating a model's understanding of specific objects within a complex 3D scene into descriptive captions. Recent advancements in Large Language Models (LLMs) have shown great potential in this area. Existing methods capture the visual information from RoIs as input tokens for LLMs. However, this approach may not provide enough detailed information for LLMs to generate accurate region-specific captions. In this paper, we introduce Self-RoI, a Large Language Model with multi-modal self-perception capabilities for 3D RoI captioning. To ensure LLMs receive more precise and sufficient information, Self-RoI incorporates Implicit Textual Info. Perception to construct a multi-modal vision-language information. This module utilizes a simple mapping network to generate textual information about basic properties of RoI from vision-following response of LLMs. This textual information is then integrated with the RoI's visual representation to form a comprehensive multi-modal instruction for LLMs. Given the limited availability of 3D RoI-captioning data, we propose a two-stage training strategy to optimize Self-RoI efficiently. In the first stage, we align 3D RoI vision and caption representations. In the second stage, we focus on 3D RoI vision-caption interaction, using a disparate contrastive embedding module to improve the reliability of the implicit textual information and employing language modeling loss to ensure accurate caption generation. Our experiments demonstrate that Self-RoI significantly outperforms previous 3D RoI captioning models. Moreover, the Implicit Textual Info. Perception can be integrated into other multi-modal LLMs for performance enhancement. We will make our code available for further research.},
  archive      = {J_TMM},
  author       = {Lu Shi and Shichao Kan and Yi Jin and Linna Zhang and Yigang Cen},
  doi          = {10.1109/TMM.2025.3557703},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2935-2948},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-modal self-perception enhanced large language model for 3D region-of-interest captioning with limited data},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards open-vocabulary video semantic segmentation. <em>TMM</em>, <em>27</em>, 2924-2934. (<a href='https://doi.org/10.1109/TMM.2025.3557719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation in videos has been a focal point of recent research. However, existing models encounter challenges when faced with unfamiliar categories. To address this, we introduce the Open Vocabulary Video Semantic Segmentation (OV-VSS) task, designed to accurately segment every pixel across a wide range of open-vocabulary categories, including those that are novel or previously unexplored. To enhance OV-VSS performance, we propose a robust baseline, OV2VSS, which integrates a spatial-temporal fusion module, allowing the model to utilize temporal relationships across consecutive frames. Additionally, we incorporate a random frame enhancement module, broadening the model's understanding of semantic context throughout the entire video sequence. Our approach also includes video text encoding, which strengthens the model's capability to interpret textual information within the video context. Comprehensive evaluations on benchmark datasets such as VSPW and Cityscapes highlight OV-VSS's zero-shot generalization capabilities, especially in handling novel categories. The results validate OV2VSS's effectiveness, demonstrating improved performance in semantic segmentation tasks across diverse video datasets.},
  archive      = {J_TMM},
  author       = {Xinhao Li and Yun Liu and Guolei Sun and Min Wu and Le Zhang and Ce Zhu},
  doi          = {10.1109/TMM.2025.3557719},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2924-2934},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Towards open-vocabulary video semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive framework embedded with LLM for knowledge graph construction. <em>TMM</em>, <em>27</em>, 2912-2923. (<a href='https://doi.org/10.1109/TMM.2025.3557717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph construction is aimed at storing and representing the knowledge of the objective world in a structured form. Existing methods for automatic construction of knowledge graphs have problems such as difficulty in understanding potential semantics and low precision. The emergence of Large Language Models (LLMs) provides an effective way for automatic knowledge graph construction. However, using LLMs as automatic knowledge graph construction engines relies on the embedding of schema layers, which brings challenges to the input length of LLMs. In this paper, we present a framework for Adaptive Construction of Knowledge Graph by leveraging the exceptional generation capabilities of LLMs and the latent relational semantic information of triples, named ACKG-LLM. Our proposed framework divides the knowledge graph construction task into three subtasks within a unified pipeline: triple extraction of open information, additional relational semantic information embedding and knowledge graph normalization based on schema-level embedding. The framework can construct knowledge graphs in different domains, making up for the defects of existing frameworks that need to retrain and fine-tune the internal model. Extensive experiments demonstrate that our proposed ACKG-LLM performs favorably against representative methods on the REBEL and WiKi-NRE datasets.},
  archive      = {J_TMM},
  author       = {Qingwang Wang and Chaohui Li and Yi Liu and Qiubai Zhu and Jian Song and Tao Shen},
  doi          = {10.1109/TMM.2025.3557717},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2912-2923},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An adaptive framework embedded with LLM for knowledge graph construction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3UR-LLM: An end-to-end multimodal large language model for 3D scene understanding. <em>TMM</em>, <em>27</em>, 2899-2911. (<a href='https://doi.org/10.1109/TMM.2025.3557620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal Large Language Models (MLLMs) exhibit impressive capabilities in 2D tasks, yet encounter challenges in discerning the spatial positions, interrelations, and causal logic in scenes when transitioning from 2D to 3D representations. We find that the limitations mainly lie in: i) the high annotation cost restricting the scale-up of volumes of 3D scene data, and ii) the lack of a straightforward and effective way to perceive 3D information which results in prolonged training durations and complicates the streamlined framework. To this end, we develop a pipeline based on open-source 2D MLLMs and LLMs to generate high-quality 3D-text pairs and construct 3DS-160 K, to enhance the pre-training process. Leveraging this high-quality pre-training data, we introduce the 3UR-LLM model, an end-to-end 3D MLLM designed for precise interpretation of 3D scenes, showcasing exceptional capability in navigating the complexities of the physical world. 3UR-LLM directly receives 3D point cloud as input and project 3D features fused with text instructions into a manageable set of tokens. Considering the computation burden derived from these hybrid tokens, we design a 3D compressor module to cohesively compress the 3D spatial cues and textual narrative. 3UR-LLM achieves promising performance with respect to the previous SOTAs, for instance, 3UR-LLM exceeds its counterparts by 7.1% CIDEr on ScanQA, while utilizing fewer training resources. The code and model weights for 3UR-LLM and the 3DS-160 K benchmark are available at 3UR-LLM.},
  archive      = {J_TMM},
  author       = {Haomiao Xiong and Yunzhi Zhuge and Jiawen Zhu and Lu Zhang and Huchuan Lu},
  doi          = {10.1109/TMM.2025.3557620},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2899-2911},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {3UR-LLM: An end-to-end multimodal large language model for 3D scene understanding},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrAME: Trajectory-anchored multi-view editing for text-guided 3D gaussian manipulation. <em>TMM</em>, <em>27</em>, 2886-2898. (<a href='https://doi.org/10.1109/TMM.2025.3557618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant strides in the field of 3D scene editing, current methods encounter substantial challenge, particularly in preserving 3D consistency during the multi-view editing process. To tackle this challenge, we propose a progressive 3D editing strategy that ensures multi-view consistency via a Trajectory-Anchored Scheme (TAS) with a dual-branch editing mechanism. Specifically, TAS facilitates a tightly coupled iterative process between 2D view editing and 3D updating, preventing error accumulation yielded from the text-to-image process. Additionally, we explore the connection between optimization-based methods and reconstruction-based methods, offering a unified perspective for selecting superior design choices, supporting the rationale behind the designed TAS. We further present a tuning-free View-Consistent Attention Control (VCAC) module that leverages cross-view semantic and geometric reference from the source branch to yield aligned views from the target branch during the editing of 2D views. To validate the effectiveness of our method, we analyze 2D examples to demonstrate the improved consistency with the VCAC module. Extensive quantitative and qualitative results in text-guided 3D scene editing clearly indicate that our method can achieve superior editing quality compared with state-of-the-art 3D scene editing methods.},
  archive      = {J_TMM},
  author       = {Chaofan Luo and Donglin Di and Xun Yang and Yongjia Ma and Zhou Xue and Wei Chen and Xiaofei Gou and Yebin Liu},
  doi          = {10.1109/TMM.2025.3557618},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2886-2898},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TrAME: Trajectory-anchored multi-view editing for text-guided 3D gaussian manipulation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VideoDreamer: Customized multi-subject text-to-video generation with disen-mix finetuning on language-video foundation models. <em>TMM</em>, <em>27</em>, 2875-2885. (<a href='https://doi.org/10.1109/TMM.2025.3557634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customized text-to-video generation aims to generate text-guided videos with user-given subjects, which has gained increasing attention. However, existing works are primarily limited to single-subject oriented text-to-video generation, leaving the more challenging problem of customized multi-subject generation unexplored. In this paper, we fill this gap and propose a novel VideoDreamer framework, which can generate temporally consistent text-guided videos that faithfully preserve the visual features of the given multiple subjects. Specifically, VideoDreamer adopts the pretrained Stable Diffusion with temporal modules as its base video generator, taking the power of the text-to-image model to generate diversified content. The video generator is further customized for multi-subjects, which leverages the proposed Disen-Mix Finetuning and Human-in-the-Loop Re-finetuning strategy, to tackle the attribute binding problem of multi-subject generation. Additionally, we present a disentangled motion customization strategy to finetune the temporal modules so that we can generate videos with both customized subjects and motions. To evaluate the performance of customized multi-subject text-to-video generation, we introduce the MultiStudioBench benchmark. Extensive experiments demonstrate the remarkable ability of VideoDreamer to generate videos with new content such as new events and backgrounds, tailored to the customized multiple subjects.},
  archive      = {J_TMM},
  author       = {Hong Chen and Xin Wang and Guanning Zeng and Yipeng Zhang and Yuwei Zhou and Feilin Han and Yaofei Wu and Wenwu Zhu},
  doi          = {10.1109/TMM.2025.3557634},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2875-2885},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {VideoDreamer: Customized multi-subject text-to-video generation with disen-mix finetuning on language-video foundation models},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward chinese food understanding: A cross-modal ingredient-level benchmark. <em>TMM</em>, <em>27</em>, 2863-2874. (<a href='https://doi.org/10.1109/TMM.2024.3387735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although there are several food-level benchmarks for food-related learning, the lack of fine-grained ingredient annotation significantly impedes progress in food scene understanding. In this study, we focus on Chinese food understanding which involves fine-grained ingredient detection and cross-modal ingredient retrieval. Specifically, to support studies on Chinese food understanding, we build the first cross-modal ingredient-level dataset called CMIngre, which contains 8,001 image-text pairs from three different sources, i.e. dishes, recipes, and user-generated content, covering 429 distinct ingredients and 95,290 bounding boxes. Based on CMIngre, we evaluate the performance of traditional CNN-based detection algorithms and transformer-based pre-trained large models for ingredient detection. We also propose baseline methods for the cross-modal ingredient retrieval task in both the end-to-end and two-stage settings. Extensive experiments on CMIngre demonstrate the effectiveness of our proposed methods on food understanding.},
  archive      = {J_TMM},
  author       = {Lanjun Wang and Chenyu Zhang and An-An Liu and Bo Yang and Mingwang Hu and Xinran Qiao and Lei Wang and Jianlin He and Qiang Liu},
  doi          = {10.1109/TMM.2024.3387735},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2863-2874},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Toward chinese food understanding: A cross-modal ingredient-level benchmark},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Threefold encoder interaction: Hierarchical multi-grained semantic alignment for cross-modal food retrieval. <em>TMM</em>, <em>27</em>, 2848-2862. (<a href='https://doi.org/10.1109/TMM.2025.3543067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current cross-modal food retrieval approaches focus mainly on the global visual appearance of food without explicitly considering multi-grained information. Additionally, direct calculation of the global similarity of image-recipe pairs is not particularly effective in terms of latent alignment, which suffers from mismatch during the mutual image-recipe retrieval process. This paper proposes a threefold encoder interaction (TEI) cross-modal food retrieval framework to maintain the multi-granularity of food images and the multi-levels of textual recipes to address the aforementioned challenges. The TEI framework comprises an image encoder, a recipe encoder, and a multi-grained interaction encoder. We simultaneously propose a multi-grained relation-aware attention (MRA) embedded in the multi-grained interaction encoder to capture multi-grained food visual features. The multi-grained interaction similarity scores are calculated to better establish the multi-grained correlation between recipe and image entities based on the extracted hierarchical textual and multi-grained visual features. Finally, a hierarchical multi-grained semantic alignment loss is designed to supervise the whole process of cross-modal training using the multi-grained interaction similarity scores. Extensive qualitative and quantitative experiments on the Recipe1M dataset have demonstrated that the proposed TEI framework achieves multi-grained semantic alignment between image and text modalities and is superior to other state-of-the-art methods in cross-modal food retrieval tasks.},
  archive      = {J_TMM},
  author       = {Qi Wang and Dong Wang and Weidong Min and Di Gai and Qing Han and Cheng Zha and Yuling Zhong},
  doi          = {10.1109/TMM.2025.3543067},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2848-2862},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Threefold encoder interaction: Hierarchical multi-grained semantic alignment for cross-modal food retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PGD-GP: A chinese named entity recognition model for constructing food safety standard knowledge graph. <em>TMM</em>, <em>27</em>, 2836-2847. (<a href='https://doi.org/10.1109/TMM.2024.3373249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive range of food safety standards poses a significant challenge to efficiently accessing specific information within this domain, necessitating innovative solutions to streamline the process. In response, researchers are focusing on constructing a knowledge graph based on food safety standards to facilitate efficient associative querying. Named entity recognition is a pivotal element in this endeavor due to its critical impact on the accuracy and quality of the knowledge graph. To address the nuanced challenges of accurately identifying nested entity boundaries and rectifying entity class imbalances in food safety standards, we present PGD-GP, a novel Chinese named entity recognition model. This model is based on Projected Gradient Descent for adversarial training and Global Pointer. The model innovatively refines the Chinese Bert model at the encoding layer, employing the adversarial training method PGD to iteratively introduce perturbations to character vectors, thereby significantly enhancing the model's robustness and adaptability to texts. The decoding layer leverages Global Pointer to accurately determine dependencies and relative positional relationships between characters, thus facilitating more precise recognition of entity boundaries. To combat the issue of class imbalance, Circle Loss is utilized as the loss function. We developed and annotated the Food Safety Standard Dataset using a specifically tailored ontology rule for food safety standards. Comparative experiments conducted on the Food Safety Standard Dataset and the public Resume dataset demonstrate that PGD-GP surpasses six mainstream baseline models in performance, thereby validating the effectiveness and robustness of PGD-GP. Building upon the foundation of PGD-GP and the Food Safety Standard Dataset, we implemented a prototype system that integrates a food safety standard-based knowledge graph with associated queries. This system serves as an efficient, accurate, and comprehensive intelligent assistant, enabling researchers to effectively acquire food safety standard information.},
  archive      = {J_TMM},
  author       = {Yi Chen and Qiuxu Fan and Xianpeng Yuan and Qinghui Zhang and Yu Dong},
  doi          = {10.1109/TMM.2024.3373249},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2836-2847},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PGD-GP: A chinese named entity recognition model for constructing food safety standard knowledge graph},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust visual food recognition for enriching nutrition knowledge bases. <em>TMM</em>, <em>27</em>, 2825-2835. (<a href='https://doi.org/10.1109/TMM.2025.3542962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring nutrition information and health-related knowledge about food is a common need among individuals. However, using conventional food names as search queries often fails to yield accurate matches to entries within food nutrition knowledge bases (FoodnKB), which frequently utilize scientific or product names. In this study, we present a method for enriching FoodnKB entries with imagery and facilitating visual access to food-related knowledge through image recognition. We start with an official food nutrition database and propose a consensus-based approach using Large Language Models to identify visually discernible and directly edible foods, expanding food synonyms and harnessing diverse web-based food images for comprehensive visual representation. To minimize manual annotation of noisy web images, we introduce a cyclic training-based area under the margin metric (cAUM) approach that effectively distinguishes appropriate images, including rare instances, from noisy ones. Additionally, we design a generic accuracy gap (AccGap) algorithm to automatically estimate the noise ratio of the web-harnessed data. Our integrated cAUM and AccGap method demonstrates superior performance in noise detection and enhancement of image recognition accuracy compared to existing noise-robust frameworks. Furthermore, we successfully apply the visually enriched FoodnKB and food recognition capabilities within a smart nutritionist mobile application.},
  archive      = {J_TMM},
  author       = {Zhaoyan Ming and Zeyu Xie and Chao Zhang and Kui Su and Changzheng Yuan and Tat-Seng Chua},
  doi          = {10.1109/TMM.2025.3542962},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2825-2835},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust visual food recognition for enriching nutrition knowledge bases},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High throughput shelf life determination of atlantic cod (Gadus morhua l.) by use of hyperspectral imaging. <em>TMM</em>, <em>27</em>, 2809-2824. (<a href='https://doi.org/10.1109/TMM.2025.3561661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fish quality and shelf life can be evaluated using various assessment methods, such as sensory analysis, biochemical tests, microbiological evaluations, and physicochemical analyses. However, these methods are invasive and time-consuming, driving interest in technologies capable of estimating shelf life through non-invasive procedures. This study investigates the potential of hyperspectral imaging as a non-invasive technology for predicting the shelf life of Atlantic cod. A storage experiment was conducted that included both gutted fish with heads (GFWH) and fillets, with sensory evaluation and biochemical measurements employed to determine shelf life. Subsequently, hyperspectral images of the fish samples were captured under industrial production conditions, and the spectral data were analyzed using different regression algorithms. The majority of the regression techniques utilized in this research successfully predicted shelf life for both fillets and GFWH, achieving a root mean square error (RMSE) lower than one day. While most regression models exhibited comparable performance in predicting the shelf life of fillets, deep learning-based models demonstrated superior performance for GFWH. These results suggest that hyperspectral imaging technology has significant potential as a non-invasive tool for estimating the shelf life of Atlantic cod, thereby enabling effective quality-based sorting, reducing food waste, and enhancing sustainability in the seafood supply chain.},
  archive      = {J_TMM},
  author       = {Samuel Ortega and Tatiana N. Ageeva and Silje Kristoffersen and Karsten Heia and Heidi A. Nilsen},
  doi          = {10.1109/TMM.2025.3561661},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2809-2824},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {High throughput shelf life determination of atlantic cod (Gadus morhua l.) by use of hyperspectral imaging},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FoodSAM: Any food segmentation. <em>TMM</em>, <em>27</em>, 2795-2808. (<a href='https://doi.org/10.1109/TMM.2023.3330047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we explore the zero-shot capability of the Segment Anything Model (SAM) for food image segmentation. To address the lack of class-specific information in SAM-generated masks, we propose a novel framework, called FoodSAM. This innovative approach integrates the coarse semantic mask with SAM-generated masks to enhance semantic segmentation quality. Besides, we recognize that the ingredients in food can be supposed as independent individuals, which motivated us to perform instance segmentation on food images. Furthermore, FoodSAM extends its zero-shot capability to encompass panoptic segmentation by incorporating an object detector, which renders FoodSAM to effectively capture non-food object information. Drawing inspiration from the recent success of promptable segmentation, we also extend FoodSAM to promptable segmentation, supporting various prompt variants. Consequently, FoodSAM emerges as an all-encompassing solution capable of segmenting food items at multiple levels of granularity. Remarkably, this pioneering framework stands as the first-ever work to achieve instance, panoptic, and promptable segmentation on food images. Extensive experiments demonstrate the feasibility and impressing performance of FoodSAM, validating SAM's potential as a prominent and influential tool within the domain of food image segmentation.},
  archive      = {J_TMM},
  author       = {Xing Lan and Jiayi Lyu and Hanyu Jiang and Kun Dong and Zehai Niu and Yi Zhang and Jian Xue},
  doi          = {10.1109/TMM.2023.3330047},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2795-2808},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FoodSAM: Any food segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal recipe retrieval with fine-grained prompting alignment and evidential semantic consistency. <em>TMM</em>, <em>27</em>, 2783-2794. (<a href='https://doi.org/10.1109/TMM.2024.3384672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alignment between the food images and the corresponding recipes is an emerging cross-modal representation learning task. In this task, the recipes are composed of three components, i.e., food title, ingredient lists, and cooking instructions, which require a fine-grained alignment between the features of the two modalities. Existing methods usually aggregate the recipes into global embeddings and then align them with the global image embeddings. Meanwhile, semantic classification is frequently used in these methods to regularize the embeddings of the two modalities. While these methods are efficient, there remain two problems. (1) Forcing the alignment between the global images and recipes embeddings may result in losing the component-specific information. (2) The high diversity of food appearance leads to high uncertainty in the semantic classification of food images and recipes. To solve these problems, we propose a Fine-grained Prompting and Alignment (FPA) model to enhance the feature extraction and bring more component-specific information for fine-grained alignment. Furthermore, to regularize the semantic information contained in the cross-modal features, we design an Evidential Semantic Consistency (ESC) loss to keep the cross-modal semantic consistency. We have conducted comprehensive experiments on the benchmark dataset Recipe1M and the state-of-the-art results on the cross-modal recipe retrieval task demonstrate the effectiveness of our method.},
  archive      = {J_TMM},
  author       = {Xu Huang and Jin Liu and Zhizhong Zhang and Yuan Xie and Yongqiang Tang and Wensheng Zhang and Xiaohui Cui},
  doi          = {10.1109/TMM.2024.3384672},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2783-2794},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal recipe retrieval with fine-grained prompting alignment and evidential semantic consistency},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CookGALIP: Recipe controllable generative adversarial CLIPs with sequential ingredient prompts for food image generation. <em>TMM</em>, <em>27</em>, 2772-2782. (<a href='https://doi.org/10.1109/TMM.2024.3377540'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating food images from recipes is a challenging task in food analysis, as recipes contain lengthy texts far beyond the semantic information in food images, making it difficult to align the features of two modalities. Existing studies usually concatenate the representations of ingredients and cooking instructions directly, and use the concatenated representations to generate food images through generative adversarial networks (GANs). However, previous models generally ignore the sequential information contained in complicated procedural instructions, which leads to semantic inconsistency between recipes and generated food images. Furthermore, it is still difficult for current models to distinguish and control fine-grained features, causing the entangled ingredient features in food images. To this end, we propose CookGALIP, which strengthens semantic consistency and controllability for food image generation. Based on the recently proposed text-to-image framework GALIP, two modules are specially designed. 1) To incorporate the sequential relationships into the food image generation process, we propose a Recipe Fusion Module (RFM) to fuse the semantics of cooking instructions, so as to balance the semantic complexity between modalities and improve the semantic consistency of recipes and generated food images. 2) To distinguish and control the fine-grained ingredient features, we introduce the Ingredient Control Module (ICM) to generate sequential ingredient prompts, which enables more refined control over the recipe-to-food synthesis process. Experimental results on Recipe1M and Vireo Food-172 datasets show that the proposed model outperforms the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Mengling Xu and Jie Wang and Ming Tao and Bing-Kun Bao and Changsheng Xu},
  doi          = {10.1109/TMM.2024.3377540},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2772-2782},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CookGALIP: Recipe controllable generative adversarial CLIPs with sequential ingredient prompts for food image generation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A progressively-passing-then-disentangling approach to recipe recommendation. <em>TMM</em>, <em>27</em>, 2760-2771. (<a href='https://doi.org/10.1109/TMM.2024.3373255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing popularity of online food blogs and food ordering services has made personalized recipe recommendation a vital aspect of our emotional well-being. However, existing solutions, mainly based on graph neural networks, still face significant challenges, such as (a) focusing on exploiting the user-recipe interactions while neglecting other crucial pairwise and high-order relationships, and (b) failing to explicitly distinguish the distinct factors, e.g., hedonic and healthy, that influence recipe selection. To address these issues, we propose a progressively-passing-then-disentangling approach named P2D. Our approach utilizes a three-stage progressive message-passing mechanism for better representation learning. Specifically, we incorporate the extra pairwise relationships between recipes and nutrients, ingredients, and visual contents to create fine-grained and multimodal recipe representations. We next refine these representations via message passing between high-order recipe relationships to learn people's shared food preferences. Based on them, we could derive comprehensive user representations, which are subsequently transformed into disentangled forms that correspond to various decision factors through contrastive and mutual information regularization. Experimental results demonstrate both the superiority and the rationality of our method: (a) P2D outperforms the state-of-the-art recipe recommendation methods by a large margin under various metrics, (b) ablation studies confirm the positive impact of each of its components, and (c) our visualization analysis empirically supports the advantage of explicitly disentangling decision factors.},
  archive      = {J_TMM},
  author       = {Chunlai Dong and Haochao Ying and Renjun Hu and Yuyang Xu and Jintai Chen and Fuzhen Zhuang and Jian Wu},
  doi          = {10.1109/TMM.2024.3373255},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2760-2771},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A progressively-passing-then-disentangling approach to recipe recommendation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust multi-graph contrastive network for incomplete multi-view clustering. <em>TMM</em>, <em>27</em>, 2747-2759. (<a href='https://doi.org/10.1109/TMM.2023.3347639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food categorization is pivotal in numerous aspects of everyday life, assisting in the selection of food, managing diets, and addressing essential survival requirements. By leveraging the complementary information of various views, multi-view learning usually achieves superior performance compared to the single-view learning methods. However, characterized by the unrestrained openness of internet platforms and potential inconsistencies in food data collection processes, multi-view features often suffer from data loss, resulting in incomplete multi-view food data. Conventional multi-view clustering methods often falter in effectively capitalizing on the diverse correlations contained in food data, and exhibit limitations in dealing with the noise and irregularities pervading different views. Addressing these challenges, this paper presents the Robust Multi-Graph Contrastive network (RMGC) for multi-view food clustering. RMGC artfully combines multi-view representation learning with multi-graph contrastive regularization, creating a cohesive framework to manage incomplete multi-view data. By developing a multi-view encoding network, RMGC seamlessly blends various views into a cohesive representation, astutely assessing the significance of each view. More importantly, the proposed robust multi-graph contrastive regularization enhances the precision of the learned representation and successfully counteracts the noise and unreliability in multi-view data. The experiments conducted across several multi-view datasets manifest the effectiveness of RMGC, showing its superiority over existing methods. Our method not only making an advancement in food categorization but also contributes to the broader field of multi-view learning, offering innovative solutions for handling incomplete and noisy multi-view data.},
  archive      = {J_TMM},
  author       = {Zhe Xue and Yawen Li and Zhongchao Guan and Wenling Li and Meiyu Liang and Hai Zhou},
  doi          = {10.1109/TMM.2023.3347639},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2747-2759},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust multi-graph contrastive network for incomplete multi-view clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale spiking pyramid wireless communication framework for food recognition. <em>TMM</em>, <em>27</em>, 2734-2746. (<a href='https://doi.org/10.1109/TMM.2024.3368964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food recognition applications in human health have recently garnered significant attention in the field of computer vision. With the advancement of mobile devices, robust food recognition in wireless communication has become a practical and challenging application scenario. We propose a novel Multi-scale Spiking Pyramid Transmission Network (MSPTN) to tackle this challenge. The MSPTN learns diverse and complementary local and global feature maps simultaneously, generating a comprehensive description of food images that capture the correlations of feed-specific features. The feature sender uses a three-layer Spiking Neural Network (SNN). The proposed sender compresses features into sparse and discrete spike trains, significantly reducing the required transmission bandwidth and improving channel utilization and energy efficiency. Our model introduces the Compressed Factorized Bilinear block (CFB), which employs a low-rank feature approximation to reduce computational complexity and feature transmission volume while preserving the discriminate features. The enhancement reasoning module is proposed to enhance the received features by projecting them into a higher-dimensional space and utilizing the self-attention mechanism and sum pooling to compress them back to the original dimension. We conduct extensive experiments on the ETH Food-101 and Food2k datasets. Our results reveal that the MSPTN demonstrates state-of-the-art recognition performance, even with binary spike trains. Meanwhile, the MSPTN also exhibits remarkable robustness in wireless communication scenarios. With the combination of CFB, SNN, and EFB, our model achieves significant efficiency gains, including a nearly nine-fold decrease in feature transmission volume and a three-fold improvement in runtime & computational memory speed.},
  archive      = {J_TMM},
  author       = {Wenrui Li and Jiahui Li and Mengyao Ma and Xiaopeng Hong and Xiaopeng Fan},
  doi          = {10.1109/TMM.2024.3368964},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2734-2746},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale spiking pyramid wireless communication framework for food recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From canteen food to daily meals: Generalizing food recognition to more practical scenarios. <em>TMM</em>, <em>27</em>, 2724-2733. (<a href='https://doi.org/10.1109/TMM.2024.3371212'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precise recognition of food categories plays a pivotal role for intelligent health management, attracting significant research attention in recent years. Prominent benchmarks, such as Food-101 and VIREO Food-172, provide abundant food image resources that catalyze the prosperity of research in this field. Nevertheless, these datasets are well-curated from canteen scenarios and thus deviate from food appearances in daily life. This discrepancy poses great challenges in effectively transferring classifiers trained on these canteen datasets to broader daily-life scenarios encountered by humans. Toward this end, we present two new benchmarks, namely DailyFood-172 and DailyFood-16, specifically designed to curate food images from everyday meals. These two datasets are used to evaluate the transferability of approaches from the well-curated food image domain to the everyday-life food image domain. In addition, we also propose a simple yet effective baseline method named Multi-Cluster Reference Learning (MCRL) to tackle the aforementioned domain gap. MCRL is motivated by the observation that food images in daily-life scenarios exhibit greater intra-class appearance variance compared with those in well-curated benchmarks. Notably, MCRL can be seamlessly coupled with existing approaches, yielding non-trivial performance enhancements. We hope our new benchmarks can inspire the community to explore the transferability of food recognition models trained on well-curated datasets toward practical real-life applications.},
  archive      = {J_TMM},
  author       = {Guoshan Liu and Yang Jiao and Jingjing Chen and Bin Zhu and Yu-Gang Jiang},
  doi          = {10.1109/TMM.2024.3371212},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2724-2733},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {From canteen food to daily meals: Generalizing food recognition to more practical scenarios},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ESE-GAN: Zero-shot food image classification based on low dimensional embedding of visual features. <em>TMM</em>, <em>27</em>, 2713-2723. (<a href='https://doi.org/10.1109/TMM.2024.3353457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing zero-shot learning based image classification methods transform the zero-shot learning problem into supervised learning by applying generative adversarial network (GAN) to synthesize visual features of unseen classes. However, the visual features generated by the generator tend to be biased towards seen classes, and the discriminator is too weak to generate high-quality image features. To solve these problems, we propose a novel zero-shot food image classification method based on low dimensional embedding of visual features. Our method applies reinforced semantic guidance to increase the discriminative ability of the model by enhancing the strong distribution of input features. Moreover, the visual space is utilized as the embedding space to reduce the bias towards seen classes by reducing the distance between semantic information and visual features in the embedding space. Finally, the feature distribution of unseen classes is further specified by improving the prototype similarity function. Extensive experiments on three food datasets and four general benchmark datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TMM},
  author       = {Gaojie Li and Yaochen Li and Jingle Liu and Wei Guo and Wenneng Tang and Yuehu Liu},
  doi          = {10.1109/TMM.2024.3353457},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2713-2723},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ESE-GAN: Zero-shot food image classification based on low dimensional embedding of visual features},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: When multimedia meets food: Multimedia computing for food data analysis and applications. <em>TMM</em>, <em>27</em>, 2708-2712. (<a href='https://doi.org/10.1109/TMM.2025.3566452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TMM},
  author       = {Weiqing Min and Shuqiang Jiang and Petia Radeva and Vladimir Pavlovic and Chong-Wah Ngo and Kiyoharu Aizawa and Wanqing Li},
  doi          = {10.1109/TMM.2025.3566452},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2708-2712},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Guest editorial: When multimedia meets food: Multimedia computing for food data analysis and applications},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subjective and objective quality assessment of non-uniformly distorted omnidirectional images. <em>TMM</em>, <em>27</em>, 2695-2707. (<a href='https://doi.org/10.1109/TMM.2025.3535372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional image quality assessment (OIQA) has been one of the hot topics in IQA with the continuous development of VR techniques, and achieved much success in the past few years. However, most studies devote themselves to the uniform distortion issue, i.e., all regions of an omnidirectional image are perturbed by the “same amount” of noise, while ignoring the non-uniform distortion issue, i.e., partial regions undergo “different amount” of perturbation with the other regions in the same omnidirectional image. Additionally, nearly all OIQA models are verified on the platforms containing a limited number of samples, which largely increases the over-fitting risk and therefore impedes the development of OIQA. To alleviate these issues, we elaborately explore this topic from both subjective and objective perspectives. Specifically, we construct a large OIQA database containing 10,320 non-uniformly distorted omnidirectional images, each of which is generated by considering quality impairments on one or two camera len(s). Then we meticulously conduct psychophysical experiments and delve into the influence of both holistic and individual factors (i.e., distortion range and viewing condition) on omnidirectional image quality. Furthermore, we propose a perception-guided OIQA model for non-uniform distortion by adaptively simulating users' viewing behavior. Experimental results demonstrate that the proposed model outperforms state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Jiebin Yan and Jiale Rao and Xuelin Liu and Yuming Fang and Yifan Zuo and Weide Liu},
  doi          = {10.1109/TMM.2025.3535372},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2695-2707},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Subjective and objective quality assessment of non-uniformly distorted omnidirectional images},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving visual object tracking through visual prompting. <em>TMM</em>, <em>27</em>, 2682-2694. (<a href='https://doi.org/10.1109/TMM.2025.3535323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a discriminative model to distinguish a target from its surrounding distractors is essential to generic visual object tracking. Dynamic target representation adaptation against distractors is challenging due to the limited discriminative capabilities of prevailing trackers. We present a new visual Prompting mechanism for generic Visual Object Tracking (PiVOT) to address this issue. PiVOT proposes a prompt generation network with the pre-trained foundation model CLIP to automatically generate and refine visual prompts, enabling the transfer of foundation model knowledge for tracking. While CLIP offers broad category-level knowledge, the tracker, trained on instance-specific data, excels at recognizing unique object instances. Thus, PiVOT first compiles a visual prompt highlighting potential target locations. To transfer the knowledge of CLIP to the tracker, PiVOT leverages CLIP to refine the visual prompt based on the similarities between candidate objects and the reference templates across potential targets. Once the visual prompt is refined, it can better highlight potential target locations, thereby reducing irrelevant prompt information. With the proposed prompting mechanism, the tracker can generate improved instance-aware feature maps through the guidance of the visual prompt, thus effectively reducing distractors. The proposed method does not involve CLIP during training, thereby keeping the same training complexity and preserving the generalization capability of the pretrained foundation model. Extensive experiments across multiple benchmarks indicate that PiVOT, using the proposed prompting method can suppress distracting objects and enhance the tracker.},
  archive      = {J_TMM},
  author       = {Shih-Fang Chen and Jun-Cheng Chen and I-Hong Jhuo and Yen-Yu Lin},
  doi          = {10.1109/TMM.2025.3535323},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2682-2694},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Improving visual object tracking through visual prompting},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STBA: Towards evaluating the robustness of DNNs for query-limited black-box scenario. <em>TMM</em>, <em>27</em>, 2666-2681. (<a href='https://doi.org/10.1109/TMM.2025.3535328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive studies have revealed that deep neural networks (DNNs) are vulnerable to adversarial attacks, especially black-box ones, which can heavily threaten the DNNs deployed in the real world. Many attack techniques have been proposed to explore the vulnerability of DNNs and further help to improve their robustness. Despite the significant progress made recently, existing black-box attack methods still suffer from unsatisfactory performance due to the vast number of queries needed to optimize desired perturbations. Besides, the other critical challenge is that adversarial examples built in a noise-adding manner are abnormal and struggle to successfully attack robust models, whose robustness is enhanced by adversarial training against small perturbations. There is no doubt that these two issues mentioned above will significantly increase the risk of exposure and result in a failure to dig deeply into the vulnerability of DNNs. Hence, it is necessary to evaluate DNNs' fragility sufficiently under query-limited settings in a non-additional way. In this paper, we propose the Spatial Transform Black-box Attack (STBA), a novel framework to craft formidable adversarial examples in the query-limited scenario. Specifically, STBA introduces a flow field to the high-frequency part of clean images to generate adversarial examples and adopts the following two processes to enhance their naturalness and significantly improve the query efficiency: a) we apply an estimated flow field to the high-frequency part of clean images to generate adversarial examples instead of introducing external noise to the benign image, and b) we leverage an efficient gradient estimation method based on a batch of samples to optimize such an ideal flow field under query-limited settings. Compared to existing score-based black-box baselines, extensive experiments indicated that STBA could effectively improve the imperceptibility of the adversarial examples and remarkably boost the attack success rate under query-limited settings.},
  archive      = {J_TMM},
  author       = {Renyang Liu and Kwok-Yan Lam and Wei Zhou and Sixing Wu and Jun Zhao and Dongting Hu and Mingming Gong},
  doi          = {10.1109/TMM.2025.3535328},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2666-2681},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {STBA: Towards evaluating the robustness of DNNs for query-limited black-box scenario},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Listen with seeing: Cross-modal contrastive learning for audio-visual event localization. <em>TMM</em>, <em>27</em>, 2650-2665. (<a href='https://doi.org/10.1109/TMM.2025.3535359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world physiological and psychological scenarios, there often exists a robust complementary correlation between audio and visual signals. Audio-Visual Event Localization (AVEL) aims to identify segments with Audio-Visual Events (AVEs) that contain both audio and visual tracks in unconstrained videos. Prior studies have predominantly focused on audio-visual cross-modal fusion methods, overlooking the fine-grained exploration of the cross-modal information fusion mechanism. Moreover, due to the inherent heterogeneity of multi-modal data, inevitable new noise is introduced during the audio-visual fusion process. To address these challenges, we propose a novel Cross-modal Contrastive Learning Network (CCLN) for AVEL, comprising a backbone network and a branch network. In the backbone network, drawing inspiration from physiological theories of sensory integration, we elucidate the process of audio-visual information fusion, interaction, and integration from an information-flow perspective. Notably, the Self-constrained Bi-modal Interaction (SBI) module is a bi-modal attention structure integrated with audio-visual fusion information, and through gated processing of the audio-visual correlation matrix, it effectively captures inter-modal correlation. The Foreground Event Enhancement (FEE) module emphasizes the significance of event-level boundaries by elongating the distance between scene events during training through adaptive weights. Furthermore, we introduce weak video-level labels to constrain the cross-modal semantic alignment of audio-visual events and design a weakly supervised cross-modal contrastive learning loss (WCCL Loss) function, which enhances the quality of fusion representation in the dual-branch contrastive learning framework. Extensive experiments conducted on the AVE dataset for both fully supervised and weakly supervised event localization, as well as Cross-Modal Localization (CML) tasks, demonstrate the superior performance of our model compared to state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Chao Sun and Min Chen and Chuanbo Zhu and Sheng Zhang and Ping Lu and Jincai Chen},
  doi          = {10.1109/TMM.2025.3535359},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2650-2665},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Listen with seeing: Cross-modal contrastive learning for audio-visual event localization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Debiased mapping for full-reference image quality assessment. <em>TMM</em>, <em>27</em>, 2638-2649. (<a href='https://doi.org/10.1109/TMM.2025.3535280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ideal full-reference image quality (FR-IQA) model should exhibit both high separability for images with different quality and compactness for images with the same or indistinguishable quality. However, existing learning-based FR-IQA models that directly compare images in deep-feature space, usually overly emphasize the quality separability, neglecting to maintain the compactness when images are of similar quality. In our work, we identify that the perception bias mainly stems from an inappropriate subspace where images are projected and compared. For this issue, we propose a Debiased Mapping based quality Measure (DMM), leveraging orthonormal bases formed by singular value decomposition (SVD) in the deep features domain. The SVD effectively decomposes the quality variations into singular values and mapping bases, enabling quality inference with more reliable feature difference measures. Extensive experimental results reveal that our proposed measure could mitigate the perception bias effectively and demonstrates excellent quality prediction performance on various IQA datasets.},
  archive      = {J_TMM},
  author       = {Baoliang Chen and Hanwei Zhu and Lingyu Zhu and Shanshe Wang and Jingshan Pan and Shiqi Wang},
  doi          = {10.1109/TMM.2025.3535280},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2638-2649},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Debiased mapping for full-reference image quality assessment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical context measurement network for single hyperspectral image super-resolution. <em>TMM</em>, <em>27</em>, 2623-2637. (<a href='https://doi.org/10.1109/TMM.2025.3535371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single hyperspectral image super-resolution aims to enhance the spatial resolution of a hyperspectral image without relying on any auxiliary information. Despite the abundant spectral information, the inherent high-dimensionality in hyperspectral images still remains a challenge for memory efficiency. Recently, recursion-based methods have been proposed to reduce memory requirements. However, these methods utilize the reconstruction features as feedback embedding to explore context information, leading to sub-optimal performance as they ignore the complementarity of different hierarchical levels of information in the context. Additionally, existing methods equivalently compensate the previous feedback information to the current band, resulting in an indistinct and untargeted introduction of the context. In this paper, we propose a hierarchical context measurement network to construct corresponding measurement strategies for different hierarchical information, capturing comprehensive and powerful complementary knowledge from the context. Specifically, a feature-wise similarity measurement module is designed to calculate global cross-layer relationships between the middle features of the current band and those of the context, so as to explore the embedded middle features discriminatively through generated global dependencies. Furthermore, considering the pixel-wise correspondence between the reconstruction features and the super-resolved results, we propose a pixel-wise similarity measurement module for the complementary reconstruction features embedding, exploring detailed complementary information within the embedded reconstruction features by dynamically generating a spatially adaptive filter for each pixel. Experimental results reported on three benchmark hyperspectral datasets reveal that the proposed method outperforms other state-of-the-art peers in both visual and metric evaluations.},
  archive      = {J_TMM},
  author       = {Heng Wang and Cong Wang and Yuan Yuan},
  doi          = {10.1109/TMM.2025.3535371},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2623-2637},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical context measurement network for single hyperspectral image super-resolution},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relighting from a single image: Datasets and deep intrinsic-based architecture. <em>TMM</em>, <em>27</em>, 2608-2622. (<a href='https://doi.org/10.1109/TMM.2025.3535397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image scene relighting aims to generate a realistic new version of an input image so that it appears to be illuminated by a new target light condition. Although existing works have explored this problem from various perspectives, generating relit images under arbitrary light conditions remains highly challenging, and related datasets are scarce. Our work addresses this problem from both the dataset and methodological perspectives. We propose two new datasets: a synthetic dataset with the ground truth of intrinsic components and a real dataset collected under laboratory conditions. These datasets alleviate the scarcity of existing datasets. To incorporate physical consistency in the relighting pipeline, we establish a two-stage network based on intrinsic decomposition, giving outputs at intermediate steps, thereby introducing physical constraints. When the training set lacks ground truth for intrinsic decomposition, we introduce an unsupervised module to ensure that the intrinsic outputs are satisfactory. Our method outperforms the state-of-the-art methods in performance, as tested on both existing datasets and our newly developed datasets. Furthermore, pretraining our method or other prior methods using our synthetic dataset can enhance their performance on other datasets. Since our method can accommodate any light conditions, it is capable of producing animated results.},
  archive      = {J_TMM},
  author       = {Yixiong Yang and Hassan Ahmed Sial and Ramon Baldrich and Maria Vanrell},
  doi          = {10.1109/TMM.2025.3535397},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2608-2622},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Relighting from a single image: Datasets and deep intrinsic-based architecture},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MISF-net: Modality-invariant and -specific fusion network for RGB-T crowd counting. <em>TMM</em>, <em>27</em>, 2593-2607. (<a href='https://doi.org/10.1109/TMM.2025.3535330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accurately perform crowd counting, utilizing the complementary relationship between RGB and thermal images to analyze the crowd has become the focus of current research. Due to different imaging principles, multi-modal images often contain different contents, which are their modality-specific information. For example, RGB images contain more texture and color details, while thermal images contain thermal radiation information. Meanwhile, they also describe the same target content, e.g., crowds, which are modality-invariant. However, existing methods only design different modules to directly fuse RGB and thermal image features, which did not fully consider the above facts. In this paper, by analyzing the similarities and differences between multi-modal images, we propose a Modality-Invariant and -Specific Fusion Network (MISF-Net) for RGB-T Crowd Counting. Specifically, we design a modality decomposition and fusion module (MDFM), which decomposes RGB and thermal image features into modality-invariant and -specific features by using the similarity and difference supervision between multi-modal features. Besides, reconstruction supervision is also used to prevent network learning from generating bias. After that, different fusion strategies are applied to the invariant and specific features, respectively. In addition, to adapt to the variations in size of different pedestrians, we design a modality-invariant fusion module (MIFM). Finally, after the fusion decoder, MISF-Net can obtain a more accurate crowd density map. Comprehensive experiments on the RGB-T crowd counting dataset show that our MISF-Net can achieve competitive performance.},
  archive      = {J_TMM},
  author       = {Baoyang Mu and Feng Shao and Zhengxuan Xie and Hangwei Chen and Zhongjie Zhu and Qiuping Jiang},
  doi          = {10.1109/TMM.2025.3535330},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2593-2607},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MISF-net: Modality-invariant and -specific fusion network for RGB-T crowd counting},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing distributed source coding with encoder-centric frequency adaptation and spatial transformation. <em>TMM</em>, <em>27</em>, 2582-2592. (<a href='https://doi.org/10.1109/TMM.2024.3521700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methodologies in distributed source coding have predominantly investigated decoder-focused strategies, emphasizing the alignment and exploitation of side information. This study introduces a paradigm shift by presenting an encoder-centric algorithm that conducts proactive optimization in the frequency domain. This shift is motivated by the current deep learning models' tendency to passively extract high-frequency elements, such as contours and content in the spatial domain at the encoder side, without considering the frequency characteristics of these spatial components. Unlike current trends, the proposed scheme actively selects the essential frequency components directly in the frequency domain by introducing an adaptive self-learning filter, enabling the encoder to discern and retain critical frequency components effectively and precisely. Furthermore, we align the side information in the spatial domain before feature extraction and implement an affine transformation-based alignment strategy to utilize the side information better. By leveraging the shared frequency domain components of the image pairs, the proposed algorithm adeptly learns affine coefficients to accomplish precise spatial alignment. This dual strategy of proactive encoder optimization and decoder alignment via affine transformations is highly efficient, outperforming existing state-of-the-art methods in distributed source coding when tested across two diverse datasets by an average of 0.5 dB in PSNR.},
  archive      = {J_TMM},
  author       = {Hao Xu and Bin Tan and Yihao Chen and Die Hu and Jun Wu},
  doi          = {10.1109/TMM.2024.3521700},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2582-2592},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing distributed source coding with encoder-centric frequency adaptation and spatial transformation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PMMTalk$:$ speech-driven 3D facial animation from complementary pseudo multi-modal features. <em>TMM</em>, <em>27</em>, 2570-2581. (<a href='https://doi.org/10.1109/TMM.2024.3521701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech-driven 3D facial animation has improved a lot recently while most related works only utilize acoustic modality and neglect the influence of visual and textual cues, leading to unsatisfactory results in terms of precision and coherence. We argue that visual and textual cues are not trivial information. Therefore, we present a novel framework, namely PMMTalk, using complementary Pseudo Multi-Modal features for improving the accuracy of facial animation. The framework entails three modules: PMMTalk encoder, cross-modal alignment module, and PMMTalk decoder. Specifically, the PMMTalk encoder employs the off-the-shelf talking head generation architecture and speech recognition technology to extract visual and textual information from speech, respectively. Following this, the cross-modal alignment module aligns the audio-image-text features at temporal and semantic levels. Subsequently, the PMMTalk decoder is employed to predict lip-syncing facial blendshape coefficients. Contrary to prior methods, PMMTalk only requires an additional random reference face image but yields more accurate results. Additionally, it is artist-friendly as it seamlessly integrates into standard animation production workflows by introducing facial blendshape coefficients. Finally, given the scarcity of 3D talking face datasets, we introduce a large-scale 3D Chinese Audio-Visual Facial Animation (3D-CAVFA) dataset. Extensive experiments and user studies show that our approach outperforms the state of the art. Codes and datasets are available at PMMTalk.},
  archive      = {J_TMM},
  author       = {Tianshun Han and Shengnan Gui and Yiqing Huang and Baihui Li and Lijian Liu and Benjia Zhou and Ning Jiang and Quan Lu and Ruicong Zhi and Yanyan Liang and Du Zhang and Jun Wan},
  doi          = {10.1109/TMM.2024.3521701},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2570-2581},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PMMTalk$:$ speech-driven 3D facial animation from complementary pseudo multi-modal features},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal hierarchical knowledge distillation for image aesthetics assessment. <em>TMM</em>, <em>27</em>, 2556-2569. (<a href='https://doi.org/10.1109/TMM.2024.3521765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of image aesthetics assessment (IAA) is rapidly advancing due to its wide applications. However, relying solely on single-modal information for aesthetic evaluation presents inherent limitations. While multimodal IAA models incorporating user comments have achieved significant advancements, these comments are often unavailable due to privacy concerns and practical considerations, and they also introduce additional computational overhead during inference. To address this issue, we propose a cross-modal hierarchical knowledge distillation method, termed HKD-IAA, to enhance the performance of unimodal image models effectively. Specifically, HKD-IAA comprises four components: feature extraction, feature decomposition, hierarchical knowledge distillation, and dynamic decay. During training, we first decompose the extracted features into a weighted sum of basic aesthetic elements and their corresponding weights, thereby reducing the learning difficulty for the student model. Building on this, we design a new hierarchical knowledge distillation framework, which aligns features at the feature, relation, and response levels to effectively transfer the knowledge from the teacher model. Finally, we introduce a dynamic decay strategy to adjust the weight of the distillation loss, thereby enhancing the student model's learning effectiveness during training. Extensive experiments on two benchmark datasets validate that the proposed method achieves state-of-the-art performance using only visual modal data. Our code is available at https://github.com/Hangwei-Chen/HKD-IAA.},
  archive      = {J_TMM},
  author       = {Hangwei Chen and Feng Shao and Weiyi Jing and Huizhi Wang and Qiuping Jiang},
  doi          = {10.1109/TMM.2024.3521765},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2556-2569},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal hierarchical knowledge distillation for image aesthetics assessment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NIR-assisted image denoising: A selective fusion approach and a real-world benchmark dataset. <em>TMM</em>, <em>27</em>, 2543-2555. (<a href='https://doi.org/10.1109/TMM.2024.3521833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant progress in image denoising, it is still challenging to restore fine-scale details while removing noise, especially in extremely low-light environments. Leveraging near-infrared (NIR) images to assist visible RGB image denoising shows the potential to address this issue, becoming a promising technology. Nonetheless, existing works still struggle with taking advantage of NIR information effectively for real-world image denoising, due to the content inconsistency between NIR-RGB images and the scarcity of real-world paired datasets. To alleviate the problem, we propose an efficient Selective Fusion Module (SFM), which can be plug-and-played into the advanced denoising networks to merge the deep NIR-RGB features. Specifically, we sequentially perform the global and local modulation for NIR and RGB features, and then integrate the two modulated features. Furthermore, we present a Real-world NIR-Assisted Image Denoising (Real-NAID) dataset, which covers diverse scenarios as well as various noise levels. Extensive experiments on both synthetic and our real-world datasets demonstrate that the proposed method achieves better results than state-of-the-art ones.},
  archive      = {J_TMM},
  author       = {Rongjian Xu and Zhilu Zhang and Renlong Wu and Wangmeng Zuo},
  doi          = {10.1109/TMM.2024.3521833},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2543-2555},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {NIR-assisted image denoising: A selective fusion approach and a real-world benchmark dataset},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning distinguishable degradation maps for unknown image super-resolution. <em>TMM</em>, <em>27</em>, 2530-2542. (<a href='https://doi.org/10.1109/TMM.2024.3521839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing super-resolution (SR) methods assume that the degradation is fixed (e.g., bicubic downsampling), whereas their performance would be degraded if the actual degradation differs from this assumption. To deal with unknown degradations, existing unknown SR methods are committed to learning degradation representation to generate high-resolution images. Nevertheless, they ignore that the impact of degradations on images is related to image content, or they learn degradation representations without any constraints. In this article, we propose a degradation maps extractor for unknown SR. Specifically, we learn degradation maps and condense them into a one-dimensional representation space to distinguish various degradations, which obtains distinguishable degradation maps and preserves the connection with the image contents. Furthermore, we propose a degradation map-guided SR (DMGSR) network, in which the degradation maps adaptively influence the SR process by applying channel attention and spatial attention to middle features. With the cooperation of the degradation maps extractor and the degradation maps-guided SR network, our network can flexibly handle various degradations. Experimental results show that our model achieves state-of-the-art performance in quantitative and qualitative metrics for the unknown SR task.},
  archive      = {J_TMM},
  author       = {Zhenbing Liu and Jieyu Huang and Wenhao Wang and Haoxiang Lu and Rushi Lan},
  doi          = {10.1109/TMM.2024.3521839},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2530-2542},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning distinguishable degradation maps for unknown image super-resolution},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffUIE: Learning latent global priors in diffusion models for underwater image enhancement. <em>TMM</em>, <em>27</em>, 2516-2529. (<a href='https://doi.org/10.1109/TMM.2024.3521710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater imagery often suffers from light attenuation and color distortion, resulting in images with low contrast and blurriness. Enhancing these images is crucial yet challenging due to the complex degradation and noise inherent in underwater environments. In this study, we introduce a novel diffusion model, termed Underwater Image Enhancement(UIE) Diffusion, which leverages a global feature prior for effective underwater image enhancement. To our knowledge, this is the inaugural application of a diffusion model to the task of underwater image enhancement, setting a new benchmark in performance. Our approach begins with the introduction of a global feature prior to augment the diffusion model, mitigating the impact of noise and distortion during training. We then incorporate an underwater image degradation model to facilitate the learning of mappings between high-quality and degraded underwater images. To address over-enhancement caused by high-frequency components, we employ scaling factors to modulate the influence of frequency features during diffusion. Additionally, we enhance the model's stability during inference by integrating a backward diffusion process into its training. Comprehensive evaluations on multiple public datasets demonstrate that UIE Diffusion surpasses existing state-of-the-art methods in both subjective outcomes and objective assessments.},
  archive      = {J_TMM},
  author       = {Yuhao Qing and Si Liu and Hai Wang and Yueying Wang},
  doi          = {10.1109/TMM.2024.3521710},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2516-2529},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DiffUIE: Learning latent global priors in diffusion models for underwater image enhancement},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEDN: A spatiotemporal encoder-decoder network for end-to-end object removal forgery detection in high-resolution videos. <em>TMM</em>, <em>27</em>, 2503-2515. (<a href='https://doi.org/10.1109/TMM.2024.3521804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing popularity of high-resolution (HR) video and the continuous growth of network bandwidth, the challenge of object removal detection in HR videos has attracted significant attention. Expert forgers leverage the rich detail in HR videos for meticulous pixel manipulation and apply sophisticated postprocessing techniques to hide high-frequency artifacts, thereby making forgery detection and localization more difficult when existing schemes are used. Additionally, the end-to-end framework simplifies the detection and localization process, which has not been considered in previous work. To solve the above issues, a spatiotemporal encoder−decoder network (SEDN) is proposed for end-to-end object removal forgery detection in HR videos. In the SEDN, a new model composed of a 3D asymmetric dual-stream network (3D-ADSN) and Transformer is proposed. The 3D-ADSN is utilized as the encoder, which fully integrates the high-frequency and low-frequency spatiotemporal information of videos. Transformer is utilized as the decoder to capture the global structure spatiotemporal information of the long-range feature sequence obtained by the encoder. This network combination successfully achieves simultaneous detection in the temporal and spatial domains without any additional postprocessing calculations. The experimental results demonstrate the better performance of the SEDN at different resolutions.},
  archive      = {J_TMM},
  author       = {Lizhi Xiong and Linsen Ding and Mengqi Cao and Zhihua Xia and Yun-Qing Shi},
  doi          = {10.1109/TMM.2024.3521804},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2503-2515},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SEDN: A spatiotemporal encoder-decoder network for end-to-end object removal forgery detection in high-resolution videos},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). No-reference point cloud quality assessment via graph convolutional network. <em>TMM</em>, <em>27</em>, 2489-2502. (<a href='https://doi.org/10.1109/TMM.2024.3521845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) point cloud, as an emerging visual media format, is increasingly favored by consumers as it can provide more realistic visual information than two-dimensional (2D) data. Similar to 2D plane images and videos, point clouds inevitably suffer from quality degradation and information loss through multimedia communication systems. Therefore, automatic point cloud quality assessment (PCQA) is of critical importance. In this work, we propose a novel no-reference PCQA method by using a graph convolutional network (GCN) to characterize the mutual dependencies of multi-view 2D projected image contents. The proposed GCN-based PCQA (GC-PCQA) method contains three modules, i.e., multi-view projection, graph construction, and GCN-based quality prediction. First, multi-view projection is performed on the test point cloud to obtain a set of horizontally and vertically projected images. Then, a perception-consistent graph is constructed based on the spatial relations among different projected images. Finally, reasoning on the constructed graph is performed by GCN to characterize the mutual dependencies and interactions between different projected images, and aggregate feature information of multi-view projected images for final quality prediction. Experimental results on two publicly available benchmark databases show that our proposed GC-PCQA can achieve superior performance than state-of-the-art quality assessment metrics.},
  archive      = {J_TMM},
  author       = {Wu Chen and Qiuping Jiang and Wei Zhou and Feng Shao and Guangtao Zhai and Weisi Lin},
  doi          = {10.1109/TMM.2024.3521845},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2489-2502},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {No-reference point cloud quality assessment via graph convolutional network},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FasterSal: Robust and real-time single-stream architecture for RGB-D salient object detection. <em>TMM</em>, <em>27</em>, 2477-2488. (<a href='https://doi.org/10.1109/TMM.2024.3521699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D Salient Object Detection (SOD) aims to segment the most prominent areas and objects in a given pair of RGB and depth images. Most current models adopt a dual-stream structure to extract information from both RGB and depth images. However, this leads to an exponential increase in the number of parameters and computations in the model. Moreover, the discrepancy between RGB pretrained and the 3D geometric relationships in depth maps present a challenge for the encoder in capturing spatial structural details. These issues impact the model's accuracy in locating salient objects and distinguishing edge details. To address these, we propose a novel early feature fusion network, named FasterSal, which enables more efficient RGB-D SOD. FasterSal uses a single stream structure to receive RGB images and depth maps, extracting features based on the 3D geometric relationships in the depth map while fully leveraging the pretrained RGB encoder. This approach effectively avoids the inconsistencies between depth modality and the RGB pretrained encoder. It also significantly reduces the number of network parameters while maintaining efficient feature encoding capabilities. To achieve finer edge learning, the detail-aware loss and texture enhancement module are introduced. These modules are designed to extract latent details in high-frequency component features and to enhance the edge learning capability of the model using distance information. Experimental results on several benchmark datasets confirm the effectiveness and superiority of our method over the state-of-the-art approaches, achieving a good balance between performance and speed with only 3.4 million parameters and a CPU operating speed of 63 FPS.},
  archive      = {J_TMM},
  author       = {Jin Zhang and Ruiheng Zhang and Lixin Xu and Xiankai Lu and Yushu Yu and Min Xu and He Zhao},
  doi          = {10.1109/TMM.2024.3521699},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2477-2488},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FasterSal: Robust and real-time single-stream architecture for RGB-D salient object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SkyML: A MLaaS federation design for multicloud-based multimedia analytics. <em>TMM</em>, <em>27</em>, 2463-2476. (<a href='https://doi.org/10.1109/TMM.2024.3521768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of deep learning has precipitated a surge in public machine learning as a service (MLaaS) for multimedia analysis. However, reliance on a single MLaaS can result in product dependency and a loss of better performance offered by multiple MLaaSes. Consequently, many enterprises opt for an intercloud broker capable of managing jobs across various clouds. Though existing works explore the efficient utilization of inter-cloud computational resources and the enhancement of inter-cloud data transfer throughput, they disregard improving the overall accuracy of multiple MLaaSes. In response, we conduct a measurement study on object detection services, which are designed to identify and locate various objects within an image. We discover that combining predictions from multiple MLaaSes can improve analytical performance. However, more MLaaSes do not necessarily equate to better performance. Therefore, we propose SkyML, a user-side MLaaS federation broker that selects a subset of MLaaSes based on the characteristics of the request to achieve optimal multimedia analytical performance. Initially, we design a combinatorial reinforcement learning approach to select the sound MLaaS combination, thereby maximizing user experience. We also present an ingenious, automated taxonomy unification algorithm to minimize human efforts in merging MLaaS-specific labels into a user-preferred label space. Moreover, we devise an optimized ensemble strategy to aggregate predictions from the selected MLaaSes. Evaluations indicate that our similarity-based taxonomy unification approach can reduce annotation costs by 90%. Moreover, real-world trace-driven evaluations further prove that our MLaaS selection method can achieve similar levels of accuracy with a 67% reduction in inference fees.},
  archive      = {J_TMM},
  author       = {Shuzhao Xie and Yuan Xue and Yifei Zhu and Zhi Wang},
  doi          = {10.1109/TMM.2024.3521768},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2463-2476},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SkyML: A MLaaS federation design for multicloud-based multimedia analytics},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical motion-enhanced matching framework for few-shot action recognition. <em>TMM</em>, <em>27</em>, 2450-2462. (<a href='https://doi.org/10.1109/TMM.2024.3521712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-Shot Action Recognition (FSAR) aims to recognize novel class action with limited annotated training data from the same class. Most FSAR methods subconsciously follow the few-shot image classification solutions by solely focusing on appearance-level matching between support and query videos, such as part-level matching, frame-level matching, and segment-level matching. However, these methods, almost always, have two main limitations: 1) generally ignore the relationship among these part-, frame- and segment-level features and 2) may mismatch the same class actions under fast-term and slow-term dynamics. To this end, we present a novel Hierarchical Motion-enhanced Matching (HM${^{2}}$) framework to hierarchically learn the relation-aware multi-modal features, and jointly promote the multi-modal matching, including appearance-level matching on segments, frames, and parts, as well as the motion-level matching on dynamics. Specifically, we first propose a new Hierarchical Tokenizer (HT) to learn multi-modal features, namely utilizing a hierarchical Transformer to learn appearance-level features, along with a Slow-Fast Aware Motion (SFAM) strategy to learn motion-level features covering fast- and slow-term dynamics. Next, we propose a new Relation-aware Matcher (RM) to match the multi-modal features, by leveraging a Hierarchical Relational Graph Convolutional Network (H-RGCN) to capture the relationship among these appearance-level features. Further, a Dual Sample-to-Class Matching (DSCM) strategy is proposed to measure the bidirectional similarities among appearance- and motion-modal features by sample-to-class matching and class-to-sample matching. Extensive experiments on four golden FSAR datasets demonstrate significant performance improvements of HM${^{2}}$ compared with the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Hailiang Gao and Guo-Sen Xie and Rui Yan and Qiongjie Cui and Hongyu Qu and Xiangbo Shu},
  doi          = {10.1109/TMM.2024.3521712},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2450-2462},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical motion-enhanced matching framework for few-shot action recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-granularity context perception network for open set recognition of camouflaged objects. <em>TMM</em>, <em>27</em>, 2436-2449. (<a href='https://doi.org/10.1109/TMM.2024.3521723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open set recognition (OSR) aims to identify whether a test sample belongs to a semantic class in the classifier training set. Existing OSR methods exhibit prominent performance on various image datasets. However, they are primarily designed for general object recognition rather than more complex camouflaged object recognition. When an object is camouflaged, i.e., it exhibits a similar pattern to the background, it is difficult to finely identify it and differentiate between known and unknown categories. To address this problem, we propose a novel multi-granularity context perception network (MCPNet) for OSR of camouflaged objects, which can accurately identify camouflaged objects by fusing coarse-grained and fine-grained context features. In MCPNet, the vision transformer is first utilized to extract coarse-grained context features to locate the approximate location of camouflaged objects. Then, an adaptive local focus module (ALFM) is proposed to pick out the most discriminative regions and learn the fine-grained context of these regions. Finally, multi-granular context features are fused to obtain recognition results. During the training, a contrastive clustering module (CCM) is introduced to guide the network to effectively utilize multi-granularity context to generate high-confidence decision boundaries. We also built two camouflaged object classification datasets named ACOC and NCOC which mainly consist of artificial camouflage and natural camouflage respectively to facilitate research in OSR of camouflaged objects. Experimental results on two datasets show that MCPNet outperforms state-of-the art methods.},
  archive      = {J_TMM},
  author       = {Ze Song and Xudong Kang and Xiaohui Wei and Renwei Dian and Jinyang Liu and Shutao Li},
  doi          = {10.1109/TMM.2024.3521723},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2436-2449},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-granularity context perception network for open set recognition of camouflaged objects},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning video salient object detection progressively from unlabeled videos. <em>TMM</em>, <em>27</em>, 2423-2435. (<a href='https://doi.org/10.1109/TMM.2024.3521783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning-based video salient object detection (VSOD) has achieved some breakthroughs, but these methods rely on expensive annotated videos with pixel-wise annotations or weak annotations. In this paper, based on the similarities and differences between VSOD and image salient object detection (SOD), we propose a novel VSOD method via a progressive framework that locates and segments salient objects in sequence without utilizing any video annotation. To efficiently use the knowledge learned in the SOD dataset for VSOD efficiently, we introduce dynamic saliency to compensate for the lack of motion information of SOD during the locating process while maintaining the same fine segmenting process. Specifically, we utilize the coarse locating model trained on the image dataset, to identify frames with both static and dynamic saliency. Locating results of these frames are selected as spatiotemporal location labels. Moreover, by tracking salient objects in adjacent frames, the number of spatiotemporal location labels is increased. On the basis of these location labels, a two-stream locating network with an optical flow branch is proposed to capture salient objects in videos. The results with respect to five public benchmarks demonstrate that our method outperforms the state-of-the-art weakly and unsupervised methods.},
  archive      = {J_TMM},
  author       = {Binwei Xu and Qiuping Jiang and Haoran Liang and Dingwen Zhang and Ronghua Liang and Peng Chen},
  doi          = {10.1109/TMM.2024.3521783},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2423-2435},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning video salient object detection progressively from unlabeled videos},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FER-former: Multimodal transformer for facial expression recognition. <em>TMM</em>, <em>27</em>, 2412-2422. (<a href='https://doi.org/10.1109/TMM.2024.3521788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing demands for intuitive interactions in virtual reality have led to surging interests in facial expression recognition (FER). There are however several issues commonly seen in existing methods, including narrow receptive fields and homogenous supervisory signals. To address these issues, we propose in this paper a novel multimodal supervision-steering transformer for facial expression recognition in the wild, referred to as FER-former. Specifically, to address the limitation of narrow receptive fields, a hybrid feature extraction pipeline is designed by cascading both prevailing CNNs and transformers. To deal with the issue of homogenous supervisory signals, a heterogeneous domain-steering supervision module is proposed to incorporate text-space semantic correlations to enhance image features, based on the similarity between image and text features. Additionally, a FER-specific transformer encoder is introduced to characterize conventional one-hot label-focusing and CLIP-based text-oriented tokens in parallel for final classification. Based on the collaboration of multifarious token heads, global receptive fields with multimodal semantic cues are captured, delivering superb learning capability. Extensive experiments on popular benchmarks demonstrate the superiority of the proposed FER-former over the existing state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Yande Li and Mingjie Wang and Minglun Gong and Yonggang Lu and Li Liu},
  doi          = {10.1109/TMM.2024.3521788},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2412-2422},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {FER-former: Multimodal transformer for facial expression recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleash the power of vision-language models by visual attention prompt and multimodal interaction. <em>TMM</em>, <em>27</em>, 2399-2411. (<a href='https://doi.org/10.1109/TMM.2024.3521785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained vision-language models (VLMs), equipped with parameter-efficient tuning (PET) methods like prompting, have shown impressive knowledge transferability on new downstream tasks, but they are still prone to be limited by catastrophic forgetting and overfitting dilemma due to large gaps among tasks. Furthermore, the underlying physical mechanisms of prompt-based tuning methods (especially for visual prompting) remain largely unexplored. It is unclear why these methods work solely based on learnable parameters as prompts for adaptation. To address the above challenges, we present a new prompt-based framework for vision-language models, termed Uni-prompt. Our framework transfers VLMs to downstream tasks by designing visual prompts from an attention perspective that reduces the transfer/solution space, which enables the vision model to focus on task-relevant regions of the input image while also learning task-specific knowledge. Additionally, Uni-prompt further aligns visual-text prompts learning through a pretext task with masked representation modeling interactions, which implicitly learns a global cross-modal matching between visual and language concepts for consistency. We conduct extensive experiments on the few-shot classification task and achieve significant improvement using our Uni-prompt method while requiring minimal extra parameters cost.},
  archive      = {J_TMM},
  author       = {Wenyao Zhang and Letian Wu and Zequn Zhang and Tao Yu and Chao Ma and Xin Jin and Xiaokang Yang and Wenjun Zeng},
  doi          = {10.1109/TMM.2024.3521785},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2399-2411},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unleash the power of vision-language models by visual attention prompt and multimodal interaction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HTACPE: A hybrid transformer with adaptive content and position embedding for sample learning efficiency of hyperspectral tracker. <em>TMM</em>, <em>27</em>, 2384-2398. (<a href='https://doi.org/10.1109/TMM.2024.3521819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer architecture has demonstrated significant potential in hyperspectral object tracking by leveraging global correlation learning to accurately represent the data distribution. However, existing hyperspectral object trackers based on transformer models typically rely on costly pre-trained models, making them prone to crashing due to overfitting when tuned on small-scale hyperspectral videos, greatly limiting their performance. To address this challenge, in this paper, a Hybrid Transformer with Adaptive Content and Position Embedding (HTACPE) tracker is proposed to improve the learning efficiency of the tracking model, and fully explore the spectral-spatial information. Specifically, an Adaptive Content and Position Embedding Module (ACPEM) is designed to dynamically learn the balance between focusing on positional and content-based information, which allows the model to effectively handle datasets of various sizes. To enhance the spectral-spatial information, a Spectral Grouping Module (SGM) is designed to learn the high-frequency information in complex scenarios, thereby enhancing diversified features. It operates in parallel with the ACPEM feature learning module. Furthermore, a Dynamic Reliability Refinement Module (DRRM) is incorporated to address challenges related to accurate object position perception, iteratively refining prediction parameters to enhance the reliability of the model. Extensive experiments demonstrate that the proposed HTACPE achieves satisfactory tracking performance both qualitatively and quantitatively, especially with insufficient training data.},
  archive      = {J_TMM},
  author       = {Ye Wang and Shaohui Mei and Mingyang Ma and Yuheng Liu and Yuru Su},
  doi          = {10.1109/TMM.2024.3521819},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2384-2398},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HTACPE: A hybrid transformer with adaptive content and position embedding for sample learning efficiency of hyperspectral tracker},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PMNet: Predator-mimicking network for video camouflaged object detection. <em>TMM</em>, <em>27</em>, 2374-2383. (<a href='https://doi.org/10.1109/TMM.2024.3521775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The predator has the ability to quickly respond to the misjudged decision and hunt the camouflaged target by analyzing its movement. Those decision compensation and movement analysis for hunting are closely tied to temporal and spatial information. This can be mirrored in the video camouflaged object detection (VCOD) task where the captured temporal information may be misjudged as well as the spatial information tends to be inaccurate in complex scenes. Thus, two key factors should be considered in the VCOD task: How can a model cope with the misjudged temporal information; How can spatial features interact with the temporal information to understand dynamic scenes? To this end, we propose a predator-mimicking network (PMNet) equipped with a selective temporal alignment module (STAM) and a temporal-spatial feedback module (T-SFM). The STAM is designed to alleviate the influence of the misjudged motion trajectory by adopting our adaptive selection mechanism from a novel perspective. In T-SFM, the temporal information works as the self-knowledge to provide assistance and interact with spatial features, enabling the model to effectively detect the camouflaged object. Experimental results demonstrate that our method achieves state-of-the-art performance on VCOD benchmarks. Furthermore, our model can be generalized in the video salient object detection (VSOD) task and also outperforms existing state-of-the-art methods. The source code will be publicly available at https://github.com/LiuTingWed/CriDiff.},
  archive      = {J_TMM},
  author       = {Miao Zhang and Beiqi Hu and Shunyu Yao and Yongri Piao and Huchuan Lu},
  doi          = {10.1109/TMM.2024.3521775},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2374-2383},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {PMNet: Predator-mimicking network for video camouflaged object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing real-world active speaker detection with multi-modal extraction pre-training. <em>TMM</em>, <em>27</em>, 2362-2373. (<a href='https://doi.org/10.1109/TMM.2024.3521791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual active speaker detection (AV-ASD) aims to identify which visible face is speaking in a scene with one or more persons. Most existing AV-ASD methods prioritize capturing speech-lip correspondence. However, there is a noticeable gap in addressing the challenges from real-world AV-ASD scenarios. Due to the presence of low-quality noisy videos in such cases, AV-ASD systems without a selective listening ability are short of effectively filtering out disruptive voice components from mixed audio inputs. In this paper, we propose a Multi-modal Speech Extraction-to-Detection framework named ‘MuSED’, which is pre-trained with audio-visual target speech extraction to learn the denoising ability, then it is fine-tuned with the AV-ASD task. Meanwhile, to better capture the multi-modal information and deal with real-world problems such as missing modality, MuSED is modelled on the time domain directly and integrates the multi-modal plus-and-minus augmentation strategy. Our experiments demonstrate that MuSED substantially outperforms the state-of-the-art AV-ASD methods and achieves 95.6% mAP on the AVA-ActiveSpeaker dataset, 98.3% AP on the ASW dataset, and 97.9% F1 on the Columbia AV-ASD dataset, respectively. We will publicly release the code in due course.},
  archive      = {J_TMM},
  author       = {Ruijie Tao and Xinyuan Qian and Rohan Kumar Das and Xiaoxue Gao and Jiadong Wang and Haizhou Li},
  doi          = {10.1109/TMM.2024.3521791},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2362-2373},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing real-world active speaker detection with multi-modal extraction pre-training},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confident multi-view stereo. <em>TMM</em>, <em>27</em>, 2347-2361. (<a href='https://doi.org/10.1109/TMM.2024.3521698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving the Multi-View Stereo (MVS) problem is a cornerstone in computer vision, with depth map estimation and fusion being one of the most critical approaches. The depth confidence map is pivotal in ensuring the precision and completeness of the reconstruction outcomes. These algorithms frequently encounter a trade-off between completeness and accuracy in the confidence map, which can significantly impair the final reconstruction results. This paper analyzes the causes and phenomena of these issues, namely Confidence Jitter, Confidence Gap, and Confidence Disappearance. From these insights, a multi-view stereo network named CF-MVSNet is introduced, comprising three essential components. Firstly, the method mitigates the Confidence Jitter problem through two confidence fusion strategies. Secondly, it narrows the depth sampling space to near sub-pixel levels, addressing the Confidence Gap through neighborhood-average pooling. Lastly, the algorithm tackles the Confidence Disappearance problem resulting from multi-scale classification and regression with a loss function named CL. Our proposed method demonstrates superior performance across two critical metrics: the completeness of the depth map and the accuracy of the reconstructed point cloud, outperforming current state-of-the-art MVS methods.},
  archive      = {J_TMM},
  author       = {Xin Ma and Qiang Li and Yuan Yuan and Qi Wang},
  doi          = {10.1109/TMM.2024.3521698},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2347-2361},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Confident multi-view stereo},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hear me, see me, understand me: Audio-visual autism behavior recognition. <em>TMM</em>, <em>27</em>, 2335-2346. (<a href='https://doi.org/10.1109/TMM.2024.3521838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a novel problem of audio-visual autism behavior recognition, which includes social behavior recognition, an essential aspect previously omitted in AI-assisted autism screening research. We define the task at hand as one that is audio-visual autism behavior recognition, which uses audio and visual cues, including any speech present in the audio, to recognize autism-related behaviors. To facilitate this new research direction, we collected an audio-visual autism spectrum dataset (AV-ASD), currently the largest video dataset for autism screening using a behavioral approach. It covers an extensive range of autism-associated behaviors, including those related to social communication and interaction. To pave the way for further research on this new problem, we intensively explored leveraging foundation models and multimodal large language models across different modalities. Our experiments on the AV-ASD dataset demonstrate that integrating audio (mainly ambient sound), visual, and speech (predominately spoken language) modalities significantly enhances the performance in autism behavior recognition. Additionally, we explored the use of a post-hoc to ad-hoc pipeline in a multimodal large language model to investigate its potential to augment the model's explanatory capability during autism behavior recognition.},
  archive      = {J_TMM},
  author       = {Shijian Deng and Erin E. Kosloski and Siddhi Patel and Zeke A. Barnett and Yiyang Nan and Alexander Kaplan and Sisira Aarukapalli and William T. Doan and Matthew Wang and Harsh Singh and Pamela R. Rollins and Yapeng Tian},
  doi          = {10.1109/TMM.2024.3521838},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2335-2346},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hear me, see me, understand me: Audio-visual autism behavior recognition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind video quality assessment at the edge. <em>TMM</em>, <em>27</em>, 2320-2334. (<a href='https://doi.org/10.1109/TMM.2024.3521704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the proliferation of user-generated videos on the Internet, blind video quality assessment (BVQA) at the edge attracts growing attention. The usage of deep-learning-based methods is restricted to be applied at the edge due to their large model sizes and high computational complexity. In light of this, a novel lightweight BVQA method called GreenBVQA is proposed in this work. GreenBVQA features a small model size, low computational complexity, and high performance. Its processing pipeline includes: video data cropping, unsupervised representation generation, supervised feature selection, and mean-opinion-score (MOS) regression and ensembles. We conduct experimental evaluations on three BVQA datasets and show that GreenBVQA can offer state-of-the-art performance in the Pearson Linear Correlation Coefficient (PLCC) and the Spearman Rank Order Correlation Coefficient (SROCC) metrics while demanding significantly smaller model sizes and lower computational complexity. Thus, GreenBVQA is well-suited for edge devices.},
  archive      = {J_TMM},
  author       = {Zhanxuan Mei and Yun-Cheng Wang and C.-C. Jay Kuo},
  doi          = {10.1109/TMM.2024.3521704},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2320-2334},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Blind video quality assessment at the edge},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MulDeF: A model-agnostic debiasing framework for robust multimodal sentiment analysis. <em>TMM</em>, <em>27</em>, 2304-2319. (<a href='https://doi.org/10.1109/TMM.2024.3521836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multimodal sentiment analysis (MSA) has gained prominence with the proliferation of social media. However, prior studies have often disregarded the possibility of spurious correlations between multimodal data and sentiment labels. Neglecting these factors often results in significant performance degradation, hampering the model's ability to generalize in out-of-distribution (OOD) scenarios. To gain a comprehensive understanding of multimodal knowledge and enhance the model's generalization across diverse distribution scenarios, we present the Multimodal Debiasing Framework (MulDeF). This model-agnostic framework addresses label bias through causal intervention and tackles multimodal biases using counterfactual reasoning. During the training phase, MulDeF rectifies multimodal representations through frontdoor adjustment in causal intervention, effectively eliminating label bias. In order to model conditional expectation calculations within the context of frontdoor adjustment, we introduce multimodal causal attention (MCA). In the inference phase, it employs counterfactual reasoning to eliminate multimodal biases. To further refine our debiasing strategy, we categorize multimodal biases into two distinct types: nonverbal bias and verbal bias. Nonverbal bias is addressed at the utterance level, involving the establishment of unimodal models for audio and visual modalities to estimate their biases concerning sentiment labels. Conversely, verbal bias mitigation occurs at the word level. Here, we mask “harmless” words to generate corresponding counterfactual texts, which are then assessed by the text model to identify word-level bias. Experimental results validate the effectiveness of MulDeF, showcasing its superior performance in OOD settings compared to state-of-the-art methods, while also achieving competitive results in independent and identically distributed (IID) settings.},
  archive      = {J_TMM},
  author       = {Ruohong Huan and Guowei Zhong and Peng Chen and Ronghua Liang},
  doi          = {10.1109/TMM.2024.3521836},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2304-2319},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MulDeF: A model-agnostic debiasing framework for robust multimodal sentiment analysis},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-language meets the skeleton: Progressively distillation with cross-modal knowledge for 3D action representation learning. <em>TMM</em>, <em>27</em>, 2293-2303. (<a href='https://doi.org/10.1109/TMM.2024.3521718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action representation learning aims to interpret and understand human behaviors by encoding the skeleton sequences, which can be categorized into two primary training paradigms: supervised learning and self-supervised learning. However, the former one-hot classification requires labor-intensive predefined action categories annotations, while the latter involves skeleton transformations (e.g., cropping) in the pretext tasks that may impair the skeleton structure. To address these challenges, we introduce a novel skeleton-based training framework (C$^{2}$VL) based on Cross-modal Contrastive learning that uses the progressive distillation to learn task-agnostic human skeleton action representation from the Vision-Language knowledge prompts. Specifically, we establish the vision-language action concept space through vision-language knowledge prompts generated by pre-trained large multimodal models (LMMs), which enrich the fine-grained details that the skeleton action space lacks. Moreover, we propose the intra-modal self-similarity and inter-modal cross-consistency softened targets in the cross-modal representation learning process to progressively control and guide the degree of pulling vision-language knowledge prompts and corresponding skeletons closer. These soft instance discrimination and self-knowledge distillation strategies contribute to the learning of better skeleton-based action representations from the noisy skeleton-vision-language pairs. During the inference phase, our method requires only the skeleton data as the input for action recognition and no longer for vision-language prompts. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate that our method outperforms the previous methods and achieves state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Yang Chen and Tian He and Junfeng Fu and Ling Wang and Jingcai Guo and Ting Hu and Hong Cheng},
  doi          = {10.1109/TMM.2024.3521718},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2293-2303},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Vision-language meets the skeleton: Progressively distillation with cross-modal knowledge for 3D action representation learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LGSNet: Local-global semantics learning object detection. <em>TMM</em>, <em>27</em>, 2281-2292. (<a href='https://doi.org/10.1109/TMM.2024.3521850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention learns capturing the long-range dependencies between embeddings (e.g., image pixels). However, the memory overhead and computation cost are prohibitive due to being quadratic in term of the spatial resolution. The structure analysis reveals two crucial roles in the attention: the correlation-based dependency structure and feature normalization. In this work, an efficacious Local-Global Semantics (LGS) module is proposed to alleviate the above issues by modeling the local semantic aggregation and global semantic interaction. Our LGS module contains a group convolution and an Efficient Global Semantic Attention (EGSA). Firstly, the group convolution aggregates local semantics. Secondly, considering a feature map as a sequence of 2-D channel representations, EGSA formulates a general model for the global semantic interaction. The linear correlation is computed between global semantics. LGS has the linear memory overhead and computation cost in term of the spatial resolution. The LGS module can be smoothly incorporated into object detection frameworks. The experiment results verify its effectiveness on two popular detection datasets: the MS COCO and PASCAL VOC.},
  archive      = {J_TMM},
  author       = {Yang Li and Licheng Jiao and Xu Liu and Fang Liu and Lingling Li and Puhua Chen},
  doi          = {10.1109/TMM.2024.3521850},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2281-2292},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {LGSNet: Local-global semantics learning object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AES-AUDIO: An encryption scheme for audio supporting differentiated decryption. <em>TMM</em>, <em>27</em>, 2268-2280. (<a href='https://doi.org/10.1109/TMM.2024.3521757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an audio encryption scheme that supports differentiated decryption, called AES-AUDIO, in which an audio only needs to be encrypted once and can be decrypted into different resolutions as needed. First, we design four security levels, confidential, harsh, noisy, and clear, based on the audio resolution perceived by human auditory perception. Second, the audio data in decimal floating-point numbers (D-FPNs) are unfolded to 32 bits (B-FPNs). Third, we design a region of interest (RoI) encryption algorithm for the audio with the B-FPN format, where the result preserves some perceptual information as needed. Fourth, we construct the AES-AUDIO scheme based on the RoI encryption algorithm, which allows the audio to be encrypted once and then decrypted into different security levels. It supports changing parameters to alter the perception effect corresponding to the security level. Overall, it achieves a balance between the security and usability of the protected audio. User experiments verify that the audios produced by differential decryption can achieve the expected security levels. Some security tests also yielded excellent results, such as an NSCR value of 1.},
  archive      = {J_TMM},
  author       = {Ruoyu Zhao and Yushu Zhang and Junhao Ji and Shuang Yi and Wenying Wen and Rushi Lan},
  doi          = {10.1109/TMM.2024.3521757},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2268-2280},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AES-AUDIO: An encryption scheme for audio supporting differentiated decryption},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient image super-resolution with feature interaction weighted hybrid network. <em>TMM</em>, <em>27</em>, 2256-2267. (<a href='https://doi.org/10.1109/TMM.2024.3521753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight image super-resolution aims to reconstruct high-resolution images from low-resolution images using low computational costs. However, existing methods result in the loss of middle-layer features due to activation functions. To minimize the impact of intermediate feature loss on reconstruction quality, we propose a Feature Interaction Weighted Hybrid Network (FIWHN), which comprises a series of Wide-residual Distillation Interaction Block (WDIB) as the backbone. Every third WDIB forms a Feature Shuffle Weighted Group (FSWG) by applying mutual information shuffle and fusion. Moreover, to mitigate the negative effects of intermediate feature loss, we introduce Wide Residual Weighting units within WDIB. These units effectively fuse features of varying levels of detail through a Wide-residual Distillation Connection (WRDC) and a Self-Calibrating Fusion (SCF). To compensate for global feature deficiencies, we incorporate a Transformer and explore a novel architecture to combine CNN and Transformer. We show that our FIWHN achieves a favorable balance between performance and efficiency through extensive experiments on low-level and high-level tasks.},
  archive      = {J_TMM},
  author       = {Wenjie Li and Juncheng Li and Guangwei Gao and Weihong Deng and Jian Yang and Guo-Jun Qi and Chia-Wen Lin},
  doi          = {10.1109/TMM.2024.3521753},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2256-2267},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient image super-resolution with feature interaction weighted hybrid network},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hypergraph based contextual relationship modeling method for multimodal emotion recognition in conversation. <em>TMM</em>, <em>27</em>, 2243-2255. (<a href='https://doi.org/10.1109/TMM.2024.3521738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion Recognition in Conversation (ERC) has gained considerable attention due to its importance in human-computer interaction. In ERC task, the combination of multimodal information and contextual information is necessary since it can help the model understand emotional changes in the context from multiple perspectives. As Graph Neural Networks (GNNs) have shown the superiority in relation modeling, many graph-based methods have been proposed to improve the performance of emotion recognition by utilizing the edges to mine the contextual relationship and multimodal relationship in a conversation. However, the existence of numerous redundant edges and excessively complex modality interaction in the graph hinders the model from capturing the truly effective dependency information for emotion recognition. In this paper, we propose a Hypergraph based Contextual Relationship Modeling Method (HyperCRM) to carry out the ERC task. HyperCRM models a conversation as a hypergraph instead of a graph, which defines two types of hyperedges, namely speaker-level hyperedge and sequence-level hyperedge, to represent the contextual relationship within the same speaker and the local sequence of the conversation, respectively. Multimodal information is leveraged here as the node feature representation by the feature concatenation. In addition, an improved hypergraph convolution method is designed to capture the long-range contextual information by three-stage information propagation in the hypergraph, including node-hyperedge, hyperedge-hyperedge and node-hyperedge. The extensive experiments on two public datasets shows the new State-Of-The-Art (SOTA) results, to further demonstrate that the proposed method can simply make use of the multimodal information and effectively model the complex contextual relationships in the conversation.},
  archive      = {J_TMM},
  author       = {Nannan Lu and Zhiyuan Han and Zhen Tan},
  doi          = {10.1109/TMM.2024.3521738},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2243-2255},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A hypergraph based contextual relationship modeling method for multimodal emotion recognition in conversation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TR-adapter: Parameter-efficient transfer learning for video question answering. <em>TMM</em>, <em>27</em>, 2232-2242. (<a href='https://doi.org/10.1109/TMM.2024.3521708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of large-scale pre-trained models for vision-language tasks has gained significant attention and has shown promising results in the video question answering. However, the increasing size of these models has made the fully fine-tuning strategy impractical. Therefore, there is a growing need for research in parameter-efficient transfer learning for downstream tasks. To address this challenge, we introduce a novel parameter-efficient transfer learning technique based on a temporal reasoning adapter for the video question answering task. Our proposed approach captures the temporal relationship within videos, enabling the model to possess visual reasoning ability and knowledge acquisition ability from language models. Our extensive experiments on four video question answering datasets indicate that our method can match or even outperform fully fine-tuning strategies and state-of-the-art models, while having the advantage of parameter efficiency.},
  archive      = {J_TMM},
  author       = {Yuanyuan Wang and Meng Liu and Xuemeng Song and Liqiang Nie},
  doi          = {10.1109/TMM.2024.3521708},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2232-2242},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TR-adapter: Parameter-efficient transfer learning for video question answering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relation inference enhancement network for visual commonsense reasoning. <em>TMM</em>, <em>27</em>, 2221-2231. (<a href='https://doi.org/10.1109/TMM.2024.3521725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When presented with a question regarding an image, Visual Commonsense Reasoning (VCR) offers not only a correct answer but also a rationale to justify the answer. Existing methods simply combine features from multiple modalities onto a shared dimension space, which doesn't align with human reasoning patterns, resulting in inadequate cross-modal and intra-modal reasoning behaviors. On the one hand, inadequate cross-modal reasoning arises from existing models relying on semantic correlations between answers and rationales in both textual modalities rather than the generative process of human reasoning from visual to textual modality. On the other hand, inadequate intra-modal reasoning arises from the incapacity of existing models to leverage previously acquired object relations beyond current observations like humans. To this end, we propose a novel Relation Inference Enhancement Network (RIE-Net), which enhances reasoning ability based on cross-modal image analysis and introduces intra-modal relational reasoning modules to memorize reasoning knowledge. To enhance the cross-modal association between images and rationales, RIE-Net introduces a cross-modal image analysis module, which eliminates language bias between answers and rationales by generating rationale from images. In addition, to comprehend and retain relational knowledge, RIE-Net introduces intra-modal relational reasoning modules to capture prior knowledge associated with various object categories and enhance the model's understanding of visual-spatial relationships. Quantitative and qualitative evaluations of the public VCR dataset demonstrate that our approach performs favorably against state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Mengqi Yuan and Gengyun Jia and Bing-Kun Bao},
  doi          = {10.1109/TMM.2024.3521725},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2221-2231},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Relation inference enhancement network for visual commonsense reasoning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Viscoelastic cluster-constrained PBD-based soft tissue behavior and interactive media applications for surgical simulation. <em>TMM</em>, <em>27</em>, 2206-2220. (<a href='https://doi.org/10.1109/TMM.2024.3521762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The virtual surgery engine represents a crucial research domain within biomedical and information sciences. To address the real-time and realistic demands of virtual surgical robots for soft tissue deformation and cutting, the surface information and internal structure of organ models have been redefined. An enhanced Three-Parameter Mass-Ogden model, which incorporates nonlinearity and viscoelasticity in soft tissues, has been developed based on extended position dynamics. Cluster constraints for filling particles were introduced to improve the smoothness of surgical procedures. The relaxation and creep characteristics of real soft tissues were accounted for by evaluating the responses of various biological tissues to external stress and loads using the HY-0580 high-performance mechanical testing machine. Eight experiments were conducted for each tissue type, and five sets of valid data were averaged and fitted using the Three-Parameter Mass-Ogden mixed model. Surgical simulations were conducted using Abaqus, incorporating Young's modulus, stress-strain relationships, cutting depth, pressure distribution, real-time feedback, and comprehensive visualization. The model's effectiveness was further validated. The surgical platform was integrated into a virtual reality-based digital twin robot simulator for minimally invasive surgery, achieving a surgical operation refresh rate of 78.5 Hz, a visual refresh rate of 60 Hz, and a haptic feedback refresh rate of 1000 Hz. Comparative analysis with the Mass-Spring Model (MSM) and Finite Element Method (FEM) shows our model's superior balance of accuracy and efficiency. MSM is fast but imprecise, while FEM is accurate but computationally intensive.},
  archive      = {J_TMM},
  author       = {Zijun Wang and Shijie Li and Jun Peng and Yonghang Tai and Zhengtao Yu},
  doi          = {10.1109/TMM.2024.3521762},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2206-2220},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Viscoelastic cluster-constrained PBD-based soft tissue behavior and interactive media applications for surgical simulation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking affine transform for efficient image enhancement: A color space perspective. <em>TMM</em>, <em>27</em>, 2194-2205. (<a href='https://doi.org/10.1109/TMM.2024.3521826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, we have observed significant advancements in learning-based techniques for image-enhancement tasks. However, most of the existing methods are either purely based on image-to-image convolutional neural networks, which cannot handle high-resolution images in real-time, or resort to 3D Lookup Tables, which fall short of local tone adjustments. In this paper, we rethink affine transform through a color space perspective, and then propose AttnBL (Attentional Bilateral Grid Learning), a novel hybrid image enhancement algorithm to process ultra-high-definition images in real-time. Our algorithm consists of two paths, the low-resolution chroma prediction path that aims to learn the chroma coefficients and the full-resolution luma adaptation path that aims to preserve brightness details. Specifically, we propose a carefully designed hierarchical transformer to capture the global information in an efficient way and introduce a feature extraction module to adaptively learn a luma guidance for bilateral upsampling. Our algorithm can process a 4K-resolution image in 20 milliseconds. This efficiency provides a practical solution for high resolution real-time preview. Without bells and whistles, our model outperforms previous state-of-the-art methods on two well-known datasets in image enhancement tasks both quantitatively and qualitatively. Our analysis also provides some interesting findings that may enlighten further studies.},
  archive      = {J_TMM},
  author       = {Di Li and Susanto Rahardja},
  doi          = {10.1109/TMM.2024.3521826},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2194-2205},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rethinking affine transform for efficient image enhancement: A color space perspective},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MaskBlur: Spatial and angular data augmentation for light field image super-resolution. <em>TMM</em>, <em>27</em>, 2181-2193. (<a href='https://doi.org/10.1109/TMM.2024.3521781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation (DA) is an effective approach for enhancing model performance with limited data, such as light field (LF) image super-resolution (SR). LF images inherently possess rich spatial and angular information. Nonetheless, there is a scarcity of DA methodologies explicitly tailored for LF images, and existing works tend to concentrate solely on either the spatial or angular domain. This paper proposes a novel spatial and angular DA strategy named MaskBlur for LF image SR by concurrently addressing spatial and angular aspects. MaskBlur consists of spatial blur and angular dropout two components. Spatial blur is governed by a spatial mask, which controls where pixels are blurred, i.e., pasting pixels between the low-resolution and high-resolution domains. The angular mask is responsible for angular dropout, i.e., selecting which views to perform the spatial blur operation. By doing so, MaskBlur enables the model to treat pixels differently in the spatial and angular domains when super-resolving LF images rather than blindly treating all pixels equally. Extensive experiments demonstrate the efficacy of MaskBlur in significantly enhancing the performance of existing SR methods. We further extend MaskBlur to other LF image tasks such as denoising, deblurring, low-light enhancement, and real-world SR.},
  archive      = {J_TMM},
  author       = {Wentao Chao and Fuqing Duan and Yulan Guo and Guanghui Wang},
  doi          = {10.1109/TMM.2024.3521781},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2181-2193},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MaskBlur: Spatial and angular data augmentation for light field image super-resolution},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VLAB: Enhancing video language pretraining by feature adapting and blending. <em>TMM</em>, <em>27</em>, 2168-2180. (<a href='https://doi.org/10.1109/TMM.2024.3521729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale image-text contrastive pre-training models, such as CLIP, have been demonstrated to effectively learn high-quality multimodal representations. However, there is limited research on learning video-text representations for general video multimodal tasks based on these powerful features. Towards this goal, we propose a novel video-text pre-training method dubbed VLAB: Video Language pre-training by feature Adapting and Blending, which transfers CLIP representations to video pre-training tasks and develops unified video multimodal models for a wide range of video-text tasks. Specifically, VLAB is founded on two key strategies: feature adapting and feature blending. In the former, we introduce a new video adapter module to address CLIP's deficiency in modeling temporal information and extend the model's capability to encompass both contrastive and generative tasks. In the latter, we propose an end-to-end training method that further enhances the model's performance by exploiting the complementarity of image and video features. We validate the effectiveness and versatility of VLAB through extensive experiments on highly competitive video multimodal tasks, including video text retrieval, video captioning, and video question answering. Remarkably, VLAB outperforms competing methods significantly and sets new records in video question answering on MSRVTT, MSVD, and TGIF datasets. It achieves an accuracy of 49.6, 60.9, and 79.0, respectively.},
  archive      = {J_TMM},
  author       = {Xingjian He and Sihan Chen and Fan Ma and Zhicheng Huang and Xiaojie Jin and Zikang Liu and Dongmei Fu and Yi Yang and Jing Liu and Jiashi Feng},
  doi          = {10.1109/TMM.2024.3521729},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2168-2180},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {VLAB: Enhancing video language pretraining by feature adapting and blending},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DIP: Diffusion learning of inconsistency pattern for general DeepFake detection. <em>TMM</em>, <em>27</em>, 2155-2167. (<a href='https://doi.org/10.1109/TMM.2024.3521766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of deepfake generation techniques, the importance of deepfake detection in protecting multimedia content integrity has become increasingly obvious. Recently, temporal inconsistency clues have been explored to improve the generalizability of deepfake video detection. According to our observation, the temporal artifacts of forged videos in terms of motion information usually exhibits quite distinct inconsistency patterns along horizontal and vertical directions, which could be leveraged to improve the generalizability of detectors. In this paper, a transformer-based framework for Diffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits directional inconsistencies for deepfake video detection. Specifically, DIP begins with a spatiotemporal encoder to represent spatiotemporal information. A directional inconsistency decoder is adopted accordingly, where direction-aware attention and inconsistency diffusion are incorporated to explore potential inconsistency patterns and jointly learn the inherent relationships. In addition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to contrast spatiotemporally augmented sample pairs and prevent the model from overfitting nonessential forgery artifacts. Extensive experiments on several public datasets demonstrate that our method could effectively identify directional forgery clues and achieve state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Fan Nie and Jiangqun Ni and Jian Zhang and Bin Zhang and Weizhe Zhang},
  doi          = {10.1109/TMM.2024.3521766},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2155-2167},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DIP: Diffusion learning of inconsistency pattern for general DeepFake detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MENSA: Multi-dataset harmonized pretraining for semantic segmentation. <em>TMM</em>, <em>27</em>, 2127-2140. (<a href='https://doi.org/10.1109/TMM.2024.3521851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing pretraining methods for semantic segmentation are hampered by the task gap between global image -level pretraining and local pixel-level finetuning. Joint dense-level pretraining is a promising alternative to exploit off-the-shelf annotations from diverse segmentation datasets but suffers from low-quality class embeddings and inconsistent data and supervision signals across multiple datasets by directly employing CLIP. To overcome these challenges, we propose a novel Multi-datasEt harmoNized pretraining framework for Semantic sEgmentation (MENSA). MENSA incorporates high-quality language embeddings and momentum-updated visual embeddings to effectively model the class relationships in the embedding space and thereby provide reliable supervision information for each category. To further adapt to multiple datasets, we achieve one-to-many pixel-embedding pairing with cross-dataset multi-label mapping through cross-modal information exchange to mitigate inconsistent supervision signals and introduce region-level and pixel-level cross-dataset mixing for varying data distribution. Experimental results demonstrate that MENSA is a powerful foundation segmentation model that consistently outperforms popular supervised or unsupervised ImageNet pretrained models for various benchmarks under standard fine-tuning. Furthermore, MENSA is shown to significantly benefit frozen-backbone fine-tuning and zero-shot learning by endowing pixel-level distinctiveness to learned representations.},
  archive      = {J_TMM},
  author       = {Bowen Shi and Xiaopeng Zhang and Yaoming Wang and Wenrui Dai and Junni Zou and Hongkai Xiong},
  doi          = {10.1109/TMM.2024.3521851},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2127-2140},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MENSA: Multi-dataset harmonized pretraining for semantic segmentation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLIP-based modality compensation for visible-infrared image re-identification. <em>TMM</em>, <em>27</em>, 2112-2126. (<a href='https://doi.org/10.1109/TMM.2024.3521764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared image re-identification (VIReID) aims to match objects with the same identity appearing across different modalities. Given the significant differences between visible and infrared images, VIReID poses a formidable challenge. Most existing methods focus on extracting modality-shared features while ignore modality-specific features, which often also contain crucial important discriminative information. In addition, high-level semantic information of the objects, such as shape and appearance, is also crucial for the VIReID task. To further enhance the retrieval performance, we propose a novel one-stage CLIP-based Modality Compensation (CLIP-MC) method for the VIReID task. Our method introduces a new prompt learning paradigm that leverages the semantic understanding capabilities of CLIP to recover missing modality information. CLIP-MC comprises three key modules: Instance Text Prompt Generation (ITPG), Modality Compensation (MC), and Modality Context Learner (MCL). Specifically, the ITPG module facilitates effective alignment and interaction between image tokens and text tokens, enhancing the text encoder's ability to capture detailed visual information from the images. This ensures that the text encoder generates fine-grained descriptions of the images. The MCL module captures the unique information of each modality and generates modality-specific context tokens, which are more flexible compared to fixed text descriptions. Guided by the modality-specific context, the text encoder discovers missing modality information from the images and produces compensated modality features. Finally, the MC module combines the original and compensated modality features to obtain complete modality features that contain more discriminative information. We conduct extensive experiments on three VIReID datasets and compare the performance of our method with other existing approaches to demonstrate its effectiveness and superiority.},
  archive      = {J_TMM},
  author       = {Gang Hu and Yafei Lv and Jianting Zhang and Qian Wu and Zaidao Wen},
  doi          = {10.1109/TMM.2024.3521764},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2112-2126},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CLIP-based modality compensation for visible-infrared image re-identification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Texture-content dual guided network for visible and infrared image fusion. <em>TMM</em>, <em>27</em>, 2097-2111. (<a href='https://doi.org/10.1109/TMM.2024.3521840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The preservation and enhancement of texture information is crucial for the fusion of visible and infrared images. However, most current deep neural network (DNN)-based methods ignore the differences between texture and content, leading to unsatisfactory fusion results. To further enhance the quality of fused images, we propose a texture-content dual guided (TCDG-Net) network, which produces the fused image by the guidance inferred from source images. Specifically, a texture map is first estimated jointly by combining the gradient information of visible and infrared images. Then, the features learned by the shallow feature extraction (SFE) module are enhanced with the guidance of the texture map. To effectively model the texture information in the long-range dependencies, we design the texture-guided enhancement (TGE) module, in which the texture-guided attention mechanism is utilized to capture the global similarity of the texture regions in source images. Meanwhile, we employ the content-guided enhancement (CGE) module to refine the content regions in the fused result by utilizing the complement of the texture map. Finally, the fused image is generated by adaptively integrating the enhanced texture and content information. Extensive experiments on three benchmark datasets demonstrate the effectiveness of the proposed TCDG-Net in terms of qualitative and quantitative evaluations. Besides, the fused images generated by our proposed TCDG-Net also show better performance in downstream tasks, such as objection detection and semantic segmentation.},
  archive      = {J_TMM},
  author       = {Kai Zhang and Ludan Sun and Jun Yan and Wenbo Wan and Jiande Sun and Shuyuan Yang and Huaxiang Zhang},
  doi          = {10.1109/TMM.2024.3521840},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2097-2111},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Texture-content dual guided network for visible and infrared image fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive knowledge distillation with attention-based multi-modal fusion for robust dim object detection. <em>TMM</em>, <em>27</em>, 2083-2096. (<a href='https://doi.org/10.1109/TMM.2024.3521793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated object detection in aerial images is crucial in both civil and military applications. Existing computer vision-based object detection methods are not robust enough to precisely detect dim objects in aerial images due to the cluttered backgrounds, various observing angles, small object scales, and severe occlusions. Recently, electroencephalography (EEG)-based object detection methods have received increasing attention owing to the advanced cognitive capabilities of human vision. However, how to combine the human intelligence with computer intelligence to achieve robust dim object detection is still an open question. In this paper, we propose a novel approach to efficiently fuse and exploit the properties of multi-modal data for dim object detection. Specifically, we first design a brain-computer interface (BCI) paradigm called eye-tracking-based slow serial visual presentation (ESSVP) to simultaneously collect the paired EEG and image data when subjects search for the dim objects in aerial images. Then, we develop an attention-based multi-modal fusion network to selectively aggregate the learned features of EEG and image modalities. Furthermore, we propose an adaptive multi-teacher knowledge distillation method to efficiently train the multi-modal dim object detector for better performance. To evaluate the effectiveness of our method, we conduct extensive experiments on the collected dataset in subject-dependent and subject-independent tasks. The experimental results demonstrate that the proposed dim object detection method exhibits superior effectiveness and robustness compared to the baselines and the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Zhen Lan and Zixing Li and Chao Yan and Xiaojia Xiang and Dengqing Tang and Han Zhou and Jun Lai},
  doi          = {10.1109/TMM.2024.3521793},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2083-2096},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive knowledge distillation with attention-based multi-modal fusion for robust dim object detection},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HFGlobalFormer: When high-frequency recovery meets global context modeling for compressed image deraindrop. <em>TMM</em>, <em>27</em>, 2070-2082. (<a href='https://doi.org/10.1109/TMM.2024.3521831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When transmission medium and compression degradation are intertwined, new challenges emerge. This study addresses the problem of raindrop removal from compressed images, where raindrops obscure large areas of the background and compression leads to the loss of high-frequency (HF) information. The restoration of the former requires global contextual information, while the latter necessitates guidance for high-frequency details, resulting in a conflict in utilizing these two types of information when designing existing methods. To address this issue, we propose a novel transformer architecture that leverages the advantages of attention mechanism and HF-friendly design to effectively restore the compressed raindrop images at the framework, component, and module levels. Specifically, at the framework level, we integrate relative position multi-head self-attention and convolutional layers into the proposed low-high-frequency transformer (LHFT), where the former captures global contextual information and the latter focuses on high-frequency information. Their combination effectively resolves the issue of mixed degradation. At the component level, we utilize high-frequency depth-wise convolution (HFDC) with zero-mean kernels to improve the capability to extract high-frequency features, drawing inspiration from typical high-frequency filters like Prewitt and Sobel operators. Finally, at the module level, we introduce a low-high-attention module (LHAM) to adaptively allocate the importance of low and high frequencies along channels for effective fusion. We establish the JPEG-compressed raindrop image dataset and conduct extensive experiments on different compression rates. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods without increasing computational costs.},
  archive      = {J_TMM},
  author       = {Rongqun Lin and Wenhan Yang and Baoliang Chen and Pingping Zhang and Yue Liu and Shiqi Wang and Sam Kwong},
  doi          = {10.1109/TMM.2024.3521831},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2070-2082},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HFGlobalFormer: When high-frequency recovery meets global context modeling for compressed image deraindrop},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Token masking transformer for weakly supervised object localization. <em>TMM</em>, <em>27</em>, 2059-2069. (<a href='https://doi.org/10.1109/TMM.2024.3521828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object localization (WSOL) is both a promising and challenging task that aims to achieve object localization exclusively through image category labels for supervision. Visual transformers have recently been applied to WSOL, demonstrating significant success through the exploitation of long-range feature dependencies in self-attention mechanisms. However, the transformer-based approach suffers from the same partial activation problem as the CNN-based approach due to the use of the classification task to train self-attention map, i.e., only a few discriminative regions are assigned high attention response and thus the localization map does not cover the whole object. To alleviate this problem, we propose a plug-and-play Token Masking Transformer (TMT) method to help transformer-based WSOL methods to obtain a more complete localization map by dynamic discriminative token masking. Specifically, a batch-wise discriminative token selection strategy is first introduced to flexibly determine the tokens to be masked in each image. Then, we design a token masking transformer block to perform token masking and inspire the network to mine more object-related tokens. Besides, we also design an intermediate token activation loss to further improve the performance of TMT by imposing constraints on intermediate tokens. Extensive experiments demonstrate that our TMT can substantially improve the performance of existing transformer-based methods without increasing the computational cost, and achieves state-of-the-art performance on two mainstream benchmarks.},
  archive      = {J_TMM},
  author       = {Wenhao Xu and Changwei Wang and Rongtao Xu and Shibiao Xu and Weiliang Meng and Man Zhang and Xiaopeng Zhang},
  doi          = {10.1109/TMM.2024.3521828},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2059-2069},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Token masking transformer for weakly supervised object localization},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StyleAM: Perception-oriented unsupervised domain adaption for no-reference image quality assessment. <em>TMM</em>, <em>27</em>, 2043-2058. (<a href='https://doi.org/10.1109/TMM.2024.3521705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have shown great potential in no-reference image quality assessment (NR-IQA). However, the annotation of NR-IQA is labor-intensive and time-consuming, which severely limits its application, especially for authentic images. To relieve the dependence on quality annotation, some works have applied unsupervised domain adaptation (UDA) to NR-IQA. However, the above methods ignore the fact that the alignment space used in classification is sub-optimal, since the space is not elaborately designed for perception. To solve this challenge, we propose an effective perception-oriented unsupervised domain adaptation method StyleAM (Style Alignment and Mixup) for NR-IQA, which transfers sufficient knowledge from label-rich source domain data to label-free target domain images. Specifically, we find a more compact and reliable space i.e., feature style space for perception-oriented UDA based on an interesting observation, that the feature style (i.e., the mean and variance) of the deep layer in DNNs is exactly associated with the quality score in NR-IQA. Therefore, we propose to align the source and target domains in a more perceptual-oriented space i.e., the feature style space, to reduce the intervention from other quality-irrelevant feature factors. Furthermore, to increase the consistency (i.e., ordinal/continuous characteristics) between quality score and its feature style, we also propose a novel feature augmentation strategy Style Mixup, which mixes the feature styles (i.e., the mean and variance) before the last layer of DNNs together with mixing their labels. Extensive experimental results on many cross-domain settings (e.g., synthetic to authentic, and multiple distortions to one distortion) have demonstrated the effectiveness of our proposed StyleAM on NR-IQA.},
  archive      = {J_TMM},
  author       = {Yiting Lu and Xin Li and Jianzhao Liu and Zhibo Chen},
  doi          = {10.1109/TMM.2024.3521705},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2043-2058},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {StyleAM: Perception-oriented unsupervised domain adaption for no-reference image quality assessment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VB-KGN: Variational bayesian kernel generation networks for motion image deblurring. <em>TMM</em>, <em>27</em>, 2028-2042. (<a href='https://doi.org/10.1109/TMM.2024.3521805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion blur estimation is a critical and fundamental task in scene analysis and image restoration. While most state-of-the-art deep learning-based methods for single-image motion image deblurring focus on constructing deep networks or developing training strategies, the characterization of motion blur has received less attention. In this paper, we innovatively propose a non-parametric Variational Bayesian Kernel Generation Network (VB-KGN) for characterizing motion blur in a single image. To solve this model, we employ the variational inference framework to approximate the expected statistical distribution of motion blur images in a data-driven manner. The qualitative and quantitative evaluations of our experimental results demonstrate that our proposed model can generate highly accurate motion blur kernels, significantly improving motion image deblurring performance and substantially reducing the need for extensive training sample preprocessing for deblurring tasks.},
  archive      = {J_TMM},
  author       = {Ying Fu and Xinyu Zhu and Xiaojie Li and Xin Wang and Xi Wu and Shu Hu and Yi Wu and Siwei Lyu and Wei Liu},
  doi          = {10.1109/TMM.2024.3521805},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2028-2042},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {VB-KGN: Variational bayesian kernel generation networks for motion image deblurring},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDANet: Modality-aware domain alignment network for visible-infrared person re-identification. <em>TMM</em>, <em>27</em>, 2015-2027. (<a href='https://doi.org/10.1109/TMM.2024.3521822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification is a challenging task in video surveillance. Most existing works achieve performance gains by aligning feature distributions or image styles across modalities, whereas the multi-granularity information and domain knowledge are usually neglected. Motivated by these issues, we propose a novel modality-aware domain alignment network (MDANet) for visible-infrared person re-identification (VI-ReID), which utilizes global-local context cues and the generalized domain alignment strategy to solve modal differences and poor generalization. Firstly, modality-aware global-local context attention (MGLCA) is proposed to obtain multi-granularity context features and identity-aware patterns. Secondly, we present a generalized domain alignment learning head (GDALH) to relieve the modality discrepancy and enhance the generalization of MDANet, whose core idea is to enrich feature diversity in the domain alignment procedure. Finally, the entire network model is trained by proposing cross-modality circle, classification, and domain alignment losses in an end-to-end fashion. We conduct comprehensive experiments on two standards and their corrupted VI-ReID datasets to validate the robustness and generalization of our approach. MDANet is obviously superior to the most state-of-the-art methods. Specifically, the proposed method can gain 8.86% and 2.50% in Rank-1 accuracy on SYSU-MM01 (all-search and single-shot mode) and RegDB (infrared to visible mode) datasets, respectively. The source code will be made available soon.},
  archive      = {J_TMM},
  author       = {Xu Cheng and Hao Yu and Kevin Ho Man Cheng and Zitong Yu and Guoying Zhao},
  doi          = {10.1109/TMM.2024.3521822},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {2015-2027},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MDANet: Modality-aware domain alignment network for visible-infrared person re-identification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive feature mining and external knowledge-assisted text-pedestrian image retrieval. <em>TMM</em>, <em>27</em>, 1973-1987. (<a href='https://doi.org/10.1109/TMM.2024.3521812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-Pedestrian Image Retrieval employs textual description of pedestrian's appearance to identify the corresponding pedestrian image. This task involves modality discrepancy and the challenges posed by textual diversity of pedestrians with the same identity. Although advancements have been made in text-pedestrian image retrieval, current methods do not comprehensively address these challenges. Thus, this paper proposes a progressive feature mining and external knowledge- assisted feature purification method. Specifically, we implement a progressive mining mode, enabling the model to extract discriminative features from overlooked information. This enhances the model's feature representation capabilities and prevents the loss of discriminative information. To further mitigate the challenges posed by modality discrepancy and text diversity in cross-modal matching, we propose to use external knowledge of other samples from the same modality. This approach accentuates identity-consistent features and diminishes identity-inconsistent ones, refining feature representation and reducing interference from textual diversity and negative sample correlation features of the same modality. Extensive experiments on three challenging datasets demonstrate the effectiveness and superiority of the proposed method, with its retrieval performance outstripping that of large-scale model-based methods on large-scale datasets.},
  archive      = {J_TMM},
  author       = {Huafeng Li and Shedan Yang and Yafei Zhang and Dapeng Tao and Zhengtao Yu},
  doi          = {10.1109/TMM.2024.3521812},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1973-1987},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Progressive feature mining and external knowledge-assisted text-pedestrian image retrieval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reordered $k$-means: A new baseline for view-unaligned multi-view clustering. <em>TMM</em>, <em>27</em>, 1962-1972. (<a href='https://doi.org/10.1109/TMM.2024.3521747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current multi-view clustering methods necessitate that a sample's features be view-aligned or at least partially aligned across different views. Regrettably, real-world applications often fail to meet this requirement due to spatial, temporal, or spatiotemporal mismatches, resulting in the view-unaligned issue. To tackle this issue, we conceptualize the view-unaligned problem and demonstrate that it can be transformed into a view-aligned problem through reordering. Building on this concept, we introduce an innovative reorder matrix that realigns view-unaligned features. Utilizing these realigned features, we develop a sophisticated and efficient approach called Reordered $k$-means (RKM), which merges NMF with $k$-means. Unlike traditional $k$-means, our method converts the binary challenge into an $\ell _{0}$ problem, confirming the merit of this advancement. Furthermore, RKM's efficacy is affirmed on benchmarks, indicating substantial enhancements in handling the view-unaligned issue and maintaining competitive results with view-aligned problems.},
  archive      = {J_TMM},
  author       = {Zhiqiang Fu and Yao Zhao and Dongxia Chang and Yiming Wang and Jie Wen},
  doi          = {10.1109/TMM.2024.3521747},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1962-1972},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Reordered $k$-means: A new baseline for view-unaligned multi-view clustering},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Make graph-based referring expression comprehension great again through expression-guided dynamic gating and regression. <em>TMM</em>, <em>27</em>, 1950-1961. (<a href='https://doi.org/10.1109/TMM.2024.3521844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One common belief is that with complex models and pre-training on large-scale datasets, transformer-based methods for referring expression comprehension (REC) perform much better than existing graph-based methods. We observe that since most graph-based methods adopt an off-the-shelf detector to locate candidate objects (i.e., regions detected by the object detector), they face two challenges that result in subpar performance: (1) the presence of significant noise caused by numerous irrelevant objects during reasoning, and (2) inaccurate localization outcomes attributed to the provided detector. To address these issues, we introduce a plug-and-adapt module guided by sub-expressions, called dynamic gate constraint (DGC), which can adaptively disable irrelevant proposals and their connections in graphs during reasoning. We further introduce an expression-guided regression strategy (EGR) to refine location prediction. Extensive experimental results on the RefCOCO, RefCOCO+, RefCOCOg, Flickr30 K, RefClef, and Ref-reasoning datasets demonstrate the effectiveness of the DGC module and the EGR strategy in consistently boosting the performances of various graph-based REC methods. Without any pretaining, the proposed graph-based method achieves better performance than the state-of-the-art (SOTA) transformer-based methods.},
  archive      = {J_TMM},
  author       = {Jingcheng Ke and Dele Wang and Jun-Cheng Chen and I-Hong Jhuo and Chia-Wen Lin and Yen-Yu Lin},
  doi          = {10.1109/TMM.2024.3521844},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1950-1961},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Make graph-based referring expression comprehension great again through expression-guided dynamic gating and regression},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spotlight text detector: Spotlight on candidate regions like a camera. <em>TMM</em>, <em>27</em>, 1937-1949. (<a href='https://doi.org/10.1109/TMM.2024.3521824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The irregular contour representation is one of the tough challenges in scene text detection. Although segmentation-based methods have achieved significant progress with the help of flexible pixel prediction, the overlap of geographically close texts hinders detecting them separately. To alleviate this problem, some shrink-based methods predict text kernels and expand them to restructure texts. However, the text kernel is an artificial object with incomplete semantic features that are prone to incorrect or missing detection. In addition, different from the general objects, the geometry features (aspect ratio, scale, and shape) of scene texts vary significantly, which makes it difficult to detect them accurately. To consider the above problems, we propose an effective spotlight text detector (STD), which consists of a spotlight calibration module (SCM) and a multivariate information extraction module (MIEM). The former concentrates efforts on the candidate kernel, like a camera focus on the target. It obtains candidate features through a mapping filter and calibrates them precisely to eliminate some false positive samples. The latter designs different shape schemes to explore multiple geometric features for scene texts. It helps extract various spatial relationships to improve the model's ability to recognize kernel regions. Ablation studies prove the effectiveness of the designed SCM and MIEM. Extensive experiments verify that our STD is superior to existing state-of-the-art methods on various datasets, including ICDAR2015, CTW1500, MSRA-TD500, and Total-Text.},
  archive      = {J_TMM},
  author       = {Xu Han and Junyu Gao and Chuang Yang and Yuan Yuan and Qi Wang},
  doi          = {10.1109/TMM.2024.3521824},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1937-1949},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spotlight text detector: Spotlight on candidate regions like a camera},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepEraser: Deep iterative context mining for generic text eraser. <em>TMM</em>, <em>27</em>, 1914-1925. (<a href='https://doi.org/10.1109/TMM.2024.3521809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present DeepEraser, an effective deep network for generic text removal. DeepEraser utilizes a recurrent architecture that erases the text in an image via iterative operations. Our idea comes from the process of erasing pencil script, where the text area designated for removal is subject to continuous monitoring and the text is attenuated progressively, ensuring a thorough and clean erasure. Technically, at each iteration, an innovative erasing module is deployed, which not only explicitly aggregates the previous erasing progress but also mines additional semantic context to erase the target text. Through iterative refinements, the text regions are progressively replaced with more appropriate content and finally converge to a relatively accurate status. Furthermore, a custom mask generation strategy is introduced to improve the capability of DeepEraser for adaptive text removal, as opposed to indiscriminately removing all the text in an image. Our DeepEraser is notably compact with only 1.4 M parameters and trained in an end-to-end manner. To verify its effectiveness, extensive experiments are conducted on several prevalent benchmarks, including SCUT-Syn, SCUT-EnsText, and Oxford Synthetic text dataset. The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generalization ability in custom mask text removal.},
  archive      = {J_TMM},
  author       = {Hao Feng and Wendi Wang and Shaokai Liu and Jiajun Deng and Wengang Zhou and Houqiang Li},
  doi          = {10.1109/TMM.2024.3521809},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1914-1925},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DeepEraser: Deep iterative context mining for generic text eraser},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Black-box targeted adversarial attack on segment anything (SAM). <em>TMM</em>, <em>27</em>, 1901-1913. (<a href='https://doi.org/10.1109/TMM.2024.3521769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep recognition models are widely vulnerable to adversarial examples, which change the model output by adding quasi-imperceptible perturbation to the image input. Recently, Segment Anything Model (SAM) has emerged to become a popular foundation model in computer vision due to its impressive generalization to unseen data and tasks. Realizing flexible attacks on SAM is beneficial for understanding the robustness of SAM in the adversarial context. To this end, this work aims to achieve a targeted adversarial attack (TAA) on SAM. Specifically, under a specific prompt, the goal is to make the predicted mask of an adversarial example resemble that of a given target image. The task of TAA on SAM has been realized in the white-box setup by assuming access to prompt and model, which is thus less practical. To address the issue of prompt dependence, we propose a simple yet effective approach by only attacking the image encoder. Moreover, we propose a novel regularization loss to enhance the cross-model transferability by increasing the feature dominance of adversarial images over random natural images. Extensive experiments verify the effectiveness of our proposed method to conduct a successful black-box TAA on SAM.},
  archive      = {J_TMM},
  author       = {Sheng Zheng and Chaoning Zhang and Xinhong Hao},
  doi          = {10.1109/TMM.2024.3521769},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1901-1913},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Black-box targeted adversarial attack on segment anything (SAM)},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An active multi-target domain adaptation strategy: Progressive class prototype rectification. <em>TMM</em>, <em>27</em>, 1874-1886. (<a href='https://doi.org/10.1109/TMM.2024.3521740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to single-source to single-target (1S1T) domain adaptation, single-source to multi-target (1SmT) domain adaptation is more practical but also more challenging. In 1SmT scenarios, the significant differences in feature distributions between various target domains increase the difficulty for models to adapt to multiple domains. Moreover, 1SmT requires effective transfer to each target domain while maintaining performance in the source domain, demanding higher generalization capabilities from the model. In 1S1T scenarios, active domain adaptation methods improve generalization by incorporating a few target domain samples, but these methods are rarely applied in 1SmT due to potential sampling bias and outlier interference. To address this, we propose Progressive Prototype Refinement (PPR), an active multi-target domain adaptation method combining 1SmT with active learning to enhance cross-domain knowledge transfer. Specifically, an uncertainty assessment strategy is used to select representative samples from multiple target domains, forming a candidate set for model training. Based on the Lindeberg--Levy central limit theorem, we sample from a Gaussian distribution using corrected prototype statistics to augment the classifier's feature input, allowing the model to learn transitional information between domains. Finally, a mapping matrix is used for cross-domain alignment, addressing incomplete class coverage and outlier interference. Extensive experiments on multiple benchmark datasets demonstrate PPR's superior performance, with a 6.35% improvement on the PACS dataset and a 17.32% improvement on the Remote Sensing dataset.},
  archive      = {J_TMM},
  author       = {Yanan Zhu and Jiaqiu Ai and Le Wu and Dan Guo and Wei Jia and Richang Hong},
  doi          = {10.1109/TMM.2024.3521740},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1874-1886},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An active multi-target domain adaptation strategy: Progressive class prototype rectification},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Position and orientation aware one-shot learning for medical action recognition from signal data. <em>TMM</em>, <em>27</em>, 1860-1873. (<a href='https://doi.org/10.1109/TMM.2024.3521703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a position and orientation-aware one-shot learning framework for medical action recognition from signal data. The proposed framework comprises two stages and each stage includes signal-level image generation (SIG), cross-attention (CsA), and dynamic time warping (DTW) modules and the information fusion between the proposed privacy-preserved position and orientation features. The proposed SIG method aims to transform the raw skeleton data into privacy-preserved features for training. The CsA module is developed to guide the network in reducing medical action recognition bias and more focusing on important human body parts for each specific action, aimed at addressing similar medical action related issues. Moreover, the DTW module is employed to minimize temporal mismatching between instances and further improve model performance. Furthermore, the proposed privacy-preserved orientation-level features are utilized to assist the position-level features in both of the two stages for enhancing medical action recognition performance. Extensive experimental results on the widely-used and well-known NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets all demonstrate the effectiveness of the proposed method, which outperforms the other state-of-the-art methods with general dataset partitioning by 2.7%, 6.2% and 4.1%, respectively.},
  archive      = {J_TMM},
  author       = {Leiyu Xie and Yuxing Yang and Zeyu Fu and Syed Mohsen Naqvi},
  doi          = {10.1109/TMM.2024.3521703},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1860-1873},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Position and orientation aware one-shot learning for medical action recognition from signal data},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DivDiff: A conditional diffusion model for diverse human motion prediction. <em>TMM</em>, <em>27</em>, 1848-1859. (<a href='https://doi.org/10.1109/TMM.2024.3521821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diverse human motion prediction (HMP) aims to predict multiple plausible future motions given an observed human motion sequence. It is a challenging task due to the diversity of potential human motions while ensuring an accurate description of future human motions. Current solutions are either low-diversity or limited in expressiveness. Recent denoising diffusion probabilistic models (DDPM) demonstrate promising performance in various generative tasks. However, introducing DDPM directly into diverse HMP incurs some issues. While DDPM can enhance the diversity of potential human motion patterns, the predicted human motions gradually become implausible over time due to significant noise disturbances in the forward process of DDPM. This phenomenon leads to the predicted human motions being unrealistic, seriously impacting the quality of predicted motions and restricting their practical applicability in real-world scenarios. To alleviate this, we propose a novel conditional diffusion-based generative model, called DivDiff, to predict more diverse and realistic human motions. Specifically, the DivDiff employs DDPM as our backbone and incorporates Discrete Cosine Transform (DCT) and Transformer mechanisms to encode the observed human motion sequence as a condition to instruct the reverse process of DDPM. More importantly, we design a diversified reinforcement sampling function (DRSF) to enforce human skeletal constraints on the predicted human motions. DRSF utilizes the acquired information from human skeletal as prior knowledge, thereby reducing significant disturbances introduced during the forward process. Extensive results received in the experiments on two widely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model obtains competitive performance on both diversity and accuracy.},
  archive      = {J_TMM},
  author       = {Hua Yu and Yaqing Hou and Wenbin Pei and Yew-Soon Ong and Qiang Zhang},
  doi          = {10.1109/TMM.2024.3521821},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1848-1859},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DivDiff: A conditional diffusion model for diverse human motion prediction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STNet: Deep Audio–Visual fusion network for robust speaker tracking. <em>TMM</em>, <em>27</em>, 1835-1847. (<a href='https://doi.org/10.1109/TMM.2024.3521737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual speaker tracking aims to determine the location of human targets in a scene using signals captured by a multi-sensor platform, whose accuracy and robustness can be improved by multi-modal fusion methods. Recently, several fusion methods have been proposed to model the correlation in multiple modalities. However, for the speaker tracking problem, the cross-modal interaction between audio and visual signals hasn't been well exploited. To this end, we present a novel Speaker Tracking Network (STNet) with a deep audio-visual fusion model in this work. We design a visual-guided acoustic measurement method to fuse heterogeneous cues in a unified localization space, which employs visual observations via a camera model to construct the enhanced acoustic map. For feature fusion, a cross-modal attention module is adopted to jointly model multi-modal contexts and interactions. The correlated information between audio and visual features is further interacted in the fusion model. Moreover, the STNet-based tracker is applied to multi-speaker cases by a quality-aware module, which evaluates the reliability of multi-modal observations to achieve robust tracking in complex scenarios. Experiments on the AV16.3 and CAV3D datasets show that the proposed STNet-based tracker outperforms uni-modal methods and state-of-the-art audio-visual speaker trackers.},
  archive      = {J_TMM},
  author       = {Yidi Li and Hong Liu and Bing Yang},
  doi          = {10.1109/TMM.2024.3521737},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1835-1847},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {STNet: Deep Audio–Visual fusion network for robust speaker tracking},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A twist representation and shape refinement method for human mesh recovery. <em>TMM</em>, <em>27</em>, 1821-1834. (<a href='https://doi.org/10.1109/TMM.2024.3521780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human mesh recovery from single RGB images or monocular videos is a challenging task. The twist representation utilized in existing inverse kinematics-based methods fails to accurately describe the twisting posture when the estimated bone direction is imprecise. Additionally, supervising SMPL shape parameters has the issue of shape estimation overfitting due to limited training data. This often results in compromised bone lengths that subsequently impair the precision of joint positions. To address these issues, we propose a framework that breaks down both human pose and shape into finer components, effectively managing and minimizing errors within each component. The proposed framework integrates two key advancements: the advanced Ortho-Twist and Swing Representation (OTSR) and the Skeleton-Focused Shape Refinement (SFSR). OTSR offers a more sophisticated representation for limb rotations compared to the traditional twist angle and swing representation to enhance the accuracy of twisting posture estimation. SFSR refines the estimated SMPL shape parameters by fitting bone lengths using the estimated joint positions, thereby significantly mitigating shape overfitting and enhancing joint position accuracy in the recovered mesh. We conduct experiments on the Human3.6 M and 3DPW datasets. The results demonstrate the superiority of the proposed framework in both single-image and video scenarios. Additionally, the ablation studies confirm the effectiveness of our proposed modules, and further generalizability experiments demonstrate that our two key advancements can serve as plug-and-play modules to enhance existing methods.},
  archive      = {J_TMM},
  author       = {Xiaoyang Hao and Han Li and Jing Sun and Lei Wang and Jianping Fan},
  doi          = {10.1109/TMM.2024.3521780},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1821-1834},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A twist representation and shape refinement method for human mesh recovery},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised low-light image enhancement with self-paced learning. <em>TMM</em>, <em>27</em>, 1808-1820. (<a href='https://doi.org/10.1109/TMM.2024.3521752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement (LIE) aims to restore images taken under poor lighting conditions, thereby extracting more information and details to robustly support subsequent visual tasks. While past deep learning (DL)-based techniques have achieved certain restoration effects, these existing methods treat all samples equally, ignoring the fact that difficult samples may be detrimental to the network's convergence at the initial training stages of network training. In this paper, we introduce a self-paced learning (SPL)-based LIE method named SPNet, which consists of three key components: the feature extraction module (FEM), the low-light image decomposition module (LIDM), and a pre-trained denoise module. Specifically, for a given low-light image, we first input the image, its pseudo-reference image, and its histogram-equalized version into the FEM to obtain preliminary features. Second, to avoid ambiguities during the early stages of training, these features are then adaptively fused via an SPL strategy and processed for retinex decomposition via LIDM. Third, we enhance the network performance by constraining the gradient prior relationship between the illumination components of the images. Finally, a pre-trained denoise module reduces noise inherent in LIE. Extensive experiments on nine public datasets reveal that the proposed SPNet outperforms eight state-of-the-art DL-based methods in both qualitative and quantitative evaluations and outperforms three conventional methods in quantitative assessments.},
  archive      = {J_TMM},
  author       = {Yu Luo and Xuanrong Chen and Jie Ling and Chao Huang and Wei Zhou and Guanghui Yue},
  doi          = {10.1109/TMM.2024.3521752},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1808-1820},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unsupervised low-light image enhancement with self-paced learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetric hallucination with knowledge transfer for few-shot learning. <em>TMM</em>, <em>27</em>, 1797-1807. (<a href='https://doi.org/10.1109/TMM.2024.3521802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data hallucination or augmentation is a straightforward solution for few-shot learning (FSL), where FSL is proposed to classify a novel object under limited training samples. Common hallucination strategies use visual or textual knowledge to simulate the distribution of a given novel category and generate more samples for training. However, the diversity and capacity of generated samples through these techniques can be insufficient when the knowledge domain of the novel category is narrow. Therefore, the performance improvement of the classifier is limited. To address this issue, we propose a Symmetric data hallucination strategy with Knowledge Transfer (SHKT) that interacts with multi-modal knowledge in both visual and textual spaces. Specifically, we first calculate the relations based on semantic knowledge and select the most related categories of a given novel category for hallucination. Second, we design two parameter-free data hallucination strategies to enrich the training samples by mixing the given and selected samples in both visual and textual spaces. The generated visual and textual samples improve the visual representation and enrich the textual supervision, respectively. Finally, we connect the visual and textual knowledge through transfer calculation, which not only exchanges content from different modalities but also constrains the distribution of the generated samples during the training. We apply our method to four benchmark datasets and achieve state-of-the-art performance in all experiments. Specifically, compared to the baseline on the Mini-ImageNet dataset, it achieves 12.84% and 3.46% accuracy improvements for 1 and 5 support training samples, respectively.},
  archive      = {J_TMM},
  author       = {Shuo Wang and Xinyu Zhang and Meng Wang and Xiangnan He},
  doi          = {10.1109/TMM.2024.3521802},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1797-1807},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Symmetric hallucination with knowledge transfer for few-shot learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-assisted mamba for remote sensing image super-resolution. <em>TMM</em>, <em>27</em>, 1783-1796. (<a href='https://doi.org/10.1109/TMM.2024.3521798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in remote sensing image (RSI) super-resolution (SR) has exhibited remarkable performance using deep neural networks, e.g., Convolutional Neural Networks and Transformers. However, existing SR methods often suffer from either a limited receptive field or quadratic computational overhead, resulting in sub-optimal global representation and unacceptable computational costs in large-scale RSI. To alleviate these issues, we develop the first attempt to integrate the Vision State Space Model (Mamba) for RSI-SR, which specializes in processing large-scale RSI by capturing long-range dependency with linear complexity. To achieve better SR reconstruction, building upon Mamba, we devise a Frequency-assisted Mamba framework, dubbed FMSR, to explore the spatial and frequent correlations. In particular, our FMSR features a multi-level fusion architecture equipped with the Frequency Selection Module (FSM), Vision State Space Module (VSSM), and Hybrid Gate Module (HGM) to grasp their merits for effective spatial-frequency fusion. Considering that global and local dependencies are complementary and both beneficial for SR, we further recalibrate these multi-level features for accurate feature fusion via learnable scaling adaptors. Extensive experiments on AID, DOTA, and DIOR benchmarks demonstrate that our FMSR outperforms state-of-the-art Transformer-based methods HAT-L in terms of PSNR by 0.11 dB on average, while consuming only 28.05% and 19.08% of its memory consumption and complexity, respectively.},
  archive      = {J_TMM},
  author       = {Yi Xiao and Qiangqiang Yuan and Kui Jiang and Yuzeng Chen and Qiang Zhang and Chia-Wen Lin},
  doi          = {10.1109/TMM.2024.3521798},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1783-1796},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frequency-assisted mamba for remote sensing image super-resolution},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DuPMAM: An efficient dual perception framework equipped with a sharp testing strategy for point cloud analysis. <em>TMM</em>, <em>27</em>, 1760-1771. (<a href='https://doi.org/10.1109/TMM.2024.3521735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenges in point cloud analysis are primarily attributed to the irregular and unordered nature of the data. Numerous existing approaches, inspired by the Transformer, introduce attention mechanisms to extract the 3D geometric features. However, these intricate geometric extractors incur high computational overhead and unfavorable inference latency. To tackle this predicament, in this paper, we propose a lightweight and faster attention-based network, named Dual Perception MAM (DuPMAM), for point cloud analysis. Specifically, we present a novel simple Point Multiplicative Attention Mechanism (PMAM). It is implemented solely through single feed-forward fully connected layers, hence leading to lower model complexity and superior inference speed. Based on that, we further devise a dual perception strategy by constructing both a local attention block and a global attention block to learn fine-grained geometric and overall representational features, respectively. Consequently, compared to the existing approaches, our method has excellent perception of local details and global contours of the point cloud objects. In addition, we ingeniously design a Graph-Multiscale Perceptual Field (GMPF) testing strategy for model performance enhancement. It has significant advantage over the traditional voting strategy and is generally applicable to point cloud tasks, encompassing classification, part segmentation and indoor scene segmentation. Empowered by the GMPF testing strategy, DuPMAM delivers the new State-of-the-Art on the real-world dataset ScanObjectNN, the synthetic dataset ModelNet40 and the part segmentation dataset ShapeNet, and compared to the recent GB-Net, our DuPMAM trains 6 times faster and tests 2 times faster.},
  archive      = {J_TMM},
  author       = {Yijun Chen and Xianwei Zheng and Zhulun Yang and Xutao Li and Jiantao Zhou and Yuanman Li},
  doi          = {10.1109/TMM.2024.3521735},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1760-1771},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DuPMAM: An efficient dual perception framework equipped with a sharp testing strategy for point cloud analysis},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MGKsite: Multi-modal knowledge-driven site selection via intra and inter-modal graph fusion. <em>TMM</em>, <em>27</em>, 1722-1735. (<a href='https://doi.org/10.1109/TMM.2024.3521742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Site selection aims to select optimal locations for new stores, which is crucial in business management and urban computing. The early data-driven models heavily relied on feature engineering, which could not effectively model the complex relationships and diverse influences among different data. To alleviate such issues, the knowledge-driven paradigm is proposed based on urban knowledge graphs (KGs). However, the research on them is at an early stage. They omit extra multi-modal information corresponding to brands and stores due to two main challenges, i.e., (1) building available datasets, and (2) designing effective models. It constrains the expressive ability and practical value of previous models. To this end, we first construct new multi-modal urban KGs for site selection with three extra modal (i.e., visual, textual, and acoustic) attributes. Then, we propose a novel multi-modal knowledge-driven model (MGKsite). Concretely, a graph neural network (GNN) based fusion network is designed to fuse the features based on the attribute K-Nearest Neighbor (KNN) graph, which models both intra and inter-modal correlations among the features. The fused embeddings are further injected into the knowledge-driven backbones for learning and inference. Experiments prove promising capacities of MGKsite from five aspects, i.e., superiority, effectiveness, sensitivity, transferability and complexity.},
  archive      = {J_TMM},
  author       = {Ke Liang and Lingyuan Meng and Hao Li and Meng Liu and Siwei Wang and Sihang Zhou and Xinwang Liu and Kunlun He},
  doi          = {10.1109/TMM.2024.3521742},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1722-1735},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {MGKsite: Multi-modal knowledge-driven site selection via intra and inter-modal graph fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate-PGNet: Learning to assemble perceptual body parts for accurate human skeleton establishment. <em>TMM</em>, <em>27</em>, 1706-1721. (<a href='https://doi.org/10.1109/TMM.2024.3521763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human skeleton establishment aims to provide accurate localization information of the human body from RGB images and establish a complete human skeleton for many applications, such as action recognition, video surveillance, and human-computer interaction. Considering the inherent human body structure, many recent methods group the relevant body parts and utilize the deep convolutional network to learn the visual context from the part groups. However, the grouping approaches used in these methods heavily rely on prior knowledge of the human body shape but lose important relationships between parts. In this paper, we introduce the Accurate Part Grouping Network (Accurate-PGNet), a novel network for hierarchically grouping body parts in a data-driven manner. In contrast to the previous methods, we use neural architecture search (NAS) to optimize the architecture of Accurate-PGNet and properly group the body parts. The part grouping respects the diverse visual patterns of parts, producing groups containing different body parts. From each group, we learn the visual feature map. It helps to capture the correlation between parts and predict their locations. The feature maps of the part groups are merged hierarchically to capture the higher-order context of parts in larger groups. We extensively evaluated our method on the challenging benchmarks, demonstrating that Accurate-PGNet effectively helps to achieve state-of-the-art results.},
  archive      = {J_TMM},
  author       = {Renjie Zhang and Di Lin and Xin Wang and George Baciu and C. L. Philip Chen and Ping Li},
  doi          = {10.1109/TMM.2024.3521763},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1706-1721},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Accurate-PGNet: Learning to assemble perceptual body parts for accurate human skeleton establishment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPDFusion:A semantic prior knowledge-driven method for infrared and visible image fusion. <em>TMM</em>, <em>27</em>, 1691-1705. (<a href='https://doi.org/10.1109/TMM.2024.3521848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion is currently an important research direction in the field of multimodal image fusion, which aims to utilize the complementary information between infrared images and visible images to generate a new image containing richer information. In recent years, many deep learning-based methods for infrared and visible image fusion have emerged.However, most of these approaches ignore the importance of semantic information in image fusion, resulting in the generation of fused images that do not perform well enough in human visual perception and advanced visual tasks.To address this problem, we propose a semantic prior knowledge-driven infrared and visible image fusion method. The method utilizes a pre-trained semantic segmentation model to acquire semantic information of infrared and visible images, and drives the fusion process of infrared and visible images through semantic feature perception module and semantic feature embedding module.Meanwhile, we divide the fused image into each category block and consider them as components, and utilize the regional semantic adversarial loss to enhance the adversarial network generation ability in different regions, thus improving the quality of the fused image.Through extensive experiments on widely used datasets, the results show that our approach outperforms current leading algorithms in both human eye visualization and advanced visual tasks.},
  archive      = {J_TMM},
  author       = {Quanquan Xiao and Haiyan Jin and Haonan Su and Yuanlin Zhang and Zhaolin Xiao and Bin Wang},
  doi          = {10.1109/TMM.2024.3521848},
  journal      = {IEEE Transactions on Multimedia},
  pages        = {1691-1705},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SPDFusion:A semantic prior knowledge-driven method for infrared and visible image fusion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
