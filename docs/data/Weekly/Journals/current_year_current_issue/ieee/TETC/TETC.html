<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TETC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tetc">TETC - 56</h2>
<ul>
<li><details>
<summary>
(2025). Breakout local search solution to the offloading decision problem in a multi-access edge computing cloud-enabled network. <em>TETC</em>, <em>13</em>(3), 1328-1338. (<a href='https://doi.org/10.1109/TETC.2025.3598369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud offloading is an important technique for Internet of Things systems, as it allows devices with limited capabilities to access the powerful resources in the cloud when executing their applications. However, relying solely on the remote cloud is problematic, as the long access time from the far distance to the server makes real-time applications impossible to be executed. Multi-access edge computing addresses this by deploying cloud servers near the devices. The issue then becomes how to allocate devices between either remote cloud and multi-access edge computing, based on the device requirements. In this paper, we propose a Breakout Local Search-based solution that, given our designed binary integer linear programming model of the offloading problem, finds a near-optimal configuration for allocating devices between the two cloud types. The proposal is based on iterating between exploiting the local optimum found so far and perturbation of the current solution to explore more the search space. A comparison study shows that our proposal is better than baseline and conventional algorithms, speeding up the total service delay of tasks by at least 30 ms.},
  archive      = {J_TETC},
  author       = {Mina Kato and Tiago Koketsu Rodrigues and Nei Kato},
  doi          = {10.1109/TETC.2025.3598369},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1328-1338},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Breakout local search solution to the offloading decision problem in a multi-access edge computing cloud-enabled network},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentive mechanism design for hierarchical federated learning with selfishness queue stability. <em>TETC</em>, <em>13</em>(3), 1316-1327. (<a href='https://doi.org/10.1109/TETC.2025.3562336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential privacy breaches in centralized artificial intelligence model training have raised significant public concern. Hierarchical federated learning, as a technology addressing privacy and network efficiency issues, coordinates local devices using edge servers for model training and parameter updates, thereby reducing communication with central cloud servers and diminishing the risk of privacy leaks. However, in this context, the rise of node selfishness presents a significant challenge, undermining training efficiency and the quality of local models, thereby impacting the overall system’s performance. This paper addresses the issue by introducing a virtual node selfish queue to characterize dynamic selfishness, considering both training costs and rewards, and formulating the problem of maximizing model quality within the bounds of controlled node selfishness. Utilizing Lyapunov optimization, this issue is divided into two subproblems: controlling the quantity of node data and optimizing node associations. To solve these, we propose the Data Quantity Control and Client Association (DCCA) algorithm, based on the Hungarian method. This algorithm is shown to ensure boundedness, stability, and optimality in the system. Experimental results demonstrate that the DCCA algorithm enhances model quality by 8.43% and 13.83% compared to the Fmore and FedAvg algorithms, respectively.},
  archive      = {J_TETC},
  author       = {Zhuo Li and Fangxing Geng},
  doi          = {10.1109/TETC.2025.3562336},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1316-1327},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Incentive mechanism design for hierarchical federated learning with selfishness queue stability},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GLAMP: Generative learning for adversarially-robust malware prediction. <em>TETC</em>, <em>13</em>(3), 1299-1315. (<a href='https://doi.org/10.1109/TETC.2025.3583872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Generative Malware Defense strategy. When an antivirus company detects a malware sample $m$, they should: (i) generate a set ${Var}(m)$ of several variants of $m$ and then (ii) train their malware classifiers on their usual training set augmented with ${Var}(m)$. We believe this leads to a more proactive defense by making the classifiers more robust to future malware developed by the attacker. We formally define the malware generation problem as a non-traditional optimization problem. Our novel GLAMP (Generative Learning for Adversarially-robust Malware Prediction) framework analyzes the complexity of the malware generation problem and includes novel malware variant generation algorithms for (i) that leverage the complexity results. Our experiments show that a sufficiently large percentage of samples generated by GLAMP are able to evade both commercial anti-virus and machine learning classifiers with evasion rates up to 83.81% and 50.54%, respectively. GLAMP then proposes an adversarial training model as well. Our experiments show that GLAMP generates running malware that can evade 11 white boxclassifiers and 4 commercial (i.e., black box) detectors. Our experiments show GLAMP’s best adversarial training engine improves the recall by 16.1% and the F1 score by 2.4%-5.4% depending on the test set used.},
  archive      = {J_TETC},
  author       = {Saurabh Kumar and Cristian Molinaro and Lirika Sola and V. S. Subrahmanian},
  doi          = {10.1109/TETC.2025.3583872},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1299-1315},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {GLAMP: Generative learning for adversarially-robust malware prediction},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving publicly verifiable outsourced distributed computation scheme for matrix multiplication. <em>TETC</em>, <em>13</em>(3), 1285-1298. (<a href='https://doi.org/10.1109/TETC.2025.3584354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publicly verifiable outsourced computation (PVC) facilitates the data owner to outsource some computation-intensive tasks to the powerful but untrusted cloud server, while enabling any client to check the integrity of results with little cost. Matrix multiplication is a fundamental operation in mathematics, which is widely used in many real-world applications. In this paper, we focus on PVC for matrix multiplication (PVC2M) and propose a new primitive called privacy-preserving publicly verifiable outsourced distributed computation scheme (PPVDC) for matrix multiplication. Different from the existing PVC2M solutions, our proposed scheme offers higher efficiency and reliability, where the computation is jointly calculated by multiple workers. In such a distributed setting, the computation result can be recovered if the number of workers who perform the computation honestly is no less than threshold. Besides, another technical highlight is to enhance privacy. Even though all workers are corrupted and may collude, they are unable to obtain any knowledge about the matrix $M$ outsourced by the data owner and the vector $x$ issued by the client at the end of the protocol. Security analysis demonstrates that our proposed PPVDC scheme can meet the desired security requirements under the computational Diffie-Hellman assumption. The detailed performance analysis and experimental evaluation further validate the efficiency of our scheme.},
  archive      = {J_TETC},
  author       = {Qiang Wang and Yiheng Chen and Fucai Zhou and Jian Xu},
  doi          = {10.1109/TETC.2025.3584354},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1285-1298},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Privacy-preserving publicly verifiable outsourced distributed computation scheme for matrix multiplication},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path integral quantum annealing optimizations validated on 0-1 multidimensional knapsack problem. <em>TETC</em>, <em>13</em>(3), 1272-1284. (<a href='https://doi.org/10.1109/TETC.2025.3583224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum Annealing (QA) is a metaheuristic designed to enhance Simulated Annealing by leveraging concepts from quantum mechanics, improving parallelization on classical computers. Studies have shown promising results for this technique in the field of NP-hard problems and constrained optimization. In this article, we examine Path Integral Quantum Annealing (PIQA), a well-known technique for simulating QA on conventional computers. We then propose optimizations to the algorithm, offering hardware software developers a suite of parallelization techniques evaluated for their effectiveness in enhancing quality and speed. The proposed approach encompasses four distinct degrees of optimization, leveraging techniques based on multiple-trial parallelism and a novel pre-optimization method. The article further proposes a methodology for handling multiple instances within the search space, whereby problem data is replicated into slices and allocated to concurrent processes during the simulation. Through empirical trials, we evaluate the impact of our optimization techniques on the convergence speed of the algorithm compared to unoptimized PIQA, using the Multidimensional Knapsack Problem as a benchmark. Our findings show that these optimizations, applied individually or collectively, enable the algorithm to achieve equal or superior results with fewer simulation steps. Overall, the results highlight the potential for future implementations of optimized PIQA on dedicated hardware.},
  archive      = {J_TETC},
  author       = {Evelina Forno and Riccardo Pignari and Vittorio Fra and Enrico Macii and Gianvito Urgese},
  doi          = {10.1109/TETC.2025.3583224},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1272-1284},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Path integral quantum annealing optimizations validated on 0-1 multidimensional knapsack problem},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved modular multiplication algorithms using solely IEEE 754 binary floating-point operations. <em>TETC</em>, <em>13</em>(3), 1259-1271. (<a href='https://doi.org/10.1109/TETC.2025.3582551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose three modular multiplication algorithms that use only the IEEE 754 binary floating-point operations. Several previous studies have used floating-point operations to perform modular multiplication. However, they considered only positive integers and did not utilize the dedicated sign bit in the floating-point representation. Our first algorithm is an extension of these studies, which are based on Shoup multiplication. By allowing operands to be negative, we increased the maximum supported modulus size by approximately 1.21 times. Our remaining two algorithms are based on Montgomery multiplication for positive and signed integers, respectively. Although these algorithms require more round-to-integral operations, they support a modulus size of up to twice as large as that for Shoup multiplication for positive integers. For processors with relatively low round-to-integral performance, we propose versions of the three algorithms without the round-to-integral operation. Evaluations on four CPUs with different levels of instruction performance show that floating-point-based algorithms, including the proposed algorithms, can be regarded as alternatives to integer-based algorithms for mid-sized moduli, especially when floating-point operations are faster on the processors.},
  archive      = {J_TETC},
  author       = {Yukimasa Sugizaki and Daisuke Takahashi},
  doi          = {10.1109/TETC.2025.3582551},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1259-1271},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Improved modular multiplication algorithms using solely IEEE 754 binary floating-point operations},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSTable: A new white-box cipher for embedded devices in IoT against side-channel attacks. <em>TETC</em>, <em>13</em>(3), 1242-1258. (<a href='https://doi.org/10.1109/TETC.2025.3575787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedded devices such as sensors and surveillance cameras play a critical role in the Internet of Things (IoT). However, their unattended and wireless features expose them to a high risk of side-channel attacks. These attacks exploit information leakage through side channels to deduce secret keys or even extract implementations of cryptographic algorithms. The possession of such knowledge empowers attackers to decrypt sensitive information transmitted among IoT devices, posing a significant threat to data confidentiality. To address this issue, we propose LSTable, a new white-box cipher enlightened by LS-Design. Instead of directly using secret keys for encryption and decryption, LSTable transforms secret keys into key-dependent lookup tables to mitigate side-channel attacks, and the size of these tables is designed to fit the hardware constraints of embedded devices. The security analysis of LSTable shows its security in both the black-box and white-box models. Furthermore, experimental evaluations on different devices exhibit that even the efficiency of the slowest instances of LSTable is 2.2 to 14.8 times that of existing space-hard white-box ciphers with IoT-friendly table sizes, while the energy consumption is only around 1/13 to 1/3.},
  archive      = {J_TETC},
  author       = {Yang Shi and Yimin Li and Qiaoliang Ouyang and Jiayao Gao and Shengjie Zhao},
  doi          = {10.1109/TETC.2025.3575787},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1242-1258},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {LSTable: A new white-box cipher for embedded devices in IoT against side-channel attacks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel RFET-based FPGA architecture based on delay-aware packing algorithm. <em>TETC</em>, <em>13</em>(3), 1230-1241. (<a href='https://doi.org/10.1109/TETC.2025.3572712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable devices are attracting growing interest as both a potential alternative and complement to traditional CMOS technology. This paper develops a novel field-programmable gate array (FPGA) architecture based on MClusters, which is made of fast and area-efficient 2-input look-up tables (LUTs) through reconfigurable field-effect transistors (RFETs). To fully utilize the MClusters, we propose an SAT-based delay-aware packing algorithm for the technology mapping. In addition, we integrate a partitioning algorithm to divide the circuit into several sub-circuits to further reduce the global routing resources and their associated switching energy of the system. Finally, we develop an efficient technology/circuit/system co-design framework for optimizing the overall performance of FPGAs. Based on comprehensive benchmarking, results demonstrate that optimal design yields significant reductions of up to 39% area, 36% wire length, and 40% switching energy compared to traditional CMOS 6-input LUT FPGAs.},
  archive      = {J_TETC},
  author       = {Sheng Lu and Liuting Shang and Sungyong Jung and Qilian Liang and Chenyun Pan},
  doi          = {10.1109/TETC.2025.3572712},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1230-1241},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A novel RFET-based FPGA architecture based on delay-aware packing algorithm},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What, when, where to compute-in-memory for efficient matrix multiplication during machine learning inference. <em>TETC</em>, <em>13</em>(3), 1215-1229. (<a href='https://doi.org/10.1109/TETC.2025.3574508'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix multiplication is the dominant computation during Machine Learning (ML) inference. To efficiently perform such multiplication operations, Compute-in-memory (CiM) paradigms have emerged as a highly energy efficient solution. However, integrating compute in memory poses key questions, such as 1) What type of CiM to use: Given a multitude of CiM design characteristics, determining their suitability from architecture perspective is needed. 2) When to use CiM: ML inference includes workloads with a variety of memory and compute requirements, making it difficult to identify when CiM is more beneficial than standard processing cores. 3) Where to integrate CiM: Each memory level has different bandwidth and capacity, creating different data reuse opportunities for CiM integration. To answer such questions regarding on-chip CiM integration for accelerating ML workloads, we use an analytical architecture-evaluation methodology with tailored mapping algorithm. The mapping algorithm aims to achieve highest weight reuse and reduced data movements for a given CiM prototype and workload. Our analysis considers the integration of CiM prototypes into the cache levels of a tensor-core-like architecture, and shows that CiM integrated memory improves energy efficiency by up to $3.4 \times$ and throughput by up to $15.6 \times$ compared to established baseline with INT-8 precision. We believe the proposed work provides insights into what type of CiM to use, and when and where to optimally integrate it in the cache hierarchy for efficient matrix multiplication.},
  archive      = {J_TETC},
  author       = {Tanvi Sharma and Mustafa Ali and Indranil Chakraborty and Kaushik Roy},
  doi          = {10.1109/TETC.2025.3574508},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1215-1229},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {What, when, where to compute-in-memory for efficient matrix multiplication during machine learning inference},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The cancelable multimodal template protection algorithm based on random index. <em>TETC</em>, <em>13</em>(3), 1200-1214. (<a href='https://doi.org/10.1109/TETC.2025.3574359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current multimodal template protection methods typically require encryption or transformation of the original biometric features. However, these operations carry certain risks, as attackers may reverse-engineer or decrypt the protected multimodal templates to retrieve partial or complete information about the original templates, leading to the leakage of the original biometric features. To address this issue, we propose a cancelable multimodal template protection method based on random indexing. First, hash functions are used to generate integer sequences as index values, which are then employed to create single-modal cancelable templates using random binary vectors. Second, the single-modal cancelable templates are used as indices for random binary sequences, which locate the corresponding template information and are filled into the fusion cancelable template at the respective positions, achieving template fusion. The resulting template is unrelated to the original biometric features. Finally, without directly storing the binary factor sequences, an XOR operation is performed on the extended biometric feature vectors and random binary sequences to generate the encoded key. Experimental results demonstrate that the proposed method significantly enhances performance on the FVC2002DB1 fingerprint, MMCBNU_6000 finger-vein, and NUPT_FPV databases, while also satisfying the standards for cancelable biometric feature design. We also analyze four privacy and security attacks against this scheme.},
  archive      = {J_TETC},
  author       = {Huabin Wang and Mingzhao Wang and Xinxin Liu and Yingfan Cheng and Fei Liu and Jian Zhou and Liang Tao},
  doi          = {10.1109/TETC.2025.3574359},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1200-1214},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {The cancelable multimodal template protection algorithm based on random index},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DT-net: Point cloud completion network with neighboring adaptive denoiser and splitting-based upsampling transformer. <em>TETC</em>, <em>13</em>(3), 1185-1199. (<a href='https://doi.org/10.1109/TETC.2025.3573505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion, which involves inferring missing regions of 3D objects from partial observations, remains a challenging problem in 3D vision and robotics. Existing learning-based frameworks typically leverage an encoder-decoder architecture to predict the complete point cloud based on the global shape representation extracted from the incomplete input, or further introduce a refinement network to optimize the obtained complete point cloud in a coarse-to-fine manner, which is unable to capture fine-grained local geometric details and filled with noisy points in the thin or complex structure. In this article, we propose a novel coarse-to-fine point cloud completion framework called DT-Net, by focusing on coarse point cloud denoising and multi-level upsampling. Specifically, we propose a Neighboring Adaptive Denoiser (NAD) to effectively denoise the coarse point cloud generated by an autoencoder, and reduce noise around the slender structures, making them clear and well represented. Moreover, a novel Splitting-based Upsampling Transformer (SUT), which effectively incorporates spatial and semantic relationships between local neighborhoods in the point cloud, is also proposed for multi-level upsampling. Extensive qualitative and quantitative experiments demonstrate that our method outperforms state-of-the-art methods under widely used benchmarks.},
  archive      = {J_TETC},
  author       = {Aihua Mao and Qing Liu and Yuxuan Tang and Sheng Ye and Ran Yi and Minjing Yu and Yong-Jin Liu},
  doi          = {10.1109/TETC.2025.3573505},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1185-1199},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DT-net: Point cloud completion network with neighboring adaptive denoiser and splitting-based upsampling transformer},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PipeDAP: An efficient communication framework for scheduling decoupled all-reduce primitives in distributed DNN training. <em>TETC</em>, <em>13</em>(3), 1170-1184. (<a href='https://doi.org/10.1109/TETC.2025.3573522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication scheduling effectively improves the scalability of distributed deep learning by overlapping computation and communication tasks during training. However, existing communication scheduling frameworks based on tensor partitioning suffer from two fundamental issues: (1) partitioning schemes at the data volume level introduce extensive startup overheads leading to higher energy consumption, and (2) partitioning schemes at the communication primitive level do not provide optimal scheduling resulting in longer training time. In this article, we propose an efficient communication mechanism, namely PipeDAP, which schedules decoupled all-reduce operations in a near-optimal order to minimize the time and energy consumption of training DNN models. We build the mathematical model for PipeDAP and derive the near-optimal scheduling order of the reduce-scatter and all-gather operations. Meanwhile, we leverage simultaneous communication of reduce-scatter and all-gather operations to further reduce the startup overheads. We implement the PipeDAP architecture on PyTorch framework, and apply it for distributed training of benchmark DNN models. Experimental results on two GPU clusters demonstrate that PipeDAP achieves up to 1.82x speedup and saves up to 45.4% of energy consumption compared to the state-of-the-art communication scheduling frameworks.},
  archive      = {J_TETC},
  author       = {Yunqi Gao and Bing Hu and Mahdi Boloursaz Mashhadi and Wei Wang and Rahim Tafazolli and Mérouane Debbah},
  doi          = {10.1109/TETC.2025.3573522},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1170-1184},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PipeDAP: An efficient communication framework for scheduling decoupled all-reduce primitives in distributed DNN training},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware-software co-design low-complexity fuzzy algorithm for high-dimensional feature space with in-situ learning. <em>TETC</em>, <em>13</em>(3), 1156-1169. (<a href='https://doi.org/10.1109/TETC.2025.3573013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploitation of fuzzy techniques in machine learning for modelling uncertainty is highly versatile and widespread. In addition to providing a suitable fuzzy algorithm, its hardware implementation targeting proper performance in real-time applications is another challenge. In this paper, we propose a technique to deal with high-dimensional systems. Despite considering the uncertainty in the data, the proposed algorithm (HCLFA) is low-complex, thanks to the small number of calculations required. In the proposed algorithm, a D-dimensional system is transformed into a D one-dimensional system, and a Gaussian function is distributed over each training data. The main contribution is to provide a complete fuzzy system with tuning one parameter and simple operation without any complex mathematical relationships. The paper also presents an efficient memristor-crossbar hardware structure for the algorithm. The circuit is designed, and the ArC One hardware platform is used to demonstrate the results experimentally. It can learn and adapt to the application without the need for a host system and can be utilized as a basis for providing evolutionary systems. The results and comparisons indicate the significant superiority of the proposed algorithm, and achieve an accuracy of $99.67 \pm 0.25$ on the MNIST dataset.},
  archive      = {J_TETC},
  author       = {Mohamad Momtaz Sheykh Ahmad and Sajad Haghzad Klidbary},
  doi          = {10.1109/TETC.2025.3573013},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1156-1169},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hardware-software co-design low-complexity fuzzy algorithm for high-dimensional feature space with in-situ learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Q-point: A numeric format for quantum circuit simulation using polar form complex numbers. <em>TETC</em>, <em>13</em>(3), 1142-1155. (<a href='https://doi.org/10.1109/TETC.2025.3572935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum circuit simulation is playing a critical role in the current era of quantum computing. However, quantum circuit simulation suffers from huge memory requirements that scale exponentially according to the number of qubits. Our observation reveals that the conventional complex number representation using real and imaginary values adds to the memory overhead beyond the intrinsic cost of simulating quantum states. Instead, using the radius and phase value of a complex number better reflects the properties of the complex values used in the quantum circuit simulation providing better memory efficiency. This paper proposes q-Point, a compact numeric format for quantum circuit simulation that utilizes polar form representation instead of rectangular form representation to store complex numbers. The proposed q-Point format consists of three fields: i) exponent bits for radius value ii) mantissa bits for radius value iii) mantissa bits for phase value. However, a naive application of the q-Point format has the potential to cause issues with both simulation accuracy and simulation speed. To preserve simulation accuracy with fewer bits, we use a multi-level encoding scheme that employs different mantissa bits depending on the exponent range. Additionally, to prevent possible slowdown due to the add operation in polar form complex numbers, we use a technique that adaptively applies both polar and rectangular forms. Equipped with these optimizations, the proposed q-Point format demonstrates reasonable simulation accuracy while using only half of the memory requirement using the baseline format. Additionally, the q-Point format enables an average of 1.37× and 1.16× faster simulation for QAOA and VQE benchmark circuits.},
  archive      = {J_TETC},
  author       = {Seungwoo Choi and Enhyeok Jang and Youngmin Kim and Sungwoo Ahn and Won Woo Ro},
  doi          = {10.1109/TETC.2025.3572935},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1142-1155},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Q-point: A numeric format for quantum circuit simulation using polar form complex numbers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time access control for background and co-occurrence image privacy protection. <em>TETC</em>, <em>13</em>(3), 1130-1141. (<a href='https://doi.org/10.1109/TETC.2025.3572396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s digital age, the proliferation of social networks and advanced camera technology has led to countless images being shared on online social platforms daily, potentially resulting in significant breaches of personal privacy. In recent years, many methods have been proposed to protect image privacy, allowing users to be notified of potential privacy leaks before publishing their photos. However, most existing research primarily addresses the privacy protection of image owners or co-owners, while neglecting the privacy of people who appear in the background of others’ images or who are co-occurring with others in the same image. In this paper, we propose a system capable of conducting real-time access control for protecting privacy of every individual appearing in a photo, as well as the privacy of people who co-occur in the same image. Specifically, we first detect all the faces in the image, then use a facial recognition algorithm to identify the corresponding users’ privacy policies, and finally determine whether the image violates any user’s privacy policy. In order to provide real-time access control, we have designed a facial attribute index tree to speed up the process of user identification. The experimental results show that compared with the method without our proposed index tree, our approach improves the time efficiency by almost two orders of magnitude while maintaining the accuracy of more than 97%.},
  archive      = {J_TETC},
  author       = {Chaoquan Cai and Dan Lin and Kannappan Palaniappan and Chris Clifton},
  doi          = {10.1109/TETC.2025.3572396},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1130-1141},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Real-time access control for background and co-occurrence image privacy protection},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic task replication with imperfect fault detection in multicore cyber-physical systems. <em>TETC</em>, <em>13</em>(3), 1113-1129. (<a href='https://doi.org/10.1109/TETC.2025.3572277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task replication is a common technique for achieving fault tolerance. However, its effectiveness is limited by the accuracy of the fault detection mechanism; imperfect detection imposes a ceiling on achievable reliability. While perfect fault detection mechanisms offer higher reliability, they introduce significant overhead. To address this, we introduce Dynamic Task Replication, a fault tolerance technique that dynamically determines the number of replicas at runtime to overcome the limitations of imperfect fault detection. Our primary contribution, Reliability-Aware Replica-Efficient Dynamic Task Replication, optimizes this approach by minimizing the expected number of replicas while achieving the desired reliability target. We incorporate actual execution times into the reliability assessment. Additionally, we propose the Energy-Aware Reliability-Guaranteeing scheduling technique, which integrates our optimized replication method into hard real-time systems and leverages Dynamic Voltage and Frequency Scaling to minimize energy consumption while ensuring reliability and system schedulability. Experimental results demonstrate that our method requires 24% fewer replicas on average than the N-Modular Redundancy technique, with the advantage increasing to 58% for tasks with low base reliabilities. Furthermore, our scheduling technique significantly conserves energy and enhances feasibility compared to existing methods across diverse system workloads.},
  archive      = {J_TETC},
  author       = {Hossein Hosseini and Mohsen Ansari and Jörg Henkel},
  doi          = {10.1109/TETC.2025.3572277},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1113-1129},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Dynamic task replication with imperfect fault detection in multicore cyber-physical systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MALITE: Lightweight malware detection and classification for constrained devices. <em>TETC</em>, <em>13</em>(3), 1099-1112. (<a href='https://doi.org/10.1109/TETC.2025.3566370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, malware is one of the primary cyber threats to organizations, pervading all types of computing devices, including resource constrained devices such as mobile phones, tablets and embedded devices like Internet-of-Things (IoT) devices. In recent years, researchers have leveraged machine learning based strategies for malware detection and classification. However, malware analysis approaches can only be employed in resource constrained environments if the methods are lightweight in nature. In this paper, we present MALITE, a lightweight malware analysis system, that can distinguish between benign and malicious binaries and classify various malware families. MALITE converts a binary into a grayscale or an RGB image requiring low memory and battery power consumption and uses computationally inexpensive malware analysis strategies. We have designed MALITE-MN, a lightweight neural network based architecture and MALITE-HRF, an ultra lightweight random forest based method that uses histogram features extracted by a sliding window. An extensive empirical evaluation is conducted on seven publicly available datasets (Malimg, Microsoft BIG, Dumpware10, MOTIF, Drebin, CICAndMal2017 and MalNet), and performance is compared to four state-of-the-art baselines. The results show that MALITE-MN and MALITE-HRF not only accurately identify and classify malware but also respectively consume several orders of magnitude lower resources (in terms of both memory as well as computation capabilities), making them much more suitable for resource constrained environments.},
  archive      = {J_TETC},
  author       = {Sidharth Anand and Barsha Mitra and Soumyadeep Dey and Abhinav Rao and Rupsha Dhar and Jaideep Vaidya},
  doi          = {10.1109/TETC.2025.3566370},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1099-1112},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {MALITE: Lightweight malware detection and classification for constrained devices},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid quantum ResNet for time series classification. <em>TETC</em>, <em>13</em>(3), 1083-1098. (<a href='https://doi.org/10.1109/TETC.2025.3563944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residual networks (ResNet) are known to be effective for image classification. However, challenges such as computational time remain because of the significant number of parameters. Quantum computing using quantum entanglement and quantum parallelism is an emerging computing paradigm that addresses this issue. Although quantum advantage is still studied in many research fields, quantum machine learning is a research area that leverages the strengths of quantum computing and machine learning. In this study, we investigated the quantum speedup with respect to the number of parameters in each model for a time-series classification task. This paper proposes a novel hybrid quantum residual network (HQResNet) inspired by the classical ResNet for time-series classification. HQResNet introduces a classical layer before a quantum convolutional neural network (QCNN), where the QCNN is used as a residual block. These structures enable shortcut connections and are particularly effective in achieving classification tasks without a data re-uploading scheme. We used ultra-wide-band (UWB) channel impulse response data to demonstrate the performance of the proposed algorithm and compared the state-of-the-art benchmarks with HQResNet using evaluation metrics. The results show that HQResNet achieved high performance with a small number of trainable parameters.},
  archive      = {J_TETC},
  author       = {Dae-Il Noh and Seon-Geun Jeong and Won-Joo Hwang},
  doi          = {10.1109/TETC.2025.3563944},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1083-1098},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hybrid quantum ResNet for time series classification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing graph processing workloads in heterogeneous CPU-PIM systems. <em>TETC</em>, <em>13</em>(3), 1068-1082. (<a href='https://doi.org/10.1109/TETC.2025.3563249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing-in-Memory (PIM) offers a promising architecture to alleviate the memory wall challenge in graph processing applications. The key aspect of PIM is to incorporate logic within the memory, thereby leveraging the near-data advantages. State-of-the-art PIM-based graph processing accelerators tend to offload more to the memory in order to maximize near-data benefits, causing significant load imbalance in PIM systems. In this paper, we demonstrate that this intention is not true and that host processors still play a vital role in heterogeneous CPU-PIM systems. For this purpose, we propose CAPLBS, an online contention-aware Processing-in-Memory load-balance scheduler for graph processing applications in CPU-PIM systems. The core concept of CAPLBS is to steal workload candidates back to host processors with minimal off-chip data synchronization overhead when some host processors are idle. To model data contentions among workloads and determine the stealing decision, a measurement structure called Locality Cohesive Subgraph is proposed by deeply exploring the connectivity of the input graph and the memory access patterns of deployed graph applications. Experimental results show that CAPLBS achieved an average speed-up of 4.8× and 1.3× (up to 9.1× and 1.9×) compared with CPU-only and the upper bound of locality-aware fine-grained in-memory atomics. Moreover, CAPLBS adds no hardware overhead and works well with existing CPU-PIM graph processing accelerators.},
  archive      = {J_TETC},
  author       = {Sheng Xu and Chun Li and Le Luo and Ming Zheng and Liang Yan and Xingqi Zou and Xiaoming Chen},
  doi          = {10.1109/TETC.2025.3563249},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1068-1082},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Balancing graph processing workloads in heterogeneous CPU-PIM systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and implementation of cost-effective end-to-end authentication protocol for PUF-enabled IoT devices. <em>TETC</em>, <em>13</em>(3), 1055-1067. (<a href='https://doi.org/10.1109/TETC.2025.3563064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ubiquitous presence of Internet of Things (IoT) prospers in every aspect of human life. The low-powered sensors, actuators, and mobile devices in IoT transfer a high volume of security-sensitive data. Unmonitored IoT devices are highly susceptible to security vulnerabilities. Their operating environment, with minimal or no safeguards, allows physical invasion. The conventional end-to-end authentications protocols are inadequate because of the limited resources and ambient working environment of IoT. In this direction, a lightweight and secure end-to-end authentication protocol is proposed for the Physically Unclonability Function (PUF) embedded IoT devices by processing them in pairs. PUF promises to be a unique hardware-based security solution for resource-constrained devices. The proposed protocol exploits the coherent conduct of public and private key-based cryptosystems with PUF. The protocol integrates the concept of ECC with ECDH and the cryptographic hash function. Security of the proposed protocol is validated using authentication validation, BAN logic, Scyther tool, and against different adversarial attacks. The performance evaluation and extensive comparative study of the proposed protocol highlight its lightweight feature. The practical feasibility of the proposed protocol is verified by an empirical evaluation using an Arbiter PUF implemented on Xilinx Spartan-3E FPGA and Raspberry Pi as an IoT device.},
  archive      = {J_TETC},
  author       = {Sourav Roy and Mahabub Hasan Mahalat and Bibhash Sen},
  doi          = {10.1109/TETC.2025.3563064},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1055-1067},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Design and implementation of cost-effective end-to-end authentication protocol for PUF-enabled IoT devices},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLOG-CD: Curriculum learning based on oscillating granularity of class decomposed medical image classification. <em>TETC</em>, <em>13</em>(3), 1043-1054. (<a href='https://doi.org/10.1109/TETC.2025.3562620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Curriculum learning strategies have been proven to be effective in various applications and have gained significant interest in the field of machine learning. It has the ability to improve the final model’s performance and accelerate the training process. However, in the medical imaging domain, data irregularities can make the recognition task more challenging and usually result in misclassification between the different classes in the dataset. Class-decomposition approaches have shown promising results in solving such a problem by learning the boundaries within the classes of the data set. In this paper, we present a novel convolutional neural network (CNN) training method based on the curriculum learning strategy and the class decomposition approach, which we call CLOG-CD, to improve the performance of medical image classification. We evaluated our method on four different imbalanced medical image datasets, such as Chest X-ray (CXR), brain tumour, digital knee x-ray, and histopathology colorectal cancer (CRC). CLOG-CD utilises the learnt weights from the decomposition granularity of the classes, and the training is accomplished from descending to ascending order (i.e. anti-curriculum technique). We also investigated the classification performance of our proposed method based on different acceleration factors and pace function curricula. We used two pre-trained networks, ResNet-50 and DenseNet-121, as the backbone for CLOG-CD. The results with ResNet-50 show that CLOG-CD has the ability to improve classification performance with an accuracy of 96.08% for the CXR dataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee x-ray, and 99.17% for the CRC dataset, compared to other training strategies. In addition, with DenseNet-121, CLOG-CD has achieved 94.86%, 94.63%, 76.19%, and 99.45% for CXR, brain tumour, digital knee x-ray, and CRC datasets, respectively.},
  archive      = {J_TETC},
  author       = {Asmaa Abbas and Mohamed Medhat Gaber and Mohammed M. Abdelsamea},
  doi          = {10.1109/TETC.2025.3562620},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1043-1054},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CLOG-CD: Curriculum learning based on oscillating granularity of class decomposed medical image classification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximated coded computing: Towards fast, private and secure distributed machine learning. <em>TETC</em>, <em>13</em>(3), 1030-1042. (<a href='https://doi.org/10.1109/TETC.2025.3562192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a large-scale distributed machine learning system, coded computing has attracted wide-spread attention since it can effectively alleviate the impact of stragglers. However, several emerging problems greatly limit the performance of coded distributed systems. First, an existence of colluding workers who collude results with each other leads to serious privacy leakage issues. Second, there are few existing works considering security issues in data transmission of distributed computing systems/or coded distributed machine learning systems. Third, the number of required results for which need to wait increases with the degree of decoding functions. In this article, we design a secure and private approximated coded distributed computing (SPACDC) scheme that deals with the above-mentioned problems simultaneously. Our SPACDC scheme guarantees data security during the transmission process using a new encryption algorithm based on elliptic curve cryptography. Especially, the SPACDC scheme does not impose strict constraints on the minimum number of results required to be waited for. An extensive performance analysis is conducted to demonstrate the effectiveness of our SPACDC scheme. Furthermore, we present a secure and private distributed learning algorithm based on the SPACDC scheme, which can provide information-theoretic privacy protection for training data. Our experiments show that the SPACDC-based deep learning algorithm achieves a significant speedup over the baseline approaches.},
  archive      = {J_TETC},
  author       = {Houming Qiu and Kun Zhu and Nguyen Cong Luong and Dusit Niyato},
  doi          = {10.1109/TETC.2025.3562192},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1030-1042},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Approximated coded computing: Towards fast, private and secure distributed machine learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Certificateless sanitizable signature with designated verifier. <em>TETC</em>, <em>13</em>(3), 1019-1029. (<a href='https://doi.org/10.1109/TETC.2025.3562050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new type of digital signature, sanitizable signature enables a semi-trusted entity to alter a signed document and re-create a signature of the altered document in the name of original signer. This approach offers an effective solution to sanitize sensitive information in signed documents while ensuring the authenticity of sanitized documents. Most of current sanitizable signature schemes have the complex certificate management issue or the key escrow limitation. Recently, two certificateless sanitizable signature schemes have been proposed to address the above issues. However, they both rely on costly bilinear pairings, which incur high computation costs to create signature, make sanitization and perform verification. In the work, we design a pairing-free certificateless sanitizable signature scheme with a designated verifier. The proposed scheme achieves signature verification through a designated verifier, thereby preventing malicious propagation and illegal abuse of signatures. By eliminating the need for pairing operations, the scheme offers substantial improvements in computational efficiency. Security proofs demonstrate that it satisfies existential unforgeability and immutability against adaptive chosen message attacks. In addition, simulation experiments indicate that our approach reduces the computation costs of signature generation, sanitization, and verification by approximately 88.15%/88.48%, 99.98%/99.01%, and 71.22%/78.64%, respectively, when compared to the most recent two certificateless sanitizable signature schemes.},
  archive      = {J_TETC},
  author       = {Qi Sun and Yang Lu and Yinxia Sun and Jiguo Li},
  doi          = {10.1109/TETC.2025.3562050},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1019-1029},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Certificateless sanitizable signature with designated verifier},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable and RISC-V programmable near-memory computing architectures for edge nodes. <em>TETC</em>, <em>13</em>(3), 1003-1018. (<a href='https://doi.org/10.1109/TETC.2025.3555869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of data-centric algorithms, particularly artificial intelligence (AI) and machine learning (ML), has exposed the limitations of centralized processing infrastructures, driving a shift towards edge computing. This necessitates stringent constraints on energy efficiency, which traditional von Neumann architectures struggle to meet. The compute-in-memory (CIM) paradigm has emerged as a better candidate due to its efficient exploitation of the available memory bandwidth. However, existing CIM solutions require a high implementation effort and lack flexibility from a software integration standpoint. This work proposes a novel, software-friendly, general-purpose, and low-integration-effort near-memory computing (NMC) approach, paving the way for the adoption of CIM-based systems in the next generation of edge computing nodes. Two architectural variants, NM-Caesar and NM-Carus, are proposed and characterized to target different trade-offs in area efficiency, performance, and flexibility, covering a wide range of embedded microcontrollers. Post-layout simulations show up to 28.0 × and 53.9 × lower execution time and 25.0 × and 35.6 × higher energy efficiency at system level, respectively, compared to the execution of the same tasks on a state-of-the-art RISC-V CPU (RV32IMC). NM-Carus achieves a peak energy efficiency of 306.7 GOPS/W in 8-bit matrix multiplications, surpassing recent state-of-the-art in- and near-memory circuits.},
  archive      = {J_TETC},
  author       = {Michele Caon and Clément Choné and Pasquale Davide Schiavone and Alexandre Levisse and Guido Masera and Maurizio Martina and David Atienza},
  doi          = {10.1109/TETC.2025.3555869},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1003-1018},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Scalable and RISC-V programmable near-memory computing architectures for edge nodes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Virtual reinforcement learning for defect prediction in smart manufacturing. <em>TETC</em>, <em>13</em>(3), 990-1002. (<a href='https://doi.org/10.1109/TETC.2025.3546244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has focused on the integration of smart manufacturing and deep learning owing to the widespread application of neural computation. For deep learning, how to construct the architecture of a neural network is a critical issue. Especially on defect prediction or detection, a proper neural architecture could effectively extract features from the given manufacturing data to accomplish the targeted task. In this paper, we introduce a Virtual Space concept to effectively shrink the search space of potential neural network structures, with the aim of downgrading the computation complexity for learning and accuracy derivation. In addition, a novel reinforcement learning model, namely, Virtual Proximal Policy Optimization (Virtu-PPO), is developed to efficiently and effectively discover the optimal neural network structure. We also propose an optimization strategy to enhance the searching process of neural architecture for defect prediction. In addition, the proposed model is applied on several real-world manufacturing datasets to show the performance and practicability of defect prediction.},
  archive      = {J_TETC},
  author       = {Yi-Cheng Chen and Mu-Ping Chang and Wang-Chien Lee},
  doi          = {10.1109/TETC.2025.3546244},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {990-1002},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Virtual reinforcement learning for defect prediction in smart manufacturing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two low-cost and security-enhanced implementations against side-channel attacks of NTT for lattice-based cryptography. <em>TETC</em>, <em>13</em>(3), 977-989. (<a href='https://doi.org/10.1109/TETC.2025.3552941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lattice-based cryptography is considered secure against quantum computing attacks. However, naive implementations on embedded devices are vulnerable to side-channel attacks (SCAs) with full key recovery possible through power and electromagnetic leakage analysis. This article presents two protection schemes, masking and shuffling, for the baseline Radix-2 multi-path delay commutator (R2MDC) number theoretic transform (NTT) architecture. The proposed masking NTT scheme introduces a random number to protect the secret key during the decryption phase and leverages the linear property of arithmetic transform in NTT polynomial multiplication. By adjusting the comparing decoding threshold, the masking method greatly reduces the ratio of $t$-$test$ value exceeding the threshold of unprotected NTT scheme from 77.38% to 3.91%. An ingenious shuffling transform process is also proposed to disturb the calculation sequence of butterfly transformation, adapting to the high-throughput architecture of R2MDC-NTT. This shuffling NTT scheme does not require operations to remove shuffle or additional operation cycles, reducing the leakage ratio to 13.49% with minimal extra hardware resources and wide applicability. The proposed masking and shuffling techniques effectively suppress side-channel leakage, improving the security of hardware architecture while maintaining a balance between overall performance and additional hardware resources.},
  archive      = {J_TETC},
  author       = {Yijun Cui and Jiatong Tian and Chuanchao Lu and Yang Li and Ziying Ni and Chenghua Wang and Weiqiang Liu},
  doi          = {10.1109/TETC.2025.3552941},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {977-989},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Two low-cost and security-enhanced implementations against side-channel attacks of NTT for lattice-based cryptography},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel porcelain fingerprinting technique. <em>TETC</em>, <em>13</em>(3), 964-976. (<a href='https://doi.org/10.1109/TETC.2025.3546602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Porcelain, as a significant cultural heritage, embodies the wisdom of human civilization. However, existing anti-counterfeiting and authentication technologies for porcelain are often unreliable and costly. This paper proposes a physical unclonable functions (PUF) design based on crack physical feature extraction for the anti-counterfeiting and authentication of Gold-Wire porcelain. The proposed method generates PUF information by extracting inherent physical deviations in the surface cracks of Gold-Wire porcelain. First, a standard crack extraction process is established using digital image processing to obtain crack information from the porcelain surface. Then, a physical feature extraction model based on the chain code encoding technique and the Delaunay triangulation technique is used to derive the physical feature values from the cracks. Subsequently, a PUF encoding algorithm is designed to convert these physical feature values into a PUF response. Finally, the security and reliability of the designed PUF are evaluated, and a PUF-based porcelain authentication protocol is developed. Experimental results show that the proposed PUF exhibits 50.16% uniqueness and 98.85% reliability, and the PUF data successfully passed the NIST randomness test, demonstrating that the proposed technology can effectively achieve low-cost, high-reliability anti-counterfeiting for commercial porcelain.},
  archive      = {J_TETC},
  author       = {Chengjie Wang and Yuejun Zhang and Ziyu Zhou},
  doi          = {10.1109/TETC.2025.3546602},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {964-976},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A novel porcelain fingerprinting technique},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SoSTA: Skill-oriented stable task assignment with bidirectional preferences in crowdsourcing. <em>TETC</em>, <em>13</em>(3), 947-963. (<a href='https://doi.org/10.1109/TETC.2025.3548672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional task assignment approaches in crowdsourcing platforms have focused on optimizing utility for workers or tasks, often neglecting the general utility of the platform and the influence of mutual preference considering skill availability and budget restrictions. This oversight can destabilize task allocation outcomes, diminishing user experience, and, ultimately, the platform’s long-term utility and gives rise to the Worker Task Stable Matching (WTSM) problem. To solve WTSM, we propose the Skill-oriented Stable Task Assignment with a Bi-directional Preference (SoSTA) method based on deferred acceptance strategy. SoSTA aims to generate stable allocations between tasks and workers considering mutually their preferences, optimizing overall utility while following skill and budget constraints. Our study redefines the general utility of the platform as an amalgamation of utilities on both the workers’ and tasks’ sides, incorporating the preference lists of each worker or task based on their respective utility scores for the other party. SoSTA incorporates Multi Skill-oriented Stable Worker Task Mapping (Multi-SoS-WTM) algorithm for contributions with multiple skills per worker. SoSTA is rational, non-wasteful, fair, and hence stable. SoSTA outperformed other approaches in the simulations of the MeetUp dataset. SoSTA improves execution speed by 80%, task completion rate by 60%, and user happiness by 8%.},
  archive      = {J_TETC},
  author       = {Riya Samanta and Soumya K. Ghosh and Sajal K. Das},
  doi          = {10.1109/TETC.2025.3548672},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {947-963},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SoSTA: Skill-oriented stable task assignment with bidirectional preferences in crowdsourcing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLCL: Feature-level contrastive learning for few-shot image classification. <em>TETC</em>, <em>13</em>(3), 935-946. (<a href='https://doi.org/10.1109/TETC.2025.3546366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification is the task of recognizing unseen classes using a limited number of samples. In this paper, we propose a new contrastive learning method called Feature-Level Contrastive Learning (FLCL). FLCL conducts contrastive learning at the feature level and leverages the subtle relationships between positive and negative samples to achieve more effective classification. Additionally, we address the challenges of requiring a large number of negative samples and the difficulty of selecting high-quality negative samples in traditional contrastive learning methods. For feature learning, we design a Feature Enhancement Coding (FEC) module to analyze the interactions and correlations between nonlinear features, enhancing the quality of feature representations. In the metric stage, we propose a centered hypersphere projection metric to map feature vectors onto the hypersphere, improving the comparison between the support and query sets. Experimental results on four few-shot classification benchmark datasets demonstrate that our method, while simple in design, outperforms previous methods and achieves state-of-the-art performance. A detailed ablation study further confirms the effectiveness of each component of our model.},
  archive      = {J_TETC},
  author       = {Wenming Cao and Jiewen Zeng and Qifan Liu},
  doi          = {10.1109/TETC.2025.3546366},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {935-946},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {FLCL: Feature-level contrastive learning for few-shot image classification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum implementation and analysis of SHA-2 and SHA-3. <em>TETC</em>, <em>13</em>(3), 919-934. (<a href='https://doi.org/10.1109/TETC.2025.3546648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computers have the potential to solve a number of hard problems that are believed to be almost impossible to solve by classical computers. This observation has sparked a surge of research to apply quantum algorithms against the cryptographic systems to evaluate its quantum resistance. In assessing the security strength of the cryptographic algorithms against the upcoming quantum threats, it is crucial to precisely estimate the quantum resource requirement (generally in terms of circuit depth and quantum bit count). The National Institute of Standards and Technology by the US government specified five quantum security levels so that the relative quantum strength of a given cipher can be compared to the standard ones. There have been some progress in the NIST-specified quantum security levels for the odd levels (i.e., 1, 3 and 5), following the work of Jaques et al. (Eurocrypt’20). However, levels 2 and 4, which correspond to the quantum collision finding attacks for the SHA-2 and SHA-3 hash functions, quantum attack complexities are arguably not well-studied. This is where our article fits in. In this article, we present novel techniques for optimizing the quantum circuit implementations for SHA-2 and SHA-3 algorithms in all the categories specified by NIST. After that, we evaluate the quantum circuits of target cryptographic hash functions for quantum collision search. Finally, we define the quantum attack complexity for levels 2 and 4, and comment on the security strength of the extended level. We present new concepts to optimize the quantum circuits at the component level and the architecture level.},
  archive      = {J_TETC},
  author       = {Kyungbae Jang and Sejin Lim and Yujin Oh and Hyunjun Kim and Anubhab Baksi and Sumanta Chakraborty and Hwajeong Seo},
  doi          = {10.1109/TETC.2025.3546648},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {919-934},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Quantum implementation and analysis of SHA-2 and SHA-3},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing wet-neuromorphic computing using bacterial gene regulatory neural networks. <em>TETC</em>, <em>13</em>(3), 902-918. (<a href='https://doi.org/10.1109/TETC.2025.3546119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biocomputing envisions the development computing paradigms using biological systems, ranging from micron-level components to collections of cells, including organoids. This paradigm shift exploits hidden natural computing properties, to develop miniaturized wet-computing devices that can be deployed in harsh environments, and to explore designs of novel energy-efficient systems. In parallel, we witness the emergence of AI hardware, including neuromorphic processors with the aim of improving computational capacity. This study brings together the concept of biocomputing and neuromorphic systems by focusing on the bacterial gene regulatory networks and their transformation into Gene Regulatory Neural Networks (GRNNs). We explore the intrinsic properties of gene regulations, map this to a gene-perceptron function, and propose an application-specific sub-GRNN search algorithm that maps the network structure to match a computing problem. Focusing on the model organism Escherichia coli, the base-GRNN is initially extracted and validated for accuracy. Subsequently, a comprehensive feasibility analysis of the derived GRNN confirms its computational prowess in classification and regression tasks. Furthermore, we discuss the possibility of performing a well-known digit classification task as a use case. Our analysis and simulation experiments show promising results in the offloading of computation tasks to GRNN in bacterial cells, advancing wet-neuromorphic computing using natural cells.},
  archive      = {J_TETC},
  author       = {Samitha Somathilaka and Sasitharan Balasubramaniam and Daniel P. Martins},
  doi          = {10.1109/TETC.2025.3546119},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {902-918},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Analyzing wet-neuromorphic computing using bacterial gene regulatory neural networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting entity information for robust prediction over event knowledge graphs. <em>TETC</em>, <em>13</em>(3), 890-901. (<a href='https://doi.org/10.1109/TETC.2025.3534243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Script event prediction is the task of predicting the subsequent event given a sequence of events that already took place. It benefits task planning and process scheduling for event-centric systems including enterprise systems, IoT systems, etc. Sequence-based and graph-based learning models have been applied to this task. However, when learning data is limited, especially in a multiple-participant-involved enterprise environment, the performance of such models falls short of expectations as they heavily rely on large-scale training data. To take full advantage of given data, in this article we propose a new type of knowledge graph (KG) that models not just events but also entities participating in the events, and we design a collaborative event prediction model exploiting such KGs. Our model identifies semantically similar vertices as collaborators to resolve unknown events, applies gated graph neural networks to extract event-wise sequential features, and exploits a heterogeneous attention network to cope with entity-wise influence in event sequences. To verify the effectiveness of our approach, we designed multiple-choice narrative cloze tasks with inadequate knowledge. Our experimental evaluation with three datasets generated from well-known corpora shows our method can successfully defend against such incompleteness of data and outperforms the state-of-the-art approaches for event prediction.},
  archive      = {J_TETC},
  author       = {Han Yu and Hongming Cai and Shengtung Tsai and Mengyao Li and Pan Hu and Jiaoyan Chen and Bingqing Shen},
  doi          = {10.1109/TETC.2025.3534243},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {890-901},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Exploiting entity information for robust prediction over event knowledge graphs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pervasive edge computing model for proactive intelligent data migration. <em>TETC</em>, <em>13</em>(3), 878-889. (<a href='https://doi.org/10.1109/TETC.2025.3528994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, there is a great attention of the research community for the intelligent management of data in a context-aware manner at the intersection of the Internet of Things (IoT) and Edge Computing (EC). In this article, we propose a strategy to be adopted by autonomous edge nodes related to their decision on what data should be migrated to specific locations of the infrastructure and support the desired requests for processing. Our intention is to arm nodes with the ability of learning the access patterns of offloaded data-driven tasks and predict which data should be migrated to the original ‘owners’ of tasks. Naturally, these tasks are linked to the processing of data that are absent at the original hosting nodes indicating the required data assets that need to be accessed directly. To identify these data intervals, we employ an ensemble scheme that combines a statistically oriented model and a machine learning scheme. Hence, we are able not only to detect the density of the requests but also to learn and infer the ‘strong’ data assets. The proposed approach is analyzed in detail by presenting the corresponding formulations being also evaluated and compared against baselines and models found in the respective literature.},
  archive      = {J_TETC},
  author       = {Georgios Boulougaris and Kostas Kolomvatsos},
  doi          = {10.1109/TETC.2025.3528994},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {878-889},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A pervasive edge computing model for proactive intelligent data migration},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual test-time adaptation with weighted contrastive learning and pseudo-label correction. <em>TETC</em>, <em>13</em>(3), 866-877. (<a href='https://doi.org/10.1109/TETC.2025.3528985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time adaptability is often required to maintain system accuracy in scenarios involving domain shifts caused by constantly changing environments. While continual test-time adaptation has been proposed to handle such scenarios, existing methods rely on high-accuracy pseudo-labels. Moreover, contrastive learning methods for continuous test-time adaptation consider the aggregation of features from the same class while neglecting the problem of aggregating similar features within the same class. Therefore, we propose “Weighted Contrastive Learning” and apply it to both pre-training and continual test-time adaptation. To address the issue of catastrophic forgetting caused by continual adaptation, previous studies have employed source-domain knowledge to stochastically recover the target-domain model. However, significant domain shifts may cause the source-domain knowledge to behave as noise, thus impacting the model's adaptability. Therefore, we propose “Domain-aware Pseudo-label Correction” to mitigate catastrophic forgetting and error accumulation without accessing the original source-domain data while minimizing the impact on model adaptability. The thorough evaluations in our experiments have demonstrated the effectiveness of our proposed approach.},
  archive      = {J_TETC},
  author       = {Shih-Chieh Chuang and Ching-Hu Lu},
  doi          = {10.1109/TETC.2025.3528985},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {866-877},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Continual test-time adaptation with weighted contrastive learning and pseudo-label correction},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software-defined number formats for high-speed belief propagation. <em>TETC</em>, <em>13</em>(3), 853-865. (<a href='https://doi.org/10.1109/TETC.2025.3528972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the design and implementation of Software-Defined Floating-Point (SDF) number formats for high-speed implementation of the Belief Propagation (BP) algorithm. SDF formats are designed specifically to meet the numeric needs of the computation and are more compact representations of the data. They reduce memory footprint and memory bandwidth requirements without sacrificing accuracy, given that BP for loopy graphs inherently involves algorithmic errors. This article designs several SDF formats for sum-product BP applications by careful analysis of the computation. Our theoretical analysis leads to the design of 16-bit (half-precision) and 8-bit (mini-precision) widths. We moreover present highly efficient software implementation of the proposed SDF formats which is centered around conversion to hardware-supported single-precision arithmetic hardware. Our solution demonstrates negligible conversion overhead on commercially available CPUs. For Ising grids with sizes from 100 × 100 to 500 × 500, the 16- and 8-bit SDF formats along with our conversion module produce equivalent accuracy to double-precision floating-point format but with 2.86× speedups on average on an Intel Xeon processor. Particularly, increasing the grid size results in higher speed-up. For example, the proposed half-precision format with 3-bit exponent and 13-bit mantissa achieved the minimum and maximum speedups of 1.30× and 1.39× over single-precision, and 2.55× and 3.40× over double-precision, by increasing grid size from 100 × 100 to 500 × 500.},
  archive      = {J_TETC},
  author       = {Amir Sabbagh Molahosseini and JunKyu Lee and Hans Vandierendonck},
  doi          = {10.1109/TETC.2025.3528972},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {853-865},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Software-defined number formats for high-speed belief propagation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scatter-gather DMA performance analysis within an SoC-based control system for trapped-ion quantum computing. <em>TETC</em>, <em>13</em>(3), 841-852. (<a href='https://doi.org/10.1109/TETC.2025.3528899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatter-gather dynamic-memory-access (SG-DMA) is utilized in applications that require high bandwidth and low latency data transfers between memory and peripherals, where data blocks, described using buffer descriptors (BDs), are distributed throughout the memory system. The data transfer organization and requirements of a Trapped-Ion Quantum Computer (TIQC) possess characteristics similar to those targeted by SG-DMA. In particular, the ion qubits in a TIQC are manipulated by applying control sequences consisting primarily of modulated laser pulses. These optical pulses are defined by parameters that are (re)configured by the electrical control system. Variations in the operating environment and equipment make it necessary to create and run a wide range of control sequence permutations, which can be well represented as BD regions distributed across the main memory. In this article, we experimentally evaluate the latency and throughput of SG-DMA on Xilinx radiofrequency SoC (RFSoC) devices under a variety of BD and payload sizes as a means of determining the benefits and limitations of an RFSoC system architecture for TIQC applications.},
  archive      = {J_TETC},
  author       = {Tiamike Dudley and Jim Plusquellic and Eirini Eleni Tsiropoulou and Joshua Goldberg and Daniel Stick and Daniel Lobser},
  doi          = {10.1109/TETC.2025.3528899},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {841-852},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Scatter-gather DMA performance analysis within an SoC-based control system for trapped-ion quantum computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving deep neural network reliability via transient-fault-aware design and training. <em>TETC</em>, <em>13</em>(3), 829-840. (<a href='https://doi.org/10.1109/TETC.2024.3520672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have revolutionized several fields, including safety- and mission-critical applications, such as autonomous driving and space exploration. However, recent studies have highlighted that transient hardware faults can corrupt the model's output, leading to high misprediction probabilities. Since traditional reliability strategies, based on modular hardware, software replications, or matrix multiplication checksum impose a high overhead, there is a pressing need for efficient and effective hardening solutions tailored for DNNs. In this article we present several network design choices and a training procedure that increase the robustness of standard deep models and thoroughly evaluate these strategies with experimental analyses on vision classification tasks. We name DieHardNet the specialized DNN obtained by applying all our hardening techniques that combine knowledge from experimental hardware faults characterization and machine learning studies. We conduct extensive ablation studies to quantify the reliability gain of each hardening component in DieHardNet. We perform over 10,000 instruction-level fault injections to validate our approach and expose DieHardNet executed on GPUs to an accelerated neutron beam equivalent to more than 570,000 years of natural radiation. Our evaluation demonstrates that DieHardNet can reduce the critical error rate (i.e., errors that modify the inference) up to 100 times compared to the unprotected baseline model, without causing any increase in inference time.},
  archive      = {J_TETC},
  author       = {Fernando Fernandes dos Santos and Niccolò Cavagnero and Marco Ciccone and Giuseppe Averta and Angeliki Kritikakou and Olivier Sentieys and Paolo Rech and Tatiana Tommasi},
  doi          = {10.1109/TETC.2024.3520672},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {829-840},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Improving deep neural network reliability via transient-fault-aware design and training},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy efficient approximate computing framework for DNN acceleration using a probabilistic-oriented method. <em>TETC</em>, <em>13</em>(3), 816-828. (<a href='https://doi.org/10.1109/TETC.2024.3522307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing (AxC) has recently emerged as a successful approach for optimizing energy consumption in error-tolerant applications, such as deep neural networks (DNNs). The enormous model size and high computation cost of DNNs present significant challenges for deployment in energy-efficient and resource-constrained computing systems. Emerging DNN hardware accelerators based on AxC designs selectively approximate the non-critical segments of computation to address these challenges. However, a systematic and principled approach that incorporates domain knowledge and approximate hardware for optimal approximation is still lacking. In this paper, we propose a probabilistic-oriented AxC (PAxC) framework that provides high energy savings with acceptable quality by considering the overall probability effect of approximation. To achieve aggressive approximate designs, we utilize the minimum likelihood error to determine the AxC synergy profile at both application and circuit levels. This enables effective coordination of the trade-off between energy and accuracy. Compared with a baseline design, the power-delay product (PDP) is significantly reduced by up to 83.66% with an acceptable accuracy reduction. Simulation and a case study of the image process validate the effectiveness of the proposed framework.},
  archive      = {J_TETC},
  author       = {Pengfei Huang and Ke Chen and Chenghua Wang and Weiqiang Liu},
  doi          = {10.1109/TETC.2024.3522307},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {816-828},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Energy efficient approximate computing framework for DNN acceleration using a probabilistic-oriented method},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D invisible cloak: A robust person stealth attack against object detector in complex 3D physical scenarios. <em>TETC</em>, <em>13</em>(3), 799-815. (<a href='https://doi.org/10.1109/TETC.2024.3513392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel physical stealth attack against the person detectors in real world. For the first time, we consider the impacts of those complex and challenging 3D physical constraints (e.g., radian, wrinkle, occlusion, angle, etc.) on person stealth attacks, and propose 3D transformations to generate robust 3D invisible cloak. We launch the person stealth attacks in 3D physical space instead of 2D plane by printing the adversarial patches on real clothes. Anyone wearing the cloak can evade the detection of person detectors and achieve stealth under challenging and complex 3D physical scenarios. Experimental results in various indoor and outdoor physical scenarios show that, the proposed person stealth attack method is robust and effective even under those complex and challenging physical conditions, such as the cloak is wrinkled, obscured, curved, and from different/large angles. The attack success rate of the generated adversarial patch in digital domain (Inria dataset) is 86.56% against YOLO v2 and 80.32% against YOLO v5, while the static and dynamic stealth attack success rates of the generated 3D invisible cloak in physical world are 100%, 77% against YOLO v2 and 100%, 83.95% against YOLO v5, respectively, which are significantly better than state-of-the-art works.},
  archive      = {J_TETC},
  author       = {Mingfu Xue and Can He and Yushu Zhang and Zhe Liu and Weiqiang Liu},
  doi          = {10.1109/TETC.2024.3513392},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {799-815},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {3D invisible cloak: A robust person stealth attack against object detector in complex 3D physical scenarios},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiker+: A framework for the generation of efficient spiking neural networks FPGA accelerators for inference at the edge. <em>TETC</em>, <em>13</em>(3), 784-798. (<a href='https://doi.org/10.1109/TETC.2024.3511676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Including Artificial Neural Networks in embedded systems at the edge allows applications to exploit Artificial Intelligence capabilities directly within devices operating at the network periphery, containing sensitive data within the boundaries of the edge device. This facilitates real-time decision-making, reduces latency and power consumption, and enhances privacy and security. Spiking Neural Networks (SNNs) offer a promising computing paradigm in these environments. However, deploying efficient SNNs in resource-constrained edge devices requires highly parallel and reconfigurable hardware implementations. We introduce Spiker+, a comprehensive framework for generating efficient, low-power, and low-area SNN accelerators on Field Programmable Gate Arrays for inference at the edge. Spiker+ presents a configurable multi-layer SNN hardware architecture, a library of highly efficient neuron architectures, and a design framework to enable easy, Python-based customization of accelerators. Spiker+ is tested on three benchmark datasets: MNIST, Spiking Heidelberg Dataset (SHD), and AudioMNIST. On MNIST, it outperforms state-of-the-art SNN accelerators in terms of resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMS (BRAMs), and power consumption, draining only 180 mW, with comparable latency (780 $\mu$s/img) and accuracy (97%). On SHD and AudioMNIST, Spiker+ requires 18,268 and 10,124 logic cells, respectively, requiring 51 and 16 BRAMs, consuming 430 mW and 290 mW, with an accuracy of 75% and 95%. These results underscore the significance of Spiker+ in the hardware-accelerated SNN landscape, making it an excellent solution for deploying configurable and tunable SNN architectures in resource and power-constrained edge applications.},
  archive      = {J_TETC},
  author       = {Alessio Carpegna and Alessandro Savino and Stefano Di Carlo},
  doi          = {10.1109/TETC.2024.3511676},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {784-798},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Spiker+: A framework for the generation of efficient spiking neural networks FPGA accelerators for inference at the edge},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A crowdsourcing-driven AI model design framework to public health policy-adherence assessment. <em>TETC</em>, <em>13</em>(3), 768-783. (<a href='https://doi.org/10.1109/TETC.2024.3496835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on a public health policy-adherence assessment (PHPA) application that aims to automatically assess people's public health policy adherence during emergent global health crisis events (e.g., COVID-19, MonkeyPox) by leveraging massive public health policy adherence imagery data from the social media. In particular, we study an optimal AI model design problem in the PHPA application, where the goal is to leverage the crowdsourced human intelligence to accurately identify the optimal AI model design (i.e., network architecture and hyperparameter configuration combination) without the need of AI experts. However, two critical challenges exist in our problem: 1) it is challenging to effectively optimize the AI model design given the interdependence between network architecture and hyperparameter configuration; 2) it is non-trivial to leverage the human intelligence queried from ordinary crowd workers to identify the optimal AI model design in the PHPA application. To address these challenges, we develop CrowdDesign, a subjective logic-driven human-AI collaborative learning framework that explores the complementary strength of AI and human intelligence to jointly identify the optimal network architecture and hyperparameter configuration of an AI model in the PHPA application. The experimental results from two real-world PHPA applications demonstrate that CrowdDesign consistently outperforms the state-of-the-art baseline methods by achieving the best PHPA performance.},
  archive      = {J_TETC},
  author       = {Yang Zhang and Ruohan Zong and Lanyu Shang and Dong Wang},
  doi          = {10.1109/TETC.2024.3496835},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {768-783},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A crowdsourcing-driven AI model design framework to public health policy-adherence assessment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoaT: Compiler-assisted two-stage offloading approach for data-intensive applications under NMP framework. <em>TETC</em>, <em>13</em>(3), 753-767. (<a href='https://doi.org/10.1109/TETC.2024.3495218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As we head toward a data-centric era, conventional computing systems become inadequate to meet the evolving demands of the applications. As a result, the near-memory processing (NMP) computing paradigm emerges as a potential alternative framework where regions of an application are offloaded for execution near the memory. Although some interesting research works have been proposed in recent times, none of them have considered placing processing cores jointly on the primary memories and cache memory. Further, they did not consider the data locality offered by the last level cache (LLC) and the estimated execution time of an application region together while designing the offloading strategy. This paper presents a novel hybrid NMP computation framework comprising a traditional multicore processor, NMP-enabled 3D memories and NMP-enabled LLC. The application source code is processed through a compilation framework to identify potential offloadable regions. The paper further proposes a two-stage offloading strategy, CoaT, which determines the execution location of the application regions based on the region’s overall execution time and the data locality offered by the LLC. A comprehensive series of experiments conducted using well-established simulators for large data-intensive applications, provides strong evidence of the efficacy of our approach. The results demonstrate significant reductions in execution time (averaging 60% with a maximum reduction of 64%), un-core energy consumption (averaging 34% with a maximum reduction of 44%), and off-chip data block transfer count (averaging 61% with a maximum reduction of 80%) compared to the state-of-the-art policies. The proposed policy achieves a speedup of 2.6x (on average) and 3.1x (maximum) w.r.t. the conventional execution.},
  archive      = {J_TETC},
  author       = {Satanu Maity and Mayank Goel and Manojit Ghose},
  doi          = {10.1109/TETC.2024.3495218},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {753-767},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CoaT: Compiler-assisted two-stage offloading approach for data-intensive applications under NMP framework},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Darwin: A DRAM-based multi-level processing-in-memory architecture for column-oriented database. <em>TETC</em>, <em>13</em>(3), 739-752. (<a href='https://doi.org/10.1109/TETC.2024.3493132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Darwin, a practical LRDIMM-based multi-level Processing-in-memory (PIM) architecture for data analytics, which exploits the internal bandwidth of DRAM using the bank-, bank group-, chip-, and rank-level parallelisms. Considering the properties of data analytics operators and DRAM's area constraints, Darwin maximizes the internal bandwidth by placing the PIM processing units, buffers, and control circuits across the hierarchy of DRAM. Darwin supports a novel PIM instruction architecture that concatenates instructions for multiple thread executions on bank group processing entities, addressing the command bottleneck by enabling separate control of up to 512 different in-memory processing units simultaneously. We build a cycle-accurate simulation framework to evaluate Darwin with various DRAM configurations, optimization schemes and workloads. Darwin achieves up to 14.7× speedup over the non-optimized version, leveraging many optimization schemes. Darwin architecture achieves 4.0 × −43.9× higher throughput and reduces energy consumption by 85.7% than the baseline CPU system (Intel Xeon Gold 6226 + 4 channels of DDR4-2933) for essential data analytics operators. Compared to the state-of-the-art PIM, Darwin achieves up to 7.5× and 7.1× in the basic query operators and TPC-H queries, respectively. Darwin in GDDR6 configuration requires only 5.6% area overhead, suggesting a promising PIM solution for the future main memory system.},
  archive      = {J_TETC},
  author       = {Donghyuk Kim and Jae-Young Kim and Wontak Han and Jongsoon Won and Haerang Choi and Yongkee Kwon and Joo-Young Kim},
  doi          = {10.1109/TETC.2024.3493132},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {739-752},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Darwin: A DRAM-based multi-level processing-in-memory architecture for column-oriented database},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Area-time efficient hardware implementation for binary ring-LWE based post-quantum cryptography. <em>TETC</em>, <em>13</em>(3), 724-738. (<a href='https://doi.org/10.1109/TETC.2024.3482324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-quantum cryptography (PQC) has recently gained intensive attention as the existing public-key cryptosystems are vulnerable to quantum attacks. The ring-learning-with-errors (RLWE)-based PQC is one promising type of the lattice-based schemes. A light variant, called binary RLWE (BRLWE), was developed with applications to Internet-of-Things (IoT) and edge computing. However, deploying the number theoretic transform (NTT) is not beneficial to the parameter settings of the BRLWE-based scheme. This article presents three high-speed architectures of decryption for the BRLWE-based scheme with low area-time complexity. The first one is modified and corrected from the low-latency design of the previous work. The second and third ones utilize the multiplexer-based design for multiplication and innovatively exploit the property of the skew-circulant matrix to reduce the computational latency. Moreover, the third one applies the Karatsuba algorithm to reduce the number of multiplications. However, the results demonstrate that it is not in favor of the design since the multiplication is involved in an integer and a binary number, not both integers. Let the lengths of the secret and public keys be $n$ and $n\log _{2}q$ bits. The synthesized results reveal that the second and third architectures are superior to the lookup table (LUT)-based and linear-feedback shift register (LFSR)-based designs in the previous works in terms of area-time complexity. The FPGA implementation results indicate the second design outperforms the Karatsuba and Toeplitz matrix vector product (TMVP)-initiated accelerators in the literatures by reductions of 62.4% and 51.7% in area-time complexity for the case of $(n, q) = (256, 256)$. As $(n,q)=(512,256)$, the improvements are 44.3% and 28.3%. The third architecture is also superior to these high-speed designs. The proposed implementations are efficient in area-time complexity and are suitable for high-performance applications.},
  archive      = {J_TETC},
  author       = {Shao-I Chu and Syuan-An Ke},
  doi          = {10.1109/TETC.2024.3482324},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {724-738},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Area-time efficient hardware implementation for binary ring-LWE based post-quantum cryptography},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault tolerance in triplet network training: Analysis, evaluation and protection methods. <em>TETC</em>, <em>13</em>(3), 714-723. (<a href='https://doi.org/10.1109/TETC.2024.3481962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the tolerance of Triplet Networks (TNs) with a focus on faults in the training process. For compatibility with the existing literature. So-called stuck-at faults of a functional nature are considered for the operation of the neurons and activation function. While TNs are shown to be generally robust against such faults in the anchor and positive subnetworks, the presented analysis reveals a significant vulnerability in the negative subnetwork, in which stuck-at faults can lead to false convergence and training failures. An in-depth treatment is provided to show the incorrect convergence of training in the presence of stuck-at faults, highlighting the behavior of the network with faulty neurons. Extensive simulations are presented to evaluate the impact of these faults and propose two innovative fault-tolerant methods: the regularization of the anchor outputs and the modified margin. Simulation shows that false convergence can be very efficiently avoided by utilizing the proposed techniques, and thus the overall accuracy loss of the TN is negligible. These findings contribute to the understanding of fault tolerance in emerging neural networks such as TNs and offer practical solutions for enhancing their robustness against faults.},
  archive      = {J_TETC},
  author       = {Ziheng Wang and Farzad Niknia and Shanshan Liu and Pedro Reviriego and Ahmed Louri and Fabrizio Lombardi},
  doi          = {10.1109/TETC.2024.3481962},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {714-723},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fault tolerance in triplet network training: Analysis, evaluation and protection methods},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGSEI: Adaptive graph structure estimation with long-tail distributed implicit graphs. <em>TETC</em>, <em>13</em>(3), 698-713. (<a href='https://doi.org/10.1109/TETC.2024.3480132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empowered by their remarkable advantages, graph neural networks (GNN) serve as potent tools for embedding graph-structured data and finding applications across various domains. Particularly, a prevalent assumption in most GNNs is the reliability of the underlying graph structure. This assumption, often implicit, can inadvertently lead to the propagation of misleading information through structures like false links. In response to this challenge, numerous methods for graph structure learning (GSL) have been developed. Among these methods, one popular approach is to construct a simple and intuitive K-nearest neighbor (KNN) graph as a sample to infer true graph structure. However, KNN graphs that follow the single-point distribution can easily mislead the true graph structure estimation. The primary reason is that, from a statistical perspective, the KNN graph, as a sample, follows a single-point distribution, whereas the true graph structure, as the population, as a whole mostly follows a long-tail distribution. In theory, the sample and the population should share the same distribution; otherwise, accurately inferring the true graph structure becomes challenging. To address this problem, this paper proposes an Adaptive Graph Structure Estimation with Long-Tail Distributed Implicit Graph, referred to as AGSEI. AGSEI comprises three main components: long-tail implicit graph construction, explicit graph structure estimation, and joint optimization. The first component relies on a multi-layer graph convolutional network to learn low-order to high-order node representations, compute node similarity, and construct several corresponding long-tail implicit graphs. Since the original imperfect graph structure can mislead GNNs into propagating false information, it reduces the reliability of the long-tail implicit graphs. AGSEI attempts to limit the aggregation of irrelevant information by introducing the Hilbert-Schmidt independence criterion. That is, maximizing the dependence between the predicted label and ground truth. With this strategy, AGSEI can learn node features dependent on labels to facilitate the construction of reliable long-tail implicit graphs, and then provide adaptive multi-view graph structure information to support subsequent GSL. In the second component, the graph structure is estimated using the stochastic block model (SBM) with the Expectation-Maximization algorithm. Considering that it is difficult for a single GSL to approach the true graph structure, the third part considers the joint optimization of the long-tail implicit graph construction and the explicit graph structure estimation. This involves optimizing the two parts alternately until the model converges. We conducted multiple experiments on five public datasets, including tasks such as classification and clustering. These experiments not only demonstrated the performance of AGSEI but also confirmed that the graph structures it estimates align with the long-tail distribution.},
  archive      = {J_TETC},
  author       = {Yunfei He and Yang Wu and Lishan Huang and Zhenwan Peng and Fei Yang and Yiwen Zhang and Victor S Sheng},
  doi          = {10.1109/TETC.2024.3480132},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {698-713},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {AGSEI: Adaptive graph structure estimation with long-tail distributed implicit graphs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TReX- reusing vision transformer’s attention for efficient xbar-based computing. <em>TETC</em>, <em>13</em>(3), 686-697. (<a href='https://doi.org/10.1109/TETC.2024.3480524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high computation overhead of Vision Transformers (ViTs), In-memory Computing architectures are being researched towards energy-efficient deployment in edge-computing scenarios. Prior works have proposed efficient algorithm-hardware co-design and IMC-architectural improvements to improve the energy-efficiency of IMC-implemented ViTs. However, all prior works have neglected the overhead and co-depencence of attention blocks on the accuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose TReX- an attention-reuse-driven ViT optimization framework that effectively performs attention reuse in ViT models to achieve optimal accuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer encoders for attention reuse to achieve near-iso-accuracy performance while meeting the user-specified delay requirement. Based on our analysis on the Imagenet-1k dataset, we find that TReX achieves 2.3× (2.19×) EDAP reduction and 1.86× (1.79×) TOPS/mm$^{2}$ improvement with $\sim$1% accuracy drop in case of DeiT-S (LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP reduction compared to state-of-the-art token pruning and weight sharing approaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal accuracy compared to baseline at 1.6× lower EDAP.},
  archive      = {J_TETC},
  author       = {Abhishek Moitra and Abhiroop Bhattacharjee and Youngeun Kim and Priyadarshini Panda},
  doi          = {10.1109/TETC.2024.3480524},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {686-697},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {TReX- reusing vision transformer’s attention for efficient xbar-based computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully parallel, one-cycle random shuffling for efficient countermeasure against side channel attack and its complexity verification. <em>TETC</em>, <em>13</em>(3), 669-685. (<a href='https://doi.org/10.1109/TETC.2024.3478228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hiding countermeasures are the most widely utilized techniques for thwarting side-channel attacks. Commonly, the Fisher-Yates algorithm is adopted in hiding countermeasures with permuted operation for its security and efficiency in implementation, yet the inherently sequential nature of the algorithm imposes limitations on hardware acceleration. In this work, we propose a novel method named Addition Round Rotation ($\mathsf {ARR}$), which can introduce a time-area trade-off with block-based permutation. Our findings indicate that this approach can achieve a permutation brute force complexity level ranging from $2^{128}$, with the modified version achieving up to $2^{288}$ in a single clock cycle, while maintaining substantial resistance against second-order analysis. To substantiate the security of our proposed method, we introduce a new validation technique – Identity Verification. This technique allows theoretical validation of the proposed algorithm’s security and is consistent with the experimental results. Finally, we introduce an actual hardware design and provide the implementation results on Application-Specific Integrated Circuit (ASIC). The measured performance demonstrates that our proposal fully supports the practical applicability.},
  archive      = {J_TETC},
  author       = {Jong-Yeon Park and Dongsoo Lee and Seonggyeom Kim and Wonil Lee and Bo Gyeong Kang and Kouichi Sakurai},
  doi          = {10.1109/TETC.2024.3478228},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {669-685},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fully parallel, one-cycle random shuffling for efficient countermeasure against side channel attack and its complexity verification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning with curriculum design for quantum state classification. <em>TETC</em>, <em>13</em>(3), 654-668. (<a href='https://doi.org/10.1109/TETC.2024.3479202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In quantum information science, one of the ambitious goals is to look for an efficient technique for classifying multiple quantum states. To solve the binary classification problem for multiple quantum states characterized by parameters, we propose a deep reinforcement learning with curriculum design (DRL-CD) method. In DRL-CD, a series of tasks are created, using state parameter intervals and fidelity thresholds, to form a curriculum. Then, a quantum state binary classifier can be obtained by utilizing deep reinforcement learning (DRL) to solve each task in the designed curriculum. In particular, we construct a training set by sampling the state parameter interval corresponding to each task, and each task is accomplished by learning the control strategies capable of steering the sampled quantum states to the target state. In addition, a knowledge review method is proposed to prevent DRL from forgetting the learned classification knowledge. Some state classification problems of the spin-1/2 quantum system and $\Lambda$-type atomic system are solved by the proposed DRL-CD method, and comparison experiments with deep Q-network (DQN) and stochastic gradient descent (SGD) show the better classification performance of DRL-CD.},
  archive      = {J_TETC},
  author       = {Haixu Yu and Xudong Zhao},
  doi          = {10.1109/TETC.2024.3479202},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {654-668},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Deep reinforcement learning with curriculum design for quantum state classification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QuripfeNet: Quantum-resistant IPFE-based neural network. <em>TETC</em>, <em>13</em>(3), 640-653. (<a href='https://doi.org/10.1109/TETC.2024.3479193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to protect the sensitive information in many applications involving neural networks, several privacy-preserving neural networks that operate on encrypted data have been developed. Unfortunately, existing encryption-based privacy-preserving neural networks are mainly built on classical cryptography primitives, which are not secure from the threat of quantum computing. In this paper, we propose the first quantum-resistant solution to protect neural network inferences based on an inner-product functional encryption scheme. The selected state-of-the-art functional encryption scheme based on lattice-based cryptography works with integer-type inputs, which is not directly compatible with neural network computations that operate in the floating point domain. We propose a polynomial-based secure convolution layer to allow a neural network to resolve this problem, along with a technique that reduces memory consumption. The proposed solution, named QuripfeNet, was applied in LeNet-5 and evaluated using the MNIST dataset. In a single-threaded implementation (CPU), QuripfeNet took 107.4 seconds for an inference to classify one image, achieving accuracy of 97.85%, which is very close to the unencrypted version. Additionally, the GPU-optimized QuripfeNet took 25.9 seconds to complete the same task, which is improved by 4.15× compared to the CPU version.},
  archive      = {J_TETC},
  author       = {KyungHyun Han and Wai-Kong Lee and Angshuman Karmakar and Myung-Kyu Yi and Seong Oun Hwang},
  doi          = {10.1109/TETC.2024.3479193},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {640-653},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {QuripfeNet: Quantum-resistant IPFE-based neural network},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pip-SW: Pipeline architectures for accelerating smith-waterman algorithm on FPGA platforms. <em>TETC</em>, <em>13</em>(3), 628-639. (<a href='https://doi.org/10.1109/TETC.2024.3472649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Smith-Waterman algorithm, which is founded on a dynamic programming approach, serves as a precise tool for aligning biological sequences. Despite its utility, the algorithm grapples with computational complexity and resource demands. Various implementations across multi-core, GPU, and FPGA platforms have sought to expedite the algorithm, yet frequently encounter issues such as suboptimal speedup, heightened reliance on external memory resources, and an exclusive focus on the forward step of the algorithm. To tackle these challenges, this study introduces an architecture aimed at accelerating the Smith-Waterman algorithm on FPGA platforms. Our architecture capitalizes on a pipeline structure that integrates optimized circuitry for parallel computations and employs memory allocation techniques, thus delivering an efficient, low power and cost-effective implementation for biological sequence alignment. Our assessments, coupled with comparisons against alternative FPGA implementations supporting protein sequence alignment, reveal a 17% increase in operating frequency and a 17% enhancement in Giga cell updates per second. Moreover, our approach competes with GPU-based solutions, showcasing comparable performance metrics alongside superior energy efficiency, with a 35% improvement. We substantiate the utility and performance of our pipeline architecture on FPGA platforms using four benchmark datasets. The validation results demonstrate a speedup ranging from 10 to 45 times for alignment score computation compared to the CPU platform.},
  archive      = {J_TETC},
  author       = {Mahmood Kalemati and Ali Dehghan Nayeri and Somayyeh Koohi},
  doi          = {10.1109/TETC.2024.3472649},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {628-639},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Pip-SW: Pipeline architectures for accelerating smith-waterman algorithm on FPGA platforms},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). (In)security of stream ciphers against quantum annealing attacks on the example of the grain 128 and grain 128a ciphers. <em>TETC</em>, <em>13</em>(3), 614-627. (<a href='https://doi.org/10.1109/TETC.2024.3474856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security level of a cipher is a key parameter. While general-purpose quantum computers significantly threaten modern symmetric ciphers, other quantum approaches like quantum annealing have been less concerning. However, this paper argues that a quantum annealer specifically designed to attack Grain 128 and Grain 128a ciphers could soon be technologically feasible. Such an annealer would require 5,751 (6,761) qubits and 77,496 (94,865) couplers, with a qubit connectivity of 225 (245). This work also shows that modern stream ciphers like Grain 128 and Grain 128a may be vulnerable to quantum annealing attacks. Although the exact complexity of quantum annealing is unknown, heuristic estimates suggest that for many problems with $N$ variables, a $\sqrt{N}$ exponential advantage over simulated annealing may hold. We detail how to transform algebraic attacks on Grain ciphers into the QUBO problem, making our attack potentially more efficient than classical brute-force methods. We demonstrate that applying our attack to rescaled Grain cipher versions, Grain $l$ and Grain $la$, overtakes brute-force and Grover’s attacks for sufficiently large $l$, assuming quantum annealing’s exponential benefit over simulated annealing.},
  archive      = {J_TETC},
  author       = {Michał Wroński and Elżbieta Burek and Mateusz Leśniak},
  doi          = {10.1109/TETC.2024.3474856},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {614-627},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {(In)security of stream ciphers against quantum annealing attacks on the example of the grain 128 and grain 128a ciphers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A virtual reality perceptual study of multi-technique redirected walking method. <em>TETC</em>, <em>13</em>(3), 604-613. (<a href='https://doi.org/10.1109/TETC.2024.3471249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within virtual reality experiences, locomotion methods manage the user’s movement within the virtual environment. The use of natural locomotion, common in virtual reality, can be limited in video games with large scenarios. Thus, video games with gamepad or teleport-based locomotion methods are gaining importance. Redirected walking methods focus on maximizing the exploitation of the real workspace. As the user moves in the real environment, subtle modifications are applied to that movement within the virtual environment. Although the results of the Multi-Technique Redirected Walking (MTRW) method that combines the application of four gain algorithms are promising, a perceptual evaluation with users is needed to determine its suitability. This article presents the perceptual evaluation of the presence and cybersickness factors for the MTRW method, comparing it with a Fully Natural Walking (FNW) method. The presence factor was measured with the Igroup Presence Questionnaire (IPQ), and no significant differences in the overall presence score were detected between the FNW and the MTRW methods. The cybersickness factor was measured using the Simulator Sickness Questionnaire (SSQ) and, this time, significant differences in cybersickness between the two locomotion methods were obtained. The potential increase in cybersickness should be weighed against the benefit of maximizing workspace utilization.},
  archive      = {J_TETC},
  author       = {Jesus Mayor and Laura Raya and Sofia Bayona and Alberto Sanchez},
  doi          = {10.1109/TETC.2024.3471249},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {604-613},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A virtual reality perceptual study of multi-technique redirected walking method},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NGQR: A novel generalized quantum image representation. <em>TETC</em>, <em>13</em>(3), 591-603. (<a href='https://doi.org/10.1109/TETC.2024.3471086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the size limitations of existing quantum image models in terms of accurate image representation, as well as inaccurate image operation and retrieval, we propose a Novel Generalized Quantum Image Representation (NGQR) for images of arbitrary size and type. For generalizing the size model, we first propose the Perception-Aided Encoding (PE) method to perceive the target qubits in the quantum information. Based on PE, we propose the quantum image representation PE-NGQR, which accurately ignores redundant information thereby targeting valid pixels for operations and retrieval. Then, to accurately represent the needed pixel information without redundancy, we propose the Coherent-Size Encoding (CE) method. The CE can encode an arbitrary number of quantum states. Based on CE, we propose CE-NGQR, a quantum image model capable of accurate image representation, processing and retrieval. Specifically, we describe in detail the concept, representation and quantum circuits of NGQR. We provide detailed quantum circuits and simulations of NGQR-based operations and geometric transformations. Moreover, NGQR enables flexible quantum image scaling. We illustrate the complementarity of the proposed PE-NGQR and CE-NGQR through complexity simulations and clarify the respective applicability scenarios. Finally, comparisons and analyses with existing quantum image models demonstrate the versatility and flexibility advantages of NGQR.},
  archive      = {J_TETC},
  author       = {Zheng Xing and Xiaochen Yuan and Chan-Tong Lam and Penousal Machado},
  doi          = {10.1109/TETC.2024.3471086},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {591-603},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {NGQR: A novel generalized quantum image representation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DALTON - Deep local learning in SNNs via local weights and surrogate-derivative transfer. <em>TETC</em>, <em>13</em>(3), 578-590. (<a href='https://doi.org/10.1109/TETC.2024.3440932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct training of Spiking Neural Networks (SNNs) is a challenging task because of their inherent temporality. Added to it, the vanilla Back-propagation based methods are not applicable either, due to the non-differentiability of the spikes in SNNs. Surrogate-Derivative based methods with Back-propagation Through Time (BPTT) address these direct training challenges quite well; however, such methods are not neuromorphic-hardware friendly for the On-chip training of SNNs. Recently formalized Three-Factor based Rules (TFR) for direct local-training of SNNs are neuromorphic-hardware friendly; however, they do not effectively leverage the depth of the SNN architectures (we show it empirically here), thus, are limited. In this work, we present an improved version of a conventional three-factor rule, for local learning in SNNs which effectively leverages depth – in the context of learning features hierarchically. Taking inspiration from the Back-propagation algorithm, we theoretically derive our improved, local, three-factor based learning method, named DALTON (Deep LocAl Learning via local WeighTs and SurrOgate-Derivative TraNsfer), which employs weights and surrogate-derivative transfer from the local layers. Along the lines of TFR, our proposed method DALTON is also amenable to the neuromorphic-hardware implementation. Through extensive experiments on static (MNIST, FMNIST, & CIFAR10) and event-based (N-MNIST, DVS128-Gesture, & DVS-CIFAR10) datasets, we show that our proposed local-learning method DALTON makes effective use of the depth in Convolutional SNNs, compared to the vanilla TFR implementation.},
  archive      = {J_TETC},
  author       = {Ramashish Gaurav and Duy Anh Do and Thinh T. Doan and Yang Yi},
  doi          = {10.1109/TETC.2024.3440932},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {578-590},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DALTON - Deep local learning in SNNs via local weights and surrogate-derivative transfer},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DP-PartFIM: Frequent itemset mining using differential privacy and partition. <em>TETC</em>, <em>13</em>(3), 567-577. (<a href='https://doi.org/10.1109/TETC.2024.3443060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Itemset mining is a popular data mining technique for extracting interesting and valuable information from large datasets. However, since datasets contain sensitive private data, it is not permitted to directly mine the data or share the mining results. Previous privacy-preserving frequent itemset mining research was not efficient because of the use of privacy budgets or long transaction truncation strategies, which are impractical for large datasets. In this article, we propose a more efficient partition mining technology, DP-PartFIM, based on differential privacy, which protects privacy while mining data. DP-PartFIM uses partition mining to mine frequent itemsets and constructs vertical data storage formats for each partition, which makes the algorithm equally efficient for large datasets. To protect data privacy, DP-PartFIM adds Laplace noise to support candidate itemsets. The experimental results show that, compared with the classical privacy-preserving itemset mining methods, DP-PartFIM better guarantees data utility and privacy.},
  archive      = {J_TETC},
  author       = {Xinyu Liu and Wensheng Gan and Lele Yu and Yining Liu},
  doi          = {10.1109/TETC.2024.3443060},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {567-577},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DP-PartFIM: Frequent itemset mining using differential privacy and partition},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
