<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc">TAFFC - 91</h2>
<ul>
<li><details>
<summary>
(2025). Reading moods by mouse-cursor tracking: Representational similarity analysis. <em>TAFFC</em>, <em>16</em>(3), 2499-2506. (<a href='https://doi.org/10.1109/TAFFC.2025.3550304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theories of Constructed Emotion and Grounded Cognition suggest that our sensorimotor experiences underpin the formation of emotions. This study explores this premise by examining how movements of a computer cursor can reflect moods of participants. We conducted an experiment where participants engaged in a simple choice-reaching task, with their mouse-cursor movements tracked pixel by pixel. Mood assessments were conducted using the PANAS-X scale before and after the task. Through Intersubject Representational Similarity Analysis, we investigated the correlation between the patterns of mouse movements and self-reported moods. Our findings reveal a significant association between negative emotions, such as fear and hostility, and certain movement patterns, e.g., randomness and deviations from a direct path. Furthermore, our machine learning-based Representational Similarity Analysis (ML-RSA) underscores the value of second-order similarity measures, revealing meaningful alignments between sensorimotor behaviors and emotional states across distinct measurement domains. These findings highlight the potential of cursor-tracking as a tool for exploring the interplay between emotion and action.},
  archive      = {J_TAFFC},
  author       = {Takashi Yamauchi and Kunxia Wang},
  doi          = {10.1109/TAFFC.2025.3550304},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2499-2506},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Reading moods by mouse-cursor tracking: Representational similarity analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of algorithmic transparency on user experience and physiological responses in affect-aware task adaptation. <em>TAFFC</em>, <em>16</em>(3), 2491-2498. (<a href='https://doi.org/10.1109/TAFFC.2025.3530318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In affect-aware task adaptation, users’ psychological states are recognized with diverse measurements and used to adapt computer-based tasks. User experience with such adaptation improves as the accuracy of psychological state recognition and task adaptation increases. However, it is unclear how user experience is influenced by algorithmic transparency: the degree to which users understand the computer's decision-making process. We thus created an affect-aware task adaptation system with 4 algorithmic transparency levels (none/low/medium/high) and conducted a study where 93 participants first experienced adaptation with no transparency for 16 minutes, then with one of the other 3 levels for 16 minutes. User experience questionnaires and physiological measurements (respiration, skin conductance, heart rate) were analyzed with mixed 2×3 analyses of variance (time × transparency group). Self-reported interest/enjoyment and competence were lower with low transparency than with medium/high transparency, but did not differ between medium and high transparency. The transparency level may also influence participants’ respiratory responses to adaptation errors, but this finding is based on ad-hoc t-tests and should be considered preliminary. Overall, results show that the degree of algorithmic transparency does influence self-reported user experience. Since transparency information is relatively easy to provide, it may represent a worthwhile design element in affective computing.},
  archive      = {J_TAFFC},
  author       = {Mohammad Sohorab Hossain and Joshua D. Clapp and Vesna D. Novak},
  doi          = {10.1109/TAFFC.2025.3530318},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2491-2498},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Effects of algorithmic transparency on user experience and physiological responses in affect-aware task adaptation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anti-stress: A wearable device with real-time breathing feedback for stress relief. <em>TAFFC</em>, <em>16</em>(3), 2479-2490. (<a href='https://doi.org/10.1109/TAFFC.2025.3564868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relieving stress is crucial for the health of modern individuals. Deep breathing exercises have been shown to be effective for stress relief. However, the effectiveness of deep breathing exercises is limited if feedback on breathing patterns is not provided instantly. For this, we propose a wearable device called Anti-Stress, which can provide users with breathing patterns in real-time, enabling them to dynamically adjust their breathing according to Anti-Stress instructions for optimal relaxation. In addition, Anti-Stress provides users with objective stress indices by measuring the user's heart rate variability (HRV). The accuracy of Anti-Stress in detecting breathing patterns is up to 99%, and its response time is less than 200 ms, ensuring users can receive immediate guidance. To evaluate the effectiveness of Anti-Stress, we recruited 60 participants to conduct an empirical experiment. The ANCOVA analysis shows that the experimental group significantly reduces stress by using Anti-Stress compared to the control group without it. Furthermore, the usability questionnaires including SUS and QUIS demonstrate Anti-Stress's higher usability compared to a traditional non-biofeedback app. Our analysis also reveals that people with high openness or neuroticism personalities had better stress relief by using Anti-Stress. The results of this study demonstrate the feasibility and practicality of our respiratory feedback mechanism in helping users relieve stress and enhancing the effectiveness of deep breathing exercises.},
  archive      = {J_TAFFC},
  author       = {Jin-Wei Hou and Edward T.-H. Chu and Chia-Rong Lee},
  doi          = {10.1109/TAFFC.2025.3564868},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2479-2490},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Anti-stress: A wearable device with real-time breathing feedback for stress relief},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting the intrinsic neighborhood semantic structure for domain adaptation in EEG-based emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2466-2478. (<a href='https://doi.org/10.1109/TAFFC.2025.3564272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent non-stationarity and individual differences present in electroencephalogram (EEG) signals, developing a generalizable model that performs well on new subjects is challenging in EEG-based emotion recognition. Most existing domain adaptation (DA) methods typically mitigate these discrepancies by aligning the marginal distributions of domain feature representations. However, when there is a significant difference in the class-conditional distribution between domain features and labels, the domain-invariant features learned by aligning marginal distributions may have limited discriminative ability for unlabeled target instances or even prove counterproductive. To address this issue, we propose a Neighborhood Semantic Aware Learning-based Dynamic Graph Attention Convolution (NSAL-DGAT) approach that learns target semantic information by considering the inter-domain semantic topological structure, thereby improving classifier adaptation for target instances. Specifically, the proposed NSAL framework is designed to capitalize on the insight that after domain feature alignment, some target samples and their neighboring source samples exhibit similar semantics. By leveraging the neighborhood topological structure, we extract and incorporate semantic target features to train a more transferable classifier. Besides, we implement an entropy weighting mechanism to emphasize representative target semantic information, encouraging target instances to prioritize high-confidence individuals within the source neighborhood. We have conducted extensive experiments on the public SEED dataset and our collected the Hearing-Impaired EEG Dataset (HIED). The experimental results underscore the efficacy of our proposed NSAL-DGAT approach, showcasing state-of-the-art accuracy in subject-dependent as well as subject-independent scenarios.},
  archive      = {J_TAFFC},
  author       = {Yi Yang and Ze Wang and Yu Song and Ziyu Jia and Boyu Wang and Tzyy-Ping Jung and Feng Wan},
  doi          = {10.1109/TAFFC.2025.3564272},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2466-2478},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploiting the intrinsic neighborhood semantic structure for domain adaptation in EEG-based emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of multimodal self-supervised architectures for daily life affect recognition. <em>TAFFC</em>, <em>16</em>(3), 2454-2465. (<a href='https://doi.org/10.1109/TAFFC.2025.3562552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of affects (an umbrella term including but not limited to emotions, mood, and stress) in daily life is crucial for maintaining mental well-being and preventing long-term health issues. Wearable devices, such as smart bands, can collect physiological data including heart rate variability, electrodermal activity, skin temperature, and acceleration facilitating daily life affect monitoring via machine learning models. However, accurately labeling this data for model evaluation is challenging in affective computing research, as individuals often provide subjective, inaccurate, or incomplete labels in their daily lives. This study introduces the adaptation of self-supervised learning architectures for multimodal daily life stress and emotion recognition tasks, focusing on self-representation and contrastive learning methods. By leveraging unlabeled multimodal physiological signals, we aim to alleviate the need for extensive labeled data and enhance model generalizability. Our research demonstrates that self-supervised learning can effectively learn meaningful representations from physiological data without explicit labels, offering a promising approach for developing robust affect recognition systems that can operate in dynamic and uncontrolled environments. This work represents a significant improvement in recognizing affects in the wild, with potential implications for personalized mental health support and timely interventions.},
  archive      = {J_TAFFC},
  author       = {Yekta Said Can and Mohamed Benouis and Bhargavi Mahesh and Elisabeth André},
  doi          = {10.1109/TAFFC.2025.3562552},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2454-2465},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Application of multimodal self-supervised architectures for daily life affect recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-kernel embedding fusion framework for physiological signal based emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2440-2453. (<a href='https://doi.org/10.1109/TAFFC.2025.3562905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physiological signal-based emotion recognition requires effective fusion of multi-modal physiological signals to improve recognition accuracy. In this paper, a multi-kernel embedding fusion framework (MKEFF) is proposed for multi-modal physiological signal emotion recognition. Specifically, multi-kernel learning and kernel approximation techniques are used to compute the multi-kernel embeddings of the original feature vectors of each modality independently. The embeddings are then fed in parallel to their respective representation learning layer, where the proposed sparse relation learning method is applied to all the modalities to explore the correlation and diversity among them. Finally, a distribution alignment based fusion method is proposed to align each modality in the subspace, and a weighted summation fusion is performed to obtain the fused representations. Extensive cross-subject emotion recognition experiments are conducted on three public datasets, DEAP, DECAF, and SEED-IV, to evaluate the proposed method. The experimental results demonstrate that the proposed method achieves better classification performance and interpretability than the state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Xinrun He and Jian Huang and Zhongzheng Fu and Yixuan Li and Dongrui Wu},
  doi          = {10.1109/TAFFC.2025.3562905},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2440-2453},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A multi-kernel embedding fusion framework for physiological signal based emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wearable sensor-based multimodal physiological responses of socially anxious individuals in social contexts on zoom. <em>TAFFC</em>, <em>16</em>(3), 2428-2439. (<a href='https://doi.org/10.1109/TAFFC.2025.3562787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correctly identifying an individual’s social context from passively worn sensors holds promise for delivering just-in-time adaptive interventions (JITAIs) to treat social anxiety. In this study, we present results using passively collected data from a within-subjects experiment that assessed physiological responses across different social contexts (i.e., alone versus with others), social phases (i.e., pre- and post-interaction versus during an interaction), social interaction sizes (i.e., dyadic versus group interactions), and levels of social threat (i.e., implicit versus explicit social evaluation). Participants in the study ($N=46$) reported moderate to severe social anxiety symptoms as assessed by the Social Interaction Anxiety Scale ($\geq$34 out of 80). Univariate paired difference tests, multivariate random forest models, and cluster analyses were used to explore physiological response patterns across different social and non-social contexts. Our results suggest that social context is more reliably distinguishable than social phase, group size, or level of social threat, and that there is considerable variability in physiological response patterns even among distinguishable contexts. Implications for real-world context detection and future deployment of JITAIs are discussed.},
  archive      = {J_TAFFC},
  author       = {Emma R. Toner and Mark Rucker and Zhiyuan Wang and Maria A. Larrazabal and Lihua Cai and Debajyoti Datta and Haroon Lone and Mehdi Boukhechba and Bethany A. Teachman and Laura E. Barnes},
  doi          = {10.1109/TAFFC.2025.3562787},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2428-2439},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Wearable sensor-based multimodal physiological responses of socially anxious individuals in social contexts on zoom},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoAffinity: A multimodal dataset for cognitive load and affect assessment in remote collaboration. <em>TAFFC</em>, <em>16</em>(3), 2410-2427. (<a href='https://doi.org/10.1109/TAFFC.2025.3562559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the relationship between cognitive load and affective state in remote work is vital for designing intuitive collaboration. We present CoAffinity, a multimodal dataset encompassing eight structured remote-work tasks, during which 39 participants provided self-reported measures (arousal, valence, positive/negative affect, and cognitive-load) while being recorded via audio, video, and physiological signals (PPG and GSR). Spanning over 38 hours of annotated data, our approach involved precise timestamp alignment, short and long-session labelling, and subsequent machine-learning and deep-learning benchmarks. Key findings show that integrating multiple modalities, especially physiological data, significantly improves the detection of cognitive load and emotion, while group synchrony metrics highlight how physiological coherence shifts under varied task demands. By capturing complex cognitive-emotional dynamics in realistic remote settings, CoAffinity aims to advance affective computing, inform human-computer interaction research, and foster more empathetic remote collaboration tools.},
  archive      = {J_TAFFC},
  author       = {Tamil Selvan Gunasekaran and Kunal Gupta and Yun Suen Pai and Huidong Bai and Mark Billinghurst},
  doi          = {10.1109/TAFFC.2025.3562559},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2410-2427},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {CoAffinity: A multimodal dataset for cognitive load and affect assessment in remote collaboration},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stream dynamic heterogeneous graph recurrent neural network for multi-label multi-modal emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2396-2409. (<a href='https://doi.org/10.1109/TAFFC.2025.3561439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of the relationship between emotions and physiological signals of subjects under multimedia stimulation is an emerging field, and many important advances are made. However, there are still some challenges: 1) How to effectively utilize the complementarity among spatial-spectral-temporal domain information. 2) How to employ the heterogeneity and the correlation among multi-modal physiological signals simultaneously. 3) How to improve the robustness of the model dealing with missing channels. 4) How to model the dependency among different emotions. In this paper, we propose a novel two-stream Dynamic Heterogeneous Graph Recurrent Neural Network called DHGRNN. Specifically, DHGRNN consists of a spatial-temporal stream, a spatial-spectral stream, a fusion layer, and a multi-label classifier. Each stream is composed of a graph transformer network, evolved graph convolutional neural network, and gated recurrent units. We propose a graph-based two-stream structure to fuse the information of the spatial-spectral-temporal domain simultaneously. Graph transformer network and evolved graph convolutional neural network are used to model the heterogeneity and correlation of multi-modal physiological signals, respectively. To deal with the problem of robustness in the face of missing channel data, we transform it into the problem of dynamic graphs and use a dynamic graph neural network to improve the robustness. In addition, we propose a multi-label classifier to model the dependency among different emotion dimensions. Experiments on three public datasets demonstrate that our proposed model outperforms existing state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Jing Wang and Zhiyang Feng and Xiaojun Ning and Youfang Lin and Badong Chen and Ziyu Jia},
  doi          = {10.1109/TAFFC.2025.3561439},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2396-2409},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Two-stream dynamic heterogeneous graph recurrent neural network for multi-label multi-modal emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial label learning for emotion recognition from EEG. <em>TAFFC</em>, <em>16</em>(3), 2381-2395. (<a href='https://doi.org/10.1109/TAFFC.2025.3562027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully supervised learning has recently achieved promising performance in various electroencephalography (EEG) learning tasks by training on large datasets with ground truth labels. However, labeling EEG data for affective experiments is challenging, as it can be difficult for participants to accurately distinguish between similar emotions, resulting in ambiguous labeling (reporting multiple emotions for one EEG instance). This notion could cause model performance degradation, as the ground truth is hidden within multiple candidate labels. To address this issue, Partial Label Learning (PLL) has been proposed to identify the ground truth from candidate labels during the training phase, and has shown good performance in the computer vision domain. However, PLL methods have not yet been adopted for EEG representation learning or implemented for emotion recognition tasks. In this paper, we adapt and re-implement six state-of-the-art PLL approaches for emotion recognition from EEG on two large emotion datasets (SEED-IV and SEED-V). These datasets contain four and five categories of emotions, respectively. We evaluate the performance of all methods in classical, circumplex-based and real-world experiments. The results show that PLL methods can achieve strong results in affective computing from EEG and achieve comparable performance to fully supervised learning. We also investigate the effect of label disambiguation, a key step in many PLL methods. The results show that in most cases, label disambiguation would benefit the model when the candidate labels are generated based on their similarities to the ground truth rather than obeying a uniform distribution. This finding suggests the potential of using label disambiguation-based PLL methods for circumplex-based and real-world affective tasks.},
  archive      = {J_TAFFC},
  author       = {Guangyi Zhang and Ali Etemad},
  doi          = {10.1109/TAFFC.2025.3562027},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2381-2395},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Partial label learning for emotion recognition from EEG},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EmoSphere++: Emotion-controllable zero-shot text-to-speech via emotion-adaptive spherical vector. <em>TAFFC</em>, <em>16</em>(3), 2365-2380. (<a href='https://doi.org/10.1109/TAFFC.2025.3561267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional text-to-speech (TTS) has advanced significantly, but challenges persist due to the complexity of emotions and limitations in emotional speech datasets and models. A key issue with previous studies is the reliance on limited emotional speech datasets or extensive manual annotations, which restrict generalization across different speakers and emotional styles. To address this, we propose EmoSphere++, an emotion-controllable zero-shot TTS model capable of generating expressive speech with fine-grained control over emotional style and intensity—without requiring manual annotations. We introduce a novel emotion-adaptive spherical vector that effectively captures emotional style and intensity, along with a joint attribute style encoder that enhances generalization to both seen and unseen speakers. To further improve emotion transfer in zero-shot scenarios, we introduce an additional disentanglement method to enhance the style transfer performance for zero-shot scenarios. Through both objective and subjective evaluations, we demonstrate the benefits of the proposed model in emotion style and intensity modeling, as well as its effectiveness in enhancing emotional expressiveness across both seen and unseen speakers.},
  archive      = {J_TAFFC},
  author       = {Deok-Hyeon Cho and Hyung-Seok Oh and Seung-Bin Kim and Seong-Whan Lee},
  doi          = {10.1109/TAFFC.2025.3561267},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2365-2380},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {EmoSphere++: Emotion-controllable zero-shot text-to-speech via emotion-adaptive spherical vector},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual facial features transfer for facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 2352-2364. (<a href='https://doi.org/10.1109/TAFFC.2025.3561139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) models based on deep learning mostly rely on a supervised train-once-test-all approach. These approaches assume that a model trained on an in-the-wild facial expression dataset with one type of domain distribution will perform well on a test dataset with a domain distribution shift. However, facial images in real-world can be from different domain distributions from which the model has been trained. However, re-training models on only new domain distributions will severely affect the performance of the previous domain. Re-training on all previous and new data can improve overall performance but is computationally expansive. In this study, we oppose the train-once-test-all approach and propose a buffer-based continual learning approach to enhance the performance of multiple in-the-wild datasets. We propose a model that continually leverages attention to important facial features from the pre-trained model to improve performance in multiple datasets. We validated our model using split-in-the-wild datasets where the dataset is provided to the model in an incremental setting instead of all at once. Furthermore, to evaluate the model performance, we continually used three in-the-wild datasets representing different domains (Domain-FER). Extensive experiments on these datasets reveal that the proposed model achieves better results than other Continual FER models.},
  archive      = {J_TAFFC},
  author       = {Rahul Singh Maharjan and Lorenzo Bonicelli and Marta Romeo and Simone Calderara and Angelo Cangelosi and Rita Cucchiara},
  doi          = {10.1109/TAFFC.2025.3561139},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2352-2364},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Continual facial features transfer for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affect and personality aided modeling of transcribed speech for depression severity estimation. <em>TAFFC</em>, <em>16</em>(3), 2334-2351. (<a href='https://doi.org/10.1109/TAFFC.2025.3560476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of depression has gained significant attention due to its potential for early diagnosis and intervention. We propose a novel method that seamlessly integrates emotion, sentiment, and personality features as distinct yet interconnected components within a unified transformer-based architecture for depression severity estimation. Our key contribution lies in a joint cross-attention technique, which adeptly fuses the information gleaned from these different text representations, allowing for the nuanced interplay between them to be effectively captured. This technique not only enables the model to comprehend intricate interdependencies but also enhances the model’s ability to discern subtle contextual cues within the textual data. We undertake a comprehensive experimental environment to meticulously evaluate the discrete components comprising the architecture. The resultant findings gleaned from these experiments substantiate the self-contained efficacy of the envisioned architecture. Finally, we compare our method with state-of-the-art studies utilizing different combinations of audial, visual, and textual modalities. The final results demonstrate that our method achieves promising results in automatic depression severity estimation. This study underscores the potential of text-driven analysis in mental health assessment, opening avenues for more effective, accessible, and non-intrusive depression severity estimation tools.},
  archive      = {J_TAFFC},
  author       = {Kaan Gönç and Hamdi Dibeklioğlu},
  doi          = {10.1109/TAFFC.2025.3560476},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2334-2351},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Affect and personality aided modeling of transcribed speech for depression severity estimation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CorMulT: A semi-supervised modality correlation-aware multimodal transformer for sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 2321-2333. (<a href='https://doi.org/10.1109/TAFFC.2025.3559866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis is an active research area that combines multiple data modalities, e.g., text, image and audio, to analyze human emotions, and benefits a variety of applications. Existing multimodal sentiment analysis methods can be roughly classified as modality interaction-based methods, modality transformation-based methods and modality similarity-based methods. However, most of these methods highly rely on the strong correlations between modalities, and cannot fully uncover and utilize the correlations between modalities to enhance sentiment analysis. Therefore, these methods usually achieve unsatisfactory performance for identifying the sentiment of multimodal data with weak correlations. To address this issue, we proposed a two-stage semi-supervised model termed Correlation-aware Multimodal Transformer (CorMulT) which consists of pre-training stage and prediction stage. At the pre-training stage, a modality correlation contrastive learning module is designed to efficiently learn modality correlation coefficients between different modalities. At the prediction stage, the learned correlation coefficients are fused with modality representations to make the sentiment prediction. According to the experiments on the popular multimodal dataset CMU-MOSEI, CorMulT obviously surpasses the state-of-the-art multimodal sentiment analysis methods.},
  archive      = {J_TAFFC},
  author       = {Yangmin Li and Ruiqi Zhu and Wengen Li},
  doi          = {10.1109/TAFFC.2025.3559866},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2321-2333},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {CorMulT: A semi-supervised modality correlation-aware multimodal transformer for sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring emotion expression recognition in older adults interacting with a virtual coach. <em>TAFFC</em>, <em>16</em>(3), 2303-2320. (<a href='https://doi.org/10.1109/TAFFC.2025.3558141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The EMPATHIC project aimed to design an emotionally expressive virtual coach capable of engaging healthy seniors to improve well-being and promote independent aging. In particular, the system’s human sensing capabilities allow for the perception of emotional states to provide a personalized experience. This paper outlines the development of the emotion expression recognition module of the virtual coach, encompassing data collection, annotation design, and a first methodological approach, all tailored to the project requirements. With the latter, we investigate the role of various modalities, individually and combined, for discrete emotion expression recognition in this context: speech from audio, and facial expressions, gaze, and head dynamics from video. The collected corpus includes users from Spain, France, and Norway, and was annotated separately for the audio and video channels with distinct emotional labels, allowing for a performance comparison across cultures and label types. Results confirm the informative power of the modalities studied for the emotional categories considered, with multimodal methods generally outperforming others (around 68% accuracy with audio labels and 72-74% with video labels). The findings are expected to contribute to the limited literature on emotion recognition applied to older adults in conversational human-machine interaction, and guide the development of future systems.},
  archive      = {J_TAFFC},
  author       = {Cristina Palmero and Mikel deVelasco and Mohamed Amine Hmani and Aymen Mtibaa and Leila Ben Letaifa and Pau Buch-Cardona and Raquel Justo and Terry Amorese and Eduardo González-Fraile and Begoña Fernández-Ruanova and Jofre Tenorio-Laranga and Anna Torp Johansen and Micaela Rodrigues da Silva and Liva Jenny Martinussen and Maria Stylianou Korsnes and Gennaro Cordasco and Anna Esposito and Mounim A. El-Yacoubi and Dijana Petrovska-Delacrétaz and M. Inés Torres and Sergio Escalera},
  doi          = {10.1109/TAFFC.2025.3558141},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2303-2320},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploring emotion expression recognition in older adults interacting with a virtual coach},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hugging rain man: A novel facial action units dataset for analyzing atypical facial expressions in children with autism spectrum disorder. <em>TAFFC</em>, <em>16</em>(3), 2287-2302. (<a href='https://doi.org/10.1109/TAFFC.2025.3558914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children with Autism Spectrum Disorder (ASD) often exhibit atypical facial expressions. However, the specific objective facial features that underlie this subjective perception remain unclear. In this paper, we introduce a novel dataset, Hugging Rain Man (HRM), which includes facial action units (AUs) manually annotated by FACS experts for both children with ASD and typically developing (TD) children. The dataset comprises a rich collection of posed and spontaneous facial expressions, totaling approximately 130,000 frames, along with 22 AUs, 10 Action Descriptors (ADs), and atypicality ratings. A statistical analysis of static images from the HRM reveals significant differences between the ASD and TD groups across multiple AUs and ADs when displaying the same emotional expressions, confirming that participants with ASD tend to demonstrate more irregular and diverse expression patterns. Subsequently, a temporal regression method was employed to analyze atypicality of dynamic sequences, thereby bridging the gap between subjective perception and objective facial characteristics. Furthermore, baseline results for AU detection are provided for future research reference. This work not only contributes to our understanding of the unique facial expression characteristics associated with ASD but also provides potential tools for ASD early screening.},
  archive      = {J_TAFFC},
  author       = {Yanfeng Ji and Shutong Wang and Ruyi Xu and Jingying Chen and Yuxuan Quan and Xinzhou Jiang and Zhengyu Deng and Junpeng Liu},
  doi          = {10.1109/TAFFC.2025.3558914},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2287-2302},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Hugging rain man: A novel facial action units dataset for analyzing atypical facial expressions in children with autism spectrum disorder},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDL: Dynamic direction learning for semi-supervised facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 2274-2286. (<a href='https://doi.org/10.1109/TAFFC.2025.3558884'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most semi-supervised facial expression recognition (FER) algorithms leverage pseudo-labeling to mine additional information from unlabeled samples. Despite its good performance, two critical issues persist: class imbalance and domain shift. The former is a typical challenge due to the significant variation in sample numbers across different FER classes, resulting in highly imbalanced pseudo labels in existing semi-supervised methods. For the latter, given that labeled and unlabeled data usually come from different sources, a considerable domain gap might exist, leading the model to generate low-quality pseudo labels. To tackle these issues, we introduce a novel semi-supervised FER algorithm called Dynamic Direction Learning (DDL), which consists of adaptive balance learning (ABL) and adaptive alignment learning (AAL). ABL allows a balanced training process by dynamically adjusting the constraints of self-training based on the performance of a balanced validation dataset. Moreover, AAL adaptively aligns the feature distribution of labeled and unlabeled data by minimizing their distance in feature space. Additionally, a role rotation mechanism (RRM) is proposed to avoid confirmation bias, which further improves self-training. Extensive experiments demonstrate that DDL achieves state-of-the-art performance on different FER datasets.},
  archive      = {J_TAFFC},
  author       = {Yaqi Li and Jing Jiang and Yuhang Zhang and Han Fang and Jiani Hu and Weihong Deng},
  doi          = {10.1109/TAFFC.2025.3558884},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2274-2286},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {DDL: Dynamic direction learning for semi-supervised facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards speaker-unknown emotion recognition in conversation via progressive contrastive deep supervision. <em>TAFFC</em>, <em>16</em>(3), 2261-2273. (<a href='https://doi.org/10.1109/TAFFC.2025.3558222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in conversation has attained increasing attention for perceiving user emotion in practical conversational applications. Conversational utterances spoken alternately by different speakers inspire most studies to leverage speaker information based on golden speaker labels. In this work, we challenge the existing paradigm of utilizing available speaker labels with a more realistic scenario, where the speaker identity of each utterance is unknown during inference. We propose Progressive Contrastive Deep Supervision for multimodal emotion recognition in conversation (PCDS), incorporating speaker diarization and emotion recognition into one unified framework. To facilitate joint task learning, we inject speaker and emotion bias into the network progressively via contrastive deep supervision, with the task-irrelevant contrast being the intermediate transition. To obtain explicit speaker dependency, we propose a speaker contrast and clustering module (SCC) to endow the capability of partitioning speakers into groups even when neither speaker label nor number of speakers is known as a priori. Experiments on two ERC benchmarks, including IEMOCAP and MELD demonstrate the effectiveness of the proposed method. We also show that progressive contrastive deep supervision helps reconcile the underlying tension between speaker diarization and emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Siyuan Shen and Feng Liu and Hanyang Wang and Aimin Zhou},
  doi          = {10.1109/TAFFC.2025.3558222},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2261-2273},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Towards speaker-unknown emotion recognition in conversation via progressive contrastive deep supervision},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing emotion regulation in mental disorder treatment: An AIGC-based closed-loop music intervention system. <em>TAFFC</em>, <em>16</em>(3), 2245-2260. (<a href='https://doi.org/10.1109/TAFFC.2025.3557873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental disorders have increased rapidly and have emerged as a serious social health issue in the recent decade. Undoubtedly, the timely treatment of mental disorders is crucial. Emotion regulation has been proven to be an effective method for treating mental disorders. Music therapy as one of the methods that can achieve emotional regulation has gained increasing attention in the field of mental disorder treatment. However, traditional music therapy methods still face some unresolved issues, such as the lack of real-time capability and the inability to form closed-loop systems. With the advancement of artificial intelligence (AI), especially AI-generated content (AIGC), AI-based music therapy holds promise in addressing these issues. In this paper, an AIGC-based closed-loop music intervention system demonstration is proposed to regulate emotions for mental disorder treatment. This system demonstration consists of an emotion recognition model and a music generation model. The emotion recognition model can assess mental states, while the music generation model generates the corresponding emotional music for regulation. The system continuously performs recognition and regulation, thus forming a closed-loop process. In the experiment, we first conduct experiments on both the emotion recognition model and the music generation model to validate the accuracy of the recognition model and the music quality generated by the music generation models. In conclusion, we conducted comprehensive tests on the entire system to verify its feasibility and effectiveness.},
  archive      = {J_TAFFC},
  author       = {Lin Shen and Haojie Zhang and Cuiping Zhu and Ruobing Li and Kun Qian and Fuze Tian and Bin Hu and Björn W. Schuller and Yoshiharu Yamamoto},
  doi          = {10.1109/TAFFC.2025.3557873},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2245-2260},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Enhancing emotion regulation in mental disorder treatment: An AIGC-based closed-loop music intervention system},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). THERADIA WoZ: An ecological corpus for appraisal-based affect research in healthcare. <em>TAFFC</em>, <em>16</em>(3), 2233-2244. (<a href='https://doi.org/10.1109/TAFFC.2025.3557465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present THERADIA WoZ, an ecological corpus designed for audiovisual research on affect in healthcare. Two groups of senior individuals, consisting of 52 healthy participants and 9 individuals with Mild Cognitive Impairment (MCI), performed Computerised Cognitive Training (CCT) exercises while receiving support from a virtual assistant, tele-operated by a human in the role of a Wizard-of-Oz (WoZ). The audiovisual expressions produced by the participants were fully transcribed, and partially annotated based on dimensions derived from recent appraisal theory models, including novelty, intrinsic pleasantness, goal conduciveness, and coping. Additionally, the annotations included 23 affective labels from the literature of achievement affects. We present the data collection, transcription, and annotation protocols, alongside a detailed analysis of the annotated dimensions and labels. Baseline methods and results for their automatic prediction are also presented. Results reveal that the dimensions of appraisal theory can be predicted, with the performance varying across different modalities. The corpus aims to serve as a valuable resource for researchers in affective computing, and is made available to both industry and academia.},
  archive      = {J_TAFFC},
  author       = {Hippolyte Fournier and Sina Alisamir and Safaa Azzakhnini and Isabella Zsoldos and Eléonore Trân and Gérard Bailly and Frédéric Elisei and Béatrice Bouchot and Brice Varini and Patrick Constant and Joan Fruitet and Franck Tarpin-Bernard and Solange Rossato and François Portet and Olivier Koenig and Hanna Chainay and Fabien Ringeval},
  doi          = {10.1109/TAFFC.2025.3557465},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2233-2244},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {THERADIA WoZ: An ecological corpus for appraisal-based affect research in healthcare},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study of super-resolution on low-resolution micro-expression recognition. <em>TAFFC</em>, <em>16</em>(3), 2215-2232. (<a href='https://doi.org/10.1109/TAFFC.2025.3556442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER) in low-resolution (LR) scenarios presents an important and complex challenge, particularly for practical applications such as group MER in crowded environments. Despite considerable advancements in super-resolution (SR) techniques for enhancing the quality of LR images and videos, few study has focused on investigate SR for improving LR MER. The scarcity of investigation can be attributed to the inherent difficulty in capturing the subtle motions of micro-expressions, even in original-resolution MER samples, which becomes even more challenging in LR samples due to the loss of distinctive features. Furthermore, a lack of systematic benchmarking and thorough analysis of SR-assisted MER methods has been noted. This paper tackles these issues by conducting a series of benchmark experiments that integrate both SR and MER methods, guided by an in-depth literature survey. Specifically, we employ seven cutting-edge state-of-the-art (SOTA) MER techniques and evaluate their performance on samples generated from 22 SOTA SR techniques, thereby addressing the problem of SR in MER. Through our empirical study, we uncover the primary challenges associated with SR-assisted MER and identify avenues to tackle these challenges by leveraging recent advancements in both SR and MER methodologies. Our analysis provides insights for progressing toward more efficient SR-assisted MER.},
  archive      = {J_TAFFC},
  author       = {Ling Zhou and Mingpei Wang and Xiaohua Huang and Wenming Zheng and Qirong Mao and Guoying Zhao},
  doi          = {10.1109/TAFFC.2025.3556442},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2215-2232},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {An empirical study of super-resolution on low-resolution micro-expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view facial expressions analysis of autistic children in social play. <em>TAFFC</em>, <em>16</em>(3), 2200-2214. (<a href='https://doi.org/10.1109/TAFFC.2025.3557458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atypical facial expressions during interaction are among the early symptoms of autism spectrum disorder (ASD) and are included in standard diagnostic assessments. However, current methods rely on subjective human judgments, introducing bias and limiting objectivity. This paper proposes an automated framework for objective and quantitative assessment of autistic children’s facial expressions during social play. Initially, we utilize four synchronized cameras to record interactions between ASD children and teachers during structured activities dominated by the teacher. To address challenges posed by head movements and occluded faces, we introduce a multi-view facial expression recognition strategy. Its effectiveness is demonstrated by experiments in real-world applications. To quantify the patterns of affect status and the dynamic complexity of facial expressions, we use the temporally accumulated distribution of the basic facial expressions and the multi-dimensional multi-scale entropy of the facial expression sequence. Analysis of these features revealed significant differences between ASD and TD groups. Experimental results, derived from our quantified features, confirm conclusions drawn from previous research and experiential observations. With these facial expression features, ASD and typically developing (TD) children are accurately classified (accuracy 92.1%, precision 94.4%, 89.5% sensitivity, 94.7% specificity) in empirical experiments, suggesting the potential of our framework for improved ASD assessment.},
  archive      = {J_TAFFC},
  author       = {Jiabei Zeng and Yujian Yuan and Lu Qu and Fei Chang and Xuran Sun and Jinqiuyu Gong and Xuling Han and Min Liu and Hang Zhao and Qiaoyun Liu and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TAFFC.2025.3557458},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2200-2214},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multi-view facial expressions analysis of autistic children in social play},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCRVT: Multi-hierarchical cross-reconstruction networks with versatile transformer for speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2189-2199. (<a href='https://doi.org/10.1109/TAFFC.2025.3557153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two challenges for Speech Emotion Recognition (SER) are efficiently capturing features to explore speech-emotion correlations and reducing the SER model redundancy while increasing performance. To address these challenges, we propose Multi-Hierarchical Cross-Reconstruction Networks with Versatile Transformer (MCRVT), including a Spectrogram-Based SER Model with Versatile Attention (SSER-VA), a pre-trained model (WavLM), and Multi-Hierarchical Feature Fusion with Cross Attention (MHFF-CA). Specifically, SSER-VA takes the log-Mel spectrogram as the input, and we propose a versatile attention module to reduce redundancy and increase efficiency. MHFF-CA is employed to refine intermediate features from WavLM and SSER-VA to improve competitiveness in various tasks, where Contrastive Reconstruction Networks (CRN) and Cross Attention-based Feature Fusion (CAF) are utilized to reconstruct and fuse features, respectively. Effectiveness experiments on IEMOCAP and MELD datasets illustrate MCRVT's superior performance compared to SOTA methods. Exploratory experiments on D-vlog, BioCAS2024, and IEMOCAP (speaker-independent setting) datasets demonstrate the competitiveness of our method in various speech classification tasks.},
  archive      = {J_TAFFC},
  author       = {Xin-Heng Li and Zhen-Tao Liu and Yu-Jie Zou and Jinhua She and Kaoru Hirota},
  doi          = {10.1109/TAFFC.2025.3557153},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2189-2199},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MCRVT: Multi-hierarchical cross-reconstruction networks with versatile transformer for speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion recognition by learning the manifold of fused multiscale information of EEG signals. <em>TAFFC</em>, <em>16</em>(3), 2172-2188. (<a href='https://doi.org/10.1109/TAFFC.2025.3555226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has consistently indicated that the fusion of electroencephalography (EEG) features from multiple modalities can integrate cognitive state expressions across diverse dimensions, resulting in a substantial increase in emotion recognition accuracy. However, redundant information within the fused multimodal features could lead to the curse of dimensionality and overfitting of the learning model. In this work, we propose a multiscale EEG feature fusion and representation strategy for EEG emotion recognition named manifold of multiscale information fusion (MMIF), in which the optimal manifold of the multiscale fusion of local and global brain activation patterns can be automatically learned to realize an efficient representation of emotional EEG signals. To evaluate the performance, in this work, both off- and online EEG emotion recognition experiments were conducted, and the experimental results consistently verified the effectiveness and feasibility of the MMIF applied in real-time emotion decoding systems. Furthermore, the analytical experiments confirmed the discriminative capabilities and cognitive interpretability of the MMIF. In summary, the proposed MMIF model may provide an efficient avenue for exploring representations and enhancing the discrimination of multimodal fusion features, which may also provide a promising solution for designing online affective braincomputer interaction systems.},
  archive      = {J_TAFFC},
  author       = {Cunbo Li and Shuhan Zhang and Yufeng Mu and Lei Yang and Yueheng Peng and Fali Li and Yangsong Zhang and Zhen Liang and Zehong Cao and Feng Wan and Dezhong Yao and Peiyang Li and Peng Xu},
  doi          = {10.1109/TAFFC.2025.3555226},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2172-2188},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Emotion recognition by learning the manifold of fused multiscale information of EEG signals},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing cross-dataset EEG emotion recognition: A novel approach with emotional EEG style transfer network. <em>TAFFC</em>, <em>16</em>(3), 2157-2171. (<a href='https://doi.org/10.1109/TAFFC.2025.3555439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG)-based emotion recognition has achieved remarkable success in both subject-dependent and subject-independent scenarios. However, overcoming the challenges associated with reduced performance in EEG emotion recognition across devices, time, space, and subjects (i.e., cross-dataset) remains a significant obstacle for affective brain-computer interfaces (aBCIs). The key issue lies in the distributional mismatch between source and target domain EEG signals. To tackle the significant inter-domain differences in cross-dataset EEG emotion recognition, this paper introduces an innovative framework termed the Emotional EEG Style Transfer Network (E$^{2}$STN), which aims to effectively capture the emotional content information from the source domain and the style features from the target domain, facilitating the reconstruction of stylized emotion EEG representations. These stylized EEG representations significantly enhance the discriminative prediction performance in cross-dataset EEG emotion recognition. Specifically, E$^{2}$STN consists of three key modules: a Transfer Module for domain style transfer, a Transfer Evaluation Module for evaluating transfer quality, and a Discriminative Module for making discriminative predictions. Extensive experiments demonstrate that E$^{2}$STN achieves the state-of-the-art performance in cross-dataset emotion EEG recognition. To the best of our knowledge, this is the first work to explicitly address cross-dataset emotion EEG recognition. The experimental results provide a valuable benchmark for future research in this area.},
  archive      = {J_TAFFC},
  author       = {Yijin Zhou and Fu Li and Yang Li and Youshuo Ji and Lijian Zhang and Yuanfang Chen and Huaning Wang},
  doi          = {10.1109/TAFFC.2025.3555439},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2157-2171},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Enhancing cross-dataset EEG emotion recognition: A novel approach with emotional EEG style transfer network},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SeeNet: A soft emotion expert and data augmentation method to enhance speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2142-2156. (<a href='https://doi.org/10.1109/TAFFC.2025.3555406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition (SER) systems are designed to enable machines to recognize emotional states in human speech during human-computer interactions, enhancing the interactive experience. While considerable progress has been achieved in this field recently, SER systems still encounter challenges related to performance and robustness, primarily stemming from the limited labeled data. To this end, we propose a novel multitask learning framework to learn a distinctive and robust emotional representation by our “Soft Emotion Expert Network (SeeNet)”. SeeNet consists of three components: a pretrained model, an auxiliary task soft emotion expert (SEE) module and an energy-based mixup (EBM) data augmentation module. The pretrained model and EBM module are employed to mitigate the challenges arising from limited labeled data, thereby enhancing the model performance and bolstering robustness. The SEE module as an auxiliary task is designed to assist the main task of SER by enhancing the distinction between samples exhibiting high similarity across categories. This aims to further improve the performance and robustness of the system. Comprehensive experiments on three different settings and multiple datasets are conducted to evaluate the performance and robustness of our proposed method. The experimental results demonstrate that SeeNet surpasses the state-of-the-art (SOTA) methods in both performance and robustness.},
  archive      = {J_TAFFC},
  author       = {Qifei Li and Yingming Gao and Yuhua Wen and Ziping Zhao and Ya Li and Björn W. Schuller},
  doi          = {10.1109/TAFFC.2025.3555406},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2142-2156},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SeeNet: A soft emotion expert and data augmentation method to enhance speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UA-DAAN: An uncertainty-aware dynamic adversarial adaptation network for EEG-based depression recognition. <em>TAFFC</em>, <em>16</em>(3), 2130-2141. (<a href='https://doi.org/10.1109/TAFFC.2025.3555433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a common mental disorder characterized by symptoms such as a depressed mood, loss of interest, low self-esteem, and anxiety. Clinical diagnosis of depression often faces challenges due to the lack of objective indicators and the subjectivity of psychiatrists and patients. In recent years, with the rapid advancement of artificial intelligence technology, automatic depression diagnosis methods based on physiological signals have emerged. These methods have helped enhance the accuracy and objectivity of diagnosis. One such physiological signal used is the electroencephalogram (EEG), which is an easily obtainable, noninvasive, and cost-effective electrical signal recording the activity of neurons in the cerebral cortex. EEG is commonly used to observe brain states and diagnose mental illnesses. However, due to the high individual variability of EEG signals, existing methods often do not adequately address the issue of removing individual variability. Additionally, achieving high model reliability in disease recognition is crucial, but existing methods typically lack uncertainty estimation of recognition results. To tackle these challenges, this study introduces an uncertainty-aware dynamic adversarial adaptation network (UA-DAAN). This network incorporates adversarial learning concepts to address the significant individual variability in EEG data. It utilizes uncertainty-aware optimization of the dynamic domain adversarial process in a Bayesian neural network (BNN) to enhance the transferability of class-related features between source and target domains, improving the overall model's robustness, accuracy, and reliability. The experimental results strongly prove the effectiveness of this model in depression recognition.},
  archive      = {J_TAFFC},
  author       = {Jian Shen and Lechun You and Yu Ma and Zeguang Zhao and Huajian Liang and Yanan Zhang and Bin Hu},
  doi          = {10.1109/TAFFC.2025.3555433},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2130-2141},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {UA-DAAN: An uncertainty-aware dynamic adversarial adaptation network for EEG-based depression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An EEG method to identify image preference with an Explicit/Implicit task brain-computer interface. <em>TAFFC</em>, <em>16</em>(3), 2116-2129. (<a href='https://doi.org/10.1109/TAFFC.2025.3554534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately determining an individual's preference for images remains a major challenge in the field of emotional research. This study proposes a novel paradigm for identifying individual image preferences using electroencephalography (EEG) signals and brain-computer interface (BCI). The paradigm involves both explicit and implicit tasks, where participants perform a typical event-related potential-based brain-computer interface(ERP-BCI) operation and their subjective image preferences are identified, respectively. Two experiments with a total of 27 participants demonstrate that event-related potential (ERP) signals during explicit BCI tasks are significantly influenced by target image preferences, enabling high-accuracy image preference recognition. Online experiments selecting positive and negative preference images from a candidate pool show top-1 accuracy approaching 100% and top-3 accuracy exceeding 90%. These results indicate the effectiveness of the proposed EEG-based image preference recognition paradigm, laying the groundwork for preference analysis applications.},
  archive      = {J_TAFFC},
  author       = {Yulei Li and Shuyi Li and Hongzhi Qi},
  doi          = {10.1109/TAFFC.2025.3554534},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2116-2129},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {An EEG method to identify image preference with an Explicit/Implicit task brain-computer interface},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal cross-subject emotion feature alignment and recognition with EEG and eye movements. <em>TAFFC</em>, <em>16</em>(3), 2102-2115. (<a href='https://doi.org/10.1109/TAFFC.2025.3554399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal emotion recognition has attracted much attention in human-computer interaction, because it provides complementary information for the recognition model. However, the distribution drift among subjects and the heterogeneity of different modalities pose challenges to multi-modal emotion recognition, thereby limiting its practical application. Most of the current multi-modal emotion recognition methods are difficult to suppress above uncertainties in fusion. In this paper, we propose a cross-subject multi-modal emotion recognition framework, which jointly learns subject-independent representation and common feature between EEG and eye movements. First, we design the dynamic adversarial domain adaptation for cross-subject distribution alignment, dynamically selecting source domains in training. Second, we simultaneously capture intra-modal and inter-modal emotion-related features by both self-attention and cross-attention mechanisms, thus obtaining the robust and complementary representation of emotional information. Then, two contrastive loss functions are imposed on above network to further reduce inter-modal heterogeneity, and mine higher-order semantic similarity between synchronously collected multi-modal data. Finally, we used the output of the softmax layer as the predicted value. The experimental results on several multi-modal emotion datasets with EEG and eye movements demonstrate that our method is significantly superior to the state-of-the-art emotion recognition approaches.},
  archive      = {J_TAFFC},
  author       = {Qi Zhu and Ting Zhu and Lunke Fei and Chuhang Zheng and Wei Shao and David Zhang and Daoqiang Zhang},
  doi          = {10.1109/TAFFC.2025.3554399},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2102-2115},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multi-modal cross-subject emotion feature alignment and recognition with EEG and eye movements},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From translation to generative LLMs: Classification of code-mixed affective tasks. <em>TAFFC</em>, <em>16</em>(3), 2090-2101. (<a href='https://doi.org/10.1109/TAFFC.2025.3553399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code-mixed (CM) discourse combines multiple languages in a single text. It is commonly used in informal discourse in countries with several official languages, but also in many other countries in combination with English or neighboring languages. With the recent rise of large transformer language models dominating NLP tasks, we explored their effectiveness in CM contexts. We developed four new bilingual pre-trained masked language models for Hinglish and English-Slovene languages, tailored to handle informal language. We then evaluated monolingual, bilingual, few-lingual, massively multilingual, and larger generative models across multiple languages using two affective tasks involving CM texts: sentiment analysis and offensive speech prediction in social media posts. We compared these models with two translation baselines, one obtained with a neural machine translation tool and the other produced by large generative models. The experiments conducted in five languages: French, Hindi, Russian, Slovene, and Tamil, reveal that fine-tuned bilingual models and multilingual models designed for social media texts outperform others, with massively multilingual and monolingual models following, while larger generative models lag. For the affective tasks studied, models generally performed better on CM data than on non-CM data. The monolingual models with translated datasets rarely compete with multilingual models trained on CM datasets.},
  archive      = {J_TAFFC},
  author       = {Anjali Yadav and Tanya Garg and Matej Klemen and Matej Ulčar and Basant Agarwal and M. Robnik-Šikonja},
  doi          = {10.1109/TAFFC.2025.3553399},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2090-2101},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {From translation to generative LLMs: Classification of code-mixed affective tasks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Injecting multimodal information into pre-trained language model for multimodal sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 2074-2089. (<a href='https://doi.org/10.1109/TAFFC.2025.3553149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing availability of computational and data resources, numerous powerful pre-trained language models (PLMs) have emerged for natural language processing tasks. However, how to inject nonverbal modalities into PLMs to handle multimodal information remains a practical problem. In this paper, we explore the application of PLM on multimodal sentiment analysis from a different perspective. Unlike many recent methods that develop multimodal fusion layers that are sequential to attention layers, we investigate the effectiveness of cross-modal additive attention that is parallel to attention layers, which takes the language modality as dominant modality. Moreover, we devise a gating mechanism to control the flow of nonverbal information by estimating its discriminative level. In this way, we can prevent noisy multimodal information from damaging the performance of pre-trained language model. In our framework, nonverbal modalities serve as auxiliary roles to provide the model with additional information and improve the understanding of multimodal human language. Additionally, cross-modal margin and matching losses are proposed to align the distributions of various modalities and simultaneously retain modality-specific information, which to some extent address the shortcoming of contrastive learning loss. Comprehensive experiments show that our approach surpasses existing state-of-the-art methods on multimodal sentiment analysis and emotion recognition tasks.},
  archive      = {J_TAFFC},
  author       = {Sijie Mai and Ying Zeng and Aolin Xiong and Haifeng Hu},
  doi          = {10.1109/TAFFC.2025.3553149},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2074-2089},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Injecting multimodal information into pre-trained language model for multimodal sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MemRank: Memory-augmented similarity ranking for video-based depression severity estimation. <em>TAFFC</em>, <em>16</em>(3), 2062-2073. (<a href='https://doi.org/10.1109/TAFFC.2025.3553090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have shown substantial promise in visual depression severity estimation. Nonetheless, their effectiveness is limited by the scarce availability of labeled depression data, potentially leading to overfitting during representation learning. One feasible approach to address this issue is to incorporate, in the training objective, regularization that considers the unique characteristics of depression data. Typical regularization includes the similarity ranking through ordered consistency between visual features and their target scores. However, previous ranking methods are limited to using only samples within a mini-batch, resulting in a decreased regularization effect in depression representation learning. To address this limitation, we propose MemRank, a global similarity ranking method that operates not only on mini-batch samples but also on a well-designed feature memory, which stores smoothed and dynamically updated feature prototypes at diverse levels of depression during training. Furthermore, we show that incorporating the feature memory in the regression loss enhances the stability of training a deep regressor, leading to improved depression predictions. Empirically and analytically, we show that our MemRank outperforms alternative ranking methods and achieves state-of-the-art results on two benchmark datasets.},
  archive      = {J_TAFFC},
  author       = {Yonghong Li and Zeqiang Wei and Guodong Guo and Xiuzhuang Zhou},
  doi          = {10.1109/TAFFC.2025.3553090},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2062-2073},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MemRank: Memory-augmented similarity ranking for video-based depression severity estimation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal depression assessment framework integrating personality and gait for older adults with medical conditions. <em>TAFFC</em>, <em>16</em>(3), 2048-2061. (<a href='https://doi.org/10.1109/TAFFC.2025.3552835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elderly individuals often suffer from underlying medical conditions, resulting in a significant decline in quality of life and a heightened susceptibility to depression. Presently, AI screening tools based on behavioral indicators offer an objective and effective approach to diagnosing depression. However, current AI depression screening tools are primarily tailored to adolescents and adults, exhibiting shortcomings in their applicability and accuracy for elderly individuals with underlying medical conditions. To address the above issues, first, this paper constructs a depression dataset for elderly people with underlying diseases by using semi-structured interviews. Second, based on cognitive science insights, it is recognized that personality factors significantly influence behavioral expressions and also determine the attitudes of elderly individuals toward current life circumstances/health issues. Therefore, besides annotating depression severity, the Big Five-10 personality scale was utilized to annotate participant personalities. Finally, a late fusion-based multi-task learning framework was proposed, and the effects of introducing gait information and personality annotation on the performance of depression assessment were investigated. The experimental findings affirm the importance of integrating gait information and personality assessment in improving depression detection effectiveness. This study provides valuable foundational resources, as well as beneficial references and insights, for the research on depression in the elderly.},
  archive      = {J_TAFFC},
  author       = {Yuliang Zhao and Huawei Zhang and Jian Li and Siyang Song and Chao Lian and Yinghao Liu and Yulin Wang and Changzeng Fu},
  doi          = {10.1109/TAFFC.2025.3552835},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2048-2061},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multimodal depression assessment framework integrating personality and gait for older adults with medical conditions},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying stable EEG patterns in manipulation task for negative emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2033-2047. (<a href='https://doi.org/10.1109/TAFFC.2025.3551330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative emotion recognition during manipulation task plays crucial role in human-machine interaction, where diverse cognitive variables coexist and influence each other. However, traditional emotion experiments often overemphasize emotion induction while overlooking other practical cognitive tasks, which leads participants to suffer from simplistic emotional experiences and ultimately compromises the real-world applicability of the emotional data collected. To incorporate critical cognitive variables into emotion elicitation, we utilize joystick-based real-time emotion annotation to encourage subjects to continuously feel emotional intensity, to advisedly decide when to manipulate the joystick, and to physically operate it. Consequently, at least two essential cognitive variables—decision-making and action—are integrated into emotion perception. Following this, we develop a novel negative emotion dataset called CRED, which includes a variety of physiological data, particularly Electroencephalograph (EEG). To assess the stability of emotional EEG patterns, we employ strict statistical analysis and a dual-branch transformer (DBT) with the gradient-based attribution method on the proposed CRED. Additionally, two well-known public datasets (SEED and SEED-V) are used to verify the DBT. Compared to traditional methods, DBT improves classification accuracy by approximately 5% on CRED and by around 2% on the public datasets. The experimental results indicate that the occipital lobe plays a crucial role in the discrimination of negative emotions; the critical frequency bands vary between the five emotions in the CRED. Specifically, the low-delta rhythm is associated with anger, while fear is influenced by both theta and alpha rhythms; disgust is found to be significant in the theta rhythm; and for neutral emotions, both low-delta and alpha rhythms are identified as crucial. In summary, our findings demonstrate the existence of stable emotional EEG patterns when additional cognitive variables are involved.},
  archive      = {J_TAFFC},
  author       = {Yu Pei and Shaokai Zhao and Liang Xie and Zhiguo Luo and Dongdong Zhou and Chuang Ma and Ye Yan and Erwei Yin},
  doi          = {10.1109/TAFFC.2025.3551330},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2033-2047},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Identifying stable EEG patterns in manipulation task for negative emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PeTracker: Poincaré-based dual-strategy emotion tracker for emotion recognition in conversation. <em>TAFFC</em>, <em>16</em>(3), 2020-2032. (<a href='https://doi.org/10.1109/TAFFC.2025.3549926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing use of interactive applications, the importance of Emotion recognition in conversation (ERC) is growing. Current research in the ERC domain mainly emphasizes the extraction of contextual information. However, challenges arise due to multi-turn conversation scenarios and the natural transformation of emotions, particularly in identifying subtle emotion transfers. Moreover, emotions exhibit nonlinear characteristics in semantic spaces, leading to potential confusion when discerning similar semantic emotions in the Euclidean semantic space. To address these issues, this study proposes a Poincaré-based dual-strategy emotion tracker for emotion recognition in conversation (PeTracker), which introduces the hyperbolic space representation in the ERC domain. Based on the spatial properties of the hyperbolic space representation to capture the nonlinear relationships among features, PeTracker encompasses two learning strategies. Poincaré emotional geometry curriculum learning (PGCL) and Poincaré emotional stratification contrastive learning (PSCL). In PGCL, the similarity of emotion labels is effectively discerned using the Poincaré distance, quantifying emotion transfer distances and facilitating the identification of subtle emotion transfers in utterance. In PSCL, PeTracker extracts and adapts multi-level features, mapping them to the Poincaré ball space to build emotion prototype-based contrastive learning. This process enhances the model’s ability to distinguish between similar emotion labels. while alleviating potential label confusion issues. Experimental results on three general datasets demonstrate that PeTracker achieves optimal or near-optimal performance. Furthermore, the study investigates the role and impact of the Poincaré ball in differentiating similar emotions.},
  archive      = {J_TAFFC},
  author       = {YuKun Cao and Luobin Huang and Yijia Tang},
  doi          = {10.1109/TAFFC.2025.3549926},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2020-2032},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {PeTracker: Poincaré-based dual-strategy emotion tracker for emotion recognition in conversation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReSup: Reliable label noise suppression for facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 2006-2019. (<a href='https://doi.org/10.1109/TAFFC.2025.3549017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the ambiguous and subjective property of the facial expression, the label noise is widely existing in the FER dataset. For this problem, in the training phase, current methods often directly predict whether the label is noised or not, aiming to reduce the contribution of the noised data. However, we argue that this kind of method suffers from the low reliability of such noise data decision operation. It makes that some mistakenly abounded clean data are not utilized sufficiently and some mistakenly kept noised data disturbing the model learning. In this paper, we propose a more reliable noise-label suppression method called ReSup. First, instead of directly predicting noised or not, ReSup makes the noise data decision by modeling the distribution of noise and clean labels simultaneously according to the disagreement between the prediction and the target. Specifically, to achieve optimal distribution modeling, ReSup models the similarity distribution of all samples. To further enhance the reliability of our noise decision results, ReSup uses two networks to jointly achieve noise suppression. Specifically, ReSup utilize the property that two networks are less likely to make the same mistakes, making two networks swap decisions and tending to trust decisions with high agreement. Extensive experiments on popular datasets shows the effectiveness of ReSup.},
  archive      = {J_TAFFC},
  author       = {Xiang Zhang and Yan Lu and Huan Yan and Jinyang Huang and Yu Gu and Yusheng Ji and Zhi Liu and Bin Liu},
  doi          = {10.1109/TAFFC.2025.3549017},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2006-2019},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {ReSup: Reliable label noise suppression for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomic modulations to cardiac dynamics in response to affective touch: Differences between social touch and self-touch. <em>TAFFC</em>, <em>16</em>(3), 1996-2005. (<a href='https://doi.org/10.1109/TAFFC.2025.3548778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The autonomic nervous system plays a vital role in self-regulation and responding to environmental demands. Autonomic dynamics have been hypothesized to be involved in perceptual awareness and establishment and maintenance of the sense of a bodily self at a neural level. We hypothesized that the autonomic activity measured from cardiac dynamics could differentiate between social touch and self-touch. In our study, we used a newly developed method to analyze the temporal dynamics of cardiac sympathetic and parasympathetic activities during an ecologically valid affective touch experiment. We revealed that different types of touch conditions—social-touch, self-touch, and a control object-touch—resulted in a decrease in sympathetic activity. This decrease was more pronounced during social touch, as compared to the other conditions. Moreover, we quantified an increase in parasympathetic activity specifically during social touch, further distinguishing it from self-touch. Importantly, by combining the sympathetic and parasympathetic indices, we successfully differentiated social touch from the other experimental conditions, indicating that social touch exhibited the most substantial changes in cardiac autonomic indices. These findings may have important clinical implications as they provide insights into the neurophysiology of touch, relevant for aberrant affective touch processing in specific psychiatric disorders and for the comprehension of nociceptive touch.},
  archive      = {J_TAFFC},
  author       = {Diego Candia-Rivera and Rebecca Boehme and Paula C. Salamone},
  doi          = {10.1109/TAFFC.2025.3548778},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1996-2005},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Autonomic modulations to cardiac dynamics in response to affective touch: Differences between social touch and self-touch},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiview attention fusion for explainable body language behavior recognition. <em>TAFFC</em>, <em>16</em>(3), 1984-1995. (<a href='https://doi.org/10.1109/TAFFC.2025.3548781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Body language behavior , including gestures and fine-grained movements not only reflects human emotions, but also serves as a versatile cue for enhancing emotional intelligence and creating responsive technologies. In this work, we explore the efficacy of multiview-multimodal cues for explainable prediction of bodily behavior. This paper proposes an attention fusion method that combines features extracted from (1) multiview videos termed “RGB”, (2) their multiview Discrete Cosine Transform representations termed “DCT” and (3) three stream skeleton features termed “Skeleton”, via a transformer-based approach. We evaluate our approach on the diverse BBSI (Balazia et al., 2022) and Drive&Act (Martin et al., 2019) datasets. Empirical results confirm that the RGB, DCT and Skeleton features enable discovery of multiple class-specific behaviors resulting in explainable predictions. Our key findings are: (a) Multimodal approaches outperform unimodal counterparts in categorizing bodily behavioral classes; (b) Efficient class predictions and plausible explanations are achieved with both unimodal and multimodal approaches; and (c) Empirical results confirm the superiority of our approach compared to state-of-the-art methods on both datasets.},
  archive      = {J_TAFFC},
  author       = {Surbhi Madan and Rishabh Jain and Ramanathan Subramanian and Abhinav Dhall},
  doi          = {10.1109/TAFFC.2025.3548781},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1984-1995},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multiview attention fusion for explainable body language behavior recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a robust group-level emotion recognition via uncertainty-aware learning. <em>TAFFC</em>, <em>16</em>(3), 1970-1983. (<a href='https://doi.org/10.1109/TAFFC.2025.3547994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group-level emotion recognition (GER) is an inseparable part of human behavior analysis, aiming to recognize an overall emotion in a multi-person scene. However, the existing methods are devoted to combing diverse emotion cues while ignoring the inherent uncertainties under unconstrained environments, such as congestion and occlusion occurring within a group. Additionally, since only group-level labels are available, inconsistent emotion predictions among individuals in one group can confuse the network. In this paper, we propose an uncertainty-aware learning (UAL) method to extract more robust representations for GER. By explicitly modeling the uncertainty, we adopt stochastic embedding sourced from a Gaussian distribution instead of deterministic point embedding. It helps capture the probabilities of emotions and facilitates diverse inferences. Additionally, we adaptively assign uncertainty-sensitive scores as the fusion weights for individuals’ faces within a group. Moreover, we developed an image enhancement module to evaluate and filter samples, strengthening the model’s data-level robustness against uncertainties. The overall three-branch model, encompassing face, object, and scene components, is guided by a proportional-weighted fusion strategy and integrates the proposed uncertainty-aware method to produce the final group-level output. Experimental results demonstrate the effectiveness and generalization ability of our method across three widely used databases.},
  archive      = {J_TAFFC},
  author       = {Qing Zhu and Qirong Mao and Jialin Zhang and Xiaohua Huang and Wenming Zheng},
  doi          = {10.1109/TAFFC.2025.3547994},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1970-1983},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Towards a robust group-level emotion recognition via uncertainty-aware learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Micro-expression key frame inference. <em>TAFFC</em>, <em>16</em>(3), 1955-1969. (<a href='https://doi.org/10.1109/TAFFC.2025.3548284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions (MEs) are brief, involuntary facial movements critical for detecting lies, drawing growing interest in psychology and computer science. However, annotating ME can burden human coders with excessive time commitment and overwhelming information that compromises coding reliability and efficiency. Such difficulties in data annotation also led to the small sample size problem and hindered the development of ME analysis. Specifically, our psychological research highlights the complexities involved in human annotation of key frames. To facilitate the annotating process of ME, we proposed the Micro-Expression Key Frame Inference (ME-KFI) problem, aiming to identify MEs’ temporal locations from a single frame, reducing manual annotation effort. We propose a Micro-Expression Contrastive Identification Annotation (MECIA) method as a solution to ME-KFI, including three modules: a contrastive module, an identification module, and an annotation module, corresponding to the three steps of manual annotation. The network’s outputs infer the key frame of ME clips. MECIA demonstrates superior performance over random baselines on SAMM and CAS(ME)$^{2}$ databases and maintains comparable recognition accuracy with ground-truth clips.},
  archive      = {J_TAFFC},
  author       = {Su-Jing Wang and Yu-Han Miao and Jingting Li and Ling Zhou and Zizhao Dong and Mengyi Sun and Xiaolan Fu},
  doi          = {10.1109/TAFFC.2025.3548284},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1955-1969},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Micro-expression key frame inference},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stress severity detection in college students using emotional pulse signals and deep learning. <em>TAFFC</em>, <em>16</em>(3), 1942-1954. (<a href='https://doi.org/10.1109/TAFFC.2025.3547753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {College students face increasing stress from difficulties with studies, employment, and social interactions, which, if left unaddressed, may lead to depression and physical illnesses. Currently, the detection of stress severity relies on self-assessment scales, while machine learning or deep learning-based approaches primarily focus on classification. This study proposes an approach using pulse signals containing emotional cues and deep learning to automatically detect the severity of stress in college students. First, pulse signals of 177 college students were collected using photoplethysmography (PPG) during they watched five virtual reality (VR) emotional scenes, including calm, sadness, happiness, fear, and tension. Pulse rate variability (PRV) and discrete PPG (dPPG) were extracted from these signals as input for detecting stress severity. Then, the proposed stress detection framework, 1DCNN-BiLSTM + Cross-Attention + XGBoost, was employed to detect stress severity, incorporating an emotional Cross-Attention mechanism. The impact of induced emotions on stress severity detection performance was examined. The results indicated that stress severity detection in emotional scenes outperformed in calm. Furthermore, the detection performance that integrates multiple emotions surpassed single emotions. The fusion of PRV and dPPG signals yielded the best detection performance. This study provides an end-to-end automated approach for detecting stress severity in college students.},
  archive      = {J_TAFFC},
  author       = {Mi Li and Junzhe Li and Yanbo Chen and Bin Hu},
  doi          = {10.1109/TAFFC.2025.3547753},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1942-1954},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Stress severity detection in college students using emotional pulse signals and deep learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing correctness, fairness, and robustness of speech emotion recognition models. <em>TAFFC</em>, <em>16</em>(3), 1929-1941. (<a href='https://doi.org/10.1109/TAFFC.2025.3547218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models for speech emotion recognition (SER) can be trained for different tasks and are usually evaluated based on a few available datasets per task. Tasks could include arousal, valence, dominance, emotional categories, or tone of voice. Those models are mainly evaluated in terms of correlation or recall, and always show some errors in their predictions. The errors manifest themselves in model behaviour, which can be very different along different dimensions even if the same recall or correlation is achieved by the model. This paper introduces a testing framework to investigate behaviour of speech emotion recognition models, by requiring different metrics to reach a certain threshold in order to pass a test. The test metrics can be grouped in terms of correctness, fairness, and robustness. It also provides a method for automatically specifying test thresholds for fairness tests, based on the datasets used, and recommendations on how to select the remaining test thresholds. We evaluated a xLSTM-based and nine transformer-based acoustic foundation models against a convolutional baseline model, testing their performance on arousal, valence, dominance, and emotional category classification. The test results highlight, that models with high correlation or recall might rely on shortcuts – such as text sentiment –, and differ in terms of fairness.},
  archive      = {J_TAFFC},
  author       = {Anna Derington and Hagen Wierstorf and Ali Özkil and Florian Eyben and Felix Burkhardt and Björn W. Schuller},
  doi          = {10.1109/TAFFC.2025.3547218},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1929-1941},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Testing correctness, fairness, and robustness of speech emotion recognition models},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond overfitting: Doubly adaptive dropout for generalizable AU detection. <em>TAFFC</em>, <em>16</em>(3), 1916-1928. (<a href='https://doi.org/10.1109/TAFFC.2025.3545915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Action Units (AUs) are essential for conveying psychological states and emotional expressions. While automatic AU detection systems leveraging deep learning have progressed, they often overfit to specific datasets and individual features, limiting their cross-domain applicability. To overcome these limitations, we propose a doubly adaptive dropout approach for cross-domain AU detection, which enhances the robustness of convolutional feature maps and spatial tokens against domain shifts. This approach includes a Channel Drop Unit (CD-Unit) and a Token Drop Unit (TD-Unit), which work together to reduce domain-specific noise at both the channel and token levels. The CD-Unit preserves domain-agnostic local patterns in feature maps, while the TD-Unit helps the model identify AU relationships generalizable across domains. An auxiliary domain classifier, integrated at each layer, guides the selective omission of domain-sensitive features. To prevent excessive feature dropout, a progressive training strategy is used, allowing for selective exclusion of sensitive features at any model layer. Our method consistently outperforms existing techniques in cross-domain AU detection, as demonstrated by extensive experimental evaluations. Visualizations of attention maps also highlight clear and meaningful patterns related to both individual and combined AUs, further validating the approach's effectiveness.},
  archive      = {J_TAFFC},
  author       = {Yong Li and Yi Ren and Xuesong Niu and Yi Ding and Xiu-Shen Wei and Cuntai Guan},
  doi          = {10.1109/TAFFC.2025.3545915},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1916-1928},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Beyond overfitting: Doubly adaptive dropout for generalizable AU detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the relationship between stress-physiology and pain in the daily life of patients with chronic widespread pain. <em>TAFFC</em>, <em>16</em>(3), 1903-1915. (<a href='https://doi.org/10.1109/TAFFC.2025.3545477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic widespread pain remains a complex and incompletely understood condition. To complement existing pain assessment strategies, this study explored the ecological validity of unobtrusively captured daily life physiological signals as indicators of pain. Therefore, we collected physiological data using a wearable wristband from 46 patients with chronic widespread pain for seven days. Linear mixed-effect models revealed several significant associations between physiological signals, such as mean heart rate and momentary pain intensity. However, making individual pain predictions with multivariate machine learning models did not add value. While this study underscores the potential of ambulatory physiology for pain assessment, future research should validate and expand upon these initial findings to further enhance pain management strategies.},
  archive      = {J_TAFFC},
  author       = {Emilie Pattyn and Nattapong Thammasan and Hannah Davidoff and Walter De Raedt and Gudrun Vera Eisele and Ruud van Stiphout and Maarten De Vos and Olivia J. Kirtley and Peter Van Wambeke and Bart Morlion and Elfi Vergaelen and Chris Van Hoof},
  doi          = {10.1109/TAFFC.2025.3545477},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1903-1915},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploring the relationship between stress-physiology and pain in the daily life of patients with chronic widespread pain},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic and emotional dual channel for emotion recognition in conversation. <em>TAFFC</em>, <em>16</em>(3), 1885-1902. (<a href='https://doi.org/10.1109/TAFFC.2025.3544608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in conversation (ERC) aims at accurately identifying emotional states expressed in conversational content. Existing ERC methods, although relying on semantic understanding, often encounter challenges when confronted with incomplete or misleading semantic information. In addition, when dealing with the interaction between emotional and semantic information, existing methods are often difficult to effectively distinguish the complex relationship between the two, which affects the accuracy of emotion recognition. To address the problems of semantic misdirection and emotional cross-talk encountered by traditional models when confronted with complex conversational data, we propose a semantic and emotional dual channel (SEDC) strategy for emotion recognition in conversations to process emotional and semantic information independently. Under this strategy, emotion information provides an auxiliary recognition function when the semantics are unclear or lacking, enhancing the accuracy of the model. Our model consists of two modules: the emotion processing module accurately captures the emotional features of each utterance through contrastive learning, and then constructs a dialogue emotion propagation map to simulate the emotional information conveyed in the dialogue; the semantic processing module combines an external knowledge base to enhance the semantic expression of the dialogue through knowledge enhancement strategies. This divide-and-conquer approach allows us to more deeply analyze the emotional and semantic dimensions of complex dialogues. Experimental results on the IEMOCAP, EmoryNLP, MELD, and DailyDialog datasets show that our approach significantly outperforms existing techniques and effectively improves the accuracy of dialogue emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Zhenyu Yang and Zhibo Zhang and Yuhu Cheng and Tong Zhang and Xuesong Wang},
  doi          = {10.1109/TAFFC.2025.3544608},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1885-1902},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Semantic and emotional dual channel for emotion recognition in conversation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empathy level alignment via reinforcement learning for empathetic response generation. <em>TAFFC</em>, <em>16</em>(3), 1873-1884. (<a href='https://doi.org/10.1109/TAFFC.2025.3544594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empathetic response generation, aiming to understand the user’s situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Traditional approaches typically employ maximum likelihood estimation as the optimization objective during training, yet fail to align the empathy levels between generated and target responses. To this end, we propose an empathetic response generation framework using reinforcement learning (EmpRL). The framework develops an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. EmpRL utilizes the pre-trained T5 model as the generator and further fine-tunes it to initialize the policy. To align the empathy levels between generated and target responses within a given context, an empathy reward function containing three empathy communication mechanisms—emotional reaction, interpretation, and exploration—is constructed using pre-designed and pre-trained empathy identifiers. During reinforcement learning training, the proximal policy optimization algorithm is used to fine-tune the policy, enabling the generation of empathetic responses. Both automatic and human evaluations demonstrate that the proposed EmpRL framework significantly improves the quality of generated responses, enhances the similarity in empathy levels between generated and target responses, and produces empathetic responses covering both affective and cognitive aspects.},
  archive      = {J_TAFFC},
  author       = {Hui Ma and Bo Zhang and Bo Xu and Jian Wang and Hongfei Lin and Xiao Sun},
  doi          = {10.1109/TAFFC.2025.3544594},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1873-1884},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Empathy level alignment via reinforcement learning for empathetic response generation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic depression recognition with an ensemble of multimodal spatio-temporal routing features. <em>TAFFC</em>, <em>16</em>(3), 1855-1872. (<a href='https://doi.org/10.1109/TAFFC.2025.3543226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression, driven by growing societal pressures, significantly disrupts individuals’ physical and mental health. Automatic Depression Recognition (ADR) via facial videos has gained attention to enhance diagnostic accuracy and efficiency. However, extant methods often segment videos, losing long-term behavioral cues and introducing noise, while also exhibiting performance drops across diverse cultural and racial datasets. This study proposes a multimodal ADR approach encompassing three key components: (1) Long-term Depression Behavior Module (LDBM) employing a Transformer to capture extended depression cues, (2) Noisy Information Elimination (NIE) strategy leveraging LDBM attention scores to reduce noise and boost diagnostic precision, and (3) Multimodal Spatio-temporal Routing Feature Ensemble (MSRE) that fuses texture, Facial Action Primitives (FAPs), and Remote Photoplethysmography (rPPG) data for improved cross-dataset generalizability. Experiments on AVEC 2013, AVEC 2014, and a newly constructed CMDep dataset of 123 clinically diagnosed participants validate our method, achieving MAE/RMSE scores of 5.38/6.74, 5.09/6.83, and 5.59/8.03, respectively. The CMDep dataset includes facial expression and voice signals, with labels derived from BDI-II scores. Additionally, our method has been integrated into a user-friendly mobile application, providing a tool for real-time self-assessment of depression. This integration broadens the scope of depression detection, making it accessible to diverse populations worldwide.},
  archive      = {J_TAFFC},
  author       = {Yaowei Wang and Zulong Lin and Chengrong Yang and Yujue Zhou and Yun Yang},
  doi          = {10.1109/TAFFC.2025.3543226},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1855-1872},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Automatic depression recognition with an ensemble of multimodal spatio-temporal routing features},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SigWavNet: Learning multiresolution signal wavelet network for speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1839-1854. (<a href='https://doi.org/10.1109/TAFFC.2025.3537991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of human-computer interaction and psychological assessment, speech emotion recognition (SER) plays an important role in deciphering emotional states from speech signals. Despite advancements, challenges persist due to system complexity, feature distinctiveness issues, and noise interference. This paper introduces a new end-to-end (E2E) deep learning multi-resolution framework for SER, addressing these limitations by extracting meaningful representations directly from raw waveform speech signals. By leveraging the properties of the fast discrete wavelet transform (FDWT), including the cascade algorithm, conjugate quadrature filter, and coefficient denoising, our approach introduces a learnable model for both wavelet bases and denoising through deep learning techniques. The framework incorporates an activation function for learnable asymmetric hard thresholding of wavelet coefficients. Our approach exploits the capabilities of wavelets for effective localization in both time and frequency domains. We then combine one-dimensional dilated convolutional neural networks (1D dilated CNN) with a spatial attention layer and bidirectional gated recurrent units (Bi-GRU) with a temporal attention layer to efficiently capture the nuanced spatial and temporal characteristics of emotional features. By handling variable-length speech without segmentation and eliminating the need for pre or post-processing, the proposed model outperformed state-of-the-art methods on IEMOCAP and EMO-DB datasets.},
  archive      = {J_TAFFC},
  author       = {Alaa Nfissi and Wassim Bouachir and Nizar Bouguila and Brian Mishara},
  doi          = {10.1109/TAFFC.2025.3537991},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1839-1854},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SigWavNet: Learning multiresolution signal wavelet network for speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-guided reconstruction network for sentiment analysis with uncertain missing modalities. <em>TAFFC</em>, <em>16</em>(3), 1825-1838. (<a href='https://doi.org/10.1109/TAFFC.2025.3541743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Sentiment Analysis (MSA) is an attractive research that aims to integrate sentiment expressed in textual, visual, and acoustic signals. There are two main problems in the existing methods: 1) the dominant role of the text is underutilization in unaligned multimodal data, and 2) the modality under uncertain missing feature is not sufficiently explored. This paper proposes a Text-guided Reconstruction Network (TgRN) for MSA with uncertain missing modalities in non-aligned sequences. The TgRN network includes three primary modules: Text-guided Extraction Module (TEM), Reconstruction Module (RM) and Text-guided Fusion Module (TFM). First, the TEM consists of the text-guided cross attention units and self-attention units to capture inter-modal features and intra-modal features, respectively. Second, leveraging enhanced attention units and a three-way squeeze-and-excitation block, the RM is designed to learn semantic information from incomplete data and reconstruct missing modality features. Third, the TFM utilizes a progressive modality-mixing adaptation gate to explore the dynamic correlations between nonverbal and verbal modalities, effectively addressing the modality gap issue. Finally, under the supervision of sentiment prediction loss and reconstruction loss, the TgRN effectively processes both uncertain missing-modality conditions and ideal complete modality conditions. Extensive experiments on CMU-MOSI and CH-SIMS demonstrate that our proposed method outperforms state-of-the-art approaches.},
  archive      = {J_TAFFC},
  author       = {Piao Shi and Min Hu and Satoshi Nakagawa and Xiangming Zheng and Xuefeng Shi and Fuji Ren},
  doi          = {10.1109/TAFFC.2025.3541743},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1825-1838},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Text-guided reconstruction network for sentiment analysis with uncertain missing modalities},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal depression prediction. <em>TAFFC</em>, <em>16</em>(3), 1814-1824. (<a href='https://doi.org/10.1109/TAFFC.2025.3542023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While existing depression prediction methods based on deep learning show promise, their practical application is hindered by the lack of trustworthiness, as these deep models are often deployed as black box models, leaving us uncertain on the confidence of their predictions. For high-risk clinical applications like depression prediction, uncertainty quantification is essential in decision-making. In this paper, we introduce conformal depression prediction (CDP), a depression prediction method with uncertainty quantification based on conformal prediction (CP), giving valid confidence intervals with theoretical coverage guarantees for the model predictions. CDP is a plug-and-play module that requires neither model retraining nor an assumption about the depression data distribution. As CDP provides only an average coverage guarantee across all inputs rather than per-input performance guarantee, we further propose CDP-ACC, an improved conformal prediction with approximate conditional coverage. CDP-ACC firstly estimates the prediction distribution through neighborhood relaxation, and then introduces a conformal score function by constructing nested sequences, so as to provide a tighter prediction interval adaptive to specific input. We empirically demonstrate the application of CDP in uncertainty-aware facial depression prediction, as well as the effectiveness and superiority of CDP-ACC on the AVEC 2013 and AVEC 2014 datasets.},
  archive      = {J_TAFFC},
  author       = {Yonghong Li and Shan Qu and Xiuzhuang Zhou},
  doi          = {10.1109/TAFFC.2025.3542023},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1814-1824},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Conformal depression prediction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDRS: Sentiment-aware disentangled representation shifting for multimodal sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 1802-1813. (<a href='https://doi.org/10.1109/TAFFC.2025.3539225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) aims to leverage the complementary information from multiple modalities for affective understanding of user-generated videos. Existing methods mainly focused on designing sophisticated feature fusion strategies to integrate the separately extracted multimodal representations, ignoring the interference of the information irrelevant to sentiment. In this paper, we propose to disentangle the unimodal representations into sentiment-specific and sentiment-independent features, the former of which are fused for the MSA task. Specifically, we design a novel Sentiment-aware Disentangled Representation Shifting framework, termed SDRS, with two components. Interactive sentiment-aware representation disentanglement aims to extract sentiment-specific feature representations for each nonverbal modality by considering the contextual influence of other modalities with the newly developed cross-attention autoencoder. Attentive cross-modal representation shifting tries to shift the textual representation in a latent token space using the nonverbal sentiment-specific representations after projection. The shifted representation is finally employed to fine-tune a pre-trained language model for multimodal sentiment analysis. Extensive experiments are conducted on three public benchmark datasets, i.e., CMU-MOSI, CMU-MOSEI, and CH-SIMS. The results demonstrate that the proposed SDRS framework not only obtains state-of-the-art results based solely on multimodal labels but also outperforms the methods that additionally require the labels of each modality.},
  archive      = {J_TAFFC},
  author       = {Sicheng Zhao and Zhenhua Yang and Henglin Shi and Xiaocheng Feng and Lingpengkun Meng and Bing Qin and Chenggang Yan and Jianhua Tao and Guiguang Ding},
  doi          = {10.1109/TAFFC.2025.3539225},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1802-1813},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SDRS: Sentiment-aware disentangled representation shifting for multimodal sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing the visual road scene for driver stress estimation. <em>TAFFC</em>, <em>16</em>(3), 1787-1801. (<a href='https://doi.org/10.1109/TAFFC.2025.3539003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the contribution of the visual road scene to estimate the driver-reported stress levels. Our research leverages on previous work showing that environmental factors, such as traffic congestion, weather conditions, and driving context, impact driver’s stress. Each of the models we evaluated is trained and tested with the publicly available AffectiveROAD dataset to estimate three categories of driver-reported stress level. We test three types of modelling approaches: (i) single-frame baselines (Random Forest, SVM, and Convolutional Neural Networks); (ii) Temporal Segment Networks (TSN) and two variants of it, which use learned weights (TSN-w) and LSTM (TSN-LSTM) as consensus functions; and (iii) video classification Transformers. Our experiments reveal that the TSN-w, TSN-LSTM, and Transformer models achieve statistically equivalent performances, all significantly outperforming the other models. Particularly noteworthy is TSN-w, which attains the highest performance observed with an average accuracy of 0.77. We further provide an explainability analysis using Class Activation Mapping and image semantic segmentation to identify the elements of the road scene that contribute the most to high levels of stress. Our results demonstrate that the visible road scene offers significant contextual information for estimating driver-reported stress levels, with potential implications for the design of safer urban road environments.},
  archive      = {J_TAFFC},
  author       = {Cristina Bustos and Albert Sole-Ribalta and Neska Elhaouij and Javier Borge-Holthoefer and Agata Lapedriza and Rosalind Picard},
  doi          = {10.1109/TAFFC.2025.3539003},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1787-1801},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Analyzing the visual road scene for driver stress estimation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupled multi-perspective fusion for speech depression detection. <em>TAFFC</em>, <em>16</em>(3), 1772-1786. (<a href='https://doi.org/10.1109/TAFFC.2025.3538519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech Depression Detection (SDD) has garnered attention from researchers due to its low cost and convenience. However, current algorithms lack methods for extracting interpretable acoustic features based on clinical manifestations. In addition, effectively fusing these features to overcome individual heterogeneity remains a challenge. This study proposes a decoupled multi-perspective fusion (DMPF) model. The model extracts five key features of voiceprint, emotion, pause, energy, and tremor based on the multi-perspective clinical manifestations. These features are then decoupled into common and private features, which fused through graph attention network to obtain the comprehensive depression representation. Notably, this study has collected a depression speech dataset, which includes standardized and comprehensive tasks along with diagnostic labels provided by psychologists. Extensive subject-independent experiments were conducted on the DAIC-WOZ, MODMA and MPSC datasets. The voiceprint features can automatically cluster the depressed and non-depressed populations. Furthermore, DMPF can effectively fuse common and private features from different perspectives, achieving AUC of 84.20%, 85.34%, 86.13% on three datasets. The results illustrate the interpretability of multi-perspective features and demonstrate that the combination of speech manifestations can enhance the detection ability, which can provide a multi-perspective observational tool for physicians and clinical practice.},
  archive      = {J_TAFFC},
  author       = {Minghui Zhao and Hongxiang Gao and Lulu Zhao and Zhongyu Wang and Fei Wang and Wenming Zheng and Jianqing Li and Chengyu Liu},
  doi          = {10.1109/TAFFC.2025.3538519},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1772-1786},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Decoupled multi-perspective fusion for speech depression detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RVISA: Reasoning and verification for implicit sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 1760-1771. (<a href='https://doi.org/10.1109/TAFFC.2025.3537799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the context of the increasing social demand for fine-grained sentiment analysis (SA), implicit sentiment analysis (ISA) poses a significant challenge owing to the absence of salient cue words in expressions. Thus, reliable reasoning is required to understand how sentiment is evoked, enabling the identification of implicit sentiments. In the era of large language models (LLMs), encoder-decoder (ED) LLMs have emerged as popular backbone models for SA applications, given their impressive text comprehension and reasoning capabilities across diverse tasks. In comparison, decoder-only (DO) LLMs exhibit superior natural language generation and in-context learning capabilities. However, their responses may contain misleading or inaccurate information. To accurately identify implicit sentiments with reliable reasoning, this study introduces a two-stage reasoning framework named Reasoning and Verification for Implicit Sentiment Analysis (RVISA), which leverages the generation ability of DO LLMs and reasoning ability of ED LLMs to train an enhanced reasoner. The framework involves three-hop reasoning prompting to explicitly furnish sentiment elements as cues. The generated rationales are then used to fine-tune an ED LLM into a skilled reasoner. Additionally, we develop a straightforward yet effective answer-based verification mechanism to ensure the reliability of reasoning learning. Evaluation of the proposed method on two benchmark datasets demonstrates that it achieves state-of-the-art performance in ISA.},
  archive      = {J_TAFFC},
  author       = {Wenna Lai and Haoran Xie and Guandong Xu and Qing Li},
  doi          = {10.1109/TAFFC.2025.3537799},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1760-1771},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {RVISA: Reasoning and verification for implicit sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LineConGraphs: Line conversation graphs for effective emotion recognition using graph neural networks. <em>TAFFC</em>, <em>16</em>(3), 1747-1759. (<a href='https://doi.org/10.1109/TAFFC.2025.3537538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion Recognition in Conversations (ERC) is an important aspect of affective computing with practical applications in healthcare, education, chatbots, and social media platforms. Previous approaches to $\text {ERC}$ analysis involved using graph neural network architectures to model both speaker and long-term contextual information. In this paper, we introduce new models for $\text {ERC}$ analysis: the LineConGCN and LineConGAT models, which are constructed using a graph construction strategy for conversations called line conversational graphs (LineConGraphs). LineConGraph is designed to capture short-term conversational context using one previous and future utterance, while also capturing long-term context using GCN or GAT layers without explicitly integrating into the graph construction strategy. We evaluate the performance of our proposed models on two benchmark datasets, $\text {IEMOCAP}$ and $\text {MELD}$, and show that our LineConGAT model outperforms the state-of-the-art methods with an F1-score of $\text {64.58}\%$ and $\text {76.50}\%$. Furthermore, we demonstrate that incorporating sentiment shift information into line conversation graphs further enhances $\text {ERC}$ performance in the case of LineConGCN models. We also evaluate the performance of our proposed model by embedding speaker information into LineConGCN and LineConGAT models and show that LineConGAT and LineConGAT with speaker embeddings performed equally for ERC analysis.},
  archive      = {J_TAFFC},
  author       = {Gokul S Krishnan and Sarala Padi and Craig S. Greenberg and Balaraman Ravindran and Dinesh Manocha and Ram D. Sriram},
  doi          = {10.1109/TAFFC.2025.3537538},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1747-1759},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {LineConGraphs: Line conversation graphs for effective emotion recognition using graph neural networks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSE-GResNet: A simple and highly efficient network for facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 1732-1746. (<a href='https://doi.org/10.1109/TAFFC.2025.3535811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) has recently attracted extensive attention in computer vision. However, existing methods mostly focus on the explicit performance and overlook their computational resources. Hence, achieving competitive performance while maintaining the model efficiency is still a huge challenge. To tackle these issues, we propose a highly lightweight yet effective Channel Shift-Enhancement Gabor-ResNet (CSE-GResNet) to capture the crucial visual properties in facial images. Concretely, we incorporate the Gabor Convolution (GConv) into ResNet to produce the robust GResNet as our backbone with limited memory cost. Furthermore, we propose extremely efficient Channel-Shift Module and Channel-Enhancement Module to insert in the GResNet in cascade. They are adopted to obtain and aggregate the facial informative representation from adjacent channels for extracting the subtle facial expression representation. We conduct extensive experiments on three wild datasets: RAF-DB, FER2013 and SFEW. The results show that the proposed CSE-GResNet achieves superior performance against the state-of-the-art methods with less computational and memory cost.},
  archive      = {J_TAFFC},
  author       = {Shaoping Jiang and Xiaofen Xing and Fang Liu and Xiangmin Xu and Lin Wang and Kailing Guo},
  doi          = {10.1109/TAFFC.2025.3535811},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1732-1746},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {CSE-GResNet: A simple and highly efficient network for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale hyperbolic contrastive learning for cross-subject EEG emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1716-1731. (<a href='https://doi.org/10.1109/TAFFC.2025.3535542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) serves as a reliable and objective signal for affective computing applications. However, individual differences in EEG signals pose a significant challenge for emotion recognition tasks across subjects. To address this, we proposed a novel method called Multi-Scale Hyperbolic Contrastive Learning (MSHCL), which leverages event-relatedness to learn subject-invariant representations. MSHCL employs contrastive losses at two different scales—emotion and stimulus—to effectively capture complex EEG patterns within a hyperbolic space hierarchy. Our method is evaluated on three datasets: SEED, MPED, and FACED. It achieves 89.3% accuracy on the three-class task for SEED, 38.8% on the seven-class task for MPED, and 77.0% and 45.7% on the binary and nine-class tasks for FACED in cross-subject emotion recognition. These results demonstrate that the proposed MSHCL method superior performance over other baselines and its effectiveness in learning subject-invariant representations.},
  archive      = {J_TAFFC},
  author       = {Jiang Chang and Zhixin Zhang and Yuhua Qian and Pan Lin},
  doi          = {10.1109/TAFFC.2025.3535542},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1716-1731},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multi-scale hyperbolic contrastive learning for cross-subject EEG emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SVD-guided multimodal feature fusion for emotion recognition from facial videos. <em>TAFFC</em>, <em>16</em>(3), 1705-1715. (<a href='https://doi.org/10.1109/TAFFC.2025.3528636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition based on facial videos aims to extract features from different modalities to identify human emotions. The previous work focus on designing various fusion schemes to combine heterogeneous modal data. However, most studies have overlooked the role of different modalities in emotion recognition and have not fully utilized the intrinsic connections between modalities. Furthermore, the multimodal data from facial videos also contain various distractions bad for emotion analysis. How to reduce the impact of distractions and enable a model to mine effective information for emotion recognition from different modalities is still a challenge problem. To address above issue, we propose a SVD-guided multimodal feature fusion method based on facial video for emotion recognition, which uses a hierarchical fusion mechanism and adopts different loss strategies at each level to learn multimodal feature representation. Specifically, we fuse the facial expression and rPPG signal (or Point-of-Gaze) by using the weak supervision strategy and contrastive learning. Subsequently, the fused feature of facial expression and rPPG signal and the fused feature of facial expression and Point-of-Gaze are combined together to construct the unified multimodal feature matrix. Based on this, Singular Value Decomposition (SVD) is used to refine the redundancy information caused by the multimodal fusion and guide the neural network to learn discriminative emotion feature. At the same time, a consistent loss is developed to enhance the multimodal representation. Experiments on three public datasets show that the proposed method achieves better results over the compared methods.},
  archive      = {J_TAFFC},
  author       = {Jindi Bao and Jianjun Qian and Jian Yang},
  doi          = {10.1109/TAFFC.2025.3528636},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1705-1715},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SVD-guided multimodal feature fusion for emotion recognition from facial videos},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectro-temporal modulations incorporated two-stream robust speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1693-1704. (<a href='https://doi.org/10.1109/TAFFC.2025.3531638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based speech emotion recognition (SER) models have shown impressive results in controlled environments, but their performance significantly degrades in noisy conditions. This paper proposes a robust two-stream SER model by combining spectro-temporal modulation features with conventional acoustic features. Experiments were conducted on German (EMODB) and English (RAVDESS) datasets using the clean-train-noisy-test paradigm. The results demonstrate that spectro-temporal modulation features offer superior robustness in noisy conditions compared with conventional acoustic features such as MFCCs and time-frequency features from Mel-spectrograms. Additionally, we analyze weights of modulation features and demonstrate the model emphasizes contours of formants and harmonics, which are crucial features for speech perception in noise, for robust SER. Incorporating the stream of spectro-temporal modulations not only enhances the robustness of the model but also provides deeper insights into the task of SER in noise.},
  archive      = {J_TAFFC},
  author       = {Yih-Liang Shen and Pei-Chin Hsieh and Tai-Shih Chi},
  doi          = {10.1109/TAFFC.2025.3531638},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1693-1704},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Spectro-temporal modulations incorporated two-stream robust speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling fine-grained relations in dynamic space-time graphs for video-based facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 1675-1692. (<a href='https://doi.org/10.1109/TAFFC.2025.3530973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions in videos inherently mirror the dynamic nature of real-world facial events. Consequently, facial expression recognition (FER) should employ a dynamic graph-based representation to effectively capture the relational structure of facial expressions rather than relying on conventional grid or sequence methods. However, existing graph-based approaches have their limitations. Frame-level graph methods provide a coarse representation of the facial graph across time and space, while landmark-based graph methods need to introduce additional facial landmarks, resulting in a static graph structure. To address these challenges, we propose spatial-temporal relation-aware dynamic graph convolutional networks (ST-RDGCN). This fine-grained relation modeling approach enables the dynamic modeling of evolving facial expressions in videos through dynamic space-time graphs, eliminating the need for facial landmarks. ST-RDGCN encompasses three graph construction paradigms: dynamic independent space graph, dynamic joint space-time graph, and dynamic cross space-time graph. Furthermore, we propose a relation-aware space-time graph convolution (RSTG-Conv) operator to learn informative spatiotemporal correlations in dynamic space-time graphs. In extensive experimental evaluations, our ST-RDGCN demonstrates state-of-the-art performance on the five popular video-based FER datasets, achieving overall accuracy scores of 99.69%, 91.67%, 56.51%, 69.37%, and 49.03% on the CK+, Oulu-CASIA, AFEW, DFEW, and FERV39k datasets, respectively. In particular, our ST-RDGCN outperforms the current best method by 3.6% in UAR on the most challenging FERV39k dataset. Furthermore, our analysis reveals that the dynamic cross space-time graph scheme is the most effective among the three dynamic graph construction schemes.},
  archive      = {J_TAFFC},
  author       = {Changqin Huang and Fan Jiang and Zhongmei Han and Xiaodi Huang and Shijin Wang and Yanlai Zhu and Yunliang Jiang and Bin Hu},
  doi          = {10.1109/TAFFC.2025.3530973},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1675-1692},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Modeling fine-grained relations in dynamic space-time graphs for video-based facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DurFlex-EVC: Duration-flexible emotional voice conversion leveraging discrete representations without text alignment. <em>TAFFC</em>, <em>16</em>(3), 1660-1674. (<a href='https://doi.org/10.1109/TAFFC.2025.3530920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional voice conversion (EVC) involves modifying various acoustic characteristics, such as pitch and spectral envelope, to match a desired emotional state while preserving the speaker’s identity. Existing EVC methods often rely on text transcriptions or time-alignment information and struggle to handle varying speech durations effectively. In this paper, we propose DurFlex-EVC, a duration-flexible EVC framework that operates without the need for text or alignment information. We introduce a unit aligner that models contextual information by aligning speech with discrete units representing content, eliminating the need for text or speech-text alignment. Additionally, we design a style autoencoder that effectively disentangles content and emotional style, allowing precise manipulation of the emotional characteristics of the speech. We further enhance emotional expressiveness through a hierarchical stylize encoder that applies the target emotional style at multiple hierarchical levels, refining the stylization process to improve the naturalness and expressiveness of the converted speech. Experimental results from subjective and objective evaluations demonstrate that our approach outperforms baseline models, effectively handling duration variability and enhancing emotional expressiveness in the converted speech.},
  archive      = {J_TAFFC},
  author       = {Hyung-Seok Oh and Sang-Hoon Lee and Deok-Hyeon Cho and Seong-Whan Lee},
  doi          = {10.1109/TAFFC.2025.3530920},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1660-1674},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {DurFlex-EVC: Duration-flexible emotional voice conversion leveraging discrete representations without text alignment},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional connectivity analysis of children with autism under emotional clips. <em>TAFFC</em>, <em>16</em>(3), 1646-1659. (<a href='https://doi.org/10.1109/TAFFC.2025.3528920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder with marked impairments in neural system functioning. Electroencephalography (EEG) offers a promising approach to investigate the neurophysiological basis of ASD, however, most EEG studies in ASD focus on spontaneous brain activity. Emotional processing deficits are a core feature of ASD, but related connectivity patterns remain underexplored due to challenges in data collection and analysis. This study investigates functional brain connectivity differences between children with ASD (n = 32) and typically developing (TD) children (n = 32) across five frequency bands and four connectivity indices. We designed an SVM-MRMR pipeline to classify ASD and TD children using these features. Our findings reveal that ASD children exhibit more coordinated intra-brain networks and oscillatory patterns in the high-frequency range. Additionally, they show an increased number of long-range connections in the Theta band, particularly between the left and right hemispheres. ASD children also demonstrate increased frontal lobe connectivity during positive emotions and heightened temporal lobe activity during negative emotions. Functional connectivity under positive and negative emotional clips achieved a classification accuracy exceeding 85%. These findings suggest that functional connectivity derived from portable EEG devices may serve as a potential biomarker for diagnosing and classifying ASD in real-world applications.},
  archive      = {J_TAFFC},
  author       = {Chang Cai and Jiahui Wang and Jun Lin and Kang Yang and Huicong Kang and Jingying Chen and Wei Wu},
  doi          = {10.1109/TAFFC.2025.3528920},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1646-1659},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Functional connectivity analysis of children with autism under emotional clips},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phonetically-anchored domain adaptation for cross-lingual speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1631-1645. (<a href='https://doi.org/10.1109/TAFFC.2025.3530105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of cross-lingual speech emotion recognition (SER) modeling has significantly increased due to its wide range of applications. Previous studies have primarily focused on technical strategies to adapt features, domains, and labels across languages, often overlooking the underlying commonalities between the languages. In this study, we address the language adaptation challenge in cross-lingual scenarios by incorporating vowel-phonetic constraints. Our approach is structured in two main parts. First, we investigate the vowel-phonetic commonalities associated with specific emotions across languages, particularly focusing on common vowels that prove to be valuable for SER modeling. Second, we utilize these identified common vowels as anchors to facilitate cross-lingual SER. To demonstrate the effectiveness of our approach, we conduct case studies using American English and Taiwanese Mandarin with two naturalistic emotional speech corpora: the MSP-Podcast and BIIC-Podcast corpora. The approach leverages evidence that certain vowels, including monophthongs and diphthongs, exhibit emotion-specific commonality across languages, serving as phonetic anchors to enhance unsupervised cross-lingual SER learning. The proposed model surpasses baseline performance, highlighting the importance of phonetic similarities for effective language adaptation in cross-lingual SER scenarios.},
  archive      = {J_TAFFC},
  author       = {Shreya G. Upadhyay and Luz Martinez-Lucas and William Katz and Carlos Busso and Chi-Chun Lee},
  doi          = {10.1109/TAFFC.2025.3530105},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1631-1645},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Phonetically-anchored domain adaptation for cross-lingual speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised cross-domain facial expression recognition via class adaptive self-training. <em>TAFFC</em>, <em>16</em>(3), 1618-1630. (<a href='https://doi.org/10.1109/TAFFC.2025.3529947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Cross-Domain Facial Expression Recognition (CD-FER) aims to transfer the recognition ability from annotated source domains to unlabeled target domains. Despite the advancements in CD-FER techniques based on marginal distribution matching, certain inherent properties of facial expressions, such as implicit class margins and imbalanced class distributions, still leave room for improvement in existing models. In this paper, we propose a Class-Adaptive Self-Training (CAST) model for unsupervised CD-FER. In addition to domain alignment, the CAST model leverages self-training to learn pseudo labels and dually enhance aligned representations for explicit class distinction, considering implicit class margins. Furthermore, the CAST model conducts a comprehensive analysis of the negative effects of class distributions on pseudo-label learning from perspectives of class-level representation distributions and predicted probabilities, and subsequently proposes specific solutions. By jointly matching class-level representation distributions and class distributions, the CAST model successfully alleviates conditional distribution discrepancies between domains, which is particularly pertinent for facial expression properties. Experimental results, including assessments on multiple target domains and evaluations of multiple FER models, demonstrate the effectiveness, superiority, and universality of the CAST model.},
  archive      = {J_TAFFC},
  author       = {Shanmin Wang and Qingshan Liu},
  doi          = {10.1109/TAFFC.2025.3529947},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1618-1630},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Unsupervised cross-domain facial expression recognition via class adaptive self-training},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal sentiment analysis with mutual information-based disentangled representation learning. <em>TAFFC</em>, <em>16</em>(3), 1606-1617. (<a href='https://doi.org/10.1109/TAFFC.2025.3529732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis seeks to utilize various types of signals to identify underlying emotions and sentiments. A key challenge in this field lies in multimodal representation learning, which aims to develop effective methods for integrating multimodal features into cohesive representations. Recent advancements include two notable approaches: one focuses on decomposing multimodal features into modality-invariant and -specific components, while the other emphasizes the use of mutual information to enhance the fusion of modalities. Both strategies have demonstrated effectiveness and yielded remarkable results. In this paper, we propose a novel learning framework that combines the strengths of these two approaches, termed mutual information-based disentangled multimodal representation learning. Our approach involves estimating different types of information during feature extraction and fusion stages. Specifically, we quantitatively assess and adjust the proportions of modality-invariant, -specific, and -complementary information during feature extraction. Subsequently, during fusion, we evaluate the amount of information retained by each modality in the fused representation. We employ mutual information or conditional mutual information to estimate each type of information content. By reconciling the proportions of these different types of information, our approach achieves state-of-the-art performance on popular sentiment analysis benchmarks, including CMU-MOSI and CMU-MOSEI.},
  archive      = {J_TAFFC},
  author       = {Hao Sun and Ziwei Niu and Hongyi Wang and Xinyao Yu and Jiaqing Liu and Yen-Wei Chen and Lanfen Lin},
  doi          = {10.1109/TAFFC.2025.3529732},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1606-1617},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multimodal sentiment analysis with mutual information-based disentangled representation learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TFAGL: A novel agent graph learning method using time-frequency EEG for major depressive disorder detection. <em>TAFFC</em>, <em>16</em>(3), 1592-1605. (<a href='https://doi.org/10.1109/TAFFC.2025.3527459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The abnormality in depression exhibits reciprocal imbalanced connectivity between brain regions rather than increased or decreased activity of one particular area. Current works primarily align the distributions of EEG electrodes with insufficient simulation of neurophysiological structures. Moreover, they neglect significant collaborative relationships among diverse brain regions, which limits the performance of MDD detection. Considering the comprehensive information across brain regions and domains, we propose a novel EEG-based MDD detection model named Time-Frequency Agent Graph Learning (TFAGL), to capture the specific whole-brain level collaborative mechanism of MDD. Specifically, we generate agent nodes adaptively to perform global interactions among regions to sufficiently simulate the function of principal neurons, thereby forming a dynamic local-global connectivity graph to capture connectivity patterns for intra- and inter-regions. Furthermore, interactive learning across different receptive fields through multi-scale graph convolution is applied for each domain and connectivity. Besides, we construct feature extractors for both time and frequency domains and apply intra- and inter-domain constraints to remove redundancy and enhance the discriminability, thus obtaining comprehensive information representations. Extensive experiments on the public EEG MDD detection datasets demonstrate the superiority of TFAGL compared with the state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Zihua Xu and C. L. Philip Chen and Tong Zhang},
  doi          = {10.1109/TAFFC.2025.3527459},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1592-1605},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {TFAGL: A novel agent graph learning method using time-frequency EEG for major depressive disorder detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multitask transformer for cross-corpus speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1581-1591. (<a href='https://doi.org/10.1109/TAFFC.2025.3526592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has significantly advanced the field of Speech Emotion Recognition (SER), yet its efficacy in cross-corpus scenarios remains a challenge. To overcome this limitation, recent studies demonstrate the success of multitask learning, which uses auxiliary tasks to reduce difference between source and target dataset (or transfer knowledge from source to target datasets). Despite the efforts, the overall accuracy for cross-corpus SER is still relatively low and needs attention. To improve performance, we propose a multitask framework with SER as the primary task and contrastive learning and information maximization as auxiliary tasks. We design the auxiliary tasks innovatively to use the target data without emotional labels to develop a better understanding of the target data. The core of our multitask framework is a pre-trained transformer. While transformers have gained attention in SER, their application to cross-corpus scenarios is still limited. Multimodal approaches for cross-corpus scenario is substantially limited as well. We use text as the second modality, developing separate multitask transformers for audio and text and conduct decision-level fusion during inference. We use publicly available and widely used speech corpora, including the IEMOCAP, MSP-IMPROV and EMO-DB databases. The results demonstrate the benefits of the proposed approach, achieving improved performance on the benchmark databases in cross-corpus settings.},
  archive      = {J_TAFFC},
  author       = {Chung-Soo Ahn and Rajib Rana and Carlos Busso and Jagath C. Rajapakse},
  doi          = {10.1109/TAFFC.2025.3526592},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1581-1591},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multitask transformer for cross-corpus speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECDaily: A large-scale benchmark for emotion cause extraction in conversations. <em>TAFFC</em>, <em>16</em>(3), 1570-1580. (<a href='https://doi.org/10.1109/TAFFC.2024.3524124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite growing interest in emotion cause analysis in conversations, existing research is limited by the small scale of available datasets, with the largest benchmarks containing only around 1,000 conversations. This inadequacy poses significant challenges for training effective models and conducting reliable evaluations. Moreover, traditional benchmarks' definition of emotion causes as singular, continuous spans fails to capture the complex nature of conversational emotions, where causes are often scattered across multiple utterances. To address these limitations, we construct the Emotion Cause of DailyDialog (ECDaily), a large-scale dataset containing 13,118 conversations and 102,970 utterances - ten times larger than existing ones. ECDaily uniquely incorporates both individual and aggregated cause span annotations. In addition to the Individual-ECE and Individual-ECPE tasks, we introduce two new tasks - Aggregated-ECE and Aggregated-ECPE - along with a two-stage approach for handling multiple-span causes. We establish five baseline systems using several pre-trained language models for both individual and aggregated tasks. Extensive experiments demonstrate the effectiveness of the baselines trained on ECDaily across multiple tasks, and indicate that ECDaily serves as a robust and comprehensive benchmark for advancing emotion cause analysis in conversations.},
  archive      = {J_TAFFC},
  author       = {Xiangqing Shen and Ke Li and Jiaming An and Zixiang Ding and Rui Xia},
  doi          = {10.1109/TAFFC.2024.3524124},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1570-1580},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {ECDaily: A large-scale benchmark for emotion cause extraction in conversations},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TriagedMSA: Triaging sentimental disagreement in multimodal sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 1557-1569. (<a href='https://doi.org/10.1109/TAFFC.2024.3524789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multimodal sentiment analysis models are effective at capturing sentiment commonalities across different modalities and discerning emotions. However, these models still face significant challenges when analyzing samples with sentiment polarity differences across modalities. Neural networks struggle to process such divergent sentiment samples, particularly when they are scarce within datasets. While larger datasets could help address this limitation, collecting and annotating them is resource-intensive. To overcome this challenge, we propose TriagedMSA, a multimodal sentiment analysis model with triage capability. Our model introduces the Sentiment Disagreement Triage Network, which identifies sentiment disagreement between modalities within a sample. This triage mechanism reduces mutual influence by learning to distinguish between samples of sentiment agreement and disagreement. To process these two sample types, we develop the Sentiment Selection Attention Network and the Sentiment Commonality Attention Network, both of which enhance modality interaction learning. Furthermore, we propose the Adaptive Polarity Detection (APD) algorithm, which ensures the generalizability of our model across different datasets, regardless of whether unimodal labels are available. The APD algorithm adaptively determines sentiment polarity disagreement or agreement between modalities. We conduct experiments on three multimodal sentiment analysis datasets: CMU-MOSI, CMU-MOSEI and CH-SIMS.v2. The results demonstrate that our proposed methodology outperforms existing state-of-the-art approaches.},
  archive      = {J_TAFFC},
  author       = {Yuanyi Luo and Wei Liu and Qiang Sun and Sirui Li and Jichunyang Li and Rui Wu and Xianglong Tang},
  doi          = {10.1109/TAFFC.2024.3524789},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1557-1569},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {TriagedMSA: Triaging sentimental disagreement in multimodal sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From EEG to eye movements: Cross-modal emotion recognition using constrained adversarial network with dual attention. <em>TAFFC</em>, <em>16</em>(3), 1543-1556. (<a href='https://doi.org/10.1109/TAFFC.2024.3524418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition is a fundamental part of affective computing, obtaining performance gain from multimodal methods. Electroencephalography (EEG) and eye movements are extensively used as they contain complementary information. However, the inconvenient acquisition of EEG is hindering the extensive adoption of multimodal emotion recognition in daily applications while eye movements are more convenient to collect but with lower performance. To tackle this issue, we propose a Constrained Adversarial Network with Dual Attention (CANDA), exploiting the complementary information from multiple modalities during training to improve the test-time performance of single easily acquired modality, i.e., transferring knowledge from a stronger modality to a weaker modality. During training, a common joint space is learned to diminish the distribution discrepancy among different modalities and incorporate the multimodal representations. During test, single modality is converted to the common space achieving comparable performance to multiple modalities. Extensive experiments demonstrate that our model achieves the state-of-the-art performance for cross-modal emotion recognition. Specifically, the mean accuracy increases around 15% on SEED, 15% on SEED-IV, and 2% on SEED-V compared to the latest baseline for emotion recognition. Visualization of features in the joint space illustrates that the distribution of different modalities aligns together with the discriminative ability regarding various emotions.},
  archive      = {J_TAFFC},
  author       = {Yiting Wang and Jia-Wen Liu and Bao-Liang Lu and Wei-Long Zheng},
  doi          = {10.1109/TAFFC.2024.3524418},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1543-1556},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {From EEG to eye movements: Cross-modal emotion recognition using constrained adversarial network with dual attention},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment triplet extraction with multi-view contrastive learning. <em>TAFFC</em>, <em>16</em>(3), 1526-1542. (<a href='https://doi.org/10.1109/TAFFC.2024.3521608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment Triplet Extraction (STE) is a challenging Aspect-based Sentiment Analysis task that involves identifying aspect terms, aspect categories, opinion terms, and their corresponding sentiment polarities in sentences. However, the complex relationships and implicit elements constituting sentiment triplets (aspect, opinion, polarity) or (aspect, category, polarity) pose a significant challenge. This paper proposes a novel model called Multi-view Contrastive Learning (MCL) for STE. We treat STE as a text generation task and employ Contrastive Learning at both the triplet and sentiment views. At the triplet view, the source text is used as an anchor, and the target text is regarded as positive samples, while negative samples are obtained by destroying triplet elements in the target text. At the sentiment view, aspect terms are concatenated with their corresponding opinion terms or categories, and the same sentiment polarity in the dataset is used as positive samples, while different polarities are considered negative samples. Our experimental results show that the proposed model outperforms the baseline GAS-EXTRACTION by a significant margin, with every improvement on F1 of 5.21 for Aspect Sentiment Triplet Extraction and 2.98 for Aspect Category Sentiment Detection. These results highlight the effectiveness of incorporating Contrastive Learning in the STE task.},
  archive      = {J_TAFFC},
  author       = {Wenfang Wu and Daling Wang and Ming Wang and Shi Feng and Yifei Zhang},
  doi          = {10.1109/TAFFC.2024.3521608},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1526-1542},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Sentiment triplet extraction with multi-view contrastive learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition with label-noisy under dual-branch noise extraction and suppression. <em>TAFFC</em>, <em>16</em>(3), 1514-1525. (<a href='https://doi.org/10.1109/TAFFC.2024.3519359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy labels in Facial Expression Recognition (FER) datasets severely affect the performance of FER models. We propose a novel dual-branch noise extraction and suppression method to address this issue. This algorithm reduces the model’s impact from noisy labels by decreasing the dataset noise ratio and suppressing label-noisy samples. The method comprises three primary stages: sample extraction, pseudo-label generation, and re-training. The approach initially extracts label-noisy samples from the dataset by computing an exponential moving average of the model predictions and the joint probability distribution matrix of noisy and actual labels. The remaining samples form a clean dataset. Next, the training weights of the clean dataset are utilized to assign appropriate pseudo-labels to the label-noisy samples. Subsequently, the noisy labels are replaced with pseudo-labels to create a corrected dataset. The corrected and clean datasets are combined to create the reconstructed dataset, reducing noisy labels within the dataset. Finally, the model is retrained using the reconstructed dataset. Furthermore, this study introduces a novel gradient suppression smoothing function specifically designed to mitigate the impact of label-noisy samples in the dataset during the re-training process. The proposed algorithm is robust, with accuracies of 91.17%, 91.56%, and 91.58% on the RAF-DB dataset with 10%, 20%, and 30% noisy labels, and accuracies of 89.91%, 90.17%, and 89.69% on the corresponding FERPlus.},
  archive      = {J_TAFFC},
  author       = {Yunfei Li and Hao Liu and Daihong Jiang and Jiuzhen Liang},
  doi          = {10.1109/TAFFC.2024.3519359},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1514-1525},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Facial expression recognition with label-noisy under dual-branch noise extraction and suppression},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Major depressive disorder detection using graph domain adaptation with global message-passing based on EEG signals. <em>TAFFC</em>, <em>16</em>(3), 1500-1513. (<a href='https://doi.org/10.1109/TAFFC.2024.3515457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) has been widely used for the detection of major depressive disorder (MDD). Currently, several methods have been proposed to process EEG signals for MDD detection using deep learning algorithms, and some advantages have been achieved. However, the extraction of EEG features for MDD detection remains challenging and most methods have difficulty in extracting common features of EEG signals across subjects. We propose a graph domain adaptation with global message-passing (GMP-GDA) for MDD detection. In addition, an adjacency matrix weighting algorithm is designed in the global message-passing module to learn the weight combinations of different adjacency matrices instead of feature transformation matrices to achieve cross-domain global message-passing between multi-source and target domains. Experimental results demonstrate that the proposed method achieves the superior detection result in each frequency band compared to the baseline systems. Meanwhile, ablation experiments demonstrate the effectiveness of our proposed method.},
  archive      = {J_TAFFC},
  author       = {Hui Wang and Jinghui Yin and Siyuan Gao and Ju Liu and Qiang Wu},
  doi          = {10.1109/TAFFC.2024.3515457},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1500-1513},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Major depressive disorder detection using graph domain adaptation with global message-passing based on EEG signals},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mouse-cursor tracking: Simple scoring algorithms that make it work. <em>TAFFC</em>, <em>16</em>(3), 1488-1499. (<a href='https://doi.org/10.1109/TAFFC.2024.3519257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mouse-cursor tracking, a new action-based measure of behavior, has emerged as one of the promising applications of affective computing. As facial expressions, gaits, electroencephalogram (EEG), and electrodermal activity (EDA) inform the emotions of computer users, the movement of the computer mouse-cursor reveals when people feel anxious, relaxed, attentive, joyful, and sad. However, the mouse tracking analysis has not previously been subject to systematic investigations of psychometric properties. The choice of motor features, experimental manipulations, and data transformation methods is ad hoc. In this study, we evaluate the impact of psychological factors on mouse-based affective computing and propose simple scoring algorithms that incorporate psychometric features such as the frame of reference, habituation, and measurement error. Our results demonstrate that our new dimensionality reduction method, merged PCA, outperforms conventional procedures, improving prediction performance by about $15-30\%$.},
  archive      = {J_TAFFC},
  author       = {Takashi Yamauchi and Shanle Longmire-Monford and Anton Leontyev and Kunxia Wang},
  doi          = {10.1109/TAFFC.2024.3519257},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1488-1499},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Mouse-cursor tracking: Simple scoring algorithms that make it work},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards cyberbullying detection: Building, benchmarking and longitudinal analysis of aggressiveness and Conflicts/Attacks datasets from twitter. <em>TAFFC</em>, <em>16</em>(3), 1473-1487. (<a href='https://doi.org/10.1109/TAFFC.2024.3518587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offense and hate speech are a source of online conflicts which have become common in social media and, as such, their study is a growing topic of research in machine learning and natural language processing. This article presents two Portuguese language offense-related datasets that deepen the study of the subject: an Aggressiveness dataset and a Conflicts/Attacks dataset. While the former is similar to other offense detection related datasets, the latter constitutes a novelty due to the use of the history of the interaction between users. Several studies were carried out to construct and analyze the data in the datasets. The first study included gathering expressions of verbal aggression witnessed by adolescents to guide data extraction for the datasets. The second study included extracting data from Twitter (in Portuguese) that matched the most frequent expressions/words/sentences that were identified in the previous study. The third study consisted in the development of the Aggressiveness dataset, the Conflicts/Attacks dataset, and classification models. In our fourth study, we proposed to examine whether online aggression and conflicts/attacks revealed any trend changes over time with a sample of 86 adolescents. With this study, we also proposed to investigate whether the amount of tweets sent over a period of 273 days was related to online aggression and conflicts/attacks. Finally, we analyzed the percentage of participants who participated in the aggressions and/or attacks/conflicts.},
  archive      = {J_TAFFC},
  author       = {Paula Ferreira and Nádia Pereira and Hugo Rosa and Sofia Oliveira and Luísa Coheur and Sofia Francisco and Sidclay Souza and Ricardo Ribeiro and João P. Carvalho and Paula Paulino and Isabel Trancoso and Ana Margarida Veiga-Simão},
  doi          = {10.1109/TAFFC.2024.3518587},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1473-1487},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Towards cyberbullying detection: Building, benchmarking and longitudinal analysis of aggressiveness and Conflicts/Attacks datasets from twitter},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedAR: Federated artificial resampling for imbalanced facial emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1461-1472. (<a href='https://doi.org/10.1109/TAFFC.2024.3516822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as an essential tool for computing devices to participate in collaborative training of deep learning models. However, due to the decentralized distribution of data over clients/local computing devices, the class imbalance problem has become evident, causing severe degradation in the performance of the global model. Motivated by the emergence of FL models in emotion recognition, the current study proposes an FL-based facial emotion recognition system by addressing local imbalance data problems encountered in client devices. First, the local imbalance problem is mitigated by utilizing the data-level artificial resampling method on the client side. To address the possibility of an adversarial attack using imbalanced data, the local training is equipped with a pre-training check to verify if the data being used is imbalanced above a predefined threshold of imbalance ratio. In case of high imbalance, a pre-training step will balance the data locally without sharing any information with other participants thereby ensuring privacy in the FL framework. Experiments have been conducted by using benchmark facial emotion recognition data with a balanced testing strategy. It indicated that considerable improvement can be achieved by the proposed FL-based facial emotion recognition model.},
  archive      = {J_TAFFC},
  author       = {Sankhadeep Chatterjee and Kushankur Ghosh and Saranya Bhattacharjee and Asit Kumar Das and Soumen Banerjee},
  doi          = {10.1109/TAFFC.2024.3516822},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1461-1472},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {FedAR: Federated artificial resampling for imbalanced facial emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EmoDialect: Leveraging fuzzy matching and dialect-emotion mapping for sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 1444-1460. (<a href='https://doi.org/10.1109/TAFFC.2024.3514862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment Analysis is a well-explored field in natural language processing, that relies on intricate textual features. However, recent models tend to overlook the influence of dialects, emotions, and their associations, leading to inaccurate classifications. This work presents EmoDialect, a novel fuzzy framework designed to enhance sentiment analysis by mapping dialect with emotions and hence, their coalition coined as EmoDialect. The introduced EmoDialect incorporates dialect-emotion associations in feature extraction and utilizes fuzzy matching for dialect identification. Further, it leverages tweaked term frequency-inverse document frequency and parts-of-speech tagged $\mathcal {N}-$grams to capture dialect-specific sentiment cues. This enhanced EmoDialect feature set enhances sentiment analysis by attuning to the unique linguistic and emotional characteristics of diverse English dialects. Tests conducted on diverse corpora spanning various domains demonstrate the remarkable superiority and consistency of EmoDialect in terms of weighted average F1-scores of 92%, 86.7%, and 93% in dialect, sentiment, and text classification respectively, overtaking its predecessors by a wide margin. Also, EmoDialect was extended to dialect translation, and the related examinations revealed the F1-score of 86.15% warranting its ability to aid cross-cultural communication.},
  archive      = {J_TAFFC},
  author       = {Cherukula Madhu and Sudhakar M.S.},
  doi          = {10.1109/TAFFC.2024.3514862},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1444-1460},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {EmoDialect: Leveraging fuzzy matching and dialect-emotion mapping for sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonverbal leadership in joint full-body improvisation. <em>TAFFC</em>, <em>16</em>(3), 1431-1443. (<a href='https://doi.org/10.1109/TAFFC.2024.3514933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate nonverbal leadership and address two research questions: 1) is it possible to perceive leadership from nonverbal cues in an unstructured joint full-body activity with no designated leader? 2) what are its nonverbal indicators? To address these questions, we propose eight cues of nonverbal leadership and conduct a two-step validation study on a novel dataset (video, MoCap) of dance improvisation. To explore various leadership strategies, we introduce constraints on how dancers communicate by manipulating their shared sensory channels. In the first stage, 27 persons carried out continuous annotation of leadership in the recorded videos; in the second stage, 92 persons watched 25 short segments indicating who the leader was and reported perceived leadership cues. The results indicate 1) a high consensus among observers regarding nonverbal leadership, but only for certain video segments, and 2) that five leadership cues were frequently observed in our dataset. In the final part, we explore the feasibility of automatically detecting nonverbal leadership using hand-crafted cues and standard machine learning techniques.},
  archive      = {J_TAFFC},
  author       = {Radoslaw Niewiadomski and Léa Chauvigné and Maurizio Mancini and Gualtiero Volpe and Antonio Camurri},
  doi          = {10.1109/TAFFC.2024.3514933},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1431-1443},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Nonverbal leadership in joint full-body improvisation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing EEG-based cross-subject emotion recognition via adaptive source joint domain adaptation. <em>TAFFC</em>, <em>16</em>(3), 1419-1430. (<a href='https://doi.org/10.1109/TAFFC.2024.3514635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG emotion recognition is crucial in both human-machine interaction and healthcare. However, recognizing emotions across different subjects remains challenging due to individual variability. While existing multi-source domain adaptation methods have been utilized for cross-subject EEG emotion decoding, they often struggle with irrelevant or weakly relevant source domains, leading to negative transfer. Additionally, variations within subdomains are often neglected in these studies. We propose a joint domain adaptation method, Adaptive Source Joint Domain Adaptation (ASJDA) to address these issues. ASJDA utilizes an unsupervised adaptive source selection strategy to select a subset of source domains by evaluating the Jensen-Shannon divergence between the source and target domains, choosing those most relevant to the target. Subsequently, it implements joint domain adaptation with these chosen sources at both the domain and category subdomain levels. Our proposed method outperforms existing state-of-the-art methods, achieving cross-subject accuracies of 96.81% in SEED, 89.69% in SEED-IV, and 69.31% in DEAP. This work significantly advances the state of the art in EEG emotion recognition by effectively addressing the challenges of cross-subject variability.},
  archive      = {J_TAFFC},
  author       = {Ke Liu and Xin Luo and Wenrui Zhu and Zhu Liang Yu and Hong Yu and Bin Xiao and Wei Wu},
  doi          = {10.1109/TAFFC.2024.3514635},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1419-1430},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Enhancing EEG-based cross-subject emotion recognition via adaptive source joint domain adaptation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition with vision transformer using fused shifted windows. <em>TAFFC</em>, <em>16</em>(3), 1406-1418. (<a href='https://doi.org/10.1109/TAFFC.2024.3511628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions contain massive affective information. Previous methods have focused on using diverse models based on CNN or Transformer to handle the facial expression recognition(FER) task. However, most of them treat the FER task as a general image classification task and neglect the impact of regions of interest (ROIs) for the performance of FER. To verify the influence of different ROIs, in this paper, we propose a vision Transformer based on Fused Shifted windows (FSwin), called FSwin Transformer. The semantic information of the face, obtained by ROIs, guides the FSwin Transformer to focus more on the key regions. The fused shifted windows enable the model to perform global semantic interactions, allowing it to concentrate on key regions without losing the topological structural information of the entire face. Additionally, learnable parameters are introduced to learn the feature weights expressed by each ROI, helping the model dynamically adjust the attention distribution. We have conducted controlled experiments to quantitatively verify the impact of ROIs. And the experimental results show that with the increase of the number of ROIs, the accuracy of FER is significantly improved, demonstrating that the key ROIs play an important role in feature extraction. The results on Jaffe, CK+, FER2013, AffectNet have reached 99.8%, 99.0%, 74.8%, and 68.9%, respectively, which all set new state-of-the-art. Extensive cross-dataset experiments also show that the FSwin Transformer has good generalization ability, proving that our proposed model has a beneficial effect on FER tasks.},
  archive      = {J_TAFFC},
  author       = {Xiao Sun and Rui Wang and Shaokai Chen and Meng Wang},
  doi          = {10.1109/TAFFC.2024.3511628},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1406-1418},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Facial expression recognition with vision transformer using fused shifted windows},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demographic-guided behavior patterns contrast for personality prediction. <em>TAFFC</em>, <em>16</em>(3), 1392-1405. (<a href='https://doi.org/10.1109/TAFFC.2024.3512206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, personality has been considered as a valuable personal factor being incorporated into the provision of personalized learning. Although some studies have endeavored to obtain learners’ personalities implicitly from their learning behaviors, they failed to achieve satisfactory prediction performance. On the one hand, most existing approaches ignore the imbalanced distribution of personality classes, which causes the personality classifiers to be biased toward the non-extreme personality class. On the other hand, the related methods normally focus on constructing statistical behavior features, while the sequence information of learning behaviors is ignored, but actually it can reflect learners’ behavior patterns more finely. In this paper, inspired by the human learning strategy in the face of small samples, we propose an effective Demographic-Guided Behavior Patterns Contrast (DGBPC) model to classify learners’ personalities through the demographic-guided contrast of learners’ coarse behavior patterns. Besides, we construct and publish the Personality and Learning Behavior Dataset (PLBD), which should be one of the largest public datasets regarding Big-Five personality and learning behavior sequence according to our knowledge. The experimental results on PLBD demonstrate that our DGBPC model could generate learner representations with higher discrimination and outperform the related methods in terms of balanced accuracy.},
  archive      = {J_TAFFC},
  author       = {Yu Ji and Wen Wu and Hui Lin and Wenxin Hu and Yi Hu and Liang Kang and Xi Chen and Liang He},
  doi          = {10.1109/TAFFC.2024.3512206},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1392-1405},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Demographic-guided behavior patterns contrast for personality prediction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A micro-expression recognition network based on attention mechanism and motion magnification. <em>TAFFC</em>, <em>16</em>(3), 1379-1391. (<a href='https://doi.org/10.1109/TAFFC.2024.3510302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions (MEs) are spontaneous facial movements that reveal an individual’s genuine emotions and play a crucial role in various domains, including lie detection, criminal analysis, mental health treatment, national security, and others. Micro-expression recognition is a highly complex aspect within the domain of affective computing, aimed at identifying subtle facial motions that are difficult for humans to discern accurately. To model the subtle facial muscle motions and the brief duration of MEs, we propose a robust micro-expression recognition (MER) network, named the attention mechanism-based motion magnification guided micro-expression recognition network (AM-MM-MER). This network consists of two primary components: the ST-MEMM network, which enhances subtle motions in micro-expression videos to reveal imperceptible facial muscle motions, and the AM-MER, which focuses on facial landmarks related to micro-expressions and incorporates novel landmark positions to extract the underlying relationships among these landmarks, thereby reducing interference from video magnification and irrelevant identity features. Extensive analysis on the CASME II and SAMM datasets demonstrates the high accuracy and effectiveness of the proposed network, achieving superior results compared to state-of-the-art methods. Ablation studies further illustrate the robustness of the proposed network.},
  archive      = {J_TAFFC},
  author       = {Falin Wu and Yu Xia and Boyi Ma and Tianyang Hu and Jingyao Yang and Haoxin Li and Di Huang},
  doi          = {10.1109/TAFFC.2024.3510302},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1379-1391},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A micro-expression recognition network based on attention mechanism and motion magnification},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGILE: Attribute-guided identity independent learning for facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 1362-1378. (<a href='https://doi.org/10.1109/TAFFC.2024.3508536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within computer vision, Facial Expression Recognition (FER) is a challenging task involving bottlenecks such as obtaining well-separated and compact expression embeddings that are invariant of identity. This study introduces AGILE (Attribute-Guided Identity-Independent Learning), an innovative approach to enhance FER by distilling identity information and promoting discriminative features. Initially, an adaptive $\beta$ Variational Autoencoder (VAE) is proposed from a fixed $\beta$-VAE architecture leveraging the theory of single-dimensional Kalman filter. This enhances disentangled feature learning without compromising the reconstruction quality. Now, to achieve the desired FER objective, we design a two-stage modular scheme built within the framework of adaptive $\beta$-VAE. In the first stage, an expression-driven identity modeling is proposed where an identity encoder is trained with a novel training loss to embed the most likely state corresponding to every subject in latent representation. In the next stage, keeping the identity encoder fixed, an expression encoder is trained with explicit guidance for the latent variables using an adversarial excitation and inhibition mechanism. This form of supervision enhances the transparency and interpretability of the expression space and helps to capture discriminative expression embeddings required for the downstream classification task. Experimental evaluations demonstrate that AGILE outperforms existing methods in identity and expression separability in the latent space and achieves superior performance over state-of-the-art methods on both lab-controlled and in-the-wild datasets, with recognition accuracies of 99.00% on CK+, 90.00% on Oulu-CASIA, 89.01% on MMI, 67.20% on Aff-Wild2, and 68.97% on AFEW.},
  archive      = {J_TAFFC},
  author       = {Mohd Aquib and Nishchal K. Verma and M. Jaleel Akhtar},
  doi          = {10.1109/TAFFC.2024.3508536},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1362-1378},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {AGILE: Attribute-guided identity independent learning for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SentireCache: Accelerate sentiment classification with saliency-based caching. <em>TAFFC</em>, <em>16</em>(3), 1349-1361. (<a href='https://doi.org/10.1109/TAFFC.2025.3578574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning methodologies have demonstrated exceptional efficacy in sentiment classification tasks. However, their extended inference times often impede practical deployment, particularly in resource-constrained environments. This paper addresses the challenge of reducing inference time by introducing a novel in-GPU caching approach, termed SentireCache, specifically designed for sentiment classification tasks. While traditional caching methods with the cosine similarity measurement have shown some reduction in inference time, they suffer from low hit rates and accuracy. To overcome this limitation, we incorporate a token filtering mechanism based on saliency into the caching system, along with simplified similarity calculation methods. The effectiveness of our proposed approach is theoretically analyzed. Moreover, extensive experimentation is conducted to compare SentireCache with other state-of-the-art caching methods. The results demonstrate a significant 37.7% reduction in inference time with an average performance degradation of 4.69%.},
  archive      = {J_TAFFC},
  author       = {Yilong Zhu and Juncheng Jia and Mianxiong Dong and Jun Qi},
  doi          = {10.1109/TAFFC.2025.3578574},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1349-1361},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SentireCache: Accelerate sentiment classification with saliency-based caching},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of experimental protocols: Towards a uniform framework in virtual reality affective research. <em>TAFFC</em>, <em>16</em>(3), 1334-1348. (<a href='https://doi.org/10.1109/TAFFC.2025.3554496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of affective computing with virtual reality (VR) often uses machine learning to analyze users’ emotional responses through physiological and behavioral signals, enabling personalized interactions within VR environments. However, current research in this field is characterized by inconsistent experimental protocols, which hinders comprehensive conclusions and cross-study comparisons. To address this gap, a systematic review was conducted following the PRISMA guidelines, identifying 24 studies that used physiological measures and machine learning to predict emotions in VR settings. The review covers five key areas: experimental protocols, VR environments, implicit measurements, emotion models, and machine learning approaches. In addition, it provides guidelines for standardizing data collection, biosignal processing, and emotion modeling. These proposed guidelines aim to establish consistent reporting practices and experimental protocols, thus improving the comparability and reproducibility of future VR affective computing research.},
  archive      = {J_TAFFC},
  author       = {Allison Bayro and Heejin Jeong},
  doi          = {10.1109/TAFFC.2025.3554496},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1334-1348},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A systematic review of experimental protocols: Towards a uniform framework in virtual reality affective research},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech-based depression assessment: A comprehensive survey. <em>TAFFC</em>, <em>16</em>(3), 1318-1333. (<a href='https://doi.org/10.1109/TAFFC.2024.3521327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression (major depressive disorder) is one of the most common mental illnesses worldwide, causing feelings of sadness and loss of interest, and is a leading cause of suicidal ideation. Limited access to mental health services, stigma, patient privacy and delay in seeking help are the most significant barriers to assessment and effective treatment. In order to enhance the accuracy of depression prediction, automated strategies employing computational models have been widely explored in literature. To this end, automatic Speech Depression Recognition (SDR) methods stand out, as speech comprises a valuable marker of mental health. Interestingly, recording speech comprises a less intrusive and more portable approach than capturing video, thus more easily accepted, especially by the younger generations, who are at a considerable risk of social isolation due to addiction to social networks and excessive use of mobile devices. In this context, this paper presents an up-to-date survey on SDR. More specifically, we a) detail the major challenges and key issues on SDR, b) summarise the most recent approaches existing in the related literature, and c) highlight the open problems. At the same time, we illustrate a framework encompassing the latest tendencies for SDR, along with a suitable comparison of the achieved performances. Finally, we highlight future trends and present the overall findings, providing researchers with best practices and techniques to address the major challenges of SDR, as well as stimulating discussion and improvement in the field.},
  archive      = {J_TAFFC},
  author       = {Samara Soares Leal and Stavros Ntalampiras and Roberto Sassi},
  doi          = {10.1109/TAFFC.2024.3521327},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1318-1333},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Speech-based depression assessment: A comprehensive survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A knowledge distillation-based approach to speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1307-1317. (<a href='https://doi.org/10.1109/TAFFC.2025.3574178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to rapid advancements in deep learning, Transformer-based architectures have proven effective in speech emotion recognition (SER), largely due to their ability to model long-term dependencies more effectively than recurrent networks. The current Transformer architecture is not well-suited for SER because its large parameter number demands significant computational resources, making it less feasible in environments with limited resources. Furthermore, its application to SER is limited because human emotions, which are expressed in long segments of continuous speech, are inherently complex and ambiguous. Therefore, designing specialized Transformer models tailored for SER is essential. To address these challenges, we propose a novel knowledge distillation framework that combines meta-knowledge and curriculum-based distillation. Specifically, we fine-tune the teacher model to optimize it for the SER task. For the student model, we embed individual sequence time points into variable tokens, which are used to aggregate the global speech representation. Additionally, we combine supervised contrastive and cross-entropy loss to increase the inter-class distance between learnable features. Finally, we optimize the student model using both meta-knowledge and the curriculum-based distillation framework. Experimental results on two benchmark datasets, IEMOCAP and MELD, demonstrate that our method performs competitively with state-of-the-art approaches in SER.},
  archive      = {J_TAFFC},
  author       = {Ziping Zhao and Jixin Liu and Haishuai Wang and Danushka Bandara and Jianhua Tao},
  doi          = {10.1109/TAFFC.2025.3574178},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1307-1317},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A knowledge distillation-based approach to speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ParaLBench: A large-scale benchmark for computational paralinguistics over acoustic foundation models. <em>TAFFC</em>, <em>16</em>(3), 1290-1306. (<a href='https://doi.org/10.1109/TAFFC.2024.3506554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational paralinguistics (ComParal) aims to develop algorithms and models to automatically detect, analyze, and interpret non-verbal information from speech communication, e. g., emotion, health state, age, and gender. Despite its rapid progress, it heavily depends on sophisticatedly designed models given specific paralinguistic tasks. Thus, the heterogeneity and diversity of ComParal models largely prevent the realistic implementation of ComParal models. Recently, with the advent of acoustic foundation models because of self-supervised learning, developing more generic models that can efficiently perceive a plethora of paralinguistic information has become an active topic in speech processing. However, it lacks a unified evaluation framework for a fair and consistent performance comparison. To bridge this gap, we conduct a large-scale benchmark, namely ParaLBench, which concentrates on standardizing the evaluation process of diverse paralinguistic tasks, including critical aspects of affective computing such as emotion recognition and emotion dimensions prediction, over different acoustic foundation models. This benchmark contains ten datasets with thirteen distinct paralinguistic tasks, covering short-, medium- and long-term characteristics. Each task is carried out on 14 acoustic foundation models under a unified evaluation framework, which allows for an unbiased methodological comparison and offers a grounded reference for the ComParal community. Based on the insights gained from ParaLBench, we also point out potential research directions, i. e., the cross-corpus generalizability, to propel ComParal research in the future. The code associated with this study will be available to foster the transparency and replicability of this work for succeeding researchers.},
  archive      = {J_TAFFC},
  author       = {Zixing Zhang and Weixiang Xu and Zhongren Dong and Kanglin Wang and Yimeng Wu and Jing Peng and Runming Wang and Dong-Yan Huang},
  doi          = {10.1109/TAFFC.2024.3506554},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1290-1306},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {ParaLBench: A large-scale benchmark for computational paralinguistics over acoustic foundation models},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive masking oriented self-taught learning for occluded facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 1277-1289. (<a href='https://doi.org/10.1109/TAFFC.2025.3544677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-taught learning (STL) is a promising solution that reduces the performance gap between weakly supervised and fully supervised learning for easily accessible, label-free images. The success of traditional STL solutions relies on the assumption that the target appearance is completely visible and well-defined. In real-world facial expression recognition scenarios, however, saliency regions are often partially occluded, which significantly hampers the generalization capability of STL methods. Nevertheless, few studies have investigated the impact of occlusion on STL. In this paper, we propose an interweaved autoencoder network for weakly supervised facial expression recognition in occlusion scenarios. The key innovation of our network lies in the Residual Connection Union (RCU) blocks that can integrate the Convolutional Neural Network (CNN) and Transformer layers into a multi-scale structure. The RCU enables a progressive masking strategy to accurately identify and focus on contributive yet often overlooked image patches by analyzing the relationships among region-level target representations. In addition, we introduce a self-knowledge distillation module for the effective training of the proposed autoencoder network. Extensive experiments are conducted on four public datasets to demonstrate the superiority of our method over related works.},
  archive      = {J_TAFFC},
  author       = {Bin Kang and Shuangshuang Wang and Zongyu Wang and Xin Li and Haie Dou and Lei Wang and Zhijie Xia},
  doi          = {10.1109/TAFFC.2025.3544677},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1277-1289},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Progressive masking oriented self-taught learning for occluded facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connecting cross-modal representations for compact and robust multimodal sentiment analysis with sentiment word substitution error. <em>TAFFC</em>, <em>16</em>(3), 1265-1276. (<a href='https://doi.org/10.1109/TAFFC.2024.3490694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Sentiment Analysis (MSA) seeks to fuse textual, acoustic, and visual information to predict a speaker’s sentiment states effectively. However, in real-world scenarios, the text modality received by MSA systems is often obtained through automatic speech recognition (ASR) models. Unfortunately, ASR may erroneously recognize sentiment words as phonetically similar neutral alternatives, leading to sentiment degradation in text and impacting MSA accuracy. Recent attempts aim to first identify the sentiment word substitution (SWS) error in ASR results and then refine the corrupted word embeddings using multimodal information for final multimodal fusion. However, such a method includes a burdensome and ambiguous detection operation and ignores the inherent correlations and heterogeneity among different modalities. To address these issues, we propose a more compact system, termed ARF-MSA consisting of three key components to achieving robust MSA with SWS errors: 1) Alignment: we establish connections between the “text-acoustic’ and “text-visual” representations to effectively map the “text-acoustic-visual” data into a unified sentiment space by leveraging their multimodal correlation knowledge; 2) Refinement: we perform fine-grained comparisons between the text modality and the other two modalities in the unified sentiment space, enabling refinement of the sentiment expression within the text modality more concisely; 3) Fusion: Finally, we hierarchically fuse the dominant and non-dominant representation from three heterogeneity modalities to obtain the multimodal feature for MSA. We conduct extensive experiments on the real-world datasets and the results demonstrate the effectiveness of our model.},
  archive      = {J_TAFFC},
  author       = {Qiyuan Sun and Haolin Zuo and Rui Liu and Haizhou Li},
  doi          = {10.1109/TAFFC.2024.3490694},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1265-1276},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Connecting cross-modal representations for compact and robust multimodal sentiment analysis with sentiment word substitution error},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial extremely low-resource autonomous affective learning. <em>TAFFC</em>, <em>16</em>(3), 1261-1264. (<a href='https://doi.org/10.1109/TAFFC.2025.3589682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAFFC},
  author       = {Xinzhou Xu and Björn W. Schuller and Elisabeth André and Erik Cambria},
  doi          = {10.1109/TAFFC.2025.3589682},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1261-1264},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Guest editorial extremely low-resource autonomous affective learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
