<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOG</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tog">TOG - 132</h2>
<ul>
<li><details>
<summary>
(2025). Correct your balance heuristic: Optimizing balance-style multiple importance sampling weights. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3730819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple importance sampling (MIS) is a vital component of most rendering algorithms. MIS computes a weighted sum of samples from many different techniques to achieve generalization, that is, to handle a wide range of scene types and lighting effects. A key factor to the performance of MIS is the choice of weighting function. The go-to default - the balance heuristic - performs well in many cases, but prior work has shown that it can yield unsatisfactory results. A number of challenges cause this suboptimal performance, including low-variance techniques, sample correlation, and unknown sampling densities. Prior work has suggested improvements for some of these problems, but a general optimal solution has yet to be found. We propose a general and practical weight correction scheme: We optimize, on-the-fly, a set of correction factors that are multiplied into any baseline MIS heuristic (e.g., balance or power). We demonstrate that this approach yields consistently better equal-time performance on two rendering applications: bidirectional algorithms and resampled importance sampling for direct illumination.},
  archive      = {J_TOG},
  author       = {Qingqin Hua and Pascal Grittmann and Philipp Slusallek},
  doi          = {10.1145/3730819},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Correct your balance heuristic: Optimizing balance-style multiple importance sampling weights},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sobol' sequences with guaranteed-quality 2D projections. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3730821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-discrepancy sequences, and more particularly Sobol' sequences are gold standard for drawing highly uniform samples for quasi-Monte Carlo applications. They produce so-called ( t,s )-sequences, that is, sequences of s -dimensional samples whose uniformity is controlled by a non-negative integer quality factor t. The Monte Carlo integral estimator has a convergence rate that improves as t decreases. Sobol' construction in base 2 also provides extremely fast sampling point generation using efficient xor-based arithmetic. Computer graphics applications, such as rendering, often require high uniformity in consecutive 2D projections and in higher-dimensional projections at the same time. However, it can be shown that, in the classical Sobol' construction, only a single 2D sequence of points (up to scrambling), constructed using irreducible polynomials x and x + 1, achieves the ideal t = 0 property. Reusing this sequence in projections necessarily loses high dimensional uniformity. We prove the existence and construct many 2D Sobol' sequences having t = 1 using irreducible polynomials p and p 2 + p + 1. They can be readily combined to produce higher-dimensional low discrepancy sequences with a high-quality t = 1, guaranteed in consecutive pairs of dimensions. We provide the initialization table that can be directly used with any existing Sobol' implementation, along with the corresponding generator matrices, for an optimized 692-dimensional Sobol' construction. In addition to guaranteeing the (1, 2)-sequence property for all consecutive pairs, we ensure that t ≤ 4 for consecutive 4D projections up to 2 15 points.},
  archive      = {J_TOG},
  author       = {Nicolas Bonneel and David Coeurjolly and Jean-Claude Iehl and Victor Ostromoukhov},
  doi          = {10.1145/3730821},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sobol' sequences with guaranteed-quality 2D projections},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving global motion estimation in sparse IMU-based motion capture with physics. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3730822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By learning human motion priors, motion capture can be achieved by 6 inertial measurement units (IMUs) in recent years with the development of deep learning techniques, even though the sensor inputs are sparse and noisy. However, human global motions are still challenging to be reconstructed by IMUs. This paper aims to solve this problem by involving physics. It proposes a physical optimization scheme based on multiple contacts to enable physically plausible translation estimation in the full 3D space where the z-directional motion is usually challenging for previous works. It also considers gravity in local pose estimation which well constrains human global orientations and refines local pose estimation in a joint estimation manner. Experiments demonstrate that our method achieves more accurate motion capture for both local poses and global motions. Furthermore, by deeply integrating physics, we can also estimate 3D contact, contact forces, joint torques, and interacting proxy surfaces. Code is available at https://xinyu-yi.github.io/GlobalPose/.},
  archive      = {J_TOG},
  author       = {Xinyu Yi and Shaohua Pan and Feng Xu},
  doi          = {10.1145/3730822},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Improving global motion estimation in sparse IMU-based motion capture with physics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to assemble with alternative plans. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3730824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a reinforcement learning framework for constructing assemblies composed of rigid parts, which are commonly seen in many historical masonry buildings and bridges. Traditional construction methods for such structures often depend on dense scaffolding to stabilize their intermediate assembly steps, making the process both labor-intensive and time-consuming. This work utilizes multiple robots to collaboratively assemble structures, offering temporary support by holding parts in place without additional scaffolding. Precomputing the robotic assembly process to ensure structural stability involves a time-consuming offline process due to the combinatorial nature of its search space. However, the precomputed assembly plans may get disrupted during real-world execution due to unforeseen changes, such as setup modifications or delays in part delivery. Recomputing these plans using traditional offline methods results in significant project delays. Therefore, we propose a reinforcement learning-based approach in which a neural network is trained to efficiently generate alternative assembly plans for a given structure online, enabling adaptation to external changes. To enable effective and efficient training, we introduce three key innovations: a GPU-based stability simulator for parallelizing simulations, a novel curriculum-based training scheme to address sparse rewards during training, and a new graph neural network architecture for efficiently encoding assembly geometry. We validate our approach by training reinforcement learning agents on various assemblies and evaluating their performance on unseen assembly tasks. Furthermore, we demonstrate the effectiveness of our framework in planning multi-robot assembly processes, effectively handling disruptions in both simulation and physical environments.},
  archive      = {J_TOG},
  author       = {Ziqi Wang and Wenjun Liu and Jingwen Wang and Gabriel Vallat and Fan Shi and Stefana Parascho and Maryam Kamgarpour},
  doi          = {10.1145/3730824},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning to assemble with alternative plans},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial appearance capture at home with patch-level reflectance prior. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3730825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing facial appearance capture methods can reconstruct plausible facial reflectance from smartphone-recorded videos. However, the reconstruction quality is still far behind the ones based on studio recordings. This paper fills the gap by developing a novel daily-used solution with a co-located smartphone and flashlight video capture setting in a dim room. To enhance the quality, our key observation is to solve facial reflectance maps within the data distribution of studio-scanned ones. Specifically, we first learn a diffusion prior over the Light Stage scans and then steer it to produce the reflectance map that best matches the captured images. We propose to train the diffusion prior at the patch level to improve generalization ability and training stability, as current Light Stage datasets are in ultra-high resolution but limited in data size. Tailored to this prior, we propose a patch-level posterior sampling technique to sample seamless full-resolution reflectance maps from this patch-level diffusion model. Experiments demonstrate our method closes the quality gap between low-cost and studio recordings by a large margin, opening the door for everyday users to clone themselves to the digital world.},
  archive      = {J_TOG},
  author       = {Yuxuan Han and Junfeng Lyu and Kuan Sheng and Minghao Que and Qixuan Zhang and Lan Xu and Feng Xu},
  doi          = {10.1145/3730825},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Facial appearance capture at home with patch-level reflectance prior},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast subspace fluid simulation with a temporally-aware basis. <em>TOG</em>, <em>44</em>(4), 1-18. (<a href='https://doi.org/10.1145/3730826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel reduced-order fluid simulation technique leveraging Dynamic Mode Decomposition (DMD) to achieve fast, memory-efficient, and user-controllable subspace simulation. We demonstrate that our approach combines the strengths of both spatial reduced order models (ROMs) as well as spectral decompositions. By optimizing for the operator that evolves a system state from one timestep to the next, rather than the system state itself, we gain both the compressive power of spatial ROMs as well as the intuitive physical dynamics of spectral methods. The latter property is of particular interest in graphics applications, where user control of fluid phenomena is of high demand. We demonstrate this in various applications including spatial and temporal modulation tools and fluid upscaling with added turbulence. We adapt DMD for graphics applications by reducing computational overhead, incorporating user-defined force inputs, and optimizing memory usage with randomized SVD. The integration of OptDMD and DMD with Control (DMDc) facilitates noise-robust reconstruction and real-time user interaction. We demonstrate the technique's robustness across diverse simulation scenarios, including artistic editing, time-reversal, and super-resolution. Through experimental validation on challenging scenarios, such as colliding vortex rings and boundary-interacting plumes, our method also exhibits superior performance and fidelity with significantly fewer basis functions compared to existing spatial ROMs. Leveraging the inherent linearity of the DMD formulation, we demonstrate a range of diverse applications. This work establishes another avenue for developing real-time, high-quality fluid simulations, enriching the space of fluid simulation techniques in interactive graphics and animation.},
  archive      = {J_TOG},
  author       = {Siyuan Chen and Yixin Chen and Jonathan Panuelos and Otman Benchekroun and Yue Chang and Eitan Grinspun and Zhecheng Wang},
  doi          = {10.1145/3730826},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast subspace fluid simulation with a temporally-aware basis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fully-statistical wave scattering model for heterogeneous surfaces. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3730828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous surfaces exhibit spatially varying geometry and material, and therefore admit diverse appearances. Existing computer graphics works can only model heterogeneity using explicit structures or statistical parameters that describe a coarser level of detail. We extend the boundary by introducing a new model that describes the heterogeneous surfaces fully statistically at the microscopic level, with rich geometry and material details that are comparable to the wavelengths of light. We treat the heterogeneous surfaces as a mixture of stochastic vector processes. We adapt the well-known generalized Harvey-Shack theory to quantify the mean scattered intensity, i.e., the BRDF of these surfaces. We further explore the covariance statistic of the scattered field and derive its rank-1 decomposition. This leads to a practical algorithm that samples the speckles (fluctuating intensities) from the statistics, enriching the appearance without explicit definition of heterogeneous surfaces. The formulations are analytic, and we validate the quantities by comprehensive numerical simulations. Our heterogeneous surface model demonstrates various applications including corrosion (natural), particle deposition (man-made), and height-correlated mixture (artistic). Code for this paper is available at https://github.com/Rendering-at-ZJU/HeteroSurface.},
  archive      = {J_TOG},
  author       = {Zhengze Liu and Yuchi Huo and Yifan Peng and Rui Wang},
  doi          = {10.1145/3730828},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A fully-statistical wave scattering model for heterogeneous surfaces},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid near-wall model for kinetic simulation of turbulent boundary layer flows. <em>TOG</em>, <em>44</em>(4), 1-24. (<a href='https://doi.org/10.1145/3730829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Turbulent boundary layer represents one of the most complex but interesting phenomena in fluid flows. While the generation and alteration of sheared vortices in various interacting scales near the boundary are visually appealing, it is difficult to correctly replicate such phenomena by simulation, especially at high Reynolds numbers. Practical methodologies typically incorporate empirical wall modeling to substantially curtail the computational expenses while retaining physical consistency. Nevertheless, these are predominantly applicable to steady-state flow solvers. While complex scenarios involving dynamic fluid-solid interaction and its application to create time-dependent flow phenomena invariably necessitate unsteady flow solvers, the underlying wall modeling techniques are imprecise, leading to a different formation of near-wall vortices, especially for the highly efficient lattice Boltzmann solver operating on Cartesian grids. In this paper, we propose a novel hybrid near-wall model for the lattice Boltzmann solver, which can handle turbulent boundary layer flows in a simple and efficient manner, inspired by measuring the degree of boundary layer separation. Our model comprises both macroscopic and mesoscopic algebraic models, which collaborate to let the low dissipation lattice Boltzmann solver naturally form the turbulent boundary layer appropriately. By leveraging the multi-resolution technique, accurate simulation outcomes can be obtained. Our model is parameterized to approximate different physical attributes of the solid surface that can potentially influence the boundary layer distribution, and comparable boundary layer flow behaviors can be simulated at various grid resolutions. Rigorous benchmark tests are carried out to validate our model at different grid resolutions by comparing with experimental data and visualizations. We showcase the applications of our new model in both facilitating computational design and generating visual animations, accompanied by specific examples and comparisons with actual experimental setups and photographic images. All demonstrations affirm the physical consistency of our solver even when simulated with a relatively coarse grid resolution.},
  archive      = {J_TOG},
  author       = {Mengyun Liu and Kai Bai and Xiaopei Liu},
  doi          = {10.1145/3730829},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Graph.},
  title        = {A hybrid near-wall model for kinetic simulation of turbulent boundary layer flows},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast but accurate: A real-time hyperelastic simulator with robust frictional contact. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3730834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a GPU-friendly framework for real-time implicit simulation of elastic material in the presence of frictional contacts. The integration of hyperelasticity, non-interpenetration contact, and friction in real-time simulations presents formidable nonlinear and non-smooth problems, which are highly challenging to solve. By incorporating nonlinear complementarity conditions within the local-global framework, we achieve rapid convergence in addressing these challenges. While the structure of local-global methods is not fully GPU-friendly, our proposal of a simple yet efficient solver with sparse presentation of the system inverse enables highly parallel computing while maintaining a fast convergence rate. Moreover, our novel splitting strategy for non-smooth indicators not only amplifies overall performance but also refines the complementarity preconditioner, enhancing the accuracy of frictional behavior modeling. Through extensive experimentation, the robustness of our framework in managing real-time contact scenarios, ranging from large-scale systems and extreme deformations to non-smooth contacts and precise friction interactions, has been validated. Compatible with a wide range of hyperelastic models, our approach maintains efficiency across both low and high stiffness materials. Despite its remarkable efficiency, robustness, and generality, our method is elegantly simple, with its core contributions grounded solely on standard matrix operations.},
  archive      = {J_TOG},
  author       = {Ziqiu Zeng and Siyuan Luo and Fan Shi and Zhongkai Zhang},
  doi          = {10.1145/3730834},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast but accurate: A real-time hyperelastic simulator with robust frictional contact},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hand-shadow poser. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3730836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand shadow art is a captivating art form, creatively using hand shadows to reproduce expressive shapes on the wall. In this work, we study an inverse problem: given a target shape, find the poses of left and right hands that together best produce a shadow resembling the input. This problem is nontrivial, since the design space of 3D hand poses is huge while being restrictive due to anatomical constraints. Also, we need to attend to the input's shape and crucial features, though the input is colorless and textureless. To meet these challenges, we design Hand-Shadow Poser, a three-stage pipeline, to decouple the anatomical constraints (by hand) and semantic constraints (by shadow shape): (i) a generative hand assignment module to explore diverse but reasonable left/right-hand shape hypotheses; (ii) a generalized hand-shadow alignment module to infer coarse hand poses with a similarity-driven strategy for selecting hypotheses; and (iii) a shadow-feature-aware refinement module to optimize the hand poses for physical plausibility and shadow feature preservation. Further, we design our pipeline to be trainable on generic public hand data, thus avoiding the need for any specialized training dataset. For method validation, we build a benchmark of 210 diverse shadow shapes of varying complexity and a comprehensive set of metrics, including a novel DINOv2-based evaluation metric. Through extensive comparisons with multiple baselines and user studies, our approach is demonstrated to effectively generate bimanual hand poses for a large variety of hand shapes for over 85% of the benchmark cases.},
  archive      = {J_TOG},
  author       = {Hao Xu and Yinqiao Wang and Niloy J. Mitra and Shuaicheng Liu and Pheng-Ann Heng and Chi-Wing Fu},
  doi          = {10.1145/3730836},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Hand-shadow poser},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BANG: Dividing 3D assets via generative exploded dynamics. <em>TOG</em>, <em>44</em>(4), 1-21. (<a href='https://doi.org/10.1145/3730840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is "Generative Exploded Dynamics", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence. BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows. The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition.},
  archive      = {J_TOG},
  author       = {Longwen Zhang and Qixuan Zhang and Haoran Jiang and Yinuo Bai and Wei Yang and Lan Xu and Jingyi Yu},
  doi          = {10.1145/3730840},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {BANG: Dividing 3D assets via generative exploded dynamics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAST: Component-aligned 3D scene reconstruction from an RGB image. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3730841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relations. This enables understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using Masked Auto Encoder (MAE) and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST applies a physics-aware correction mechanism, which leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. Experimental results demonstrate that CAST significantly improves the quality of single-image 3D scene reconstruction, offering enhanced realism and accuracy in scene understanding and reconstruction tasks. CAST has practical applications in virtual content creation, such as immersive game environments and film production, where real-world setups can be seamlessly integrated into virtual landscapes. Additionally, CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.},
  archive      = {J_TOG},
  author       = {Kaixin Yao and Longwen Zhang and Xinhao Yan and Yan Zeng and Qixuan Zhang and Lan Xu and Wei Yang and Jiayuan Gu and Jingyi Yu},
  doi          = {10.1145/3730841},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {CAST: Component-aligned 3D scene reconstruction from an RGB image},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HoLa: B-rep generation using a holistic latent representation. <em>TOG</em>, <em>44</em>(4), 1-25. (<a href='https://doi.org/10.1145/3730842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel representation for learning and generating Computer-Aided Design (CAD) models in the form of boundary representations (B-Reps). Our representation unifies the continuous geometric properties of B-Rep primitives in different orders (e.g., surfaces and curves) and their discrete topological relations in a holistic latent (HoLa) space. This is based on the simple observation that the topological connection between two surfaces is intrinsically tied to the geometry of their intersecting curve. Such a prior allows us to reformulate topology learning in B-Reps as a geometric reconstruction problem in Euclidean space. Specifically, we eliminate the presence of curves, vertices, and all the topological connections in the latent space by learning to distinguish and derive curve geometries from a pair of surface primitives via a neural intersection network. To this end, our holistic latent space is only defined on surfaces but encodes a full B-Rep model, including the geometry of surfaces, curves, vertices, and their topological relations. Our compact and holistic latent space facilitates the design of a first diffusion-based generator to take on a large variety of inputs including point clouds, single/multi-view images, 2D sketches, and text prompts. Our method significantly reduces ambiguities, redundancies, and incoherences among the generated B-Rep primitives, as well as training complexities inherent in prior multi-step B-Rep learning pipelines, while achieving greatly improved validity rate over current state of the art: 82% vs. ≈50%.},
  archive      = {J_TOG},
  author       = {Yilin Liu and Duoteng Xu and Xingyao Yu and Xiang Xu and Daniel Cohen-Or and Hao Zhang and Hui Huang},
  doi          = {10.1145/3730842},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. Graph.},
  title        = {HoLa: B-rep generation using a holistic latent representation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TokenVerse: Versatile multi-concept personalization in token modulation space. <em>TOG</em>, <em>44</em>(4), 1-11. (<a href='https://doi.org/10.1145/3730843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present TokenVerse - a method for multi-concept personalization, leveraging a pre-trained text-to-image diffusion model. Our framework can disentangle complex visual elements and attributes from as little as a single image, while enabling seamless plug-and-play generation of combinations of concepts extracted from multiple images. As opposed to existing works, TokenVerse can handle multiple images with multiple concepts each, and supports a wide-range of concepts, including objects, accessories, materials, pose, and lighting. Our work exploits a DiT-based text-to-image model, in which the input text affects the generation through both attention and modulation (shift and scale). We observe that the modulation space is semantic and enables localized control over complex concepts. Building on this insight, we devise an optimization-based framework that takes as input an image and a text description, and finds for each word a distinct direction in the modulation space. These directions can then be used to generate new images that combine the learned concepts in a desired configuration. We demonstrate the effectiveness of TokenVerse in challenging personalization settings, and showcase its advantages over existing methods.},
  archive      = {J_TOG},
  author       = {Daniel Garibi and Shahar Yadin and Roni Paiss and Omer Tov and Shiran Zada and Ariel Ephrat and Tomer Michaeli and Inbar Mosseri and Tali Dekel},
  doi          = {10.1145/3730843},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {TokenVerse: Versatile multi-concept personalization in token modulation space},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sphere carving: Bounding volumes for signed distance fields. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3730845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Sphere Carving , a novel method for automatically computing bounding volumes that closely bound a procedurally defined implicit surface. Starting from an initial bounding volume located far from the object, we iteratively approach the surface by leveraging the signed distance function information. Field function queries define a set of empty spheres, from which we extract intersection points that are used to compute a bounding volume. Our method is agnostic of the function representation and only requires a conservative signed distance field as input. This encompasses a large set of procedurally defined implicit surface models such as exact or Lipschitz functions, BlobTrees, or even neural representations. Sphere Carving is conceptually simple, independent of the function representation, requires a small number of function queries to create bounding volumes, and accelerates queries in Sphere Tracing and polygonization.},
  archive      = {J_TOG},
  author       = {Hugo Schott and Theo Thonat and Thibaud Lambert and Eric Guérin and Eric Galin and Axel Paris},
  doi          = {10.1145/3730845},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sphere carving: Bounding volumes for signed distance fields},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segment-based light transport simulation. <em>TOG</em>, <em>44</em>(4), 1-10. (<a href='https://doi.org/10.1145/3730847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel segment-based light transport framework that uses segments as the basic unit of light transport. Unlike vertex-based formulations, our segment-based formulation naturally accommodates the disconnected subpaths encountered in photon density estimation and path filtering methods, and opens the door to a wide range of new rendering methods that consider segments as a sampling primitive. To facilitate the development of segment-based rendering methods, we introduce several segment sampling techniques and estimation strategies, including a highly-performant recursive estimator. One of our key contributions is a general-purpose segment sampling framework based on marginal multiple importance sampling (MMIS). To demonstrate the practicality of our sampling framework, we show how it allows us to easily implement a robust bidirectional path filtering method — challenging under a vertex-based formulation — achieving superior filtering efficiency and convergence compared to state-of-the-art approaches.},
  archive      = {J_TOG},
  author       = {Wenyou Wang and Rex West and Toshiya Hachisuka},
  doi          = {10.1145/3730847},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Segment-based light transport simulation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Creating fluid-interactive virtual agents by an efficient simulator with local-domain control. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3730848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of digital twin systems, establishing simulation environments for creating and testing virtual agents has garnered substantial attention across various applications. The obtained control policies endow virtual agents with more realistic behaviors and interactive capabilities, finding applications in both computer animation and robotic control. While rigid-body simulators are widely used for virtual agents, achieving similar feats in fluid environments presents formidable challenges due to high complexity and exorbitant costs. One major reason is that most fluid simulators feature a fixed domain, which struggles to enable agents to freely navigate in an unbounded, obstacle-filled space, especially when computational resources are limited, thus restricting their wide utility for creating virtual agents. In this paper, we introduce a novel fluid-solid interaction simulator grounded in an efficient lattice Boltzmann solver. A key feature of this simulator is a dynamically moving local domain that encircles the agent, offering greater flexibility for obtaining control policy while maintaining efficiency in simulation. Previous methods, which anchored a square moving local domain along with the agent, suffered from severe spurious flows when the agent underwent rapid acceleration especially when the domain had to rotate, such as during a U-turn. This led to inaccurate results and instability. Conversely, we propose a novel domain-tracking method that harnesses optimal control techniques to address this issue. Our approach not only bolsters local-domain simulation stability, but also improves efficiency by employing a slender domain, which broadens the application scope of direct fluid-solid interactions for virtual agents. We validate our method by comparing simulations to physical phenomena and obtaining control policies for various virtual agents to accomplish challenging tasks. This effort culminates in a series of animations that vividly demonstrate the efficacy of the entire framework potentially used in both computer animation and robotics.},
  archive      = {J_TOG},
  author       = {Wenbin Song and Heng Zhang and Yang Wang and Xiaopei Liu},
  doi          = {10.1145/3730848},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Creating fluid-interactive virtual agents by an efficient simulator with local-domain control},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TetWeave: Isosurface extraction using on-the-fly delaunay tetrahedral grids for gradient-based mesh optimization. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3730851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce TetWeave, a novel isosurface representation for gradient-based mesh optimization that jointly optimizes the placement of a tetrahedral grid used for Marching Tetrahedra and a novel directional signed distance at each point. TetWeave constructs tetrahedral grids on-the-fly via Delaunay triangulation, enabling increased flexibility compared to predefined grids. The extracted meshes are guaranteed to be watertight, two-manifold and intersection-free. The flexibility of TetWeave enables a resampling strategy that places new points where reconstruction error is high and allows to encourage mesh fairness without compromising on reconstruction error. This leads to high-quality, adaptive meshes that require minimal memory usage and few parameters to optimize. Consequently, TetWeave exhibits near-linear memory scaling relative to the vertex count of the output mesh — a substantial improvement over predefined grids. We demonstrate the applicability of TetWeave to a broad range of challenging tasks in computer graphics and vision, such as multi-view 3D reconstruction, mesh compression and geometric texture generation. Our code is available at https://github.com/AlexandreBinninger/TetWeave.},
  archive      = {J_TOG},
  author       = {Alexandre Binninger and Ruben Wiersma and Philipp Herholz and Olga Sorkine-Hornung},
  doi          = {10.1145/3730851},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {TetWeave: Isosurface extraction using on-the-fly delaunay tetrahedral grids for gradient-based mesh optimization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Claycode: Stylable and deformable 2D scannable codes. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3730853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Claycode, a novel 2D scannable code designed for extensive stylization and deformation. Unlike traditional matrix-based codes (e.g. , QR codes), Claycodes encode their message in a tree structure. During the encoding process, bits are mapped into a topology tree, which is then depicted as a nesting of color regions drawn within the boundaries of a target polygon shape. When decoding, Claycodes are extracted and interpreted in real-time from a camera stream. We detail the end-to-end pipeline and show that Claycodes allow for extensive stylization without compromising their functionality. We then empirically demonstrate Claycode's high tolerance to heavy deformations, outperforming traditional 2D scannable codes in scenarios where they typically fail.},
  archive      = {J_TOG},
  author       = {Marco Maida and Alberto Crescini and Marco Perronet and Elena Camuffo},
  doi          = {10.1145/3730853},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Claycode: Stylable and deformable 2D scannable codes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive phase-field-FLIP for very large scale two-phase fluid simulation. <em>TOG</em>, <em>44</em>(4), 1-23. (<a href='https://doi.org/10.1145/3730854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing the visually compelling features of large-scale water phenomena, such as the spray clouds of crashing waves, stormy seas, or waterfalls, involves simulating not only the water but also the motion of the air interacting with it. However, current solutions in the visual effects industry still largely rely on single-phase solvers and non-physical "white-water" heuristics. To address these limitations, we present Phase-Field-FLIP (PF-FLIP), a hybrid Eulerian/Lagrangian method for the fully physics-based simulation of very large-scale, highly turbulent multiphase flows at high Reynolds numbers and high fluid density contrasts. PF-FLIP transports mass and momentum in a consistent, non-dissipative manner and, unlike most existing multiphase approaches, does not require a surface reconstruction step. Furthermore, we employ spatial adaptivity across all critical components of the simulation algorithm, including the pressure Poisson solver. We augment PF-FLIP with a dual multiresolution scheme that couples an efficient treeless adaptive grid with adaptive particles, along with a fast adaptive Poisson solver tailored for high-density-contrast multiphase flows. Our method enables the simulation of two-phase flow scenarios with a level of physical realism and detail previously unattainable in graphics, supporting billions of particles and adaptive 3D resolutions with thousands of grid cells per dimension on a single workstation.},
  archive      = {J_TOG},
  author       = {Bernhard Braun and Jan Bender and Nils Thuerey},
  doi          = {10.1145/3730854},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. Graph.},
  title        = {Adaptive phase-field-FLIP for very large scale two-phase fluid simulation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical inverse rendering of textured and translucent appearance. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3730855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse rendering has emerged as a standard tool to reconstruct the parameters of appearance models from images (e.g., textured BSDFs). In this work, we present several novel contributions motivated by the practical challenges of recovering high-resolution surface appearance textures, including spatially-varying subsurface scattering parameters. First, we propose Laplacian mipmapping , which combines differentiable mipmapping and a Laplacian pyramid representation into an effective preconditioner. This seemingly simple technique significantly improves the quality of recovered surface textures on a set of challenging inverse rendering problems. Our method automatically adapts to the render and texture resolutions, only incurs moderate computational cost and achieves better quality than prior work while using fewer hyperparameters. Second, we introduce a specialized gradient computation algorithm for textured, path-traced subsurface scattering, which facilitates faithful reconstruction of translucent materials. By using path tracing, we enable the recovery of complex appearance while avoiding the approximations of the previously used diffusion dipole methods. Third, we demonstrate the application of both these techniques to reconstructing the textured appearance of human faces from sparse captures. Our method recovers high-quality relightable appearance parameters that are compatible with current production renderers.},
  archive      = {J_TOG},
  author       = {Philippe Weier and Jérémy Riviere and Ruslan Guseinov and Stephan Garbin and Philipp Slusallek and Bernd Bickel and Thabo Beeler and Delio Vicini},
  doi          = {10.1145/3730855},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Practical inverse rendering of textured and translucent appearance},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Order matters: Learning element ordering for graphic design generation. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3730858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past few years have witnessed an emergent interest in building generative models for the graphic design domain. For adoption of powerful deep generative models with Transformer-based neural backbones, prior approaches formulate designs as ordered sequences of elements, and simply order the elements in a random or raster manner. We argue that such naive ordering methods are sub-optimal and there is room for improving sample quality through a better choice of order between graphic design elements. In this paper, we seek to explore the space of orderings to find the ordering strategy that optimizes the performance of graphic design generation models. For this, we propose a model, namely G enerative O rder L earner (GOL), which trains an autoregressive generator on design sequences, jointly with an ordering network that sort design elements to maximize the generation quality. With unsupervised training on vector graphic design data, our model is capable of learning a content-adaptive ordering approach, called neural order. Our experiments show that the generator trained with our neural order converges faster, achieving remarkably improved generation quality compared with using alternative ordering baselines. We conduct comprehensive analysis of our learned order to have a deeper understanding of its ordering behaviors. In addition, our learned order can generalize well to diffusion-based generative models and help design generators scale up excellently.},
  archive      = {J_TOG},
  author       = {Bo Yang and Ying Cao},
  doi          = {10.1145/3730858},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Order matters: Learning element ordering for graphic design generation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital animation of powder-snow avalanches. <em>TOG</em>, <em>44</em>(4), 1-20. (<a href='https://doi.org/10.1145/3730862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Powder-snow avalanches are natural phenomena that result from an instability in the snow cover on a mountain relief. It begins with a dense avalanche core moving fast down the mountain. During its evolution, the snow particles in the avalanche front mix with the air, forming a suspended turbulent cloud of snow dust surrounding the dense snow avalanche. This paper introduces a physically-based framework using the Finite Volume Method to simulate powder-snow avalanches under complex terrains. Specifically, the primary goal is to simulate the turbulent snow cloud dynamics within the avalanche in a visually realistic manner. Our approach relies on a multi-layer model that splits the avalanche into two main layers: dense and powder-snow. The dense-snow layer flow is simulated by solving a type of Shallow Water Equations suited for intricate basal surfaces, known as the Savage-Hutter model. The powder-snow layer flow is modeled as a two-phase mixture of miscible fluids and simulated using Navier-Stokes equations. Moreover, we propose a novel model for the transition layer, which is responsible for coupling the avalanche main layers, including the snow mass injected into the powder-snow cloud from the snow entrainment processes and its injection velocity. In brief, our framework comprehensively simulates powder-snow avalanches, allowing us to render convincing animations of one of the most complex gravity-driven flows.},
  archive      = {J_TOG},
  author       = {Filipe Nascimento and Fabricio S. Sousa and Afonso Paiva},
  doi          = {10.1145/3730862},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Digital animation of powder-snow avalanches},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Piecewise ruled approximation for freeform mesh surfaces. <em>TOG</em>, <em>44</em>(4), 1-18. (<a href='https://doi.org/10.1145/3730866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A ruled surface is a shape swept out by moving a line in 3D space. Due to their simple geometric forms, ruled surfaces have applications in various domains such as architecture and engineering. In the past, various approaches have been proposed to approximate a target shape using developable surfaces, which are special ruled surfaces with zero Gaussian curvature. However, methods for shape approximation using general ruled surfaces remain limited and often require the target shape to be either represented as parametric surfaces or have non-positive Gaussian curvature. In this paper, we propose a method to compute a piecewise ruled surface that approximates an arbitrary freeform mesh surface. We first use a group-sparsity formulation to optimize the given mesh shape into an approximately piecewise ruled form, in conjunction with a tangent vector field that indicates the ruling directions. Afterward, we utilize the optimization result to extract seams that separate smooth families of rulings, and use the seams to construct the initial rulings. Finally, we further optimize the positions and orientations of the rulings to improve the alignment with the input target shape. We apply our method to a variety of freeform shapes with different topologies and complexity, demonstrating its effectiveness in approximating arbitrary shapes.},
  archive      = {J_TOG},
  author       = {Yiling Pan and Zhixin Xu and Bin Wang and Bailin Deng},
  doi          = {10.1145/3730866},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Piecewise ruled approximation for freeform mesh surfaces},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In search of empty spheres: 3D apollonius diagrams on GPU. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3730868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel comprehensive construction algorithm of Apollonius diagrams designed for GPUs. Efficient and robust algorithms have been proposed for the computation of Voronoi diagrams or Power diagrams. In contrast, Apollonius cells are neither convex nor bounded by straight boundaries, making their computation complex, especially in more than two dimensions. Their parallel computation also represents a challenge because of the sequential nature of state-of-the-art algorithms. In this article, we tackle the computation of these diagrams from the geometry of their cells. Our strategy is based on a core cell topology update allowing the iterative insertion of new sites found through nearest neighbor queries. To benefit from the highly parallel environment of modern GPUs and fit their memory restriction, we define a lightweight data structure allowing the representation of the complex topology of Apollonius cells. Additionally, we provide several space exploration procedures for their efficient construction under both homogeneous and heterogeneous spatial distributions. Our method outperforms the fastest state-of-the-art CPU implementation while computing the complete geometry. As a possible use case, we show an application for molecular illustration.},
  archive      = {J_TOG},
  author       = {Cyprien Plateau-Holleville and Benjamin Stamm and Vincent Nivoliers and Maxime Maria and Stéphane Mérillou},
  doi          = {10.1145/3730868},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {In search of empty spheres: 3D apollonius diagrams on GPU},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A versatile quaternion-based constrained rigid body dynamics. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3730872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a constrained Rigid Body Dynamics (RBD) that guarantees satisfaction of kinematic constraints, enabling direct simulation of complex mechanical systems with arbitrary kinematic structures. To ensure constraint satisfaction, we use an implicit integration scheme. For this purpose, we derive compatible dynamic equations expressed through the quaternion time derivative, adopting an additive approach to quaternion updates instead of a multiplicative one, while enforcing quaternion unit-length as a constraint. We support all joints between rigid bodies that restrict subsets of the three translational or three rotational degrees of freedom, including position- and force-based actuation. Their constraints are formulated such that Lagrange multipliers are interpretable as joint forces and torques. We discuss a unified solution strategy for systems with redundant constraints, overactuation, and passive degrees of freedom, by eliminating redundant constraints and navigating the subspaces spanned by multipliers. As our method uses a standard additive update, we can interface with unconditionally-stable implicit integrators. Moreover, the simulation can readily be made differentiable as we show with examples.},
  archive      = {J_TOG},
  author       = {Guirec Maloisel and Ruben Grandia and Christian Schumacher and Espen Knoop and Moritz Bächer},
  doi          = {10.1145/3730872},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A versatile quaternion-based constrained rigid body dynamics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The mokume dataset and inverse modeling of solid wood textures. <em>TOG</em>, <em>44</em>(4), 1-18. (<a href='https://doi.org/10.1145/3730874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the Mokume dataset for solid wood texturing consisting of 190 cube-shaped samples of various hard and softwood species documented by high-resolution exterior photographs, annual ring annotations, and volumetric computed tomography (CT) scans. A subset of samples further includes photographs along slanted cuts through the cube for validation purposes. Using this dataset, we propose a three-stage inverse modeling pipeline to infer solid wood textures using only exterior photographs. Our method begins by evaluating a neural model to localize year rings on the cube face photographs. We then extend these exterior 2D observations into a globally consistent 3D representation by optimizing a procedural growth field using a novel iso-contour loss. Finally, we synthesize a detailed volumetric color texture from the growth field. For this last step, we propose two methods with different efficiency and quality characteristics: a fast inverse procedural texture method, and a neural cellular automaton (NCA). We demonstrate the synergy between the Mokume dataset and the proposed algorithms through comprehensive comparisons with unseen captured data. We also present experiments demonstrating the efficiency of our pipeline's components against ablations and baselines. Our code, the dataset, and reconstructions are available via https://mokumeproject.github.io/.},
  archive      = {J_TOG},
  author       = {Maria Larsson and Hodaka Yamaguchi and Ehsan Pajouheshgar and I-Chao Shen and Kenji Tojo and Chia-Ming Chang and Lars Hansson and Olof Broman and Takashi Ijiri and Ariel Shamir and Wenzel Jakob and Takeo Igarashi},
  doi          = {10.1145/3730874},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {The mokume dataset and inverse modeling of solid wood textures},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Streaming-aware neural monte carlo rendering framework with unified denoising-compression and client collaboration. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3730879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in cloud rendering have brought us a promising alternative for interactive photorealistic rendering on lightweight devices, which used to be only available on high-end platforms equipped with powerful graphic cards. This technique enables users to perform rendering-related creative tasks, such as 3D product visualization and lighting design, from the comfort of any location using handheld devices, rather than being confined to the front of a noisy heat-generating workstation. However, existing large-scale cloud rendering systems that stream path-traced frames from the server to the client present extremely high rendering costs and transmission bandwidth requirements, even with advanced path-tracing acceleration and video compression techniques. To alleviate these problems, we propose a novel streaming-aware rendering framework that is able to learn a joint optimal model integrating two path-tracing acceleration techniques (adaptive sampling and denoising) and video compression technique. Our joint model can fully exploit the inherent connections between these techniques and thus achieve substantially reduced rendering costs and enhanced compression quality. We also introduce the collaboration of client rendering ability to assist the frame decoding by rendering G-buffers as the shared side information. We demonstrate that appropriately incorporating the geometry and material priors from G-buffers into a neural compression pipeline can significantly reduce the streaming bandwidth in a cloud rendering system, and lighten the compression module design for computation efficiency. Our experiments show that our method delivers the best quality at various bitrates compared to existing Monte Carlo rendering streaming schemes, while remaining lightweight and efficient for cross-platform thin clients, including mobiles and tablets.},
  archive      = {J_TOG},
  author       = {Hangming Fan and Yuchi Huo and Chuankun Zheng and Chonghao Hu and Yazhen Yuan and Rui Wang},
  doi          = {10.1145/3730879},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Streaming-aware neural monte carlo rendering framework with unified denoising-compression and client collaboration},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing pin-pression gripper and learning its dexterous grasping with online in-hand adjustment. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3730880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel design of parallel-jaw grippers drawing inspiration from pin-pression toys. The proposed pin-pression gripper features a distinctive mechanism in which each finger integrates a 2D array of pins capable of independent extension and retraction. This unique design allows the gripper to instantaneously customize its finger's shape to conform to the object being grasped by dynamically adjusting the extension/retraction of the pins. In addition, the gripper excels in in-hand re-orientation of objects for enhanced grasping stability again via dynamically adjusting the pins. To learn the dynamic grasping skills of pin-pression grippers, we devise a dedicated reinforcement learning algorithm with careful designs of state representation and reward shaping. To achieve a more efficient grasp-while-lift grasping mode, we propose a curriculum learning scheme. Extensive evaluations demonstrate that our design, together with the learned skills, leads to highly flexible and robust grasping with much stronger generality to unseen objects than alternatives. We also highlight encouraging physical results of sim-to-real transfer on a physically manufactured pin-pression gripper, demonstrating the practical significance of our novel gripper design and grasping skill. Demonstration videos for this paper are available at https://github.com/siggraph-pin-pression-gripper/pin-pression-gripper-video.},
  archive      = {J_TOG},
  author       = {Hewen Xiao and Xiuping Liu and Hang Zhao and Jian Liu and Kai Xu},
  doi          = {10.1145/3730880},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Designing pin-pression gripper and learning its dexterous grasping with online in-hand adjustment},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Closed-form generalized winding numbers of rational parametric curves for robust containment queries. <em>TOG</em>, <em>44</em>(4), 1-9. (<a href='https://doi.org/10.1145/3730886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive closed-form expressions for generalized winding numbers of rational parametric curves for robust containment queries. Given an oriented rational parametric curve and a query point, the generalized winding number can be reformulated to an integral of a rational polynomial. The key to computing the integral lies in using the residue theorem. Then, add up the contributions of each curve to obtain the generalized winding numbers of a set of rational parametric curves. Furthermore, the derivatives of generalized winding numbers are easily derived. Consequently, the expressions for generalized winding numbers are concise and computationally efficient, becoming faster than state-of-the-art methods. Moreover, the computational costs for various query points are almost the same.},
  archive      = {J_TOG},
  author       = {Shibo Liu and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1145/3730886},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-9},
  shortjournal = {ACM Trans. Graph.},
  title        = {Closed-form generalized winding numbers of rational parametric curves for robust containment queries},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polynomial 2D biharmonic coordinates for high-order cages. <em>TOG</em>, <em>44</em>(4), 1-10. (<a href='https://doi.org/10.1145/3730887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive closed-form expressions of biharmonic coordinates for 2D high-order cages, enabling the transformation of the input polynomial curves into polynomial curves of any order. Central to our derivation is the use of the high-order boundary element method. We demonstrate the practicality and effectiveness of our method on various 2D deformations. In practice, users can easily manipulate the Bézier control points to perform the desired intuitive deformation, as the biharmonic coordinates provide an enriched deformation space and encourage the alignment between the boundary cage and its interior geometry.},
  archive      = {J_TOG},
  author       = {Shibo Liu and Tielin Dai and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1145/3730887},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Polynomial 2D biharmonic coordinates for high-order cages},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic analysis and design of linear elastic shell lattice metamaterials. <em>TOG</em>, <em>44</em>(4), 1-18. (<a href='https://doi.org/10.1145/3730888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an asymptotic analysis of shell lattice metamaterials based on Ciarlet's shell theory, introducing a new metric— asymptotic directional stiffness (ADS)—to quantify how the geometry of the middle surface governs the effective stiffness. We prove a convergence theorem that rigorously characterizes ADS and establishes its upper bound, along with necessary and sufficient condition for achieving it. As a key result, our theory provides the first rigorous explanation for the high bulk modulus observed in Triply Periodic Minimal Surfaces (TPMS)-based shell lattices. To optimize ADS on general periodic surfaces, we propose a triangular-mesh-based discretization and shape optimization framework. Numerical experiments validate the theoretical findings and demonstrate the effectiveness of the optimization under various design objectives. Our implementation is available at https://github.com/lavenklau/minisurf.},
  archive      = {J_TOG},
  author       = {Di Zhang and Ligang Liu},
  doi          = {10.1145/3730888},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Asymptotic analysis and design of linear elastic shell lattice metamaterials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Field smoothness-controlled partition for quadrangulation. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3730889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel partition method for reliable feature-aligned quadrangulation. The core insight of the partition is that smooth streamlines distant from singularities are more suitable as patch boundaries. This allows singularities to be enclosed within patches, resulting in straighter patch boundaries and reducing the distorting influence of singularities. Accordingly, we introduce a new patch quality control mechanism that keeps the patch boundaries inside regions with high field smoothness. Combined with other common metrics (e.g., aligning boundaries with field and feature lines), we develop a practical partition algorithm that first iteratively traces paths in field smoothness-controlled regions to form patches and then removes redundant paths to simplify the patch layout. We demonstrate the effectiveness and practicability of our partitions by using them to generate quality quad meshes on a massive test data set. Compared with state-of-the-art methods, our approach produces quad meshes with significantly enhanced quality while maintaining similar reliability, validating the core insight. Code and data for this paper are at https://github.com/AnderLiang/Field-Smoothness-Controlled-Quadrangulation.},
  archive      = {J_TOG},
  author       = {Zhongxuan Liang and Wei Du and Xiao-Ming Fu},
  doi          = {10.1145/3730889},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Field smoothness-controlled partition for quadrangulation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting tradition and beyond: A customized bilateral filtering framework for point cloud denoising. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3730891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have become the dominant solution for point cloud denoising, offering strong generalization capabilities through data-driven training. However, traditional methods, despite their drawbacks of heavy parameter tuning and weak generalization, retain unique advantages in interpretability and theoretical robustness. This complementarity motivates us to explore a hybrid solution that leverages data-driven paradigms to overcome the performance constraints of traditional methods. In this paper, we revisit the classic bilateral filter (BF) as a case study and identify three key limitations hindering its performance: excessive parameter tuning, suboptimal neighborhood quality, and fixed parameters across the entire model. To address them, we propose CustomBF, a novel framework for customizing BF components at a per-point level. CustomBF employs multigraph encoders and a mutual guidance strategy to analyze local patches, enabling the customization of BF components including center point normal, neighborhood point coordinates, Gaussian function parameters, and neighborhood radius for each point. Experimental results demonstrate that this component-customized bilateral filter outperforms state-of-the-art methods and achieves robust denoising even in complex scenarios. It highlights the potential of hybrid methods to extend the applicability and effectiveness of traditional techniques.},
  archive      = {J_TOG},
  author       = {Peng Li and Zeyong Wei and Honghua Chen and Xuefeng Yan and Mingqiang Wei},
  doi          = {10.1145/3730891},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Revisiting tradition and beyond: A customized bilateral filtering framework for point cloud denoising},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransparentGS: Fast inverse rendering of transparent objects with gaussians. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3730892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.},
  archive      = {J_TOG},
  author       = {Letian Huang and Dongwei Ye and Jialin Dan and Chengzhi Tao and Huiwen Liu and Kun Zhou and Bo Ren and Yuanqi Li and Yanwen Guo and Jie Guo},
  doi          = {10.1145/3730892},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {TransparentGS: Fast inverse rendering of transparent objects with gaussians},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moment bounds are differentiable: Efficiently approximating measures in inverse rendering. <em>TOG</em>, <em>44</em>(4), 1-21. (<a href='https://doi.org/10.1145/3730899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {All rendering methods aim at striking a balance between realism and efficiency. This is particularly relevant for differentiable rendering, where the additional aspect of differentiablity w.r.t. scene parameters causes increased computational complexity while, on the other hand, in the common application of inverse rendering, the diverse effects of real image formation must be faithfully reproduced. An important effect in rendering is the attenuation of light as it travels through different media (visibility, shadows, transmittance, transparency). This can be modeled as an integral over non-negative functions and has been successfully approximated in forward rendering by so-called moments. We show that moment-based approximations are differentiable in the parameters defining the moments, and that this leads to efficient and practical methods for inverse rendering. In particular, we demonstrate the method at the examples of shadow mapping and visibility in volume rendering, leading to approximations that are similar in efficiency to existing ad-hoc techniques while being significantly more accurate.},
  archive      = {J_TOG},
  author       = {Markus Worchel and Marc Alexa},
  doi          = {10.1145/3730899},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Moment bounds are differentiable: Efficiently approximating measures in inverse rendering},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentiable geometric acoustic path tracing using time-resolved path replay backpropagation. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3730900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable rendering has become a key ingredient in solving challenging inverse problems in computer graphics and vision. Existing systems can simulate and differentiate the spatial propagation of light. We exploit the duality of light transport simulations and geometric acoustics to apply differential rendering techniques to established acoustic simulation methods. The resulting system is capable of simulating sound according to the geometrical acoustics model and computing derivatives of the output energy spectrograms with respect to arbitrary parameters of the scene, including materials, emitters, microphones, and scene geometry. Contrary to current differentiable transient rendering, we can handle arbitrary simulation depths and achieve constant memory and linear execution times by presenting a temporal extension of Path Replay Backpropagation [Vicini et al. 2021]. We verify our model against established simulation software, and demonstrate the capabilities of optimization with gradients at examples of inverse acoustics and optimizing room parameters. This opens up a new field of research for acoustic optimization that could be as impactful for the acoustic community as differentiable rendering was for the graphics community.},
  archive      = {J_TOG},
  author       = {Ugo Finnendahl and Markus Worchel and Tobias Jüterbock and Daniel Wujecki and Fabian Brinkmann and Stefan Weinzierl and Marc Alexa},
  doi          = {10.1145/3730900},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable geometric acoustic path tracing using time-resolved path replay backpropagation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boolean operation for CAD models using a hybrid representation. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3730908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boolean operations for Boundary Representation (B-Rep) models are among the most commonly used functions in Computer Aided Design (CAD) systems. They are also one of the most delicate soft modules, with challenges arising from complex algorithmic flows and efficiency and accuracy issues, especially in extreme cases. Common issues encountered in processing complex models include low efficiency, missing results, and non-watertightness. In this paper, we propose a novel algorithm for efficient and accurate Boolean operations on B-Rep models. This is achieved by establishing a bijective mapping between B-Rep models and the corresponding triangle meshes with controllable approximation error, thus mapping B-Rep Boolean operations to mesh Boolean operations. By using conservative intersection detection on the mesh to locate all surface intersection curves and carefully handling degeneration and topology errors, we ensure that the results are consistently watertight and correct. We demonstrate the superior efficiency of the proposed method using the open-source geometry engine OCCT, the commercial engine ACIS, and the commercial software Rhino as benchmarks.},
  archive      = {J_TOG},
  author       = {Yingyu Yang and Xiaohong Jia and Bolun Wang and Jieyin Yang and Shiqing Xin and Dong-Ming Yan},
  doi          = {10.1145/3730908},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Boolean operation for CAD models using a hybrid representation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On-the-fly reconstruction for large-scale novel view synthesis from unposed images. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3730913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy reconstruction from photos, enabling free-viewpoint navigation. Nonetheless, pose estimation using Structure from Motion and 3DGS optimization can still each take between minutes and hours of computation after capture is complete. SLAM methods combined with 3DGS are fast but struggle with wide camera baselines and large scenes. We present an on-the-fly method to produce camera poses and a trained 3DGS immediately after capture. Our method can handle dense and wide-baseline captures of ordered photo sequences and large-scale scenes. To do this, we first introduce fast initial pose estimation, exploiting learned features and a GPU-friendly mini bundle adjustment. We then introduce direct sampling of Gaussian primitive positions and shapes, incrementally spawning primitives where required, significantly accelerating training. These two efficient steps allow fast and robust joint optimization of poses and Gaussian primitives. Our incremental approach handles large-scale scenes by introducing scalable radiance field construction, progressively clustering 3DGS primitives, storing them in anchors, and offloading them from the GPU. Clustered primitives are progressively merged, keeping the required scale of 3DGS at any viewpoint. We evaluate our solution on a variety of datasets and show that it can provide on-the-fly processing of all the capture scenarios and scene sizes we target. At the same time our method remains competitive - in speed, image quality, or both - with other methods that only handle specific capture styles or scene sizes.},
  archive      = {J_TOG},
  author       = {Andreas Meuleman and Ishaan Shah and Alexandre Lanvin and Bernhard Kerbl and George Drettakis},
  doi          = {10.1145/3730913},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {On-the-fly reconstruction for large-scale novel view synthesis from unposed images},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DesignManager: An agent-powered copilot for designers to integrate AI design tools into creative workflows. <em>TOG</em>, <em>44</em>(4), 1-26. (<a href='https://doi.org/10.1145/3730919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creative design is an inherently complex and iterative process characterized by continuous exploration, evaluation, and refinement. While recent advances in generative AI have demonstrated remarkable potential in supporting specific design tasks, there remains a critical gap in understanding how these technologies can enhance the holistic design process rather than just isolated stages. This paper introduces DesignManager, a novel AI-powered design support system that aims to transform how designers collaborate with AI throughout their creative workflow. Through a formative study examining designers' current practices with generative AI, we identified key challenges and opportunities in integrating AI into the creative design process. Based on these insights, we developed DesignManager as an interactive copilot system that provides node-based visualization of design evolution, enabling designers to track, modify, and branch their design processes while maintaining meaningful dialogue-based collaboration. The system offers two collaboration modes: DesignManager-guiding and Designer-guiding. Designers can engage in conversational interactions with the DesignManager to obtain design inspiration and tool recommendations, and proactively advance the design progress. The system employs an agent framework to manage decoupled contextual information emerged during the design process, facilitating deep understanding of designers' needs and providing context-aware assistance. Our technical evaluation validated the effectiveness of context decoupling and the use of agent framework, while the open-ended user study with experts demonstrated that DesignManager successfully supports intuitive intention expression, flexible process control, and deeper creative articulation. This work contributes to the understanding of how AI can evolve from task-specific tools to collaborative partners in creative design processes.},
  archive      = {J_TOG},
  author       = {Weitao You and Yinyu Lu and Zirui Ma and Nan Li and Mingxu Zhou and Xue Zhao and Pei Chen and Lingyun Sun},
  doi          = {10.1145/3730919},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. Graph.},
  title        = {DesignManager: An agent-powered copilot for designers to integrate AI design tools into creative workflows},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural co-optimization of structural topology, manufacturable layers, and path orientations for fiber-reinforced composites. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3730922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a neural network-based computational framework for the simultaneous optimization of structural topology, curved layers, and path orientations to achieve strong anisotropic strength in fiber-reinforced thermoplastic composites while ensuring manufacturability. Our framework employs three implicit neural fields to represent geometric shape, layer sequence, and fiber orientation. This enables the direct formulation of both design and manufacturability objectives - such as anisotropic strength, structural volume, machine motion control, layer curvature, and layer thickness - into an integrated and differentiable optimization process. By incorporating these objectives as loss functions, the framework ensures that the resultant composites exhibit optimized mechanical strength while remaining its manufacturability for filament-based multi-axis 3D printing across diverse hardware platforms. Physical experiments demonstrate that the composites generated by our co-optimization method can achieve an improvement of up to 33.1% in failure loads compared to composites with sequentially optimized structures and manufacturing sequences.},
  archive      = {J_TOG},
  author       = {Tao Liu and Tianyu Zhang and Yongxue Chen and Weiming Wang and Yu Jiang and Yuming Huang and Charlie C. L. Wang},
  doi          = {10.1145/3730922},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural co-optimization of structural topology, manufacturable layers, and path orientations for fiber-reinforced composites},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A divide-and-conquer approach for global orientation of non-watertight scene-level point clouds using 0-1 integer optimization. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3730923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orienting point clouds is a fundamental problem in computer graphics and 3D vision, with applications in reconstruction, segmentation, and analysis. While significant progress has been made, existing approaches mainly focus on watertight, object-level 3D models. The orientation of large-scale, non-watertight 3D scenes remains an underexplored challenge. To address this gap, we propose DACPO (Divide-And-Conquer Point Orientation), a novel framework that leverages a divide-and-conquer strategy for scalable and robust point cloud orientation. Rather than attempting to orient an unbounded scene at once, DACPO segments the input point cloud into smaller, manageable blocks, processes each block independently, and integrates the results through a global optimization stage. For each block, we introduce a two-step process: estimating initial normal orientations by a randomized greedy method and refining them by an adapted iterative Poisson surface reconstruction. To achieve consistency across blocks, we model inter-block relationships using an an undirected graph, where nodes represent blocks and edges connect spatially adjacent blocks. To reliably evaluate orientation consistency between adjacent blocks, we introduce the concept of the visible connected region , which defines the region over which visibility-based assessments are performed. The global integration is then formulated as a 0-1 integer-constrained optimization problem, with block flip states as binary variables. Despite the combinatorial nature of the problem, DACPO remains scalable by limiting the number of blocks (typically a few hundred for 3D scenes) involved in the optimization. Experiments on benchmark datasets demonstrate DACPO's strong performance, particularly in challenging large-scale, non-watertight scenarios where existing methods often fail. The source code is available at https://github.com/zd-lee/DACPO.},
  archive      = {J_TOG},
  author       = {Zhuodong Li and Fei Hou and Wencheng Wang and Xuequan Lu and Ying He},
  doi          = {10.1145/3730923},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A divide-and-conquer approach for global orientation of non-watertight scene-level point clouds using 0-1 integer optimization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When gaussian meets surfel: Ultra-fast high-fidelity radiance field rendering. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3730925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for radiance field rendering, wherein a set of 2D opaque surfels with view-dependent colors represent the coarse-scale geometry and appearance of scenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale appearance details. The rendering with GESs consists of two passes - surfels are first rasterized through a standard graphics pipeline to produce depth and color maps, and then Gaussians are splatted with depth testing and color accumulation on each pixel order independently. The optimization of GESs from multi-view images is performed through an elaborate coarse-to-fine procedure, faithfully capturing rich scene appearance. The entirely sorting-free rendering of GESs not only achieves very fast rates, but also produces view-consistent images, successfully avoiding popping artifacts under view changes. The basic GES representation can be easily extended to achieve antialiasing in rendering (Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage (Compact-GES), and reconstruct better scene geometries by replacing 3D Gaussians with 2D Gaussians (2D-GES). Experimental results show that GESs advance the state-of-the-arts as a compelling representation for ultra-fast high-fidelity radiance field rendering.},
  archive      = {J_TOG},
  author       = {Keyang Ye and Tianjia Shao and Kun Zhou},
  doi          = {10.1145/3730925},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {When gaussian meets surfel: Ultra-fast high-fidelity radiance field rendering},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MonetGPT: Solving puzzles enhances MLLMs' image retouching skills. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3730926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially-designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.},
  archive      = {J_TOG},
  author       = {Niladri Shekhar Dutt and Duygu Ceylan and Niloy J. Mitra},
  doi          = {10.1145/3730926},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {MonetGPT: Solving puzzles enhances MLLMs' image retouching skills},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional procedural wave noise. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3730928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While precise spectral control can be achieved through sparse convolution, corresponding state of the art noise models are typically too expensive for solid noise. We introduce an alternative, wave-based procedural noise model, fast enough to be used in any dimension. We express the noise in the spectral domain and then apply an inverse Fourier transform (FT), requiring the computation of a multidimensional integral. Our contribution is a novel, efficient way to perform this computation, using a sum of precomputed complex-valued hyperplanar wave-functions, oriented in random directions. We show that using suitable wave profiles and combination operators, our model is able to extend to 3D a number of Gaussian and non-Gaussian noises, including Gabor, by-example and Phasor noises, as well as generate novel cellular noises. Our versatile and controllable solid noise model is very compact, a key feature for complex power spectrum and animated noises. We illustrate this through the design of 2D, 3D, and 3D+t materials using color, transparency and style transfer functions.},
  archive      = {J_TOG},
  author       = {Pascal Guehl and Rémi Allègre and Guillaume Gilet and Basile Sauvage and Marie-Paule Cani and Jean-Michel Dischler},
  doi          = {10.1145/3730928},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Multi-dimensional procedural wave noise},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tiny is not small enough: High quality, low-resource facial animation models through hybrid knowledge distillation. <em>TOG</em>, <em>44</em>(4), 1-18. (<a href='https://doi.org/10.1145/3730929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training of high-quality, robust machine learning models for speech-driven 3D facial animation requires a large, diverse dataset of high-quality audio-animation pairs. To overcome the lack of such a dataset, recent work has introduced large pre-trained speech encoders that are robust to variations in the input audio and, therefore, enable the facial animation model to generalize across speakers, audio quality, and languages. However, the resulting facial animation models are prohibitively large and lend themselves only to offline inference on a dedicated machine. In this work, we explore on-device, real-time facial animation models in the context of game development. We overcome the lack of large datasets by using hybrid knowledge distillation with pseudo-labeling. Given a large audio dataset, we employ a high-performing teacher model to train very small student models. In contrast to the pre-trained speech encoders, our student models only consist of convolutional and fully-connected layers, removing the need for attention context or recurrent updates. In our experiments, we demonstrate that we can reduce the memory footprint to up to 3.4 MB and required future audio context to up to 81 ms while maintaining high-quality animations. This paves the way for on-device inference, an important step towards realistic, model-driven digital characters.},
  archive      = {J_TOG},
  author       = {Zhen Han and Mattias Teye and Derek Yadgaroff and Judith Bütepage},
  doi          = {10.1145/3730929},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Tiny is not small enough: High quality, low-resource facial animation models through hybrid knowledge distillation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One model to rig them all: Diverse skeleton rigging with UniRig. <em>TOG</em>, <em>44</em>(4), 1-18. (<a href='https://doi.org/10.1145/3730930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of 3D content creation, encompassing both AI-powered methods and traditional workflows, is driving an unprecedented demand for automated rigging solutions that can keep pace with the increasing complexity and diversity of 3D models. We introduce UniRig , a novel, unified framework for automatic skeletal rigging that leverages the power of large autoregressive models and a bone-point cross-attention mechanism to generate both high-quality skeletons and skinning weights. Unlike previous methods that struggle with complex or non-standard topologies, UniRig accurately predicts topologically valid skeleton structures thanks to a new Skeleton Tree Tokenization method that efficiently encodes hierarchical relationships within the skeleton. To train and evaluate UniRig, we present Rig-XL , a new large-scale dataset of over 14,000 rigged 3D models spanning a wide range of categories. UniRig significantly outperforms state-of-the-art academic and commercial methods, achieving a 215% improvement in rigging accuracy and a 194% improvement in motion accuracy on challenging datasets. Our method works seamlessly across diverse object categories, from detailed anime characters to complex organic and inorganic structures, demonstrating its versatility and robustness. By automating the tedious and time-consuming rigging process, UniRig has the potential to speed up animation pipelines with unprecedented ease and efficiency. Project Page: https://zjp-shadow.github.io/works/UniRig/},
  archive      = {J_TOG},
  author       = {Jia-Peng Zhang and Cheng-Feng Pu and Meng-Hao Guo and Yan-Pei Cao and Shi-Min Hu},
  doi          = {10.1145/3730930},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {One model to rig them all: Diverse skeleton rigging with UniRig},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). C-tubes: Design and optimization of tubular structures composed of developable strips. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3730933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce C-tubes , 3D tubular structures composed of developable surface strips. C-tubes can be understood as a generalization of Monge surfaces—a special class of sweep surfaces—towards the recently introduced conenets. This observation allows formulating a constructive algorithm to create tubular structures that ensures developability of the constituent surfaces, while significantly broadening the design space. Our novel form-finding tool enables design exploration by solving for the input variables of the constructive algorithm so that the C-tube best conforms to user-specified objectives. We discuss several case studies that illustrate the versatility of our approach for the design and fabrication of complex structures, with applications in architecture, furniture, and lighting design.},
  archive      = {J_TOG},
  author       = {Michele Vidulis and Klara Mundilova and Quentin Becker and Florin Isvoranu and Mark Pauly},
  doi          = {10.1145/3730933},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {C-tubes: Design and optimization of tubular structures composed of developable strips},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WishGI: Lightweight static global illumination baking via spherical harmonics fitting. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3730935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global illumination combines direct and indirect lighting to create realistic lighting effects, bringing virtual scenes closer to reality. Static global illumination is a crucial component of virtual scene rendering, leveraging precomputation and baking techniques to significantly reduce runtime computational costs. Unfortunately, many existing works prioritize visual quality by relying on extensive texture storage and massive pixel-level texture sampling, leading to large performance overhead. In this paper, we introduce an illumination reconstruction method that effectively reduces sampling in fragment shader and avoids additional render passes, making it well-suited for low-end platforms. To achieve high-quality global illumination with reduced memory usage, we adopt a spherical harmonics fitting approach for baking effective illumination information and propose an inverse probe distribution method that generates unique probe associations for each mesh. This association, which can be generated offline in the local space, ensures consistent lighting quality across all instances of the same mesh. As a consequence, our method delivers highly competitive lighting effects while using only approximately 5% of the memory required by mainstream industry techniques.},
  archive      = {J_TOG},
  author       = {Junke Zhu and Zehan Wu and Qixing Zhang and Cheng Liao and Zhangjin Huang},
  doi          = {10.1145/3730935},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {WishGI: Lightweight static global illumination baking via spherical harmonics fitting},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer IMU calibrator: Dynamic on-body IMU calibration for inertial motion capture. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3730937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel dynamic calibration method for sparse inertial motion capture systems, which is the first to break the restrictive absolute static assumption in IMU calibration, i.e., the coordinate drift R G′ G and measurement offset R BS remain constant during the entire motion, thereby significantly expanding their application scenarios. Specifically, we achieve real-time estimation of R G′ G and R BS under two relaxed assumptions: i) the matrices change negligibly in a short time window; ii) the human movements/IMU readings are diverse in such a time window. Intuitively, the first assumption reduces the number of candidate matrices, and the second assumption provides diverse constraints, which greatly reduces the solution space and allows for accurate estimation of R G′ G and R BS from a short history of IMU readings in real time. To achieve this, we created synthetic datasets of paired R G′ G , R BS matrices and IMU readings, and learned their mappings using a Transformer-based model. We also designed a calibration trigger based on the diversity of IMU readings to ensure that assumption ii) is met before applying our method. To our knowledge, we are the first to achieve implicit IMU calibration (i.e., seamlessly putting IMUs into use without the need for an explicit calibration process), as well as the first to enable long-term and accurate motion capture using sparse IMUs. The code and dataset are available at https://github.com/ZuoCX1996/TIC.},
  archive      = {J_TOG},
  author       = {Chengxu Zuo and Jiawei Huang and Xiao Jiang and Yuan Yao and Xiangren Shi and Rui Cao and Xinyu Yi and Feng Xu and Shihui Guo and Yipeng Qin},
  doi          = {10.1145/3730937},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Transformer IMU calibrator: Dynamic on-body IMU calibration for inertial motion capture},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HexHex: Highspeed extraction of hexahedral meshes. <em>TOG</em>, <em>44</em>(4), 1-20. (<a href='https://doi.org/10.1145/3730940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern hexahedral mesh generation relies on integer-grid maps (IGM), which map the Cartesian grid of integer iso-surfaces to a structure-aligned and conforming hexahedral cell complex discretizing the target shape. The hexahedral mesh is formed by iso-surfaces of the map such that an extraction algorithm is needed to convert the implicit map representation into an explicit mesh. State-of-the-art algorithms have been designed with two goals in mind, i.e., (i) unconditional robustness and (ii) tolerance to map defects in the form of inverted or degenerate tetrahedra. Because of significant advancements in the generation of locally injective maps, the tolerance to map defects has become irrelevant. At the same time, there is a growing demand for efficiently handling significantly larger mesh complexities, unfortunately not well served by the state-of-the-art since the tolerance to map defects induces a high runtime cost. Consequently, we present HexHex, a novel (unconditionally robust) hexahedral mesh extraction algorithm for locally injective integer-grid maps designed for maximal performance and scalability. Key contributions include a novel and highly compact mesh data structure based on so-called propellers and a conservative rasterization technique, significantly reducing the number of required exact predicate tests. HexHex not only offers lower asymptotic runtime complexities from a theoretical perspective but also lower constants, enabling in practice a 30x speedup for medium-sized examples and a larger speedup for more complex inputs, specifically when the hex-to-tet ratio is large. We provide a C++ reference implementation, supporting multi-core parallelization and the extraction of curved (piecewise-linear) hexahedral mesh edges and faces, e.g., valuable for subsequent higher-order mesh generation.},
  archive      = {J_TOG},
  author       = {Tobias Kohler and Martin Heistermann and David Bommes},
  doi          = {10.1145/3730940},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {HexHex: Highspeed extraction of hexahedral meshes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal first passage for epsilon-free walk-on-spheres. <em>TOG</em>, <em>44</em>(4), 1-11. (<a href='https://doi.org/10.1145/3730942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, grid-free Monte Carlo methods have gained increasing popularity for solving fundamental partial differential equations. For a given point in the domain, the Walk-on-Spheres method solves a boundary integral equation by integrating recursively over the largest possible sphere. When the walks approach boundaries with Dirichlet conditions, the number of path vertices increases considerably, since the step size becomes smaller with decreasing distance to the boundary. In practice, the walks are terminated once they reach an epsilon-shell around the boundary. This, however, introduces bias, leading to a trade-off between accuracy and performance. Instead of using spheres, we propose to utilize geometric primitives that share more than one point with the boundary to increase the likelihood of immediately terminating. Along the boundary of those new geometric primitives a sampling probability is needed, which corresponds to the exit probability of a Brownian motion. This is known as a first passage problem. Utilizing that Laplace equations are invariant under conformal maps, we transform exit points from unit circles to the exit points of our geometric primitives, for which we describe a suitable placement strategy. With this, we obtain a novel approach to solve the Laplace equation in two dimensions, which does not require an epsilon-shell, significantly reduces the number of path vertices, and reduces inaccuracies near Dirichlet boundaries.},
  archive      = {J_TOG},
  author       = {Paul Himmler and Tobias Günther},
  doi          = {10.1145/3730942},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Conformal first passage for epsilon-free walk-on-spheres},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NAM: Neural adjoint maps for refining shape correspondences. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3730943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel approach to refine 3D shape correspondences by leveraging multi-layer perceptions within the framework of functional maps. Central to our contribution is the concept of Neural Adjoint Maps , a novel neural representation that generalizes the traditional solution of functional maps for estimating correspondence between manifolds. Fostering our neural representation, we propose an iterative algorithm explicitly designed to enhance the precision and robustness of shape correspondence across diverse modalities such as meshes and point clouds. By harnessing the expressive power of non-linear solutions, our method captures intricate geometric details and feature correspondences that conventional linear approaches often overlook. Extensive evaluations on standard benchmarks and challenging datasets demonstrate that our approach achieves state-of-the-art accuracy for both isometric and non-isometric meshes and for point clouds where traditional methods frequently struggle. Moreover, we show the versatility of our method in tasks such as signal and neural field transfer, highlighting its broad applicability to domains including computer graphics, medical imaging, and other fields demanding precise transfer of information among 3D shapes. Our work sets a new standard for shape correspondence refinement, offering robust tools across various applications.},
  archive      = {J_TOG},
  author       = {Giulio Viganò and Maks Ovsjanikov and Simone Melzi},
  doi          = {10.1145/3730943},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {NAM: Neural adjoint maps for refining shape correspondences},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Painless differentiable rotation dynamics. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3730944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the formulation of forward and differentiable rigid-body dynamics using Lie-algebra rotation derivatives. In particular, we show how this approach can easily be applied to incremental-potential formulations of forward dymamics, and we introduce a novel definition of adjoints for differentiable dynamics. In contrast to other parameterizations of rotations (notably the popular rotation-vector parameterization), our approach leads to painlessly simple and compact derivatives, better conditioning, and higher runtime efficiency. We demonstrate our approach on fundamental rigid-body problems, but also on Cosserat rods as an example of multi-rigid-body dynamics.},
  archive      = {J_TOG},
  author       = {Magí Romanyà-Serrasolsas and Juan J. Casafranca and Miguel A. Otaduy},
  doi          = {10.1145/3730944},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Painless differentiable rotation dynamics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric contact potential. <em>TOG</em>, <em>44</em>(4), 1-24. (<a href='https://doi.org/10.1145/3731142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Barrier potentials gained popularity as a means for robust contact handling in physical modeling and for modeling self-avoiding shapes. The key to the success of these approaches is adherence to geometric constraints, i.e., avoiding intersections, which are the cause of most robustness problems in complex deformation simulation with contact. However, existing barrier-potential methods may lead to spurious forces and imperfect satisfaction of the geometric constraints. They may have strong resolution dependence, requiring careful adaptation of the potential parameters to the object discretizations. We present a systematic derivation of a continuum potential defined for smooth and piecewise smooth surfaces, starting from identifying a set of natural requirements for contact potentials, including the barrier property, locality, differentiable dependence on shape, and absence of forces in rest configurations. Our potential is formulated independently of surface discretization and addresses the shortcomings of existing potential-based methods while retaining their advantages. We present a discretization of our potential that is a drop-in replacement for the potential used in the incremental potential contact formulation [Li et al. 2020], and compare its behavior to other potential formulations, demonstrating that it has the expected behavior. The presented formulation connects existing barrier approaches, as all recent existing methods can be viewed as a variation of the presented potential, and lays a foundation for developing alternative (e.g., higher-order) versions.},
  archive      = {J_TOG},
  author       = {Zizhou Huang and Maxwell Paik and Zachary Ferguson and Daniele Panozzo and Denis Zorin},
  doi          = {10.1145/3731142},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Geometric contact potential},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single edge collapse quad-dominant mesh reduction. <em>TOG</em>, <em>44</em>(4), 1-23. (<a href='https://doi.org/10.1145/3731143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh reduction using quadric error metrics is the industry standard for producing level-of-detail (LOD) geometry for meshes. Although industry tools produce visually excellent LODs, mesh topology is often ruined during decimation. This is because tools focus on triangle simplification and preserving rendered appearance, whereas artists often produce quad dominant meshes with clean edge topology. Artist created manual LODs preserve both appearance and quad topology. Furthermore, most existing tools for quad decimation only accept pure quad meshes and cannot handle any triangles. The gap between quad and triangular mesh decimation is because they are built on fundamentally different operations, triangle simplification uses single edge collapses, whereas quad decimation requires that entire sets of edges be collapsed atomically. In this work, we demonstrate that single edge collapse can be used to preserve most input quads without degrading geometric quality. Single edge collapse quad preservation is made possible by introducing dihedral-angle weighted quadrics for every edge, allowing optimization to evenly space edges while preserving features. It is further enabled by explicitly ordering edge collapses with nearly equivalent quadric error in a way that preserves quad topology. In addition to quad preservation, we demonstrate that by introducing weights for quadrics on certain edges, our framework can be used to preserve symmetry and joint influences. To demonstrate our approach is suitable for skinned mesh decimation on triangle meshes, we show that QEM with attributes can preserve joint influences better than prior work. We implement and test our approach on 67 static and 19 animated meshes from Sketchfab. On both static and animated meshes, our approach consistently outperforms prior work with lower Chamfer and Hausdorff distance, while preserving more quad topology when present.},
  archive      = {J_TOG},
  author       = {Julian Knodt},
  doi          = {10.1145/3731143},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. Graph.},
  title        = {Single edge collapse quad-dominant mesh reduction},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple importance reweighting for path guiding. <em>TOG</em>, <em>44</em>(4), 1-11. (<a href='https://doi.org/10.1145/3731144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary path guiding employs an iterative training scheme to fit radiance distributions. However, existing methods combine the estimates generated in each iteration merely within image space, overlooking differences in the convergence of distribution fitting over individual light paths. This paper formulates the estimation combination task as a path reweighting process. To compute spatio-directional varying combination weights, we propose multiple importance reweighting , leveraging the importance distributions from multiple guiding iterations. We demonstrate that our proposed path-level reweighting makes guiding algorithms less sensitive to noise and overfitting in distributions. This facilitates a finer subdivision of samples both spatially and temporally (i.e., over iterations), which leads to additional improvements in the accuracy of distributions and samples. Inspired by adaptive multiple importance sampling (AMIS), we introduce a simple yet effective mixture-based weighting scheme with theoretically guaranteed consistency, demonstrating good practical performance compared to alternative weighting schemes. To further foster usage with high sample rates, we introduce a hyperparameter that controls the size of sample storage. When this size limit is exceeded, low-valued samples are splatted during rendering and reweighted using a partial mixture of distributions. We found limiting the storage size reduces memory overhead and keeps variance reduction and bias comparable to the unlimited ones. Our method is largely agnostic to the underlying guiding method and compatible with conventional pixel reweighting techniques. Extensive evaluations underscore the feasibility of our approach in various scenes, achieving variance reduction with negligible bias over state-of-the-art solutions within equal sample rates and rendering time.},
  archive      = {J_TOG},
  author       = {Zhimin Fan and Yiming Wang and Chenxi Zhou and Ling-Qi Yan and Yanwen Guo and Jie Guo},
  doi          = {10.1145/3731144},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Multiple importance reweighting for path guiding},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bernstein bounds for caustics. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3731145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systematically simulating specular light transport requires an exhaustive search for triangle tuples containing admissible paths. Given the extreme inefficiency of enumerating all combinations, we significantly reduce the search domain by stochastically sampling such tuples. The challenge is to design proper sampling probabilities that keep the noise level controllable. Our key insight is that by bounding the irradiance contributed by each triangle tuple at a given position, we can sample a subset of triangle tuples with potentially high contributions. Although low-contribution tuples are assigned a negligible probability, the overall variance remains low. Therefore, we derive position and irradiance bounds for caustics casted by each triangle tuple, introducing a bounding property of rational functions on a Bernstein basis. When formulating position and irradiance expressions into rational functions, we handle non-rational parts through remainder variables to maintain bounding validity. Finally, we carefully design the sampling probabilities by optimizing the upper bound of the variance, expressed only using the position and irradiance bounds. The bound-driven sampling of triangle tuples is intrinsically unbiased even without defensive sampling. It can be combined with various unbiased and biased root-finding techniques within a local triangle domain. Extensive evaluations show that our method enables the fast and reliable rendering of complex caustics effects. Yet, our method is efficient for no more than two specular vertices, where complexity grows sublinearly to the number of triangles and linearly to that of emitters, and does not consider the Fresnel and visibility terms. We also rely on parameters to control subdivisions.},
  archive      = {J_TOG},
  author       = {Zhimin Fan and Chen Wang and Yiming Wang and Boxuan Li and Yuxuan Guo and Ling-Qi Yan and Yanwen Guo and Jie Guo},
  doi          = {10.1145/3731145},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Bernstein bounds for caustics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quadric-based silhouette sampling for differentiable rendering. <em>TOG</em>, <em>44</em>(4), 1-20. (<a href='https://doi.org/10.1145/3731146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically based differentiable rendering has established itself as key to inverse rendering, in which scenes are recovered from images through gradient-based optimization. Taking the derivative of the rendering equation is made difficult by the presence of discontinuities in the integrand at object silhouettes. To obtain correct derivatives w.r.t. changing geometry, accounting e.g. for changing penumbras or silhouettes in glossy reflections, differentiable renderers must compute an integral over these silhouettes. Prior work proposed importance sampling of silhouette edges for a given shading point. The main challenge is to efficiently reject parts of the mesh without silhouettes during sampling, which has been done using top-down traversal of a tree. Inaccuracies of this existing rejection procedure result in many samples with zero contribution. Thus, variance remains high and subsequent work has focused on alternatives such as area sampling or path space differentiable rendering. We propose an improved rejection test. It reduces variance substantially, which makes edge sampling in a unidirectional path tracer competitive again. Our rejection test relies on two approximations to the triangle planes of a mesh patch: A bounding box in dual space and dual quadrics. Additionally, we improve the heuristics used for stochastic traversal of the tree. We evaluate our method in a unidirectional path tracer and achieve drastic improvements over the original edge sampling and outperform methods based on area sampling.},
  archive      = {J_TOG},
  author       = {Mariia Soroka and Christoph Peters and Steve Marschner},
  doi          = {10.1145/3731146},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Quadric-based silhouette sampling for differentiable rendering},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear-time transport with rectified flows. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3731147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching probability distributions allows to compare or interpolate them, or model their manifold. Optimal transport is a tool that solves this matching problem. However, despite the development of numerous exact and approximate algorithms, these approaches remain too slow for large datasets due to the inherent challenge of optimizing transport plans. Taking intuitions from recent advances in rectified flows we propose an algorithm that, while not resulting in optimal transport plans, produces transport plans from uniform densities to densities stored on grids that resemble the optimal ones in practice. Our algorithm has linear-time complexity with respect to the problem size and is embarrassingly parallel. It is also trivial to implement, essentially computing three summed-area tables and advecting particles with velocities easily computed from these tables using simple arithmetic. This already allows for applications such as stippling and area-preserving mesh parameterization. Combined with linearized transport ideas, we further extend our approach to match two non-uniform distributions. This allows for wider applications such as shape interpolation or barycenters, matching the quality of more complex optimal or approximate transport solvers while resulting in orders of magnitude speedups. We illustrate our applications in 2D and 3D.},
  archive      = {J_TOG},
  author       = {Khoa Do and David Coeurjolly and Pooran Memari and Nicolas Bonneel},
  doi          = {10.1145/3731147},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Linear-time transport with rectified flows},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shape space spectra. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3731148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eigenanalysis of differential operators, such as the Laplace operator or elastic energy Hessian, is typically restricted to a single shape and its discretization, limiting reduced order modeling (ROM). We introduce the first eigenanalysis method for continuously parameterized shape families. Given a parametric shape, our method constructs spatial neural fields that represent eigen-functions across the entire shape space. It is agnostic to the specific shape representation, requiring only an inside/outside indicator function that depends on shape parameters. Eigenfunctions are computed by minimizing a variational principle over nested spaces with orthogonality constraints. Since eigenvalues may swap dominance at points of multiplicity, we jointly train multiple eigenfunctions while dynamically reordering them based on their eigenvalues at each step. Through causal gradient filtering , this reordering is reflected in backpropagation. Our method enables applications to operate over shape space, providing a single ROM that encapsulates vibration modes for all shapes, including previously unseen ones. Since our eigenanalysis is differentiable with respect to shape parameters, it facilitates eigenfunction-aware shape optimization. We evaluate our approach on shape optimization for sound synthesis and locomotion, as well as reduced-order modeling for elastodynamic simulation.},
  archive      = {J_TOG},
  author       = {Yue Chang and Otman Benchekroun and Maurizio M. Chiaramonte and Peter Yichen Chen and Eitan Grinspun},
  doi          = {10.1145/3731148},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Shape space spectra},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RigAnything: Template-free autoregressive rigging for diverse 3D assets. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present RigAnything , a novel autoregressive transformer-based model, which makes 3D assets rig-ready by probabilistically generating joints and skeleton topologies and assigning skinning weights in a template-free manner. Unlike most existing auto-rigging methods, which rely on predefined skeleton templates and are limited to specific categories like humanoid, RigAnything approaches the rigging problem in an autoregressive manner, iteratively predicting the next joint based on the global input shape and the previous prediction. While autoregressive models are typically used to generate sequential data, RigAnything extends its application to effectively learn and represent skeletons, which are inherently tree structures. To achieve this, we organize the joints in a breadth-first search (BFS) order, enabling the skeleton to be defined as a sequence of 3D locations and the parent index. Furthermore, our model improves the accuracy of position prediction by leveraging diffusion modeling, ensuring precise and consistent placement of joints within the hierarchy. This formulation allows the autoregressive model to efficiently capture both spatial and hierarchical relationships within the skeleton. Trained end-to-end on both RigNet and Objaverse datasets, RigAnything demonstrates state-of-the-art performance across diverse object types, including humanoids, quadrupeds, marine creatures, insects, and many more, surpassing prior methods in quality, robustness, generalizability, and efficiency. It achieves significantly faster performance than existing auto-rigging methods, completing rigging in under a few seconds per shape.},
  archive      = {J_TOG},
  author       = {Isabella Liu and Zhan Xu and Wang Yifan and Hao Tan and Zexiang Xu and Xiaolong Wang and Hao Su and Zifan Shi},
  doi          = {10.1145/3731149},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {RigAnything: Template-free autoregressive rigging for diverse 3D assets},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A monte carlo rendering framework for simulating optical heterodyne detection. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3731150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical heterodyne detection (OHD) employs coherent light and optical interference techniques (Fig. 1-(A)) to extract physical parameters, such as velocity or distance, which are encoded in the frequency modulation of the light. With its superior signal-to-noise ratio compared to incoherent detection methods, such as time-of-flight lidar, OHD has become integral to applications requiring high sensitivity, including autonomous navigation, atmospheric sensing, and biomedical velocimetry. However, current simulation tools for OHD focus narrowly on specific applications, relying on domain-specific settings like restricted reflection functions, scene configurations, or single-bounce assumptions, which limit their applicability. In this work, we introduce a flexible and general framework for spectral-domain simulation of OHD. We demonstrate that classical radiometry-based path integral formulation can be adapted and extended to simulate the OHD measurements in the spectral domain. This enables us to leverage the rich modeling and sampling capabilities of existing Monte Carlo path tracing techniques. Our formulation shares structural similarities with transient rendering but operates in the spectral domain and accounts for the Doppler effect (Fig. 1-(B)). While simulators for the Doppler effect in incoherent (intensity) detection methods exist, they are largely not suitable to simulate OHD. We use a microsurface interpretation to show that these two Doppler imaging techniques capture different physical quantities and thus need different simulation frameworks. We validate the correctness and predictive power of our simulation framework by qualitatively comparing the simulations with real-world captured data for three different OHD applications—FMCW lidar, blood flow velocimetry, and wind Doppler lidar (Fig. 1-(C)).},
  archive      = {J_TOG},
  author       = {Juhyeon Kim and Craig Benko and Magnus Wrenninge and Ryusuke Villemin and Zeb Barber and Wojciech Jarosz and Adithya Pediredla},
  doi          = {10.1145/3731150},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {A monte carlo rendering framework for simulating optical heterodyne detection},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). C5D: Sequential continuous convex collision detection using cone casting. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3731151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In physics-based simulation of rigid or nearly rigid objects, collisions often become the primary performance bottleneck, particularly when enforcing intersection-free constraints. Previous simulation frameworks rely on primitive-level CCD algorithms. Due to the large number of colliding surface primitives to process, those methods are computationally intensive and heavily dependent on advanced parallel computing resources such as GPUs, which are often inaccessible due to competing tasks or capped threading capacity in applications like policy training for robotics. To address these limitations, we propose a sequential CCD algorithm for convex shapes undergoing constant affine motion. This approach uses the conservative advancement method to iteratively refine a lower-bound estimate of the TOI, exploiting the linearity of affine motion and the efficiency of convex shape distance computation. Our CCD algorithm integrates seamlessly into the ABD framework, achieving a 10-fold speed-up over primitive-level CCD. Its high single-threaded efficiency further enables significant throughput improvements via scene-level parallelism, making it well-suited for resource-constrained environments.},
  archive      = {J_TOG},
  author       = {Xiaodi Yuan and Fanbo Xiang and Yin Yang and Hao Su},
  doi          = {10.1145/3731151},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {C5D: Sequential continuous convex collision detection using cone casting},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving partial differential equations in participating media. <em>TOG</em>, <em>44</em>(4), 1-21. (<a href='https://doi.org/10.1145/3731152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of solving partial differential equations (PDEs) in domains with complex microparticle geometry that is impractical, or intractable, to model explicitly. Drawing inspiration from volume rendering, we propose tackling this problem by treating the domain as a participating medium that models microparticle geometry stochastically , through aggregate statistical properties (e.g., particle density). We first introduce the problem setting of PDE simulation in participating media. We then specialize to exponential media and describe the properties that make them an attractive model of microparticle geometry for PDE simulation problems. We use these properties to develop two new algorithms, volumetric walk on spheres and volumetric walk on stars , that generalize previous Monte Carlo algorithms to enable efficient and discretization-free simulation of linear elliptic PDEs (e.g., Laplace) in participating media. We demonstrate experimentally that our algorithms can solve Laplace boundary value problems with complex microparticle geometry more accurately and more efficiently than previous approaches, such as ensemble averaging and homogenization.},
  archive      = {J_TOG},
  author       = {Bailey Miller and Rohan Sawhney and Keenan Crane and Ioannis Gkioulekas},
  doi          = {10.1145/3731152},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Solving partial differential equations in participating media},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VR-doh: Hands-on 3D modeling in virtual reality. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce VR-Doh, an open-source, hands-on 3D modeling system that enables intuitive creation and manipulation of elastoplastic objects in Virtual Reality (VR). By customizing the Material Point Method (MPM) for real-time simulation of hand-induced large deformations and enhancing 3D Gaussian Splatting for seamless rendering, VR-Doh provides an interactive and immersive 3D modeling experience. Users can naturally sculpt, deform, and edit objects through both contact- and gesture-based hand-object interactions. To achieve real-time performance, our system incorporates localized simulation techniques, particle-level collision handling, and the decoupling of physical and appearance representations, ensuring smooth and responsive interactions. VR-Doh supports both object creation and editing, enabling diverse modeling tasks such as designing food items, characters, and interlocking structures, all resulting in simulation-ready assets. User studies with both novice and experienced participants highlight the system's intuitive design, immersive feedback, and creative potential. Compared to existing geometric modeling tools, VR-Doh offers enhanced accessibility and natural interaction, making it a powerful tool for creative exploration in VR.},
  archive      = {J_TOG},
  author       = {Zhaofeng Luo and Zhitong Cui and Shijian Luo and Mengyu Chu and Minchen Li},
  doi          = {10.1145/3731154},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {VR-doh: Hands-on 3D modeling in virtual reality},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CK-MPM: A compact-kernel material point method. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3731155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Material Point Method (MPM) has become a cornerstone of physics-based simulation, widely used in geomechanics and computer graphics for modeling phenomena such as granular flows, viscoelasticity, fracture mechanics, etc. Despite its versatility, the original MPM suffers from cell-crossing instabilities caused by discontinuities in particle-grid transfer kernels. Existing solutions mostly mitigate these issues by adopting smoother shape functions, but at the cost of increased numerical diffusion and computational overhead due to larger kernel support. In this paper, we propose a novel C 2 -continuous compact kernel for MPM that achieves a unique balance in terms of stability, accuracy, and computational efficiency. Our method integrates seamlessly with Affine Particle-In-Cell (APIC) and Moving Least Squares (MLS) MPM, while only doubling the number of grid nodes associated with each particle compared to linear kernels. At its core is an innovative dual-grid framework, which associates particles with grid nodes exclusively within the cells they occupy on two staggered grids, ensuring consistent and stable force computations. We demonstrate that our method can be conveniently implemented using a domain-specific language, Taichi, or based on open-source GPU MPM frameworks, achieving faster runtime and less numerical diffusion compared to quadratic B-spline MPM. Comprehensive validation through unit tests, comparative studies, and stress tests demonstrates the efficacy of our approach in conserving both linear and angular momentum, handling stiff materials, and scaling efficiently for large-scale simulations. Our results highlight the transformative potential of compact, high-order kernels in advancing MPM's capabilities for stable, accurate, and high-performance simulations.},
  archive      = {J_TOG},
  author       = {Michael Liu and Xinlei Wang and Minchen Li},
  doi          = {10.1145/3731155},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {CK-MPM: A compact-kernel material point method},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topological offsets. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3731157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Topological Offsets , a novel approach to generate manifold and self-intersection-free offset surfaces that are topologically equivalent to an offset infinitesimally close to the surface. Our approach, by construction, creates a manifold, watertight, and self-intersection-free offset surface strictly enclosing the input, while doing a best effort to move it to a prescribed distance from the input. Differently from existing approaches, we embed the input in a background mesh and insert a topological offset around the input with purely combinatorial operations. The topological offset is then inflated/deflated to match the user-prescribed distance while enforcing that no intersections or non-manifold configurations are introduced. We evaluate the effectiveness and robustness of our approach on the Thingi10k dataset, and show that topological offsets are beneficial in multiple graphics applications, including (1) converting non-manifold surfaces to manifold ones, (2) creating layered offsets, and (3) reliably computing finite offsets.},
  archive      = {J_TOG},
  author       = {Daniel Zint and Zhouyuan Chen and Yifei Zhu and Denis Zorin and Teseo Schneider and Daniele Panozzo},
  doi          = {10.1145/3731157},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Topological offsets},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AlignTex: Pixel-precise texture generation from multi-view artwork. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current 3D asset creation pipelines typically consist of three stages: creating multi-view concept art, producing 3D meshes based on the artwork, and painting textures for the meshes—an often labor-intensive process. Automated texture generation offers significant acceleration, but prior methods, which fine-tune 2D diffusion models with multi-view input images, often fail to preserve pixel-level details. These methods primarily emphasize semantic and subject consistency, which do not meet the requirements of artwork-guided texture workflows. To address this, we present AlignTex , a novel framework for generating high-quality textures from 3D meshes and multi-view artwork, ensuring both appearance detail and geometric consistency. AlignTex operates in two stages: aligned image generation and texture refinement. The core of our approach, AlignNet , resolves complex misalignments by extracting information from both the artwork and the mesh, generating images compatible with orthographic projection while maintaining geometric and visual fidelity. After projecting aligned images into the texture space, further refinement addresses seams and self-occlusion using an inpainting model and a geometry-aware texture dilation method. Experimental results demonstrate that AlignTex outperforms baseline methods in generation quality and efficiency, offering a practical solution to enhance 3D asset creation in gaming and film production.},
  archive      = {J_TOG},
  author       = {Yuqing Zhang and Hao Xu and Yiqian Wu and Sirui Chen and Sirui Lin and Xiang Li and Xifeng Gao and Xiaogang Jin},
  doi          = {10.1145/3731158},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {AlignTex: Pixel-precise texture generation from multi-view artwork},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeurCross: A neural approach to computing cross fields for quad mesh generation. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3731159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quadrilateral mesh generation plays a crucial role in numerical simulations within Computer-Aided Design and Engineering (CAD/E). Producing high-quality quadrangulation typically requires satisfying four key criteria. First, the quadrilateral mesh should closely align with principal curvature directions. Second, singular points should be strategically placed and effectively minimized. Third, the mesh should accurately conform to sharp feature edges. Lastly, quadrangulation results should exhibit robustness against noise and minor geometric variations. Existing methods generally involve first computing a regular cross field to represent quad element orientations across the surface, followed by extracting a quadrilateral mesh aligned closely with this cross field. A primary challenge with this approach is balancing the smoothness of the cross field with its alignment to pre-computed principal curvature directions, which are sensitive to small surface perturbations and often ill-defined in spherical or planar regions. To tackle this challenge, we propose NeurCross , a novel framework that simultaneously optimizes a cross field and a neural signed distance function (SDF), whose zero-level set serves as a proxy of the input shape. Our joint optimization is guided by three factors: faithful approximation of the optimized SDF surface to the input surface, alignment between the cross field and the principal curvature field derived from the SDF surface, and smoothness of the cross field. Acting as an intermediary, the neural SDF contributes in two essential ways. First, it provides an alternative, optimizable base surface exhibiting more regular principal curvature directions for guiding the cross field. Second, we leverage the Hessian matrix of the neural SDF to implicitly enforce cross field alignment with principal curvature directions, thus eliminating the need for explicit curvature extraction. Extensive experiments demonstrate that NeurCross outperforms the state-of-the-art methods in terms of singular point placement, robustness against surface noise and surface undulations, and alignment with principal curvature directions and sharp feature curves.},
  archive      = {J_TOG},
  author       = {Qiujie Dong and Huibiao Wen and Rui Xu and Shuangmin Chen and Jiaran Zhou and Shiqing Xin and Changhe Tu and Taku Komura and Wenping Wang},
  doi          = {10.1145/3731159},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeurCross: A neural approach to computing cross fields for quad mesh generation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating past and future in digital painting processes. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3731160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework to generate past and future processes for drawing process videos. Given a canvas image uploaded by a user, the framework can generate both preceding and succeeding states of the drawing process, and the generated states can be reused as inputs for further state generation. We observe that the user queries typically have one-to-one or many-to-many states, and in many cases, involve non-contiguous states. This necessitates a backend that solves a set-to-set problem with arbitrary combinations of past or future states. To this end, we repurpose video diffusion models to learn the set-to-set mapping with pretrained video priors. We implement the system with strong diffusion transformer backbones ( e.g. , CogVideoX and LTXVideo) and high-quality data processing ( e.g. , sampling short shots from long videos of real drawing records). Experiments show that the generated states are diverse in drawing contexts and resemble human drawing processes. This capability may aid artists in visualizing potential outcomes, generating creative inspirations, or refining existing workflows.},
  archive      = {J_TOG},
  author       = {Lvmin Zhang and Chuan Yan and Yuwei Guo and Jinbo Xing and Maneesh Agrawala},
  doi          = {10.1145/3731160},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generating past and future in digital painting processes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic preconditioning for neural field optimization. <em>TOG</em>, <em>44</em>(4), 1-10. (<a href='https://doi.org/10.1145/3731161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural fields are a highly effective representation across visual computing. This work observes that fitting these fields is greatly improved by incorporating spatial stochasticity during training, and that this simple technique can replace or even outperform custom-designed hierarchies and frequency-space constructions. The approach is formalized as implicitly operating on a blurred version of the field, evaluated in-expectation by sampling with Gaussian-distributed offsets. Querying the blurred field during optimization greatly improves convergence and robustness, akin to the role of preconditioners in numerical linear algebra. This implicit, sampling-based perspective fits naturally into the neural field paradigm, comes at no additional cost, and is extremely simple to implement. We describe the basic theory of this technique, including details such as handling boundary conditions, and extending to a spatially-varying blur. Experiments demonstrate this approach on representations including coordinate MLPs, neural hashgrids, triplanes, and more, across tasks including surface reconstruction and radiance fields. In settings where custom-designed hierarchies have already been developed, stochastic preconditioning nearly matches or improves their performance with a simple and unified approach; in settings without existing hierarchies it provides an immediate boost to quality and robustness.},
  archive      = {J_TOG},
  author       = {Selena Ling and Merlin Nimier-David and Alec Jacobson and Nicholas Sharp},
  doi          = {10.1145/3731161},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Stochastic preconditioning for neural field optimization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic mesh processing on the GPU. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3731162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a system for dynamic triangle mesh processing entirely on the GPU. Our system features an efficient data structure that enables rapid updates to mesh connectivity and attributes. By partitioning the mesh into small patches, we process all dynamic updates for each patch within the GPU's fast shared memory. This approach leverages speculative processing for conflict handling, minimizing rollback costs, maximizing parallelism, and reducing locking overhead. Additionally, we introduce a new programming model for dynamic mesh processing. This model provides concise semantics for dynamic updates, abstracting away concerns about conflicting updates during parallel execution. At the core of our model is the cavity operator , a general mesh update operator that facilitates any dynamic operation by removing a set of mesh elements and inserting new ones into the resulting void. We applied our system to various GPU applications, including isotropic remeshing, surface tracking, mesh decimation, and Delaunay edge flips. On large inputs, our system achieves an order-of-magnitude speedup compared to multi-threaded CPU solutions and is more than two orders of magnitude faster than state-of-the-art single-threaded CPU solutions. Furthermore, our data structure outperforms state-of-the-art GPU static data structures in terms of both speed and memory efficiency.},
  archive      = {J_TOG},
  author       = {Ahmed H. Mahmoud and Serban D. Porumbescu and John D. Owens},
  doi          = {10.1145/3731162},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dynamic mesh processing on the GPU},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian wave splatting for computer-generated holography. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3731163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art neural rendering methods optimize Gaussian scene representations from a few photographs for novel-view synthesis. Building on these representations, we develop an efficient algorithm, dubbed Gaussian Wave Splatting, to turn these Gaussians into holograms. Unlike existing computergenerated holography (CGH) algorithms, Gaussian Wave Splatting supports accurate occlusions and view-dependent effects for photorealistic scenes by leveraging recent advances in neural rendering. Specifically, we derive a closed-form solution for a 2D Gaussian-to-hologram transform that supports occlusions and alpha blending. Inspired by classic computer graphics techniques, we also derive an efficient approximation of the aforementioned process in the Fourier domain that is easily parallelizable and implement it using custom CUDA kernels. By integrating emerging neural rendering pipelines with holographic display technology, our Gaussian-based CGH framework paves the way for next-generation holographic displays.},
  archive      = {J_TOG},
  author       = {Suyeon Choi and Brian Chao and Jacqueline Yang and Manu Gopakumar and Gordon Wetzstein},
  doi          = {10.1145/3731163},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Gaussian wave splatting for computer-generated holography},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GenAnalysis: Joint shape analysis by learning man-made shape generators with deformation regularizations. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3731164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present GenAnalysis, an implicit shape generation framework that allows joint analysis of man-made shapes, including shape matching and joint shape segmentation. The key idea is to enforce an as-affine-as-possible (AAAP) deformation between synthetic shapes of the implicit generator that are close to each other in the latent space, which we achieve by designing a regularization loss. It allows us to understand the shape variation of each shape in the context of neighboring shapes and also offers structure-preserving interpolations between the input shapes. We show how to extract these shape variations by recovering piecewise affine vector fields in the tangent space of each shape. These vector fields provide single-shape segmentation cues. We then derive shape correspondences by iteratively propagating AAAP deformations across a sequence of intermediate shapes. These correspondences are then used to aggregate single-shape segmentation cues into consistent segmentations. We conduct experiments on the ShapeNet dataset to show superior performance in shape matching and joint shape segmentation over previous methods.},
  archive      = {J_TOG},
  author       = {Yuezhi Yang and Haitao Yang and Kiyohiro Nakayama and Xiangru Huang and Leonidas Guibas and Qixing Huang},
  doi          = {10.1145/3731164},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {GenAnalysis: Joint shape analysis by learning man-made shape generators with deformation regularizations},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interspatial attention for efficient 4D human video generation. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3731165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating photorealistic videos of digital humans in a controllable manner is crucial for a plethora of applications. Existing approaches either build on methods that employ template-based 3D representations or emerging video generation models but suffer from poor quality or limited consistency and identity preservation when generating individual or multiple digital humans. In this paper, we introduce a new interspatial attention (ISA) mechanism as a scalable building block for modern diffusion transformer (DiT)-based video generation models. ISA is a new type of cross attention that uses relative positional encodings tailored for the generation of human videos. Leveraging a custom-developed video variation autoencoder, we train a latent ISA-based diffusion model on a large corpus of video data. Our model achieves state-of-the-art performance for 4D human video synthesis, demonstrating remarkable motion consistency and identity preservation while providing precise control of the camera and body poses. Our code and model are publicly released at https://dsaurus.github.io/isa4d/.},
  archive      = {J_TOG},
  author       = {Ruizhi Shao and Yinghao Xu and Yujun Shen and Ceyuan Yang and Yang Zheng and Changan Chen and Yebin Liu and Gordon Wetzstein},
  doi          = {10.1145/3731165},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interspatial attention for efficient 4D human video generation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeFillet: Detection and removal of fillet regions in polygonal CAD models. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3731166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Filleting is a fundamental operation in CAD systems, akin to a ball rolling between two adjacent surface patches, resulting in a seamless connection. The reverse process, which we refer to as DeFillet in this paper, is crucial for CAE analysis and secondary design phases. However, it presents significant challenges, particularly when the input data originates from surface reconstruction or discretization processes. Our DeFillet algorithm is inspired by the observation that the rolling-ball center defines an osculating sphere, while the Voronoi diagram of surface samples provides sufficiently many rolling-ball center candidates. By leveraging this insight, we compute a transformation between the Voronoi vertices and the surface samples, enabling the efficient identification of fillet regions. Subsequently, we formulate the reconstruction of sharp features as a quadratic optimization problem. Our method's effectiveness has been validated through extensive testing using self-constructed models and 100 filleted models selected from the Fusion 360 Gallery dataset. The code for this paper is publicly available at https://github.com/xiaowuga/DeFillet.},
  archive      = {J_TOG},
  author       = {Jing-En Jiang and Hanxiao Wang and Mingyang Zhao and Dong-Ming Yan and Shuangmin Chen and Shiqing Xin and Changhe Tu and Wenping Wang},
  doi          = {10.1145/3731166},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeFillet: Detection and removal of fillet regions in polygonal CAD models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sketch2Anim: Towards transferring sketch storyboards into 3D animation. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3731167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storyboarding is widely used for creating 3D animations. Animators use the 2D sketches in storyboards as references to craft the desired 3D animations through a trial-and-error process. The traditional approach requires exceptional expertise and is both labor-intensive and time-consuming. Consequently, there is a high demand for automated methods that can directly translate 2D storyboard sketches into 3D animations. This task is under-explored to date and inspired by the significant advancements of motion diffusion models, we propose to address it from the perspective of conditional motion synthesis. We thus present Sketch2Anim , composed of two key modules for sketch constraint understanding and motion generation. Specifically, due to the large domain gap between the 2D sketch and 3D motion, instead of directly conditioning on 2D inputs, we design a 3D conditional motion generator that simultaneously leverages 3D keyposes, joint trajectories, and action words, to achieve precise and fine-grained motion control. Then, we invent a neural mapper dedicated to aligning user-provided 2D sketches with their corresponding 3D keyposes and trajectories in a shared embedding space, enabling, for the first time , direct 2D control of motion generation. Our approach successfully transfers storyboards into high-quality 3D motions and inherently supports direct 3D animation editing, thanks to the flexibility of our multi-conditional motion generator. Comprehensive experiments and evaluations, and a user perceptual study demonstrate the effectiveness of our approach. The code, data, trained models, and sketch-based motion designing interface are at https://zhongleilz.github.io/Sketch2Anim/.},
  archive      = {J_TOG},
  author       = {Lei Zhong and Chuan Guo and Yiming Xie and Jiawei Wang and Changjian Li},
  doi          = {10.1145/3731167},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sketch2Anim: Towards transferring sketch storyboards into 3D animation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming unstructured hair strands into procedural hair grooms. <em>TOG</em>, <em>44</em>(4), 1-20. (<a href='https://doi.org/10.1145/3731168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, reconstruction methods have been developed that can recover strand-level hair geometry from images. However, these methods recover a vast number of individual hair strands that are difficult to edit and simulate. Many methods also rely on neural priors to infer non-visible inner hair, which can result in poor inner hair structure for complex hairstyles, such as curly hair. We propose an inverse hair grooming pipeline that transforms the imperfect 3D strands from these reconstruction methods into procedural hair grooms that consist of a small set of guide strands and hair grooming operators, inspired by pipelines used by artists in popular 3D modeling tools such as Blender and Houdini. We take a probabilistic view of these hair grooms and design various optimization strategies and loss functions to optimize for the guide strands and operator parameters. Due to the proceduralism, our resulting grooms can naturally represent challenging hairstyles, have structurally sound inner hair, and are easily editable.},
  archive      = {J_TOG},
  author       = {Wesley Chang and Andrew L. Russell and Stephane Grabli and Matt Jen-Yuan Chiang and Christophe Hery and Doug Roble and Ravi Ramamoorthi and Tzu-Mao Li and Olivier Maury},
  doi          = {10.1145/3731168},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Transforming unstructured hair strands into procedural hair grooms},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controllable complex freezing dynamics simulation on thin films. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The freezing of thin films is a mesmerizing natural phenomenon, inspiring photographers to capture its beauty through their lenses and digital artists to recreate its allure using effects tools. In this paper, we present a novel method for physically simulating the intricate freezing dynamics on thin films. By accounting for the influence of phase and temperature changes on surface tension, our method reproduces Marangoni freezing and the "Snow-Globe Effect", characterized by swirling ice dendrites on the film. We introduce a novel Phase Map method on top of the state-of-the-art Moving Eulerian-Lagrangian Particles (MELP) meshless framework, enabling dendritic crystal simulation on mobile particles and offering precise control over freezing patterns. We demonstrate that our method is able to capture a wide range of dynamic freezing processes of soap bubbles and is stable for complex boundaries in our experiments.},
  archive      = {J_TOG},
  author       = {Yijie Liu and Taiyuan Zhang and Xiaoxiao Yan and Nuoming Liu and Bo Ren},
  doi          = {10.1145/3731170},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Controllable complex freezing dynamics simulation on thin films},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MyTimeMachine: Personalized facial age transformation. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3731172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20~40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), a method that combines a global aging prior with a personalized photo collection (ranging from as few as 10 images, ideally 50) to learn individualized age transformations. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our method demonstrates strong performance on fair-use imagery of widely recognizable individuals, producing photorealistic and identity-consistent age transformations that generalize well across diverse appearances. It also extends naturally to video, delivering high-quality, temporally consistent results that closely resemble actual appearances at target ages—outperforming state-of-the-art approaches.},
  archive      = {J_TOG},
  author       = {Luchao Qi and Jiaye Wu and Bang Gong and Annie N. Wang and David W. Jacobs and Roni Sengupta},
  doi          = {10.1145/3731172},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {MyTimeMachine: Personalized facial age transformation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IntrinsicEdit: Precise generative image manipulation in intrinsic space. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3731173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative diffusion models have advanced image editing by delivering high-quality results through intuitive interfaces such as prompts, scribbles, and semantic drawing. However, these interfaces lack precise control, and associated editing methods often specialize in a single task. We introduce a versatile workflow for a range of editing tasks which operates in an intrinsic-image latent space, enabling semantic, local manipulation with pixel precision while automatically handling effects like reflections and shadows. We build on the RGB↔X diffusion framework and address its key deficiencies: the lack of identity preservation and the need to update multiple channels to achieve plausible results. We propose an edit-friendly diffusion inversion and prompt-embedding optimization to enable precise and efficient editing of only the relevant channels. Our method achieves identity preservation and resolves global illumination, without requiring task-specific model fine-tuning. We demonstrate state-of-the-art performance across a variety of tasks on complex images, including material adjustments, object insertion and removal, global relighting, and their combinations.},
  archive      = {J_TOG},
  author       = {Linjie Lyu and Valentin Deschaintre and Yannick Hold-Geoffroy and Miloš Hašan and Jae Shin Yoon and Thomas Leimkühler and Christian Theobalt and Iliyan Georgiev},
  doi          = {10.1145/3731173},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {IntrinsicEdit: Precise generative image manipulation in intrinsic space},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased differential visibility using fixed-step walk-on-spherical-caps and closest silhouettes. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3731174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing derivatives of path integrals under evolving scene geometry is a fundamental problem in physics-based differentiable rendering, which requires differentiating discontinuities in the visibility function. Warped-area reparameterization is a powerful technique to compute differential visibility, and key is construction of a velocity field that is continuous in the domain interior and agrees with defined velocities on boundaries. Robustly and efficiently constructing such fields remains challenging. We present a novel velocity field construction for differential visibility. Inspired by recent Monte Carlo solvers for partial differential equations (PDEs), we formulate the velocity field via Laplace's equation and solve it with a walk-on-spheres (WoS) algorithm. To improve efficiency, we introduce a fixed-step WoS that terminates random walks after a fixed step count, resulting in a continuous but non-harmonic velocity field still valid for warped-area reparameterization. Furthermore, to practically apply our method to complex 3D scenes, we propose an efficient cone query to find the closest silhouettes on a boundary. Our cone query finds the closest point under the geodesic distance on a unit sphere, and is analogous to the closest point query by WoS to compute Euclidean distance. As a result, our method generalizes WoS to perform random walks on spherical caps over the unit sphere. We demonstrate that this enables a more robust and efficient unbiased estimator for differential visibility.},
  archive      = {J_TOG},
  author       = {Lifan Wu and Nathan Morrical and Sai Praveen Bangaru and Rohan Sawhney and Shuang Zhao and Chris Wyman and Ravi Ramamoorthi and Aaron Lefohn},
  doi          = {10.1145/3731174},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unbiased differential visibility using fixed-step walk-on-spherical-caps and closest silhouettes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vector-valued monte carlo integration using ratio control variates. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3731175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variance reduction techniques are widely used for reducing the noise of Monte Carlo integration. However, these techniques are typically designed with the assumption that the integrand is scalar-valued. Recognizing that rendering and inverse rendering broadly involve vector-valued integrands, we identify the limitations of classical variance reduction methods in this context. To address this, we introduce ratio control variates, an estimator that leverages a ratio-based approach instead of the conventional difference-based control variates. Our analysis and experiments demonstrate that ratio control variables can significantly reduce the mean squared error of vector-valued integration compared to existing methods and are broadly applicable to various rendering and inverse rendering tasks.},
  archive      = {J_TOG},
  author       = {Haolin Lu and Delio Vicini and Wesley Chang and Tzu-Mao Li},
  doi          = {10.1145/3731175},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Vector-valued monte carlo integration using ratio control variates},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rectangular surface parameterization. <em>TOG</em>, <em>44</em>(4), 1-21. (<a href='https://doi.org/10.1145/3731176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a method for computing surface parameterizations that map infinitesimal axis-aligned squares in the plane to infinitesimal rectangles on the surface. Such rectangular parameterizations are needed for a broad range of tasks, from physical simulation to geometric modeling to computational fabrication. Our main contribution is a novel strategy for constructing frame fields that are perfectly orthogonal and exactly integrable, in the limit of mesh refinement. In contrast to past strategies for achieving integrability, we obtain maps that are less distorted and better preserve target field directions. The method supports user-defined distortion measures, sharp feature alignment, prescribed or automatic cone singularities, and direct control over boundary behavior (e.g., sizing or aspect ratio). By quantizing and contouring these maps we obtain high-quality anisotropic quad meshes, even without element-based optimization. Empirically, we outperform state-of-the-art research and commercial mesh generation algorithms in terms of element quality, accuracy, and asymptotic convergence rate in end-to-end simulation tasks, are competitive with the widely-used ZBrush package for automatic retopology, and provide Chebyshev nets of superior quality to methods specifically tailored to digital fabrication.},
  archive      = {J_TOG},
  author       = {Etienne Corman and Keenan Crane},
  doi          = {10.1145/3731176},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rectangular surface parameterization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dress-1-to-3: Single image to simulation-ready 3D outfit with diffusion prior and differentiable physics. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3731177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in large models have significantly advanced image-to-3D reconstruction. However, the generated models are often fused into a single piece, limiting their applicability in downstream tasks. This paper focuses on 3D garment generation, a key area for applications like virtual try-on with dynamic garment animations, which require garments to be separable and simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs physics-plausible, simulation-ready separated garments with sewing patterns and humans from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model for creating coarse sewing patterns with a pre-trained multi-view diffusion model to produce multi-view images. The sewing pattern is further refined using a differentiable garment simulator based on the generated multi-view images. Versatile experiments demonstrate that our optimization approach substantially enhances the geometric alignment of the reconstructed 3D garments and humans with the input image. Furthermore, by integrating a texture generation module and a human motion generation module, we produce customized physics-plausible and realistic dynamic garment demonstrations. Our project page is https://dress-1-to-3.github.io/.},
  archive      = {J_TOG},
  author       = {Xuan Li and Chang Yu and Wenxin Du and Ying Jiang and Tianyi Xie and Yunuo Chen and Yin Yang and Chenfanfu Jiang},
  doi          = {10.1145/3731177},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dress-1-to-3: Single image to simulation-ready 3D outfit with diffusion prior and differentiable physics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting fabric appearance through thread scattering and inversion. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fashion industry has a real need to preview fabric designs using the actual threads they intend to use, ensuring that the designs they envisage can be physically realized. Unfortunately, today's fabric rendering relies on either hand-tuned parameters or parameters acquired from already fabricated cloth. Furthermore, existing curve-based scattering models are not suitable for this problem: they are either not naturally differentiable due to discrete fiber count parameters, or require a more detailed geometry representation, introducing extra complexity. In this work, we bridge this gap by presenting a novel pipeline that captures and digitizes physical threads and predicts the appearance of the fabric based on the weaving pattern. We develop a practical thread scattering model based on simulations of multiple fiber scattering within a thread. Using a cost-efficient multi-view setup, we capture threads of diverse colors and materials. We apply differentiable rendering to digitize threads, demonstrating that our model significantly improves the reconstruction accuracy compared to existing models, matching both reflection and transmission. We leverage a two-scale rendering technique to efficiently render woven cloth. We validate that our digital threads, combined with simulated woven yarn geometry, can accurately predict the fabric appearance by comparing to real samples. We show how our work can aid designs using diverse thread profiles, woven patterns, and textured design patterns.},
  archive      = {J_TOG},
  author       = {Mengqi (Mandy) Xia and Zhaoyang Zhang and Sumit Chaturvedi and Yutong Yi and Rundong Wu and Holly Rushmeier and Julie Dorsey},
  doi          = {10.1145/3731178},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Predicting fabric appearance through thread scattering and inversion},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive algebraic reuse of reordering in cholesky factorizations with dynamic sparsity patterns. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3731179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Parth, a fill-reducing ordering method for sparse Cholesky solvers with dynamic sparsity patterns (e.g., in physics simulations with contact or geometry processing with local remeshing). Parth facilitates the selective reuse of fill-reducing orderings when sparsity patterns exhibit temporal coherence, avoiding full symbolic analysis by localizing the effect of dynamic sparsity changes on the ordering vector. We evaluated Parth on over 175,000 linear systems collected from both physics simulations and geometry processing applications, and show that for some of the most challenging physics simulations, it achieves up to 14x reordering runtime speedup, resulting in a 2x speedup in Cholesky solve time—even on top of well-optimized solvers such as Apple Accelerate and Intel MKL.},
  archive      = {J_TOG},
  author       = {Behrooz Zarebavani and Danny M. Kaufman and David I. W. Levin and Maryam Mehri Dehnavi},
  doi          = {10.1145/3731179},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Adaptive algebraic reuse of reordering in cholesky factorizations with dynamic sparsity patterns},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leapfrog flow maps for real-time fluid simulation. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Leapfrog Flow Maps (LFM) to simulate incompressible fluids with rich vortical flows in real time. Our key idea is to use a hybrid velocityimpulse scheme enhanced with leapfrog method to reduce the computational workload of impulse-based flow map methods, while possessing strong ability to preserve vortical structures and fluid details. In order to accelerate the impulse-to-velocity projection, we develop a fast matrix-free Algebraic Multigrid Preconditioned Conjugate Gradient (AMGPCG) solver with customized GPU optimization, which makes projection comparable with impulse evolution in terms of time cost. We demonstrate the performance of our method and its efficacy in a wide range of examples and experiments, such as real-time simulated burning fire ball and delta wingtip vortices.},
  archive      = {J_TOG},
  author       = {Yuchen Sun and Junlin Li and Ruicheng Wang and Sinan Wang and Zhiqi Li and Bart G. van Bloemen Waanders and Bo Zhu},
  doi          = {10.1145/3731180},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Leapfrog flow maps for real-time fluid simulation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing 3D anisotropic frame fields with odeco tensors. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3731181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a method to synthesize a 3D tensor field within a constrained geometric domain represented as a tetrahedral mesh. Whereas previous techniques optimize for isotropic fields, we focus on anisotropic tensor fields that are smooth and aligned with the domain boundary or user guidance. The key ingredient of our method is a novel computational design framework, built on top of the symmetric orthogonally decomposable (odeco) tensor representation, to optimize the stretching ratios and orientations for each tensor in the domain. In contrast to past techniques designed only for isotropic tensors, we demonstrate the efficacy of our approach in generating smooth volumetric tensor fields with high anisotropy and shape conformity, especially for the domain with complex shapes. We apply these anisotropic tensor fields to various applications, such as anisotropic meshing, structural mechanics, and fabrication.},
  archive      = {J_TOG},
  author       = {Haikuan Zhu and Hongbo Li and Hsueh-Ti Derek Liu and Wenping Wang and Jing Hua and Zichun Zhong},
  doi          = {10.1145/3731181},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Designing 3D anisotropic frame fields with odeco tensors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-performance CPU cloth simulation using domain-decomposed projective dynamics. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3731182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whenever the concept of high-performance cloth simulation is brought up, GPU acceleration is almost always the first that comes to mind. Leveraging immense parallelization, GPU algorithms have demonstrated significant success recently, whereas CPU methods are somewhat overlooked. Indeed, the need for an efficient CPU simulator is evident and pressing. In many scenarios, high-end GPUs may be unavailable or are already allocated to other tasks, such as rendering and shading. A high-performance CPU alternative can greatly boost the overall system capability and user experience. Inspired by this demand, this paper proposes a CPU algorithm for high-resolution cloth simulation. By partitioning the garment model into multiple (but not massive) sub-meshes or domains, we assign per-domain computations to individual CPU processors. Borrowing the idea of projective dynamics that breaks the computation into global and local steps, our key contribution is a new parallelization paradigm at domains for both global and local steps so that domain-level calculations are sequential and lightweight. The CPU has much fewer processing units than a GPU. Our algorithm mitigates this disadvantage by wisely balancing the scale of the parallelization and convergence. We validate our method in a wide range of simulation problems involving high-resolution garment models. Performance-wise, our method is at least one order faster than existing CPU methods, and it delivers a similar performance compared with the state-of-the-art GPU algorithms in many examples, but without using a GPU.},
  archive      = {J_TOG},
  author       = {Zixuan Lu and Ziheng Liu and Lei Lan and Huamin Wang and Yuko Ishiwaka and Chenfanfu Jiang and Kui Wu and Yin Yang},
  doi          = {10.1145/3731182},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {High-performance CPU cloth simulation using domain-decomposed projective dynamics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JGS2: Near second-order converging Jacobi/Gauss-seidel for GPU elastodynamics. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3731183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In parallel simulation, convergence and parallelism are often seen as inherently conflicting objectives. Improved parallelism typically entails lighter local computation and weaker coupling, which unavoidably slow the global convergence. This paper presents a novel GPU algorithm that achieves convergence rates comparable to fullspace Newton's method while maintaining good parallelizability just like the Jacobi method. Our approach is built on a key insight into the phenomenon of overshoot. Overshoot occurs when a local solver aggressively minimizes its local energy without accounting for the global context, resulting in a local update that undermines global convergence. To address this, we derive a theoretically second-order optimal solution to mitigate overshoot. Furthermore, we adapt this solution into a pre-computable form. Leveraging Cubature sampling, our runtime cost is only marginally higher than the Jacobi method, yet our algorithm converges nearly quadratically as Newton's method. We also introduce a novel full-coordinate formulation for more efficient pre-computation. Our method integrates seamlessly with the incremental potential contact method and achieves second-order convergence for both stiff and soft materials. Experimental results demonstrate that our approach delivers high-quality simulations and outperforms state-of-the-art GPU methods with 50× to 100× better convergence.},
  archive      = {J_TOG},
  author       = {Lei Lan and Zixuan Lu and Chun Yuan and Weiwei Xu and Hao Su and Huamin Wang and Chenfanfu Jiang and Yin Yang},
  doi          = {10.1145/3731183},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {JGS2: Near second-order converging Jacobi/Gauss-seidel for GPU elastodynamics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time knit deformation and rendering. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knit structure consists of interlocked yarns, with each yarn comprising multiple plies comprising tens to hundreds of twisted fibers. This intricate geometry and the large number of geometric primitives present substantial challenges for achieving high-fidelity simulation and rendering in real-time applications. In this work, we introduce the first real-time framework that takes an animated stitch mesh as input and enhances it with yarn-level simulation and fiber-level rendering. Our approach relies on a knot-based representation to model interlocked yarn contacts. The knot positions are interpolated from the underlying mesh, and associated yarn control points are optimized using a physically inspired energy formulation, which is solved through a GPU-based Gauss-Newton scheme for real-time performance. The optimized control points are sent to the GPU rasterization pipeline and rendered as yarns with fiber-level details. In real-time rendering, we introduce several decomposition strategies to enable realistic lighting effects on complex knit structures, even under environmental lighting, while maintaining computational and memory efficiency. Our simulation faithfully reproduces yarn-level structures under deformations, e.g., stretching and shearing, capturing interlocked yarn behaviors. The rendering pipeline achieves near-ground-truth visual quality while being 120,000× faster than path tracing reference with fiber-level geometries. The whole system provides real-time performance and has been evaluated through various application scenarios, including knit simulation for small patches and full garments and yarn-level relaxation in the design pipeline.},
  archive      = {J_TOG},
  author       = {Tao Huang and Haoyang Shi and Mengdi Wang and Yuxing Qiu and Yin Yang and Kui Wu},
  doi          = {10.1145/3731184},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time knit deformation and rendering},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UltraMeshRenderer: Efficient structure and management of GPU out-of-core memory for real-time rendering of gigantic 3D meshes. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3731186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPUs can encounter memory capacity constraints, which pose challenges for achieving real-time rendering performance when processing large 3D models that exceed available memory. State-of-the-art out-of-core rendering frameworks have leveraged Level of Detail (LOD) and frame-to-frame coherence data management techniques to optimize memory usage and minimize CPU-to-GPU data transfer costs. However, the size of view-dependently selected data may still exceed GPU memory capacity, and data transfer remains the most significant bottleneck in overall performance costs. To address these, we introduce a new GPU out-of-core rendering approach that includes a LOD selection method that takes into account both memory and coherence constraints and a parallel in-place GPU memory management algorithm that efficiently assembles the data of the current frame with GPU-resident data from the previous frame and transferred data. Our approach bounds memory usage and data transfer costs, prioritizes and schedules the transfer of essential data, incrementally refining the LOD over subsequent frames to converge toward the desired visual fidelity. Our parallel memory management algorithm consolidates frame-different and reusable data, dynamically reallocating GPU memory slots for efficient in-place operations. Hierarchical LOD representations remain a core component, and we emphasize their role in supporting adaptive data transfer and coherence management, characterized by a uniform depth and near-equal patch size at all levels. Our approach adapts seamlessly to scenarios with varying levels of coherence by balancing real-time performance with visual consistency. Experimental results demonstrate that our system achieves significant performance improvements, rendering scenes with billions of triangles in real-time, outperforming existing methods while maintaining consistent visual quality during dynamic interactions.},
  archive      = {J_TOG},
  author       = {Huadong Zhang and Lizhou Cao and Chao Peng},
  doi          = {10.1145/3731186},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {UltraMeshRenderer: Efficient structure and management of GPU out-of-core memory for real-time rendering of gigantic 3D meshes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse geometric locomotion. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3731187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous tasks in robotics and character animation involve solving combinations of inverse kinematics and motion planning problems that require the precise design of pose sequences to achieve desired motion objectives. Accounting for the complex interplay between body deformations and resulting motion, especially through interactions with the environment, poses significant challenges for the design of such pose sequences. We propose a computational framework to address these challenges in scenarios where the motion of a deformable body is entirely determined by dynamic changes of its shape. Complementing recent methods on the forward problem—mapping shape sequences to global motion trajectories based on a geometric formulation of locomotion—we address the inverse problem of optimizing shape sequences to achieve user-defined motion objectives. We demonstrate the effectiveness of our method through a diverse set of examples, producing realistic shape sequences that result in desired motion trajectories.},
  archive      = {J_TOG},
  author       = {Quentin Becker and Oliver Gross and Mark Pauly},
  doi          = {10.1145/3731187},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Inverse geometric locomotion},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 4D gaussian videos with motion layering. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3731189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online free-view navigation in volumetric videos requires high-quality rendering and real-time streaming in order to provide immersive user experiences. However, existing methods ( e.g. , dynamic NeRF and 3DGS) may not handle dynamic scenes with complex motions, and their models may not be streamable due to storage and bandwidth constraints. In this paper, we propose a novel 4D Gaussian Video (4DGV) approach that enables the creation and streaming of photorealistic, volumetric videos for dynamic scenes over the Internet. The core of our 4DGV is a novel streamable group of Gaussians (GOG) representation based on motion layering. Each GOG consists of static and dynamic points obtained via lifting 2D segmentation into 3D in motion layering, where the deformation of each dynamic point is represented as the temporal offset of its attributes. We also adaptively convert static points back to dynamic points to handle the appearance change, (e.g. , moving shadows and reflections), of static objects through optimization. To support real-time streaming of 4DGVs, we show that by applying quantization on Gaussian attributes and H.265 encoding on deformation offsets, our GOG representation can be significantly compressed (to around 6% of the original model size) without sacrificing the accuracy (PSNR loss less than 0.01dB). Extensive experiments on standard benchmarks demonstrate that our method outperforms state-of-the-art volumetric video approaches, with superior rendering quality and minimum storage overheads.},
  archive      = {J_TOG},
  author       = {Pinxuan Dai and Peiquan Zhang and Zheng Dong and Ke Xu and Yifan Peng and Dandan Ding and Yujun Shen and Yin Yang and Xinguo Liu and Rynson W. H. Lau and Weiwei Xu},
  doi          = {10.1145/3731189},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {4D gaussian videos with motion layering},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cirrus: Adaptive hybrid particle-grid flow maps on GPU. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3731190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the adaptive hybrid particle-grid flow map method, a novel flow-map approach that leverages Lagrangian particles to simultaneously transport impulse and guide grid adaptation, introducing a fully adaptive flow map-based fluid simulation framework. The core idea of our method is to maintain flow-map trajectories separately on grid nodes and particles: the grid-based representation tracks long-range flow maps at a coarse spatial resolution, while the particle-based representation tracks both long and short-range flow maps, enhanced by their gradients, at a fine resolution. This hybrid Eulerian-Lagrangian flow-map representation naturally enables adaptivity for both advection and projection steps. We implement this method in Cirrus , a GPU-based fluid simulation framework designed for octree-like adaptive grids enhanced with particle trackers. The efficacy of our system is demonstrated through numerical tests and various simulation examples, achieving up to 512 × 512 × 2048 effective resolution on an RTX 4090 GPU. We achieve a 1.5 to 2× speedup with our GPU optimization over the Particle Flow Map method on the same hardware, while the adaptive grid implementation offers efficiency gains of one to two orders of magnitude by reducing computational resource requirements. The source code has been made publicly available at: https://wang-mengdi.github.io/proj/25-cirrus/.},
  archive      = {J_TOG},
  author       = {Mengdi Wang and Fan Feng and Junlin Li and Bo Zhu},
  doi          = {10.1145/3731190},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Cirrus: Adaptive hybrid particle-grid flow maps on GPU},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational surface reconstruction using natural neighbors. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3731191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface reconstruction from points is a fundamental problem in computer graphics. While numerous methods have been proposed, it remains challenging to reconstruct from sparse and non-uniform point distributions, particularly when normals are absent. We present a robust and scalable method for reconstructing an implicit surface from points without normals. By exploring the locality of natural neighborhoods, we propose local reformulations of a previous global method, known for its ability to surface sparse points but high computational cost, thereby significantly improving its scalability while retaining its robustness. Experiments show that our method achieves comparable speed to existing reconstruction methods on large inputs while producing fewer artifacts in under-sampled regions.},
  archive      = {J_TOG},
  author       = {Jianjun Xia and Tao Ju},
  doi          = {10.1145/3731191},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Variational surface reconstruction using natural neighbors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluid simulation on compressible flow maps. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3731192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a unified compressible flow map framework designed to accommodate diverse compressible flow systems, including high-Mach-number flows (e.g., shock waves and supersonic aircraft), weakly compressible systems (e.g., smoke plumes and ink diffusion), and incompressible systems evolving through compressible acoustic quantities (e.g., free-surface shallow water). At the core of our approach is a theoretical foundation for compressible flow maps based on Lagrangian path integrals, a novel advection scheme for the conservative transport of density and energy, and a unified numerical framework for solving compressible flows with varying pressure treatments. We validate our method across three representative compressible flow systems, characterized by varying fluid morphologies, governing equations, and compressibility levels, demonstrating its ability to preserve and evolve spatiotemporal features such as vortical structures and wave interactions governed by different flow physics. Our results highlight a wide range of novel phenomena, from ink torus breakup to delta wing tail vortices and vortex shedding on free surfaces, significantly expanding the range of fluid systems that flow-map methods can handle.},
  archive      = {J_TOG},
  author       = {Duowen Chen and Zhiqi Li and Taiyuan Zhang and Jinjin He and Junwei Zhou and Bart G. van Bloemen Waanders and Bo Zhu},
  doi          = {10.1145/3731192},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fluid simulation on compressible flow maps},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDGE: Epsilon-difference gradient evolution for buffer-free flow maps. <em>TOG</em>, <em>44</em>(4), 1-11. (<a href='https://doi.org/10.1145/3731193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the Epsilon Difference Gradient Evolution (EDGE) method for accurate flow-map calculation on grids via Hermite interpolation without using velocity buffers. Our key idea is to integrate Gradient Evolution for accurate first-order derivatives and a tetrahedron-based Epsilon Difference scheme to compute higher-order derivatives with reduced memory consumption. EDGE achieves O (1) memory usage, independent of flow map length, while maintaining vorticity preservation comparable to buffer-based methods. We validate our methods across diverse vortical flow scenarios, demonstrating up to 90% backward map memory reduction and significant computational efficiency, broadening the applicability of flow-map methods to large-scale and complex fluid simulations.},
  archive      = {J_TOG},
  author       = {Zhiqi Li and Ruicheng Wang and Junlin Li and Duowen Chen and Sinan Wang and Bo Zhu},
  doi          = {10.1145/3731193},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {EDGE: Epsilon-difference gradient evolution for buffer-free flow maps},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clebsch gauge fluid on particle flow maps. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel gauge fluid solver that evolves Clebsch wave functions on particle flow maps (PFMs). The key insight underlying our work is that particle flow maps exhibit superior performance in transporting point elements—such as Clebsch components—compared to line and surface elements, which were the focus of previous methods relying on impulse and vortex gauge variables for flow maps. Our Clebsch PFM method incorporates three main contributions: a novel gauge transformation enabling accurate transport of wave functions on particle flow maps, an enhanced velocity reconstruction method for coarse grids, and a PFM-based simulation framework designed to better preserve fine-scale flow structures. We validate the Clebsch PFM method through a wide range of benchmark tests and simulation examples, ranging from leapfrogging vortex rings and vortex reconnections to Kelvin-Helmholtz instabilities, demonstrating that our method outperforms its impulse- or vortex-based counterparts on particle flow maps, particularly in preserving and evolving small-scale features.},
  archive      = {J_TOG},
  author       = {Zhiqi Li and Candong Lin and Duowen Chen and Xinyi Zhou and Shiying Xiong and Bo Zhu},
  doi          = {10.1145/3731194},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Clebsch gauge fluid on particle flow maps},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented vertex block descent. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertex Block Descent is a fast physics-based simulation method that is unconditionally stable, highly parallelizable, and capable of converging to the implicit Euler solution. We extend it using an augmented Lagrangian formulation to address some of its fundamental limitations. First, we introduce a mechanism to handle hard constraints with infinite stiffness without introducing numerical instabilities. Second, we substantially improve the convergence in the presence of high stiffness ratios. These changes we introduce allow simulating complex contact scenarios involving rigid bodies with stacking and friction, articulated bodies connected with hard constraints, including joints with limited degrees of freedom, and stiff systems interacting with soft bodies. We present evaluations using a parallel GPU implementation that can deliver real-time performance and stable simulations with low iteration counts for millions of objects interacting via collisions, various joint/attachment constraints, and springs of various stiffness. Our results show superior performance, convergence, and stability compared to the state-of-the-art alternatives.},
  archive      = {J_TOG},
  author       = {Chris Giles and Elie Diaz and Cem Yuksel},
  doi          = {10.1145/3731195},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Augmented vertex block descent},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightning-fast boundary element method. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3731196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boundary element methods (BEM) for solving linear elliptic partial differential equations have gained traction in a wide range of graphics applications: they eliminate the need for volumetric meshing by solving for variables exclusively on the domain boundary through a linear boundary integral equation (BIE). However, BEM often generate dense and ill-conditioned linear systems that lead to poor computational scalability and substantial memory demands for large-scale problems, limiting their applicability and efficiency in practice. In this paper, we address these limitations by generalizing the Kaporin-based approach to asymmetric preconditioning: we construct a sparse approximation of the inverse-LU factorization of arbitrary BIE matrices in a massively parallel manner. Our sparse inverse-LU factorization, when employed as a preconditioner for the generalized minimal residual (GMRES) method, significantly enhances the efficiency of BIE solves, often yielding orders-of-magnitude speedups in solving times.},
  archive      = {J_TOG},
  author       = {Jiong Chen and Florian Schäfer and Mathieu Desbrun},
  doi          = {10.1145/3731196},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Lightning-fast boundary element method},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discrete torsion of connection forms on simplicial meshes. <em>TOG</em>, <em>44</em>(4), 1-10. (<a href='https://doi.org/10.1145/3731197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While discrete (metric) connections have become a staple of n -vector field design and analysis on simplicial meshes, the notion of torsion of a discrete connection has remained unstudied. This is all the more surprising as torsion is a crucial component in the fundamental theorem of Riemannian geometry, which introduces the existence and uniqueness of the Levi-Civita connection induced by the metric. In this paper, we extend the existing geometry processing toolbox by providing torsion control over discrete connections. Our approach consists in first introducing a new discrete Levi-Civita connection for a metric with locally-constant curvature to replace the hinge connection of a triangle mesh whose curvature is concentrated at singularities; from this reference connection, we define the discrete torsion of a connection to be the discrete dual 1-form by which a connection deviates from our discrete Levi-Civita connection. We discuss how the curvature and torsion of a discrete connection can then be controlled and assigned in a manner consistent with the continuous case. We also illustrate our approach through theoretical analysis and practical examples arising in vector and frame design.},
  archive      = {J_TOG},
  author       = {Theo Braune and Mark Gillespie and Yiying Tong and Mathieu Desbrun},
  doi          = {10.1145/3731197},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Discrete torsion of connection forms on simplicial meshes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluid simulation on vortex particle flow maps. <em>TOG</em>, <em>44</em>(4), 1-24. (<a href='https://doi.org/10.1145/3731198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the V ortex P article F low M ap (VPFM) method to simulate incompressible flow with complex vortical evolution in the presence of dynamic solid boundaries. The core insight of our approach is that vorticity is an ideal quantity for evolution on particle flow maps, enabling significantly longer flow map distances compared to other fluid quantities like velocity or impulse. To achieve this goal, we developed a hybrid Eulerian-Lagrangian representation that evolves vorticity and flow map quantities on vortex particles, while reconstructing velocity on a background grid. The method integrates three key components: (1) a vorticity-based particle flow map framework, (2) an accurate Hessian evolution scheme on particles, and (3) a solid boundary treatment for no-through and no-slip conditions in VPFM. These components collectively allow a substantially longer flow map length ( 3–12 times longer) than the state-of-the-art, enhancing vorticity preservation over extended spatiotemporal domains. We validated the performance of VPFM through diverse simulations, demonstrating its effectiveness in capturing complex vortex dynamics and turbulence phenomena.},
  archive      = {J_TOG},
  author       = {Sinan Wang and Junwei Zhou and Fan Feng and Zhiqi Li and Yuchen Sun and Duowen Chen and Greg Turk and Bo Zhu},
  doi          = {10.1145/3731198},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fluid simulation on vortex particle flow maps},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MaterialPicker: Multi-modal DiT-based material generation. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality material generation is key for virtual environment authoring and inverse rendering. We propose MaterialPicker, a multi-modal material generator leveraging a Diffusion Transformer (DiT) architecture, improving and simplifying the creation of high-quality materials from text prompts and/or photographs. Our method can generate a material based on an image crop of a material sample, even if the captured surface is distorted, viewed at an angle or partially occluded, as is often the case in photographs of natural scenes. We further allow the user to specify a text prompt to provide additional guidance for the generation. We finetune a pre-trained DiT-based video generator into a material generator, where each material map is treated as a frame in a video sequence. We evaluate our approach both quantitatively and qualitatively and show that it enables more diverse material generation and better distortion correction than previous work.},
  archive      = {J_TOG},
  author       = {Xiaohe Ma and Valentin Deschaintre and Miloš Hašan and Fujun Luan and Kun Zhou and Hongzhi Wu and Yiwei Hu},
  doi          = {10.1145/3731199},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {MaterialPicker: Multi-modal DiT-based material generation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative on-sensor array cameras. <em>TOG</em>, <em>44</em>(4), 1-18. (<a href='https://doi.org/10.1145/3731200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern nanofabrication techniques have enabled us to manipulate the wave-front of light with sub-wavelength-scale structures, offering the potential to replace bulky refractive surfaces in conventional optics with ultrathin metasurfaces. In theory, arrays of nanoposts provide unprecedented control over manipulating the wavefront in terms of phase, polarization, and amplitude at the nanometer resolution. A line of recent work successfully investigates flat computational cameras that replace compound lenses with a single metalens or an array of metasurfaces a few millimeters from the sensor. However, due to the inherent wavelength dependence of metalenses, in practice, these cameras do not match their refractive counterparts in image quality for broadband imaging, and may even suffer from hallucinations when relying on generative reconstruction methods. In this work, we investigate a collaborative array of metasurface elements that are jointly learned to perform broadband imaging. To this end, we learn a nanophotonics array with 100-million nanoposts that is end-to-end jointly optimized over the full visible spectrum—a design task that existing inverse design methods or learning approaches cannot support due to memory and compute limitations. We introduce a distributed meta-optics learning method to tackle this challenge. This allows us to optimize a large parameter array along with a learned metaatom proxy and a non-generative reconstruction method that is parallax-aware and noise-aware. The proposed camera performs favorably in simulation and in all experimental tests irrespective of the scene illumination spectrum.},
  archive      = {J_TOG},
  author       = {Jipeng Sun and Kaixuan Wei and Thomas Eboli and Congli Wang and Cheng Zheng and Zhihao Zhou and Arka Majumdar and Wolfgang Heidrich and Felix Heide},
  doi          = {10.1145/3731200},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Collaborative on-sensor array cameras},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arenite: A physics-based sandstone simulator. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3731201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Arenite, a novel physics-based approach for modeling sandstone structures. The key insight of our work is that simulating a combination of stress and multi-factor erosion enables the generation of a wide variety of sandstone structures observed in nature. We isolate the key shape-forming phenomena: multi-physics fabric interlocking, wind and fluvial erosion, and particle-based deposition processes. Complex 3D structures such as arches, alcoves, hoodoos, or buttes can be achieved by creating simple 3D structures with user-painted erodable areas and vegetation and running the simulation. We demonstrate the algorithm on a wide variety of structures, and our GPU-based implementation achieves the simulation in less than 5 minutes on a desktop computer for our most complex example.},
  archive      = {J_TOG},
  author       = {Zhanyu Yang and Aryamaan Jain and Guillaume Cordonnier and Marie-Paule Cani and Zhaopeng Wang and Bedrich Benes},
  doi          = {10.1145/3731201},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Arenite: A physics-based sandstone simulator},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive dynamics++: A framework for stable, continuous, and consistent animation across resolution and time. <em>TOG</em>, <em>44</em>(4), 1-20. (<a href='https://doi.org/10.1145/3731202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently developed Progressive Dynamics framework [Zhang et al. 2024] addresses the long-standing challenge in enabling rapid iterative design for high-fidelity cloth and shell animation. In this work, we identify fundamental limitations of the original method in terms of stability and temporal continuity. For robust progressive dynamics simulation we seek methods that provide: (1) stability across all levels of detail (LOD) and timesteps, (2) temporally continuous animations without jumps or jittering, and (3) user-controlled balancing between geometric consistency and enrichment at each timestep, thereby making it a practical previewing tool with high-quality results at the finest level to be used as the final output. We propose a general framework, Progressive Dynamics++, for constructing a family of progressive dynamics integration methods that advance physical simulation states forward in both time and spatial resolution, which includes Zhang et al. [2024]'s method as one member. We analyze necessary stability conditions for Progressive Dynamics integrators and introduce a novel, stable method that significantly improves temporal continuity, supported by a new quantitative measure. Additionally, we present a quantitative analysis of the trade-off between geometric consistency and enrichment, along with strategies for balancing between these aspects in transitions across resolution and time.},
  archive      = {J_TOG},
  author       = {Jiayi Eris Zhang and Doug L. James and Danny M. Kaufman},
  doi          = {10.1145/3731202},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Progressive dynamics++: A framework for stable, continuous, and consistent animation across resolution and time},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Putting rigid bodies to rest. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3731203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the analysis and design of the resting configurations of a rigid body, without the use of physical simulation. In particular, given a rigid body in R 3 , we identify all possible stationary points, as well as the probability that the body will stop at these points, assuming a random initial orientation and negligible momentum. The forward version of our method can hence be used to automatically orient models, to provide feedback about object stability during the design process, and to furnish plausible distributions of shape orientation for natural scene modeling. Moreover, a differentiable inverse version of our method lets us design shapes with target resting behavior, such as dice with target, nonuniform probabilities. Here we find solutions that would be nearly impossible to find using classical techniques, such as dice with additional unstable faces that provide more natural overall geometry. From a technical point of view, our key observation is that rolling equilibria can be extracted from the Morse-Smale complex of the support function over the Gauss map. Our method is hence purely geometric, and does not make use of random sampling, or numerical time integration. Yet surprisingly, this purely geometric model makes extremely accurate predictions of rest behavior, which we validate both numerically, and via physical experiments. Moreover, for computing rest statistics, it is orders of magnitude faster than state of the art rigid body simulation, opening the door to inverse design—rather than just forward analysis.},
  archive      = {J_TOG},
  author       = {Hossein Baktash and Nicholas Sharp and Qingnan Zhou and Alec Jacobson and Keenan Crane},
  doi          = {10.1145/3731203},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Putting rigid bodies to rest},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal r-adaptive in-timestep remeshing for elastodynamics. <em>TOG</em>, <em>44</em>(4), 1-19. (<a href='https://doi.org/10.1145/3731204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a coupled mesh-adaptation model and physical simulation algorithm to jointly generate, per timestep, optimal adaptive remeshings and implicit solutions for the simulation of frictionally contacting elastodynamics. To do so, we begin with Ferguson et al.'s [2023] recently developed in-timestep remeshing (ITR) framework, which proposes an Incremental Potential based objective for mesh refinement, and a corresponding, locally greedy remeshing algorithm to minimize it. While this initial ITR framework demonstrates significant improvements, its greedy remeshing does not generate optimal meshes, and so does not converge to improving physical solutions with increasing mesh resolution. In practice, due to lack of optimality, the original ITR framework can and will find mesh and state solutions with unnecessarily low-quality geometries and corresponding physical solution artifacts. At the same time, we also identify additional fundamental challenges to adaptive simulation in terms of both ITR's original remeshing objective and its corresponding optimization problem formulation. In this work, in order to extend the ITR framework to high-quality, optimal in-timestep remeshing, we first construct a new remeshing objective function built from simple, yet critical, updates to the Incremental Potential energy, and a corresponding constrained model problem, whose minimizers provide locally optimal remeshings for physical problems. We then propose a new in-timestep remeshing optimization that jointly solves, per-timestep, for a new locally optimal remeshing and the next physical state defined upon it. To evaluate and demonstrate our extension of the ITR framework, we apply it to the optimal r-adaptive ITR simulation of frictionally contacting elasto-dynamics and statics. To enable r-adaptivity we additionally propose a new numerical method to robustly compute derivatives of the L 2 -projection operator necessary for optimal mesh-to-mesh state mappings within solves, a constraint model to enable on-boundary node adaptivity, and an efficient Newton-type optimization method for practically solving each per-timestep r-adaptive ITR solution. We extensively evaluate our method on challenging large-deformation and frictionally contacting scenarios. Here we observe optimal r-adaptivity captures comparable and better accuracy than unadapted meshes orders-of-magnitude larger, with corresponding significant advantages in both computation speedup and decrease in memory usage.},
  archive      = {J_TOG},
  author       = {Jiahao Wen and Jernej Barbič and Danny M. Kaufman},
  doi          = {10.1145/3731204},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optimal r-adaptive in-timestep remeshing for elastodynamics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Offset geometric contact. <em>TOG</em>, <em>44</em>(4), 1-21. (<a href='https://doi.org/10.1145/3731205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel contact model, termed Offset Geometric Contact (OGC), for guaranteed penetration-free simulation of codimensional objects with minimal computational overhead. Our method is based on constructing a volumetric shape by offsetting each face along its normal direction, ensuring orthogonal contact forces, thus allows large contact radius without artifacts. We compute vertex-specific displacement bounds to guarantee penetration-free simulation, which improves convergence and avoids the need for expensive continuous collision detection. Our method relies solely on massively parallel local operations, avoiding global synchronization and enabling efficient GPU implementation. Experiments demonstrate real-time, large-scale simulations with performance more than two orders of magnitude faster than prior methods while maintaining consistent computational budgets.},
  archive      = {J_TOG},
  author       = {Anka He Chen and Jerry Hsu and Ziheng Liu and Miles Macklin and Yin Yang and Cem Yuksel},
  doi          = {10.1145/3731205},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Offset geometric contact},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffuse-CLoC: Guided diffusion for physics-based character look-ahead control. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Diffuse-CLoC, a guided diffusion framework for physics-based look-ahead control that enables intuitive, steerable, and physically realistic motion generation. While existing kinematics motion generation with diffusion models offer intuitive steering capabilities with inference-time conditioning, they often fail to produce physically viable motions. In contrast, recent diffusion-based control policies have shown promise in generating physically realizable motion sequences, but the lack of kinematics prediction limits their steerability. Diffuse-CLoC addresses these challenges through a key insight: modeling the joint distribution of states and actions within a single diffusion model makes action generation steerable by conditioning it on the predicted states. This approach allows us to leverage established conditioning techniques from kinematic motion generation while producing physically realistic motions. As a result, we achieve planning capabilities without the need for a high-level planner. Our method handles a diverse set of unseen long-horizon downstream tasks through a single pre-trained model, including static and dynamic obstacle avoidance, motion in-betweening, and task-space control. Experimental results show that our method significantly outperforms the traditional hierarchical framework of high-level motion diffusion and low-level tracking.},
  archive      = {J_TOG},
  author       = {Xiaoyu Huang and Takara Truong and Yunbo Zhang and Fangzhou Yu and Jean Pierre Sleiman and Jessica Hodgins and Koushil Sreenath and Farbod Farshidian},
  doi          = {10.1145/3731206},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Diffuse-CLoC: Guided diffusion for physics-based character look-ahead control},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MiSo: A DSL for robust and efficient solve and MInimize problems. <em>TOG</em>, <em>44</em>(4), 1-18. (<a href='https://doi.org/10.1145/3731207'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many problems in computer graphics can be formulated as finding the global minimum of a function subject to a set of non-linear constraints (Minimize), or finding all solutions of a system of non-linear constraints (Solve). We introduce MiSo, a domain-specific language and compiler for generating efficient C++ code for low-dimensional Minimize and Solve problems, that uses interval methods to guarantee conservative results while using floating point arithmetic. We demonstrate that MiSo-generated code shows competitive performance compared to hand-optimized codes for several computer graphics problems, including high-order collision detection with non-linear trajectories, surface-surface intersection, and geometrical validity checks for finite element simulation.},
  archive      = {J_TOG},
  author       = {Federico Sichetti and Enrico Puppo and Zizhou Huang and Marco Attene and Denis Zorin and Daniele Panozzo},
  doi          = {10.1145/3731207},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {MiSo: A DSL for robust and efficient solve and MInimize problems},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ANIME-rod: Adjustable nonlinear isotropic materials for elastic rods. <em>TOG</em>, <em>44</em>(4), 1-23. (<a href='https://doi.org/10.1145/3731208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a method to simulate large deformations of 3D elastic rods under arbitrary nonlinear isotropic 3D solid materials. Rod elastic energies in existing graphics literature are derived from volumetric models under the small-strain linearization assumptions. While the resulting equations can and are commonly applied to large deformations, the material modeling has been limited to a single material, namely linear Hooke law. Starting from any 3D solid nonlinear isotropic elastic energy density function ψ , we derive our rod elastic energy by subjecting the 3D solid volumetric material to the limit process whereby rod thickness is decreased to zero. This enables us to explain rod stretching, bending and twisting in a unified model. Care must be taken to adequately model cross-sectional in-plane and out-of-plane deformations. Our key insight is to compute the three cross-sectional deformation modes corresponding to bending (in the two directions) and twisting, using linear theory. Then, given any ψ , we use these modes to derive an analytical formula for a 5D "macroscopic" large-deformation rod elastic energy function of the local longitudinal stretch, radial scaling, the two bending curvatures and torsion. Our model matches linear theory for small deformations, including cross-sectional shrinkage due to Poisson's effect, and produces correct bending and torsional constants. Our experiments demonstrate that our energy closely matches volumetric FEM even under large stretches and curvatures, whereas commonly used methods in graphics deviate from it. We also compare to closely related work from mechanics literature; we give an explicit expansion of all energy terms in terms of the rod cross-section diameter, allowing independent adjustment of stretching, bending and twisting. Finally, we observe an inherent limitation in the ability of rod models to control nonlinear bendability and twistability. We propose to "relax" rod physics to more easily control nonlinear bending and twisting in computer graphics applications.},
  archive      = {J_TOG},
  author       = {Huanyu Chen and Jiahao Wen and Jernej Barbič},
  doi          = {10.1145/3731208},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. Graph.},
  title        = {ANIME-rod: Adjustable nonlinear isotropic materials for elastic rods},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MoVer: Motion verification for motion graphics animations. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3731209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While large vision-language models can generate motion graphics animations from text prompts, they regularly fail to include all spatio-temporal properties described in the prompt. We introduce MoVer, a motion verification DSL based on first-order logic that can check spatio-temporal properties of a motion graphics animation. We identify a general set of such properties that people commonly use to describe animations (e.g., the direction and timing of motions, the relative positioning of objects, etc.). We implement these properties as predicates in MoVer and provide an execution engine that can apply a MoVer program to any input SVG-based motion graphics animation. We then demonstrate how MoVer can be used in an LLM-based synthesis and verification pipeline for iteratively refining motion graphics animations. Given a text prompt, our pipeline synthesizes a motion graphics animation and a corresponding MoVer program. Executing the verification program on the animation yields a report of the predicates that failed and the report can be automatically fed back to LLM to iteratively correct the animation. To evaluate our pipeline, we build a synthetic dataset of 5600 text prompts paired with ground truth MoVer verification programs. We find that while our LLM-based pipeline is able to automatically generate a correct motion graphics animation for 58.8% of the test prompts without any iteration, this number raises to 93.6% with up to 50 correction iterations. Our code and dataset are at https://mover-dsl.github.io.},
  archive      = {J_TOG},
  author       = {Jiaju Ma and Maneesh Agrawala},
  doi          = {10.1145/3731209},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {MoVer: Motion verification for motion graphics animations},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMLS-splatting: Efficient mesh reconstruction from multi-view images via point representation. <em>TOG</em>, <em>44</em>(4), 1-11. (<a href='https://doi.org/10.1145/3731210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view mesh reconstruction has long been a challenging problem in graphics and computer vision. In contrast to recent volumetric rendering methods that generate meshes through post-processing, we propose an end-to-end mesh optimization approach called IMLS-Splatting. Our method leverages the sparsity and flexibility of point clouds to efficiently represent the underlying surface. To achieve this, we introduce a splatting-based differentiable Implicit Moving-Least Squares (IMLS) algorithm that enables the fast conversion of point clouds into SDFs and texture fields, optimizing both mesh reconstruction and rasterization. Additionally, the IMLS representation ensures that the reconstructed SDF and mesh maintain continuity and smoothness without the need for extra regularization. With this efficient pipeline, our method enables the reconstruction of highly detailed meshes in approximately 11 minutes, supporting high-quality rendering and achieving state-of-the-art reconstruction performance. Our code is available at https://github.com/SilenKZYoung/IMLS-Splatting.},
  archive      = {J_TOG},
  author       = {Kaizhi Yang and Liu Dai and Isabella Liu and Xiaoshuai Zhang and Xiaoyan Sun and Xuejin Chen and Zexiang Xu and Hao Su},
  doi          = {10.1145/3731210},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {ACM Trans. Graph.},
  title        = {IMLS-splatting: Efficient mesh reconstruction from multi-view images via point representation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DGH: 3D head generation with composable hair and face. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.},
  archive      = {J_TOG},
  author       = {Chengan He and Junxuan Li and Tobias Kirschstein and Artem Sevastopolsky and Shunsuke Saito and Qingyang Tan and Javier Romero and Chen Cao and Holly Rushmeier and Giljoo Nam},
  doi          = {10.1145/3731211},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {3DGH: 3D head generation with composable hair and face},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Faraday cage estimation of normals for point clouds and ribbon sketches. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3731212'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method (FaCE) for normal estimation of unoriented point clouds and VR ribbon sketches that leverages a modeling of the Faraday cage effect. Input points, or a sampling of the ribbons, form a conductive cage and shield the interior from external fields. The gradient of the maximum field strength over external field scenarios is used to estimate a normal at each input point or ribbon. The electrostatic effect is modeled with a simple Poisson system, accommodating intuitive user-driven sculpting via the specification of point charges and Faraday cage points. On inputs sampled from clean, watertight meshes, our method achieves comparable normal quality to existing methods tailored for this scenario. On inputs containing interior structures and artifacts, our method produces superior surfacing output when combined with Poisson Surface Reconstruction. In the case of ribbon sketches, our method accommodates sparser ribbon input while maintaining an accurate geometry, allowing for greater flexibility in the artistic process. We demonstrate superior performance to an existing approach for surfacing ribbon sketches in this sparse setting.},
  archive      = {J_TOG},
  author       = {Daniel Scrivener and Daniel Cui and Ellis Coldren and S. Mazdak Abulnaga and Mikhail Bessmeltsev and Edward Chien},
  doi          = {10.1145/3731212},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Faraday cage estimation of normals for point clouds and ribbon sketches},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QUASAR: Quad-based adaptive streaming and rendering. <em>TOG</em>, <em>44</em>(4), 1-18. (<a href='https://doi.org/10.1145/3731213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As AR/VR systems evolve to demand increasingly powerful GPUs, physically separating compute from display hardware emerges as a natural approach to enable a lightweight, comfortable form factor. Unfortunately, splitting the system into a client-server architecture leads to challenges in transporting graphical data. Simply streaming rendered images over a network suffers in terms of latency and reliability, especially given variable bandwidth. Although image-based reprojection techniques can help, they often do not support full motion parallax or disocclusion events. Instead, scene geometry can be streamed to the client, allowing local rendering of novel views. Traditionally, this has required a prohibitively large amount of interconnect bandwidth, excluding the use of practical networks. This paper presents a new quad-based geometry streaming approach that is designed with compression and the ability to adjust Quality-of-Experience (QoE) in response to target network bandwidths. Our approach advances previous work by introducing a more compact data structure and a temporal compression technique that reduces data transfer overhead by up to 15×, reducing bandwidth usage to as low as 100 Mbps. We optimized our design for hardware video codec compatibility and support an adaptive data streaming strategy that prioritizes transmitting only the most relevant geometry updates. Our approach achieves image quality comparable to, and in many cases exceeds, state-of-the-art techniques while requiring only a fraction of the bandwidth, enabling real-time geometry streaming on commodity headsets over WiFi.},
  archive      = {J_TOG},
  author       = {Edward Lu and Anthony Rowe},
  doi          = {10.1145/3731213},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Graph.},
  title        = {QUASAR: Quad-based adaptive streaming and rendering},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Echoes of the coliseum: Towards 3D live streaming of sports events. <em>TOG</em>, <em>44</em>(4), 1-17. (<a href='https://doi.org/10.1145/3731214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-centered live events have always played a pivotal role in shaping culture and fostering social connections. Traditional 2D live transmissions fail to replicate the immersive quality of physical attendance. Addressing this gap, this paper proposes LiveSplats , a framework towards real-time, photo-realistic 3D reconstructions of live events using high-performance 3D Gaussian Splatting. Our solution capitalizes on strong geometric priors to optimize through distributed processing and load balancing, enabling interactive, freely explorable 3D experiences. By dividing scene reconstruction into actor-centric and environment-specific tasks, we employ hierarchical coarse-to-fine optimization to rapidly and accurately reconstruct human actors based on pose data, refining their geometry and appearance with photometric loss. For static environments, we focus on view-dependent appearance changes, streamlining rendering efficiency and maximizing GPU performance. To facilitate evaluation, we introduce (and distribute) a synthetic benchmark dataset of basketball games, offering high visual fidelity as ground truth. In both our synthetic benchmark and publicly available benchmarks, LiveSplats consistently outperforms existing approaches. The dataset is available at https://humansensinglab.github.io/basket-multiview.},
  archive      = {J_TOG},
  author       = {Junkai Huang and Saswat Subhajyoti Mallick and Alejandro Amat and Marc Ruiz Olle and Albert Mosella-Montoro and Bernhard Kerbl and Francisco Vicente Carrasco and Fernando De la Torre},
  doi          = {10.1145/3731214},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Echoes of the coliseum: Towards 3D live streaming of sports events},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-aligned parametrization in penner coordinates. <em>TOG</em>, <em>44</em>(4), 1-21. (<a href='https://doi.org/10.1145/3731216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parametrization is a key element of many geometric modeling tasks. Seamless parametrization, in particular, is needed as a starting point for many algorithms for quadrangulation and conversion to high-order patches, as well as for the construction of seamless texture maps and displacement maps. Seamless parametrizations are difficult to compute robustly, in part because, in general, it is not known if one exists for a given mesh connectivity or for a particular configuration of singularities. Recently, Penner-coordinate-based methods that allow for connectivity changes have been shown to achieve a perfect success rate on a widely used dataset (Thingi10k). However, previously proposed Penner coordinate methods do not support sharp feature alignment or soft alignment with preferred directions on the surface, both of which are important for practical applications, especially those involving models with sharp features. In this paper, we extend Penner coordinates to surfaces with sharp features to which the parametrization needs to be aligned. Our algorithm extends the holonomy signature description of seamless parametrizations to surfaces with marked feature curves. We describe sufficient conditions for obtaining feasible solutions and describe a two-phase method to efficiently enforce feature constraints or minimize residual errors when solutions are unattainable. We demonstrate that the resulting algorithm works robustly on the Thingi10k dataset with automatic feature labeling, and the resulting seamless parametrizations can be optimized, quantized, and quadrangulated, completing the quad mesh generation pipeline.},
  archive      = {J_TOG},
  author       = {Ryan Capouellez and Rodrigo Singh and Martin Heistermann and David Bommes and Denis Zorin},
  doi          = {10.1145/3731216},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Feature-aligned parametrization in penner coordinates},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational green and biharmonic coordinates for 2D polynomial cages. <em>TOG</em>, <em>44</em>(4), 1-20. (<a href='https://doi.org/10.1145/3731421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present closed-form expressions for Green and biharmonic coordinates with respect to polynomial curved 2D cages, enabling reliable cage-based image deformation both to and from a curved cage. We further provide closed-form expressions for first- and second-order derivatives of these coordinates with respect to the encoded position. This enables the use of variational solvers for interacting with the 2D shape at arbitrary points while keeping the fast decoding strength of cage-based deformation, which we illustrate for a variety of elastic deformation energies.},
  archive      = {J_TOG},
  author       = {Élie Michel and Alec Jacobson and Siddhartha Chaudhuri and Jean-Marc Thiery},
  doi          = {10.1145/3731421},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Variational green and biharmonic coordinates for 2D polynomial cages},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meschers: Geometry processing of impossible objects. <em>TOG</em>, <em>44</em>(4), 1-10. (<a href='https://doi.org/10.1145/3731422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Impossible objects, geometric constructions that humans can perceive but that cannot exist in real life, have been a topic of intrigue in visual arts, perception, and graphics, yet no satisfying computer representation of such objects exists. Previous work embeds impossible objects in 3D, cutting them or twisting/bending them in the depth axis. Cutting an impossible object changes its local geometry at the cut, which can hamper downstream graphics applications, such as smoothing, while bending makes it difficult to relight the object. Both of these can invalidate geometry operations, such as distance computation. As an alternative, we introduce Meschers, meshes capable of representing impossible constructions akin to those found in M.C. Escher's woodcuts. Our representation has a theoretical foundation in discrete exterior calculus and supports the use-cases above, as we demonstrate in a number of example applications. Moreover, because we can do discrete geometry processing on our representation, we can inverse-render impossible objects. We also compare our representation to cut and bend representations of impossible objects.},
  archive      = {J_TOG},
  author       = {Ana Dodik and Isabella Yu and Kartik Chandra and Jonathan Ragan-Kelley and Joshua Tenenbaum and Vincent Sitzmann and Justin Solomon},
  doi          = {10.1145/3731422},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Meschers: Geometry processing of impossible objects},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid tours: A clip-based system for authoring long-take touring shots. <em>TOG</em>, <em>44</em>(4), 1-13. (<a href='https://doi.org/10.1145/3731423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-take touring (LTT) shots are characterized by smooth camera motion over a long distance that seamlessly connects different views of the captured scene. These shots offer a compelling way to visualize 3D spaces. However, filming LTT shots directly is very difficult, and rendering them based on a virtual reconstruction of a scene is resource-intensive and prone to many visual artifacts. We propose Hybrid Tours , a hybrid approach to creating LTT shots that combines the capture of short clips representing potential tour segments with a custom interactive application that lets users filter and combine these segments into longer camera trajectories. We show that Hybrid Tours makes capturing LTT shots much easier than the traditional single-take approach, and that clip-based authoring and reconstruction leads to higher-fidelity results at a lower cost than common image-based rendering workflows.},
  archive      = {J_TOG},
  author       = {Xinrui Liu and Longxiulin Deng and Abe Davis},
  doi          = {10.1145/3731423},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Hybrid tours: A clip-based system for authoring long-take touring shots},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViSA: Physics-based virtual stunt actors for ballistic stunts. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3731424'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce ViSA (Virtual Stunt Actors), an interactive animation system designed to create realistic ballistic stunt actions frequently seen in filmmaking and TV production. By providing spatial constraints suitable for the desired stunt scene, our system generates physically plausible motions satisfying the given constraints. The problem is formulated as a deep reinforcement learning task, incorporating a novel state and action spaces, as well as straightforward yet effective rewards for ballistic stunt actions. Users can receive a fast response within several minutes and continue to choreograph complex stunt scenes in an interactive manner. We demonstrate ballistic stunt scenes resembling those in various films and TV dramas, such as traffic accidents, falling down stairs, and falls from buildings. The effectiveness of the technical components and design choices in our system is demonstrated through extensive comparisons, analyses, and ablation studies.},
  archive      = {J_TOG},
  author       = {Minseok Kim and Wonjeong Seo and Sung-Hee Lee and Jungdam Won},
  doi          = {10.1145/3731424},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {ViSA: Physics-based virtual stunt actors for ballistic stunts},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PhysicsFC: Learning user-controlled skills for a physics-based football player controller. <em>TOG</em>, <em>44</em>(4), 1-21. (<a href='https://doi.org/10.1145/3731425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose PhysicsFC, a method for controlling physically simulated football player characters to perform a variety of football skills-such as dribbling, trapping, moving, and kicking-based on user input, while seamlessly transitioning between these skills. Our skill-specific policies, which generate latent variables for each football skill, are trained using an existing physics-based motion embedding model that serves as a foundation for reproducing football motions. Key features include a tailored reward design for the Dribble policy, a two-phase reward structure combined with projectile dynamics-based initialization for the Trap policy, and a Data-Embedded Goal-Conditioned Latent Guidance (DEGCL) method for the Move policy. Using the trained skill policies, the proposed football player finite state machine (PhysicsFC FSM) allows users to interactively control the character. To ensure smooth and agile transitions between skill policies, as defined in the FSM, we introduce the Skill Transition-Based Initialization (STI), which is applied during the training of each skill policy. We develop several interactive scenarios to showcase PhysicsFC's effectiveness, including competitive trapping and dribbling, give-and-go plays, and 11v11 football games, where multiple PhysicsFC agents produce natural and controllable physics-based football player behaviors. Quantitative evaluations further validate the performance of individual skill policies and the transitions between them, using the presented metrics and experimental designs.},
  archive      = {J_TOG},
  author       = {Minsu Kim and Eunho Jung and Yoonsang Lee},
  doi          = {10.1145/3731425},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {ACM Trans. Graph.},
  title        = {PhysicsFC: Learning user-controlled skills for a physics-based football player controller},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning-based virtual oculoplastic surgery simulator. <em>TOG</em>, <em>44</em>(4), 1-15. (<a href='https://doi.org/10.1145/3731426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oculoplastic surgery is a critical treatment for various eye conditions, such as ptosis, which can cause both aesthetic and functional issues. Due to the anxiety about the outcome, patients are often hesitant to undergo the necessary procedures required for the surgery. Virtual oculoplastic surgery simulation technology offers a solution to alleviate these concerns by providing realistic previews of post-surgical results. In this paper, we present a novel deep learning-based virtual oculoplastic surgery simulation system that addresses the limitations of existing methods. The proposed system aims to improve the accuracy of simulations by considering the anatomical structure and characteristics of the eye. Our method utilizes a deformable parametric mesh to enhance the controllability of the image transformation process. Furthermore, the combination of a style-based generator and a neural texture has been implemented to generate high-quality results. The proposed system is expected to facilitate better communication between doctors and patients by providing anatomically inspired high-quality simulation results. The development of this advanced virtual simulation system has the potential to enhance patient experiences and improve satisfaction with outcomes in the field of oculoplastic surgery.},
  archive      = {J_TOG},
  author       = {Seonghyeon Kim and Chang Wook Seo and Kwanggyoon Seo and Seung Han Song and Junyong Noh},
  doi          = {10.1145/3731426},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A deep learning-based virtual oculoplastic surgery simulator},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instant self-intersection repair for 3D meshes. <em>TOG</em>, <em>44</em>(4), 1-14. (<a href='https://doi.org/10.1145/3731427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-intersection repair in static 3D surface meshes presents unique challenges due to the absence of temporal motion and penetration depth information—two critical elements typically leveraged in physics-based approaches. We introduce a novel framework that transforms local contact handling into a global repair strategy through a combination of local signed tangent-point energies and their gradient diffusion. At the heart of our method is a key insight: rather than computing expensive global repulsive potentials, we can effectively approximate long-range interactions by diffusing energy gradients from local contacts throughout the mesh surface. In turn, resolving complex self-intersections reduces to simply propagating local repulsive energies through standard diffusion mechanics and iteratively solving tractable local optimizations. We further accelerate convergence through our momentum-based optimizer, which adaptively regulates momentum based on gradient statistics to prevent overshooting while maintaining rapid intersection repair. The resulting algorithm handles a variety of challenging scenarios, from shallow contacts to deep penetrations, while providing computational efficiency suitable for interactive applications.},
  archive      = {J_TOG},
  author       = {Wonjong Jang and Yucheol Jung and Gyeongmin Lee and Seungyong Lee},
  doi          = {10.1145/3731427},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Instant self-intersection repair for 3D meshes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable shared template for consistent non-rigid ICP. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3731428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-rigid registration of 3D shape collections using a template mesh is essential for constructing 3D datasets. Traditional non-rigid Iterative Closest Point (ICP) methods rely on manually selected template meshes, which can result in inconsistent registrations when applied to diverse shape collections. This inconsistency arises particularly when the template lacks common shape features with the input instances or when landmark annotations are sparse. To overcome this limitation, we propose a novel ICP framework that jointly optimizes a shared template shape and its instance-wise deformations. Our joint optimization framework assigns distinct roles to the shared template and instance-wise deformations: the template captures common shape features, while instance-wise deformations handle residual registration errors. We use stronger smoothness regularization on the instance-wise deformations in early iterations to prioritize the accumulation of common details on the template. Additionally, a distortion alignment energy minimizes interinstance map distortions, promoting consistent instance-wise deformations. On challenging 3D datasets with large shape variations, our method achieves state-of-the-art fitting accuracy and consistent results in shape averaging and deformation transfer. By removing the need for a carefully selected preset template, our method extends the capability of extrinsic non-rigid registration frameworks, offering a more robust and flexible solution for challenging registration scenarios.},
  archive      = {J_TOG},
  author       = {Yucheol Jung and Hyomin Kim and Hyejeong Yoon and Yoonha Hwang and Seungyong Lee},
  doi          = {10.1145/3731428},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Variable shared template for consistent non-rigid ICP},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Light pipe holographic display: Bandwidth-preserved kaleidoscopic guiding for AR glasses. <em>TOG</em>, <em>44</em>(4), 1-12. (<a href='https://doi.org/10.1145/3731429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a holographic display using a light pipe for augmented reality, and the hologram rendering method via bandwidth-preserved kaleidoscopic guiding method. Conventional augmented reality displays typically share optical architectures where the light engine and image combiner are adjacent. Minimizing the size of both components is highly challenging, and most commercial and research prototypes of augmented reality displays are bulky, front-heavy and sight-obstructing. Here, we propose the use of light pipe to decouple and spatially reposition the light engine from the image combiner, enabling a pragmatic glasses-type design. Through total internal reflection, light pipes have an advantage in guiding the full angular bandwidth regardless of its length. By modeling such kaleidoscopic guiding of the wavefront inside the light pipe and applying it to holographic image generation, we successfully separate the light engine from the image combiner, making the front of the device clear and lightweight. We experimentally validate that the proposed light pipe system delivers virtual images with high-quality and 3D depth cues. We further present a method to simulate and compensate for light pipe misalignment, enhancing the robustness and practicality of the proposed system.},
  archive      = {J_TOG},
  author       = {Minseok Chae and Chun Chen and Seung-Woo Nam and Yoonchan Jeong},
  doi          = {10.1145/3731429},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Light pipe holographic display: Bandwidth-preserved kaleidoscopic guiding for AR glasses},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLoD: Integrating flexible level of detail into 3D gaussian splatting for customizable rendering. <em>TOG</em>, <em>44</em>(4), 1-16. (<a href='https://doi.org/10.1145/3731430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting (3DGS) has significantly advanced computer graphics by enabling high-quality 3D reconstruction and fast rendering speeds, inspiring numerous follow-up studies. However, 3DGS and its subsequent works are restricted to specific hardware setups, either on only low-cost or on only high-end configurations. Approaches aimed at reducing 3DGS memory usage enable rendering on low-cost GPU but compromise rendering quality, which fails to leverage the hardware capabilities in the case of higher-end GPU. Conversely, methods that enhance rendering quality require high-end GPU with large VRAM, making such methods impractical for lower-end devices with limited memory capacity. Consequently, 3DGS-based works generally assume a single hardware setup and lack the flexibility to adapt to varying hardware constraints. To overcome this limitation, we propose Flexible Level of Detail (FLoD) for 3DGS. FLoD constructs a multi-level 3DGS representation through level-specific 3D scale constraints, where each level independently reconstructs the entire scene with varying detail and GPU memory usage. A level-by-level training strategy is introduced to ensure structural consistency across levels. Furthermore, the multi-level structure of FLoD allows selective rendering of image regions at different detail levels, providing additional memory-efficient rendering options. To our knowledge, among prior works which incorporate the concept of Level of Detail (LoD) with 3DGS, FLoD is the first to follow the core principle of LoD by offering adjustable options for a broad range of GPU settings. Experiments demonstrate that FLoD provides various rendering options with trade-offs between quality and memory usage, enabling real-time rendering under diverse memory constraints. Furthermore, we show that FLoD generalizes to different 3DGS frameworks, indicating its potential for integration into future state-of-the-art developments.},
  archive      = {J_TOG},
  author       = {Yunji Seo and Young Sun Choi and HyunSeung Son and Youngjung Uh},
  doi          = {10.1145/3731430},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Graph.},
  title        = {FLoD: Integrating flexible level of detail into 3D gaussian splatting for customizable rendering},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
