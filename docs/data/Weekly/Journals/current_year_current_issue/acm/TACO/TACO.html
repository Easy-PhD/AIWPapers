<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TACO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taco">TACO - 37</h2>
<ul>
<li><details>
<summary>
(2025). HopScotch: A holistic approach to data layout-aware mapping on NPUs for high-performance DNN inference. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3711821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep neural networks (DNNs) are widely utilized across a broad range of domains, scaling rapidly and often comprising hundreds of diverse layers with varying types and configurations. To accelerate DNN execution, specialized hardware solutions, known as neural processing units (NPUs), have been developed. However, this heterogeneity of layers in a DNN model may cause performance degradation on NPUs. For example, while a layer’s execution or dataflow is generally associated with a specific data access order, the data layout in on-chip memory may not be well aligned with it, introducing bubble cycles for layout reordering. Given the hundreds of diverse layers in DNNs, this layout reordering overhead presents a new challenge for achieving efficient end-to-end DNN inference on NPUs. To address this problem, this article introduces HopScotch, a holistic approach to data layout-aware mapping of DNNs on NPUs. First, HopScotch adopts a routing interconnect between the on-chip memory and the systolic array utilizing three-input multiplexers, paired with an on-chip programmable vector processor to manage arbitrary data layout reordering at runtime. Additionally, it introduces a tailored data layout to accommodate a variety of convolutional configurations within the proposed microarchitecture. Second, HopScotch presents a novel layout mapping solver that employs a top-k selection strategy based on a beam search algorithm, facilitating the efficient exploration of the vast layout mapping space at compile time. Third, the proposed layout mapping solver is integrated into the HopScotch mapping framework (HMF) to explore the layout mapping space and evaluate the resulting performance. Experiments with popular DNN models show that HopScotch reduces layout reordering costs by up to 98.2% and 90.3%, resulting in speedups of 2.62× and 1.64× in end-to-end latency, compared to XLA and GCD 2 , respectively.},
  archive      = {J_TACO},
  author       = {Suhong Lee and Boyeal Kim and Yongseok Choi and Hyuk-Jae Lee},
  doi          = {10.1145/3711821},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {HopScotch: A holistic approach to data layout-aware mapping on NPUs for high-performance DNN inference},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LarQucut: A new cutting and mapping approach for large-sized quantum circuits in distributed quantum computing (DQC) environments. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3730585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed quantum computing (DQC) is a promising way to achieve large-scale quantum computing. However, mapping large-sized quantum circuits in DQC is a challenging job; for example, it is difficult to find an ideal cutting and mapping solution when many qubits, complicated qubit operations, and diverse QPUs are involved. In this study, we propose LarQucut, a new quantum circuit cutting and mapping approach for large-sized circuits in DQC. LarQucut has several new designs. (1) LarQucut can have cutting solutions that use fewer cuts, and it does not cut a circuit into independent sub-circuits, therefore reducing the overall cutting and computing overheads. (2) LarQucut finds isomorphic sub-circuits and reuses their execution results. So, LarQucut can reduce the number of sub-circuits that need to be executed to reconstruct the large circuit's output, reducing the time spent on sampling the sub-circuits. (3) We design an adaptive quantum circuit mapping approach, which identifies qubit interaction patterns and accordingly enables the best-fit mapping policy in DQC. The experimental results show that, for large circuits with hundreds to thousands of qubits in DQC, LarQucut can provide a better cutting and mapping solution with lower overall overheads and achieves results closer to the ground truth.},
  archive      = {J_TACO},
  author       = {Xinglei Dou and Lei Liu and Zhuohao Wang and Pengyu Li},
  doi          = {10.1145/3730585},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {LarQucut: A new cutting and mapping approach for large-sized quantum circuits in distributed quantum computing (DQC) environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards optimizing learned index for high performance, memory efficiency and NUMA awareness. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3736168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned indexes provide significant performance advantages over classical ordered indexes. However, current learned indexes face challenges regarding tradeoffs between performance and space, as well as scalability issues in platforms with multiple NUMA nodes. These limitations hinder the practical application of learned indexes in production environments. This article presents DiffLex, a learned index with high-performance, memory-efficiency, and NUMA-awareness. The core design of DiffLex is to perform differentiated management based on the popularity of data. For optimal performance, DiffLex stores newly inserted data in sparse delta arrays and frequently accessed data in sparse hot cache arrays. However, for cold data that occupy a majority of the storage space, DiffLex stores them in dense arrays and conducts compression to reduce memory costs. DiffLex ensures NUMA-awareness by partitioning sparse deltas and replicating the hot cache arrays across multiple NUMA nodes. Additionally, we propose a persistent version of DiffLex tailored for emerging persistent memory devices. Our evaluation results demonstrate that DiffLex achieving 3.88× and 1.82× performance improvements compared to state-of-the-art learned indexes, while maintaining a compact index size.},
  archive      = {J_TACO},
  author       = {Lixiao Cui and Kedi Yang and Yusen Li and Gang Wang and Xiaoguang Liu},
  doi          = {10.1145/3736168},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Towards optimizing learned index for high performance, memory efficiency and NUMA awareness},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benchmarking WebAssembly for embedded systems. <em>TACO</em>, <em>22</em>(3), 1-21. (<a href='https://doi.org/10.1145/3736169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WebAssembly is a modern, low-level virtual machine with designed for improved application performance in web browsers. Recently, WebAssembly gained interest for its use outside the web, for example as a replacement for serverless container runtimes. A number of non-web WebAssembly implementations are actively supported, some of which target microcontrollers, IoT devices, and embedded systems. Such hardware platforms have strict resource constraints which may render the usage of WebAssembly impossible or too costly, for example, due to its performance overhead and memory requirements. However, it is currently unclear what performance to expect of WebAssembly on low-resource microcontrollers compared with machine code and alternative application virtual machines. To answer this question, we evaluated the processing overhead and memory characteristics of WebAssembly application virtual machines on microcontrollers, and compared it to native execution, and the established application virtual machines: MicroPython and Lua. Furthermore, we analyzed the feature-set and architecture of the WebAssembly implementations in more detail, and measured the performance impact different runtime features have. We found that WebAssembly, despite its high extensibility and versatility in supported source languages, application paradigms, and target hardware, delivers very competitive performance. We conclude that WebAssembly can find wider industry usage for embedded systems and could replace other more costly or less flexible virtualization techniques, such as Java.},
  archive      = {J_TACO},
  author       = {Konrad Moron and Stefan Wallentowitz},
  doi          = {10.1145/3736169},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Benchmarking WebAssembly for embedded systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BigLittleMCA: A spatially-optimal tiled hardware accelerator for MCMC image processing. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3736171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov-Chain Monte-Carlo (MCMC) algorithms offer a general framework for performing interpretable inference but have high overheads due to the computational complexity of the sampling process and the large number of samples required to produce an accurate result. Computer Vision is a common class of workloads that can be performed using MCMC methods. As computer vision workloads trend toward high-resolution real-time inference, it becomes challenging to perform inference in contexts such as edge computing, which operates under strict power and area budgets. Previous work explores hardware techniques for efficient sampling; however, MCMC algorithms still require many samples. We reduce the overheads of Gibbs Sampling, an MCMC algorithm, using an approach we call mixed-resolution sampling. This approach uses low-resolution inference to provide a starting point for full-resolution sampling. We evaluate this approach on three important computer vision tasks: stereo matching, optical flow, and blind source separation. Mixed-resolution sampling reduces root mean square error (RMSE) by an average of 19.6% for stereo-matching tasks, 13% for optical flow tasks, and 6.3% for blind source separation relative to traditional Gibbs Sampling. To enable real-time, explainable MCMC inference under edge power constraints, we exploit the structure of mixed-resolution sampling to architect and implement a hardware-software co-designed accelerator architecture, BigLittleMCA ( Big - Little MC MC A ccelerator). BigLittleMCA is a tiled MCMC accelerator architecture that uses a small sampler for low-resolution sampling and a large sampler for full-resolution sampling. Our results show that the architecture sustains real-time 720p inference at 30 FPS (frames per second) using 48.5% less power than prior work.},
  archive      = {J_TACO},
  author       = {Chris Kjellqvist and Lisa Wills and Alvin Lebeck},
  doi          = {10.1145/3736171},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {BigLittleMCA: A spatially-optimal tiled hardware accelerator for MCMC image processing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attack and defense: Enhancing robustness of binary hyper-dimensional computing. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3736172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyper-Dimensional Computing (HDC) has emerged as a lightweight computational model, renowned for its robust and efficient learning capabilities, particularly suitable for resource-constrained hardware. As HDC often finds its application in edge devices, the associated security challenges pose a critical concern that cannot be ignored. In this work, we aim to quantitatively delve into the robustness of binary HDC, which is widely recognized for its robustness. Employing the bit-flip attack as our initial focal point, we meticulously devise both an attack mechanism and a corresponding defense mechanism. Our objective is to comprehensively explore the robustness of the binary hyper-dimensional computation model, aiming to gain a deeper understanding of its security vulnerabilities and potential defenses. Specifically, we introduce a novel attack framework for HDC, named HyperAttack, which is capable of compromising a robust binary HDC model by maliciously flipping a minimal number of bits within its memory system (specifically, the DRAM) that houses the associative memory. The bit-flip operation is executed through the well-known Row Hammer attack, and HyperAttack optimizes the accuracy degradation by pinpointing the most vulnerable bits in the hyper-dimensional vectors (represented as binary vectors within the associative memory) of the HDC model. The proposed HyperAttack framework is grounded in the principles of fuzziness, seamlessly integrating dimensional ranking and feature similarity analysis within hypervectors to precisely identify the bits to be flipped. Furthermore, we have developed a defense mechanism named HyperDefense, designed to bolster the robustness of binary hyper-dimensional computational models against bit-flip attacks. This defense scheme is tailored specifically for HDC models, providing a robust safeguard against potential threats. HyperDefense operates directly on the associative memory of HDC models, strengthening their defenses. By meticulously modifying selected bits, HyperDefense maintains a high level of accuracy close to the original model, even in the face of increased bit flip rates. This defense mechanism leverages redundant dimensions as backups for critical information. Through a thorough analysis of dimension importance, HyperDefense achieves superior robustness by gracefully sacrificing non-critical dimensions, thus ensuring the model’s robustness against potential attacks.},
  archive      = {J_TACO},
  author       = {Haomin Li and Fangxin Liu and Zongwu Wang and Ning Yang and Shiyuan Huang and Xiaoyao Liang and Haibing Guan and Li Jiang},
  doi          = {10.1145/3736172},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Attack and defense: Enhancing robustness of binary hyper-dimensional computing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GECC: A GPU-based high-throughput framework for elliptic curve cryptography. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3736176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elliptic Curve Cryptography (ECC) is an encryption method that provides security comparable to traditional techniques like Rivest–Shamir–Adleman (RSA) but with lower computational complexity and smaller key sizes, making it a competitive option for applications such as blockchain, secure multi-party computation, and database security. However, the throughput of ECC is still hindered by the significant performance overhead associated with elliptic curve (EC) operations, which can affect their efficiency in real-world scenarios. This article presents gECC, a versatile framework for ECC optimized for GPU architectures, specifically engineered to achieve high-throughput performance in EC operations. To maximize throughput, gECC incorporates batch-based execution of EC operations and microarchitecture-level optimization of modular arithmetic. It employs Montgomery’s trick [ 40 ] to enable batch EC computation and incorporates novel computation parallelization and memory management techniques to maximize the computation parallelism and minimize the access overhead of GPU global memory. Furthermore, we analyze the primary bottleneck in modular multiplication by investigating how the user codes of modular multiplication are compiled into hardware instructions and what these instructions’ issuance rates are. We identify that the efficiency of modular multiplication is highly dependent on the number of Integer Multiply-Add (IMAD) instructions. To eliminate this bottleneck, we propose novel techniques to minimize the number of IMAD instructions by leveraging predicate registers to pass the carry information and using addition and subtraction instructions (IADD3) to replace IMAD instructions. Our experimental results show that, for ECDSA and ECDH, the two commonly used ECC algorithms, gECC can achieve performance improvements of 5.56 × and 4.94 ×, respectively, compared to the state-of-the-art GPU-based system. In a real-world blockchain application, we can achieve performance improvements of 1.56 ×, compared to the state-of-the-art CPU-based system. gECC is completely and freely available at https://github.com/CGCL-codes/gECC .},
  archive      = {J_TACO},
  author       = {Qian Xiong and Weiliang Ma and Xuanhua Shi and Yongluan Zhou and Hai Jin and Kaiyi Huang and Haozhou Wang and Zhengru Wang},
  doi          = {10.1145/3736176},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GECC: A GPU-based high-throughput framework for elliptic curve cryptography},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging iterative applications to improve the scalability of task-based programming models on distributed systems. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3743134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed tasking models such as OmpSs-2@Cluster, StarPU-MPI, and PaRSEC express HPC applications as task graphs with explicit dependencies. The single task graph unifies the representation of parallelism across CPU cores, accelerators, and distributed-memory nodes, offering higher programmer productivity compared to traditional MPI + X. Most task-based models construct the task graph sequentially, which provides a clear and familiar programming model, simplifying code development, maintenance, and porting. However, this design introduces a bottleneck in task creation and dependency management, limiting performance and scalability. As a result, unless the tasks are very coarse-grained, current distributed sequential tasking models cannot match the performance of MPI + X. Many scientific applications, however, are iterative in nature, constructing the same directed acyclic task graph at each timestep. We exploit this structure to eliminate the sequential bottleneck and control message overhead in a sequentially-constructed distributed tasking model, while preserving its simplicity and productivity. Our approach builts on the recently proposed taskiter directive for OpenMP and OmpSs-2, allowing a single iteration to be expressed as a cyclic graph. The runtime partitions the cyclic graph across nodes, precomputes the MPI transfers, and then executes the loop body at low overhead. By integrating the MPI communications directly into the application’s task graph, our approach naturally overlaps computation and communication, in some cases exposing dramatically more parallelism than fork–join MPI + OpenMP. We define the programming model and describe the full runtime implementation, and integrate our proposal into OmpSs-2@Cluster. We evaluate it using five benchmarks on up to 128 nodes of the MareNostrum 5 supercomputer. For applications with fork–join parallelism, our approach has performance similar to fork–join MPI + OpenMP, making it a viable productive alternative, unlike the existing OmpSs-2@Cluster model, which is up to 7.7 times slower than MPI + OpenMP. For a 2D Gauss–Seidel stencil computation, our approach enables 3D wavefront computation, giving performance up to 22 times faster than fork–join MPI + OpenMP and on-a-par with state-of-the-art TAMPI + OmpSs-2. All software, comprising the compiler, runtime, and benchmarks, is released open source. 1},
  archive      = {J_TACO},
  author       = {Omar Shaaban Ibrahim ali and Juliette Fournis d'Albiat and Isabel Piedrahita and Vicenç Beltran and Xavier Martorell and Paul Carpenter and Eduard Ayguadé and Jesus Labarta},
  doi          = {10.1145/3743134},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Leveraging iterative applications to improve the scalability of task-based programming models on distributed systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling language chronology: Past, present, and future. <em>TACO</em>, <em>22</em>(3), 1-31. (<a href='https://doi.org/10.1145/3743135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling languages express to a compiler—or equivalently, a code generator—a sequence of optimizations to apply. Performance tools that support a scheduling language interface allow exploration of optimizations, i.e., exploratory compilers . While scheduling languages have become a common feature of tools for experts, the proliferation of these languages without unifying common features may be confusing to users. Moreover, we recognize a need to organize the compiler developer community around common exploratory compiler infrastructure, and future advances to address, for example, data layout and data movement. To support a broader set of users may require raising the level of abstraction. This article provides a chronology of scheduling languages, discussing their origins in iterative compilation and autotuning, noting the common features that are used in existing frameworks, and calling for changes to increase their utility and portability.},
  archive      = {J_TACO},
  author       = {Mary Hall and Cosmin E. Oancea and Anne C. Elster and Ari Rasch and Sameeran Joshi and Amir Mohammad Tavakkoli and Richard Schulze},
  doi          = {10.1145/3743135},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-31},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Scheduling language chronology: Past, present, and future},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In-SRAM parallel data shuffle. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3743136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Single Instruction Multiple Data (SIMD) units are widely employed in processors for neural networks, signal processing, and high-performance computing, they suffer from expensive shuffle operations dedicated to data alignment. In fact, shuffle operations only change the layout of data and ideally should be done entirely within memory. To this end, we propose Shuffle SRAM in this article, which can shuffle multiple data elements simultaneously across SRAM banks. The key idea is exploiting inter-bank word line wise data movement to shuffle data in parallel, where all data elements on the same word line of SRAM can be shuffled simultaneously, achieving a high level of parallelism. Through suitable data layout preparation and proper control, Shuffle SRAM efficiently supports a wide range of commonly used shuffle operations. Our evaluation results show that the Shuffle SRAM can reap performance benefits of 14.3× for data reorganization only applications and 1.97× for data reorganization + computation applications over conventional shuffle architecture on general-purpose processors. With Shuffle SRAM, the state-of-the-art vector processor can obtain 2.58× energy efficiency. Compared with traditional SRAM, Shuffle SRAM only increases 3.5% additional area overhead.},
  archive      = {J_TACO},
  author       = {Chaoyang Jia and Zhang Dunbo and Qingjie Lang and Ruoxi Wang and Li Shen},
  doi          = {10.1145/3743136},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {In-SRAM parallel data shuffle},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDAS: Enabling fast data loading for GPU serverless computing. <em>TACO</em>, <em>22</em>(3), 1-23. (<a href='https://doi.org/10.1145/3743137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating GPUs into serverless computing platforms is crucial for improving efficiency. Many GPU functions, such as DNN inferences and scientific services, benefit from GPU usage, which requires only tens to hundreds of milliseconds for pure computation. Under these circumstances, fast data loading is imperative for function performance. However, existing GPU serverless systems face significant data stall issues, leading to extremely low GPU efficiency. Faced with the above problems, we observe opportunities to optimize data loading, such as data preloading and deduplicated data loading. However, these optimizations are impossible in existing GPU serverless systems due to the lack of insights into data information, such as data sizes and read-write attributes of function inputs. To address this, we propose a novel GPU serverless system, EDAS. EDAS first enhances user request specifications, allowing users to annotate data retrieved by GPU functions from the database with additional attributes. Based on this, EDAS takes over data loading from GPU functions and proposes two innovative data loading management schemes: a parallelized data loading scheme and a multi-stage resource exit scheme. Our experimental results show that EDAS reduces function duration by 16.2× and improves system throughput by 1.91× compared with the state-of-the-art serverless platform.},
  archive      = {J_TACO},
  author       = {Han Zhao and Weihao Cui and Quan Chen and Zijun Li and Zhenhua Han and Nan Wang and Yu Feng and Jieru Zhao and Chen Chen and Jingwen Leng and Minyi Guo},
  doi          = {10.1145/3743137},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {EDAS: Enabling fast data loading for GPU serverless computing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CGCGraph: Efficient CPU-GPU co-execution for concurrent dynamic graph processing. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3744904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous growth of user scale and application data, the demand for large-scale concurrent graph processing is increasing. Typically, large-scale concurrent graph processing jobs need to process corresponding snapshots of dynamically changing graph data to obtain information at different time points. To enhance the throughput of such applications, current solutions concurrently process multiple graph snapshots on the GPU. However, when dealing with rapidly changing graph data, transferring multiple snapshots of concurrent jobs to the GPU results in high data transfer overhead between CPU and GPU. Additionally, the execution mode of existing work suffers from underutilization of GPU computational resources. In this work, we introduce CGCGraph, which can be integrated into existing GPU graph processing systems like Subway, to enable efficient concurrent graph snapshot processing jobs and enhance overall system resource utilization. The key idea is to offload unshared graph data of multiple concurrent snapshots to the CPU, reducing CPU-GPU transfer overhead. By implementing CPU-GPU co-execution, there is potential for enhanced utilization of GPU computing resources. Specifically, CGCGraph leverages kernel fusion to process shared graph data concurrently on the GPU, while executing all snapshots in parallel on the CPU, with each snapshot assigned a dedicated thread. This approach enables efficient concurrent processing within a novel CPU-GPU co-execution model, incorporating three optimization strategies targeting storage, computation, and synchronization. We integrate CGCGraph with Subway, an existing system designed for out-of-GPU-memory static graph processing. Experimental results show that the integration of CGCGraph with current GPU-based systems obtains performance improvements ranging from 1.7 to 4.5 times.},
  archive      = {J_TACO},
  author       = {Yiming Sun and Jie Zhang and Huawei Cao and Yuan Zhang and Xuejun An and Junying Huang and Xiaochun Ye},
  doi          = {10.1145/3744904},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {CGCGraph: Efficient CPU-GPU co-execution for concurrent dynamic graph processing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaEC: An efficient and resilient erasure-coded KV store on disaggregated memory. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3744905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory KV stores have recently been migrated from traditional monolithic servers to disaggregated memory (DM) for higher resource utilization and elasticity. These works use replication-based schemes for fault tolerance, which can be replaced with erasure coding (EC) for space efficiency. However, existing EC schemes designed in KV stores on traditional monolithic architectures encounter performance constraints when directly implemented in DM due to the challenges in EC metadata management and consistent parity updating. This article proposes MetaEC, an erasure-coded KV store on DM with high efficiency and resilience. First, for organizing KV pairs to stripes, MetaEC logically forms data chunks and leverages lazy coding to remove the accumulating and coding latency from the critical path. Second, for efficient EC metadata management, MetaEC designs EC metadata structures based on accessing features, and employs a hybrid redundancy schema with deterministic distribution to provide fault tolerance with high storage efficiency. Third, for consistent parity updating, we design a parity updating protocol based on parity logging and co-design EC metadata structures to handle concurrent conflicts by allowing only concurrent reads or writes. Experimental results show that compared with the state-of-the-art replication-based KV stores on DM, MetaEC achieves up to 53.33% latency reduction, up to 31.01% throughput improvement, and 58.17% memory consumption savings.},
  archive      = {J_TACO},
  author       = {Qiliang Li and Min Lyu and Tian Liu and Liangliang Xu and Wei Wang and Yinlong Xu},
  doi          = {10.1145/3744905},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {MetaEC: An efficient and resilient erasure-coded KV store on disaggregated memory},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating parallel structures in DNNs via parallel fusion and operator co-optimization. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3744906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel structures have become a key pattern in deep neural networks (DNNs), offering improved efficiency and scalability. However, existing machine learning compilers (MLCs) face challenges in optimizing these structures due to limited parallel fusion scope and insufficient analysis of intra-operator characteristics. This article introduces Magneto, a framework designed to accelerate DNN inference by co-optimizing parallel operators. Magneto broadens the fusion scope and incorporates a specialized co-tuning algorithm to optimize operators jointly. Our approach addresses the unique challenges inherent in optimizing parallel structures, enabling significant performance improvements across various hardware platforms. Experimental results show that Magneto outperforms state-of-the-art NVIDIA TensorRT and AMD MIGraphX, achieving geometric mean speedups of 2.27× and 2.88×, respectively.},
  archive      = {J_TACO},
  author       = {Zhanyuan Di and Leping Wang and Zhaojia Ma and En Shao and Jie Zhao and Ziyi Ren and Siyuan Feng and Dingwen Tao and Guangming Tan and Ninghui Sun},
  doi          = {10.1145/3744906},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Accelerating parallel structures in DNNs via parallel fusion and operator co-optimization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage degradation-based topology reconfiguration algorithm for fault-tolerant multiprocessor arrays. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3744907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the integration density of multiprocessor arrays increases, the likelihood of permanent faults in processing elements (PEs) rises, requiring effective topology reconfiguration for system reliability. However, existing router-based multiprocessor arrays reconfiguration methods predominantly rely on redundancy techniques and lack effective degradation strategies for applications of varying sizes. To address this, we propose a two-stage degradation-based topology reconfiguration algorithm to construct a maximized and high-performance logical array. First, we introduce a novel fault compensation mechanism by defining a set of faulty PE candidates to identify locally optimal fault-free PEs for compensation, minimizing the compensation path. Building upon this, we develop a greedy bidirectional column reconfiguration algorithm that constructs an initial fault-free logical array with short interconnects and prove its maximality. Lastly, we propose a satisfiability-based reconfiguration algorithm, transforming the topology reconfiguration problem into a satisfiability problem via a SAT model, reducing interconnect redundancy, and further optimizing array performance. Experimental results demonstrate that the proposed algorithm consistently outperforms state-of-the-art methods in reducing communication latency and alleviating link congestion, especially under high fault density conditions. Furthermore, as array size and fault density increase, the effectiveness of the proposed method becomes more pronounced, showcasing excellent scalability and robustness.},
  archive      = {J_TACO},
  author       = {Hao Ding and Peiling Song and Yelin Li and Junyan Qian},
  doi          = {10.1145/3744907},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A two-stage degradation-based topology reconfiguration algorithm for fault-tolerant multiprocessor arrays},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Address/Data instruction steering in clustered general purpose processors. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3744908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although they differentiate between integer and floating-point datum, modern Instruction Set Architectures and their implementations do not differentiate integer datum used to address memory from integer datum used in purely arithmetic and logical computations. This is a perfectly reasonable choice as addresses are, in fact, integral quantities. However, in many cases, there is already a fundamental difference between addresses and integer data: Their width. As computer systems moved from 16 to 32, then to 64-bit pointers, with a potential future where 128-bit might be used for specific systems, the data width required to compute a given output with a given algorithm has remained the same, e.g., an ASCII character is still represented on a byte. This work aims to leverage this dichotomy to revisit hardware clustering, a well-known microarchitectural technique used to mitigate the cost of scaling processor backend structures by dividing the backend into several mostly independent execution clusters. We show that by treating instructions as manipulating addresses or data and steering them to a “data” or an “address” cluster accordingly, reasonable cluster load balancing can be achieved without the need for complex steering policies that can lead to performance on par with the baseline with limited hardware overhead. Moreover, we highlight two possible optimizations stemming from this distribution. First, the registers of the “address” cluster can easily be compressed thanks to address spatial and temporal locality. Second, if a processor requires a large address space but only processes narrow data (e.g., 32-bit data with 64-bit pointers or 64-bit data with 128-bit pointers), the “data” cluster datapath can be kept narrower than the “address” cluster datapath.},
  archive      = {J_TACO},
  author       = {Chandana S. Deshpande and Arthur Perais and Frédéric Pétrot},
  doi          = {10.1145/3744908},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Address/Data instruction steering in clustered general purpose processors},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D GNLM: Efficient 3D non-local means kernel with nested reuse strategies for embedded GPUs. <em>TACO</em>, <em>22</em>(3), 1-22. (<a href='https://doi.org/10.1145/3744909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D Non-Local Means (NLM) algorithm has become a crucial preprocessing technique for 3D image datasets due to its effectiveness in denoising while preserving fine details. This method has been proven to be highly efficient in high-demand tasks within industrial applications such as medical imaging and remote sensing. The 3D NLM algorithm computes the filtered value for each voxel by calculating the weighted average of all voxels within a 3D search window, where the weights are determined by the similarity between pairs of 3D template windows. Therefore, the computational burden becomes significant, especially in embedded GPUs with limited computational power and memory resources. To address this issue, we propose an efficient GPU parallel kernel to minimize redundant computations and memory accesses. The kernel integrates three nested reuse strategies to handle redundant computations in three dimensions: for columns, we leverage the fast data exchange mechanism to reuse column computation results via on-chip registers; for rows, we use a sliding window strategy, utilizing GPU global memory as an intermediary to store and reuse similarity values between filtered rows; and for channels, we introduce a zigzag scanning strategy that enables simultaneous computation across multiple channels and employs on-chip registers to facilitate channel computation reuse. Experimental results demonstrate that our kernel achieves an average speedup of 7.7x on the embedded Jetson AGX Xavier platform across a range of 3D image datasets compared to existing methods, showcasing exceptional performance.},
  archive      = {J_TACO},
  author       = {Xiang Li and Qiong Chang and Yun Li and Jun Miyazaki},
  doi          = {10.1145/3744909},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {3D GNLM: Efficient 3D non-local means kernel with nested reuse strategies for embedded GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ZNSFQ: An efficient and high-performance fair queue scheduling scheme for ZNS SSDs. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3746230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Zoned Namespace (ZNS) interface transfers most storage maintenance responsibilities from the underlying Solid-State Drives (SSDs) to the host. This shift creates new opportunities to ensure fairness and high performance in multi-tenant cloud computing environments at both hardware and software levels. However, when applications with different workloads share a single ZNS SSD hardware, traditional fair queueing schedulers fail to achieve fairness due to their limited awareness of workload characteristics. Moreover, allowing multiple outstanding requests to access the device simultaneously improves resource utilization but often leads to significant I/O interference among these requests. This interference results in over-throttling, which subsequently degrades the performance of existing fair queueing schedulers. To address the above problems, this article proposes an efficient and high-performance fair queueing scheduling scheme for ZNS SSD (ZNSFQ) on the host side. Firstly, ZNSFQ introduces a workload-aware fair scheduler that enhances fairness by accurately estimating the I/O cost for each application based on its workload characteristics. Secondly, to optimize performance while ensuring fairness, ZNSFQ designs a request dispatch parallelism adjuster. This adjuster manages the channel-level request dispatch parallelism for each application to minimize I/O interference. Finally, ZNSFQ employs a global adaptive coordinator to alleviate device-level I/O blocking, reducing tail latency and CPU consumption while satisfying fairness and performance. A comprehensive evaluation demonstrates that ZNSFQ significantly enhances fairness and performance compared to the latest fair queuing schedulers. In sequential access scenarios, ZNSFQ enhances fairness by over 38.13% and increases I/O bandwidth by more than 49.24%. Furthermore, in random access scenarios, it reduces CPU utilization by 70.22% while maintaining both fairness and high performance.},
  archive      = {J_TACO},
  author       = {Yachun Liu and Dan Feng and Jianxi Chen and Jing Hu and Zhouxuan Peng and Jinlei Hu},
  doi          = {10.1145/3746230},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {ZNSFQ: An efficient and high-performance fair queue scheduling scheme for ZNS SSDs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance implications of pipelining the data transfer in CPU-GPU heterogeneous systems. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3746231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the increasing demands of machine learning, heterogeneous systems combining CPUs and GPUs have emerged as the dominant architecture for parallel computing in recent years. To optimize memory management and data transfer between CPUs and GPUs, Nvidia GPUs have introduced unified virtual memory ( UVM ) and pinned memory ( PM ) over the last decade. UVM can avoid explicit memory copies and potentially overlap GPU kernel computations with CPU-GPU data transfer. PM ensures that data with high locality remains in the main memory, preventing it from being paged out. In addition to these two techniques, asynchronous memory copy ( Async Memcpy ) was introduced recently in Nvidia GPUs to improve the CPU-GPU pipeline further. By utilizing Async Memcpy , the data transfer from GPU global memory to shared memory can be overlapped with GPU computations, adding an additional stage to the CPU-GPU data transfer pipeline. A thorough performance analysis of how Async Memcpy affects the current UVM and PM CPU-GPU data transfer scheme is desired. In this article, we provide performance implications of the combined effect of UVM , PM , and Async Memcpy , exploring which applications benefit from which combination of these features. We implement all these features on a suite of 25 workloads, including microbenchmarks and realworld applications. We observe an average performance gain of 24% when utilizing UVM and a 34% gain when employing PM on realworld applications, compared to not applying any data transfer optimization techniques. The performance benefits of Async Memcpy vary across different workloads. For workloads featuring extensive shared memory usage and high compute density (e.g., kmeans and lud ), Async Memcpy delivers around a 20% performance improvement over using UVM or PM alone. In other workloads like knn , we note a 20% performance degradation when using Async Memcpy . Furthermore, we conduct an in-depth investigation of the GPU kernel using performance counters to uncover the root causes of performance differences among various data transfer models. We also perform sensitivity analyses to examine how the number of blocks and threads, as well as the L1-cache/shared memory partitioning, impact performance. We explore future research directions aimed at enhancing the data transfer pipeline by overlapping memory allocation with data transfer and computation across GPU kernels.},
  archive      = {J_TACO},
  author       = {Ruihao Li and Bagus Hanindhito and Sanjana Yadav and Qinzhe Wu and Krishna Kavi and Gayatri Mehta and Neeraja J. Yadwadkar and Lizy K. John},
  doi          = {10.1145/3746231},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Performance implications of pipelining the data transfer in CPU-GPU heterogeneous systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partitioned scheduling and analysis for a typed DAG task on heterogeneous multi-cores. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3746232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous multi-core architectures are gaining popularity in recent years as they combine the benefits of different processors, resulting in improved execution capacity and energy efficiency. However, analyzing response times and allocating resources for the typed directed acyclic graph (DAG) task, which has complex execution logic, on heterogeneous multi-core systems poses significant challenges. Major approaches may yield overly pessimistic worst-case response time (WCRT) estimates in certain scenarios while failing to adequately address critical structural characteristics inherent to typed DAG tasks. To address these limitations, this article explores the WCRT analysis and core allocations for the typed DAG task under partitioned scheduling. In this work, we first delve into the characteristics of the topology structure of the typed DAG task and propose a novel WCRT upper bound to enhance the accuracy of WCRT analysis. Then, a subtask allocation strategy is presented, which enables an effectively utilization of the resources of multi-cores. Finally, the performance of the proposed analysis algorithm and allocation strategy are tested by implementing a verification system on a real heterogeneous multi-core platform. Experimental results demonstrate that our proposed WCRT analysis algorithm exhibits substantial improvements of 38.7% and 37.43% in the theoretical analysis performance and actual analysis accuracy, respectively. Similarly, our proposed core allocation strategy improves the theoretical and the actual execution efficiency of the system by 10.6% and 7.41%, respectively. These results substantiate the practical value of our enhanced WCRT derivation methodology and allocation scheme in improving system resource utilization efficiency.},
  archive      = {J_TACO},
  author       = {Yulong Wu and Yehan Ma and Mingdong Xie and Weizhe Zhang},
  doi          = {10.1145/3746232},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Partitioned scheduling and analysis for a typed DAG task on heterogeneous multi-cores},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCSolver: Accelerating sparse iterative solvers via divide-and-conquer on GPUs. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3746233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse iterative solvers are commonly used in various fields. However, certain essential kernels of these solvers, such as sparse triangular solves (SpTRSV), present significant challenges for efficient parallelization due to data dependencies . Previous methods, like level-scheduling or multi-coloring, typically involve creating a Task Dependency Graph (TDG) to represent data dependencies and identify independent sets from the TDG for parallel execution. However, these approaches often result in limited parallelism with substantial synchronization overheads or negatively impact the solver convergence rate. This article introduces DCSolver , a Divide-and-Conquer (DC) framework designed to efficiently parallelize sparse solvers with data dependencies on GPUs. To achieve this, we break down the solver TDG into independent subgraphs, allowing us to exploit both coarse-grained and fine-grained parallelism. To efficiently allocate GPU threads for subgraphs with varying degrees of parallelism, we have developed an adaptive in-warp scheduling strategy. Additionally, we propose a hybrid parallelization scheme in DCSolver, which involves employing different parallel approaches for different DC recursions to achieve a more optimal balance between parallelism and convergence for solvers. To evaluate the effectiveness of DCSolver, we apply it to two preconditioned Krylov subspace solvers and an unstructured mesh Computational Fluid Dynamics (CFD) solver. Our results show that when compared with the state-of-the-art methods, DCSolver accelerates the time-to-solution of solvers by an average speedup of up to 26.19X.},
  archive      = {J_TACO},
  author       = {Haozhong Qiu and Chuanfu Xu and Jianbin Fang and Jian Zhang and Liang Deng and Zhe Dai and Yue Ding and Yue Wang and Zhimeng Han and Yonggang Che and Jie Liu},
  doi          = {10.1145/3746233},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {DCSolver: Accelerating sparse iterative solvers via divide-and-conquer on GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cppless: Single-source and high-performance serverless programming in c++. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3747841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of serverless computing introduced a new class of scalable, elastic, and widely available parallel workers in the cloud. Many systems and applications benefit from offloading computations and parallel tasks to dynamically allocated resources. However, the developers of C++ applications find it difficult to integrate functions due to complex deployment, lack of compatibility between client and cloud environments, and loosely typed input and output data. To enable single-source and efficient serverless acceleration in C++, we introduce Cppless , an end-to-end framework for implementing remote functions which handles the creation, deployment, and invocation of serverless functions. Cppless is built on top of LLVM and requires only two compiler extensions to automatically extract C++ function objects and deploy them to the cloud. We demonstrate that offloading parallel computations, such as from a C++ application to serverless workers, can provide up to 59x speedup with minimal cost increase while requiring only minor code modifications.},
  archive      = {J_TACO},
  author       = {Marcin Copik and Lukas Möller and Alexandru Calotoiu and Torsten Hoefler},
  doi          = {10.1145/3747841},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Cppless: Single-source and high-performance serverless programming in c++},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile-3DCNN: An acceleration framework for ultra-real-time execution of large 3D CNNs on mobile devices. <em>TACO</em>, <em>22</em>(3), 1-22. (<a href='https://doi.org/10.1145/3747842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to deploy 3D Convolutional Neural Networks (3D CNNs) on mobile devices, specifically if both real-time execution and high inference accuracy are in demand, because the increasingly large model size and complex model structure of 3D CNNs usually require tremendous computation and memory resources. Weight pruning is proposed to mitigate this challenge. However, existing pruning is either not compatible with modern parallel architectures, resulting in long inference latency or subject to significant accuracy degradation. This article proposes an end-to-end 3D CNN acceleration framework based on pruning/compilation co-design called Mobile-3DCNN that consists of two parts: a novel, fine-grained structured pruning enhanced by a prune/Winograd adaptive selection (that is mobile-hardware-friendly and can achieve high pruning accuracy), and a set of compiler optimization and code generation techniques enabled by our pruning (to fully transform the pruning benefit to real performance gains). The evaluation demonstrates that Mobile-3DCNN outperforms state-of-the-art end-to-end DNN acceleration frameworks that support 3D CNN execution on mobile devices, Alibaba Mobile Neural Networks and Pytorch-Mobile with speedup up to 34× with minor accuracy degradation, proving it is possible to execute high-accuracy large 3D CNNs on mobile devices in real-time (or even ultra-real-time).},
  archive      = {J_TACO},
  author       = {Wei Niu and Mengshu Sun and Zhengang Li and Jou-An Chen and Jiexiong Guan and Xipeng Shen and Jun Liu and Mei Zhang and Yanzhi Wang and Xue Lin and Bin Ren},
  doi          = {10.1145/3747842},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Mobile-3DCNN: An acceleration framework for ultra-real-time execution of large 3D CNNs on mobile devices},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TRACED: A temporal graph neural networks-based model for data prefetching. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3747843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern microarchitectures, machine-learning-based prefetchers use past memory requests to learn access patterns and predict memory addresses, thereby prefetching data into the cache to mitigate the processor-memory speed gap. However, they face two key challenges in capturing irregular access patterns generated by complex data structures and algorithms. One is data dispersion: the disorderliness of memory addresses makes it difficult for prefetchers to extract meaningful data features. The other is temporal and spatial complexity: existing prefetchers fail to effectively learn temporal and spatial characteristics, and thus are unable to explore more complex access patterns. To resolve these challenges, we propose TRACED, a novel temporal graph neural network-based prefetcher aimed at learning access patterns of memory addresses. TRACED consists of two key components: a dynamic clustering component and a temporal graph neural network component. In the dynamic clustering component, we introduce a similarity function to quantify the similarity of memory addresses. Based on the quantified similarity, we dynamically group unordered memory addresses into different clusters. This ensures that the memory addresses in each cluster are ordered and change smoothly, thus resolving the first challenge. The temporal graph neural network component constructs a spatiotemporal graph to represent relationships among memory addresses. This helps capture temporal and spatial characteristics both across and within clusters, thus resolving the second challenge. This article demonstrates the effectiveness of the proposed prefetcher through experiments. Specifically, in terms of accuracy, TRACED outperforms BO, SPP, DOMINO, Delta-LSTM, and VOYAGER by 2.29%–40.83% on average. Furthermore, TRACED attains remarkable coverage of 55.67% and IPC of 43.75%, outperforming all competing approaches in both metrics.},
  archive      = {J_TACO},
  author       = {He Jiang and Liuwei Fu and Dong Liu and Zhilei Ren and Yuting Chen and Lei Qiao},
  doi          = {10.1145/3747843},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {TRACED: A temporal graph neural networks-based model for data prefetching},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GenCNN: A partition-aware multi-objective mapping framework for CNN accelerators based on genetic algorithm. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3747844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) require partitioning to efficiently run on CNN accelerators, which offer multiple parallel processing dimensions, such as Processing Element (PE) array topologies and Single Instruction Multiple Data (SIMD) execution. The choice of parallelization strategy directly impacts accelerator performance. However, the vast search space for CNN partitioning and parallelization makes manual optimization costly and complex, especially when addressing both aspects simultaneously. This highlights the need for an automated framework to efficiently map CNNs onto accelerators. Our key insight is that existing approaches suffer from inadequate accelerator performance modeling and a lack of multi-objective optimization strategies that jointly consider task partitioning and convolution parallelization. To address this, we propose GenCNN, a multi-objective genetic algorithm-based mapping framework for CNN accelerators. GenCNN first constructs a fine-grained performance model that captures both off-chip data access and on-chip data processing. It then applies the Non-dominated Sorting Genetic Algorithm II improved by Multi-Objective Bayesian Optimization to derive a Pareto-optimal partitioning and parallelization strategy that balances off-chip latency and PE utilization. Finally, GenCNN optimizes scheduling and routing to minimize data transfers. Experimental results show that GenCNN achieves up to 17.66× speedup in compilation and 6.47× in execution compared with state-of-the-art mapping frameworks.},
  archive      = {J_TACO},
  author       = {Yudong Mu and Zhihua Fan and Wenming Li and Zhiyuan Zhang and Xuejun An and Dongrui Fan and Xiaochun Ye},
  doi          = {10.1145/3747844},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {GenCNN: A partition-aware multi-objective mapping framework for CNN accelerators based on genetic algorithm},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supports of data cache division for computational solid-state drives. <em>TACO</em>, <em>22</em>(3), 1-20. (<a href='https://doi.org/10.1145/3747845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational SSD ( CompSSD ), with high computing capabilities, can function not only as a storage device but also as a computing node. The data cache of the CompSSD device stores both the output data from host-side tasks and the input data for tasks executed on the CompSSD . However, current cache management strategies are optimized for traditional SSDs and are incompatible with the unique requirements of CompSSD . To address the issue of cache management for CompSSD , this article proposes a novel cache division scheme, to dynamically divide the cache into two parts, for separately buffering output data from host-side tasks and input data used by CompSSD -side tasks. To this end, we construct a mathematical model that periodically estimate an optimal cache division ratio, by considering the factors of the ratios of read/write data amount, the cache hits, and the overhead of data transfer between the storage device and the host. Besides, we propose a scheme of proactive data flushing to write the output data to the underlying flash arrays, without impacts on I/O responsiveness. The trace-driven experiments show that our scheme can improve the overall I/O latency by 35.4% on average, in contrast to existing cache management schemes for CompSSD devices.},
  archive      = {J_TACO},
  author       = {Zhibing Sha and Shuaiwen Yu and Chengyong Tang and Zhigang Cai and Peng Tang and Ming Huang and Jun Li and Jianwei Liao},
  doi          = {10.1145/3747845},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Supports of data cache division for computational solid-state drives},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ephemera: Accelerating I/O-intensive serverless workloads with a harvested in-memory file system. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3747846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing has gained popularity for its ability to shift the burden of server management from developers to cloud providers, which allows providers to exercise greater control over resource management, optimizing configurations to enhance efficiency and performance. The diversity of serverless computing tasks, from short-lived, event-driven tasks to more complex workloads, highlights the growing importance of efficient file I/O performance for I/O-intensive workloads, yet effectively handling ephemeral storage for I/O-intensive tasks remains a challenge. Traditional file system approaches often introduce substantial latency and fail to fully leverage available memory resources within the execution environment, limiting performance and efficiency. Our work stems from the observation of the under-utilization of memory resources in serverless computing platforms and the potential efficiency improvement of I/O operations using an in-memory file system. Based on this observation, we propose Ephemera , a system designed to enhance ephemeral storage efficiency and memory utilization. Ephemera satisfies three design goals: transparent memory I/O integration , heterogeneous tasks resource synergy , and harmonized cluster workload orchestration . Ephemera integrates three components: the Runtime Daemon, responsible for managing a container’s in-memory file system; the Tenant Manager, facilitating memory configuration sharing across containers; and the Cluster Controller, optimizing workload balancing. Our experiments demonstrate that Ephemera significantly improves performance for I/O-intensive tasks compared to traditional file systems. Specifically, Ephemera decreases I/O processing time by 50% on average and reduces latency by up to 95.73% in certain scenarios with negligible overhead.},
  archive      = {J_TACO},
  author       = {Lingxiao Jin and Zinuo Cai and Haoxin Wang and Zongpu Zhang and Ruhui Ma and Haibing Guan and Yuan Liu and Buyya Rajkumar},
  doi          = {10.1145/3747846},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Ephemera: Accelerating I/O-intensive serverless workloads with a harvested in-memory file system},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpMARD: A sparse-sparse matrix multiplication accelerator with reconfigurable dataflow for DNN workloads. <em>TACO</em>, <em>22</em>(3), 1-23. (<a href='https://doi.org/10.1145/3747847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning becomes increasingly popular, and its main workload is Sparse-Sparse Matrix Multiplication (SpMSpM). Most SpMSpM accelerators usually only support a single dataflow. Different dataflows have different performance in different computing environments. Therefore, the single-dataflow accelerator cannot maintain the highest performance in all environments. Compared with single-dataflow accelerators, multi-dataflow accelerators provide flexible options for different workloads and improve the overall performance. Flexagon, Sparm, and SPADA are state-of-the-art multi-dataflow accelerators. However, the computation process of Flexagon and Sparm is not fully pipelined, and SPADA cannot support inner product dataflow. Additionally, Flexagon, Sparm, and SPADA cannot switch dataflows quickly and accurately. Inspired by these observations, we present SpMARD, a SpMSpM accelerator with reconfigurable dataflow. The computation process of SpMARD is fully pipelined, and SpMARD can support six dataflow variants simultaneously. Through the design of a Two-stage Pipeline Adder Network (TPAN) and a Position-based Psum Array (PPA), SpMARD can execute element-level merging, which can hide the merging overhead. Through the quantitative analysis of dataflows, we implement a Dataflow Switcher (DSwitcher), which can switch dataflows more efficiently. For the SpMSpM workload, the performance (GOPS) of the SpMARD we proposed is 1.27 times that of Flexagon, 1.18 times that of Sparm, and 1.22 times that of SPADA.},
  archive      = {J_TACO},
  author       = {Bo Wang and Sheng Ma and Yunping Zhao and Shengbai Luo and Lizhou Wu and Jianmin Zhang and Dongsheng Li and Tiejun Li and Zhuojun Chen},
  doi          = {10.1145/3747847},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SpMARD: A sparse-sparse matrix multiplication accelerator with reconfigurable dataflow for DNN workloads},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAKV: A hotness-aware zone management approach to optimizing performance of LSM-tree-based key-value stores. <em>TACO</em>, <em>22</em>(3), 1-26. (<a href='https://doi.org/10.1145/3747848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log-Structured Merge tree-based key-value (KV) stores, like LevelDB and RocksDB, are extensively applied in large-scale data storage systems. This design excels in write-intensive environments by converting random writes into sequential append operations. Despite its advantages, KV stores struggle with real-world workloads where most updates in KV pairs are infrequent. The compaction process and hierarchical data organization result in high write and read amplification. To mitigate these issues, we propose HAKV – a hotness-aware zone management approach to optimizing performance of KV stores. HAKV first separates hot KV pairs from cold KV pairs, storing hot KV pairs in dedicated zones within persistent memory (PM), enabling centralized and lightweight compaction. Second, we propose a storage zone structure in PM to achieve space optimization for cold KV pairs. Third, to bolster cache hit ratio in PM, we provide a hierarchical data framework for hot KV pairs – and a recycling strategy for invalid hot KV pairs in a zone to enhance the space utilization of PM for hot KV pairs. Finally, we design a dynamic window-based adaptive adjustment mechanism for zone pool in PM to optimize the space utilization. Thus, HAKV significantly reduces write amplification while boosting overall read and write performance. The experimental results demonstrate that HAKV achieves write amplification reduction by up to 92.3%, 79.2%, 90.2%, 41.1%, 80.6%, and 62.4% compared with LevelDB, RocksDB, NoveLSM, LightKV, Wisckey, and UniKV, respectively, with average reduction rates of 89.6%, 74.4%, 84.9% 32.3%, 63.7%, and 42.5%. Furthermore, HAKV boosts random write performance by up to 54.2×, 51.5×, 44.2×, 4.3×, 3.1×, and 4.3×, respectively—and the average improvement reaches 25.8×, 20.9×, 23.9×, 2.7×, 2.5×, and 3.4×.},
  archive      = {J_TACO},
  author       = {Hui Sun and Qianli Yue and Guanzhong Chen and Yi Zou and Yinliang Yue and Xiao Qin},
  doi          = {10.1145/3747848},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {HAKV: A hotness-aware zone management approach to optimizing performance of LSM-tree-based key-value stores},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matrix: Multi-cipher structures dataflow for parallel and pipelined TFHE accelerator. <em>TACO</em>, <em>22</em>(3), 1-23. (<a href='https://doi.org/10.1145/3750446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully homomorphic encryption over torus (TFHE) enables the execution of arbitrary functions on encrypted data through programmable bootstrapping (PBS). However, performing all operations on ciphertext during PBS results in high computational and memory requirements, limiting the deployment of PBS in real-world scenarios. Previous TFHE accelerator designs have attempted to improve performance by employing specific dataflow and functional units, but these techniques may require large off-chip bandwidth or on-chip storage when scaling up computation capacity. Additionally, the design of specialized functional units may limit the utilization of computation units when facing dynamic secure parameter settings. To address these challenges and further improve PBS throughput in TFHE, we propose Matrix , an ASIC-based architecture that balances off-chip bandwidth and on-chip storage according to the execution flow of PBS. In Matrix , we utilize a unified special-prime-based processing element (PE) that achieves high utilization with minimal resource overhead. Furthermore, we propose a hybrid PBS dataflow that can efficiently reduce computation complexity and memory requirements. Compared to state-of-the-art TFHE accelerators, Matrix achieves 1.43 × -5.66 × throughput improvement for PBS. For ZAMA Deep-NN benchmark, we achieve 525.60× and 68.06× speedup compared to CPU and GPU, respectively. 1},
  archive      = {J_TACO},
  author       = {Ling Liang and Zhen Gu and Fahong Zhang and Zhaohui Chen and Zhirui Li and Xin Fan and Dimin Niu and Meng Li and Zhiyong Li and Zongwei Wang and Hongzhong Zheng and Yimao Cai and Yuan Xie},
  doi          = {10.1145/3750446},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Matrix: Multi-cipher structures dataflow for parallel and pipelined TFHE accelerator},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SampDedup: Sampling prediction for efficient inline data deduplication on non-volatile memory. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3750447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication is an effective technique for reducing redundant data storage space in various storage systems. Generally, deduplication consists of four steps: chunking, fingerprinting, fingerprint lookup, and data management. Recently, Non-volatile Memory (NVM) as an emerging storage device has received widespread attention. Directly applying the deduplication technique on NVM for storage cost savings faces many challenges: (a) deduplication on NVM devices suffers from computation bottleneck instead of the I/O bottleneck faced by deduplication on traditional storage devices (such as HDD and SSD); (b) new fingerprint indexes and metadata are required to be re-designed to adapt to NVM characteristics; (c) inline deduplication on NVM is more sensitive to the latency. To solve these challenges, we propose a novel Samp ling prediction-based inline data Dedup lication method ( SampDedup ) on NVM devices. It aims to ensure high deduplication ratios while reducing computation costs and latency by optimizing data chunking , fingerprinting , and fingerprint lookup . (a) For data chunking , a sampling prediction-based chunking method ( SampChunk ) is proposed to leverage chunk similarity to distinguish duplicate chunks and skip them for chunking. This method can be easily integrated into most sliding-window based and non-window based CDC chunking algorithms. (b) For fingerprinting , the commonly used SHA-1 algorithm is further optimized to reduce the extra computational overhead introduced by SampChunk, and an asynchronous fingerprinting method is proposed to reduce the fingerprinting latency of unique chunks. (c) For fingerprint lookup , we design a header fingerprint index and metadata table for each data chunk constructed by SampChunk on NVM, and we use a fast-read buffer to replace the traditional slow LRU cache to improve search efficiency. Experiments on four real-world datasets demonstrate that SampDedup consistently presents high inline data deduplication ratios on NVM with different workloads and data partitioning algorithms while saving more than 90% chunking time compared with state-of-the-art deduplication baselines.},
  archive      = {J_TACO},
  author       = {Ziyue Xu and Yichen Li and Ranzhe Deng and Liping Yi and Yusen Li and Gang Wang and Xiaoguang Liu},
  doi          = {10.1145/3750447},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {SampDedup: Sampling prediction for efficient inline data deduplication on non-volatile memory},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RACER: Avoiding end-to-end slowdowns in accelerated chip multi-processors. <em>TACO</em>, <em>22</em>(3), 1-22. (<a href='https://doi.org/10.1145/3750448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent chip multiprocessors incorporate several on-chip accelerators, marking the beginning of the Accelerated Chip Multi-Processor (XMP) era in datacenters. Despite the close proximity of accelerators and general-purpose cores, offloading functions to accelerators may not always be beneficial. Offloading to hardware accelerators can introduce several end-to-end overheads that can negate the speedup of the accelerable function. In this article, we design RACER, a hardware architecture and runtime system that evades the danger of end-to-end slowdowns when using hardware acceleration. RACER leverages a low-overhead interface between general-purpose cores and on-chip accelerators, fine-grained context switching, accelerator-initiated preemption, and seamless data motion between general-purpose cores and accelerators to improve the performance of workloads that use on-chip accelerators. We evaluate RACER on five representative request processing workloads featuring diverse memory access patterns, accelerable functions, and compute intensities. RACER improves the performance of hardware acceleration on a real XMP by an average of 1.31× on a range of diverse workloads and guarantees that accelerator offloads never cause slowdowns.},
  archive      = {J_TACO},
  author       = {Neel Patel and Ren Wang and Mohammad Alian},
  doi          = {10.1145/3750448},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {RACER: Avoiding end-to-end slowdowns in accelerated chip multi-processors},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sparsity-aware autonomous path planning accelerator with HW/SW co-design and multi-level dataflow optimization. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3750449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning is a critical task for autonomous driving, aiming to generate smooth, collision-free, and feasible paths based on input perception and localization information. The planning task is both highly time-sensitive and computationally intensive, posing significant challenges to resource-constrained autonomous driving hardware. In this article, we propose an end-to-end framework for accelerating path planning on FPGA platforms. This framework focuses on accelerating quadratic programming (QP) solving, which is the core of optimization-based path planning and has the most computationally-intensive workloads. Our method leverages a hardware-friendly alternating direction method of multipliers (ADMM) to solve QP problems while employing a highly parallelizable preconditioned conjugate gradient (PCG) method for solving the associated linear systems. We analyze the sparse patterns of matrix operations in QP and design customized storage schemes along with efficient sparse matrix multiplication and sparse matrix-vector multiplication units. Our customized design significantly reduces resource consumption for data storage and computation while dramatically speeding up matrix operations. Additionally, we propose a multi-level dataflow optimization strategy. Within individual operators, we achieve acceleration through parallelization and pipelining. For different operators in an algorithm, we analyze inter-operator data dependencies to enable fine-grained pipelining. At the system level, we map different steps of the planning process to the CPU and FPGA and pipeline these steps to enhance end-to-end throughput. We implement and validate our design on the AMD ZCU102 platform. Our implementation achieves state-of-the-art performance in both latency and energy efficiency compared with existing works, including an average 1.48× speedup over the best FPGA-based design, a 2.89× speedup compared with the state-of-the-art QP solver on an Intel i7-11800H CPU, a 5.62× speedup over an ARM Cortex-A57 embedded CPU, and a 1.56× speedup over state-of-the-art GPU-based work. Furthermore, our design delivers a 2.05× improvement in throughput compared with the state-of-the-art FPGA-based design.},
  archive      = {J_TACO},
  author       = {Yifan Zhang and Xiaoyu Niu and Hongzheng Tian and Yanjun Zhang and Bo Yu and Shaoshan Liu and Sitao Huang},
  doi          = {10.1145/3750449},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A sparsity-aware autonomous path planning accelerator with HW/SW co-design and multi-level dataflow optimization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TianheGraph: Topology-aware graph processing. <em>TACO</em>, <em>22</em>(3), 1-24. (<a href='https://doi.org/10.1145/3750450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world graph data can have billions to trillions of edges. Processing graphs at such scales requires the efficient use of parallel computing systems. However, current graph processing engines and methods struggle to scale beyond a few dozen computing nodes because they (i) cannot efficiently store and process graph data on this scale due to the huge memory footprint incurred and (ii) do not account for the variations in communication costs across different levels of the interconnection hierarchy. We introduce TianheGraph, a software approach to reduce the memory footprint of graphs and optimize graph processing on large-scale parallel systems with complex hardware interconnection components. TianheGraph integrates a new space-time-efficient graph compression technique to reduce the memory footprint of large-scale graphs. It provides a novel graph partitioning method to improve load balancing and minimize communication overhead across various levels of the interconnection hierarchy. We evaluate TianheGraph by applying it to fundamental graph operations on synthetic and real-world graphs, using up to 79,024 computing nodes and over 1.2 million processor cores. Our extensive experiments show that TianheGraph outperforms state-of-the-art parallel graph processing engines in throughput and scalability. Moreover, TianheGraph outperformed the top-ranked systems on the Graph 500 list at the time of submission.},
  archive      = {J_TACO},
  author       = {Xinbiao Gan},
  doi          = {10.1145/3750450},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {TianheGraph: Topology-aware graph processing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A low-latency on-chip cache hierarchy for load-to-use stall reduction in GPUs. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3760782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory hierarchy in Graphics Processing Units (GPUs) is conventionally designed to provide high bandwidth rather than low latency. In particular, because of the high tolerance to load-to-use latency (i.e., the time that warps wait for data fetched by memory loads), GPU L1D caches are optimized for density, capacity, and low power with latencies that are often orders of magnitude longer than conventional CPU caches. However, there are many important classes of data-parallel applications (e.g., graph, tree, priority queue processing, and sparse deep learning applications) that benefit from lower load-to-use latency than that offered by modern GPUs due to their inherent divergence and low effective Thread-Level Parallelism (TLP). This article introduces an innovative on-chip cache hierarchy that incorporates a decoupled L1D cache with reduced latency (LoTUS) and its management scheme. LoTUS is a minimally sized fully associative cache placed in each GPU subcore that captures the primary working set of data-parallel applications. It exploits conventional high-performance low-density SRAM cells and dramatically reduces load-to-use latency. We also propose an intelligent extension of LoTUS, called LoTUSage, which employs a lightweight learning-based model to predict the utility of caching requests in LoTUS. Evaluation results show that LoTUS and LoTUSage improve the average performance by 23.9% and 35.4% and reduce the average energy consumption by 27.8% and 38.5%, respectively, for the applications suffering from high load-to-use stalls with negligible area and power overheads.},
  archive      = {J_TACO},
  author       = {Negin (Sadat) (Nematollahi zadeh) Mahani and Hajar Falahati and Sina Darabi and Ahmad Javadi-Nezhad and Yunho Oh and Mohammad Sadrosadati and Hamid Sarbazi-Azad and Babak Falsafi},
  doi          = {10.1145/3760782},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {A low-latency on-chip cache hierarchy for load-to-use stall reduction in GPUs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ecmas+: Efficient circuit mapping and scheduling for surface code encoded circuit on quantum cloud platform. <em>TACO</em>, <em>22</em>(3), 1-25. (<a href='https://doi.org/10.1145/3760783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the leading candidate for quantum error correction, the surface code faces substantial overhead, such as redundant physical qubits and prolonged execution time. Reducing the space-time cost of circuit execution can significantly improve the throughput of modern quantum cloud platforms. While utilizing more physical qubits can reduce execution time, different quantum circuits vary in their ability to leverage chip resources. Therefore, optimizing the compilation of surface code circuits on quantum chips becomes critical. In this work, we address the mapping and scheduling problem in compiling surface code to reduce the cost. First, we introduce a novel metric Circuit Parallelism Degree to characterize circuit properties in detail and select the most suitable chip from a list of available options. Next, we will quantitatively assess the resources to determine if they are sufficient for the circuit. We then propose a resource-adaptive mapping and scheduling method called Ecmas+ , which customizes the initialization of chip resources for each circuit. Ecmas+ significantly reduces execution time in the double defect and lattice surgery models. Extensive numerical tests on practical datasets demonstrate that Ecmas+ outperforms state-of-the-art methods, reducing execution time by an average of 46% for the double defect model and 29.7% for the lattice surgery model.},
  archive      = {J_TACO},
  author       = {Mingzheng Zhu and Hao Fu and Haishan Song and Jun Wu and Chi Zhang and Wei Xie and Xiangyang Li},
  doi          = {10.1145/3760783},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Ecmas+: Efficient circuit mapping and scheduling for surface code encoded circuit on quantum cloud platform},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augur: Semantics-aware temporal prefetching for linked data structure. <em>TACO</em>, <em>22</em>(3), 1-27. (<a href='https://doi.org/10.1145/3762997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linked data structures (LDS), such as lists and trees, are widely used in modern applications. Traversing LDS typically involves a significant amount of pointer chasing. Due to the serial nature of memory access in pointer chasing, the incurred long memory latency of traversing LDS has become a critical performance bottleneck. Furthermore, the poor spatial locality in LDS makes it difficult for spatial prefetchers to predict access addresses. Although temporal prefetchers can handle irregular memory access patterns, hindered by the challenges of collecting semantic information, current state-of-the-art temporal prefetchers suffer from significant metadata redundancy and frequent metadata conflicts. Consequently, there remain substantial opportunities to enhance the LDS prefetching. To solve this problem, we propose Augur, a semantics-aware temporal prefetcher to enhance LDS performance. Augur utilizes a novel pruning method to obtain semantic information and effectively extracts node address correlations from the perspective of nodes in LDS, thereby diminishing the metadata redundancy and conflicts. Additionally, Augur employs efficient metadata management strategies that guarantee a minimal storage overhead. Evaluated on LDS workloads, Augur achieves an average performance speedup of 17.8% and 11.7% over the baseline stride prefetcher and state-of-the-art spatial prefetcher Berti, respectively. Furthermore, Augur outperforms the state-of-the-art temporal prefetcher MISB, Triage, and Triangel, by 17.4%, 12.8%, and 6.3%, respectively, with a significantly lower storage overhead of only 1.26 KB.},
  archive      = {J_TACO},
  author       = {Feng Xue and Junliang Wu and Chenji Han and Xinyu Li and Tingting Zhang and Tianyi Liu and Fuxin Zhang},
  doi          = {10.1145/3762997},
  journal      = {ACM Transactions on Architecture and Code Optimization},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  title        = {Augur: Semantics-aware temporal prefetching for linked data structure},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
