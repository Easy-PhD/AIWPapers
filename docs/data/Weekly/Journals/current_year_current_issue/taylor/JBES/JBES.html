<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JBES</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jbes">JBES - 19</h2>
<ul>
<li><details>
<summary>
(2025). Another look at dependence: The most predictable aspects of time series. <em>JBES</em>, <em>43</em>(3), 723-736. (<a href='https://doi.org/10.1080/07350015.2024.2424345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serial dependence and predictability are two sides of the same coin. The literature has considered alternative measures of these two fundamental concepts. In this article, we aim to distill the most predictable aspect of a univariate time series, that is, the one for which predictability is optimized. Our target measure is the mutual information between the past and future of a random process, a broad measure of predictability that takes into account all future forecast horizons, rather than focusing on the one-step-ahead prediction error mean square error. The most predictable aspect is defined as the measurable transformation of the series that maximizes the mutual information between past and future. This transformation arises from the linear combination of a set of basis functions localized at the quantiles of the unconditional distribution of the process. The mutual information is estimated as a function of the sample partial autocorrelations, using a semiparametric method that estimates an infinite sum by a regularized finite sum. The second most predictable aspect can also be defined, subject to suitable orthogonality restrictions. Finally, we illustrate the use of the most predictable aspect for testing the null hypothesis of no predictability and for point and interval prediction of the original time series.},
  archive      = {J_JBES},
  author       = {Tommaso Proietti},
  doi          = {10.1080/07350015.2024.2424345},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {723-736},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Another look at dependence: The most predictable aspects of time series},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibrated equilibrium estimation and double selection for high-dimensional partially linear measurement error models. <em>JBES</em>, <em>43</em>(3), 710-722. (<a href='https://doi.org/10.1080/07350015.2024.2422982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, measurement error data is frequently encountered and needs to be handled appropriately. As a result of additional bias induced by measurement error, many existing estimation methods fail to achieve satisfactory performances. This article studies high-dimensional partially linear measurement error models. It proposes a calibrated equilibrium (CARE) estimation method to calibrate the bias caused by measurement error and overcomes the technical difficulty of the objective function unbounded from below in high-dimensional cases due to non-convexity. To facilitate the applications of the CARE estimation method, a bootstrap approach for approximating the covariance matrix of measurement errors is introduced. For the high-dimensional or ultra-high dimensional partially linear measurement error models, a novel multiple testing method, the calibrated equilibrium multiple double selection (CARE–MUSE) algorithm, is proposed to control the false discovery rate (FDR). Under certain regularity conditions, we derive the oracle inequalities for estimation error and prediction risk, along with an upper bound on the number of falsely discovered signs for the CARE estimator. We further establish the convergence rate of the nonparametric function estimator. In addition, FDR and power guarantee for the CARE–MUSE algorithm are investigated under a weaker minimum signal condition, which is insufficient for the CARE estimator to achieve sign consistency. Extensive simulation studies and a real data application demonstrate the satisfactory finite sample performance of the proposed methods.},
  archive      = {J_JBES},
  author       = {Jingxuan Luo and Gaorong Li and Heng Peng and Lili Yue},
  doi          = {10.1080/07350015.2024.2422982},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {710-722},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Calibrated equilibrium estimation and double selection for high-dimensional partially linear measurement error models},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive kernel-based structural change test for copulas. <em>JBES</em>, <em>43</em>(3), 696-709. (<a href='https://doi.org/10.1080/07350015.2024.2422980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a structural change test for copula models based on the kernel smoothing method. The proposed approach enables adaptable estimation of the dynamic marginal distributions, either parametrically or semi-parametrically. The test statistic is formulated via the weighted quadratic distance between the local smoothing copula and the empirical copula function, using pseudo-observations of marginal distributions. The test statistic is pivotal with an asymptotic standard Normal distribution, irrespective of the marginal distributions, parameters, and estimations, and is consistent against a wide range of smoothly transitioning structural changes as well as abrupt structural breaks for copula models. Monte Carlo simulations show that the test performs well in finite samples and outperforms existing tests in the case of periodic changes.},
  archive      = {J_JBES},
  author       = {Xiaohui Lu and Yahong Zhou},
  doi          = {10.1080/07350015.2024.2422980},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {696-709},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {An adaptive kernel-based structural change test for copulas},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining instrumental variable estimators for a panel data model with factors. <em>JBES</em>, <em>43</em>(3), 684-695. (<a href='https://doi.org/10.1080/07350015.2024.2421991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the estimation of factor-augmented panel data models using observed measurements to proxy for unobserved factors or loadings and explore the use of internal instruments to address the resulting endogeneity. The main challenge consists in that economic theory rarely provides insights into which measurements to choose as proxies when several are available. To overcome this problem, we propose a new class of estimators that are linear combinations of instrumental variable estimators and establish large sample results. We also show that an optimal weighting scheme exists, leading to efficiency gains relative to an instrumental variable estimator. Simulations show that the proposed approach performs better than existing methods. We illustrate the new method using data on test scores across U.S. school districts.},
  archive      = {J_JBES},
  author       = {Matthew Harding and Carlos Lamarche and Chris Muris},
  doi          = {10.1080/07350015.2024.2421991},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {684-695},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Combining instrumental variable estimators for a panel data model with factors},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural phillips curve and a deep output gap. <em>JBES</em>, <em>43</em>(3), 669-683. (<a href='https://doi.org/10.1080/07350015.2024.2421279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many problems plague empirical Phillips curves (PCs). Among them is the hurdle that the two key components, inflation expectations and the output gap, are both unobserved. Traditional remedies include proxying for the absentees or extracting them via assumptions-heavy filtering procedures. I propose an alternative route: a Hemisphere Neural Network (HNN) whose architecture yields a final layer where components can be interpreted as latent states within a Neural PC. First, HNN conducts the supervised estimation of nonlinearities that arise when translating a high-dimensional set of observed regressors into latent states. Second, forecasts are economically interpretable. Among other findings, the contribution of real activity to inflation appears understated in traditional PCs. In contrast, HNN captures the 2021 upswing in inflation and attributes it to a large positive output gap starting from late 2020. The unique path of HNN’s gap comes from dispensing with unemployment and GDP in favor of an amalgam of nonlinearly processed alternative tightness indicators.},
  archive      = {J_JBES},
  author       = {Philippe Goulet Coulombe},
  doi          = {10.1080/07350015.2024.2421279},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {669-683},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A neural phillips curve and a deep output gap},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group sparse β-model for network. <em>JBES</em>, <em>43</em>(3), 657-668. (<a href='https://doi.org/10.1080/07350015.2024.2418849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparsity, homogeneity, and heterogeneity are three important characteristics of many real-life networks. The recently proposed Sparse β -Model divides nodes into core ones and peripheral ones to accommodate sparsity, but the parameters of core nodes are assumed to be of similar magnitude, which may not be in line with applications. In this article, we propose the Group Sparse β -Model that splits the core nodes into groups and assumes different orders of magnitude of parameters in different groups, accounting for the heterogeneity among core nodes. When the groups are known, we provide consistent and asymptotically normal moment estimators of the parameters that control the global and local density. Based on that, consistency and asymptotic normality of the maximum likelihood estimators of the remaining parameters are derived. We also establish finite-sample error bounds results. When the groups are unknown, a ratio method is proposed to detect groups, which is computationally efficient. Simulations show competitive results and the analysis of a corporate inter-relationships network illustrates the usefulness of the proposed model.},
  archive      = {J_JBES},
  author       = {Zhonghan Wang and Junlong Zhao},
  doi          = {10.1080/07350015.2024.2418849},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {657-668},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Group sparse β-model for network},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for equal average forecast accuracy in possibly unstable environments. <em>JBES</em>, <em>43</em>(3), 643-656. (<a href='https://doi.org/10.1080/07350015.2024.2418835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the issue of testing the null of equal average forecast accuracy in a model where the forecast error loss differential series has a potentially nonconstant mean function over time. We show that when time variation is present in the loss differential mean, the standard Diebold and Mariano test, which was proposed for evaluating forecasts in a stable environment, has an asymptotic size of zero, and, whilst consistent, can have reduced local power. This arises due to inconsistent estimation of the implicit long run variance estimator, which diverges under a time varying mean. We suggest a modified statistic that replaces the standard long run variance estimator based on full-sample demeaning of the loss differential series with one based on nonparametric local demeaning. The new long run variance estimator is consistent under both the null and alternative when the mean function is time varying or constant, and in both cases, the modified test recovers the asymptotic size and power properties associated with the original test in the constant mean case. The modified test therefore provides a robust method for testing the equal average forecast accuracy null, allowing for instability in the loss differential mean. The benefits of our test are demonstrated via Monte Carlo simulation and two empirical applications.},
  archive      = {J_JBES},
  author       = {David I. Harvey and Stephen J. Leybourne and Yang Zu},
  doi          = {10.1080/07350015.2024.2418835},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {643-656},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Testing for equal average forecast accuracy in possibly unstable environments},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference in experiments with matched pairs and imperfect compliance. <em>JBES</em>, <em>43</em>(3), 627-642. (<a href='https://doi.org/10.1080/07350015.2024.2416972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies inference for the local average treatment effect in randomized controlled trials with imperfect compliance where treatment status is determined according to “matched pairs.” By “matched pairs,” we mean that units are sampled iid from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. Under weak assumptions governing the quality of the pairings, we first derive the limit distribution of the usual Wald (i.e., two-stage least squares) estimator of the local average treatment effect. We show further that conventional heteroscedasticity-robust estimators of the Wald estimator’s limiting variance are generally conservative, in that their probability limits are (typically strictly) larger than the limiting variance. We therefore provide an alternative estimator of the limiting variance that is consistent. Finally, we consider the use of additional observed, baseline covariates not used in pairing units to increase the precision with which we can estimate the local average treatment effect. To this end, we derive the limiting behavior of a two-stage least squares estimator of the local average treatment effect which includes both the additional covariates in addition to pair fixed effects, and show that its limiting variance is always less than or equal to that of the Wald estimator. To complete our analysis, we provide a consistent estimator of this limiting variance. A simulation study confirms the practical relevance of our theoretical results. Finally, we apply our results to revisit a prominent experiment studying the effect of macroinsurance on microenterprise in Egypt.},
  archive      = {J_JBES},
  author       = {Yuehao Bai and Hongchang Guo and Azeem M. Shaikh and Max Tabord-Meehan},
  doi          = {10.1080/07350015.2024.2416972},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {627-642},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Inference in experiments with matched pairs and imperfect compliance},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized sparse covariance regression with high dimensional covariates. <em>JBES</em>, <em>43</em>(3), 615-626. (<a href='https://doi.org/10.1080/07350015.2024.2415109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariance regression offers an effective way to model the large covariance matrix with the auxiliary similarity matrices. In this work, we propose a sparse covariance regression (SCR) approach to handle the potentially high-dimensional predictors (i.e., similarity matrices). Specifically, we use the penalization method to identify the informative predictors and estimate their associated coefficients simultaneously. We first investigate the Lasso estimator and subsequently consider the folded concave penalized estimation methods (e.g., SCAD and MCP). However, the theoretical analysis of the existing penalization methods is primarily based on iid data, which is not directly applicable to our scenario. To address this difficulty, we establish the non-asymptotic error bounds by exploiting the spectral properties of the covariance matrix and similarity matrices. Then, we derive the estimation error bound for the Lasso estimator and establish the desirable oracle property of the folded concave penalized estimator. Extensive simulation studies are conducted to corroborate our theoretical results. We also illustrate the usefulness of the proposed method by applying it to a Chinese stock market dataset.},
  archive      = {J_JBES},
  author       = {Yuan Gao and Zhiyuan Zhang and Zhanrui Cai and Xuening Zhu and Tao Zou and Hansheng Wang},
  doi          = {10.1080/07350015.2024.2415109},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {615-626},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Penalized sparse covariance regression with high dimensional covariates},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oracle inequality for multivariate dynamic quantile forecasting. <em>JBES</em>, <em>43</em>(3), 603-614. (<a href='https://doi.org/10.1080/07350015.2024.2415107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I derive an oracle inequality for a family of possibly misspecified multivariate conditional autoregressive quantile models. The family includes standard specifications for (nonlinear) quantile prediction proposed in the literature. This inequality is used to establish that the predictor that minimizes the in-sample average check loss achieves the best out-of-sample performance within its class at a near optimal rate, even when the model is fully misspecified. An empirical application to backtesting global Growth-at-Risk shows that a combination of the generalized autoregressive conditionally heteroscedastic model and the vector autoregression for Value-at-Risk performs best out-of-sample in terms of the check loss.},
  archive      = {J_JBES},
  author       = {Jordi Llorens-Terrazas},
  doi          = {10.1080/07350015.2024.2415107},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {603-614},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {An oracle inequality for multivariate dynamic quantile forecasting},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correcting for misclassified binary regressors using instrumental variables. <em>JBES</em>, <em>43</em>(3), 592-602. (<a href='https://doi.org/10.1080/07350015.2024.2415102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimators that exploit an instrumental variable to correct for misclassification in a binary regressor typically assume that the misclassification rates are invariant across all values of the instrument. We show this assumption is invalid in routine empirical settings. We derive a new estimator which allows misclassification rates to vary across values of the instrumental variable. Our key identifying assumption, that the sum of misclassification rates remains constant across instrument values, follows from the empirical examples we present. We also show this assumption can be relaxed using moment inequalities that arise from our model. We demonstrate the usefulness of our estimator through Monte Carlo simulations and a reanalysis of the extent to which Medicaid eligibility crowds out other forms of health insurance. Correcting for measurement error substantially reduces estimates of crowd out and the extent to which Medicaid eligibility lowers the share of the uninsured.},
  archive      = {J_JBES},
  author       = {Steven J. Haider and Melvin Stephens Jr.},
  doi          = {10.1080/07350015.2024.2415102},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {592-602},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Correcting for misclassified binary regressors using instrumental variables},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation for threshold autoregressive moving-average models. <em>JBES</em>, <em>43</em>(3), 579-591. (<a href='https://doi.org/10.1080/07350015.2024.2412011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Threshold autoregressive moving-average (TARMA) models extend the popular TAR model and are among the few parametric time series specifications to include a moving average in a nonlinear setting. The state dependent reactions to shocks is particularly appealing in Economics and Finance. However, no theory is currently available when the data present heavy tails or anomalous observations. Here we provide the first theoretical framework for robust M -estimation for TARMA models and study its practical relevance. Under mild conditions, we show that the robust estimator for the threshold parameter is super-consistent, while the estimators for autoregressive and moving-average parameters are strongly consistent and asymptotically normal. The Monte Carlo study shows that the M -estimator is superior, in terms of both bias and variance, to the least squares estimator, which can be heavily affected by outliers. The findings suggest that robust M -estimation should be generally preferred to the least squares method. We apply our methodology to a set of commodity price time series; the robust TARMA fit presents smaller standard errors and superior forecasting accuracy. The results support the hypothesis of a two-regime non-linearity characterized by slow expansions and fast contractions.},
  archive      = {J_JBES},
  author       = {Greta Goracci and Davide Ferrari and Simone Giannerini and Francesco Ravazzolo},
  doi          = {10.1080/07350015.2024.2412011},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {579-591},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Robust estimation for threshold autoregressive moving-average models},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting panel data binary choice models with lagged dependent variables. <em>JBES</em>, <em>43</em>(3), 568-578. (<a href='https://doi.org/10.1080/07350015.2024.2412006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article revisits the identification and estimation of a class of semiparametric (distribution-free) panel data binary choice models with lagged dependent variables, exogenous covariates, and entity fixed effects. We provide a novel identification strategy, using an “identification at infinity” argument. In contrast with the celebrated work by Honoré and Kyriazidou published in 2000, our method permits time trends of any form and does not suffer from the “curse of dimensionality”. We propose an easily implementable conditional maximum score estimator. The asymptotic properties of the proposed estimator are fully characterized. A small-scale Monte Carlo study demonstrates that our approach performs satisfactorily in finite samples. We illustrate the usefulness of our method by presenting an empirical application to enrollment in private hospital insurance using the Household, Income and Labor Dynamics in Australia (HILDA) Survey data.},
  archive      = {J_JBES},
  author       = {Christopher R. Dobronyi and Fu Ouyang and Thomas Tao Yang},
  doi          = {10.1080/07350015.2024.2412006},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {568-578},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Revisiting panel data binary choice models with lagged dependent variables},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Endogenous kink threshold regression. <em>JBES</em>, <em>43</em>(3), 556-567. (<a href='https://doi.org/10.1080/07350015.2024.2407634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers an endogenous kink threshold regression model with an unknown threshold value in a time series as well as a panel data framework, where both the threshold variable and regressors are allowed to be endogenous. We construct our estimators from a nonparametric control function approach and derive the consistency and asymptotic distribution of our proposed estimators. Monte Carlo simulations are used to assess the finite sample performance of our proposed estimators. Finally, we apply our model to analyze the impact of COVID-19 cases on labor markets in the United States and Canada.},
  archive      = {J_JBES},
  author       = {Jianhan Zhang and Chaoyi Chen and Yiguo Sun and Thanasis Stengos},
  doi          = {10.1080/07350015.2024.2407634},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {556-567},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Endogenous kink threshold regression},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design-based theory for lasso adjustment in randomized block experiments and rerandomized experiments. <em>JBES</em>, <em>43</em>(3), 544-555. (<a href='https://doi.org/10.1080/07350015.2024.2403381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blocking, a special case of rerandomization, is routinely implemented in the design stage of randomized experiments to balance the baseline covariates. This study proposes a regression adjustment method based on the least absolute shrinkage and selection operator (Lasso) to efficiently estimate the average treatment effect in randomized block experiments with high-dimensional covariates. We derive the asymptotic properties of the proposed estimator and outline the conditions under which this estimator is more efficient than the unadjusted one. We provide a conservative variance estimator to facilitate valid inferences. Our framework allows one treated or control unit in some blocks and heterogeneous propensity scores across blocks, thus including paired experiments and finely stratified experiments as special cases. We further accommodate rerandomized experiments and a combination of blocking and rerandomization. Moreover, our analysis allows both the number of blocks and block sizes to tend to infinity, as well as heterogeneous treatment effects across blocks without assuming a true outcome data-generating model. Simulation studies and two real-data analyses demonstrate the advantages of the proposed method.},
  archive      = {J_JBES},
  author       = {Ke Zhu and Hanzhong Liu and Yuehan Yang},
  doi          = {10.1080/07350015.2024.2403381},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {544-555},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Design-based theory for lasso adjustment in randomized block experiments and rerandomized experiments},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic quantile factor analysis. <em>JBES</em>, <em>43</em>(3), 530-543. (<a href='https://doi.org/10.1080/07350015.2024.2396956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extends quantile factor analysis to a probabilistic variant that incorporates regularization and computationally efficient variational approximations. We establish through synthetic and real data experiments that the proposed estimator can, in many cases, achieve better accuracy than a recently proposed loss-based estimator. We contribute to the factor analysis literature by extracting new indexes of low , medium , and high economic policy uncertainty, as well as loose , median , and tight financial conditions. We show that the high uncertainty and tight financial conditions indexes have superior predictive ability for various measures of economic activity. In a high-dimensional exercise involving about 1000 daily financial series, we find that quantile factors also provide superior out-of-sample information compared to mean or median factors.},
  archive      = {J_JBES},
  author       = {Dimitris Korobilis and Maximilian Schröder},
  doi          = {10.1080/07350015.2024.2396956},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {530-543},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Probabilistic quantile factor analysis},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved divide-and-conquer approach to estimating mean functional, with application to average treatment effect estimation. <em>JBES</em>, <em>43</em>(3), 520-529. (<a href='https://doi.org/10.1080/07350015.2024.2395429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mean estimation is an important issue in statistical inference and machine learning. We are concerned with estimating mean functional that is a function of several nonparametric functions when there is a large amount of observations. Directly estimating such mean functional through nonparametric smoothing has the complexity of at least a quadratic order of the sample size, which is computationally prohibitive for massive data. The divide-and-conquer approach are thus readily used to alleviate the computational complexity issue, which however imposes a stringent condition on the sample size in each local machine if a locally optimal bandwidth is used. To address this issue, we suggest to use a globally optimal bandwidth in each local machine, which alleviates the restriction on the local sample sizes substantially. We show that the divide-and-conquer approach with a globally optimal bandwidth achieves the estimation efficiency bound as if all observations were pooled together. In terms of computational efficiency, our proposal outperforms the pooled algorithm dramatically. We demonstrate these properties through average treatment effect estimation from both the asymptotic and the non-asymptotic perspectives.},
  archive      = {J_JBES},
  author       = {Zhengtian Zhu and Liping Zhu},
  doi          = {10.1080/07350015.2024.2395429},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {520-529},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {An improved divide-and-conquer approach to estimating mean functional, with application to average treatment effect estimation},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distinguishing time-varying factor models. <em>JBES</em>, <em>43</em>(3), 508-519. (<a href='https://doi.org/10.1080/07350015.2024.2395424'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying factor models have been widely used to model changing relationships among economic and financial variables. The existing literature usually specifies the time-varying factor loadings as deterministic functions of time or unit root processes. This article proposes two consistent tests to distinguish between these two specifications based on a randomization approach. By setting the null hypothesis as either specification, we show that the proposed test statistics follow an asymptotic Chi-squared distribution under the respective null hypotheses and diverge to infinity in probability under the respective alternatives. Simulation studies reveal that both test statistics perform reasonably well in finite samples. We apply the proposed tests to the U.S. macroeconomic and global macroeconomic and financial datasets. The results suggest that the time-varying factor loadings as deterministic functions of time should be adopted for these two applications.},
  archive      = {J_JBES},
  author       = {Zhonghao Fu and Liangjun Su and Xia Wang},
  doi          = {10.1080/07350015.2024.2395424},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {508-519},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Distinguishing time-varying factor models},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based co-clustering in customer targeting utilizing large-scale online product rating networks. <em>JBES</em>, <em>43</em>(3), 495-507. (<a href='https://doi.org/10.1080/07350015.2024.2395423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the widely available online customer ratings on products, the individual-level rating prediction and clustering of customers and products are increasingly important for sellers to create targeting strategies for expanding the customer base and improving product ratings. However, the massive missing data problem is a significant challenge for modeling online product ratings. To address this issue, we propose a new co-clustering methodology based on a bipartite network modeling of large-scale ordinal product ratings. Our method extends existing co-clustering methods by incorporating covariates and ordinal ratings in the model-based co-clustering of a weighted bipartite network. We devise an efficient variational EM algorithm for model estimation. A simulation study demonstrates that our methodology is scalable for modeling large datasets and provides accurate estimation and clustering results. We further show that our model can successfully identify different groups of customers and products with meaningful interpretations and achieve promising predictive performance in a real application for customer targeting.},
  archive      = {J_JBES},
  author       = {Qian Chen and Amal Agarwal and Duncan K. H. Fong and Wayne S. DeSarbo and Lingzhou Xue},
  doi          = {10.1080/07350015.2024.2395423},
  journal      = {Journal of Business & Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {495-507},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Model-based co-clustering in customer targeting utilizing large-scale online product rating networks},
  volume       = {43},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
