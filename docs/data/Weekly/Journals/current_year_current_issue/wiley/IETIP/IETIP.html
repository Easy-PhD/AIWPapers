<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietip">IETIP - 4</h2>
<ul>
<li><details>
<summary>
(2025). Image compression model based on dynamic convolution and vision mamba. <em>IETIP</em>, <em>19</em>(1), e70080. (<a href='https://doi.org/10.1049/ipr2.70080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an efficient image compression scheme leveraging Vision Mamba and dynamic convolution, addressing the limitations of existing methods, such as failure to capture long-range pixel dependencies and high computational complexity. Our approach improves both global and local information learning with reduced computational cost. Experimental results on the Kodak, Tecnick and CLIC datasets show that our model achieves competitive performance with lower algorithm complexity. Our code is available on: https://github.com/Lynxsx/ICVM .},
  archive      = {J_IETIP},
  author       = {Lingchen Qiu and Enjian Bai and Yun Wu and Yuwen Cao and Xue-qin Jiang},
  doi          = {10.1049/ipr2.70080},
  journal      = {IET Image Processing},
  number       = {1},
  pages        = {e70080},
  shortjournal = {IET Image Process.},
  title        = {Image compression model based on dynamic convolution and vision mamba},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-enhanced feature pyramid SwinUNet: Advanced segmentation of lung nodules in CT images. <em>IETIP</em>, <em>19</em>(1), e70072. (<a href='https://doi.org/10.1049/ipr2.70072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of oncology, lung cancer is a leading contributor to cancer-related mortality, highlighting the need for early detection of lung nodules for effective intervention. However, accurate segmentation of lung nodules in Computed Tomography (CT) images remains a significant challenge due to issues such as heterogeneous nodule dimensions, low contrast, and their visual similarity with surrounding tissues. To address these challenges, this study proposes the Edge-Enhanced Feature Pyramid SwinUNet (EE-FPS-UNet), an advanced segmentation model that integrates a modified Swin Transformer with a feature pyramid network (FPN). The research objective is to enhance boundary delineation and multi-scale feature aggregation for improved segmentation performance. The proposed model uses the Swin Transformer to capture long-range dependencies and integrates an FPN for robust multi-scale feature aggregation. Its window-based self-attention mechanism also reduces computational complexity, making it well-suited for high-resolution CT images. Additionally, an edge detection module enhances segmentation by providing edge-related features to the decoder, improving boundary precision. A comparative analysis evaluates the EE-FPS-UNet against leading models, including PSPNet, U-Net, Attention U-Net, and DeepLabV3. The results demonstrate that the proposed model outperforms these models, achieving a Dice Similarity of 0.91 and a sensitivity of 0.89, establishing its efficacy for lung nodule segmentation.},
  archive      = {J_IETIP},
  author       = {Akila Agnes S and Arun Solomon A and K Karthick and Mejdl Safran and Sultan Alfarhood},
  doi          = {10.1049/ipr2.70072},
  journal      = {IET Image Processing},
  number       = {1},
  pages        = {e70072},
  shortjournal = {IET Image Process.},
  title        = {Edge-enhanced feature pyramid SwinUNet: Advanced segmentation of lung nodules in CT images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating linear skip-attention with transformer-based network of multi-level features extraction for partial point cloud registration. <em>IETIP</em>, <em>19</em>(1), e70055. (<a href='https://doi.org/10.1049/ipr2.70055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate point correspondences is critical for rigid point cloud registration in correspondence-based methods. Many previous learning-based methods employ encoder-decoder backbone for point feature extraction, while applying attention mechanism for sparse superpoints to deal with the partial overlap situation. However, few of these methods focus on the intermediate layers yet mainly pay attention on the top-most patch features, thus neglecting multi-faceted feature perspectives leading to potential overlap areas estimation inaccuracy. Meanwhile, obtaining correct correspondences is usually interfered with the one-to-many case and outliers. To address these issues, we propose a multi-level features extraction network with integrating linear dual attention mechanism into skip-connection stage of encoder-decoder backbone, both efficiently suppressing irrelevant information and guiding residual features to learn the common regions on which the network should focus to tackle the overlap estimation inaccuracy issue, combined with a parallel-structured decoder forming distinguishable features and potential overlapping regions. Additionally, a two-stage correspondences pruning process is designed to tackle the mismatch issue, which mainly depends on the rigid geometric constraint. Extensive experiments conducted on indoor and outdoor scene datasets demonstrate our method's accuracy and stability, by outperforming state-of-the-art methods on registrationÂ recall.},
  archive      = {J_IETIP},
  author       = {Qinyu He and Tao Sun},
  doi          = {10.1049/ipr2.70055},
  journal      = {IET Image Processing},
  number       = {1},
  pages        = {e70055},
  shortjournal = {IET Image Process.},
  title        = {Integrating linear skip-attention with transformer-based network of multi-level features extraction for partial point cloud registration},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of U-net and its variants: Advances and applications in medical image segmentation. <em>IETIP</em>, <em>19</em>(1), e70019. (<a href='https://doi.org/10.1049/ipr2.70019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images often exhibit low and blurred contrast between lesions and surrounding tissues, with considerable variation in lesion edges and shapes even within the same disease, leading to significant challenges in segmentation. Therefore, precise segmentation of lesions has become an essential prerequisite for patient condition assessment and formulation of treatment plans. Significant achievements have been made in research related to the U-Net model in recent years. It improves segmentation performance and is extensively applied in the semantic segmentation of medical images to offer technical support for consistent quantitative lesion analysis methods. First, this paper classifies medical image datasets on the basis of their imaging modalities and then examines U-Net and its various improvement models from the perspective of structural modifications. The research objectives, innovative designs, and limitations of each approach are discussed in detail. Second, we summarise the four central improvement mechanisms of the U-Net and U-Net variant algorithms: the jump-connection mechanism, the residual-connection mechanism, 3D-UNet, and the transformer mechanism. Finally, we examine the relationships among the four core enhancement mechanisms and commonly utilized medical datasets and propose potential avenues and strategies for future advancements. This paper provides a systematic summary and reference for researchers in related fields, and we look forward to designing more efficient and stable medical image segmentation network models based on the U-Net network.},
  archive      = {J_IETIP},
  author       = {Wang Jiangtao and Nur Intan Raihana Ruhaiyem and Fu Panpan},
  doi          = {10.1049/ipr2.70019},
  journal      = {IET Image Processing},
  number       = {1},
  pages        = {e70019},
  shortjournal = {IET Image Process.},
  title        = {A comprehensive review of U-net and its variants: Advances and applications in medical image segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
