<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim">SIM - 37</h2>
<ul>
<li><details>
<summary>
(2025). Logistic mixed-effects model analysis with pseudo-observations for estimating risk ratios in clustered binary data analysis. <em>SIM</em>, <em>44</em>(20-22), e70280. (<a href='https://doi.org/10.1002/sim.70280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logistic mixed-effects model has been a standard multivariate analysis method for analyzing clustered binary outcome data, for example, longitudinal studies, clustered randomized trials, and multicenter/regional studies. However, the resultant odds ratio estimator cannot be directly interpreted as an effect measure, and it is only interpreted as an approximation of the risk ratio estimator when the frequency of events is small. In this article, we propose a new statistical analysis method that enables providing a risk ratio estimator in the multilevel statistical model framework. The valid risk ratio estimation is realized via augmenting pseudo-observations to the original dataset and then analyzing the modified dataset by the logistic mixed-effects model. The resultant estimators of fixed effect coefficients are theoretically shown to be consistent estimators of the risk ratios. Also, the standard errors and confidence intervals of the risk ratios can be calculated by the bootstrap method. All of the computations are simply implementable by using the R package “glmmrr.” We illustrate the effectiveness of the proposed method via applications to a cluster-randomized trial of the maternal and child health handbook and a longitudinal study of respiratory disease. Also, we provide simulation-based evidence for the accuracy and precision of estimation of risk ratios by the proposed method.},
  archive      = {J_SIM},
  author       = {Hisashi Noma and Masahiko Gosho},
  doi          = {10.1002/sim.70280},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70280},
  shortjournal = {Stat. Med.},
  title        = {Logistic mixed-effects model analysis with pseudo-observations for estimating risk ratios in clustered binary data analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian process regression for value-censored functional and longitudinal data. <em>SIM</em>, <em>44</em>(20-22), e70277. (<a href='https://doi.org/10.1002/sim.70277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process (GP) regression is widely used for flexible and non-parametric Bayesian modeling of data arising from underlying smooth functions. This paper introduces a solution to GP regression when the observations are subject to value-based censoring. We derive exact and closed-form expressions for the conditional posterior distributions of the underlying functions in both the single-curve fitting case and in the case of a hierarchical model where multiple functions are modeled simultaneously. Our method can accommodate left, right, and interval censoring, and is directly applicable as an empirical Bayes method or integrated in a Markov–Chain Monte Carlo sampler for full posterior inference. The method is validated through extensive simulations, where it substantially outperforms naive approaches that either exclude censored observations or treat them as fully observed values. We give an application to a real-world dataset of longitudinal HIV-1 RNA measurements, where the observations are subject to left censoring due to a detection limit.},
  archive      = {J_SIM},
  author       = {Adam Gorm Hoffmann and Claus Thorn Ekstrøm and Benjamin Zeymer Christoffersen and Andreas Kryger Jensen},
  doi          = {10.1002/sim.70277},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70277},
  shortjournal = {Stat. Med.},
  title        = {Gaussian process regression for value-censored functional and longitudinal data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double negative control inference with some invalid negative control exposures for continuous outcome. <em>SIM</em>, <em>44</em>(20-22), e70276. (<a href='https://doi.org/10.1002/sim.70276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative controls have been increasingly used for causal inference when unmeasured confounding exist. Valid negative control exposures (NCEs) could not causally affect outcome, and valid negative control outcomes (NCOs) are not to be causally affected by exposure. In most observational studies, it is easy to find a valid NCO but NCEs are harder to verify due to the current limited knowledge. Invalid NCEs associated with outcome result in biased estimate of causal effects. However, previous work considering invalid negative controls is very limited. In this paper, we develop a double negative control framework for continuous outcomes in the presence of some invalid NCEs. First, we prove that it is possible to identify causal effects with a known pre-defined valid NCO and a pre-defined set of NCEs without knowing exactly their validity. Furthermore, as long as more than 50% of NCEs are valid, the average causal effect could be consistently estimated. Then we design an procedure to select valid NCEs. Finally, we give two kinds of double negative control estimators (sisvNCE and naiveNCE-Median) with a guarantee of theoretical estimation performance. Simulation results show that the performance of our method is robust when the number of invalid NCEs does not exceed a certain threshold. Application results indicate that our method has a promising role in public health.},
  archive      = {J_SIM},
  author       = {Qingqing Yang and Jinzhu Jia},
  doi          = {10.1002/sim.70276},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70276},
  shortjournal = {Stat. Med.},
  title        = {Double negative control inference with some invalid negative control exposures for continuous outcome},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Futility analyses for the MCP-mod methodology based on longitudinal models. <em>SIM</em>, <em>44</em>(20-22), e70274. (<a href='https://doi.org/10.1002/sim.70274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses futility analyses for the MCP-Mod methodology. Formulas are derived for calculating predictive and conditional power for MCP-Mod, which also cover the case when longitudinal models are used allowing to utilize incomplete data from patients at interim. A simulation study is conducted to evaluate the repeated sampling properties of the proposed decision rules and to assess the benefit of using a longitudinal versus a completer only model for decision making at interim. The results suggest that the proposed methods perform adequately and a longitudinal analysis outperforms a completer only analysis, particularly when the recruitment speed is higher and the correlation over time is larger. The proposed methodology is illustrated using real data from a dose-finding study for severe uncontrolled asthma.},
  archive      = {J_SIM},
  author       = {Björn Bornkamp and Jie Zhou and Dong Xi and Weihua Cao},
  doi          = {10.1002/sim.70274},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70274},
  shortjournal = {Stat. Med.},
  title        = {Futility analyses for the MCP-mod methodology based on longitudinal models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing stepped wedge cluster randomized trials with a baseline measurement of the outcome. <em>SIM</em>, <em>44</em>(20-22), e70273. (<a href='https://doi.org/10.1002/sim.70273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster randomized trials (SW-CRTs) are a type of uni-directional crossover designs and are increasingly common in prevention and implementation research. Although sample size formulas have been developed to support the planning of SW-CRTs, almost no prior methods incorporated the baseline measurement of the outcome—a common feature in many randomized trials and, increasingly, in cross-sectional SW-CRTs. In this article, we systematically investigate the possibility of addressing a baseline outcome measurement in designing cross-sectional SW-CRTs. We provide three linear mixed modeling approaches to adjust for the baseline outcome and derive the corresponding variance formula of the treatment effect estimator under each. The derived formulas reveal the efficiency implications of including a baseline outcome measurement, and provide a natural vehicle for the efficiency comparisons across adjustment approaches to generate practical recommendations. We validate the power and sample size methods under each baseline adjustment approach using simulations and provide an illustrative sample size calculation with a baseline outcome using the context of a real SW-CRT.},
  archive      = {J_SIM},
  author       = {Kendra Davis-Plourde and Keith Goldfeld and Heather Allore and Monica Taljaard and Fan Li},
  doi          = {10.1002/sim.70273},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70273},
  shortjournal = {Stat. Med.},
  title        = {Designing stepped wedge cluster randomized trials with a baseline measurement of the outcome},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal machine learning methods and use of cross-fitting in settings with high-dimensional confounding. <em>SIM</em>, <em>44</em>(20-22), e70272. (<a href='https://doi.org/10.1002/sim.70272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational epidemiological studies commonly seek to estimate the causal effect of an exposure on an outcome. Adjustment for potential confounding bias in modern studies is challenging due to the presence of high-dimensional confounding, which occurs when there are many confounders relative to sample size or complex relationships between continuous confounders and exposure and outcome. Doubly robust methods such as Augmented Inverse Probability Weighting (AIPW) and Targeted Maximum Likelihood Estimation (TMLE) have the potential to address these challenges, using data-adaptive approaches and cross-fitting, but despite recent advances, limited evaluation and guidance are available on their implementation in realistic settings where high-dimensional confounding is present. Motivated by an early-life cohort study, we conducted an extensive simulation study to compare the relative performance of AIPW and TMLE using data-adaptive approaches for estimating the average causal effect (ACE). We evaluated the benefits of using cross-fitting with a varying number of folds, as well as the impact of using a reduced versus full (larger, more diverse) library in the Super Learner ensemble learning approach used for implementation. We found that AIPW and TMLE performed similarly in most cases for estimating the ACE, but TMLE was more stable. Cross-fitting improved the performance of both methods, but was more important for variance estimation and coverage than for point estimates, with the number of folds a less important consideration. Using a full Super Learner library was important to reduce bias and variance in complex scenarios typical of modern health research studies.},
  archive      = {J_SIM},
  author       = {Susan Ellul and Stijn Vansteelandt and John B. Carlin and Margarita Moreno-Betancur},
  doi          = {10.1002/sim.70272},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70272},
  shortjournal = {Stat. Med.},
  title        = {Causal machine learning methods and use of cross-fitting in settings with high-dimensional confounding},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biostatisticians meet AI: Navigating shifts while preserving principles. <em>SIM</em>, <em>44</em>(20-22), e70271. (<a href='https://doi.org/10.1002/sim.70271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Bin Zhu},
  doi          = {10.1002/sim.70271},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70271},
  shortjournal = {Stat. Med.},
  title        = {Biostatisticians meet AI: Navigating shifts while preserving principles},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-robust standardization in cluster-randomized trials. <em>SIM</em>, <em>44</em>(20-22), e70270. (<a href='https://doi.org/10.1002/sim.70270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cluster-randomized trials, generalized linear mixed models and generalized estimating equations have conventionally been the default analytic methods for estimating the average treatment effect as routine practice. However, recent studies have demonstrated that their treatment effect coefficient estimators may correspond to ambiguous estimands when the models are misspecified or when there exist informative cluster sizes. In this article, we present a unified approach that standardizes output from a given regression model to ensure estimand-aligned inference for the treatment effect parameters in cluster-randomized trials. We introduce estimators for both the cluster-average and the individual-average treatment effects (marginal estimands) that are always consistent regardless of whether the specified working regression models align with the unknown data generating process. We further explore the use of a deletion-based jackknife variance estimator for inference. The development of our approach also motivates a natural test for informative cluster size. Extensive simulation experiments are designed to demonstrate the advantage of the proposed estimators under a variety of scenarios. The proposed model-robust standardization methods are implemented in the MRStdCRT R package.},
  archive      = {J_SIM},
  author       = {Fan Li and Jiaqi Tong and Xi Fang and Chao Cheng and Brennan C. Kahan and Bingkai Wang},
  doi          = {10.1002/sim.70270},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70270},
  shortjournal = {Stat. Med.},
  title        = {Model-robust standardization in cluster-randomized trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-free approach to evaluate a censored intermediate outcome as a surrogate for overall survival. <em>SIM</em>, <em>44</em>(20-22), e70268. (<a href='https://doi.org/10.1002/sim.70268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials or studies oftentimes require long-term and/or costly follow-up of participants to evaluate a novel treatment/drug/vaccine. There has been increasing interest in the past few decades in using short-term surrogate outcomes as a replacement for the primary outcome, that is, in using the surrogate outcome, which can potentially be observed sooner, to make inferences about the treatment effect on the long-term primary outcome. Very few of the available statistical methods to evaluate a surrogate are applicable to settings where both the surrogate and the primary outcome are time-to-event outcomes subject to censoring. Methods that can handle this setting tend to require parametric assumptions or be limited to assessing only the restricted mean survival time. In this paper, we propose a nonparametric approach to evaluate a censored surrogate outcome, such as time to progression, when the primary outcome is also a censored time-to-event outcome, such as time to death, and the treatment effect of interest is the difference in overall survival. Specifically, we define the proportion of the treatment effect on the primary outcome that is explained (PTE) by the censored surrogate outcome in this context, and estimate this proportion by defining and deriving an optimal transformation of the surrogate information. Our approach provides the added advantage of relaxed assumptions to guarantee that the true PTE is within , along with being model-free. The finite sample performance of our estimators is illustrated via extensive simulation studies and a real data application examining progression-free survival as a surrogate for overall survival for patients with metastatic colorectal cancer.},
  archive      = {J_SIM},
  author       = {Xuan Wang and Tianxi Cai and Lu Tian and Layla Parast},
  doi          = {10.1002/sim.70268},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70268},
  shortjournal = {Stat. Med.},
  title        = {Model-free approach to evaluate a censored intermediate outcome as a surrogate for overall survival},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Response to letter to the editor “Comments on ‘Novel non-linear models for clinical trial analysis with longitudinal data: A tutorial using SAS for both frequentist and bayesian methods’”. <em>SIM</em>, <em>44</em>(20-22), e70266. (<a href='https://doi.org/10.1002/sim.70266'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials with longitudinal continuous data, efficacy inference traditionally focuses on the difference in the mean change from baseline at a single study visit [e.g., mixed models for repeated measures (MMRM)]. Proportional MMRM (pMMRM) reparameterizes this difference as a proportional reduction relative to the placebo mean change. This proportional effect is a nonlinear combination of the means, whereas the difference is a linear combination of the means. It can not only lead to greater power at a single visit by yielding a test statistic lower-bounded by that of the difference but also offers a flexible and intuitive way to combine all or multiple visits for efficacy inference, which can further boost power. It is also asymptotically unbiased. pMMRM with visit-specific proportional effects yields identical parameter estimates to MMRM. When only MMRM outputs are used, the proportional effect calculated by the delta method yields greater power than the difference.},
  archive      = {J_SIM},
  author       = {Guoqiao Wang and Guogen Shan and Yan Li and Yijie Liao and Lon Schneider and Gary Cutter},
  doi          = {10.1002/sim.70266},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70266},
  shortjournal = {Stat. Med.},
  title        = {Response to letter to the editor “Comments on ‘Novel non-linear models for clinical trial analysis with longitudinal data: A tutorial using SAS for both frequentist and bayesian methods’”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tutorial on bayesian functional regression using stan. <em>SIM</em>, <em>44</em>(20-22), e70265. (<a href='https://doi.org/10.1002/sim.70265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript provides step-by-step instructions for implementing Bayesian functional regression models using Stan. Extensive simulations indicate that the inferential performance of the methods is comparable to that of state-of-the-art frequentist approaches. However, Bayesian approaches allow for more flexible modeling and provide an alternative when frequentist methods are not available or may require additional development. Methods and software are illustrated using the accelerometry data from the National Health and Nutrition Examination Survey (NHANES).},
  archive      = {J_SIM},
  author       = {Ziren Jiang and Ciprian Crainiceanu and Erjia Cui},
  doi          = {10.1002/sim.70265},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70265},
  shortjournal = {Stat. Med.},
  title        = {Tutorial on bayesian functional regression using stan},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian multi-factorial design and analysis for estimating combined effects of multiple interventions in a pragmatic clinical trial to improve dementia care. <em>SIM</em>, <em>44</em>(20-22), e70264. (<a href='https://doi.org/10.1002/sim.70264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factorial study designs can be important for understanding the effectiveness of interventions when multiple interventions are under investigation. In this design setting, a unit of randomization can be assigned to any combination of interventions. The rationale for taking this kind of approach can vary depending on the specific questions targeted by the research. These questions, in turn, have implications for the way in which the analyses will be conducted. The goal in this paper is to describe how we developed a factorial design along with a Bayesian analytic plan for a large cluster-randomized trial—the Emergency Departments LEading the transformation of Alzheimer's and Dementia care (ED-LEAD) study—focused on improving care for persons living with dementia.},
  archive      = {J_SIM},
  author       = {Keith S. Goldfeld and Corita R. Grudzen and Manish N. Shah and Abraham A. Brody and Joshua Chodosh and Rebecca Anthopolos},
  doi          = {10.1002/sim.70264},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70264},
  shortjournal = {Stat. Med.},
  title        = {A bayesian multi-factorial design and analysis for estimating combined effects of multiple interventions in a pragmatic clinical trial to improve dementia care},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian adaptive enrichment design for continuous biomarkers. <em>SIM</em>, <em>44</em>(20-22), e70262. (<a href='https://doi.org/10.1002/sim.70262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of precision medicine and targeted therapies in cancer, new challenges in the statistical design of clinical trials have naturally emerged. Most randomized clinical trial designs incorporating predictive biomarkers (those associated with treatment efficacy) assume biomarkers are dichotomous, or dichotomize naturally continuous biomarkers upfront, or find cut points mid-way through the trial to classify patients as biomarker-positive or biomarker-negative. However, these practices ignore or discard information about continuous and possible nonlinear or non-monotone prognostic or predictive effects. In this article, we propose a novel adaptive enrichment trial design to handle continuous biomarkers with any effect shape, including Bayesian marker-adaptive randomization. We demonstrate that this design can correctly make marker-specific trial decisions with high efficiency, resulting in improved performance and patient-centered decisions compared to adaptive cut-point selection approaches without adaptive randomization that further ignore or oversimplify true underlying marker relationships.},
  archive      = {J_SIM},
  author       = {Yue Tu and Yusha Liu and Wendy J. Mack and Lindsay A. Renfro},
  doi          = {10.1002/sim.70262},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70262},
  shortjournal = {Stat. Med.},
  title        = {Bayesian adaptive enrichment design for continuous biomarkers},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power and sample size calculation for multivariate longitudinal trials using the longitudinal rank sum test. <em>SIM</em>, <em>44</em>(20-22), e70261. (<a href='https://doi.org/10.1002/sim.70261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative diseases such as Alzheimer's and Parkinson's often exhibit complex, multivariate longitudinal outcomes that require advanced statistical methods to comprehensively evaluate treatment efficacy. The Longitudinal Rank Sum Test (LRST) offers a nonparametric framework to assess global treatment effects across multiple longitudinal endpoints without requiring multiplicity corrections. This study develops a robust methodology for power and sample size estimation specific to the LRST, integrating theoretical derivations, asymptotic properties, and practical estimation techniques under large sample conditions. Validation through numerical simulations demonstrates the accuracy of the proposed methods, while real-world applications to clinical trials in Alzheimer's disease (AD) and Parkinson's disease (PD) highlight their practical significance. This framework facilitates the design of efficient, well-powered trials, advancing the evaluation of treatments for complex diseases with multivariate longitudinal outcomes.},
  archive      = {J_SIM},
  author       = {Dhrubajyoti Ghosh and Xiaoming Xu and Sheng Luo},
  doi          = {10.1002/sim.70261},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70261},
  shortjournal = {Stat. Med.},
  title        = {Power and sample size calculation for multivariate longitudinal trials using the longitudinal rank sum test},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equivalency between the generalized bivariate bernoulli model dependency test and a logistic regression model with interaction effects. <em>SIM</em>, <em>44</em>(20-22), e70260. (<a href='https://doi.org/10.1002/sim.70260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Kazi Md Farhad Mahmud and Yanming Li and Devin C. Koestler},
  doi          = {10.1002/sim.70260},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70260},
  shortjournal = {Stat. Med.},
  title        = {Equivalency between the generalized bivariate bernoulli model dependency test and a logistic regression model with interaction effects},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handling missing outcome data in cluster randomized trials with both individual- and cluster-level dropout. <em>SIM</em>, <em>44</em>(20-22), e70259. (<a href='https://doi.org/10.1002/sim.70259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing outcome data are common in cluster randomized trials (CRTs), which can complicate inference. Further, the missingness can occur due to dropout of individuals, termed sporadically missing data, or dropout of clusters, termed systematically missing data, and these two types of missingness could have potentially different missing data mechanisms. We aimed to develop a well-performing and practical approach to handle inference in CRTs when outcome data may be both sporadically and systematically missing. To this end, we first examined the performance of several multilevel multiple imputation (MI) methods to handle sporadically and systematically missing CRT outcome data via a simulation study. Specifically, we examined performance under a multilevel covariate-dependent missingness assumption. Our findings indicated that several full conditional specification (FCS) methods designed for missingness in linear mixed models performed well under various scenarios, while an FCS approach using a two-stage estimator often performed poorly. We then developed methods for conducting sensitivity analysis to test the robustness of inferences under different missing at random (MAR) and missing not at random (MNAR) assumptions. The methods allow for different MNAR assumptions for cluster dropout and individual dropout to reflect that they may arise from different missing data mechanisms. We used graphical displays to visualize sensitivity analysis results. Our methods are illustrated using a real data application.},
  archive      = {J_SIM},
  author       = {Analissa Avila and Beth A. Glenn and Roshan Bastani and Catherine M. Crespi},
  doi          = {10.1002/sim.70259},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70259},
  shortjournal = {Stat. Med.},
  title        = {Handling missing outcome data in cluster randomized trials with both individual- and cluster-level dropout},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collapsible kernel machine regression for exposomic analyses. <em>SIM</em>, <em>44</em>(20-22), e70258. (<a href='https://doi.org/10.1002/sim.70258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important goal of environmental epidemiology is to quantify the complex health effects posed by a wide array of environmental exposures. In studies of a small number of exposures, flexible models like Bayesian kernel machine regression (BKMR) are appealing because they allow for non-linear and non-additive associations among exposures. However, this flexibility comes at the cost of low power and difficult interpretation, particularly in exposomic analyses when the number of exposures is large. We propose a flexible framework that allows for the separate selection of additive and non-additive effects, unifying additive models and kernel machine regression. The proposed approach yields increased power and simpler interpretation when there is little evidence of interaction. Further, it allows users to specify separate priors for additive and non-additive effect s, and allows for statistical inference on non-additive interactions. We extend the approach to a class of multiple index models, in which the special case of kernel machine-distributed lag models is nested. We apply the method to motivating data from a subcohort of the Human Early Life Exposome (HELIX) study containing 65 mixture components grouped into 13 distinct exposure classes.},
  archive      = {J_SIM},
  author       = {Glen McGee and Brent A. Coull and Ander Wilson},
  doi          = {10.1002/sim.70258},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70258},
  shortjournal = {Stat. Med.},
  title        = {Collapsible kernel machine regression for exposomic analyses},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of stepped-wedge cluster randomized trials when treatment effects vary by exposure time or calendar time. <em>SIM</em>, <em>44</em>(20-22), e70256. (<a href='https://doi.org/10.1002/sim.70256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped-wedge cluster randomized trials (SW-CRTs) are traditionally analyzed with models that assume an immediate and sustained treatment effect. Previous work has shown that making such an assumption in the analysis of SW-CRTs when the true underlying treatment effect varies by exposure time can produce severely misleading estimates. Alternatively, the true underlying treatment effect might vary by calendar time. Comparatively less work has examined treatment effect structure misspecification in this setting. Here, we evaluate the behavior of the linear mixed effects model-based immediate treatment effect, exposure time-averaged treatment effect, and calendar time-averaged treatment effect estimators in different scenarios where these estimators are misspecified for the true underlying treatment effect structure. We show that the immediate treatment effect estimator is relatively robust to bias when estimating a true underlying calendar time-averaged treatment effect estimand. However, when there is a true underlying calendar (exposure) time-varying treatment effect, misspecifying an analysis with an exposure (calendar) time-averaged treatment effect estimator can yield severely misleading estimates which may converge to a value with the opposite sign of the true calendar (exposure) time-averaged treatment effect estimand. In this article, we highlight these two different time scales on which treatment effects can vary in SW-CRTs and clarify potential vulnerabilities that may arise when considering different types of time-varying treatment effects in a SW design. Accordingly, we emphasize the need for researchers to carefully consider whether the treatment effect may vary as a function of exposure time or calendar time in the analysis of SW-CRTs.},
  archive      = {J_SIM},
  author       = {Kenneth M. Lee and Elizabeth L. Turner and Avi Kenny},
  doi          = {10.1002/sim.70256},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70256},
  shortjournal = {Stat. Med.},
  title        = {Analysis of stepped-wedge cluster randomized trials when treatment effects vary by exposure time or calendar time},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for similarity of dose response in multiregional clinical trials. <em>SIM</em>, <em>44</em>(20-22), e70255. (<a href='https://doi.org/10.1002/sim.70255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of determining whether the dose response relationships between subgroups and the full population in a multiregional trial are similar. Similarity is assessed in terms of the maximal deviation between the dose response curves. We consider a parametric framework and develop two powerful bootstrap tests: one for assessing the similarity between the dose response curves of a single subgroup and that of the full population, and another for comparing the dose response curves of multiple subgroups with that of the full population. We prove the validity of these tests, investigate their finite sample properties through a simulation study and illustrate the methodology with a case study.},
  archive      = {J_SIM},
  author       = {Holger Dette and Lukas Koletzko and Frank Bretz},
  doi          = {10.1002/sim.70255},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70255},
  shortjournal = {Stat. Med.},
  title        = {Testing for similarity of dose response in multiregional clinical trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-phenotype approach to joint testing of main genetic and gene-environment interaction effects. <em>SIM</em>, <em>44</em>(20-22), e70253. (<a href='https://doi.org/10.1002/sim.70253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene-environment (GxE) interactions crucially contribute to complex phenotypes. The statistical power of a GxE interaction study is limited mainly due to weak GxE interaction effect sizes. Joint tests of the main genetic and GxE effects for a univariate phenotype were proposed to utilize the individually weak GxE effects to improve the discovery of associated genetic loci. We develop a testing procedure to evaluate combined genetic and GxE effects on multiple related phenotypes to enhance the power by merging pleiotropy in the main genetic and GxE effects. We base the approach on a general linear hypothesis testing framework for multivariate regression of continuous phenotypes. We implement the generalized estimating equations (GEE) technique under the seemingly unrelated regressions (SUR) setup for binary or mixed phenotypes. We use extensive simulations to show that the test for joint multi-phenotype genetic and GxE effects outperforms the univariate joint test of genetic and GxE effects and the test for multi-phenotype GxE effect concerning power when there is pleiotropy. The test produces a higher power than the test for the multi-phenotype marginal genetic effect for a weak genetic and substantial GxE effect. For more prominent genetic effects, the latter performs better with a limited increase in power. Overall, the multi-phenotype joint approach offers robust, high power across diverse simulation scenarios. We apply the methods to lipid phenotypes with sleep duration as an environmental factor in the UK Biobank. The proposed approach identified ten independent associated genetic loci missed by other competing methods. In a multi-phenotype analysis of apolipoproteins, ApoA1, and ApoB, our approach discovered two distinct loci considering sleep duration as the environmental factor.},
  archive      = {J_SIM},
  author       = {Saurabh Mishra and Arunabha Majumdar},
  doi          = {10.1002/sim.70253},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70253},
  shortjournal = {Stat. Med.},
  title        = {A multi-phenotype approach to joint testing of main genetic and gene-environment interaction effects},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new algorithm for sampling parameters in a structured correlation matrix with application to estimating optimal combinations of muscles to quantify progression in duchenne muscular dystrophy. <em>SIM</em>, <em>44</em>(20-22), e70252. (<a href='https://doi.org/10.1002/sim.70252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this paper is to estimate an optimal combination of biomarkers for individuals with Duchenne muscular dystrophy (DMD), which provides the most sensitive combinations of biomarkers to assess disease progression (in this case, optimal with respect to standardized response mean (SRM) for 4 muscle biomarkers). The biomarker data is incomplete (missing and irregular) multivariate longitudinal data. We propose a normal model with structured covariance designed for our setting. To sample from the posterior distribution of parameters, we develop a Markov Chain Monte Carlo (MCMC) algorithm to address the positive definiteness constraint on the structured correlation matrix. In particular, we propose a novel approach to compute the support of the parameters in the structured correlation matrix; we modify the approach from [1] on the set of the largest possible submatrices of the correlation matrix, where the correlation parameter is a unique element. For each posterior sample, we compute the optimal weights of our construct. We conduct data analysis and simulation studies to evaluate the algorithm and the frequentist properties of the posteriors of correlations and weights. We found that the lower extremities are the most responsive muscles at the early and late ambulatory disease stages, and the biceps brachii is the most responsive at the nonambulatory disease stage.},
  archive      = {J_SIM},
  author       = {Michael K. Kim and Michael J. Daniels and William D. Rooney and Rebecca J. Willcocks and Glenn A. Walter and Krista H. Vandenborne},
  doi          = {10.1002/sim.70252},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70252},
  shortjournal = {Stat. Med.},
  title        = {A new algorithm for sampling parameters in a structured correlation matrix with application to estimating optimal combinations of muscles to quantify progression in duchenne muscular dystrophy},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using confidence distributions in final and interim analyses for single-arm studies or platform trials consisting of single-arm studies. <em>SIM</em>, <em>44</em>(20-22), e70251. (<a href='https://doi.org/10.1002/sim.70251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidence distributions are a frequentist alternative to the Bayesian posterior distribution. These confidence distributions have received more attention in the recent past because of their simplicity. In rare diseases, oncology, or in pediatric drug development, single-arm trials, or platform trials consisting of a series of single-arm trials are increasingly being used, both to establish proof-of-concept and to provide pivotal evidence for a marketing application. Often, these single-arm trials are designed as two-stage designs, or they include sequential or continuous monitoring approaches. They are analyzed using standard frequentist, Bayesian, or other methods. In this paper, we describe how to define analysis strategies based on confidence distributions for such single-arm trials or for platform trials that consist of a series of single arm trials. We focus on binary endpoints and show how to define the corresponding decision rules for final and interim analyses and how to derive their operating characteristics exactly, for example, without simulation. Our approach uses predictive probabilities rather than conditional probabilities (as with stochastic curtailment) to define the interim decision rules. It can be applied to platform, basket, and umbrella trials that consist of a series of single-arm trials but also to stand-alone single arm trials.},
  archive      = {J_SIM},
  author       = {Günter Heimann and Peter Jacko and Tom Parke},
  doi          = {10.1002/sim.70251},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70251},
  shortjournal = {Stat. Med.},
  title        = {Using confidence distributions in final and interim analyses for single-arm studies or platform trials consisting of single-arm studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling joint health effects of environmental exposure mixtures with bayesian additive regression trees. <em>SIM</em>, <em>44</em>(20-22), e70250. (<a href='https://doi.org/10.1002/sim.70250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying the association between mixtures of environmental exposures and health outcomes can be challenging due to issues such as correlation among the exposures and non-linearities or interactions in the exposure-response function. For this reason, one common strategy is to fit flexible nonparametric models to capture the true exposure-response surface. However, once such a model is fit, further decisions are required when it comes to summarizing the marginal and joint effects of the mixture on the outcome. In this work, we describe the use of soft Bayesian additive regression trees (BART) to estimate the exposure-risk surface describing the effect of mixtures of chemical air pollutants and temperature on asthma-related emergency department (ED) visits during the warm season in Atlanta, Georgia, from 2011 to 2018. BART is chosen for its ability to handle large datasets and for its flexibility to be incorporated as a single component of a larger model. We then summarize the results using a strategy known as accumulated local effects to extract meaningful insights into the mixture effects on asthma-related morbidity. Notably, we observe negative associations between and asthma ED visits and harmful associations between ozone and asthma ED visits, both of which are particularly strong on lower temperature days.},
  archive      = {J_SIM},
  author       = {Jacob R. Englert and Stefanie T. Ebelt and Howard H. Chang},
  doi          = {10.1002/sim.70250},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70250},
  shortjournal = {Stat. Med.},
  title        = {Modeling joint health effects of environmental exposure mixtures with bayesian additive regression trees},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group sequential trial design using stepwise monte carlo for increased flexibility and robustness. <em>SIM</em>, <em>44</em>(20-22), e70249. (<a href='https://doi.org/10.1002/sim.70249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are becoming increasingly complex, incorporating numerous parameters and degrees of freedom. Optimal analytic approaches for these intricate trial designs are often unavailable, necessitating extensive simulation to control the Type I error rate and power, while reducing sample size and achieving favorable operating characteristics. This paper proposes a general method to reduce the dimension of the design space using group stepwise methods and Monte Carlo simulations, significantly decreasing the number of iterations required to identify near-optimal parameters. The method extends classical Group Sequential Designs but does not rely on normality assumptions and can accommodate complex trial designs. We offer a simulation study comparing the optimality, precision, and efficiency (runtime) of our method to those of existing approaches and conclude that our new method offers an attractive trade-off among these three key summaries.},
  archive      = {J_SIM},
  author       = {Amitay Kamber and Elad Berkman and Tzviel Frostig and Raviv Pryluk and Bradley P. Carlin},
  doi          = {10.1002/sim.70249},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70249},
  shortjournal = {Stat. Med.},
  title        = {Group sequential trial design using stepwise monte carlo for increased flexibility and robustness},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The estimand framework in diagnostic accuracy studies. <em>SIM</em>, <em>44</em>(20-22), e70248. (<a href='https://doi.org/10.1002/sim.70248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic accuracy studies evaluate how well a diagnostic test can detect or rule out a medical condition. Different events can interfere with the conduct of the test, affecting the test result. Before starting a diagnostic test accuracy study, the clinical question of interest should be precisely defined. Based on that, strategies can be chosen for dealing with the interfering event. We introduce six different strategies for how such events could be handled. We introduce the estimand concept for diagnostic accuracy studies, which consists of the attributes population, target condition, index test, accuracy measure, and the strategies for handling interfering events. The estimand determines which effect is estimated based on the study objective. To bridge the gap between the clinical study objective and the method for the estimation, we illustrate the necessary steps using a fictitious computed tomography scan study. The defined estimand improves the structure of the planning phase, enhances the interdisciplinary exchange, and supports the interpretation based on the study objective.},
  archive      = {J_SIM},
  author       = {Alexander Fierenz and Mouna Akacha and Norbert Benda and Mahnaz Badpa and Patrick M M Bossuyt and Nandini Dendukuri and Britta Rackow and Antonia Zapf},
  doi          = {10.1002/sim.70248},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70248},
  shortjournal = {Stat. Med.},
  title        = {The estimand framework in diagnostic accuracy studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical grouped horseshoe priors for subgroup identification and estimation. <em>SIM</em>, <em>44</em>(20-22), e70246. (<a href='https://doi.org/10.1002/sim.70246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common issue in randomized clinical trials (RCTs) is the identification of subgroups and the estimation of their effects. Typically, RCTs are not powered to estimate the effects of subgroups. However, in some circumstances, treatment may work for some groups and not others, and it is of interest to identify these subgroups and estimate their treatment effects. In this paper, we introduce a novel hierarchical grouped horseshoe prior (HGHP) for subgroup identification and estimation. We show via simulation that our proposed approach yields superior positive predictive value and narrower credible intervals compared to other shrinkage priors. We apply our method to a real clinical trial for COVID-19.},
  archive      = {J_SIM},
  author       = {Ethan M. Alt and Anil Anderson and Qing Li and Jia Hua and Amarjot Kaur and Joseph G. Ibrahim},
  doi          = {10.1002/sim.70246},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70246},
  shortjournal = {Stat. Med.},
  title        = {Hierarchical grouped horseshoe priors for subgroup identification and estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of endpoint delay on the efficiency of multi arm multi stage trials. <em>SIM</em>, <em>44</em>(20-22), e70245. (<a href='https://doi.org/10.1002/sim.70245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-arm multi-stage (MAMS) is an efficient class of trial designs that helps to assess multiple treatment strategies at the same time using an adaptive design. These designs can substantially reduce the average number of samples required compared to an equivalent single stage multi-arm trial. However, if patient recruitment is continued while we await treatment outcomes, a long-term primary outcome leads to a number of ‘pipeline’ patients getting recruited in the trial, who do not benefit from the early termination of a futile arm. This study focuses on quantifying the efficiency loss a MAMS design undergoes, in terms of the expected sample size (ESS), because of outcome delay. We first estimate the number of ‘pipeline’ patients (recruited during the interim analysis (IA) while awaiting outcome data) analytically through different recruitment models, given the total recruitment time. We then compute the ESS accounting for delay and assess the Efficiency Loss (EL). The results indicate that more than 50% of the expected efficiency gain is typically lost due to delay when the delay is more than of the total recruitment length. Although the number of stages have little influence on the efficiency loss, the timing of the IA can impact the efficiency of MAMS designs with delayed outcomes; in particular, conducting the IAs earlier than an equally-spaced design can be harmful for the design. Finally, we conclude that, in order to gain maximum benefit of MAMS in terms of a reduced sample size in multi-arm trials, the outcome delay should be less than a third of the total recruitment length.},
  archive      = {J_SIM},
  author       = {Aritra Mukherjee and James M. S. Wason},
  doi          = {10.1002/sim.70245},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70245},
  shortjournal = {Stat. Med.},
  title        = {Impact of endpoint delay on the efficiency of multi arm multi stage trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A location-scale joint model for studying the link between the time-dependent subject-specific variability of blood pressure and competing events. <em>SIM</em>, <em>44</em>(20-22), e70244. (<a href='https://doi.org/10.1002/sim.70244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the high incidence of cardio and cerebrovascular diseases (CVD), and their association with morbidity and mortality, their prevention is a major public health issue. A high level of blood pressure is a well-known risk factor for these events, and an increasing number of studies suggest that blood pressure variability may also be an independent risk factor. However, these studies suffer from significant methodological weaknesses. In this work, we propose a new location-scale joint model for the repeated measures of a marker and competing events. This joint model combines a mixed model including a subject-specific and time-dependent residual variance modeled through random effects, and cause-specific proportional intensity models for the competing events. The risk of events may depend simultaneously on the current value of the variance, as well as, the current value and the current slope of the marker trajectory. The model is estimated by maximizing the likelihood function using the Marquardt–Levenberg algorithm. The estimation procedure is implemented in an R-package and is validated through a simulation study. This model is applied to study the association between blood pressure variability and the risk of CVD and death from other causes. Using data from a large clinical trial on the secondary prevention of stroke, we find that the current individual variability of blood pressure is associated with the risk of CVD and death. Moreover, the comparison with a model without heterogeneous variance shows the importance of taking into account this variability in the goodness-of-fit and for dynamic predictions.},
  archive      = {J_SIM},
  author       = {Léonie Courcoul and Christophe Tzourio and Mark Woodward and Antoine Barbieri and Hélène Jacqmin-Gadda},
  doi          = {10.1002/sim.70244},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70244},
  shortjournal = {Stat. Med.},
  title        = {A location-scale joint model for studying the link between the time-dependent subject-specific variability of blood pressure and competing events},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RISE: Two-stage rank-based identification of high-dimensional surrogate markers applied to vaccinology. <em>SIM</em>, <em>44</em>(20-22), e70241. (<a href='https://doi.org/10.1002/sim.70241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vaccine trials with long-term participant follow-up, it is of great importance to identify surrogate markers that accurately infer long-term immune responses. These markers offer practical advantages such as providing early, indirect evidence of vaccine efficacy, and can accelerate vaccine development while identifying potential biomarkers. High-throughput technologies such as RNA-sequencing have emerged as promising tools for understanding complex biological systems and informing new treatment strategies. However, these data are high-dimensional, presenting unique statistical challenges for existing surrogate marker identification methods. We introduce Rank-based Identification of high-dimensional SurrogatE Markers (RISE), a novel approach designed for small sample, high-dimensional settings typical in modern vaccine experiments. RISE uses a nonparametric univariate test to screen variables for promising candidates, followed by surrogate evaluation on independent data. Our simulation studies demonstrate RISE's desirable properties, including type one error rate control and empirical power under various conditions. Applying RISE to a clinical trial for inactivated influenza vaccination, we sought to identify genes whose expression could serve as a surrogate for the induced immune response. This analysis revealed a signature of genes appearing to function as a reasonable surrogate for the neutralizing antibody response. Pathways related to innate antiviral signaling and interferon stimulation were strongly represented in this derived surrogate, providing a clear immunological interpretation.},
  archive      = {J_SIM},
  author       = {Arthur Hughes and Layla Parast and Rodolphe Thiébaut and Boris P. Hejblum},
  doi          = {10.1002/sim.70241},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70241},
  shortjournal = {Stat. Med.},
  title        = {RISE: Two-stage rank-based identification of high-dimensional surrogate markers applied to vaccinology},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accounting for misclassification of binary outcomes in external control arm studies for unanchored indirect comparisons: Simulations and applied example. <em>SIM</em>, <em>44</em>(20-22), e70236. (<a href='https://doi.org/10.1002/sim.70236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-arm control trials are increasingly proposed as a potential approach for treatment evaluation. However, the limitations of this design restrict its methodological acceptability. Regulatory agencies have raised concerns about this approach, although it is sometimes required in applications based solely on such studies. Consequently, the need for accurate indirect treatment comparisons has become critical, especially when constructing external control arms using routinely collected data as outcome measurements may differ from those recorded in the single-arm trial leading to potential misclassification of outcomes. This study aimed to quantify the bias from ignoring misclassification of a binary outcome within unanchored indirect comparisons, through simulations, and to propose a likelihood-based method to correct this bias (i.e., the outcome-corrected model). Simulations demonstrated that ignoring misclassification results in significant bias and poor coverage probabilities. In contrast, the outcome-corrected model reduced bias, improved 95% confidence interval coverage probability and root mean square error in various scenarios. The methodology was applied to two hepatocellular carcinoma trials illustrating a practical application. The findings underscore the importance of addressing outcome misclassification in indirect comparisons. The proposed correction method may improve reliability in unanchored indirect treatment comparisons.},
  archive      = {J_SIM},
  author       = {Mikail Nourredine and Antoine Gavoille and Côme Lepage and Behrouz Kassai-Koupai and Michel Cucherat and Fabien Subtil},
  doi          = {10.1002/sim.70236},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70236},
  shortjournal = {Stat. Med.},
  title        = {Accounting for misclassification of binary outcomes in external control arm studies for unanchored indirect comparisons: Simulations and applied example},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What is fair? defining fairness in machine learning for health. <em>SIM</em>, <em>44</em>(20-22), e70234. (<a href='https://doi.org/10.1002/sim.70234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring that machine-learning (ML) models are safe, effective, and equitable across all patients is critical for clinical decision-making and for preventing the amplification of existing health disparities. In this work, we examine how fairness is conceptualized in ML for health, including why ML models may lead to unfair decisions and how fairness has been measured in diverse real-world applications. We review commonly used fairness notions within group, individual, and causal-based frameworks. We also discuss the outlook for future research and highlight opportunities and challenges in operationalizing fairness in health-focused applications.},
  archive      = {J_SIM},
  author       = {Jianhui Gao and Benson Chou and Zachary R. McCaw and Hilary Thurston and Paul Varghese and Chuan Hong and Jessica Gronsbell},
  doi          = {10.1002/sim.70234},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70234},
  shortjournal = {Stat. Med.},
  title        = {What is fair? defining fairness in machine learning for health},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient time-to-event dose-escalation guidance of multi-cycle cancer therapies. <em>SIM</em>, <em>44</em>(20-22), e70229. (<a href='https://doi.org/10.1002/sim.70229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment of cancer has rapidly evolved over time in quite dramatic ways, for example, from chemotherapies, targeted therapies to immunotherapies and chimeric antigen receptor T-cells. Nonetheless, the basic design of early phase I trials in oncology still follows predominantly a dose-escalation design. These trials monitor safety over the first treatment cycle to escalate the dose of the investigated drug. However, over time, studying additional factors such as drug combinations and/or variation in the timing of dosing became important as well. Existing designs were continuously enhanced and expanded to account for increased trial complexity. With toxicities occurring at later stages beyond the first cycle and the need to treat patients over multiple cycles, the focus on the first treatment cycle only is becoming a limitation in nowadays multi-cycle treatment therapies. Here, we introduce a multi-cycle time-to-event model (TITE-CLRM: Time-Interval-To-Event Complementary-Loglog Regression Model), allowing guidance of dose-escalation trials studying multi-cycle therapies. The challenge lies in balancing the need to monitor the safety of longer treatment periods with the need to continuously enroll patients safely. The proposed multi-cycle time-to-event model is formulated as an extension to established concepts like the escalation with overdose control principle. The model is motivated by a current drug development project and evaluated in a simulation study.},
  archive      = {J_SIM},
  author       = {Lukas A. Widmer and Sebastian Weber and Yunnan Xu and Hans-Jochen Weber},
  doi          = {10.1002/sim.70229},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70229},
  shortjournal = {Stat. Med.},
  title        = {Towards efficient time-to-event dose-escalation guidance of multi-cycle cancer therapies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplicity control in oncology clinical trials with a binary surrogate endpoint-based drop-the-losers design. <em>SIM</em>, <em>44</em>(20-22), e70209. (<a href='https://doi.org/10.1002/sim.70209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typical phase 1 oncology studies identify the maximum tolerated dose as the “optimal” dose for subsequent phases. With the advancement of molecular targeted agents and immunotherapies, however, evaluating two or more doses has become increasingly critical for dose selection. Such evaluation is often done in phase 2 studies in a randomized manner. In this article, we evaluate the strategy of applying an adaptive phase 2/3 seamless design for dose selection in oncology studies. Specifically, we consider the “drop-the-losers” design, where multiple treatment arms and a control arm are administered during the initial stage, and a more effective arm is identified for later stages by a binary surrogate endpoint such as overall response. We derive the theoretical type I error inflation scale and conduct simulation studies to illustrate the impact of various factors on the type I error inflation in such designs. Furthermore, we demonstrate the findings through the design of a lung cancer trial and introduce a software that implements the proposed design.},
  archive      = {J_SIM},
  author       = {Weibin Zhong and Jing-ou Liu and Chenguang Wang},
  doi          = {10.1002/sim.70209},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70209},
  shortjournal = {Stat. Med.},
  title        = {Multiplicity control in oncology clinical trials with a binary surrogate endpoint-based drop-the-losers design},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted trigonometric regression for suboptimal designs in circadian transcriptome studies. <em>SIM</em>, <em>44</em>(20-22), e70201. (<a href='https://doi.org/10.1002/sim.70201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circadian transcriptome studies often use trigonometric regression to model gene expression over time. Ideally, protocols in these studies would collect tissue samples at evenly distributed and equally spaced time points over a 24-hour period. This sample collection protocol is known as an equispaced design, which is considered the optimal experimental design for trigonometric regression under multiple statistical criteria. However, implementing equispaced designs in studies involving individuals is logistically challenging, and failure to employ an equispaced design could introduce variability in the statistical power of a hypothesis test relative to a model's phase-shift parameter estimates. This article is motivated by the variability in power for hypothesis testing when tissue samples are not collected under an equispaced design, and considers a weighted trigonometric regression as a remedy. Specifically, the weights for this regression are the normalized reciprocals of estimates derived from a kernel density estimator for sample collection time, which inflates the weight of samples collected at underrepresented time points. A search procedure is also introduced to identify the hyperparameter for kernel density estimation that relates to maximizing the smallest eigenvalue of the Hessian of weighted squared loss, which is motivated by the -optimality criterion from experimental design literature. Simulation studies consistently demonstrate that this weighted regression mitigates variability in power for hypothesis tests performed with an estimated model. Illustrations with six circadian transcriptome datasets further indicate that this weighted regression consistently yields larger test statistics than its unweighted counterpart for first-order trigonometric regression, or cosinor regression, which is prevalent in circadian transcriptome studies.},
  archive      = {J_SIM},
  author       = {Michael T. Gorczyca and Justice D. Sefas},
  doi          = {10.1002/sim.70201},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70201},
  shortjournal = {Stat. Med.},
  title        = {Weighted trigonometric regression for suboptimal designs in circadian transcriptome studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating heterogeneity in mixed hidden markov models with an application to the sleep-wake cycle. <em>SIM</em>, <em>44</em>(20-22), e70197. (<a href='https://doi.org/10.1002/sim.70197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sleep–wake cycle plays an important and far-reaching role in health. By utilizing personal physical activity monitors (PAMs), inferences about the sleep–wake cycle can be made. Hidden Markov models (HMMs) have been applied in this area as an accurate unsupervised approach. To account for heterogeneity in activity levels, we developed a mixed HMM that allows for individual differences. Through extensive simulations, we quantified the added gains relative to a standard HMM from using a mixed HMM to account for heterogeneity across several realistic scenarios. We found that mixed HMMs are often more accurate than standard HMMs when follow-up times are shorter. In situations with many repeated measurements per individual, a standard and mixed HMM have similar levels of accuracy, although a standard HMM is faster and easier to implement. Afterward, we applied our HMMs to actigraphy data from the National Health and Nutrition Examination Survey. We present results on sleep summary statistics by age and BMI. Summary statistics about the sleep–wake cycle extracted by the standard and mixed HMM were qualitatively similar. Differences in results, however, were likely driven by the heterogeneity in physical activity due to BMI and age, which we identified using a post hoc investigation of the data-driven clusters produced by the mixed HMM.},
  archive      = {J_SIM},
  author       = {Jordan Aron and Paul S. Albert and Mark B. Fiecas},
  doi          = {10.1002/sim.70197},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70197},
  shortjournal = {Stat. Med.},
  title        = {Incorporating heterogeneity in mixed hidden markov models with an application to the sleep-wake cycle},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimum area confidence set optimality for simultaneous confidence bands for percentiles with applications to drug shelf-life estimation. <em>SIM</em>, <em>44</em>(20-22), e70184. (<a href='https://doi.org/10.1002/sim.70184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One important property of any drug product is its stability over time. A key objective in drug stability studies is to estimate the shelf-life of a drug, involving a suitable definition of the true shelf-life and the construction of an appropriate estimate of the true shelf-life. Simultaneous confidence bands (SCBs) for percentiles in linear regression are valuable tools for determining the shelf-life in drug stability studies. In this paper, we propose a novel criterion, the Minimum Area Confidence Set (MACS) criterion, for finding the optimal SCB for percentile regression lines. This criterion focuses on the area of the constrained regions for the newly proposed pivotal quantities, which are generated from the confidence set for the unknown parameters of an SCB. We employ the new pivotal quantities to construct exact SCBs over any finite covariate intervals and use the MACS criterion to compare several SCBs of different forms. The optimal SCB under the MACS criterion can be used to construct the interval estimate of the true shelf-life. Furthermore, a new computationally efficient method is proposed for calculating the critical constants of exact SCBs for percentile regression lines. A real data example on drug stability is provided for illustration.},
  archive      = {J_SIM},
  author       = {Lingjiao Wang and Yang Han and Wei Liu and Frank Bretz},
  doi          = {10.1002/sim.70184},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70184},
  shortjournal = {Stat. Med.},
  title        = {Minimum area confidence set optimality for simultaneous confidence bands for percentiles with applications to drug shelf-life estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Letter to the editors. <em>SIM</em>, <em>44</em>(20-22), e70119. (<a href='https://doi.org/10.1002/sim.70119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Saralees Nadarajah and Tibor K. Pogány},
  doi          = {10.1002/sim.70119},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70119},
  shortjournal = {Stat. Med.},
  title        = {Letter to the editors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
