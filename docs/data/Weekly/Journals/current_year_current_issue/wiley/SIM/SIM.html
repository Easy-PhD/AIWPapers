<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim">SIM - 14</h2>
<ul>
<li><details>
<summary>
(2025). A practical framework to design immunization studies based on the beta distribution. <em>SIM</em>, <em>44</em>(23-24), e70293. (<a href='https://doi.org/10.1002/sim.70293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An optimally designed experiment reaches results quicker, at a lower cost, or with fewer observations and is therefore crucial in maximizing resource efficiency in research. In immunization studies, the primary goal is often to characterize antibody kinetics—the change in antibody concentration over time. However, nonlinear models for antibody kinetics present substantial challenges for study design, particularly the need to provide information on the parameters of interest. We propose a novel framework to facilitate the design of immunization studies using simple, understandable information. We assume that the mean antibody concentration follows the structural form of the beta density until reaching a plateau. Using the time and height of the maximum and the time and height of the plateau, we can uniquely determine the antibody kinetics curve. Optimal sampling schedules are determined using D-optimality, with D-efficiency used to compare designs. In a robustness analysis across 12 scenarios, we analyzed the framework's sensitivity to misspecification in the initial information. When misspecifying one parameter at a time, the median D-efficiencies exceeded 0.95 and the first quartiles were greater than or equal to 0.9 for all parameters, highlighting the robustness of the framework. Misspecification in the height of the plateau and time of the maximum affected the D-efficiency the most. The great advantage of the framework is that we only need intuitive information from the medical professionals to design an immunization study, in which determining the antibody kinetics is the main goal.},
  archive      = {J_SIM},
  author       = {Stefan Embacher and Andrea Berghold and Kirsten Maertens and Sereina A. Herzog},
  doi          = {10.1002/sim.70293},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70293},
  shortjournal = {Stat. Med.},
  title        = {A practical framework to design immunization studies based on the beta distribution},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating risk factors for pathogenic dose accrual from longitudinal data. <em>SIM</em>, <em>44</em>(23-24), e70291. (<a href='https://doi.org/10.1002/sim.70291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating risk factors for the incidence of a disease is crucial for understanding its etiology. For diseases caused by enteric pathogens, off-the-shelf statistical model-based approaches do not consider the biological mechanisms through which infection occurs and thus can only be used to make comparatively weak statements about the association between risk factors and incidence. Building off of established work in quantitative microbiological risk assessment, we propose a new approach to determining the association between risk factors and dose accrual rates. Our more mechanistic approach achieves a higher degree of biological plausibility, incorporates currently ignored sources of variability, and provides regression parameters that are easily interpretable as the dose accrual rate ratio due to changes in the risk factors under study. We also describe a method for leveraging information across multiple pathogens. The proposed methods are available as an R package at https://github.com/dksewell/dare . Our simulation study shows unacceptable coverage rates from generalized linear models, while the proposed approach empirically maintains the nominal rate even when the model is misspecified. Finally, we demonstrated our proposed approach by applying our method to infant data obtained through the PATHOME study ( https://reporter.nih.gov/project-details/10227256 ), discovering the impact of various environmental factors on infant enteric infections.},
  archive      = {J_SIM},
  author       = {Daniel K. Sewell and Kelly K. Baker},
  doi          = {10.1002/sim.70291},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70291},
  shortjournal = {Stat. Med.},
  title        = {Estimating risk factors for pathogenic dose accrual from longitudinal data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What's the weight? estimating controlled outcome differences in complex surveys for health disparities research. <em>SIM</em>, <em>44</em>(23-24), e70289. (<a href='https://doi.org/10.1002/sim.70289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we are motivated by the problem of estimating racial disparities in health outcomes, specifically the average controlled difference (ACD) in telomere length between Black and White individuals, using data from the National Health and Nutrition Examination Survey (NHANES). To do so, we build a propensity for race to properly adjust for other social determinants while characterizing the controlled effect of race on telomere length. Propensity score methods are broadly employed with observational data as a tool to achieve covariate balance, but how to implement them in complex surveys is less studied—in particular, when the survey weights depend on the group variable under comparison (as the NHANES sampling scheme depends on self-reported race). We propose identification formulas to properly estimate the ACD in outcomes between Black and White individuals, with appropriate weighting for both covariate imbalance across the two racial groups and generalizability . Via extensive simulation, we show that our proposed methods outperform traditional analytic approaches in terms of bias, mean squared error, and coverage when estimating the ACD for our setting of interest. In our data, we find that evidence of racial differences in telomere length between Black and White individuals attenuates after accounting for confounding by socioeconomic factors and utilizing appropriate propensity score and survey weighting techniques. Software to implement these methods and code to reproduce our results can be found in the R package svycdiff , available through the Comprehensive R Archive Network (CRAN) at cran.r-project.org/web/packages/svycdiff/ , or in a development version on GitHub at github.com/salernos/svycdiff .},
  archive      = {J_SIM},
  author       = {Stephen Salerno and Emily K. Roberts and Belinda L. Needham and Tyler H. McCormick and Fan Li and Bhramar Mukherjee and Xu Shi},
  doi          = {10.1002/sim.70289},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70289},
  shortjournal = {Stat. Med.},
  title        = {What's the weight? estimating controlled outcome differences in complex surveys for health disparities research},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep mixture of linear mixed models for complex longitudinal data. <em>SIM</em>, <em>44</em>(23-24), e70288. (<a href='https://doi.org/10.1002/sim.70288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixtures of linear mixed models are widely used for modeling longitudinal data for which observation times differ between subjects. In typical applications, temporal trends are described using a basis expansion, with basis coefficients treated as random effects varying by subject. Additional random effects can describe variation between mixture components or other known sources of variation in complex designs. A key advantage of these models is that they provide a natural mechanism for clustering. Current versions of mixtures of linear mixed models are not specifically designed for the case where there are many observations per subject and complex temporal trends, which require a large number of basis functions to capture. In this case, the subject-specific basis coefficients are a high-dimensional random effects vector, for which the covariance matrix is hard to specify and estimate, especially if it varies between mixture components. To address this issue, we consider the use of deep mixture of factor analyzers models as a prior for the random effects. The resulting deep mixture of linear mixed models is well suited for high-dimensional settings, and we describe an efficient variational inference approach to posterior computation. The efficacy of the method is demonstrated in biomedical applications and on simulated data.},
  archive      = {J_SIM},
  author       = {Lucas Kock and Nadja Klein and David J. Nott},
  doi          = {10.1002/sim.70288},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70288},
  shortjournal = {Stat. Med.},
  title        = {Deep mixture of linear mixed models for complex longitudinal data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating and evaluating counterfactual prediction models. <em>SIM</em>, <em>44</em>(23-24), e70287. (<a href='https://doi.org/10.1002/sim.70287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual prediction methods are required when a model will be deployed in a setting where treatment policies differ from the setting where the model was developed, or when a model provides predictions under hypothetical interventions to support decision-making. However, estimating and evaluating counterfactual prediction models is challenging because, unlike traditional (factual) prediction, one does not observe the potential outcomes for all individuals under all treatment strategies of interest. Here, we discuss how to estimate a counterfactual prediction model, how to assess the model's performance, and how to perform model and tuning parameter selection. We provide identification and estimation results for counterfactual prediction models and for multiple measures of counterfactual model performance, including loss-based measures, the area under the receiver operating characteristic curve, and the calibration curve. Importantly, our results allow valid estimates of model performance under counterfactual intervention even if the candidate prediction model is misspecified, permitting a wider array of use cases. We illustrate these methods using simulation and apply them to the task of developing a statin-naïve risk prediction model for cardiovascular disease.},
  archive      = {J_SIM},
  author       = {Christopher B. Boyer and Issa J. Dahabreh and Jon A. Steingrimsson},
  doi          = {10.1002/sim.70287},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70287},
  shortjournal = {Stat. Med.},
  title        = {Estimating and evaluating counterfactual prediction models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation of additive shared-frailty models for recurrent event data with dependent censoring. <em>SIM</em>, <em>44</em>(23-24), e70286. (<a href='https://doi.org/10.1002/sim.70286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event data with dependent censoring frequently arise in medical follow-up studies. In analyzing such data, one main challenge is addressing the complex dependencies among the recurrent events, failure events, and censoring events. In this paper, we focus on additive shared-frailty models for recurrent event processes and failure times, and propose a robust estimation procedure that accommodates censoring times dependent on both recurrent and failure events, even after conditioning on observed covariates. Notably, our method does not require specifying the exact dependence structure between censoring and recurrent/failure times, nor does it assume a particular frailty distribution. We show that the resulting estimates are consistent and asymptotically normal. We further assess the method's finite-sample performance through simulation studies, and illustrate its practical utility with a hospitalization dataset.},
  archive      = {J_SIM},
  author       = {Xin Chen and Jieli Ding and Liuquan Sun},
  doi          = {10.1002/sim.70286},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70286},
  shortjournal = {Stat. Med.},
  title        = {Robust estimation of additive shared-frailty models for recurrent event data with dependent censoring},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new test for assessing the covariate effect in ROC curves. <em>SIM</em>, <em>44</em>(23-24), e70284. (<a href='https://doi.org/10.1002/sim.70284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ROC curve is a statistical tool that analyzes the accuracy of a diagnostic test in which a variable is used to decide whether an individual is healthy or not. Along with that diagnostic variable, it is usual to have information on some other covariates. In some situations, it is advisable to incorporate that information into the study, as the performance of the ROC curves can be affected by them. Using the covariate-adjusted, the covariate-specific, or the pooled ROC curves, we discuss the implications of excluding or including the covariates in the analysis. Motivated by the above, a new test for comparing the covariate-adjusted and the pooled ROC curve is proposed, and the problem is illustrated by analyzing a real database.},
  archive      = {J_SIM},
  author       = {Arís Fanjul-Hevia and Juan Carlos Pardo-Fernández and Wenceslao González-Manteiga},
  doi          = {10.1002/sim.70284},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70284},
  shortjournal = {Stat. Med.},
  title        = {A new test for assessing the covariate effect in ROC curves},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analyses for missing in repeatedly measured outcome data. <em>SIM</em>, <em>44</em>(23-24), e70282. (<a href='https://doi.org/10.1002/sim.70282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss practical aspects of conducting sensitivity analyses for missing data with a repeatedly measured outcome. Our motivation is a SMART trial with a repeatedly measured outcome subject to missingness. We discuss and describe delta-based controlled imputation approaches to conducting sensitivity analyses for such trials that typically use linear mixed models for their primary analysis. We find that delta-based sensitivity analyses for trials with repeatedly measured outcome variables are enhanced by using MICE for the imputation. Further, including last-observed-before-time covariates is critical for a repeatedly observed outcome. We also develop some novel metrics for judging the adequacy of sensitivity analyses. Trial Registration: Tailoring Mobile Health Technology to Reduce Obesity and Improve Cardiovascular Health in Resource-Limited Neighborhood Environments: NCT03288207.},
  archive      = {J_SIM},
  author       = {James F. Troendle and Aparajita Sur and Eric S. Leifer and Tiffany Powell-Wiley},
  doi          = {10.1002/sim.70282},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70282},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity analyses for missing in repeatedly measured outcome data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accounting for misclassification of cause of death in weighted cumulative incidence functions for causal analyses. <em>SIM</em>, <em>44</em>(23-24), e70281. (<a href='https://doi.org/10.1002/sim.70281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misclassification between causes of death can produce bias in estimated cumulative incidence functions. When estimating causal quantities, such as comparing the cumulative incidence of death due to specific causes under interventions, such bias can lead to suboptimal decision making. Here, a consistent semiparametric estimator of the cumulative incidence function under interventions in settings with misclassification between two event types is presented. The measurement parameters for this estimator can be informed by validation data or expert knowledge. Moreover, a modified bootstrap approach to variance estimation is proposed for confidence interval construction. The proposed estimator was applied to estimate the cumulative incidence of AIDS-related mortality in the Multicenter AIDS Cohort Study under single- versus combination-drug antiretroviral therapy regimens that may be subject to confounding. The proposed estimator is shown to be consistent and performed well in finite samples via a series of simulation experiments.},
  archive      = {J_SIM},
  author       = {Jessie K. Edwards and Bonnie E. Shook-Sa and Giorgos Bakoyannis and Paul N. Zivich and Michael E. Herce and Stephen R. Cole},
  doi          = {10.1002/sim.70281},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70281},
  shortjournal = {Stat. Med.},
  title        = {Accounting for misclassification of cause of death in weighted cumulative incidence functions for causal analyses},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive biomarker-based design for early phase clinical trials. <em>SIM</em>, <em>44</em>(23-24), e70275. (<a href='https://doi.org/10.1002/sim.70275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying and quantifying predictive biomarkers is a critical issue of Precision Medicine approaches and patient-centric clinical development strategies. Early phase adaptive designs can improve trial efficiency by allowing for adaptations during the course of the trial. In this work, we are interested in adaptations based on interim analysis permitting a refinement of the existing study population according to their predictive biomarkers. At an early stage, the goal is not to precisely define the target population, but to not miss an efficacy signal that might be limited to a biomarker subgroup. In this work, we propose a one-arm two-stage early phase biomarker-guided design in the setting of an oncology trial where at the time of the interim analysis, several decisions can be made regarding stopping the entire trial early or continuing to recruit patients from the full or a selected patient population. Via simulations, we show that, although the sample size is limited, the proposed design leads to better decision-making compared to a classical design that does not consider an enrichment expansion.},
  archive      = {J_SIM},
  author       = {Alessandra Serra and Gaëlle Saint-Hilary and Sandrine Guilleminot and Julia Geronimi and Pavel Mozgunov},
  doi          = {10.1002/sim.70275},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70275},
  shortjournal = {Stat. Med.},
  title        = {Adaptive biomarker-based design for early phase clinical trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking the handling of method failure in comparison studies. <em>SIM</em>, <em>44</em>(23-24), e70257. (<a href='https://doi.org/10.1002/sim.70257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparison studies in methodological research are intended to compare methods in an evidence-based manner to help data analysts select a suitable method for their application. To provide trustworthy evidence, they must be carefully designed, implemented, and reported, especially given the many decisions made in planning and running. A common challenge in comparison studies is to handle the “failure” of one or more methods to produce a result for some (real or simulated) data sets, such that their performances cannot be measured in those instances. Despite an increasing emphasis on this topic in recent literature (focusing on non-convergence as a common manifestation), there is little guidance on proper handling and interpretation, and reporting of the chosen approach is often neglected. This paper aims to fill this gap and offers practical guidance on handling method failure in comparison studies. After exploring common handlings across various published comparison studies from classical statistics and predictive modeling, we show that the popular approaches of discarding data sets yielding failure (either for all or the failing methods only) and imputing are inappropriate in most cases. We then recommend a different perspective on method failure—viewing it as the result of a complex interplay of several factors rather than just its manifestation. Building on this, we provide recommendations on more adequate handling of method failure derived from realistic considerations. In particular, we propose considering fallback strategies that directly reflect the behavior of real-world users. Finally, we illustrate our recommendations and the dangers of inadequate handling of method failure through two exemplary comparison studies.},
  archive      = {J_SIM},
  author       = {Milena Wünsch and Moritz Herrmann and Elisa Noltenius and Mattia Mohr and Tim P. Morris and Anne-Laure Boulesteix},
  doi          = {10.1002/sim.70257},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70257},
  shortjournal = {Stat. Med.},
  title        = {Rethinking the handling of method failure in comparison studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Difference-in-differences for health policy and practice: A review of modern methods. <em>SIM</em>, <em>44</em>(23-24), e70247. (<a href='https://doi.org/10.1002/sim.70247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Difference-in-differences (DiD) is a popular observational causal inference method in health policy, employed to evaluate the real-world impact of policies and programs. To estimate treatment effects, DiD relies on a “parallel trends assumption” that treatment and comparison groups would have had parallel trajectories on average in the absence of an intervention. Recent years have seen both growing use of DiD in health policy and medicine and rapid advancements in DiD methods. To support DiD implementation in these fields, this paper reviews and synthesizes best practices and recent innovations. We provide recommendations to practitioners in four areas: (1) assessing causal assumptions; (2) adjusting for covariates and other approaches to relax causal assumptions; (3) accounting for staggered treatment timing; and (4) conducting robust inference, especially when normal-based clustered standard errors are inappropriate. For each, we explain challenges and common pitfalls in traditional DiD and recommend methods to address these. We explore current treatment of these topics through a focused literature review of medical DiD studies.},
  archive      = {J_SIM},
  author       = {Shuo Feng and Ishani Ganguli and Youjin Lee and John Poe and Andrew Ryan and Alyssa Bilinski},
  doi          = {10.1002/sim.70247},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70247},
  shortjournal = {Stat. Med.},
  title        = {Difference-in-differences for health policy and practice: A review of modern methods},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A corrected score approach for proportional hazards model with error-contaminated covariates subject to detection limits. <em>SIM</em>, <em>44</em>(23-24), e70243. (<a href='https://doi.org/10.1002/sim.70243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis under the proportional hazards model, covariates may be subject to both measurement error and detection limits. Most existing approaches only address one of these two complications and can lead to substantial bias and erroneous inference when dealing with both simultaneously. There is very limited research that addresses both these problems at the same time. These approaches are exclusively based on likelihood and require distribution assumptions on the underlying true covariates, as well as restricted independence assumptions on the censoring time. We propose a novel corrected score approach that relieves such stringent assumptions and is simpler in computation. The estimator is shown to be consistent and asymptotically normal. The finite sample performance of the proposed estimator is assessed through simulation studies and illustrated by application to data from an AIDS clinical trial. The approach can be used in the case of replicate data or instrumental data. It can also be extended to more general models and outcomes.},
  archive      = {J_SIM},
  author       = {Xiao Song and Ching-Yun Wang},
  doi          = {10.1002/sim.70243},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70243},
  shortjournal = {Stat. Med.},
  title        = {A corrected score approach for proportional hazards model with error-contaminated covariates subject to detection limits},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An approach to design adaptive clinical trials with time-to-event outcomes based on a general bayesian posterior distribution. <em>SIM</em>, <em>44</em>(23-24), e70207. (<a href='https://doi.org/10.1002/sim.70207'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are an integral component of medical research. Trials require careful design to, for example, maintain the safety of participants and to use resources efficiently. Adaptive clinical trials are often more efficient and ethical than standard or non-adaptive trials because they can require fewer participants, target more promising treatments, and stop early with sufficient evidence of effectiveness or harm. The design of adaptive trials is usually undertaken via simulation, which requires assumptions about the data-generating process to be specified a priori. Unfortunately, if such assumptions are misspecified, then the resulting trial design may not perform as expected, leading to, for example, reduced statistical power or an increased Type I error. Motivated by a clinical trial of a vaccine to protect against gastroenteritis in infants, we propose an approach to design adaptive clinical trials with time-to-event outcomes without needing to explicitly define the data-generating process. To facilitate this, we consider trial design within a general Bayesian framework where inference about the treatment effect is based on the partial likelihood. As a result, inference is robust to the form of the baseline hazard function, and we exploit this property to undertake trial design when the data-generating process is only implicitly defined. The benefits of this approach are demonstrated via an illustrative example and via redesigning our motivating clinical trial.},
  archive      = {J_SIM},
  author       = {James M. McGree and Antony M. Overstall and Mark Jones and Robert K. Mahar},
  doi          = {10.1002/sim.70207},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70207},
  shortjournal = {Stat. Med.},
  title        = {An approach to design adaptive clinical trials with time-to-event outcomes based on a general bayesian posterior distribution},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
