<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AIM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aim">AIM - 14</h2>
<ul>
<li><details>
<summary>
(2025). Automated vulnerability evaluation with large language models and vulnerability ontologies. <em>AIM</em>, <em>46</em>(3), e70031. (<a href='https://doi.org/10.1002/aaai.70031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The National Vulnerability Database (NVD) publishes over a thousand new vulnerabilities monthly, with a projected 25 percent increase in 2024, highlighting the crucial need for rapid vulnerability identification to mitigate cybersecurity attacks and save costs and resources. In this work, we propose using large language models (LLMs) to learn vulnerability evaluation from historical assessments of medical device vulnerabilities in a single manufacturer's portfolio. We highlight the effectiveness and challenges of using LLMs for automatic vulnerability evaluation and introduce a method to enrich historical data with cybersecurity ontologies, enabling the system to understand new vulnerabilities without retraining the LLM. Our LLM system integrates with the in-house application—Cybersecurity Management System (CSMS)—to help Siemens Healthineers (SHS) product cybersecurity experts efficiently assess the vulnerabilities in our products. Also, we present a comprehensive set of experiments that helps showcase the properties of the LLM and dataset, the various guardrails we have implemented to safeguard the system in production, and the guidelines for efficient integration of LLMs into the cybersecurity tool.},
  archive      = {J_AIM},
  author       = {Rikhiya Ghosh and Hans-Martin von Stockhausen and Martin Schmitt and George Marica Vasile and Sanjeev Kumar Karn and Oladimeji Farri},
  doi          = {10.1002/aaai.70031},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70031},
  shortjournal = {AI Mag.},
  title        = {Automated vulnerability evaluation with large language models and vulnerability ontologies},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal AI teacher: Integrating edge computing and reasoning models for enhanced student error analysis. <em>AIM</em>, <em>46</em>(3), e70030. (<a href='https://doi.org/10.1002/aaai.70030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends our previously published work on the virtual AI teacher (VATE) system, presented at IAAI-25. VATE is designed to autonomously analyze and correct student errors in mathematical problem-solving using advanced large language models (LLMs). By incorporating student draft images as a primary input for reasoning, the system provides fine-grained error cause analysis and supports real-time, multi-round AI—student dialogues. In this extended version, we introduce a new snap-to-solve module for handling low-reasoning tasks using edge-deployed LLMs, enabling faster and partially offline interaction. We also include expanded benchmarking experiments, including human expert evaluations and ablation studies, to assess model performance and learning outcomes. Deployed on the Squirrel AI platform, VATE demonstrates high accuracy (78.3%) in error analysis and improves student learning efficiency, with strong user satisfaction. These results suggest that VATE is a scalable, cost-effective solution with the potential to transform educational practices.},
  archive      = {J_AIM},
  author       = {Tianlong Xu and Yi-Fan Zhang and Zhendong Chu and Qingsong Wen},
  doi          = {10.1002/aaai.70030},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70030},
  shortjournal = {AI Mag.},
  title        = {Multimodal AI teacher: Integrating edge computing and reasoning models for enhanced student error analysis},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing generative recommender systems for government subsidy programs with a new RQ-VAE model: Wello and the korean government case. <em>AIM</em>, <em>46</em>(3), e70029. (<a href='https://doi.org/10.1002/aaai.70029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to an industry survey, many people miss opportunities to apply for government subsidy programs because they do not know how to apply. People also need to search manually and check whether these programs are suitable for them. To address this issue, our study developed a new generative recommender system with both users' information and government subsidy documents. Within our recommender system framework, we modify the existing Residual Quantization Variational Auto-Encoder (RQ-VAE) model to capture deep and abstract information from subsidy documents. Using semantic IDs generated for approximately 185,610 user click-stream histories and 240,000 documents, we train our recommender system to predict the semantic IDs of the next subsidy policy documents in which a user might be interested. In 2024, we successfully deployed our generative recommender system in Wello, a Korean Gov-Tech startup. In collaboration with the Korean government, our generative recommender system helped enhance program effectiveness by saving $7.8 million in unused funds and achieved $27.4 million in advertising efficiency gains. Also, Wello observed a 68% improvement in Click-Through-Ratio (CTR), increasing from 41.4% in the third quarter of 2024 to 69.6% in the fourth quarter of 2024. We thus anticipate that our generative recommender system will have a significant impact on both individuals and the government.},
  archive      = {J_AIM},
  author       = {Ji Won Kim and Jae Hong Park and Yuri Anna Kim and Sang Jun Lee},
  doi          = {10.1002/aaai.70029},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70029},
  shortjournal = {AI Mag.},
  title        = {Developing generative recommender systems for government subsidy programs with a new RQ-VAE model: Wello and the korean government case},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation and incident prevention in an enterprise AI assistant. <em>AIM</em>, <em>46</em>(3), e70028. (<a href='https://doi.org/10.1002/aaai.70028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise AI Assistants are increasingly deployed in domains where accuracy is paramount, making each erroneous output a potentially significant incident. This paper presents a comprehensive framework for monitoring, benchmarking, and continuously improving such complex, multi-component systems under active development by multiple teams. Our approach encompasses three key elements: (1) a hierarchical “severity” framework for incident detection that identifies and categorizes errors while attributing component-specific error rates, facilitating targeted improvements; (2) a scalable and principled methodology for benchmark construction, evaluation, and deployment, designed to accommodate multiple development teams, mitigate overfitting risks, and assess the downstream impact of system modifications; and (3) a continual improvement strategy leveraging multidimensional evaluation, enabling the identification and implementation of diverse enhancement opportunities. By adopting this holistic framework, organizations can systematically enhance the reliability and performance of their AI Assistants, ensuring their efficacy in critical enterprise environments. We conclude by discussing how this multifaceted approach opens avenues for various classes of enhancements, including human-AI collaborative evaluation, paving the way for more robust and trustworthy AI systems.},
  archive      = {J_AIM},
  author       = {Akash V. Maharaj and David Arbour and Daniel Lee and Uttaran Bhattacharya and Anup Rao and Austin Zane and Avi Feller and Kun Qian and Sajjadur Rahman and Yunyao Li},
  doi          = {10.1002/aaai.70028},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70028},
  shortjournal = {AI Mag.},
  title        = {Evaluation and incident prevention in an enterprise AI assistant},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to the special issue on innovative applications of artificial intelligence (IAAI 2025). <em>AIM</em>, <em>46</em>(3), e70027. (<a href='https://doi.org/10.1002/aaai.70027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This year's innovative applications of AI special issue features AI systems deployed in real-world settings, from enterprise platforms to public services, demonstrating both technical rigor and measurable benefits for organizations and society. The eight selected articles span enterprise reliability, cybersecurity, aerospace, education, healthcare logistics, government services, and scalable AI strategy. Collectively, these works illustrate how AI is progressing from research prototypes to systems that organizations now rely on for critical decisions, offering lessons learned for both researchers and practitioners.},
  archive      = {J_AIM},
  author       = {Serdar Kadıoğlu and Sean McGregor and Jan Seyler},
  doi          = {10.1002/aaai.70027},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70027},
  shortjournal = {AI Mag.},
  title        = {Introduction to the special issue on innovative applications of artificial intelligence (IAAI 2025)},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multisensory machine intelligence. <em>AIM</em>, <em>46</em>(3), e70026. (<a href='https://doi.org/10.1002/aaai.70026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The future of artificial intelligence demands a paradigm shift toward multisensory perception—to systems that can digest ongoing multisensory observations, that can discover structure in unlabeled raw sensory data, and that can intelligently fuse useful information from different sensory modalities for decision-making. While we humans naturally perceive the world by looking, listening, touching, smelling, and tasting, traditional forms of machine intelligence mostly focus on a single sensory modality, particularly vision. Therefore, my research, which I refer to as multisensory machine intelligence, seeks to bridge this gap by empowering machines to emulate and enhance human capabilities in seeing, hearing, and feeling, ultimately enabling them to comprehensively perceive, understand, and interact with multisensory world.},
  archive      = {J_AIM},
  author       = {Ruohan Gao},
  doi          = {10.1002/aaai.70026},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70026},
  shortjournal = {AI Mag.},
  title        = {Multisensory machine intelligence},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent advances in finetuning multimodal large language models. <em>AIM</em>, <em>46</em>(3), e70025. (<a href='https://doi.org/10.1002/aaai.70025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finetuning serves as the critical adaptation mechanism for multimodal large language models, bridging their pretrained knowledge with specialized downstream task requirements. This paper reviews recent finetuning advances across three key dimensions: (1) efficiency-oriented methods that reduce resource costs; (2) capability-specific techniques enhancing specialized multimodal skills; and (3) task-unifying approaches that bridge understanding and generation. We demonstrate how these directions transform multimodal large language models from versatile foundations into adaptive, human-aligned systems, providing researchers with a structured roadmap for developing next-generation multimodal AI.},
  archive      = {J_AIM},
  author       = {Zhen Wang and Lin Li and Long Chen},
  doi          = {10.1002/aaai.70025},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70025},
  shortjournal = {AI Mag.},
  title        = {Recent advances in finetuning multimodal large language models},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward robust, interactive, and human-aligned AI systems. <em>AIM</em>, <em>46</em>(3), e70024. (<a href='https://doi.org/10.1002/aaai.70024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring that AI systems do what we, as humans, actually want them to do is one of the biggest open research challenges in AI alignment and safety. My research seeks to directly address this challenge by enabling AI systems to interact with humans to learn aligned and robust behaviors. The way robots and other AI systems behave is often the result of optimizing a reward function. However, manually designing good reward functions is highly challenging and error-prone, even for domain experts. Although reward functions are often difficult to manually specify, human feedback in the form of demonstrations or preferences is often much easier to obtain but can be difficult to interpret due to ambiguity and noise. Thus, it is critical that AI systems take into account epistemic uncertainty over the human's true intent. As part of the AAAI New Faculty Highlight Program, I will give an overview of my research progress along the following fundamental research areas: (1) efficiently quantifying uncertainty over human intent, (2) directly optimizing behavior to be robust to uncertainty over human intent, and (3) actively querying for additional human input to reduce uncertainty over human intent.},
  archive      = {J_AIM},
  author       = {Daniel S. Brown},
  doi          = {10.1002/aaai.70024},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70024},
  shortjournal = {AI Mag.},
  title        = {Toward robust, interactive, and human-aligned AI systems},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-markovian planning to coordinate aerial and maritime medical evacuation platforms. <em>AIM</em>, <em>46</em>(3), e70023. (<a href='https://doi.org/10.1002/aaai.70023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transfer of patients between two aircraft using an underway watercraft increases medical evacuation reach and flexibility in maritime environments. The selection of any one of multiple underway watercraft for patient exchange is complicated by participating aircraft utilization histories and participating watercraft positions and velocities. The selection problem is modeled as a semi-Markov decision process with an action space, including both fixed land and moving watercraft exchange points. Monte Carlo tree search with root parallelization is used to select optimal exchange points and determine aircraft dispatch times. Model parameters are varied in simulation to identify representative scenarios where watercraft exchange points reduce incident response times. We find that an optimal policy with watercraft exchange points outperforms an optimal policy without watercraft exchange points and a greedy policy by 35% and 40%, respectively. In partnership with the United States Army, we deploy for the first time the watercraft exchange point by executing a mock patient transfer with a manikin between two HH-60M medical evacuation helicopters and an underway Army Logistic Support Vessel south of the Hawaiian island of Oahu. Both helicopters were dispatched in accordance with our optimized decision strategy.},
  archive      = {J_AIM},
  author       = {Mahdi Al-Husseini and Kyle H. Wray and Mykel J. Kochenderfer},
  doi          = {10.1002/aaai.70023},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70023},
  shortjournal = {AI Mag.},
  title        = {Semi-markovian planning to coordinate aerial and maritime medical evacuation platforms},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reclaiming authorship in the age of generative AI: From panic to possibility. <em>AIM</em>, <em>46</em>(3), e70022. (<a href='https://doi.org/10.1002/aaai.70022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of generative AI, particularly large language models like ChatGPT, has precipitated a seismic shift in academia. Far from a gradual evolution, its sudden emergence has jolted educational institutions, leaving many academics grappling with a perceived encroachment upon their intellectual domain. This upheaval has sparked intense debates, with concerns ranging from the erosion of academic integrity to the devaluation of scholarly labor. This essay contends that such apprehensions, while understandable, may overlook the transformative potential of AI as a collaborative tool. Drawing parallels to historical disruptions—such as the advent of photography challenging traditional art forms—we explore how AI can augment human creativity rather than supplant it. By examining the dynamics of authorship, originality, and accountability, we argue for a redefinition of these concepts in the context of AI-assisted work. Emphasizing the importance of human oversight in guiding AI outputs, we advocate for a framework that recognizes the symbiotic relationship between human intellect and machine efficiency. Such a perspective not only preserves the essence of academic rigor but also embraces the democratization of knowledge production. Ultimately, this essay calls for a balanced approach that mitigates risks while harnessing the innovative capacities of generative AI in academia.},
  archive      = {J_AIM},
  author       = {Mohsen Askari},
  doi          = {10.1002/aaai.70022},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70022},
  shortjournal = {AI Mag.},
  title        = {Reclaiming authorship in the age of generative AI: From panic to possibility},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OnAIR: Applications of the NASA on-board artificial intelligence research platform. <em>AIM</em>, <em>46</em>(3), e70020. (<a href='https://doi.org/10.1002/aaai.70020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infusing artificial intelligence algorithms into production aerospace systems can be challenging due to costs, timelines, and a risk-averse industry. We introduce the Onboard Artificial Intelligence Research (OnAIR) platform, an open-source software pipeline and cognitive architecture tool that enables full life cycle AI research for on-board intelligent systems. We begin with a description and user walk-through of the OnAIR tool. Next, we describe four use cases of OnAIR for both research and deployed onboard applications, detailing their use of OnAIR and the benefits it provided to the development and function of each respective scenario. Lastly, we describe two upcoming planned deployments which will leverage OnAIR for crucial mission outcomes. We conclude with remarks on future work and goals for the forward progression of OnAIR as a tool to enable a larger AI and aerospace research community.},
  archive      = {J_AIM},
  author       = {Evana Gizzi and Connor Firth and Caleb Adams and James Berck and P. Timothy Chase Jr and Christian Cassamajor-Paul and Rachael Chertok and Lily Clough and Jonathan Davis and Melissa De La Cruz and Matthew Dosberg and Alan Gibson and Jonathan Hammer and Ibrahim Haroon and Michael A. Johnson and Brian Kempa and James Marshall and Patrick Maynard and Brett McKinney and Leyton McKinney and Michael Monaghan and Robin Onsay and Hayley Owens and Sam Pedrotty and Daniel Rogers and Mahmooda Sultana and Jivko Sinapov and Bethany Theiling and Aaron Woodard and Caroline Zouloumian and Connor Williams},
  doi          = {10.1002/aaai.70020},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70020},
  shortjournal = {AI Mag.},
  title        = {OnAIR: Applications of the NASA on-board artificial intelligence research platform},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tiered copyrightability for generative artificial intelligence: An empirical analysis of china and the united states judicial practices. <em>AIM</em>, <em>46</em>(3), e70018. (<a href='https://doi.org/10.1002/aaai.70018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of generative artificial intelligence (AI) poses significant challenges to traditional copyright frameworks, intensifying debates over the copyrightability of AI-generated outputs. By comparing judicial practices in China and the United States, it has been observed that the United States maintains a conservative stance of adhering to substantive control, while China demonstrates an inclusive approach through the criterion of creative contribution. Building upon this, this article transcends the traditional binary judgment model and constructs a tiered copyright determination model. Based on the level of human control and contribution in the AI generation process, it introduces dimensions such as technological controllability and density of human intent, classifying generative AI into three tiers: strong protection, weak protection, and non-protection. Regarding the copyrightability of content generated by generative AI, this article argues that the issue should be addressed within the framework of copyright law itself. When human participation is involved and the substantial contribution of the direct user is reflected in the AI-generated content, meeting the requirements for copyrightable works under copyright law, corresponding protective measures should be granted.},
  archive      = {J_AIM},
  author       = {Zichun Xu and Zhilang Xu},
  doi          = {10.1002/aaai.70018},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70018},
  shortjournal = {AI Mag.},
  title        = {Tiered copyrightability for generative artificial intelligence: An empirical analysis of china and the united states judicial practices},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feeling heard: Can AI really understand human's feeling?. <em>AIM</em>, <em>46</em>(3), e70017. (<a href='https://doi.org/10.1002/aaai.70017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIM},
  author       = {Nuke F. Hatta},
  doi          = {10.1002/aaai.70017},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70017},
  shortjournal = {AI Mag.},
  title        = {Feeling heard: Can AI really understand human's feeling?},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Against AI welfare: Care practices should prioritize living beings over AI. <em>AIM</em>, <em>46</em>(3), e70016. (<a href='https://doi.org/10.1002/aaai.70016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this Comment, we critique the growing “AI welfare” movement and propose a novel guideline, the Precarity Guideline, to determine care entitlement. In contrast to approaches that emphasize potential for suffering, the Precarity Guideline is grounded in empirically identifiable features. The severity of ongoing humanitarian crises, biodiversity loss, and climate change provides additional reasons to prioritize the needs of living beings over machine learning algorithms as candidates for care.},
  archive      = {J_AIM},
  author       = {John Dorsch and Mariel K. Goddu and Kathryn Nave and Tillmann Vierkant and Mark Coeckelbergh and Paula Gürtler and Petr Urban and Friderike Spang and Maximilian Moll},
  doi          = {10.1002/aaai.70016},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70016},
  shortjournal = {AI Mag.},
  title        = {Against AI welfare: Care practices should prioritize living beings over AI},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
