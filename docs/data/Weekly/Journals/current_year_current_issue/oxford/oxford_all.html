<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>oxford</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomet">BIOMET - 15</h2>
<ul>
<li><details>
<summary>
(2025). Debiased learning of the causal net benefit with censored event time data. <em>BIOMET</em>, <em>112</em>(3), asaf051. (<a href='https://doi.org/10.1093/biomet/asaf051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cox regression is the default approach to evaluating the (relative) effect of two treatments on a survival endpoint. This standard framework has nonetheless been criticized for its canonical effect measure, the hazard ratio, having a subtle interpretation, thereby hindering policy-making. This in turn has prompted interest in other effects measures, such as the difference in restricted mean survival time, the net benefit and the win ratio, which have become increasingly popular. Developments in estimation and inference for the net benefit and win ratio proceed either under a semiparametric model, at the risk of bias due to model misspecification, or in a nonparametric model, but without the flexibility to adjust for covariates. In this paper, we overcome these challenges by introducing a scalar, model-free measure of conditional causal net benefit in terms of counterfactuals and developing a debiased estimator based on its efficient influence function. This estimator is root- n consistent and asymptotically model-free by flexibly enabling data-adaptive learning (e.g., machine learning) of the dependence of treatment, survival and censoring time on baseline covariates. By incorporating such covariates, the proposed estimators can improve the efficiency of randomized trial analyses, as well as correct for confounding and censoring bias in both randomized and observational studies. We also propose variations of the considered estimand (and estimator) that have a more favourable efficiency bound. The proposed method is illustrated by simulation studies and analysis of breast cancer data.},
  archive      = {J_BIOMET},
  author       = {Martinussen, Torben and Vansteelandt, Stijn},
  doi          = {10.1093/biomet/asaf051},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf051},
  shortjournal = {Biometrika},
  title        = {Debiased learning of the causal net benefit with censored event time data},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying and bounding the probability of necessity for causes of effects with ordinal outcomes. <em>BIOMET</em>, <em>112</em>(3), asaf049. (<a href='https://doi.org/10.1093/biomet/asaf049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the existing causal inference literature focuses on the forward-looking perspective by estimating effects of causes, the backward-looking perspective can provide insights into causes of effects. In backward-looking causal inference, the probability of necessity measures the probability that a certain event is caused by the treatment, given the observed treatment and outcome. Most existing results focus on binary outcomes. Motivated by applications with ordinal outcomes, we propose a general definition of the probability of necessity. However, identifying the probability of necessity is challenging because it involves the joint distribution of the potential outcomes. We propose the novel assumption of a monotonic incremental treatment effect to identify the probability of necessity with ordinal outcomes. We also discuss the testable implications of this key identification assumption. When it fails, we derive explicit formulas of the sharp large-sample bounds on the probability of necessity.},
  archive      = {J_BIOMET},
  author       = {Zhang, Chao and Geng, Zhi and Li, Wei and Ding, Peng},
  doi          = {10.1093/biomet/asaf049},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf049},
  shortjournal = {Biometrika},
  title        = {Identifying and bounding the probability of necessity for causes of effects with ordinal outcomes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudolikelihood estimators for graphical models: Existence and uniqueness. <em>BIOMET</em>, <em>112</em>(3), asaf045. (<a href='https://doi.org/10.1093/biomet/asaf045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical and sparse (inverse) covariance models have found widespread use in modern sample-starved high-dimensional applications. A part of their wide appeal stems from the significantly low sample sizes required for existence of the estimators, especially in comparison with the classical full covariance model. For undirected Gaussian graphical models, the minimum sample size required for the existence of maximum likelihood estimators had been an open question for almost half a century, and has recently been addressed in a series of works (Uhler, 2012; Ben-David, 2015; Gross & Sullivant, 2018). The very same question for pseudolikelihood estimators has remained unsolved ever since their introduction in the 1970s. Pseudolikelihood estimators have recently received renewed attention as they impose fewer restrictive assumptions and have better computational tractability, improved statistical performance and appropriateness in high-dimensional applications, thus renewing interest in this longstanding problem. In this paper, we undertake a comprehensive study of this open problem within the context of the two classes of pseudolikelihood methods proposed in the literature. We provide a precise answer to this question for both pseudolikelihood approaches and relate the corresponding solutions to their Gaussian counterparts.},
  archive      = {J_BIOMET},
  author       = {Roycraft, B and Rajaratnam, B},
  doi          = {10.1093/biomet/asaf045},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf045},
  shortjournal = {Biometrika},
  title        = {Pseudolikelihood estimators for graphical models: Existence and uniqueness},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent and scalable composite likelihood estimation of probit models with crossed random effects. <em>BIOMET</em>, <em>112</em>(3), asaf037. (<a href='https://doi.org/10.1093/biomet/asaf037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of crossed random effects models commonly incurs computational costs that grow faster than linearly in the sample size N ⁠ , often as fast as Ω ( N 3 / 2 ) ⁠ , making them unsuitable for large datasets. For non-Gaussian responses, integrating out the random effects to obtain a marginal likelihood poses significant challenges, especially for high-dimensional integrals for which the Laplace approximation may not be accurate. In this article we develop a composite likelihood approach to probit models that replaces the crossed random effects model with some hierarchical models that require only one-dimensional integrals. We show how to consistently estimate the crossed effects model parameters from the hierarchical model fits. We find that the computation scales linearly in the sample size. The method is illustrated by applying it to approximately five million observations from Stitch Fix, where the crossed effects formulation would require an integral of dimension larger than 700 000 ⁠ .},
  archive      = {J_BIOMET},
  author       = {Bellio, R and Ghosh, S and Owen, A B and Varin, C},
  doi          = {10.1093/biomet/asaf037},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf037},
  shortjournal = {Biometrika},
  title        = {Consistent and scalable composite likelihood estimation of probit models with crossed random effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive performance of power posteriors. <em>BIOMET</em>, <em>112</em>(3), asaf034. (<a href='https://doi.org/10.1093/biomet/asaf034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the impact of using tempered likelihoods in the production of posterior predictions. While the choice of temperature has an impact on predictive performance in small samples, we formally show that in moderate-to-large samples, tempering does not impact posterior predictions.},
  archive      = {J_BIOMET},
  author       = {McLatchie, Y and Fong, E and Frazier, D T and Knoblauch, J},
  doi          = {10.1093/biomet/asaf034},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf034},
  shortjournal = {Biometrika},
  title        = {Predictive performance of power posteriors},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bias correction of quadratic spectral estimators. <em>BIOMET</em>, <em>112</em>(3), asaf033. (<a href='https://doi.org/10.1093/biomet/asaf033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three cardinal, statistically consistent families of nonparametric estimators for the power spectral density of a time series are the lag-window, multitaper and Welch estimators. However, when estimating power spectral densities from a finite sample, each can be subject to nonignorable bias. Astfalck et al. (2024) developed a method that offers significant bias reduction for finite samples for Welch’s estimator, which this article extends to the larger family of quadratic estimators, thus providing similar theory for bias correction of lag-window and multitaper estimators as well as combinations thereof. Importantly, this theory may be used in conjunction with any and all tapers and lag-sequences designed for bias reduction, and so should be seen as an extension to valuable work in these fields, rather than a supplanting methodology. The order of computation is larger than the |$ {O}(n\log n) $| which is typical in spectral analyses, but it is not insurmountable in practice. Simulation studies support the theory with comparisons across variations of quadratic estimators.},
  archive      = {J_BIOMET},
  author       = {Astfalck, Lachlan C and Sykulski, Adam M and Cripps, Edward J},
  doi          = {10.1093/biomet/asaf033},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf033},
  shortjournal = {Biometrika},
  title        = {Bias correction of quadratic spectral estimators},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integer programming for learning directed acyclic graphs from nonidentifiable gaussian models. <em>BIOMET</em>, <em>112</em>(3), asaf032. (<a href='https://doi.org/10.1093/biomet/asaf032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of learning directed acyclic graphs from continuous observational data, generated according to a linear Gaussian structural equation model. State-of-the-art structure learning methods for this setting have at least one of the following shortcomings: (i) they cannot provide optimality guarantees and can suffer from learning suboptimal models; (ii) they rely on the stringent assumption that the noise is homoscedastic, and hence the underlying model is fully identifiable. We overcome these shortcomings and develop a computationally efficient mixed-integer programming framework for learning medium-sized problems that accounts for arbitrary heteroscedastic noise. We present an early stopping criterion under which we can terminate the branch-and-bound procedure to achieve an asymptotically optimal solution and establish the consistency of this approximate solution. In addition, we show via numerical experiments that our method outperforms state-of-the-art algorithms and is robust to noise heteroscedasticity, whereas the performance of some competing methods deteriorates under strong violations of the identifiability assumption. The software implementation of our method is available as the Python package micodag .},
  archive      = {J_BIOMET},
  author       = {Xu, Tong and Taeb, Armeen and Küçükyavuz, Simge and Shojaie, Ali},
  doi          = {10.1093/biomet/asaf032},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf032},
  shortjournal = {Biometrika},
  title        = {Integer programming for learning directed acyclic graphs from nonidentifiable gaussian models},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a turnkey approach for unbiased monte carlo estimation of smooth functions of expectations. <em>BIOMET</em>, <em>112</em>(3), asaf030. (<a href='https://doi.org/10.1093/biomet/asaf030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a smooth function f ⁠ , we develop a general approach to turn Monte Carlo samples with expectation m into an unbiased estimate of f ( m ) ⁠ . Specifically, we develop estimators that are based on randomly truncating the Taylor series expansion of f and estimating the coefficients of the truncated series. We derive their properties and propose a strategy to set their tuning parameters (which depend on m ⁠ ) automatically, with a view to making the whole approach simple to use. We develop our methods for the specific functions f ( x ) = log ⁡ x and f ( x ) = 1 / x ⁠ , as they arise in several statistical applications such as maximum likelihood estimation of latent variable models and Bayesian inference for unnormalized models. Detailed numerical studies are performed for a range of applications to determine how competitive and reliable the proposed approach is.},
  archive      = {J_BIOMET},
  author       = {Chopin, Nicolas and Crucinio, Francesca R and Singh, Sumeetpal S},
  doi          = {10.1093/biomet/asaf030},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf030},
  shortjournal = {Biometrika},
  title        = {Towards a turnkey approach for unbiased monte carlo estimation of smooth functions of expectations},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general form of covariate adjustment in clinical trials under covariate-adaptive randomization. <em>BIOMET</em>, <em>112</em>(3), asaf029. (<a href='https://doi.org/10.1093/biomet/asaf029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized clinical trials, adjusting for baseline covariates can improve credibility and efficiency for demonstrating and quantifying treatment effects. This article studies the augmented inverse propensity weighted estimator, which is a general form of covariate adjustment that uses linear, generalized linear and nonparametric or machine learning models for the conditional mean of the response given covariates. Under covariate-adaptive randomization, we establish general theorems that show a complete picture of the asymptotic normality, efficiency gain and applicability of augmented inverse propensity weighted estimators. In particular, we provide for the first time a rigorous theoretical justification of using machine learning methods with cross-fitting for dependent data under covariate-adaptive randomization. Based on the general theorems, we offer insights on the conditions for guaranteed efficiency gain and universal applicability under different randomization schemes, which also motivate a joint calibration strategy using some constructed covariates after applying augmented inverse propensity weighted estimators.},
  archive      = {J_BIOMET},
  author       = {Bannick, Marlena S and Shao, Jun and Liu, Jingyi and Du, Yu and Yi, Yanyao and Ye, Ting},
  doi          = {10.1093/biomet/asaf029},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf029},
  shortjournal = {Biometrika},
  title        = {A general form of covariate adjustment in clinical trials under covariate-adaptive randomization},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic factor analysis of high-dimensional recurrent events. <em>BIOMET</em>, <em>112</em>(3), asaf028. (<a href='https://doi.org/10.1093/biomet/asaf028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event time data arise in many studies, including in biomedicine, public health, marketing and social media analysis. High-dimensional recurrent event data involving many event types and observations have become prevalent with advances in information technology. This article proposes a semiparametric dynamic factor model for the dimension reduction of high-dimensional recurrent event data. The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies. A nearly rate-optimal smoothing-based estimator is proposed. An information criterion that consistently selects the number of factors is also developed. Simulation studies demonstrate the effectiveness of these inference tools. The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained.},
  archive      = {J_BIOMET},
  author       = {Chen, F and Chen, Y and Ying, Z and Zhou, K},
  doi          = {10.1093/biomet/asaf028},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf028},
  shortjournal = {Biometrika},
  title        = {Dynamic factor analysis of high-dimensional recurrent events},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving efficiency in transporting average treatment effects. <em>BIOMET</em>, <em>112</em>(3), asaf027. (<a href='https://doi.org/10.1093/biomet/asaf027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop flexible, semiparametric estimators of the average treatment effect transported to a new target population, which offer potential efficiency gains. Transport may be of value when the average treatment effect may differ across populations. We consider the setting where differences in the average treatment effect are due to differences in the distribution of effect modifiers, baseline covariates that modify the treatment effect. First, we propose a collaborative one-step semiparametric estimator that can improve efficiency. This approach does not require researchers to have knowledge about which covariates are effect modifiers and which differ in distribution between the populations, but it does require all covariates to be measured in the target population. Second, we propose two one-step semiparametric estimators that assume knowledge of which covariates are effect modifiers and which are both effect modifiers and differentially distributed between the populations. These estimators can be used even when not all covariates are observed in the target population; one estimator requires that only effect modifiers be observed, and the other requires that only those modifiers that are also differentially distributed be observed. We use simulations to compare the finite-sample performance of our proposed estimators and an existing semiparametric estimator of the transported average treatment effect, including in the presence of practical violations of the positivity assumption. Lastly, we apply our proposed estimators to a large-scale housing trial.},
  archive      = {J_BIOMET},
  author       = {Rudolph, K E and Williams, N T and Stuart, E A and Díaz, I},
  doi          = {10.1093/biomet/asaf027},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf027},
  shortjournal = {Biometrika},
  title        = {Improving efficiency in transporting average treatment effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general condition for bias attenuation by a nondifferentially mismeasured confounder. <em>BIOMET</em>, <em>112</em>(3), asaf026. (<a href='https://doi.org/10.1093/biomet/asaf026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world studies, the collected confounders may suffer from measurement error. Although mismeasurement of confounders is typically unintentional (originating from sources such as human oversight or imprecise machinery), deliberate mismeasurement also occurs and is becoming increasingly more common. For example, in the 2020 U.S. census, noise was added to measurements to assuage privacy concerns. Sensitive variables such as income or age are often partially censored and are only known up to a range of values. In such settings, obtaining valid estimates of the causal effect of a binary treatment can be impossible, as mismeasurement of confounders constitutes a violation of the no-unmeasured-confounding assumption. A natural question is whether the common practice of simply adjusting for the mismeasured confounder is justifiable. In this article, we answer this question in the affirmative and demonstrate that in many realistic scenarios not covered by previous literature, adjusting for the mismeasured confounders reduces bias compared to not adjusting.},
  archive      = {J_BIOMET},
  author       = {Zhang, Jeffrey and Lee, Junu},
  doi          = {10.1093/biomet/asaf026},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf026},
  shortjournal = {Biometrika},
  title        = {A general condition for bias attenuation by a nondifferentially mismeasured confounder},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: ‘Network cross-validation by edge sampling’. <em>BIOMET</em>, <em>112</em>(3), asaf023. (<a href='https://doi.org/10.1093/biomet/asaf023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asaf023},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf023},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Network cross-validation by edge sampling’},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning for piecewise-constant mean estimation: Optimality, ℓ1 and ℓ0 penalization. <em>BIOMET</em>, <em>112</em>(3), asaf018. (<a href='https://doi.org/10.1093/biomet/asaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study transfer learning for estimating piecewise-constant signals when source data, which may be relevant but disparate, are available in addition to target data. We first investigate transfer learning estimators that respectively employ ℓ 1 and ℓ 0 penalties for unisource data scenarios and then generalize these estimators to accommodate multisources. To further reduce estimation errors, especially when some sources significantly differ from the target, we introduce an informative source selection algorithm. We then examine these estimators with multisource selection and establish their minimax optimality. Unlike the common narrative in the transfer learning literature that the performance is enhanced through large source sample sizes, our approaches leverage higher observational frequencies and accommodate diverse frequencies across multiple sources. Our extensive numerical experiments show that the proposed transfer learning estimators significantly improve estimation performance compared to estimators that only use the target data.},
  archive      = {J_BIOMET},
  author       = {Wang, F and Yu, Y},
  doi          = {10.1093/biomet/asaf018},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf018},
  shortjournal = {Biometrika},
  title        = {Transfer learning for piecewise-constant mean estimation: Optimality, ℓ1 and ℓ0 penalization},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spike-and-slab prior for dimension selection in generalized linear network eigenmodels. <em>BIOMET</em>, <em>112</em>(3), asaf014. (<a href='https://doi.org/10.1093/biomet/asaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent space models are often used to model network data by embedding a network’s nodes into a low-dimensional latent space; however, choosing the dimension of this space remains a challenge. To this end, we begin by formalizing a class of latent space models we call generalized linear network eigenmodels that can model various edge types (binary, ordinal, nonnegative continuous) found in scientific applications. This model class subsumes the traditional eigenmodel by embedding it in a generalized linear model with an exponential dispersion family random component and fixes identifiability issues that hindered interpretability. We propose a Bayesian approach to dimension selection for generalized linear network eigenmodels based on an ordered spike-and-slab prior that provides improved dimension estimation and satisfies several appealing theoretical properties. We show that the model’s posterior is consistent and concentrates on low-dimensional models near the truth. We demonstrate our approach’s consistent dimension selection on simulated networks, and we use generalized linear network eigenmodels to study the effect of covariates on the formation of networks from biology, ecology and economics and the existence of residual latent structure.},
  archive      = {J_BIOMET},
  author       = {Loyal, Joshua D and Chen, Yuguo},
  doi          = {10.1093/biomet/asaf014},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf014},
  shortjournal = {Biometrika},
  title        = {A spike-and-slab prior for dimension selection in generalized linear network eigenmodels},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="biomtc">BIOMTC - 3</h2>
<ul>
<li><details>
<summary>
(2025). Statistical inference for heterogeneous treatment effect with right-censored data from synthesizing randomized clinical trials and real-world data. <em>BIOMTC</em>, <em>81</em>(4), ujaf131. (<a href='https://doi.org/10.1093/biomtc/ujaf131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneous treatment effect plays a crucial role in precision medicine. There is evidence that real-world data, even subject to biases, can be employed as supplementary evidence for randomized clinical trials to improve the statistical efficiency of the heterogeneous treatment effect estimation. In this paper, for survival data with right censoring, we consider estimating the heterogeneous treatment effect, defined as the difference of the treatment-specific conditional restricted mean survival times given covariates, by synthesizing evidence from randomized clinical trials and the real-world data with possible biases. We define an omnibus bias function to characterize the effect of biases caused by unmeasured confounders, censoring, and outcome heterogeneity, and further, identify it by combining the trial and real-world data. We propose a penalized sieve method to estimate the heterogeneous treatment effect and the bias function. We further study the theoretical properties of the proposed integrative estimators based on the theory of reproducing kernel Hilbert space and empirical process. The proposed methodology outperforms the approach solely based on the trial data through simulation studies and an integrative analysis of the data from a randomized trial and a real-world registry on early-stage non-small-cell lung cancer.},
  archive      = {J_BIOMTC},
  author       = {Mao, Guangcai and Yang, Shu and Wang, Xiaofei},
  doi          = {10.1093/biomtc/ujaf131},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujaf131},
  shortjournal = {Biometrics},
  title        = {Statistical inference for heterogeneous treatment effect with right-censored data from synthesizing randomized clinical trials and real-world data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse-intensity weighted generalized estimating equations for longitudinal data subject to irregular observation: Which variables should be included in the visit rate model?. <em>BIOMTC</em>, <em>81</em>(4), ujaf128. (<a href='https://doi.org/10.1093/biomtc/ujaf128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal data are often subject to irregular and informative visit times. Weighting generalized estimating equations by the inverse of the visit rate yields asymptotically unbiased estimates of regression coefficients provided that outcomes and visit times are conditionally independent, given the covariates in the visit model. Adding other covariates has no impact on the asymptotic bias of estimated regression coefficients, provided that conditional independence is maintained, but the impact on their variances is unknown. We show that variances are unchanged on adding variables associated with neither outcome nor visit process, and decrease on adding variables associated with outcome but not visit process. Adding variables associated with visits but not outcome may either increase or decrease variances of estimated outcome model regression coefficients, depending on the correlation structure of the covariates and the outcome. Application to a study of major depressive disorder found that the variances of estimated regression coefficients were of a similar magnitude when predictors of outcome but not visits were added to the visit rate model but consistently larger, in some cases by a factor of 2, on adding predictors of visits but not outcome. We recommend that visit process models include variables associated with outcome, but that those unassociated with the outcome be treated with caution.},
  archive      = {J_BIOMTC},
  author       = {Pullenayegum, Eleanor M and Shan, Di},
  doi          = {10.1093/biomtc/ujaf128},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujaf128},
  shortjournal = {Biometrics},
  title        = {Inverse-intensity weighted generalized estimating equations for longitudinal data subject to irregular observation: Which variables should be included in the visit rate model?},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized optimal selection design for dose optimization. <em>BIOMTC</em>, <em>81</em>(4), ujaf124. (<a href='https://doi.org/10.1093/biomtc/ujaf124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The US Food and Drug Administration (FDA) launched Project Optimus to shift the objective of dose selection from the maximum tolerated dose to the optimal biological dose (OBD), optimizing the benefit-risk tradeoff. One approach recommended by the FDA’s guidance is to conduct randomized trials comparing multiple doses. In this paper, using the selection design framework, we propose a Randomized Optimal SElection (ROSE) design, which minimizes sample size while ensuring the probability of correct selection of the OBD at pre-specified accuracy levels. The ROSE design is simple to implement, involving a straightforward comparison of the difference in response rates between two dose arms against a predetermined decision boundary. We further consider a two-stage ROSE design that allows for early selection of the OBD at the interim when there is sufficient evidence, further reducing the sample size. Simulation studies demonstrate that the ROSE design exhibits desirable operating characteristics in correctly identifying the OBD. A sample size of 15–40 patients per dosage arm typically results in a percentage of correct selection of the optimal dose ranging from 60% to 70%.},
  archive      = {J_BIOMTC},
  author       = {Wang, Shuqi and Yuan, Ying and Liu, Suyu},
  doi          = {10.1093/biomtc/ujaf124},
  journal      = {Biometrics},
  month        = {12},
  number       = {4},
  pages        = {ujaf124},
  shortjournal = {Biometrics},
  title        = {Randomized optimal selection design for dose optimization},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="biostat">BIOSTAT - 64</h2>
<ul>
<li><details>
<summary>
(2025). A bayesian semi-parametric approach to causal mediation for longitudinal mediators and time-to-event outcomes with application to a cardiovascular disease cohort study. <em>BIOSTAT</em>, <em>26</em>(1), kxaf027. (<a href='https://doi.org/10.1093/biostatistics/kxaf027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis of observational data is an important tool for investigating the potential causal effects of medications on disease-related risk factors, and on time-to-death (or disease progression) through these risk factors. However, when analyzing data from a cohort study, such analyses are complicated by the longitudinal structure of the risk factors and the presence of time-varying confounders. Leveraging data from the Atherosclerosis Risk in Communities (ARIC) cohort study, we develop a causal mediation approach, using (semi-parametric) Bayesian Additive Regression Tree (BART) models for the longitudinal and survival data. Our framework is developed using static longitudinal exposure regimes and allows for time-varying confounders and mediators, both of which can be either continuous or binary. We also identify and estimate direct and indirect causal effects in the presence of a competing event. We apply our methods to assess how medication, prescribed to target cardiovascular disease (CVD) risk factors, affects the time-to-CVD death.},
  archive      = {J_BIOSTAT},
  author       = {Bhandari, Saurabh and Daniels, Michael J and Josefsson, Maria and Lloyd-Jones, Donald M and Siddique, Juned},
  doi          = {10.1093/biostatistics/kxaf027},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf027},
  shortjournal = {Biostatistics},
  title        = {A bayesian semi-parametric approach to causal mediation for longitudinal mediators and time-to-event outcomes with application to a cardiovascular disease cohort study},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The winner’s curse under dependence: Repairing empirical bayes using convoluted densities. <em>BIOSTAT</em>, <em>26</em>(1), kxaf025. (<a href='https://doi.org/10.1093/biostatistics/kxaf025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The winner’s curse is a form of selection bias that arises when estimates are obtained for a large number of features, but only a subset of most extreme estimates is reported. It occurs in large scale significance testing as well as in rank-based selection, and imperils reproducibility of findings and follow-up study design. Several methods correcting for this selection bias have been proposed, but questions remain on their susceptibility to dependence between features since theoretical analyses and comparative studies are few. We prove that estimation through Tweedie’s formula is biased in presence of strong dependence, and propose a convolution of its density estimator to restore its competitive performance, which also aids other empirical Bayes methods. Furthermore, we perform a comprehensive simulation study comparing different classes of winner’s curse correction methods for point estimates as well as confidence intervals under dependence. We find a bootstrap method and empirical Bayes methods with density convolution to perform best at correcting the selection bias, although this correction generally does not improve the feature ranking. Finally, we apply the methods to a comparison of single-feature versus multi-feature prediction models in predicting Brassica napus phenotypes from gene expression data, demonstrating that the superiority of the best single-feature model may be illusory.},
  archive      = {J_BIOSTAT},
  author       = {Hawinkel, Stijn and Thas, Olivier and Maere, Steven},
  doi          = {10.1093/biostatistics/kxaf025},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf025},
  shortjournal = {Biostatistics},
  title        = {The winner’s curse under dependence: Repairing empirical bayes using convoluted densities},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based dimensionality reduction for single-cell RNA-seq using generalized bilinear models. <em>BIOSTAT</em>, <em>26</em>(1), kxaf024. (<a href='https://doi.org/10.1093/biostatistics/kxaf024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction is a critical step in the analysis of single-cell RNA-seq (scRNA-seq) data. The standard approach is to apply a transformation to the count matrix followed by principal components analysis (PCA). However, this approach can induce spurious heterogeneity and mask true biological variability. An alternative approach is to directly model the counts, but existing methods tend to be computationally intractable on large datasets and do not quantify uncertainty in the low-dimensional representation. To address these problems, we develop scGBM, a novel method for model-based dimensionality reduction of scRNA-seq data using a Poisson bilinear model. We introduce a fast estimation algorithm to fit the model using iteratively reweighted singular value decompositions, enabling the method to scale to datasets with millions of cells. Furthermore, scGBM quantifies the uncertainty in each cell’s latent position and leverages these uncertainties to assess the confidence associated with a given cell clustering. On real and simulated single-cell data, we find that scGBM produces low-dimensional embeddings that better capture relevant biological information while removing unwanted variation.},
  archive      = {J_BIOSTAT},
  author       = {Nicol, Phillip B and Miller, Jeffrey W},
  doi          = {10.1093/biostatistics/kxaf024},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf024},
  shortjournal = {Biostatistics},
  title        = {Model-based dimensionality reduction for single-cell RNA-seq using generalized bilinear models},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust transfer learning for individualized treatment rules in the presence of missing data. <em>BIOSTAT</em>, <em>26</em>(1), kxaf023. (<a href='https://doi.org/10.1093/biostatistics/kxaf023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment rule (ITR) is a stepping stone to precision medicine. To ensure validity, ITRs are ideally derived from randomized trial data, but the use cases of ITRs extend beyond these trial populations. Transferring knowledge from experimental data to real-world data is of interest, while experimental data with selective inclusion criteria reflect a population distribution that may differ from the real-world target. In well-designed experiments, granular information crucial to decision making can be thoroughly collected. However, part of this may not be accessible in real-world scenarios. We propose a learning scheme for ITR that simultaneously addresses the issues of covariate shift and missing covariates with a quantile-based optimal treatment objective. Specifically, we compare the outcome uncertainty across treatment arms that is due to missing covariates and use it to guide treatment selection to reduce the likelihood of worse outcomes. The performance of this method is evaluated in simulations and a sepsis data application.},
  archive      = {J_BIOSTAT},
  author       = {Sui, Zhiyu and Ding, Ying and Tang, Lu},
  doi          = {10.1093/biostatistics/kxaf023},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf023},
  shortjournal = {Biostatistics},
  title        = {Robust transfer learning for individualized treatment rules in the presence of missing data},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging population information in brain connectivity via bayesian ICA with a novel informative prior for correlation matrices. <em>BIOSTAT</em>, <em>26</em>(1), kxaf022. (<a href='https://doi.org/10.1093/biostatistics/kxaf022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain functional connectivity (FC), the temporal synchrony between brain networks, is essential to understand the functional organization of the brain and to identify changes due to neurological disorders, development, treatment, and other phenomena. Independent component analysis (ICA) is a matrix decomposition method used extensively for simultaneous estimation of functional brain topography and connectivity. However, estimation of FC via ICA is often sub-optimal due to the use of ad hoc estimation methods or temporal dimension reduction prior to ICA. Bayesian ICA can avoid dimension reduction, estimate latent variables and model parameters more accurately, and facilitate posterior inference. In this article, we develop a novel, computationally feasible Bayesian ICA method with population-derived priors on both the spatial ICs and their temporal correlation (that is, their FC). For the latter, we consider two priors: the inverse-Wishart, which is conjugate but is not ideally suited for modeling correlation matrices; and a novel informative prior for correlation matrices. For each prior, we derive a variational Bayes algorithm to estimate the model variables and facilitate posterior inference. Through extensive simulation studies, we evaluate the performance of the proposed methods and benchmark against existing approaches. We also analyze fMRI data from over 400 healthy adults in the Human Connectome Project. We find that our Bayesian ICA model and algorithms result in more accurate measures of functional connectivity and spatial brain features. Our novel prior for correlation matrices is more computationally intensive than the inverse-Wishart but provides improved accuracy and inference. The proposed framework is applicable to single-subject analysis, making it potentially clinically viable.},
  archive      = {J_BIOSTAT},
  author       = {Mejia, Amanda F and Bolin, David and Spencer, Daniel A and Eloyan, Ani},
  doi          = {10.1093/biostatistics/kxaf022},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf022},
  shortjournal = {Biostatistics},
  title        = {Leveraging population information in brain connectivity via bayesian ICA with a novel informative prior for correlation matrices},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control arm augmentation and hierarchical modeling in time-to-event trials: Advantages and pitfalls. <em>BIOSTAT</em>, <em>26</em>(1), kxaf021. (<a href='https://doi.org/10.1093/biostatistics/kxaf021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, it is often valuable to borrow information from external data sources. Unfortunately, when the external data are fully or partially incompatible with the current trial data, type I error rates can be highly inflated under traditional blanket discounting schemes such as power priors, commensurate priors, and meta-analytic predictive priors. However, such inflation of the probability of a false positive can be necessary, as the alternative is to have an underpowered study. For clinical trials with time-to-event (TTE) outcomes, this problem is exacerbated since many observations are censored. In this paper, we develop the latent exchangeability prior for TTE data. We also present a novel framework to borrow information about the treatment effect between groups as well as incorporate information from external controls. Simulation results suggest that, although efficiency gains can be achieved by borrowing information among external controls, operating characteristics in general can be quite poor under a lack of exchangeability. We apply our approach to a real clinical trial in second-line metastatic colorectal cancer.},
  archive      = {J_BIOSTAT},
  author       = {Alt, Ethan M and Chang, Xiuya and Liu, Qing and Jiang, Xun and Mo, May and Xia, H Amy and Ibrahim, Joseph G},
  doi          = {10.1093/biostatistics/kxaf021},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf021},
  shortjournal = {Biostatistics},
  title        = {Control arm augmentation and hierarchical modeling in time-to-event trials: Advantages and pitfalls},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mediation with external summary statistic information. <em>BIOSTAT</em>, <em>26</em>(1), kxaf020. (<a href='https://doi.org/10.1093/biostatistics/kxaf020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental health studies are increasingly measuring endogenous omics data ( ⁠|$ \boldsymbol{M} $|⁠ ) to study intermediary biological pathways by which an exogenous exposure ( ⁠|$ \boldsymbol{A} $|⁠ ) affects a health outcome ( ⁠|$ \boldsymbol{Y} $|⁠ ), given confounders ( ⁠|$ \boldsymbol{C} $|⁠ ). Mediation analysis is frequently performed to understand such mechanisms. If intermediary pathways are of interest, then there is likely literature establishing statistical and biological significance of the total effect, defined as the effect of |$ \boldsymbol{A} $| on |$ \boldsymbol{Y} $| given |$ \boldsymbol{C} $|⁠ . For mediation models with continuous outcomes and mediators, we show that leveraging external summary-level information on the total effect can improve estimation efficiency of the direct and indirect effects. Moreover, the efficiency gain depends on the asymptotic partial |$ R^{2} $| between the outcome ( ⁠|$ \boldsymbol{Y}\mid\boldsymbol{M},\boldsymbol{A},\boldsymbol{C} $|⁠ ) and total effect ( ⁠|$ \boldsymbol{Y}\mid\boldsymbol{A},\boldsymbol{C} $|⁠ ) models, with smaller (larger) values benefiting direct (indirect) effect estimation. We propose a robust data-adaptive estimation procedure, Mediation with External Summary Statistic Information, to improve estimation efficiency in settings with congenial external information, while simultaneously protecting against bias in settings with incongenial external information. In congenial simulation scenarios, we observe relative efficiency gains for mediation effect estimation of up to 40%. We illustrate our methodology using data from the Puerto Rico Testsite for Exploring Contamination Threats, where Cytochrome p450 metabolites are hypothesized to mediate the effect of phthalate exposure on gestational age at delivery. External summary information on the total effect comes from a recently published pooled analysis of 16 studies. The proposed framework blends mediation analysis with emerging data integration techniques.},
  archive      = {J_BIOSTAT},
  author       = {Boss, Jonathan and Hao, Wei and Cathey, Amber and Welch, Barrett M and Ferguson, Kelly K and Meeker, John D and Zhou, Xiang and Kang, Jian and Mukherjee, Bhramar},
  doi          = {10.1093/biostatistics/kxaf020},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf020},
  shortjournal = {Biostatistics},
  title        = {Mediation with external summary statistic information},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal functional mediation analysis with an application to functional magnetic resonance imaging data. <em>BIOSTAT</em>, <em>26</em>(1), kxaf019. (<a href='https://doi.org/10.1093/biostatistics/kxaf019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primary goal of task-based functional magnetic resonance imaging (fMRI) studies is to quantify the effective connectivity between brain regions when stimuli are presented. Assessing the dynamics of effective connectivity has attracted increasing attention. Causal mediation analysis serves as a widely implemented tool aiming to delineate the mechanism between task stimuli and brain activations. However, the case, where the treatment, mediator, and outcome are continuous functions, has not been studied. Causal mediation analysis for functional data is considered. Semiparametric functional linear structural equation models are introduced and causal assumptions are discussed. The proposed models allow for the estimation of individual effect curves. The models are applied to a task-based fMRI study, providing a new perspective of studying dynamic brain connectivity. The R package cfma for implementation is available on CRAN.},
  archive      = {J_BIOSTAT},
  author       = {Zhao, Yi and Luo, Xi and Sobel, Michael E and Lindquist, Martin A and Caffo, Brian S},
  doi          = {10.1093/biostatistics/kxaf019},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf019},
  shortjournal = {Biostatistics},
  title        = {Causal functional mediation analysis with an application to functional magnetic resonance imaging data},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A surrogate endpoint-based provisional approval causal roadmap, illustrated by vaccine development. <em>BIOSTAT</em>, <em>26</em>(1), kxaf018. (<a href='https://doi.org/10.1093/biostatistics/kxaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many rare diseases with no approved preventive interventions, promising interventions exist. However, it has proven difficult to conduct a pivotal phase 3 trial that could provide direct evidence demonstrating a beneficial effect of the intervention on the target disease outcome. When a promising putative surrogate endpoint(s) for the target outcome is available, surrogate-based provisional approval of an intervention may be pursued. Following the general Causal Roadmap rubric, we describe a surrogate endpoint-based provisional approval causal roadmap. Based on an observational study data set and a phase 3 randomized trial data set, this roadmap defines an approach to analyze the combined data set to draw a conservative inference about the treatment effect (TE) on the target outcome in the phase 3 study population. The observational study enrolls untreated individuals and collects baseline covariates, surrogate endpoints, and the target outcome, and is used to estimate the surrogate index—the regression of the target outcome on the surrogate endpoints and baseline covariates. The phase 3 trial randomizes participants to treated vs. untreated and collects the same data but is much smaller and hence very underpowered to directly assess TE, such that inference on TE is based on the surrogate index. This inference is made conservative by specifying 2 bias functions: one that expresses an imperfection of the surrogate index as a surrogate endpoint in the phase 3 study, and the other that expresses imperfect transport of the surrogate index in the untreated from the observational to the phase 3 study. Plug-in and nonparametric efficient one-step estimators of TE, with inferential procedures, are developed. The finite-sample performance of the estimators is evaluated in simulation studies. The causal roadmap is motivated by and illustrated with contemporary Group B Streptococcus vaccine development.},
  archive      = {J_BIOSTAT},
  author       = {Gilbert, Peter B and Peng, James and Han, Larry and Lange, Theis and Lu, Yun and Nie, Lei and Shih, Mei-Chiung and Waddy, Salina P and Wiley, Ken and Yann, Margot and Zafari, Zafar and Ghosh, Debashis and Follmann, Dean and Juraska, Michal and Díaz, Iván},
  doi          = {10.1093/biostatistics/kxaf018},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf018},
  shortjournal = {Biostatistics},
  title        = {A surrogate endpoint-based provisional approval causal roadmap, illustrated by vaccine development},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed lag interaction model with index modification. <em>BIOSTAT</em>, <em>26</em>(1), kxaf017. (<a href='https://doi.org/10.1093/biostatistics/kxaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiological evidence supports an association between exposure to air pollution during pregnancy and birth and child health outcomes. Typically, such associations are estimated by regressing an outcome on daily or weekly measures of exposure during pregnancy using a distributed lag model. However, these associations may be modified by multiple factors. We propose a distributed lag interaction model with index modification that allows for effect modification of a functional predictor by a weighted average of multiple modifiers. Our model allows for simultaneous estimation of modifier index weights and the exposure–time–response function via a spline cross-basis in a Bayesian hierarchical framework. Through simulations, we showed that our model out-performs competing methods when there are multiple modifiers of unknown importance. We applied our proposed method to a Colorado birth cohort to estimate the association between birth weight and air pollution modified by a neighborhood-vulnerability index and to a Mexican birth cohort to estimate the association between birthing-parent cardio-metabolic endpoints and air pollution modified by a birthing-parent lifetime stress index.},
  archive      = {J_BIOSTAT},
  author       = {Demateis, Danielle and India-Aldana, Sandra and Wright, Robert O and Wright, Rosalind J and Baccarelli, Andrea and Colicino, Elena and Wilson, Ander and Keller, Kayleigh P},
  doi          = {10.1093/biostatistics/kxaf017},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf017},
  shortjournal = {Biostatistics},
  title        = {Distributed lag interaction model with index modification},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating historic information to further improve power when conducting bayesian information borrowing in basket trials. <em>BIOSTAT</em>, <em>26</em>(1), kxaf016. (<a href='https://doi.org/10.1093/biostatistics/kxaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In basket trials a single therapeutic treatment is tested on several patient populations simultaneously, each of which forming a basket, where patients across all baskets on the trial share a common genetic aberration. These trials allow testing of treatments on small groups of patients, however, limited basket sample sizes can result in inadequate precision and power of estimates. It is well known that Bayesian information borrowing models such as the exchangeability-nonexchangeability (EXNEX) model can be implemented to tackle such a problem, drawing on information from one basket when making inference in another. An alternative approach to improve power of estimates, is to incorporate any historical or external information available. This paper considers models that amalgamate both forms of information borrowing, allowing borrowing between baskets in the ongoing trial whilst also drawing on response data from historical sources, with the aim to further improve treatment effect estimates. We propose several Bayesian information borrowing approaches that incorporate historical information into the model. These methods are data-driven, updating the degree of borrowing based on the level of homogeneity between information sources. A thorough simulation study is presented to draw comparisons between the proposed approaches, whilst also comparing to the standard EXNEX model in which no historical information is utilized. The models are also applied to a real-life trial example to demonstrate their performance in practice. We show that the incorporation of historic data under the novel approaches can lead to a substantial improvement in precision and power of treatment effect estimates when such data is homogeneous to the responses in the ongoing trial. Under some approaches, this came alongside an inflation in type I error rate in cases of heterogeneity. However, the use of a power prior in the EXNEX model is shown to increase power and precision, whilst maintaining similar error rates to the standard EXNEX model.},
  archive      = {J_BIOSTAT},
  author       = {Daniells, Libby and Mozgunov, Pavel and Barnett, Helen and Bedding, Alun and Jaki, Thomas},
  doi          = {10.1093/biostatistics/kxaf016},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf016},
  shortjournal = {Biostatistics},
  title        = {Incorporating historic information to further improve power when conducting bayesian information borrowing in basket trials},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and estimation of causal effects with confounders missing not at random. <em>BIOSTAT</em>, <em>26</em>(1), kxaf015. (<a href='https://doi.org/10.1093/biostatistics/kxaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making causal inferences from observational studies can be challenging when confounders are missing not at random. In such cases, identifying causal effects is often not guaranteed. Motivated by a real example, we consider a treatment-independent missingness assumption under which we establish the identification of causal effects when confounders are missing not at random. We propose a weighted estimating equation approach for estimating model parameters and introduce three estimators for the average causal effect, based on regression, propensity score weighting, and doubly robust estimation. We evaluate the performance of these estimators through simulations, and provide a real data analysis to illustrate our proposed method.},
  archive      = {J_BIOSTAT},
  author       = {Sun, Jian and Fu, Bo},
  doi          = {10.1093/biostatistics/kxaf015},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf015},
  shortjournal = {Biostatistics},
  title        = {Identification and estimation of causal effects with confounders missing not at random},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biomarker-assisted reporting in nutritional epidemiology: Addressing measurement error in exposure–disease associations. <em>BIOSTAT</em>, <em>26</em>(1), kxaf014. (<a href='https://doi.org/10.1093/biostatistics/kxaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In nutritional epidemiology, self-reported dietary data are commonly used to investigate diet–disease relationships. However, the resulting association estimates are often subject to biases due to random and systematic measurement errors. Regression calibration has emerged as a crucial method for addressing these biases by refining self-reported nutrient intake with objective biomarkers, which differ from the true values only by a random “noise” component. This paper presents methodological tools for analyzing nutritional epidemiology cohort studies involving time-to-event data when a biomarker subsample is available alongside dietary assessments. We introduce novel regression calibration methods to tackle two common challenges in this field. First, a widely used approach assumes that the log hazard ratio (HR) follows a linear function of dietary exposure. However, assessing whether this assumption holds—or if a more flexible model is needed to capture potential deviations from linearity—is often necessary. Second, another prevalent analytical strategy involves estimating HRs based on categorized dietary exposure variables. New methods are critically needed to minimize bias in defining category boundaries and estimating hazard ratios within exposure categories, both of which can be distorted by measurement error. We apply these methods to reassess the relationship between sodium and potassium intake and cardiovascular disease risk using data from the Women’s Health Initiative.},
  archive      = {J_BIOSTAT},
  author       = {Huang, Ying and Prentice, Ross L},
  doi          = {10.1093/biostatistics/kxaf014},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf014},
  shortjournal = {Biostatistics},
  title        = {Biomarker-assisted reporting in nutritional epidemiology: Addressing measurement error in exposure–disease associations},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting distributions of physical activity profiles in the national health and nutrition examination survey database using a partially linear fréchet single index model. <em>BIOSTAT</em>, <em>26</em>(1), kxaf013. (<a href='https://doi.org/10.1093/biostatistics/kxaf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object-oriented data analysis is a fascinating and evolving field in modern statistical science, with the potential to make significant contributions to biomedical applications. This statistical framework facilitates the development of new methods to analyze complex data objects that capture more information than traditional clinical biomarkers. This paper applies the object-oriented framework to analyze physical activity levels, measured by accelerometers, as response objects in a regression model. Unlike traditional summary metrics, we utilize a recently proposed representation of physical activity data as a distributional object, providing a more nuanced and complete profile of individual energy expenditure across all ranges of monitoring intensity. A novel hybrid Fréchet regression model is proposed and applied to US population accelerometer data from National Health and Nutrition Examination Survey (NHANES) 2011 to 2014. The semi-parametric nature of the model allows for the inclusion of nonlinear effects for critical variables, such as age, which are biologically known to have subtle impacts on physical activity. Simultaneously, the inclusion of linear effects preserves interpretability for other variables, particularly categorical covariates such as ethnicity and sex. The results obtained are valuable from a public health perspective and could lead to new strategies for optimizing physical activity interventions in specific American subpopulations.},
  archive      = {J_BIOSTAT},
  author       = {Matabuena, Marcos and Ghosal, Aritra and Meiring, Wendy and Petersen, Alexander},
  doi          = {10.1093/biostatistics/kxaf013},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf013},
  shortjournal = {Biostatistics},
  title        = {Predicting distributions of physical activity profiles in the national health and nutrition examination survey database using a partially linear fréchet single index model},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing the mean–variance relationship in spatially resolved transcriptomics data with spoon. <em>BIOSTAT</em>, <em>26</em>(1), kxaf012. (<a href='https://doi.org/10.1093/biostatistics/kxaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important task in the analysis of spatially resolved transcriptomics (SRT) data is to identify spatially variable genes (SVGs), or genes that vary in a 2D space. Current approaches rank SVGs based on either |$ P $| -values or an effect size, such as the proportion of spatial variance. However, previous work in the analysis of RNA-sequencing data identified a technical bias with log-transformation, violating the “mean–variance relationship” of gene counts, where highly expressed genes are more likely to have a higher variance in counts but lower variance after log-transformation. Here, we demonstrate the mean–variance relationship in SRT data. Furthermore, we propose spoon , a statistical framework using empirical Bayes techniques to remove this bias, leading to more accurate prioritization of SVGs. We demonstrate the performance of spoon in both simulated and real SRT data. A software implementation of our method is available at https://bioconductor.org/packages/spoon .},
  archive      = {J_BIOSTAT},
  author       = {Shah, Kinnary and Guo, Boyi and Hicks, Stephanie C},
  doi          = {10.1093/biostatistics/kxaf012},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf012},
  shortjournal = {Biostatistics},
  title        = {Addressing the mean–variance relationship in spatially resolved transcriptomics data with spoon},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analysis for the probability of benefit in randomized controlled trials with a binary treatment and a binary outcome. <em>BIOSTAT</em>, <em>26</em>(1), kxaf011. (<a href='https://doi.org/10.1093/biostatistics/kxaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a comprehensive understanding of the effect of a given treatment on an outcome of interest, quantification of individual treatment heterogeneity is essential, alongside estimation of the average causal effect. However, even in randomized controlled trials, quantities such as the probability of benefit or the probability of harm are not identifiable, since multiple potential outcomes cannot be observed simultaneously for the same individual. We propose a sensitivity analysis for the probability of benefit in randomized controlled trial settings with a binary treatment and a binary outcome, by quantifying the deviation from conditional independence of the two potential outcomes, given a set of measured prognostic baseline covariates. We do this using a marginal sensitivity analysis parameter that does not depend on the number or complexity of the measured covariates. We provide a guide to estimation and interpretation, and illustrate our method in simulations, as well as using a real data example from a randomized controlled trial studying the effect of umbilical vein oxytocin administration on the need for manual removal of the placenta during birth.},
  archive      = {J_BIOSTAT},
  author       = {Ciocănea-Teodorescu, Iuliana and Gabriel, Erin E and Sjölander, Arvid},
  doi          = {10.1093/biostatistics/kxaf011},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf011},
  shortjournal = {Biostatistics},
  title        = {Sensitivity analysis for the probability of benefit in randomized controlled trials with a binary treatment and a binary outcome},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic clustering using shared latent variable model for assessing alzheimer’s disease biomarkers. <em>BIOSTAT</em>, <em>26</em>(1), kxaf010. (<a href='https://doi.org/10.1093/biostatistics/kxaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The preclinical stage of many neurodegenerative diseases can span decades before symptoms become apparent. Understanding the sequence of preclinical biomarker changes provides a critical opportunity for early diagnosis and effective intervention prior to significant loss of patients’ brain functions. The main challenge to early detection lies in the absence of direct observation of the disease state and the considerable variability in both biomarkers and disease dynamics among individuals. Recent research hypothesized the existence of subgroups with distinct biomarker patterns due to co-morbidities and degrees of brain resilience. Our ability to diagnose early and intervene during the preclinical stage of neurodegenerative diseases will be enhanced by further insights into heterogeneity in the biomarker–disease relationship. In this article, we focus on Alzheimer’s disease (AD) and attempt to identify the systematic patterns within the heterogeneous AD biomarker–disease cascade. Specifically, we quantify the disease progression using a dynamic latent variable whose mixture distribution represents patient subgroups. Model estimation uses Hamiltonian Monte Carlo with the number of clusters determined by the Bayesian Information Criterion. We report simulation studies that investigate the performance of the proposed model in finite sample settings that are similar to our motivating application. We apply the proposed model to the Biomarkers of Cognitive Decline Among Normal Individuals data, a longitudinal study that was conducted over 2 decades among individuals who were initially cognitively normal. Our application yields evidence consistent with the hypothetical model of biomarker dynamics presented in Jack Jr et al. In addition, our analysis identified 2 subgroups with distinct disease-onset patterns. Finally, we develop a dynamic prediction approach to improve the precision of prognoses.},
  archive      = {J_BIOSTAT},
  author       = {Xu, Yizhen and Zeger, Scott and Wang, Zheyu},
  doi          = {10.1093/biostatistics/kxaf010},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf010},
  shortjournal = {Biostatistics},
  title        = {Probabilistic clustering using shared latent variable model for assessing alzheimer’s disease biomarkers},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and inference for causal spillover effects in egocentric-network randomized trials in the presence of network membership misclassification. <em>BIOSTAT</em>, <em>26</em>(1), kxaf009. (<a href='https://doi.org/10.1093/biostatistics/kxaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To leverage peer influence and increase population behavioral changes, behavioral interventions often rely on peer-based strategies. A common study design that assesses such strategies is the egocentric-network randomized trial (ENRT), where index participants receive a behavioral training and are encouraged to disseminate information to their peers. Under this design, a crucial estimand of interest is the Average Spillover Effect (ASpE), which measures the impact of the intervention on participants who do not receive it, but whose outcomes may be affected by others who do. The assessment of the ASpE relies on assumptions about, and correct measurement of, interference sets within which individuals may influence one another’s outcomes. It can be challenging to properly specify interference sets, such as networks in ENRTs, and when mismeasured, intervention effects estimated by existing methods will be biased. In studies where social networks play an important role in disease transmission or behavior change, correcting ASpE estimates for bias due to network misclassification is critical for accurately evaluating the full impact of interventions. We combined measurement error and causal inference methods to bias-correct the ASpE estimate for network misclassification in ENRTs, when surrogate networks are recorded in place of true ones, and validation data that relate the misclassified to the true networks are available. We investigated finite sample properties of our methods in an extensive simulation study and illustrated our methods in the HIV Prevention Trials Network (HPTN) 037 study.},
  archive      = {J_BIOSTAT},
  author       = {Chao, Ariel and Spiegelman, Donna and Buchanan, Ashley and Forastiere, Laura},
  doi          = {10.1093/biostatistics/kxaf009},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf009},
  shortjournal = {Biostatistics},
  title        = {Estimation and inference for causal spillover effects in egocentric-network randomized trials in the presence of network membership misclassification},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric mixture regression for asynchronous longitudinal data using multivariate functional principal component analysis. <em>BIOSTAT</em>, <em>26</em>(1), kxaf008. (<a href='https://doi.org/10.1093/biostatistics/kxaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transitional phase of menopause induces significant hormonal fluctuations, exerting a profound influence on the long-term well-being of women. In an extensive longitudinal investigation of women’s health during mid-life and beyond, known as the Study of Women’s Health Across the Nation (SWAN), hormonal biomarkers are repeatedly assessed, following an asynchronous schedule compared to other error-prone covariates, such as physical and cardiovascular measurements. We conduct a subgroup analysis of the SWAN data employing a semiparametric mixture regression model, which allows us to explore how the relationship between hormonal responses and other time-varying or time-invariant covariates varies across subgroups. To address the challenges posed by asynchronous scheduling and measurement errors, we model the time-varying covariate trajectories as functional data with reduced-rank Karhunen–Loéve expansions, where splines are employed to capture the mean and eigenfunctions. Treating the latent subgroup membership and the functional principal component (FPC) scores as missing data, we propose an Expectation-Maximization algorithm to effectively fit the joint model, combining the mixture regression for the hormonal response and the FPC model for the asynchronous, time-varying covariates. In addition, we explore data-driven methods to determine the optimal number of subgroups within the population. Through our comprehensive analysis of the SWAN data, we unveil a crucial subgroup structure within the aging female population, shedding light on important distinctions and patterns among women undergoing menopause.},
  archive      = {J_BIOSTAT},
  author       = {Lu, Ruihan and Li, Yehua and Yao, Weixin},
  doi          = {10.1093/biostatistics/kxaf008},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf008},
  shortjournal = {Biostatistics},
  title        = {Semiparametric mixture regression for asynchronous longitudinal data using multivariate functional principal component analysis},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random forest for dynamic risk prediction of recurrent events: A pseudo-observation approach. <em>BIOSTAT</em>, <em>26</em>(1), kxaf007. (<a href='https://doi.org/10.1093/biostatistics/kxaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events are common in clinical, healthcare, social, and behavioral studies, yet methods for dynamic risk prediction of these events are limited. To overcome some long-standing challenges in analyzing censored recurrent event data, a recent regression analysis framework constructs a censored longitudinal dataset consisting of times to the first recurrent event in multiple pre-specified follow-up windows of length |$ \tau $| (XMT models). Traditional regression models struggle with nonlinear and multiway interactions, with success depending on the skill of the statistical programmer. With a staggering number of potential predictors being generated from genetic, -omic, and electronic health records sources, machine learning approaches such as the random forest regression are growing in popularity, as they can nonparametrically incorporate information from many predictors with nonlinear and multiway interactions involved in prediction. In this article, we (i) develop a random forest approach for dynamically predicting probabilities of remaining event-free during a subsequent |$ \tau $| -duration follow-up period from a reconstructed censored longitudinal data set, (ii) modify the XMT regression approach to predict these same probabilities, subject to the limitations that traditional regression models typically have, and (iii) demonstrate how to incorporate patient-specific history of recurrent events for prediction in settings where this information may be partially missing. We show the increased ability of our random forest algorithm for predicting the probability of remaining event-free over a |$ \tau $| -duration follow-up window when compared to our modified XMT method for prediction in settings where association between predictors and recurrent event outcomes is complex in nature. We also show the importance of incorporating past recurrent event history in prediction algorithms when event times are correlated within a subject. The proposed random forest algorithm is demonstrated using recurrent exacerbation data from the trial of Azithromycin for the Prevention of Exacerbations of Chronic Obstructive Pulmonary Disease.},
  archive      = {J_BIOSTAT},
  author       = {Loe, Abigail and Murray, Susan and Wu, Zhenke},
  doi          = {10.1093/biostatistics/kxaf007},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf007},
  shortjournal = {Biostatistics},
  title        = {Random forest for dynamic risk prediction of recurrent events: A pseudo-observation approach},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized likelihood optimization for censored missing value imputation in proteomics. <em>BIOSTAT</em>, <em>26</em>(1), kxaf006. (<a href='https://doi.org/10.1093/biostatistics/kxaf006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label-free bottom-up proteomics using mass spectrometry and liquid chromatography has long been established as one of the most popular high-throughput analysis workflows for proteome characterization. However, it produces data hindered by complex and heterogeneous missing values, which imputation has long remained problematic. To cope with this, we introduce Pirat, an algorithm that harnesses this challenge using an original likelihood maximization strategy. Notably, it models the instrument limit by learning a global censoring mechanism from the data available. Moreover, it estimates the covariance matrix between enzymatic cleavage products (ie peptides or precursor ions), while offering a natural way to integrate complementary transcriptomic information when multi-omic assays are available. Our benchmarking on several datasets covering a variety of experimental designs (number of samples, acquisition mode, missingness patterns, etc.) and using a variety of metrics (differential analysis ground truth or imputation errors) shows that Pirat outperforms all pre-existing imputation methods. Beyond the interest of Pirat as an imputation tool, these results pinpoint the need for a paradigm change in proteomics imputation, as most pre-existing strategies could be boosted by incorporating similar models to account for the instrument censorship or for the correlation structures, either grounded to the analytical pipeline or arising from a multi-omic approach.},
  archive      = {J_BIOSTAT},
  author       = {Etourneau, Lucas and Fancello, Laura and Wieczorek, Samuel and Varoquaux, Nelle and Burger, Thomas},
  doi          = {10.1093/biostatistics/kxaf006},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf006},
  shortjournal = {Biostatistics},
  title        = {Penalized likelihood optimization for censored missing value imputation in proteomics},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariate-adjusted estimators of diagnostic accuracy in randomized trials. <em>BIOSTAT</em>, <em>26</em>(1), kxaf005. (<a href='https://doi.org/10.1093/biostatistics/kxaf005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials evaluating the diagnostic accuracy of a marker frequently collect information on baseline covariates in addition to information on the marker and the reference standard. However, standard estimators of sensitivity and specificity do not use data on baseline covariates and restrict the analysis to data from participants with a positive reference standard in the intervention arm being evaluated. Covariate-adjusted estimators for marginal treatment effects have been developed and been advocated for by regulatory agencies because they can improve power compared to unadjusted estimators. Despite this, similar covariate-adjusted estimators for marginal sensitivity and specificity have not yet been developed. In this manuscript, we address this gap by developing covariate-adjusted estimators for marginal sensitivity and specificity of a diagnostic test that leverage baseline covariate information. The estimators also use data from all participants, not just participants with a positive reference standard in the intervention arm being evaluated. We derive the asymptotic properties of the estimators and evaluate the finite sample properties of the estimators using simulations and by analyzing data on lung cancer screening.},
  archive      = {J_BIOSTAT},
  author       = {Steingrimsson, Jon A},
  doi          = {10.1093/biostatistics/kxaf005},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf005},
  shortjournal = {Biostatistics},
  title        = {Covariate-adjusted estimators of diagnostic accuracy in randomized trials},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mediation analysis with graph mediator. <em>BIOSTAT</em>, <em>26</em>(1), kxaf004. (<a href='https://doi.org/10.1093/biostatistics/kxaf004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a mediation analysis framework when the mediator is a graph. A Gaussian covariance graph model is assumed for graph presentation. Causal estimands and assumptions are discussed under this presentation. With a covariance matrix as the mediator, a low-rank representation is introduced and parametric mediation models are considered under the structural equation modeling framework. Assuming Gaussian random errors, likelihood-based estimators are introduced to simultaneously identify the low-rank representation and causal parameters. An efficient computational algorithm is proposed and asymptotic properties of the estimators are investigated. Via simulation studies, the performance of the proposed approach is evaluated. Applying to a resting-state fMRI study, a brain network is identified within which functional connectivity mediates the sex difference in the performance of a motor task.},
  archive      = {J_BIOSTAT},
  author       = {Xu, Yixi and Zhao, Yi},
  doi          = {10.1093/biostatistics/kxaf004},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf004},
  shortjournal = {Biostatistics},
  title        = {Mediation analysis with graph mediator},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Within-trial data borrowing for sequential multiple assignment randomized trials. <em>BIOSTAT</em>, <em>26</em>(1), kxaf003. (<a href='https://doi.org/10.1093/biostatistics/kxaf003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sequential Multiple Assignment Randomized Trial (SMART) is a complex trial design that involves randomizing a single participant multiple times in a sequential manner. This results in the branching nature of a SMART, which represents several distinct groups defined by different combinations of treatments, response statuses, etc. A SMART can then answer various scientific questions of interest, eg, the optimal dynamic treatment regime (DTR) for treating a chronic illness, what intervention to offer first, and what intervention to offer to nonresponders (or suboptimal responders). However, the analysis of a SMART can suffer from low precision, as the potentially widely branching structure can lead to reduced sample sizes in some groups of interest. In this paper, we propose a novel analysis method for a SMART in which dynamic borrowing is used to borrow strength across groups with similar expected outcomes, thus providing increased precision for the estimation of the expected outcomes of DTRs. We apply our method to a SMART evaluating various weight loss strategies using a binary endpoint of clinically significant weight loss and show by simulation that our method can improve the precision of the estimated expected outcome of a DTR, aid in the identification of the optimal DTR, and produce a clustering analysis of DTRs embedded in a SMART.},
  archive      = {J_BIOSTAT},
  author       = {Kotalik, Ales and Vock, David M and Sherwood, Nancy E and Hobbs, Brian P and Koopmeiners, Joseph S},
  doi          = {10.1093/biostatistics/kxaf003},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf003},
  shortjournal = {Biostatistics},
  title        = {Within-trial data borrowing for sequential multiple assignment randomized trials},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connectivity regression. <em>BIOSTAT</em>, <em>26</em>(1), kxaf002. (<a href='https://doi.org/10.1093/biostatistics/kxaf002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing how brain functional connectivity networks vary across individuals promises to uncover important scientific questions such as patterns of healthy brain aging through the lifespan or dysconnectivity associated with disease. In this article, we introduce a general regression framework, Connectivity Regression ( ConnReg ), for regressing subject-specific functional connectivity networks on covariates while accounting for within-network inter-edge dependence. ConnReg utilizes a multivariate generalization of Fisher’s transformation to project network objects into an alternative space where Gaussian assumptions are justified and positive semidefinite constraints are automatically satisfied. Penalized multivariate regression is fit in the transformed space to simultaneously induce sparsity in regression coefficients and in covariance elements, which capture within network inter-edge dependence. We use permutation tests to perform multiplicity-adjusted inference to identify covariates associated with connectivity, and stability selection scores to identify network edges that vary with selected covariates. Simulation studies validate the inferential properties of our proposed method and demonstrate how estimating and accounting for within-network inter-edge dependence leads to more efficient estimation, more powerful inference, and more accurate selection of covariate-dependent network edges. We apply ConnReg to the Human Connectome Project Young Adult study, revealing insights into how connectivity varies with language processing covariates and structural brain features.},
  archive      = {J_BIOSTAT},
  author       = {Desai, Neel and Baladandayuthapani, Veera and Shinohara, Russell T and Morris, Jeffrey S},
  doi          = {10.1093/biostatistics/kxaf002},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf002},
  shortjournal = {Biostatistics},
  title        = {Connectivity regression},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable randomized kernel methods for multiview data integration and prediction with application to coronavirus disease. <em>BIOSTAT</em>, <em>26</em>(1), kxaf001. (<a href='https://doi.org/10.1093/biostatistics/kxaf001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is still more to learn about the pathobiology of coronavirus disease (COVID-19) despite 4 years of the pandemic. A multiomics approach offers a comprehensive view of the disease and has the potential to yield deeper insight into the pathogenesis of the disease. Previous multiomics integrative analysis and prediction studies for COVID-19 severity and status have assumed simple relationships (ie linear relationships) between omics data and between omics and COVID-19 outcomes. However, these linear methods do not account for the inherent underlying nonlinear structure associated with these different types of data. The motivation behind this work is to model nonlinear relationships in multiomics and COVID-19 outcomes, and to determine key multidimensional molecules associated with the disease. Toward this goal, we develop scalable randomized kernel methods for jointly associating data from multiple sources or views and simultaneously predicting an outcome or classifying a unit into one of 2 or more classes. We also determine variables or groups of variables that best contribute to the relationships among the views. We use the idea that random Fourier bases can approximate shift-invariant kernel functions to construct nonlinear mappings of each view and we use these mappings and the outcome variable to learn view-independent low-dimensional representations. We demonstrate the effectiveness of the proposed methods through extensive simulations. When the proposed methods were applied to gene expression, metabolomics, proteomics, and lipidomics data pertaining to COVID-19, we identified several molecular signatures for COVID-19 status and severity. Our results agree with previous findings and suggest potential avenues for future research. Our algorithms are implemented in Pytorch and interfaced in R and available at: https://github.com/lasandrall/RandMVLearn .},
  archive      = {J_BIOSTAT},
  author       = {Safo, Sandra E and Lu, Han},
  doi          = {10.1093/biostatistics/kxaf001},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf001},
  shortjournal = {Biostatistics},
  title        = {Scalable randomized kernel methods for multiview data integration and prediction with application to coronavirus disease},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking the power of time-since-infection models: Data augmentation for improved instantaneous reproduction number estimation. <em>BIOSTAT</em>, <em>26</em>(1), kxae054. (<a href='https://doi.org/10.1093/biostatistics/kxae054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-since-infection (TSI) models, which use disease surveillance data to model infectious diseases, have become increasingly popular due to their flexibility and capacity to address complex disease control questions. However, a notable limitation of TSI models is their primary reliance on incidence data. Even when hospitalization data are available, existing TSI models have not been crafted to improve the estimation of disease transmission or to estimate hospitalization-related parameters—metrics crucial for understanding a pandemic and planning hospital resources. Moreover, their dependence on reported infection data makes them vulnerable to variations in data quality. In this study, we advance TSI models by integrating hospitalization data, marking a significant step forward in modeling with TSI models. We introduce hospitalization propensity parameters to jointly model incidence and hospitalization data. We use a composite likelihood function to accommodate complex data structure and a Monte Carlo expectation–maximization algorithm to estimate model parameters. We analyze COVID-19 data to estimate disease transmission, assess risk factor impacts, and calculate hospitalization propensity. Our model improves the accuracy of estimating the instantaneous reproduction number in TSI models, particularly when hospitalization data is of higher quality than incidence data. It enables the estimation of key infectious disease parameters without relying on contact tracing data and provides a foundation for integrating TSI models with other infectious disease models.},
  archive      = {J_BIOSTAT},
  author       = {Shi, Jiasheng and Zhou, Yizhao and Huang, Jing},
  doi          = {10.1093/biostatistics/kxae054},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae054},
  shortjournal = {Biostatistics},
  title        = {Unlocking the power of time-since-infection models: Data augmentation for improved instantaneous reproduction number estimation},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recurrent events modeling based on a reflected brownian motion with application to hypoglycemia. <em>BIOSTAT</em>, <em>26</em>(1), kxae053. (<a href='https://doi.org/10.1093/biostatistics/kxae053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with type 2 diabetes need to closely monitor blood sugar levels as their routine diabetes self-management. Although many treatment agents aim to tightly control blood sugar, hypoglycemia often stands as an adverse event. In practice, patients can observe hypoglycemic events more easily than hyperglycemic events due to the perception of neurogenic symptoms. We propose to model each patient’s observed hypoglycemic event as a lower boundary crossing event for a reflected Brownian motion with an upper reflection barrier. The lower boundary is set by clinical standards. To capture patient heterogeneity and within-patient dependence, covariates and a patient level frailty are incorporated into the volatility and the upper reflection barrier. This framework provides quantification for the underlying glucose level variability, patients heterogeneity, and risk factors’ impact on glucose. We make inferences based on a Bayesian framework using Markov chain Monte Carlo. Two model comparison criteria, the deviance information criterion and the logarithm of the pseudo-marginal likelihood, are used for model selection. The methodology is validated in simulation studies. In analyzing a dataset from the diabetic patients in the DURABLE trial, our model provides adequate fit, generates data similar to the observed data, and offers insights that could be missed by other models.},
  archive      = {J_BIOSTAT},
  author       = {Xie, Yingfa and Fu, Haoda and Huang, Yuan and Pozdnyakov, Vladimir and Yan, Jun},
  doi          = {10.1093/biostatistics/kxae053},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae053},
  shortjournal = {Biostatistics},
  title        = {Recurrent events modeling based on a reflected brownian motion with application to hypoglycemia},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the opioid syndemic in north carolina: A novel approach to modeling and identifying factors. <em>BIOSTAT</em>, <em>26</em>(1), kxae052. (<a href='https://doi.org/10.1093/biostatistics/kxae052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The opioid epidemic is a significant public health challenge in North Carolina, but limited data restrict our understanding of its complexity. Examining trends and relationships among different outcomes believed to reflect opioid misuse provides an alternative perspective to understand the opioid epidemic. We use a Bayesian dynamic spatial factor model to capture the interrelated dynamics within six different county-level outcomes, such as illicit opioid overdose deaths, emergency department visits related to drug overdose, treatment counts for opioid use disorder, patients receiving prescriptions for buprenorphine, and newly diagnosed cases of acute and chronic hepatitis C virus and human immunodeficiency virus. We design the factor model to yield meaningful interactions among predefined subsets of these outcomes, causing a departure from the conventional lower triangular structure in the loadings matrix and leading to familiar identifiability issues. To address this challenge, we propose a novel approach that involves decomposing the loadings matrix within a Markov chain Monte Carlo algorithm, allowing us to estimate the loadings and factors uniquely. As a result, we gain a better understanding of the spatio-temporal dynamics of the opioid epidemic in North Carolina.},
  archive      = {J_BIOSTAT},
  author       = {Murphy, Eva and Kline, David and Egan, Kathleen L and Lancaster, Kathryn E and Miller, William C and Waller, Lance A and Hepler, Staci A},
  doi          = {10.1093/biostatistics/kxae052},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae052},
  shortjournal = {Biostatistics},
  title        = {Understanding the opioid syndemic in north carolina: A novel approach to modeling and identifying factors},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bipartite interference and air pollution transport: Estimating health effects of power plant interventions. <em>BIOSTAT</em>, <em>26</em>(1), kxae051. (<a href='https://doi.org/10.1093/biostatistics/kxae051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating air quality interventions is confronted with the challenge of interference since interventions at a particular pollution source likely impact air quality and health at distant locations, and air quality and health at any given location are likely impacted by interventions at many sources. The structure of interference in this context is dictated by complex atmospheric processes governing how pollution emitted from a particular source is transformed and transported across space and can be cast with a bipartite structure reflecting the two distinct types of units: (i) interventional units on which treatments are applied or withheld to change pollution emissions; and (ii) outcome units on which outcomes of primary interest are measured. We propose new estimands for bipartite causal inference with interference that construe two components of treatment: a “key-associated” (or “individual”) treatment and an “upwind” (or “neighborhood”) treatment. Estimation is carried out using a covariate adjustment approach based on a joint propensity score. A reduced-complexity atmospheric model characterizes the structure of the interference network by modeling the movement of air parcels through time and space. The new methods are deployed to evaluate the effectiveness of installing flue-gas desulfurization scrubbers on 472 coal-burning power plants (the interventional units) in reducing Medicare hospitalizations among 21,577,552 Medicare beneficiaries residing across 25,553 ZIP codes in the United States (the outcome units).},
  archive      = {J_BIOSTAT},
  author       = {Zigler, Corwin and Liu, Vera and Mealli, Fabrizia and Forastiere, Laura},
  doi          = {10.1093/biostatistics/kxae051},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae051},
  shortjournal = {Biostatistics},
  title        = {Bipartite interference and air pollution transport: Estimating health effects of power plant interventions},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Scalable kernel balancing weights in a nationwide observational study of hospital profit status and heart attack outcomes. <em>BIOSTAT</em>, <em>26</em>(1), kxae050. (<a href='https://doi.org/10.1093/biostatistics/kxae050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxae050},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae050},
  shortjournal = {Biostatistics},
  title        = {Correction to: Scalable kernel balancing weights in a nationwide observational study of hospital profit status and heart attack outcomes},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling schizophrenia: A study with generalized functional linear mixed model via the investigation of functional random effects. <em>BIOSTAT</em>, <em>26</em>(1), kxae049. (<a href='https://doi.org/10.1093/biostatistics/kxae049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies have identified attenuated pre-speech activity and speech sound suppression in individuals with Schizophrenia, with similar patterns observed in basic tasks entailing button-pressing to perceive a tone. However, it remains unclear whether these patterns are uniform across individuals or vary from person to person. Motivated by electroencephalographic (EEG) data from a Schizophrenia study, we develop a generalized functional linear mixed model (GFLMM) for repeated measurements by incorporating subject-specific functional random effects associated with multiple functional predictors. To assess the significance of these functional effects, we employ two different multivariate functional principal component analysis methods, which transform the GFLMM into a conventional generalized linear mixed model, thereby facilitating its implementation with standard software. Furthermore, we introduce a cutting-edge testing approach utilizing working responses to detect both subject-specific and predictor-specific functional random effects. Monte Carlo simulation studies demonstrate the effectiveness of our proposed testing method. Application of the proposed methods to the Schizophrenia data reveals significant subject-specific effects of human brain activity in the frontal zone (Fz) and the central zone (Cz), providing valuable insights into the potential variations among individuals, from healthy controls to those diagnosed with Schizophrenia.},
  archive      = {J_BIOSTAT},
  author       = {Rui, Rongxiang and Xiong, Wei and Pan, Jianxin and Tian, Maozai},
  doi          = {10.1093/biostatistics/kxae049},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae049},
  shortjournal = {Biostatistics},
  title        = {Unveiling schizophrenia: A study with generalized functional linear mixed model via the investigation of functional random effects},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian thresholded modeling for integrating brain node and network predictors. <em>BIOSTAT</em>, <em>26</em>(1), kxae048. (<a href='https://doi.org/10.1093/biostatistics/kxae048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Progress in neuroscience has provided unprecedented opportunities to advance our understanding of brain alterations and their correspondence to phenotypic profiles. With data collected from various imaging techniques, studies have integrated different types of information ranging from brain structure, function, or metabolism. More recently, an emerging way to categorize imaging traits is through a metric hierarchy, including localized node-level measurements and interactive network-level metrics. However, limited research has been conducted to integrate these different hierarchies and achieve a better understanding of the neurobiological mechanisms and communications. In this work, we address this literature gap by proposing a Bayesian regression model under both vector-variate and matrix-variate predictors. To characterize the interplay between different predicting components, we propose a set of biologically plausible prior models centered on an innovative joint thresholded prior. This captures the coupling and grouping effect of signal patterns, as well as their spatial contiguity across brain anatomy. By developing a posterior inference, we can identify and quantify the uncertainty of signaling node- and network-level neuromarkers, as well as their predictive mechanism for phenotypic outcomes. Through extensive simulations, we demonstrate that our proposed method outperforms the alternative approaches substantially in both out-of-sample prediction and feature selection. By implementing the model to study children’s general mental abilities, we establish a powerful predictive mechanism based on the identified task contrast traits and resting-state sub-networks.},
  archive      = {J_BIOSTAT},
  author       = {Sun, Zhe and Xu, Wanwan and Li, Tianxi and Kang, Jian and Alanis-Lobato, Gregorio and Zhao, Yize},
  doi          = {10.1093/biostatistics/kxae048},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae048},
  shortjournal = {Biostatistics},
  title        = {Bayesian thresholded modeling for integrating brain node and network predictors},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BAMITA: Bayesian multiple imputation for tensor arrays. <em>BIOSTAT</em>, <em>26</em>(1), kxae047. (<a href='https://doi.org/10.1093/biostatistics/kxae047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data increasingly take the form of a multi-way array, or tensor, in several biomedical domains. Such tensors are often incompletely observed. For example, we are motivated by longitudinal microbiome studies in which several timepoints are missing for several subjects. There is a growing literature on missing data imputation for tensors. However, existing methods give a point estimate for missing values without capturing uncertainty. We propose a multiple imputation approach for tensors in a flexible Bayesian framework, that yields realistic simulated values for missing entries and can propagate uncertainty through subsequent analyses. Our model uses efficient and widely applicable conjugate priors for a CANDECOMP/PARAFAC (CP) factorization, with a separable residual covariance structure. This approach is shown to perform well with respect to both imputation accuracy and uncertainty calibration, for scenarios in which either single entries or entire fibers of the tensor are missing. For two microbiome applications, it is shown to accurately capture uncertainty in the full microbiome profile at missing timepoints and used to infer trends in species diversity for the population. Documented R code to perform our multiple imputation approach is available at https://github.com/lockEF/MultiwayImputation .},
  archive      = {J_BIOSTAT},
  author       = {Jiang, Ziren and Li, Gen and Lock, Eric F},
  doi          = {10.1093/biostatistics/kxae047},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae047},
  shortjournal = {Biostatistics},
  title        = {BAMITA: Bayesian multiple imputation for tensor arrays},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for a difference in means of a single feature after clustering. <em>BIOSTAT</em>, <em>26</em>(1), kxae046. (<a href='https://doi.org/10.1093/biostatistics/kxae046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many applications, it is critical to interpret and validate groups of observations obtained via clustering. A common interpretation and validation approach involves testing differences in feature means between observations in two estimated clusters. In this setting, classical hypothesis tests lead to an inflated Type I error rate. To overcome this problem, we propose a new test for the difference in means in a single feature between a pair of clusters obtained using hierarchical or k -means clustering. The test controls the selective Type I error rate in finite samples and can be efficiently computed. We further illustrate the validity and power of our proposal in simulation and demonstrate its use on single-cell RNA-sequencing data.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Yiqun T and Gao, Lucy L},
  doi          = {10.1093/biostatistics/kxae046},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae046},
  shortjournal = {Biostatistics},
  title        = {Testing for a difference in means of a single feature after clustering},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian subtyping for multi-state brain functional connectome with application on preadolescent brain cognition. <em>BIOSTAT</em>, <em>26</em>(1), kxae045. (<a href='https://doi.org/10.1093/biostatistics/kxae045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Converging evidence indicates that the heterogeneity of cognitive profiles may arise through detectable alternations in brain functional connectivity. Despite an unprecedented opportunity to uncover neurobiological subtypes through clustering or subtyping analyses on multi-state functional connectivity, few existing approaches are applicable to accommodate the network topology and unique biological architecture. To address this issue, we propose an innovative Bayesian nonparametric network-variate clustering analysis to uncover subgroups of individuals with homogeneous brain functional network patterns under multiple cognitive states. In light of the existing neuroscience literature, we assume there are unknown state-specific modular structures within functional connectivity. Concurrently, we identify informative network features essential for defining subtypes. To further facilitate practical use, we develop a computationally efficient variational inference algorithm to approximate posterior inference with satisfactory estimation accuracy. Extensive simulations show the superiority of our method. We apply the method to the Adolescent Brain Cognitive Development (ABCD) study, and identify neurodevelopmental subtypes and brain sub-network phenotypes under each state to signal neurobiological heterogeneity, suggesting promising directions for further exploration and investigation in neuroscience.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Tianqi and Zhao, Hongyu and Tan, Chichun and Constable, Todd and Yip, Sarah and Zhao, Yize},
  doi          = {10.1093/biostatistics/kxae045},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae045},
  shortjournal = {Biostatistics},
  title        = {Bayesian subtyping for multi-state brain functional connectome with application on preadolescent brain cognition},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recoverability of causal effects under presence of missing data: A longitudinal case study. <em>BIOSTAT</em>, <em>26</em>(1), kxae044. (<a href='https://doi.org/10.1093/biostatistics/kxae044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data in multiple variables is a common issue. We investigate the applicability of the framework of graphical models for handling missing data to a complex longitudinal pharmacological study of children with HIV treated with an efavirenz-based regimen as part of the CHAPAS-3 trial. Specifically, we examine whether the causal effects of interest, defined through static interventions on multiple continuous variables, can be recovered (estimated consistently) from the available data only. So far, no general algorithms are available to decide on recoverability, and decisions have to be made on a case-by-case basis. We emphasize the sensitivity of recoverability to even the smallest changes in the graph structure, and present recoverability results for three plausible missingness-directed acyclic graphs (m-DAGs) in the CHAPAS-3 study, informed by clinical knowledge. Furthermore, we propose the concept of a “closed missingness mechanism”: if missing data are generated based on this mechanism, an available case analysis is admissible for consistent estimation of any statistical or causal estimand, even if data are missing not at random. Both simulations and theoretical considerations demonstrate how, in the assumed MNAR setting of our study, a complete or available case analysis can be superior to multiple imputation, and estimation results vary depending on the assumed missingness DAG. Our analyses demonstrate an innovative application of missingness DAGs to complex longitudinal real-world data, while highlighting the sensitivity of the results with respect to the assumed causal model.},
  archive      = {J_BIOSTAT},
  author       = {Holovchak, Anastasiia and McIlleron, Helen and Denti, Paolo and Schomaker, Michael},
  doi          = {10.1093/biostatistics/kxae044},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae044},
  shortjournal = {Biostatistics},
  title        = {Recoverability of causal effects under presence of missing data: A longitudinal case study},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast standard error estimation for joint models of longitudinal and time-to-event data based on stochastic EM algorithms. <em>BIOSTAT</em>, <em>26</em>(1), kxae043. (<a href='https://doi.org/10.1093/biostatistics/kxae043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum likelihood inference can often become computationally intensive when performing joint modeling of longitudinal and time-to-event data, due to the intractable integrals in the joint likelihood function. The computational challenges escalate further when modeling HIV-1 viral load data, owing to the nonlinear trajectories and the presence of left-censored data resulting from the assay’s lower limit of quantification. In this paper, for a joint model comprising a nonlinear mixed-effect model and a Cox Proportional Hazards model, we develop a computationally efficient Stochastic EM (StEM) algorithm for parameter estimation. Furthermore, we propose a novel technique for fast standard error estimation, which directly estimates standard errors from the results of StEM iterations and is broadly applicable to various joint modeling settings, such as those containing generalized linear mixed-effect models, parametric survival models, or joint models with more than two submodels. We evaluate the performance of the proposed methods through simulation studies and apply them to HIV-1 viral load data from six AIDS Clinical Trials Group studies to characterize viral rebound trajectories following the interruption of antiretroviral therapy (ART), accounting for the informative duration of off-ART periods.},
  archive      = {J_BIOSTAT},
  author       = {Yu, Tingting and Wu, Lang and Bosch, Ronald J and Smith, Davey M and Wang, Rui},
  doi          = {10.1093/biostatistics/kxae043},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae043},
  shortjournal = {Biostatistics},
  title        = {Fast standard error estimation for joint models of longitudinal and time-to-event data based on stochastic EM algorithms},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of coarsening an exposure on partial identifiability in instrumental variable settings. <em>BIOSTAT</em>, <em>26</em>(1), kxae042. (<a href='https://doi.org/10.1093/biostatistics/kxae042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In instrumental variable (IV) settings, such as imperfect randomized trials and observational studies with Mendelian randomization, one may encounter a continuous exposure, the causal effect of which is not of true interest. Instead, scientific interest may lie in a coarsened version of this exposure. Although there is a lengthy literature on the impact of coarsening of an exposure with several works focusing specifically on IV settings, all methods proposed in this literature require parametric assumptions. Instead, just as in the standard IV setting, one can consider partial identification via bounds making no parametric assumptions. This was first pointed out in Alexander Balke’s PhD dissertation. We extend and clarify his work and derive novel bounds in several settings, including for a three-level IV, which will most likely be the case in Mendelian randomization. We demonstrate our findings in two real data examples, a randomized trial for peanut allergy in infants and a Mendelian randomization setting investigating the effect of homocysteine on cardiovascular disease.},
  archive      = {J_BIOSTAT},
  author       = {Gabriel, Erin E and Sachs, Michael C and Sjölander, Arvid},
  doi          = {10.1093/biostatistics/kxae042},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae042},
  shortjournal = {Biostatistics},
  title        = {The impact of coarsening an exposure on partial identifiability in instrumental variable settings},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shared parameter modeling of longitudinal data allowing for possibly informative visiting process and terminal event. <em>BIOSTAT</em>, <em>26</em>(1), kxae041. (<a href='https://doi.org/10.1093/biostatistics/kxae041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modeling of longitudinal and time-to-event data, particularly through shared parameter models (SPMs), is a common approach for handling longitudinal marker data with an informative terminal event. A critical but often neglected assumption in this context is that the visiting/observation process is noninformative, depending solely on past marker values and visit times. When this assumption fails, the visiting process becomes informative, resulting potentially to biased SPM estimates. Existing methods generally rely on a conditional independence assumption, positing that the marker model, visiting process, and time-to-event model are independent given shared or correlated random effects. Moreover, they are typically built on an intensity-based visiting process using calendar time. This study introduces a unified approach for jointly modeling a normally distributed marker, the visiting process, and time-to-event data in the form of competing risks. Our model conditions on the history of observed marker values, prior visit times, the marker’s random effects, and possibly a frailty term independent of the random effects. While our approach aligns with the shared-parameter framework, it does not presume conditional independence between the processes. Additionally, the visiting process can be defined on either a gap time scale, via proportional hazard models, or a calendar time scale, via proportional intensity models. Through extensive simulation studies, we assess the performance of our proposed methodology. We demonstrate that disregarding an informative visiting process can yield significantly biased marker estimates. However, misspecification of the visiting process can also lead to biased estimates. The gap time formulation exhibits greater robustness compared to the intensity-based model when the visiting process is misspecified. In general, enriching the visiting process with prior visit history enhances performance. We further apply our methodology to real longitudinal data from HIV, where visit frequency varies substantially among individuals.},
  archive      = {J_BIOSTAT},
  author       = {Thomadakis, Christos and Meligkotsidou, Loukia and Pantazis, Nikos and Touloumi, Giota},
  doi          = {10.1093/biostatistics/kxae041},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae041},
  shortjournal = {Biostatistics},
  title        = {Shared parameter modeling of longitudinal data allowing for possibly informative visiting process and terminal event},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional quantile principal component analysis. <em>BIOSTAT</em>, <em>26</em>(1), kxae040. (<a href='https://doi.org/10.1093/biostatistics/kxae040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces functional quantile principal component analysis (FQPCA), a dimensionality reduction technique that extends the concept of functional principal components analysis (FPCA) to the examination of participant-specific quantiles curves. Our approach borrows strength across participants to estimate patterns in quantiles, and uses participant-level data to estimate loadings on those patterns. As a result, FQPCA is able to capture shifts in the scale and distribution of data that affect participant-level quantile curves, and is also a robust methodology suitable for dealing with outliers, heteroscedastic data or skewed data. The need for such methodology is exemplified by physical activity data collected using wearable devices. Participants often differ in the timing and intensity of physical activity behaviors, and capturing information beyond the participant-level expected value curves produced by FPCA is necessary for a robust quantification of diurnal patterns of activity. We illustrate our methods using accelerometer data from the National Health and Nutrition Examination Survey, and produce participant-level 10%, 50%, and 90% quantile curves over 24 h of activity. The proposed methodology is supported by simulation results, and is available as an R package.},
  archive      = {J_BIOSTAT},
  author       = {Méndez-Civieta, Álvaro and Wei, Ying and Diaz, Keith M. and Goldsmith, Jeff},
  doi          = {10.1093/biostatistics/kxae040},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae040},
  shortjournal = {Biostatistics},
  title        = {Functional quantile principal component analysis},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection processes, transportability, and failure time analysis in life history studies. <em>BIOSTAT</em>, <em>26</em>(1), kxae039. (<a href='https://doi.org/10.1093/biostatistics/kxae039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In life history analysis of data from cohort studies, it is important to address the process by which participants are identified and selected. Many health studies select or enrol individuals based on whether they have experienced certain health related events, for example, disease diagnosis or some complication from disease. Standard methods of analysis rely on assumptions concerning the independence of selection and a person’s prospective life history process, given their prior history. Violations of such assumptions are common, however, and can bias estimation of process features. This has implications for the internal and external validity of cohort studies, and for the transportabilty of results to a population. In this paper, we study failure time analysis by proposing a joint model for the cohort selection process and the failure process of interest. This allows us to address both independence assumptions and the transportability of study results. It is shown that transportability cannot be guaranteed in the absence of auxiliary information on the population. Conditions that produce dependent selection and types of auxiliary data are discussed and illustrated in numerical studies. The proposed framework is applied to a study of the risk of psoriatic arthritis in persons with psoriasis.},
  archive      = {J_BIOSTAT},
  author       = {Cook, Richard J and Lawless, Jerald F},
  doi          = {10.1093/biostatistics/kxae039},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae039},
  shortjournal = {Biostatistics},
  title        = {Selection processes, transportability, and failure time analysis in life history studies},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scalable two-stage bayesian approach accounting for exposure measurement error in environmental epidemiology. <em>BIOSTAT</em>, <em>26</em>(1), kxae038. (<a href='https://doi.org/10.1093/biostatistics/kxae038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accounting for exposure measurement errors has been recognized as a crucial problem in environmental epidemiology for over two decades. Bayesian hierarchical models offer a coherent probabilistic framework for evaluating associations between environmental exposures and health effects, which take into account exposure measurement errors introduced by uncertainty in the estimated exposure as well as spatial misalignment between the exposure and health outcome data. While two-stage Bayesian analyses are often regarded as a good alternative to fully Bayesian analyses when joint estimation is not feasible, there has been minimal research on how to properly propagate uncertainty from the first-stage exposure model to the second-stage health model, especially in the case of a large number of participant locations along with spatially correlated exposures. We propose a scalable two-stage Bayesian approach, called a sparse multivariate normal (sparse MVN) prior approach, based on the Vecchia approximation for assessing associations between exposure and health outcomes in environmental epidemiology. We compare its performance with existing approaches through simulation. Our sparse MVN prior approach shows comparable performance with the fully Bayesian approach, which is a gold standard but is impossible to implement in some cases. We investigate the association between source-specific exposures and pollutant (nitrogen dioxide [NO 2 ])-specific exposures and birth weight of full-term infants born in 2012 in Harris County, Texas, using several approaches, including the newly developed method.},
  archive      = {J_BIOSTAT},
  author       = {Lee, Changwoo J and Symanski, Elaine and Rammah, Amal and Kang, Dong Hun and Hopke, Philip K and Park, Eun Sug},
  doi          = {10.1093/biostatistics/kxae038},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae038},
  shortjournal = {Biostatistics},
  title        = {A scalable two-stage bayesian approach accounting for exposure measurement error in environmental epidemiology},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speeding up interval estimation for r2-based mediation effect of high-dimensional mediators via cross-fitting. <em>BIOSTAT</em>, <em>26</em>(1), kxae037. (<a href='https://doi.org/10.1093/biostatistics/kxae037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is a useful tool in investigating how molecular phenotypes such as gene expression mediate the effect of exposure on health outcomes. However, commonly used mean-based total mediation effect measures may suffer from cancellation of component-wise mediation effects in opposite directions in the presence of high-dimensional omics mediators. To overcome this limitation, we recently proposed a variance-based R-squared total mediation effect measure that relies on the computationally intensive nonparametric bootstrap for confidence interval estimation. In the work described herein, we formulated a more efficient two-stage, cross-fitted estimation procedure for the R 2 measure. To avoid potential bias, we performed iterative Sure Independence Screening (iSIS) in two subsamples to exclude the non-mediators, followed by ordinary least squares regressions for the variance estimation. We then constructed confidence intervals based on the newly derived closed-form asymptotic distribution of the R 2 measure. Extensive simulation studies demonstrated that this proposed procedure is much more computationally efficient than the resampling-based method, with comparable coverage probability. Furthermore, when applied to the Framingham Heart Study, the proposed method replicated the established finding of gene expression mediating age-related variation in systolic blood pressure and identified the role of gene expression profiles in the relationship between sex and high-density lipoprotein cholesterol level. The proposed estimation procedure is implemented in R package CFR2M .},
  archive      = {J_BIOSTAT},
  author       = {Xu, Zhichao and Li, Chunlin and Chi, Sunyi and Yang, Tianzhong and Wei, Peng},
  doi          = {10.1093/biostatistics/kxae037},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae037},
  shortjournal = {Biostatistics},
  title        = {Speeding up interval estimation for r2-based mediation effect of high-dimensional mediators via cross-fitting},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic and concordance-assisted learning for risk stratification with application to alzheimer’s disease. <em>BIOSTAT</em>, <em>26</em>(1), kxae036. (<a href='https://doi.org/10.1093/biostatistics/kxae036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic prediction models capable of retaining accuracy by evolving over time could play a significant role for monitoring disease progression in clinical practice. In biomedical studies with long-term follow up, participants are often monitored through periodic clinical visits with repeat measurements until an occurrence of the event of interest (e.g. disease onset) or the study end. Acknowledging the dynamic nature of disease risk and clinical information contained in the longitudinal markers, we propose an innovative concordance-assisted learning algorithm to derive a real-time risk stratification score. The proposed approach bypasses the need to fit regression models, such as joint models of the longitudinal markers and time-to-event outcome, and hence enjoys the desirable property of model robustness. Simulation studies confirmed that the proposed method has satisfactory performance in dynamically monitoring the risk of developing disease and differentiating high-risk and low-risk population over time. We apply the proposed method to the Alzheimer’s Disease Neuroimaging Initiative data and develop a dynamic risk score of Alzheimer’s Disease for patients with mild cognitive impairment using multiple longitudinal markers and baseline prognostic factors.},
  archive      = {J_BIOSTAT},
  author       = {Li, Wen and Li, Ruosha and Feng, Ziding and Ning, Jing and For the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1093/biostatistics/kxae036},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae036},
  shortjournal = {Biostatistics},
  title        = {Dynamic and concordance-assisted learning for risk stratification with application to alzheimer’s disease},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the addams family of discrete frailty distributions for modeling multivariate case i interval-censored data. <em>BIOSTAT</em>, <em>26</em>(1), kxae035. (<a href='https://doi.org/10.1093/biostatistics/kxae035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random effect models for time-to-event data, also known as frailty models, provide a conceptually appealing way of quantifying association between survival times and of representing heterogeneities resulting from factors which may be difficult or impossible to measure. In the literature, the random effect is usually assumed to have a continuous distribution. However, in some areas of application, discrete frailty distributions may be more appropriate. The present paper is about the implementation and interpretation of the Addams family of discrete frailty distributions. We propose methods of estimation for this family of densities in the context of shared frailty models for the hazard rates for case I interval-censored data. Our optimization framework allows for stratification of random effect distributions by covariates. We highlight interpretational advantages of the Addams family of discrete frailty distributions and the K -point distribution as compared to other frailty distributions. A unique feature of the Addams family and the K -point distribution is that the support of the frailty distribution depends on its parameters. This feature is best exploited by imposing a model on the distributional parameters, resulting in a model with non-homogeneous covariate effects that can be analyzed using standard measures such as the hazard ratio. Our methods are illustrated with applications to multivariate case I interval-censored infection data.},
  archive      = {J_BIOSTAT},
  author       = {Bardo, Maximilian and Hens, Niel and Unkel, Steffen},
  doi          = {10.1093/biostatistics/kxae035},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae035},
  shortjournal = {Biostatistics},
  title        = {On the addams family of discrete frailty distributions for modeling multivariate case i interval-censored data},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian pharmacokinetics integrated phase I–II design to optimize dose-schedule regimes. <em>BIOSTAT</em>, <em>26</em>(1), kxae034. (<a href='https://doi.org/10.1093/biostatistics/kxae034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The schedule of administering a drug has profound impact on the toxicity and efficacy profiles of the drug through changing its pharmacokinetics (PK). PK is an innate and indispensable component of the dose-schedule optimization. Motivated by this, we propose a Bayesian PK integrated dose-schedule finding (PKIDS) design to identify the optimal dose-schedule regime by integrating PK, toxicity, and efficacy data. Based on the causal pathway that dose and schedule affect PK, which in turn affects efficacy and toxicity, we jointly model the three endpoints by first specifying a Bayesian hierarchical model for the marginal distribution of the longitudinal dose-concentration process. Conditional on the drug concentration in plasma, we jointly model toxicity and efficacy as a function of the concentration. We quantify the risk-benefit of regimes using utility—continuously updating the estimates of PK, toxicity, and efficacy based on interim data—and make adaptive decisions to assign new patients to appropriate dose-schedule regimes via adaptive randomization. The simulation study shows that the PKIDS design has desirable operating characteristics.},
  archive      = {J_BIOSTAT},
  author       = {Lu, Mengyi and Yuan, Ying and Liu, Suyu},
  doi          = {10.1093/biostatistics/kxae034},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae034},
  shortjournal = {Biostatistics},
  title        = {A bayesian pharmacokinetics integrated phase I–II design to optimize dose-schedule regimes},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMM for discovering decision-making dynamics using reinforcement learning experiments. <em>BIOSTAT</em>, <em>26</em>(1), kxae033. (<a href='https://doi.org/10.1093/biostatistics/kxae033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major depressive disorder (MDD), a leading cause of years of life lived with disability, presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes, such as gains or losses in the laboratory. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing (e.g. reward sensitivity) to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task within the Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we propose a novel RL-HMM (hidden Markov model) framework for analyzing reward-based decision-making. Our model accommodates decision-making strategy switching between two distinct approaches under an HMM: subjects making decisions based on the RL model or opting for random choices. We account for continuous RL state space and allow time-varying transition probabilities in the HMM. We introduce a computationally efficient Expectation-maximization (EM) algorithm for parameter estimation and use a nonparametric bootstrap for inference. Extensive simulation studies validate the finite-sample performance of our method. We apply our approach to the EMBARC study to show that MDD patients are less engaged in RL compared to the healthy controls, and engagement is associated with brain activities in the negative affect circuitry during an emotional conflict task.},
  archive      = {J_BIOSTAT},
  author       = {Guo, Xingche and Zeng, Donglin and Wang, Yuanjia},
  doi          = {10.1093/biostatistics/kxae033},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae033},
  shortjournal = {Biostatistics},
  title        = {HMM for discovering decision-making dynamics using reinforcement learning experiments},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pooling controls from nested case–control studies with the proportional risks model. <em>BIOSTAT</em>, <em>26</em>(1), kxae032. (<a href='https://doi.org/10.1093/biostatistics/kxae032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard approach to regression modeling for cause-specific hazards with prospective competing risks data specifies separate models for each failure type. An alternative proposed by Lunn and McNeil (1995) assumes the cause-specific hazards are proportional across causes. This may be more efficient than the standard approach, and allows the comparison of covariate effects across causes. In this paper, we extend Lunn and McNeil (1995) to nested case–control studies, accommodating scenarios with additional matching and non-proportionality. We also consider the case where data for different causes are obtained from different studies conducted in the same cohort. It is demonstrated that while only modest gains in efficiency are possible in full cohort analyses, substantial gains may be attained in nested case–control analyses for failure types that are relatively rare. Extensive simulation studies are conducted and real data analyses are provided using the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial (PLCO) study.},
  archive      = {J_BIOSTAT},
  author       = {Chang, Yen and Ivanova, Anastasia and Albanes, Demetrius and Fine, Jason P and Shin, Yei Eun},
  doi          = {10.1093/biostatistics/kxae032},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae032},
  shortjournal = {Biostatistics},
  title        = {Pooling controls from nested case–control studies with the proportional risks model},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exposure proximal immune correlates analysis. <em>BIOSTAT</em>, <em>26</em>(1), kxae031. (<a href='https://doi.org/10.1093/biostatistics/kxae031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immune response decays over time, and vaccine-induced protection often wanes. Understanding how vaccine efficacy changes over time is critical to guiding the development and application of vaccines in preventing infectious diseases. The objective of this article is to develop statistical methods that assess the effect of decaying immune responses on the risk of disease and on vaccine efficacy, within the context of Cox regression with sparse sampling of immune responses, in a baseline-naive population. We aim to further disentangle the various aspects of the time-varying vaccine effect, whether direct on disease or mediated through immune responses. Based on time-to-event data from a vaccine efficacy trial and sparse sampling of longitudinal immune responses, we propose a weighted estimated induced likelihood approach that models the longitudinal immune response trajectory and the time to event separately. This approach assesses the effects of the decaying immune response, the peak immune response, and/or the waning vaccine effect on the risk of disease. The proposed method is applicable not only to standard randomized trial designs but also to augmented vaccine trial designs that re-vaccinate uninfected placebo recipients at the end of the standard trial period. We conducted simulation studies to evaluate the performance of our method and applied the method to analyze immune correlates from a phase III SARS-CoV-2 vaccine trial.},
  archive      = {J_BIOSTAT},
  author       = {Huang, Ying and Follmann, Dean},
  doi          = {10.1093/biostatistics/kxae031},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae031},
  shortjournal = {Biostatistics},
  title        = {Exposure proximal immune correlates analysis},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive gaussian markov random fields for child mortality estimation. <em>BIOSTAT</em>, <em>26</em>(1), kxae030. (<a href='https://doi.org/10.1093/biostatistics/kxae030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The under-5 mortality rate (U5MR), a critical health indicator, is typically estimated from household surveys in lower and middle income countries. Spatio-temporal disaggregation of household survey data can lead to highly variable estimates of U5MR, necessitating the usage of smoothing models which borrow information across space and time. The assumptions of common smoothing models may be unrealistic when certain time periods or regions are expected to have shocks in mortality relative to their neighbors, which can lead to oversmoothing of U5MR estimates. In this paper, we develop a spatial and temporal smoothing approach based on Gaussian Markov random field models which incorporate knowledge of these expected shocks in mortality. We demonstrate the potential for these models to improve upon alternatives not incorporating knowledge of expected shocks in a simulation study. We apply these models to estimate U5MR in Rwanda at the national level from 1985 to 2019, a time period which includes the Rwandan civil war and genocide.},
  archive      = {J_BIOSTAT},
  author       = {Aleshin-Guendel, Serge and Wakefield, Jon},
  doi          = {10.1093/biostatistics/kxae030},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae030},
  shortjournal = {Biostatistics},
  title        = {Adaptive gaussian markov random fields for child mortality estimation},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction. <em>BIOSTAT</em>, <em>26</em>(1), kxae029. (<a href='https://doi.org/10.1093/biostatistics/kxae029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxae029},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae029},
  shortjournal = {Biostatistics},
  title        = {Correction},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating prior information in gene expression network-based cancer heterogeneity analysis. <em>BIOSTAT</em>, <em>26</em>(1), kxae028. (<a href='https://doi.org/10.1093/biostatistics/kxae028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is molecularly heterogeneous, with seemingly similar patients having different molecular landscapes and accordingly different clinical behaviors. In recent studies, gene expression networks have been shown as more effective/informative for cancer heterogeneity analysis than some simpler measures. Gene interconnections can be classified as “direct” and “indirect,” where the latter can be caused by shared genomic regulators (such as transcription factors, microRNAs, and other regulatory molecules) and other mechanisms. It has been suggested that incorporating the regulators of gene expressions in network analysis and focusing on the direct interconnections can lead to a deeper understanding of the more essential gene interconnections. Such analysis can be seriously challenged by the large number of parameters (jointly caused by network analysis, incorporation of regulators, and heterogeneity) and often weak signals. To effectively tackle this problem, we propose incorporating prior information contained in the published literature. A key challenge is that such prior information can be partial or even wrong. We develop a two-step procedure that can flexibly accommodate different levels of prior information quality. Simulation demonstrates the effectiveness of the proposed approach and its superiority over relevant competitors. In the analysis of a breast cancer dataset, findings different from the alternatives are made, and the identified sample subgroups have important clinical differences.},
  archive      = {J_BIOSTAT},
  author       = {Li, Rong and Xu, Shaodong and Li, Yang and Tang, Zuojian and Feng, Di and Cai, James and Ma, Shuangge},
  doi          = {10.1093/biostatistics/kxae028},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae028},
  shortjournal = {Biostatistics},
  title        = {Incorporating prior information in gene expression network-based cancer heterogeneity analysis},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Direct estimation and inference of higher-level correlations from lower-level measurements with applications in gene-pathway and proteomics studies. <em>BIOSTAT</em>, <em>26</em>(1), kxae027. (<a href='https://doi.org/10.1093/biostatistics/kxae027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the challenge of estimating correlations between higher-level biological variables (e.g. proteins and gene pathways) when only lower-level measurements are directly observed (e.g. peptides and individual genes). Existing methods typically aggregate lower-level data into higher-level variables and then estimate correlations based on the aggregated data. However, different data aggregation methods can yield varying correlation estimates as they target different higher-level quantities. Our solution is a latent factor model that directly estimates these higher-level correlations from lower-level data without the need for data aggregation. We further introduce a shrinkage estimator to ensure the positive definiteness and improve the accuracy of the estimated correlation matrix. Furthermore, we establish the asymptotic normality of our estimator, enabling efficient computation of P -values for the identification of significant correlations. The effectiveness of our approach is demonstrated through comprehensive simulations and the analysis of proteomics and gene expression datasets. We develop the R package highcor for implementing our method.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Yue and Shi, Haoran},
  doi          = {10.1093/biostatistics/kxae027},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae027},
  shortjournal = {Biostatistics},
  title        = {Direct estimation and inference of higher-level correlations from lower-level measurements with applications in gene-pathway and proteomics studies},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regression and alignment for functional data and network topology. <em>BIOSTAT</em>, <em>26</em>(1), kxae026. (<a href='https://doi.org/10.1093/biostatistics/kxae026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the brain, functional connections form a network whose topological organization can be described by graph-theoretic network diagnostics. These include characterizations of the community structure, such as modularity and participation coefficient, which have been shown to change over the course of childhood and adolescence. To investigate if such changes in the functional network are associated with changes in cognitive performance during development, network studies often rely on an arbitrary choice of preprocessing parameters, in particular the proportional threshold of network edges. Because the choice of parameter can impact the value of the network diagnostic, and therefore downstream conclusions, we propose to circumvent that choice by conceptualizing the network diagnostic as a function of the parameter. As opposed to a single value, a network diagnostic curve describes the connectome topology at multiple scales—from the sparsest group of the strongest edges to the entire edge set. To relate these curves to executive function and other covariates, we use scalar-on-function regression, which is more flexible than previous functional data-based models used in network neuroscience. We then consider how systematic differences between networks can manifest in misalignment of diagnostic curves, and consequently propose a supervised curve alignment method that incorporates auxiliary information from other variables. Our algorithm performs both functional regression and alignment via an iterative, penalized, and nonlinear likelihood optimization. The illustrated method has the potential to improve the interpretability and generalizability of neuroscience studies where the goal is to study heterogeneity among a mixture of function- and scalar-valued measures.},
  archive      = {J_BIOSTAT},
  author       = {Tu, Danni and Wrobel, Julia and Satterthwaite, Theodore D and Goldsmith, Jeff and Gur, Ruben C and Gur, Raquel E and Gertheiss, Jan and Bassett, Dani S and Shinohara, Russell T},
  doi          = {10.1093/biostatistics/kxae026},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae026},
  shortjournal = {Biostatistics},
  title        = {Regression and alignment for functional data and network topology},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating causal effects for binary outcomes using per-decision inverse probability weighting. <em>BIOSTAT</em>, <em>26</em>(1), kxae025. (<a href='https://doi.org/10.1093/biostatistics/kxae025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-randomized trials are commonly conducted for optimizing mobile health interventions such as push notifications for behavior change. In analyzing such trials, causal excursion effects are often of primary interest, and their estimation typically involves inverse probability weighting (IPW). However, in a micro-randomized trial, additional treatments can often occur during the time window over which an outcome is defined, and this can greatly inflate the variance of the causal effect estimator because IPW would involve a product of numerous weights. To reduce variance and improve estimation efficiency, we propose two new estimators using a modified version of IPW, which we call “per-decision IPW.” The second estimator further improves efficiency using the projection idea from the semiparametric efficiency theory. These estimators are applicable when the outcome is binary and can be expressed as the maximum of a series of sub-outcomes defined over sub-intervals of time. We establish the estimators’ consistency and asymptotic normality. Through simulation studies and real data applications, we demonstrate substantial efficiency improvement of the proposed estimator over existing estimators. The new estimators can be used to improve the precision of primary and secondary analyses for micro-randomized trials with binary outcomes.},
  archive      = {J_BIOSTAT},
  author       = {Bao, Yihan and Bell, Lauren and Williamson, Elizabeth and Garnett, Claire and Qian, Tianchen},
  doi          = {10.1093/biostatistics/kxae025},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae025},
  shortjournal = {Biostatistics},
  title        = {Estimating causal effects for binary outcomes using per-decision inverse probability weighting},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian estimation of covariate assisted principal regression for brain functional connectivity. <em>BIOSTAT</em>, <em>26</em>(1), kxae023. (<a href='https://doi.org/10.1093/biostatistics/kxae023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Bayesian reformulation of covariate-assisted principal regression for covariance matrix outcomes to identify low-dimensional components in the covariance associated with covariates. By introducing a geometric approach to the covariance matrices and leveraging Euclidean geometry, we estimate dimension reduction parameters and model covariance heterogeneity based on covariates. This method enables joint estimation and uncertainty quantification of relevant model parameters associated with heteroscedasticity. We demonstrate our approach through simulation studies and apply it to analyze associations between covariates and brain functional connectivity using data from the Human Connectome Project.},
  archive      = {J_BIOSTAT},
  author       = {Park, Hyung G},
  doi          = {10.1093/biostatistics/kxae023},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae023},
  shortjournal = {Biostatistics},
  title        = {Bayesian estimation of covariate assisted principal regression for brain functional connectivity},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A modeling framework for detecting and leveraging node-level information in bayesian network inference. <em>BIOSTAT</em>, <em>26</em>(1), kxae021. (<a href='https://doi.org/10.1093/biostatistics/kxae021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian graphical models are powerful tools to infer complex relationships in high dimension, yet are often fraught with computational and statistical challenges. If exploited in a principled way, the increasing information collected alongside the data of primary interest constitutes an opportunity to mitigate these difficulties by guiding the detection of dependence structures. For instance, gene network inference may be informed by the use of publicly available summary statistics on the regulation of genes by genetic variants. Here we present a novel Gaussian graphical modeling framework to identify and leverage information on the centrality of nodes in conditional independence graphs. Specifically, we consider a fully joint hierarchical model to simultaneously infer (i) sparse precision matrices and (ii) the relevance of node-level information for uncovering the sought-after network structure. We encode such information as candidate auxiliary variables using a spike-and-slab submodel on the propensity of nodes to be hubs, which allows hypothesis-free selection and interpretation of a sparse subset of relevant variables. As efficient exploration of large posterior spaces is needed for real-world applications, we develop a variational expectation conditional maximization algorithm that scales inference to hundreds of samples, nodes and auxiliary variables. We illustrate and exploit the advantages of our approach in simulations and in a gene network study which identifies hub genes involved in biological pathways relevant to immune-mediated diseases.},
  archive      = {J_BIOSTAT},
  author       = {Xi, Xiaoyue and Ruffieux, Hélène},
  doi          = {10.1093/biostatistics/kxae021},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae021},
  shortjournal = {Biostatistics},
  title        = {A modeling framework for detecting and leveraging node-level information in bayesian network inference},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based multifacet clustering with high-dimensional omics applications. <em>BIOSTAT</em>, <em>26</em>(1), kxae020. (<a href='https://doi.org/10.1093/biostatistics/kxae020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional omics data often contain intricate and multifaceted information, resulting in the coexistence of multiple plausible sample partitions based on different subsets of selected features. Conventional clustering methods typically yield only one clustering solution, limiting their capacity to fully capture all facets of cluster structures in high-dimensional data. To address this challenge, we propose a model-based multifacet clustering (MFClust) method based on a mixture of Gaussian mixture models, where the former mixture achieves facet assignment for gene features and the latter mixture determines cluster assignment of samples. We demonstrate superior facet and cluster assignment accuracy of MFClust through simulation studies. The proposed method is applied to three transcriptomic applications from postmortem brain and lung disease studies. The result captures multifacet clustering structures associated with critical clinical variables and provides intriguing biological insights for further hypothesis generation and discovery.},
  archive      = {J_BIOSTAT},
  author       = {Zong, Wei and Li, Danyang and Seney, Marianne L and Mcclung, Colleen A and Tseng, George C},
  doi          = {10.1093/biostatistics/kxae020},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae020},
  shortjournal = {Biostatistics},
  title        = {Model-based multifacet clustering with high-dimensional omics applications},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A marginal structural model for normal tissue complication probability. <em>BIOSTAT</em>, <em>26</em>(1), kxae019. (<a href='https://doi.org/10.1093/biostatistics/kxae019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of radiation therapy for cancer is to deliver prescribed radiation dose to the tumor while minimizing dose to the surrounding healthy tissues. To evaluate treatment plans, the dose distribution to healthy organs is commonly summarized as dose-volume histograms (DVHs). Normal tissue complication probability (NTCP) modeling has centered around making patient-level risk predictions with features extracted from the DVHs, but few have considered adapting a causal framework to evaluate the safety of alternative treatment plans. We propose causal estimands for NTCP based on deterministic and stochastic interventions, as well as propose estimators based on marginal structural models that impose bivariable monotonicity between dose, volume, and toxicity risk. The properties of these estimators are studied through simulations, and their use is illustrated in the context of radiotherapy treatment of anal canal cancer patients.},
  archive      = {J_BIOSTAT},
  author       = {Tang, Thai-Son and Liu, Zhihui and Hosni, Ali and Kim, John and Saarela, Olli},
  doi          = {10.1093/biostatistics/kxae019},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae019},
  shortjournal = {Biostatistics},
  title        = {A marginal structural model for normal tissue complication probability},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic EM algorithm for partially observed stochastic epidemics with individual heterogeneity. <em>BIOSTAT</em>, <em>26</em>(1), kxae018. (<a href='https://doi.org/10.1093/biostatistics/kxae018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a stochastic epidemic model progressing over dynamic networks, where infection rates are heterogeneous and may vary with individual-level covariates. The joint dynamics are modeled as a continuous-time Markov chain such that disease transmission is constrained by the contact network structure, and network evolution is in turn influenced by individual disease statuses. To accommodate partial epidemic observations commonly seen in real-world data, we propose a stochastic EM algorithm for inference, introducing key innovations that include efficient conditional samplers for imputing missing infection and recovery times which respect the dynamic contact network. Experiments on both synthetic and real datasets demonstrate that our inference method can accurately and efficiently recover model parameters and provide valuable insight at the presence of unobserved disease episodes in epidemic data.},
  archive      = {J_BIOSTAT},
  author       = {Bu, Fan and Aiello, Allison E and Volfovsky, Alexander and Xu, Jason},
  doi          = {10.1093/biostatistics/kxae018},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae018},
  shortjournal = {Biostatistics},
  title        = {Stochastic EM algorithm for partially observed stochastic epidemics with individual heterogeneity},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous clustering and estimation of networks in multiple graphical models. <em>BIOSTAT</em>, <em>26</em>(1), kxae015. (<a href='https://doi.org/10.1093/biostatistics/kxae015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models are widely used to study the dependence structure among variables. When samples are obtained from multiple conditions or populations, joint analysis of multiple graphical models are desired due to their capacity to borrow strength across populations. Nonetheless, existing methods often overlook the varying levels of similarity between populations, leading to unsatisfactory results. Moreover, in many applications, learning the population-level clustering structure itself is of particular interest. In this article, we develop a novel method, called S imultaneous C lustering and E stimation of N etworks via T ensor decomposition (SCENT), that simultaneously clusters and estimates graphical models from multiple populations. Precision matrices from different populations are uniquely organized as a three-way tensor array, and a low-rank sparse model is proposed for joint population clustering and network estimation. We develop a penalized likelihood method and an augmented Lagrangian algorithm for model fitting. We also establish the clustering accuracy and norm consistency of the estimated precision matrices. We demonstrate the efficacy of the proposed method with comprehensive simulation studies. The application to the Genotype-Tissue Expression multi-tissue gene expression data provides important insights into tissue clustering and gene coexpression patterns in multiple brain tissues.},
  archive      = {J_BIOSTAT},
  author       = {Li, Gen and Wang, Miaoyan},
  doi          = {10.1093/biostatistics/kxae015},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae015},
  shortjournal = {Biostatistics},
  title        = {Simultaneous clustering and estimation of networks in multiple graphical models},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A joint normal-ordinal (probit) model for ordinal and continuous longitudinal data. <em>BIOSTAT</em>, <em>26</em>(1), kxae014. (<a href='https://doi.org/10.1093/biostatistics/kxae014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, continuous and ordinal longitudinal variables are frequently encountered. In many of these studies it is of interest to estimate the effect of one of these longitudinal variables on the other. Time-dependent covariates have, however, several limitations; they can, for example, not be included when the data is not collected at fixed intervals. The issues can be circumvented by implementing joint models, where two or more longitudinal variables are treated as a response and modeled with a correlated random effect. Next, by conditioning on these response(s), we can study the effect of one or more longitudinal variables on another. We propose a normal-ordinal(probit) joint model. First, we derive closed-form formulas to estimate the model-based correlations between the responses on their original scale. In addition, we derive the marginal model, where the interpretation is no longer conditional on the random effects. As a consequence, we can make predictions for a subvector of one response conditional on the other response and potentially a subvector of the history of the response. Next, we extend the approach to a high-dimensional case with more than two ordinal and/or continuous longitudinal variables. The methodology is applied to a case study where, among others, a longitudinal ordinal response is predicted with a longitudinal continuous variable.},
  archive      = {J_BIOSTAT},
  author       = {Delporte, Margaux and Molenberghs, Geert and Fieuws, Steffen and Verbeke, Geert},
  doi          = {10.1093/biostatistics/kxae014},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae014},
  shortjournal = {Biostatistics},
  title        = {A joint normal-ordinal (probit) model for ordinal and continuous longitudinal data},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semiparametric gaussian mixture model for chest CT-based 3D blood vessel reconstruction. <em>BIOSTAT</em>, <em>26</em>(1), kxae013. (<a href='https://doi.org/10.1093/biostatistics/kxae013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) has been a powerful diagnostic tool since its emergence in the 1970s. Using CT data, 3D structures of human internal organs and tissues, such as blood vessels, can be reconstructed using professional software. This 3D reconstruction is crucial for surgical operations and can serve as a vivid medical teaching example. However, traditional 3D reconstruction heavily relies on manual operations, which are time-consuming, subjective, and require substantial experience. To address this problem, we develop a novel semiparametric Gaussian mixture model tailored for the 3D reconstruction of blood vessels. This model extends the classical Gaussian mixture model by enabling nonparametric variations in the component-wise parameters of interest according to voxel positions. We develop a kernel-based expectation–maximization algorithm for estimating the model parameters, accompanied by a supporting asymptotic theory. Furthermore, we propose a novel regression method for optimal bandwidth selection. Compared to the conventional cross-validation-based (CV) method, the regression method outperforms the CV method in terms of computational and statistical efficiency. In application, this methodology facilitates the fully automated reconstruction of 3D blood vessel structures with remarkable accuracy.},
  archive      = {J_BIOSTAT},
  author       = {Zeng, Qianhan and Zhou, Jing and Ji, Ying and Wang, Hansheng},
  doi          = {10.1093/biostatistics/kxae013},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae013},
  shortjournal = {Biostatistics},
  title        = {A semiparametric gaussian mixture model for chest CT-based 3D blood vessel reconstruction},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="comjnl">COMJNL - 18</h2>
<ul>
<li><details>
<summary>
(2025). Small object detection in remote sensing images through multi-scale feature fusion. <em>COMJNL</em>, <em>68</em>(9), 1329-1340. (<a href='https://doi.org/10.1093/comjnl/bxaf040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the challenges posed by background noise and the limited information available for small targets in remote sensing images, the detection performance for such targets remains unsatisfactory. To address these issues and enhance detection accuracy, we propose an improved algorithm based on RTDETR, named Adaptive Selective Transformer. Firstly, in the feature extraction network, we introduce an adaptive convolutional feature enhancement module to improve the multi-scale feature extraction capability in low-resolution remote sensing images. Secondly, we design a multi-scale enhancement structure to extract detailed information from small target images through enhanced multi-scale representation learning, thereby generating target features with stronger discriminative power. Finally, we propose a hierarchical frequency attention mechanism to achieve localized enhancement of contextual awareness, effectively capturing high-frequency local feature information of small targets. Experimental results demonstrate that the Adaptive Selective Transformer achieves superior small target detection performance, validating the effectiveness of our modifications to the original RTDETR model.},
  archive      = {J_COMJNL},
  author       = {Li, Sumin and Lin, Jinhua and Gang, Yijin and Pan, Xiuqin},
  doi          = {10.1093/comjnl/bxaf040},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1329-1340},
  shortjournal = {Comput. J.},
  title        = {Small object detection in remote sensing images through multi-scale feature fusion},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confluence: Improving network monitoring accuracy on multi-pipeline data plane. <em>COMJNL</em>, <em>68</em>(9), 1315-1328. (<a href='https://doi.org/10.1093/comjnl/bxaf039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A sketch-based method is promising for traffic monitoring in data center networks. Existing data plane programming model (e.g. P4) assumes target switch as one single pipeline, while state-of-the-art programmable switches actually contain multiple independent pipelines. The status quo approach for deploying a sketch-based measurement application on a multi-pipeline switch is to deploy a sketch instance in each pipeline individually. However, under multi-path routing, such a naive approach leads to poor accuracy. To overcome this problem, in this paper, we present Confluence , a sketch-based network measurement system for multi-pipeline switches. For monitoring network flows that have packets arrived in bursts and spread over multiple pipelines, Confluence introduces novel data structures to collect short-term traffic statistics in ingress pipelines, and converge the measurement data to egress pipelines. Confluence is carefully designed under the switch hardware constraints, and in particular, to resolve the circular dependency in querying and updating a flow’s measurement data from sketch buckets, we propose a novel algorithm and theoretically prove its effectiveness. Both theoretical analysis and experiments driven by real-world traffic traces show that Confluence delivers higher measurement accuracies than existing solutions, especially in the critical task of detecting heavy hitters. Assessment on hardware switch suggests that Confluence is practical for real-world deployment.},
  archive      = {J_COMJNL},
  author       = {Wang, Cenman and Tian, Ye and Wu, Yiwen and Zhang, Xinming},
  doi          = {10.1093/comjnl/bxaf039},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1315-1328},
  shortjournal = {Comput. J.},
  title        = {Confluence: Improving network monitoring accuracy on multi-pipeline data plane},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal traffic flow forecasting based on second-order continuous graph neural network. <em>COMJNL</em>, <em>68</em>(9), 1300-1314. (<a href='https://doi.org/10.1093/comjnl/bxaf038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal forecasting has wide applications across various domains, particularly in intelligent transportation systems, where it plays a crucial role. Traffic flow prediction, a typical spatio-temporal forecasting task, involves complex dependencies across both time and space dimensions. Current research predominantly relies on graph neural networks (GNNs) for modeling. However, deep GNN architectures often face the issue of over-smoothing. To address this challenge, recent studies have explored integrating residual connections or neural ordinary differential equations (ODEs) with GNNs. Nonetheless, existing graph ODE methods have limitations in initializing latent feature representations for time series data and capturing higher order spatio-temporal dependencies. Additionally, they struggle to extract multi-scale temporal dependencies. In this paper, we propose a framework called the Multiple Second-order Continuous Graph Neural Network. The framework utilizes a second-order continuous GNN, and experiments on four real-world datasets demonstrate that it outperforms mainstream baseline models, thereby confirming the effectiveness of the proposed method.},
  archive      = {J_COMJNL},
  author       = {Ma, Zhaobin and Lv, Zhiqiang and Xu, Zhihao and Ye, Rongkun and Li, Jianbo},
  doi          = {10.1093/comjnl/bxaf038},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1300-1314},
  shortjournal = {Comput. J.},
  title        = {Spatio-temporal traffic flow forecasting based on second-order continuous graph neural network},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing privacy and model performance in federated learning through contract-based data trading. <em>COMJNL</em>, <em>68</em>(9), 1285-1299. (<a href='https://doi.org/10.1093/comjnl/bxaf037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of the Internet of Things (IoT) has led to a huge amount of data beginning to emerge. Federated learning (FL) has received widespread attention and application as a new paradigm for data collection. However, data trading poses a threat to the privacy of data owners, and even participants in federated learning face the risk of data breaches. While many encryption methods have been proposed to mitigate these risks, the encrypted data negatively impacts the quality of the global model in federated learning. To this end, we propose an algorithm based on contract mechanisms to resolve the conflict between the privacy protection level of clients and the aggregation error on the federated learning server. Clients upload perturbed data according to their privacy protection levels, while mitigating the conflict between client data privacy protection and platform global model aggregation error. Through theoretical analysis and extensive experiments, our proposed trading method achieves desirable data utility while ensuring budget feasibility, individual rationality, and incentive compatibility.},
  archive      = {J_COMJNL},
  author       = {Liao, Gengjian and Shao, Shiyu and Feng, Zhenni},
  doi          = {10.1093/comjnl/bxaf037},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1285-1299},
  shortjournal = {Comput. J.},
  title        = {Balancing privacy and model performance in federated learning through contract-based data trading},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHISH_ATTENTION: Achieving robust phishing website detection with balanced datasets and advanced URL features. <em>COMJNL</em>, <em>68</em>(9), 1263-1284. (<a href='https://doi.org/10.1093/comjnl/bxaf036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating prevalence of phishing attacks in recent years has underscored the imperative need for a comprehensive and sophisticated response to mitigate this pervasive cyber threat. Characterized by deceptive tactics to obtain sensitive information, phishing has evolved in sophistication, resulting in severe consequences such as financial loss, identity theft, and compromise of personal data. This research addresses the inherent challenges faced by existing anti-phishing solutions, encompassing limitations in feature extraction methodologies, suboptimal feature selection, and issues related to dataset imbalance. In response to these challenges, we propose “PHISH_ATTENTION,” an advanced anti-phishing framework that integrates Variational Autoencoders and a Multi-Head Self-Attention Mechanism. The framework is further enhanced by the incorporation of a Deep Convolutional Generative Adversarial Network to effectively address dataset imbalances. Rigorous testing on benchmark datasets reveals that PHISH_ATTENTION achieves a peak detection accuracy of 98.57% with a minimal false alarm rate of 1.09%, surpassing prevailing anti-phishing models. Distinguished by its proficiency in real-time phishing website detection, the framework’s autonomous acquisition of significant URL features establishes it as a resilient and pioneering contribution to the cybersecurity domain.},
  archive      = {J_COMJNL},
  author       = {Prabhakaran, Manoj Kumar and Chandrasekar, Abinaya Devi and Meenakshi Sundaram, Parvathy},
  doi          = {10.1093/comjnl/bxaf036},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1263-1284},
  shortjournal = {Comput. J.},
  title        = {PHISH_ATTENTION: Achieving robust phishing website detection with balanced datasets and advanced URL features},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilingual knowledge graph completion based on structural features. <em>COMJNL</em>, <em>68</em>(9), 1252-1262. (<a href='https://doi.org/10.1093/comjnl/bxaf035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilingual knowledge graph completion predicts missing facts in the target language knowledge graph by learning and inferring knowledge and rules in other language knowledge graphs. Existing methods tend to use aligned entities between knowledge graphs in different languages as alignment seeds, and receive information from them through alignment seeds to promote entity alignment between different knowledge graphs. However, these methods only consider the local structural information of the knowledge graph by aggregating entity neighborhood information through aligned entities, and ignore the global structural information. At the same time, the methods aboved are hardly to learn the entity representation with sparse neighborhood in isolated subgraphs. To address these two problems, this paper proposes a multilingual knowledge graph completion method based on double-branch graph neural network and self-supervised entity alignment (DBGNN-SSL). The global and local topological structures of the knowledge graph are learned through a double-branch graph attention neural network, and more aligned entities can be iteratively generated through self-supervised learning. The experimental results on datasets DBP-5L and E-PKG verify the effectiveness of the proposed method.},
  archive      = {J_COMJNL},
  author       = {He, Jinyan and Yang, Haitong},
  doi          = {10.1093/comjnl/bxaf035},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1252-1262},
  shortjournal = {Comput. J.},
  title        = {Multilingual knowledge graph completion based on structural features},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unravelling the semantic mysteries of transformers layer by layer. <em>COMJNL</em>, <em>68</em>(9), 1237-1251. (<a href='https://doi.org/10.1093/comjnl/bxaf034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant success of transformer models and their successors in various natural language processing (NLP) applications, their internal workings are still not fully understood. Much of the current interpretability research has focused primarily on numerical components, often missing the complex semantic layers within these models. To fill this gap, this study explores the interpretability of the transformer model, a cornerstone of modern NLP, by addressing the semantic complexities of its multi-layer architecture. We identify three key questions: (i) the influence of the multi-layer structure on semantic processing, (ii) the unique contributions of each layer to model performance, and (iii) methodologies for determining optimal layer counts for the encoder and decoder. To tackle these issues, we introduce the semantic interpreter for transformer hierarchy, an innovative framework that employs convex hull metrics to visualize and assess semantic quality and quantity. Our contributions include novel methods for semantic assessment, a dual analytical framework that integrates cumulative and layer-to-layer perspectives, and insights into the dynamics of encoding and decoding. This comprehensive approach aims to enhance the understanding of Transformer models, ultimately guiding their refinement for improved interpretability and effectiveness in natural language tasks.},
  archive      = {J_COMJNL},
  author       = {Zhang, Cheng and Lv, Jinxin and Cao, Jingxu and Sheng, Jiachuan and Song, Dawei and Zhang, Tiancheng},
  doi          = {10.1093/comjnl/bxaf034},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1237-1251},
  shortjournal = {Comput. J.},
  title        = {Unravelling the semantic mysteries of transformers layer by layer},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid approach to task offloading optimization: Integrating hybrid whale genetic algorithm and reinforcement learning. <em>COMJNL</em>, <em>68</em>(9), 1225-1236. (<a href='https://doi.org/10.1093/comjnl/bxaf033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing presents a promising approach for achieving communication Quality of Service (QoS) by employing a task offloading strategy to transfer latency-sensitive tasks into edge servers. Considering the offload equalization challenge, in this paper, we propose a novel task offloading optimization method based on a Hybrid Whale Genetic Algorithm (HWGA) with Reinforcement Learning (RL) to optimize the task offloading decisions within a tri-layer edge computing architecture comprising edge, fog, and cloud layers. Due to the expansive dimensionality of the action space from the increasing number of devices, we adapt the RL into a multi-layer architecture. In this framework, multi-layer RL techniques are first utilized to determine which layer should handle the task offloading. Subsequently, the HWGA is applied to guide the task offloading decisions for devices within each layer. Simulation results demonstrate that, when compared to baseline methods, our HWGA-based approach significantly reduces task completion time and energy consumption, while improving the task success rate, particularly in high-device-density scenarios.},
  archive      = {J_COMJNL},
  author       = {Luo, Qianhua and Xie, Bo and Wang, Jiahuan and Shuai, Jiaqi and Cui, Haixia},
  doi          = {10.1093/comjnl/bxaf033},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1225-1236},
  shortjournal = {Comput. J.},
  title        = {A hybrid approach to task offloading optimization: Integrating hybrid whale genetic algorithm and reinforcement learning},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arming text-based gender inference with partition membership filtering and feature selection for online social network users. <em>COMJNL</em>, <em>68</em>(9), 1208-1224. (<a href='https://doi.org/10.1093/comjnl/bxaf032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is devoted to simulating a text categorization-based gender inference attack over online social networks primarily to inspect the effect of partition membership filter (PMF) and feature selection (FS) on the performance of an attribute inference mechanism especially for the case of the distributed representation of texts. The task turning into a binary machine learning (ML) problem in the field of artificial intelligence (AI) is studied in multilingual scenarios (i.e. Turkish and English) under four main cases. The results obtained by extensive experiments show that distributed embeddings often outperform traditional embeddings. In contrast, the case involving FS on distributed embeddings is superior to other cases two of which incorporate PMF. On the other hand, the best f1-scores obtained on Turkish and English datasets are 0.727 and 0.611 obtained with the help of Random Forest and Support Vector Machine classifiers, respectively. It is worth noting that this investigation is not handled in the existing literature on text data. Therefore, it is believed that the findings of this study will provide useful insight for researchers studying text-based attribute inference attacks as well as some other text-based binary ML tasks in the field of AI.},
  archive      = {J_COMJNL},
  author       = {Çoban, Önder and Yücel Altay, Şeyma},
  doi          = {10.1093/comjnl/bxaf032},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1208-1224},
  shortjournal = {Comput. J.},
  title        = {Arming text-based gender inference with partition membership filtering and feature selection for online social network users},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient unlearning for data security in deep learning systems. <em>COMJNL</em>, <em>68</em>(9), 1197-1207. (<a href='https://doi.org/10.1093/comjnl/bxaf031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine unlearning in the context of cybersecurity and privacy protection facilitates the removal of specific training data impacts from deep learning (DL) models, adhering to security, privacy, or compliance demands. However, traditional methods can only handle short-term, independent unlearning tasks. Conversely, real-world scenarios often involve extensive unlearning demands from users. Current methods fail to adequately address these demands due to substantial computational overhead and adverse impacts on inference accuracy, leaving the security and privacy of many users at risk. To navigate these challenges adeptly, we introduce the Multi-Agent Reinforcement Learning Data Lifecycle Management (MADLM) strategy. MADLM intricately examines the interactions between unlearning and continuous learning processes, enabling the postponement of certain tasks for combined execution to optimize computational resources. Concurrently, it employs strategic data management to maintain and enhance inference accuracy. Furthermore, by utilizing Multi-Agent Reinforcement Learning (MARL), MADLM dynamically orchestrates task scheduling to minimize computational demands, improve task response times, and bolster inference reliability, crucial for upholding stringent cybersecurity and privacy standards. Our evaluations of MADLM reveal substantial enhancements, including a 6% uplift in inference accuracy and a dramatic reduction in computational overhead to merely 12% of the original demands, effectively expanding the data security protections.},
  archive      = {J_COMJNL},
  author       = {Guo, Enting and Su, Chunhua and Li, Peng},
  doi          = {10.1093/comjnl/bxaf031},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1197-1207},
  shortjournal = {Comput. J.},
  title        = {Efficient unlearning for data security in deep learning systems},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Use of data mining in identifying the risk factors of optic neuropathy. <em>COMJNL</em>, <em>68</em>(9), 1181-1196. (<a href='https://doi.org/10.1093/comjnl/bxaf030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optic nerve carries signals from the eye to the brain, where they are interpreted as visual images. Optic neuropathy is a serious eye disease that can lead to the loss of vision in the affected eye. Identifying the risk factors for optic neuropathy from large patient data is crucial and challenging. Modern techniques can assist in recognizing these risk factors. For instance, data mining algorithms such as classification and association rules can discover knowledge from datasets in many real-world applications, particularly in the medical field. This study employed a decision tree algorithm known as J48 and an association rule algorithm called Apriori to analyze the collected data. The J48 algorithm achieved an accuracy of 90%, while the Apriori algorithm discovered 52 significant association rules with a confidence level above 80%. The goal of this study was to identify risk factors for optic neuropathy and explore the connection between optic nerve damage and other conditions. The proposed algorithms aim to reduce blindness rates and increase awareness of the risk factors associated with optic nerve damage by detecting hidden risk factors at an early stage. The study's findings show that some risk factors for optic neuropathy confirmed by medical trials are also detected by these algorithms, proving the effectiveness and applicability of data mining techniques in the medical field. Moreover, this study discovered new risk factors for optic nerve damage not previously found by medical trials. This knowledge will contribute to the early detection and prevention of blindness by recognizing risk factors for optic nerve damage.},
  archive      = {J_COMJNL},
  author       = {Al-Shamiri, Abdulkawi Yahya Radman and Yu, Dong-Jun and Li, Peipei and Al-Mahweeti, Balqis Yahya Ali Abdullah},
  doi          = {10.1093/comjnl/bxaf030},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1181-1196},
  shortjournal = {Comput. J.},
  title        = {Use of data mining in identifying the risk factors of optic neuropathy},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced code slicing with pre-trained model fine-tuned for open-source component malware detection. <em>COMJNL</em>, <em>68</em>(9), 1163-1180. (<a href='https://doi.org/10.1093/comjnl/bxaf029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open Source Software (OSS) is an essential part of modern software development, with platforms such as PyPI for Python, NPM for JavaScript, and RubyGems for Ruby facilitating code sharing and reuse. However, these repositories also pose significant security risks due to potential software supply chain attacks, where payloads are injected into components, propagating threats to downstream users and critical infrastructure. Existing automatic malicious component detection tools, particularly for PyPI, struggle to distinguish between subtle differences in malicious and benign behaviors, leading to high false positive rates. To address these issues, we systematically compare and explore these subtle differences, offering a more refined and accurate detection method, Open-Source Component Code Slices BERT (OCS-BERT). OCS-BERT leverages taint-based program slicing to isolate sensitive behavior segments and fine-tunes pre-trained model to capture subtle semantic differences across programming languages. This system excels in detecting malicious Python components and exhibits encouraging cross-language transferability to JavaScript's NPM and Ruby's RubyGems. Additionally, OCS-BERT successfully detected 107 malicious components from a total of 25,759 newly-uploaded PyPI components, taking two weeks to complete the process. This achievement demonstrates the effectiveness of our method, which serves as a potent enhancement to the current repertoire of software supply chain detection methodologies.},
  archive      = {J_COMJNL},
  author       = {Wang, Yongshan and Pang, Siyuan and Fan, Zijing and Shang, Shang and Yao, Yepeng and Jiang, Zhengwei and Liu, Baoxu},
  doi          = {10.1093/comjnl/bxaf029},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1163-1180},
  shortjournal = {Comput. J.},
  title        = {Advanced code slicing with pre-trained model fine-tuned for open-source component malware detection},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced social media crime prevention via deep learning and cryptographic data encryption. <em>COMJNL</em>, <em>68</em>(9), 1150-1162. (<a href='https://doi.org/10.1093/comjnl/bxaf028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of crime facilitated through social media platforms has become a critical concern for law enforcement and security agencies. With vast amounts of personal, sensitive, and actionable data being shared online, the risk of criminal exploitation has grown significantly, necessitating innovative approaches to crime prevention. This research addresses the problem of detecting and preventing social media-based crimes by integrating cryptographic data encryption with a focus on privacy and security. This study proposes a hybrid ECC-RSA (Elliptic Curve Cryptography—Rivest, Shamir, Adleman) model with key selection optimized using the Zebra Optimization Algorithm (ZOA) to enhance both the security and efficiency of the encryption process. Additionally, an Artificial Neural Network (ANN) is employed as a specific application to classify and detect suspicious online activities related to criminal behavior. A key challenge in this domain is achieving a balance between privacy preservation and effective crime detection. To evaluate the system, an extensive dataset of social media interactions containing both legitimate and suspicious activities was used. The results demonstrate that the integrated ECC-RSA model with ZOA optimization provides robust encryption while maintaining high detection accuracy, achieving 92% accuracy, 89% precision, and 91% recall. The ANN-based detection system successfully identifies potential criminal activities, while the cryptographic model ensures no sensitive data is exposed during analysis, maintaining user privacy. The findings suggest that the proposed hybrid model offers a promising solution for proactive crime prevention on social media, effectively balancing privacy, security, and detection performance.},
  archive      = {J_COMJNL},
  author       = {Alserhani, Faeiz},
  doi          = {10.1093/comjnl/bxaf028},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1150-1162},
  shortjournal = {Comput. J.},
  title        = {Advanced social media crime prevention via deep learning and cryptographic data encryption},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRACE: Front-running attack classification on ethereum using ensemble learning. <em>COMJNL</em>, <em>68</em>(9), 1137-1149. (<a href='https://doi.org/10.1093/comjnl/bxaf027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid evolution of blockchain technologies, Ethereum has emerged as a central platform for advanced financial applications but has concurrently experienced a rise in security vulnerabilities, particularly from front-running attacks. These attacks exploit transaction sequencing for illegal gains. To combat this, we introduce FRACE (Front-Running Attack Classification using Ensemble Learning), a novel methodology that classifies front-running attacks into displacement, insertion, and suppression using an ensemble learning model. This precise classification facilitates tailored defensive strategies, enhancing the robustness and accuracy of attack detection. Our approach achieves an accuracy of 95.36% and an F1-score of 95.30%, significantly improving the security of decentralized applications. Extensive analysis and validation on Ethereum confirm these results. Future efforts will refine these models and extend their application to other blockchain platforms, striving for a universally secure, transparent, and reliable digital transaction ecosystem.},
  archive      = {J_COMJNL},
  author       = {Zhang, Yuheng and Wang, Guojun and Li, Peiqiang and Gu, Wanyi and Chen, Houji},
  doi          = {10.1093/comjnl/bxaf027},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1137-1149},
  shortjournal = {Comput. J.},
  title        = {FRACE: Front-running attack classification on ethereum using ensemble learning},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GitHub project recommendation based on knowledge graph and developer similarity. <em>COMJNL</em>, <em>68</em>(9), 1128-1136. (<a href='https://doi.org/10.1093/comjnl/bxaf026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding and recommending projects that match developer’s interests is always an urgent problem in open-source community. There are some problems in the existing project recommendation methods, such as insufficient use of information, ignoring the relationship between projects, one-sided consideration, and so on. To solve the above problems, we propose a project recommendation model based on project knowledge graph and developer similarity, called knowledge graphs and developer interest similarity (KGDS). KGDS mines developer interest from project similarity and developer similarity. For project similarity, we first construct the project knowledge graph. Then, content features and potential features are extracted from the project Readme document and knowledge graph, respectively, and the two features are merged to enrich the developer embedding and project embedding, which solves the problem of insufficient utilization of information. For developer similarity, we first construct a developer-project matrix, then obtain the historical developers related to candidate project, and then calculate the similarity between the historical developers and the target developer, which solves the problem of one-sided consideration. Finally, we combine the two part information to recommend projects that meet the interests of developers. We have conducted experiments on the GitHub dataset, and the results show that KGDS outperforms the baseline model.},
  archive      = {J_COMJNL},
  author       = {Yu, Song and Liu, Wenlong and Wu, Hannan and Liao, Zhifang},
  doi          = {10.1093/comjnl/bxaf026},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1128-1136},
  shortjournal = {Comput. J.},
  title        = {GitHub project recommendation based on knowledge graph and developer similarity},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A yolo-like lightweight ship detection network for unmanned surface vehicles. <em>COMJNL</em>, <em>68</em>(9), 1118-1127. (<a href='https://doi.org/10.1093/comjnl/bxaf025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of unmanned surface vehicles (USVs) in maritime surveillance has highlighted the need for improved ship detection models. However, hardware limitations and environmental interference impact the perception capability of USVs. In order to address these challenges, YoloS, a lightweight ship detection network, is specifically designed for USVs in complex backgrounds. Firstly, a network component Split Widely Network (SWNet) is put forward to the backbone to reduce its redundancy. SWNet leverages inter-channel interaction and the idea of group convolution to minimize redundancy of the network. Secondly, the Small Pyramid Network (SPN) is introduced as the neck network. SPN enhances the spatial information of the target regions by capturing and highlighting significant contour and texture details within target regions. SPN also utilizes these low-level features with enhanced spatial information to guide high-level features in identifying the most discriminative fine-grained details within the target regions. In order to further achieve network lightweighting, only two output heads for low-level features are retained while still preserving the capability to extract task-critical features. Extensive experiments on different datasets have verified the effectiveness of the proposed method, and it shows that YoloS can achieve 96.5 % detection accuracy and 80.1 fps on the SMD dataset with only 2.78M model parameters and 10.4G floating point operations.},
  archive      = {J_COMJNL},
  author       = {Zhou, Weina and Shao, Wei and Hu, Wenhua},
  doi          = {10.1093/comjnl/bxaf025},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1118-1127},
  shortjournal = {Comput. J.},
  title        = {A yolo-like lightweight ship detection network for unmanned surface vehicles},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-feature adaptive framework for multimodal disinformation detection. <em>COMJNL</em>, <em>68</em>(9), 1105-1117. (<a href='https://doi.org/10.1093/comjnl/bxaf024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of disinformation on online social media has caused massive concern. Existing disinformation detection methods neglect the diverse compositional forms of tweets in real-life scenarios, making them less applicable and effective in social media settings. Meanwhile, these methods use pattern cues but overlook important aspects such as syntax, lexicon, and shallow visual semantics, and lack attention to factual content such as time, place, and person relay in both text and images, thus failing to fully explore features of disinformation and limiting detection accuracy. Furthermore, with the popularity of large language models (LLMs), the tweets generated by these models make the style of disinformation more subtle. Since existing datasets are mostly human-generated and lack style diversity, it results in weak detection capabilities of methods trained on these datasets. To address these challenges, a dual-feature adaptive framework for multimodal disinformation detection is proposed. The framework first using a similarity-based algorithm adaptively handles different tweet forms. It then enhances pattern features by bridging multimodal output from single-modal pretrained modal, and factual features are subsequently extracted using a zero-shot method based on a large vision language model. Finally, an expert network aggregates and reweights the dual-feature representation for tweets using an LLM-text detector in gating strategy. This paper also presents two multimodal disinformation datasets that include both LLM-generated and human-generated tweets reflecting real-world scenarios. The true tweets in datasets are diverse in style, while the fake tweets are more misleading. Experimentally verified, the proposed method outperforms baseline methods by an accuracy of 1.04% and 0.72% on typical datasets while also achieving a minimum accuracy drop of 0.65% and 0.87% on the proposed dataset.},
  archive      = {J_COMJNL},
  author       = {Yan, Kexiang and Liang, Gang and Wang, Lei and Sun, Mingxu and Zhao, Kui},
  doi          = {10.1093/comjnl/bxaf024},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1105-1117},
  shortjournal = {Comput. J.},
  title        = {Dual-feature adaptive framework for multimodal disinformation detection},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PEAR: Privacy-preserving and effective aggregation for byzantine-robust federated learning in real-world scenarios. <em>COMJNL</em>, <em>68</em>(9), 1087-1104. (<a href='https://doi.org/10.1093/comjnl/bxae086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables collaborative training of global models among distributed clients without sharing local data. Secure aggregation, a new security primitive of FL, enhances the confidentiality of data and model parameters. Unfortunately, privacy-preserving (PP) FL is vulnerable to common poisoning attacks by Byzantine adversaries. Existing defense strategies mainly focus on identifying abnormal local gradients over plaintexts, which provides a weak privacy guarantee. In PPFL, adversaries can escape existing defenses by uploading encrypted poisonous gradients. In addition, most mainstream aggregation algorithms assume that clients’ local training data is uniformly distributed, Independent and Identically Distributed (IID), which is unrealistic for real-world FL scenarios where data are only stored on large-scale terminal devices. To address these issues, we propose PEAR, a PP aggregation strategy based on single key-dual server CKKS full homomorphic encryption in real-world distributed scenarios, which can resist encrypted poisoning attacks. Specifically, we use cosine similarity to measure the distance between encrypted gradients. Then, we propose a novel Byzantine-tolerance aggregation mechanism using cosine similarity, which includes trust score generation that can tolerate differentiated local gradients and a two-step weight generation method that considers both the degree of gradient deviation in direction and training data size. This mechanism can achieve robustness for both IID and non-IID data without compromising privacy. Our extensive evaluations for two typical poisoning attacks on different datasets show that PEAR is robust and effective in IID and non-IID data and outperforms existing mainstream Byzantine-robust algorithms, especially achieving 16.4% to 53.2% testing error rate reduction in non-IID settings with significant label distribution and quantity skew while maintaining the same efficiency as FedAvg.},
  archive      = {J_COMJNL},
  author       = {Sun, Han and Zhang, Yan and Zhuang, Huiping and Li, Jiatong and Xu, Zhen and Wu, Liji},
  doi          = {10.1093/comjnl/bxae086},
  journal      = {The Computer Journal},
  month        = {9},
  number       = {9},
  pages        = {1087-1104},
  shortjournal = {Comput. J.},
  title        = {PEAR: Privacy-preserving and effective aggregation for byzantine-robust federated learning in real-world scenarios},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="evlett">EVLETT - 1</h2>
<ul>
<li><details>
<summary>
(2025). Genetic divergence in population mean fitness is weakly associated with environmental and geographic distance in four prairie perennial forbs. <em>EVLETT</em>, <em>9</em>(5), 522-532. (<a href='https://doi.org/10.1093/evlett/qraf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant propagules are frequently relocated between populations for restoration, especially in fragmented ecosystems like prairies, where few pristine patches remain. While research shows that plant populations often perform better in their native environments than in foreign sites, this pattern is not universal. The extent to which plant population fitness varies with distance from its site of origin remains unclear. Using aster models, we investigated the relationship of fitness with geographic distance and climate differences between the source and experimental sites for four perennial prairie forbs by planting 12 populations of each species at a north and south experimental site in the tallgrass prairie of Minnesota, USA. At both experimental sites, individuals from warmer and southern source sites had greater fitness, but the deviations of population mean fitnesses from the fitted relationships were substantial and idiosyncratic. Our results suggest limited effectiveness of geographic distance and temperature difference in predicting population mean fitness. This challenges the efficacy of long distance seed transfers as seed sourcing strategies to promote population persistence in prairie restorations.},
  archive      = {J_EVLETT},
  author       = {Peschel, Anna R and Flint, Shelby A and May, Georgiana and Shaw, Ruth G},
  doi          = {10.1093/evlett/qraf018},
  journal      = {Evolution Letters},
  month        = {10},
  number       = {5},
  pages        = {522-532},
  shortjournal = {Evol. Lett.},
  title        = {Genetic divergence in population mean fitness is weakly associated with environmental and geographic distance in four prairie perennial forbs},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jrssig">JRSSIG - 2</h2>
<ul>
<li><details>
<summary>
(2025). Regulating life, the universe and everything?. <em>JRSSIG</em>, <em>22</em>(6), 37-41. (<a href='https://doi.org/10.1093/jrssig/qmaf077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data governance has been a hot topic at the United Nations this year. Steve MacFeely gives an overview of recent developments and explains why it matters},
  archive      = {J_JRSSIG},
  author       = {MacFeely, Steve},
  doi          = {10.1093/jrssig/qmaf077},
  journal      = {Significance},
  month        = {11},
  number       = {6},
  pages        = {37-41},
  shortjournal = {Significance},
  title        = {Regulating life, the universe and everything?},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Climate change attribution: An explainer. <em>JRSSIG</em>, <em>22</em>(6), 27-29. (<a href='https://doi.org/10.1093/jrssig/qmaf074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the third of an occasional series of explainers on the statistics and data underpinning our understanding of climate change, Dario Domingo , Andrew Parnell and David B. Stephenson give an overview of the study of the links between climate change and extreme weather events},
  archive      = {J_JRSSIG},
  author       = {Parnell, Andrew and Domingo, Dario and Stephenson, David B},
  doi          = {10.1093/jrssig/qmaf074},
  journal      = {Significance},
  month        = {11},
  number       = {6},
  pages        = {27-29},
  shortjournal = {Significance},
  title        = {Climate change attribution: An explainer},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jrsssa">JRSSSA - 3</h2>
<ul>
<li><details>
<summary>
(2025). Does ad hoc language training improve the economic integration of refugees? evidence from germany’s response to the syrian refugee crisis. <em>JRSSSA</em>, <em>188</em>(4), 1168-1183. (<a href='https://doi.org/10.1093/jrsssa/qnae106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the global displacement crisis, the integration of refugees has emerged as a critical policy issue for many host countries. A key challenge involves supporting refugees in learning the language of their host country. While several European nations have instituted publicly funded language training for asylum seekers and refugees soon after their arrival, evidence on the efficacy of these early language programmes in promoting economic integration remains limited. This study examines the impact of a pioneering, large-scale ad hoc programme introduced by German policymakers, which provided basic language training to over 230,000 refugees arriving in 2015–2016. Utilizing register data on the population of asylum seekers and exploiting a cut-off date in programme eligibility, we assess the programme’s effectiveness using a regression discontinuity design. Our findings reveal no discernible effect on refugee employment over the subsequent 2 years. To explore whether language programmes are generally ineffective during refugee crises, we contrast these results with the impacts of a more comprehensive, preexisting, yet smaller-scale programme. Using a variety of difference-in-differences estimators, we find that this programme considerably increased refugee employment. These contrasting findings offer important insights for policymakers on designing effective language training programmes for refugees.},
  archive      = {J_JRSSSA},
  author       = {Marbach, Moritz and Vallizadeh, Ehsan and Harder, Niklas and Hangartner, Dominik and Hainmueller, Jens},
  doi          = {10.1093/jrsssa/qnae106},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {10},
  number       = {4},
  pages        = {1168-1183},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Does ad hoc language training improve the economic integration of refugees? evidence from germany’s response to the syrian refugee crisis},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting school enrollments in the australian capital territory. <em>JRSSSA</em>, <em>188</em>(4), 1107-1124. (<a href='https://doi.org/10.1093/jrsssa/qnae094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {School enrollment forecasts are vital for effective planning. This study introduces a probabilistic multiregional population projection model, which accounts for different components, including preschool entries, migration, grade progression, and graduations. Using different distributions with a cohort component projection and Monte Carlo simulation, this paper forecasts student enrollments for each school and academic year level in the Australian Capital Territory based on the annual record-level administrative data. In the in-sample validation tests, the model’s overall performance is robust, and the probabilistic design offers reliable prediction intervals reflecting the variation in observed values. The paper ends with a discussion on the importance of prediction intervals for informing school planning.},
  archive      = {J_JRSSSA},
  author       = {Shen, Tianyu and Raymer, James and Hendy, Caroline},
  doi          = {10.1093/jrsssa/qnae094},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {10},
  number       = {4},
  pages        = {1107-1124},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Forecasting school enrollments in the australian capital territory},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seconder of the vote of thanks to cork et al. and contribution to the discussion of ‘Methods for estimating the exposure–response curve to inform the new safety standards for fine particulate matter’ by cork et al.. <em>JRSSSA</em>, <em>188</em>(4), 986-988. (<a href='https://doi.org/10.1093/jrsssa/qnaf047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Swallow, Ben},
  doi          = {10.1093/jrsssa/qnaf047},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {10},
  number       = {4},
  pages        = {986-988},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Seconder of the vote of thanks to cork et al. and contribution to the discussion of ‘Methods for estimating the exposure–response curve to inform the new safety standards for fine particulate matter’ by cork et al.},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jrsssb">JRSSSB - 20</h2>
<ul>
<li><details>
<summary>
(2025). Correction to: Convexity and measures of statistical association. <em>JRSSSB</em>, <em>87</em>(4), 1307. (<a href='https://doi.org/10.1093/jrsssb/qkaf040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf040},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1307},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Convexity and measures of statistical association},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Parameterizing and simulating from causal models. <em>JRSSSB</em>, <em>87</em>(4), 1306. (<a href='https://doi.org/10.1093/jrsssb/qkaf039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf039},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1306},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Parameterizing and simulating from causal models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Consistent and fast inference in compartmental models of epidemics using poisson approximate likelihoods. <em>JRSSSB</em>, <em>87</em>(4), 1305. (<a href='https://doi.org/10.1093/jrsssb/qkaf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf013},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1305},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Consistent and fast inference in compartmental models of epidemics using poisson approximate likelihoods},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convexity and measures of statistical association. <em>JRSSSB</em>, <em>87</em>(4), 1281-1304. (<a href='https://doi.org/10.1093/jrsssb/qkaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent investigations on the measures of statistical association highlight essential properties such as zero-independence (the measure is zero if and only if the random variables are independent), monotonicity under information refinement, and max-functionality (the measure of association is maximal if and only if we are in the presence of a deterministic (noiseless) dependence). An open question concerns the reasons why measures of statistical associations satisfy one or more of those properties but not others. We show that convexity plays a central role in all properties. Convexity plus a form of strictness (that we are to define) are necessary and sufficient for zero-independence, and convexity and strict convexity on Dirac masses are necessary and sufficient for max-functionality. We apply the findings to study the families of measures of statistical association based on Csiszár divergences, optimal transport, kernels, as well as Chatterjee’s new correlation coefficient. We further discuss the role of convexity in guaranteeing the asymptotic unbiasedness of given data estimators, prove a central limit theorem for those estimators under independence, and show the rate of convergence under arbitrary dependence. We demonstrate the findings with numerical simulations in a multivariate response context.},
  archive      = {J_JRSSSB},
  author       = {Borgonovo, Emanuele and Figalli, Alessio and Ghosal, Promit and Plischke, Elmar and Savaré, Giuseppe},
  doi          = {10.1093/jrsssb/qkaf018},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1281-1304},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Convexity and measures of statistical association},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-resolution subsampling for linear classification with massive data. <em>JRSSSB</em>, <em>87</em>(4), 1260-1280. (<a href='https://doi.org/10.1093/jrsssb/qkaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling is one of the popular methods to balance statistical efficiency and computational efficiency in the big data era. Most approaches aim to select informative or representative sample points to achieve good overall information of the full data. The present work takes the view that sampling techniques are recommended for the region we focus on and summary measures are enough to collect the information for the rest according to a well-designed data partitioning. We propose a subsampling strategy that collects global information described by summary measures and local information obtained from selected subsample points. Thus, we call it multi-resolution subsampling. We show that the proposed method leads to a more efficient subsample-based estimator for general linear classification problems. Some asymptotic properties of the proposed method are established and connections to existing subsampling procedures are explored. Finally, we illustrate the proposed subsampling strategy via simulated and real-world examples.},
  archive      = {J_JRSSSB},
  author       = {Chen, Haolin and Dette, Holger and Yu, Jun},
  doi          = {10.1093/jrsssb/qkaf017},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1260-1280},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Multi-resolution subsampling for linear classification with massive data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence on the focal: Conformal prediction with selection-conditional coverage. <em>JRSSSB</em>, <em>87</em>(4), 1239-1259. (<a href='https://doi.org/10.1093/jrsssb/qkaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction builds marginally valid prediction intervals that cover the unknown outcome of a randomly drawn test point with a prescribed probability. However, in practice, data-driven methods are often used to identify specific test unit(s) of interest, requiring uncertainty quantification tailored to these focal units. In such cases, marginally valid conformal prediction intervals may fail to provide valid coverage for the focal unit(s) due to selection bias. This article presents a general framework for constructing a prediction set with finite-sample exact coverage, conditional on the unit being selected by a given procedure. The general form of our method accommodates arbitrary selection rules that are invariant to the permutation of the calibration units and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We also work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization-based selection, selection based on conformal p -values, and selection based on properties of preliminary conformal prediction sets. The performance of our methods is demonstrated via applications in drug discovery and health risk prediction.},
  archive      = {J_JRSSSB},
  author       = {Jin, Ying and Ren, Zhimei},
  doi          = {10.1093/jrsssb/qkaf016},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1239-1259},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Confidence on the focal: Conformal prediction with selection-conditional coverage},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased and consistent nested sampling via sequential monte carlo. <em>JRSSSB</em>, <em>87</em>(4), 1221-1238. (<a href='https://doi.org/10.1093/jrsssb/qkaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of sequential Monte Carlo methods which reformulates the essence of the nested sampling (NS) method of Skilling in terms of sequential Monte Carlo techniques. Two new algorithms are proposed: nested sampling via sequential Monte Carlo (NS-SMC) and adaptive nested sampling via sequential Monte Carlo (ANS-SMC). The new framework allows convergence results to be obtained in the setting when Markov chain Monte Carlo (MCMC) is used to produce new samples. An additional benefit is that marginal-likelihood (normalizing constant) estimates given by NS-SMC are unbiased. In contrast to NS, the analysis of our proposed algorithms does not require the (unrealistic) assumption that the simulated samples be independent. We show that a minor adjustment to our ANS-SMC algorithm recovers the original NS algorithm, which provides insights as to why NS seems to produce accurate estimates despite a typical violation of its assumptions. A numerical study is conducted where the performance of the proposed algorithms and temperature-annealed SMC is compared on challenging problems. Code for the experiments is made available online at https://github.com/LeahPrice/SMC-NS .},
  archive      = {J_JRSSSB},
  author       = {Salomone, Robert and South, Leah F and Drovandi, Christopher and Kroese, Dirk P and Johansen, Adam M},
  doi          = {10.1093/jrsssb/qkaf015},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1221-1238},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Unbiased and consistent nested sampling via sequential monte carlo},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential monte carlo testing by betting. <em>JRSSSB</em>, <em>87</em>(4), 1200-1220. (<a href='https://doi.org/10.1093/jrsssb/qkaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a Monte Carlo test, the observed dataset is fixed, and several resampled or permuted versions of the dataset are generated in order to test a null hypothesis that the original dataset is exchangeable with the resampled/permuted ones. Sequential Monte Carlo tests aim to save computational resources by generating these additional datasets sequentially one by one and potentially stopping early. While earlier tests yield valid inference at a particular prespecified stopping rule, our work develops a new anytime-valid Monte Carlo test that can be continuously monitored, yielding a p -value or e -value at any stopping time possibly not specified in advance. It generalizes the well-known method by Besag and Clifford, allowing it to stop at any time, but also encompasses new sequential Monte Carlo tests that tend to stop sooner under the null and alternative without compromising power. The core technical advance is the development of new test martingales for testing exchangeability against a very particular alternative based on a testing by betting technique. The proposed betting strategies are guided by the derivation of a simple log-optimal betting strategy, have closed-form expressions for the wealth process, provable guarantees on resampling risk, and display excellent power in practice.},
  archive      = {J_JRSSSB},
  author       = {Fischer, Lasse and Ramdas, Aaditya},
  doi          = {10.1093/jrsssb/qkaf014},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1200-1220},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Sequential monte carlo testing by betting},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework for cutting feedback within modularized bayesian inference. <em>JRSSSB</em>, <em>87</em>(4), 1171-1199. (<a href='https://doi.org/10.1093/jrsssb/qkaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard Bayesian inference enables building models that combine information from various sources, but this inference may not be reliable if components of the model are misspecified. Cut inference, a particular type of modularized Bayesian inference, is an alternative that splits a model into modules and cuts the feedback from any suspect module. Previous studies have focused on a two module case, but a more general definition of a ‘module’ remains unclear. We present a formal definition of a ‘module’ and discuss its properties. We formulate methods for identifying modules; determining the order of modules; and building the cut distribution that should be used for cut inference within an arbitrary directed acyclic graph structure. We justify the cut distribution by showing that it not only cuts the feedback but also is the best approximation to the joint distribution satisfying this condition in Kullback–Leibler divergence. We also extend cut inference for the two module case to a general multiple-module case via a sequential splitting technique and demonstrate this via illustrative applications.},
  archive      = {J_JRSSSB},
  author       = {Liu, Yang and Goudie, Robert J B},
  doi          = {10.1093/jrsssb/qkaf012},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1171-1199},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {A general framework for cutting feedback within modularized bayesian inference},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strong oracle guarantees for partial penalized tests of high-dimensional generalized linear models. <em>JRSSSB</em>, <em>87</em>(4), 1150-1170. (<a href='https://doi.org/10.1093/jrsssb/qkaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial penalized tests provide flexible approaches to testing linear hypotheses in high-dimensional generalized linear models. However, because the estimators used in these tests are local minimizers of potentially nonconvex folded-concave penalized objectives, the solutions one computes in practice may not coincide with the unknown local minima for which we have nice theoretical guarantees. To close this gap between theory and computation, we introduce local linear approximation (LLA) algorithms to compute the full and reduced model estimators for these tests and develop a theory specifically for the LLA solutions. We prove that our LLA algorithms converge to oracle estimators for the full and reduced models in two steps with overwhelming probability. We then leverage this strong oracle result and the asymptotic properties of the oracle estimators to show that the partial penalized test statistics evaluated at the LLA solutions are approximately chi-square in large samples, giving us guarantees for the tests using specific computed solutions and thereby closing the theoretical gap. In simulations, we find that our LLA tests closely agree with the oracle tests and compare favourably with alternative high-dimensional inference procedures. We demonstrate the flexibility of our LLA tests with two high-dimensional data applications.},
  archive      = {J_JRSSSB},
  author       = {Jacobson, Tate},
  doi          = {10.1093/jrsssb/qkaf010},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1150-1170},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Strong oracle guarantees for partial penalized tests of high-dimensional generalized linear models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian penalized empirical likelihood and markov chain monte carlo sampling. <em>JRSSSB</em>, <em>87</em>(4), 1127-1149. (<a href='https://doi.org/10.1093/jrsssb/qkaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce a novel methodological framework called Bayesian penalized empirical likelihood (BPEL), designed to address the computational challenges inherent in empirical likelihood (EL) approaches. Our approach has two primary objectives: (i) to enhance the inherent flexibility of EL in accommodating diverse model conditions, and (ii) to facilitate the use of well-established Markov Chain Monte Carlo sampling schemes as a convenient alternative to the complex optimization typically required for statistical inference using EL. To achieve the first objective, we propose a penalized approach that regularizes the Lagrange multipliers, significantly reducing the dimensionality of the problem while accommodating a comprehensive set of model conditions. For the second objective, our study designs and thoroughly investigates two popular sampling schemes within the BPEL context. We demonstrate that the BPEL framework is highly flexible and efficient, enhancing the adaptability and practicality of EL methods. Our study highlights the practical advantages of using sampling techniques over traditional optimization methods for EL problems, showing rapid convergence to the global optima of posterior distributions and ensuring the effective resolution of complex statistical inference challenges.},
  archive      = {J_JRSSSB},
  author       = {Chang, Jinyuan and Tang, Cheng Yong and Zhu, Yuanzheng},
  doi          = {10.1093/jrsssb/qkaf009},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1127-1149},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Bayesian penalized empirical likelihood and markov chain monte carlo sampling},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal prediction with conditional guarantees. <em>JRSSSB</em>, <em>87</em>(4), 1100-1126. (<a href='https://doi.org/10.1093/jrsssb/qkaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of constructing distribution-free prediction sets with finite-sample conditional guarantees. Prior work has shown that it is impossible to provide exact conditional coverage universally in finite samples. Thus, most popular methods only guarantee marginal coverage over the covariates or are restricted to a limited set of conditional targets, e.g. coverage over a finite set of prespecified subgroups. This paper bridges this gap by defining a spectrum of problems that interpolate between marginal and conditional validity. We motivate these problems by reformulating conditional coverage as coverage over a class of covariate shifts. When the target class of shifts is finite-dimensional, we show how to simultaneously obtain exact finite-sample coverage over all possible shifts. For example, given a collection of subgroups, our prediction sets guarantee coverage over each group. For more flexible, infinite-dimensional classes where exact coverage is impossible, we provide a procedure for quantifying the coverage errors of our algorithm. Moreover, by tuning interpretable hyperparameters, we allow the practitioner to control the size of these errors across shifts of interest. Our methods can be incorporated into existing split conformal inference pipelines, and thus can be used to quantify the uncertainty of modern black-box algorithms without distributional assumptions.},
  archive      = {J_JRSSSB},
  author       = {Gibbs, Isaac and Cherian, John J and Candès, Emmanuel J},
  doi          = {10.1093/jrsssb/qkaf008},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1100-1126},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Conformal prediction with conditional guarantees},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A conditioning tactic that increases design sensitivity in observational block designs. <em>JRSSSB</em>, <em>87</em>(4), 1085-1099. (<a href='https://doi.org/10.1093/jrsssb/qkaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an observational block design, there are I blocks of J individuals, typically with one treated individual and J − 1 controls; however, unlike a randomized block design, individuals were not randomly assigned to treatment or control. To be convincing, an observational block design must demonstrate that an ostensible treatment effect is not actually a consequence of small or moderate unmeasured biases of treatment assignment in the absence of a treatment effect. It is known that weighting to ignore blocks with a small range of responses increases the ability to distinguish a treatment effect from a bias in treatment assignment—that is, it increases the design sensitivity. Here, it is shown that a new tactic further increases design sensitivity. The new tactic involves a conditional statistic, such that blocks with moderately large ranges are considered conditionally given that the treated individual has either the largest or smallest response in the block. The new tactic is explored: (i) in terms of an asymptotic measure, the design sensitivity, (ii) in simulation of the power of a sensitivity analysis in finite samples, and (iii) in an example. Adaptive inference is briefly discussed. An R package weightedRank implements the method, contains the data, and reproduces the empirical results.},
  archive      = {J_JRSSSB},
  author       = {Rosenbaum, Paul R},
  doi          = {10.1093/jrsssb/qkaf007},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1085-1099},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {A conditioning tactic that increases design sensitivity in observational block designs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive experiments toward learning treatment effect heterogeneity. <em>JRSSSB</em>, <em>87</em>(4), 1055-1084. (<a href='https://doi.org/10.1093/jrsssb/qkaf006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding treatment effect heterogeneity has become an increasingly popular task in various fields, as it helps design personalized advertisements in e-commerce or targeted treatment in biomedical studies. However, most of the existing work in this research area focused on either analysing observational data based on strong causal assumptions or conducting post hoc analyses of randomized controlled trial data, and there has been limited effort dedicated to the design of randomized experiments specifically for uncovering treatment effect heterogeneity. In the manuscript, we develop a framework for designing and analysing response adaptive experiments toward better learning treatment effect heterogeneity. Concretely, we provide response adaptive experimental design frameworks that sequentially revise the data collection mechanism according to the accrued evidence during the experiment. Such design strategies allow for the identification of subgroups with the largest treatment effects with enhanced statistical efficiency. The proposed frameworks not only unify adaptive enrichment designs and response-adaptive randomization designs but also complement A/B test designs in e-commerce and randomized trial designs in clinical settings. We demonstrate the merit of our design with theoretical justifications and in simulation studies with synthetic e-commerce and clinical trial data.},
  archive      = {J_JRSSSB},
  author       = {Wei, Waverly and Ma, Xinwei and Wang, Jingshen},
  doi          = {10.1093/jrsssb/qkaf006},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1055-1084},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Adaptive experiments toward learning treatment effect heterogeneity},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric posterior corrections. <em>JRSSSB</em>, <em>87</em>(4), 1025-1054. (<a href='https://doi.org/10.1093/jrsssb/qkaf005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach to semiparametric inference using corrected posterior distributions. The method allows us to leverage the adaptivity, regularization, and predictive power of nonparametric Bayesian procedures to estimate low-dimensional functionals of interest without being restricted by the holistic Bayesian formalism. Starting from a conventional posterior on the whole data-generating distribution, we correct the marginal posterior for each functional of interest with the help of the Bayesian bootstrap. We provide conditions for the resulting one-step posterior to possess calibrated frequentist properties and specialize the results for several canonical examples: the integrated squared density, the mean of a missing-at-random outcome, and the average causal treatment effect on the treated. The procedure is computationally attractive, requiring only a simple, efficient postprocessing step that can be attached onto any arbitrary posterior sampling algorithm. Using the ACIC 2016 causal data analysis competition, we illustrate that our approach can outperform the existing state-of-the-art through the propagation of Bayesian uncertainty.},
  archive      = {J_JRSSSB},
  author       = {Yiu, Andrew and Fong, Edwin and Holmes, Chris and Rousseau, Judith},
  doi          = {10.1093/jrsssb/qkaf005},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1025-1054},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Semiparametric posterior corrections},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized empirical likelihood test for ultra-high dimensional means under general covariances. <em>JRSSSB</em>, <em>87</em>(4), 1001-1024. (<a href='https://doi.org/10.1093/jrsssb/qkaf004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a calibrated empirical likelihood test for ultra-high dimensional means that incorporates multiple projections. Under weak moment conditions on the distributions of data, we analyse all possible asymptotic distributions of the proposed test statistic in different scenarios. To determine the critical value and enhance test power, we employ the random symmetrization method based on the group of sign flips and use multiple selected projections. The test can still maintain the significance level asymptotically, even in the presence of heterogeneity in the data distribution. Moreover, the proposed test procedure allows for general covariance structures and ultra-high dimensional regimes. Further, the power function reveals the relation with the projection term in an asymptotic sense such that we can select suitable projections to achieve good power in various scenarios. A quasi-Newton algorithm is introduced to reduce the computational cost arising from the intensive optimizations required for computing empirical likelihood. Numerical studies evidence the promising performance of the proposed test compared with existing tests.},
  archive      = {J_JRSSSB},
  author       = {Chen, Yuexin and Zhu, Lixing and Xu, Wangli},
  doi          = {10.1093/jrsssb/qkaf004},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1001-1024},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Randomized empirical likelihood test for ultra-high dimensional means under general covariances},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmentation invariant manifold learning. <em>JRSSSB</em>, <em>87</em>(4), 978-1000. (<a href='https://doi.org/10.1093/jrsssb/qkaf003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is a widely used technique and an essential ingredient in the recent advance in self-supervised representation learning. By preserving the similarity between augmented data, the resulting data representation can improve various downstream analyses and achieve state-of-the-art performance in many applications. Despite the empirical effectiveness, most existing methods lack theoretical understanding under a general nonlinear setting. To fill this gap, we develop a statistical framework on a low-dimensional product manifold to model the data augmentation transformation. Under this framework, we introduce a new representation learning method called augmentation invariant manifold learning and design a computationally efficient algorithm by reformulating it as a stochastic optimization problem. Compared with existing self-supervised methods, the new method simultaneously exploits the manifold’s geometric structure and invariant property of augmented data and has an explicit theoretical guarantee. Our theoretical investigation characterizes the role of data augmentation in the proposed method and reveals why and how the data representation learned from augmented data can improve the k -nearest neighbour classifier in the downstream analysis, showing that a more complex data augmentation leads to more improvement in downstream analysis. Finally, numerical experiments on simulated and real data sets are presented to demonstrate the merit of the proposed method.},
  archive      = {J_JRSSSB},
  author       = {Wang, Shulei},
  doi          = {10.1093/jrsssb/qkaf003},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {978-1000},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Augmentation invariant manifold learning},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-phase rejective sampling and its asymptotic properties. <em>JRSSSB</em>, <em>87</em>(4), 957-977. (<a href='https://doi.org/10.1093/jrsssb/qkaf002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rejective sampling improves design and estimation efficiency of single-phase sampling when auxiliary information in a finite population is available. When such auxiliary information is unavailable, we propose to use two-phase rejective sampling (TPRS), which involves measuring auxiliary variables for the sample of units in the first phase, followed by the implementation of rejective sampling for the outcome in the second phase. We explore the asymptotic design properties of double expansion and regression estimators under TPRS. We show that TPRS enhances the efficiency of the double-expansion estimator, rendering it comparable to a regression estimator. We further refine the design to accommodate varying importance of covariates and extend it to multi-phase sampling. We start with the theory for the population mean and then extend the theory to parameters defined by general estimating equations. Our asymptotic results for TPRS immediately cover the existing single-phase rejective sampling, under which the asymptotic theory has not been fully established.},
  archive      = {J_JRSSSB},
  author       = {Yang, Shu and Ding, Peng},
  doi          = {10.1093/jrsssb/qkaf002},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {957-977},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Two-phase rejective sampling and its asymptotic properties},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analytic natural gradient updates for cholesky factor in gaussian variational approximation. <em>JRSSSB</em>, <em>87</em>(4), 930-956. (<a href='https://doi.org/10.1093/jrsssb/qkaf001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural gradients can improve convergence in stochastic variational inference significantly but inverting the Fisher information matrix is daunting in high dimensions. Moreover, in Gaussian variational approximation, natural gradient updates of the precision matrix do not ensure positive definiteness. To tackle this issue, we derive analytic natural gradient updates of the Cholesky factor of the covariance or precision matrix and consider sparsity constraints representing different posterior correlation structures. Stochastic normalized natural gradient ascent with momentum is proposed for implementation in generalized linear mixed models and deep neural networks.},
  archive      = {J_JRSSSB},
  author       = {Tan, Linda S L},
  doi          = {10.1093/jrsssb/qkaf001},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {930-956},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Analytic natural gradient updates for cholesky factor in gaussian variational approximation},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selecting informative conformal prediction sets with false coverage rate control. <em>JRSSSB</em>, <em>87</em>(4), 909-929. (<a href='https://doi.org/10.1093/jrsssb/qkae120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised learning, including regression and classification, conformal methods provide prediction sets for the outcome/label with finite sample coverage for any machine learning predictor. We consider here the case where such prediction sets come after a selection process. The selection process requires that the selected prediction sets be ‘informative’ in a well-defined sense. We consider both the classification and regression settings where the analyst may consider as informative only the sample with prediction sets small enough, excluding null values, or obeying other appropriate ‘monotone’ constraints. We develop a unified framework for building such informative conformal prediction sets while controlling the false coverage rate (FCR) on the selected sample. While conformal prediction sets after selection have been the focus of much recent literature in the field, the new introduced procedures, called InfoSP and InfoSCOP , are to our knowledge the first ones providing FCR control for informative prediction sets. We show the usefulness of our resulting procedures on real and simulated data.},
  archive      = {J_JRSSSB},
  author       = {Gazin, Ulysse and Heller, Ruth and Marandon, Ariane and Roquain, Etienne},
  doi          = {10.1093/jrsssb/qkae120},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {909-929},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Selecting informative conformal prediction sets with false coverage rate control},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jrsssc">JRSSSC - 12</h2>
<ul>
<li><details>
<summary>
(2025). A multivariate spatial statistical model for statistical downscaling of sea surface temperature in the great barrier reef region. <em>JRSSSC</em>, <em>74</em>(4), 1183-1213. (<a href='https://doi.org/10.1093/jrsssc/qlaf019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a statistical downscaling method to produce fine-resolution climate projections. A multivariate spatial statistical model is developed to jointly analyse high-resolution remote sensing data and coarse-resolution climate model outputs. With a basis function representation, the resulting model can achieve efficient computation and describe potentially nonstationary spatial dependence. We implement our method to produce downscaled sea surface temperature projections over the Great Barrier Reef region from CMIP6 Earth system models. Compared with the state of the art, our method reduces the mean squared predictive error substantially and produces a predictive distribution enabling holistic uncertainty quantification analyses.},
  archive      = {J_JRSSSC},
  author       = {Ekanayaka, Ayesha and Kang, Emily L and Braverman, Amy and Kalmus, Peter},
  doi          = {10.1093/jrsssc/qlaf019},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1183-1213},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {A multivariate spatial statistical model for statistical downscaling of sea surface temperature in the great barrier reef region},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian mortality modelling with pandemics: A vanishing jump approach. <em>JRSSSC</em>, <em>74</em>(4), 1150-1182. (<a href='https://doi.org/10.1093/jrsssc/qlaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends the Lee–Carter (LC) model for single- and multi-populations to account for pandemic jump effects of vanishing kind, allowing for a more comprehensive and accurate representation of mortality rates during a pandemic, characterized by a high impact at the beginning and gradually vanishing effects over subsequent periods. While the LC model is effective in capturing mortality trends, it may not always be able to account for large, unexpected jumps in mortality rates caused by pandemics or wars. Existing models allow either for transient jumps with an effect of one period only or persistent jumps. However, there is no literature on estimating mortality time series with jumps having an effect over a small number of periods, as is typically observed in pandemics. The Bayesian approach allows to quantify the uncertainty around the parameter estimates. Empirical data from the COVID-19 pandemic show the superiority of the proposed approach, compared with models with a transitory shock effect.},
  archive      = {J_JRSSSC},
  author       = {Goes, Julius and Barigou, Karim and Leucht, Anne},
  doi          = {10.1093/jrsssc/qlaf018},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1150-1182},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Bayesian mortality modelling with pandemics: A vanishing jump approach},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical modelling of on-street parking spot occupancy in smart cities. <em>JRSSSC</em>, <em>74</em>(4), 1128-1149. (<a href='https://doi.org/10.1093/jrsssc/qlaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies suggest that searching for parking is associated with significant direct and indirect costs. Therefore, it is appealing to reduce the time that car drivers spend on finding an available parking spot, especially in urban areas where the space for all road users is limited. The prediction of on-street parking spot occupancy can provide drivers with guidance on where clear parking spaces are likely to be found. This field of research has gained more and more attention in the last decade through the increasing availability of real-time parking spot occupancy data. In this paper, we pursue a statistical approach for the prediction of parking spot occupancy, where we make use of time-to-event models and semi-Markov process theory. The latter involves the employment of Laplace transformations as well as their inversion, which is an ambitious numerical task. We apply our methodology to data from the City of Melbourne in Australia. Our main result is that the semi-Markov model outperforms a Markov model in terms of both true negative rate and true positive rate while this is essentially achieved by respecting the current duration that a parking space already spends in its initial state.},
  archive      = {J_JRSSSC},
  author       = {Schneble, Marc and Kauermann, Göran},
  doi          = {10.1093/jrsssc/qlaf017},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1128-1149},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Statistical modelling of on-street parking spot occupancy in smart cities},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A flexible model for record linkage. <em>JRSSSC</em>, <em>74</em>(4), 1100-1127. (<a href='https://doi.org/10.1093/jrsssc/qlaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining data from various sources empowers researchers to explore innovative questions, for example those raised by conducting healthcare monitoring studies. However, the lack of a unique identifier often poses challenges. Record linkage procedures determine whether pairs of observations collected on different occasions belong to the same individual using partially identifying variables (e.g. birth year, postal code). Existing methodologies typically involve a compromise between computational efficiency and accuracy. Traditional approaches simplify this task by condensing information, yet they neglect dependencies among linkage decisions and disregard the one-to-one relationship required to establish coherent links. Modern approaches offer a comprehensive representation of the data generation process, at the expense of computational overhead and reduced flexibility. We propose a flexible method, that adapts to varying data complexities, addressing registration errors and accommodating changes of the identifying information over time. Our approach balances accuracy and scalability, estimating the linkage using a Stochastic Expectation Maximization algorithm on a latent variable model. We illustrate the ability of our methodology to connect observations using large real data applications and demonstrate the robustness of our model to the linking variables quality in a simulation study. The proposed algorithm FlexRL is implemented and available in an open source R package.},
  archive      = {J_JRSSSC},
  author       = {Robach, Kayané and van der Pas, Stéphanie L and van de Wiel, Mark A and Hof, Michel H},
  doi          = {10.1093/jrsssc/qlaf016},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1100-1127},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {A flexible model for record linkage},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven segmentation of observation-level logistic regression models. <em>JRSSSC</em>, <em>74</em>(4), 1077-1099. (<a href='https://doi.org/10.1093/jrsssc/qlaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a data-adaptive method to segment individual observation-based logistic regression models, focusing on motivating binary landslide data. Our method assigns observation-specific regression models and utilizes a grouped fused lasso penalty for data-adaptive model fusion when common regression coefficients are desired. However, when inherent differences persist, the models remain separate, resulting in distinct regression coefficients. To handle the large number of parameters arising from individual observation-based models, we develop a novel alternating direction method of multipliers-based algorithm. Our numerical study demonstrates improved prediction performance over conventional logistic regression models by leveraging heterogeneous data characteristics.},
  archive      = {J_JRSSSC},
  author       = {Choi, Yunjin and Park, No-Wook and Lee, Woojoo},
  doi          = {10.1093/jrsssc/qlaf015},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1077-1099},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Data-driven segmentation of observation-level logistic regression models},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying regions of concomitant compound precipitation and wind speed extremes over europe. <em>JRSSSC</em>, <em>74</em>(4), 1057-1076. (<a href='https://doi.org/10.1093/jrsssc/qlaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of simplifying the complex spatio-temporal variables associated with climate modelling is of utmost importance and comes with significant challenges. In this research, our primary objective is to develop a clustering framework to handle compound extreme events within gridded climate data across Europe. Specifically, we intend to identify subregions that display asymptotic independence between precipitation and wind speed extremes, meaning that occurrences of extreme rain and wind speed in one subregion do not affect those in the other. To achieve this, we utilize daily precipitation sums and daily maximum wind speed data derived from the ERA5 reanalysis dataset spanning from 1979 to 2022. Our approach hinges on a tuning parameter and the application of a divergence measure to spotlight disparities in extremal dependence structures without relying on specific parametric assumptions. We propose a data-driven approach to determine the tuning parameter. This enables us to generate clusters that are spatially concentrated, which can provide more insightful information about the regional distribution of compound precipitation and wind speed extremes. The proposed method is able to extract valuable information about extreme compound events while also significantly reducing the size of the dataset within reasonable computational timeframes.},
  archive      = {J_JRSSSC},
  author       = {Boulin, Alexis and Di Bernardino, Elena and Laloë, Thomas and Toulemonde, Gwladys},
  doi          = {10.1093/jrsssc/qlaf014},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1057-1076},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Identifying regions of concomitant compound precipitation and wind speed extremes over europe},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data fusion model for meteorological data using the INLA-SPDE method. <em>JRSSSC</em>, <em>74</em>(4), 1021-1056. (<a href='https://doi.org/10.1093/jrsssc/qlaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a data fusion model designed to address the problem of sparse observational data by incorporating numerical forecast models as an additional data source to improve predictions of key variables. This model is applied to two main meteorological data sources in the Philippines. The data fusion approach assumes that different data sources are imperfect representations of a common underlying process. Observations from weather stations follow a classical error model, while numerical weather forecasts involve both a constant multiplicative bias and an additive bias, which is spatially structured and time-varying. To perform inference, we use a Bayesian model averaging technique combined with integrated nested Laplace approximation. The model’s performance is evaluated through a simulation study, where it consistently results in better predictions and more accurate parameter estimates than models using only weather stations data or regression calibration, particularly in cases of sparse observational data. In the meteorological data application, the proposed data fusion model also outperforms these benchmark approaches, as demonstrated by leave-group-out cross-validation.},
  archive      = {J_JRSSSC},
  author       = {Villejo, Stephen Jun and Martino, Sara and Lindgren, Finn and Illian, Janine B},
  doi          = {10.1093/jrsssc/qlaf012},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1021-1056},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {A data fusion model for meteorological data using the INLA-SPDE method},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D-vine generalized additive model copula-based quantile regression with application to ensemble postprocessing. <em>JRSSSC</em>, <em>74</em>(4), 994-1020. (<a href='https://doi.org/10.1093/jrsssc/qlaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The D-vine copula-based quantile regression (DVQR) is a powerful tool for weather forecasting, as it is able to select informative predictor variables from a large set and takes account of nonlinear relationships among them. However, DVQR shows in its current form a lack in adaptively modelling strongly varying effects among variables, such as temporal and/or spatial effects. Consequently, we propose an extension of the current DVQR, where we specify the parameters of the bivariate copulas in the D-vine copula through Kendall’s τ to which additional covariates are linked. The parameterization of the correlation parameter allows generalized additive models (GAMs) to incorporate, e.g. linear, nonlinear, and spatial effects as well as interactions. The new method is called GAM-DVQR, and its performance is illustrated in a case study on postprocessing 2 m surface temperature ensemble weather forecasts. We investigate constant as well as time-dependent Kendall’s τ correlation models. The results indicate that the GAM-DVQR models are able to identify time-dependent correlations and significantly outperform state-of-the-art postprocessing methods. Furthermore, the introduced temporal parameterization allows a more economical and faster model estimation in comparison to DVQR using a sliding training window. To complement this article, we provide an R -package for our method called gamvinereg .},
  archive      = {J_JRSSSC},
  author       = {Jobst, David and Möller, Annette and Groß, Jürgen},
  doi          = {10.1093/jrsssc/qlaf011},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {994-1020},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {D-vine generalized additive model copula-based quantile regression with application to ensemble postprocessing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correcting for bias due to mismeasured exposure in mediation analysis with a survival outcome. <em>JRSSSC</em>, <em>74</em>(4), 969-993. (<a href='https://doi.org/10.1093/jrsssc/qlaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the impact of exposure measurement error on assessing mediation with a survival outcome modelled by Cox regression. We first derive the bias formulas of natural indirect and direct effects with a rare outcome and no exposure–mediator interaction. We then develop several calibration approaches to correct for the measurement error-induced bias, and generalize our methods to accommodate a common outcome and an exposure–mediator interaction. We apply the proposed methods to analyse the Health Professionals Follow-up Study (1986–2016) and evaluate the extent to which reduced body mass index mediates the protective effect of physical activity on risk of cardiovascular diseases.},
  archive      = {J_JRSSSC},
  author       = {Cheng, Chao and Spiegelman, Donna and Li, Fan},
  doi          = {10.1093/jrsssc/qlaf010},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {969-993},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Correcting for bias due to mismeasured exposure in mediation analysis with a survival outcome},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A gaussian sliding windows regression model for hydrological inference. <em>JRSSSC</em>, <em>74</em>(4), 946-968. (<a href='https://doi.org/10.1093/jrsssc/qlaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models are an essential tool to model, forecast, and understand the hydrological processes in watersheds. In particular, the understanding of time lags associated with the delay between rainfall occurrence and subsequent changes in streamflow is of high practical importance. Since water can take a variety of flow paths to generate streamflow, a series of distinct runoff pulses may combine to create the observed streamflow time series. Current state-of-the-art models are not able to sufficiently confront the problem complexity with interpretable parametrization, thus preventing novel insights about the dynamics of distinct flow paths from being formed. The proposed Gaussian Sliding Windows Regression Model targets this problem by combining the concept of multiple windows sliding along the time axis with multiple linear regression. The window kernels, which indicate the weights applied to different time lags, are implemented via Gaussian-shaped kernels. As a result, straightforward process inference can be achieved since each window can represent one flow path. Experiments on simulated and real-world scenarios underline that the proposed model achieves accurate parameter estimates and competitive predictive performance, while fostering explainable and interpretable hydrological modelling.},
  archive      = {J_JRSSSC},
  author       = {Schrunner, Stefan and Pishrobat, Parham and Janssen, Joseph and Jenul, Anna and Cao, Jiguo and Ameli, Ali A and Welch, William J},
  doi          = {10.1093/jrsssc/qlaf009},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {946-968},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {A gaussian sliding windows regression model for hydrological inference},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic self-learning approach to establishing individualized treatment rules from multiple benefit outcomes in a calcium supplementation trial. <em>JRSSSC</em>, <em>74</em>(4), 925-945. (<a href='https://doi.org/10.1093/jrsssc/qlaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In utero lead exposure poses risks to children’s neurobehavioral development. The Early Life Exposure in Mexico to ENvironmental Toxicants’ calcium supplementation trial studies the effect of calcium supplement in reducing maternal lead exposure to infants during pregnancy. An individualized treatment rule (ITR) is needed to guide pregnant women on taking calcium supplement. This article introduces a statistical learning method, synergistic self-learning (SS-learning), to tackle two challenges in deriving ITR with multiple outcomes, including heterogeneous multidimensional outcomes and complex missing data patterns. Applying SS-learning to the trial, important covariates were identified to form an ITR, expected to lead to higher lead reduction if implemented across the study population.},
  archive      = {J_JRSSSC},
  author       = {Zhou, Yiwang and Song, Peter X K},
  doi          = {10.1093/jrsssc/qlaf008},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {925-945},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Synergistic self-learning approach to establishing individualized treatment rules from multiple benefit outcomes in a calcium supplementation trial},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-resolution urban air quality monitoring from citizen science data with echo-state transformer networks. <em>JRSSSC</em>, <em>74</em>(4), 905-924. (<a href='https://doi.org/10.1093/jrsssc/qlaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Citizen science data for monitoring air pollution have recently emerged as a powerful yet under-explored resource to complement expensive and sparse national air quality monitors. In urban environments, these new data have the potential to allow for high-resolution and high-frequency forecasts, and thereby to provide an assessment of population exposure at neighbourhood level. The complex spatio-temporal structure of these data, however, requires new flexible methods that are also able to provide timely forecasts. In this work, we propose a novel method that first provides forecasts with a reservoir computing approach, an echo-state network, adjusts the forecast with a transformer network with attention mechanism and then merges the echo-state and transformer forecast into a combined network. The stochastic nature of the method allows for a fast and more accurate forecast then individual predictors as well as standard statistical methods. Simulation and application to San Francisco air pollution show how the proposed method is able to produce high-resolution urban maps of air quality. Additionally, we show how these forecasts can be used to provide neighbour-level exposure assessment using population data, a task that would not be achievable with sparse government-sponsored air quality networks.},
  archive      = {J_JRSSSC},
  author       = {Bonas, Matthew and Castruccio, Stefano},
  doi          = {10.1093/jrsssc/qlaf007},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {905-924},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {High-resolution urban air quality monitoring from citizen science data with echo-state transformer networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="nsr">NSR - 9</h2>
<ul>
<li><details>
<summary>
(2025). Isotropic thermal insulating cuttlebone-inspired MXene aerogel. <em>NSR</em>, <em>12</em>(10), nwaf342. (<a href='https://doi.org/10.1093/nsr/nwaf342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerogels are considered to be ideal thermal insulation materials due to their low thermal conductivity and highly porous structure, which can effectively reduce the energy consumption in aerospace, industry, and building applications. However, the anisotropic performance of most aerogels results in high thermal conductivity in the axial direction, and structures tend to collapse under the extreme temperature shock, leading to poor mechanical stability. Herein, we demonstrate a remarkable lightweight and isotropic thermal insulation aerogel material inspired by the wall–septa microstructure of cuttlebone. The cuttlebone-inspired MXene aerogel (CMA) is fabricated through freeze casting colloidal suspensions composed of Ti 3 C 2 T x MXene nanosheets, montmorillonite nanosheets, cellulose nanofibers, and polyvinyl alcohol. The CMA shows an ultralow thermal conductivity of 17.1 mW m −1 K −1 in the radial direction and 19.7 mW m −1 K −1 in the axial direction. Additionally, the CMA also exhibits a rapid sensing response, robust fire resistance/fire warning, and excellent electromagnetic interference (EMI) shielding of ∼61 dB in both radial and axial directions. The structural integrity and EMI shielding performance remain stable over a wide temperature range (−196°C to 1300°C). This performance indicates the potential of CMA as a promising alternative to existing thermal insulation under extreme conditions.},
  archive      = {J_NSR},
  author       = {Fu, Junsong and Lian, Wangwei and Deng, Yankang and Fang, Zixuan and Cheng, Qunfeng},
  doi          = {10.1093/nsr/nwaf342},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf342},
  shortjournal = {Nat. Sci. Rev.},
  title        = {Isotropic thermal insulating cuttlebone-inspired MXene aerogel},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How large language models need symbolism. <em>NSR</em>, <em>12</em>(10), nwaf339. (<a href='https://doi.org/10.1093/nsr/nwaf339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NSR},
  author       = {Deng, Xiaotie and Li, Hanyu},
  doi          = {10.1093/nsr/nwaf339},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf339},
  shortjournal = {Nat. Sci. Rev.},
  title        = {How large language models need symbolism},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Encoding of blink information via wireless contact lens for eye–machine interaction. <em>NSR</em>, <em>12</em>(10), nwaf338. (<a href='https://doi.org/10.1093/nsr/nwaf338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blinks controlled by ocular muscles and nerves can manifest as either involuntary physiological behaviors or volitional control actions, with the former serving spontaneous protective functions while the latter constitutes a biologically meaningful communicative signal. The encoding of blink information provides a novel eye–machine interaction (EMI) prototype within the realm of human–machine interaction, expanding human consciousness and capability boundaries. It facilitates motor and language rehabilitation, silent communication and even voluntary command execution. However, existing EMI devices face challenges related to wireless functionalities, ocular comfort and multi-route encoding/decoding orders. Here, we propose a wireless eye-wearable lens to encode conscious blink information via introduction of an RLC oscillating loop in the soft contact lens. The developed EMI contact lens incorporates a mechanosensitive capacitor, an inductive coil and the inherent loop resistance, generating characteristic resonance frequency for front-end capacitance signal transition or back-end control signal extraction. The EMI device delivers a sensitivity of 0.153 MHz/mmHg in the wide range of 0–70 mmHg for a normal intraocular pressure monitor and realizes conscious blink-based control command coding. A trial with participants having the EMI contact lens inserted demonstrates its wearability and biocompatibility. Finally, the five-route blink-based control command decoding mechanism is constructed via the EMI lens, linking blink counts to a drone's flight trajectory. The EMI contact lens offers an innovative prototype that transcends the capabilities of traditional brain–computer interfaces.},
  archive      = {J_NSR},
  author       = {Liu, Haiqing and Liu, Weijia and Du, Zhijian and Wu, Lifeng and Chen, Minyan and Gao, Zhiyi and Jiang, Kai and Li, La and Fan, Zhiyong and Shen, Guozhen},
  doi          = {10.1093/nsr/nwaf338},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf338},
  shortjournal = {Nat. Sci. Rev.},
  title        = {Encoding of blink information via wireless contact lens for eye–machine interaction},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multidirectional color palette of electrochromic metal–organic frameworks. <em>NSR</em>, <em>12</em>(10), nwaf326. (<a href='https://doi.org/10.1093/nsr/nwaf326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrochromic metal–organic frameworks (MOFs) combine advantages from both inorganic/organic electrochromic materials by enabling stable structures/performances as well as tunable functionalities. Their current design and synthesis, however, are inherently complicated, as they often involve amendments to the chromogenic components and/or MOF structures. Inspired by reticular chemistry, we herein demonstrate a multidirectional ‘color palette’ based on the colorful electrochromic behaviors across a total of 40 zirconium-based MOFs via systematically combining diverse naphthalene diimide (NDI)-based primary linkers (R-groups) and auxiliary linkers (X-groups), namely the NKM-908-R/NKM-906-R series ( csq / scu topology) and their corresponding NKM-908-R-TPDC-X/NKM-906-R-TPDC-X derivatives. A new broad color gamut over these robust MOF thin films was showcased via systematical crystal engineering and was thoroughly investigated. The benefits include enhancing the primary set of ‘colors’ from altering peripheral R-groups without redesigning the NDI core, introducing different X-groups as the secondary sets of ‘colors’ and topology adjustments to incorporate more X-linkers for intensification of the latter. Together, such designable color-mixing/changing sequences successfully mimicked the use of the routine color palette with molecular-level precision. This work not only holds great potential for further extension, but also provides unique insights into the development of next-generation electrochromic devices.},
  archive      = {J_NSR},
  author       = {Li, Cha and Zhang, Jinli and Lian, Yudong and Zhang, Kai and Zhang, Hao and Xu, Lin and Liu, Yanghe and Lang, Feifan and Pang, Jiandong and Bu, Xian-He},
  doi          = {10.1093/nsr/nwaf326},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf326},
  shortjournal = {Nat. Sci. Rev.},
  title        = {Multidirectional color palette of electrochromic metal–organic frameworks},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reforestation-induced aerosol cooling effects divergently modulated by various types of biogeophysical feedback. <em>NSR</em>, <em>12</em>(10), nwaf323. (<a href='https://doi.org/10.1093/nsr/nwaf323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reforestation and afforestation alter climate not only through biogeophysical processes such as changes in surface albedo, evapotranspiration and near‐surface turbulence, but also by modifying emissions of biogenic volatile organic compounds (BVOCs) that drive biogenic secondary organic aerosol (BSOA) formation. Using an Earth system model coupled with an advanced aerosol module, we quantify how biogeophysical feedback from vegetation change influences BVOC emissions, BSOA burden and aerosol radiative effects under future land‑use scenarios. Our results reveal that biogeophysical feedback either amplifies or offsets BSOA cooling, depending on regional climate–vegetation interactions. In regions where reduced surface albedo dominates, increasing temperature and BVOC emissions enhance BSOA burden and its radiative cooling. Conversely, in regions where updrafts and cloud formation are enhanced, reduced surface radiation suppresses BVOC emissions and offsets BSOA increases from vegetation changes alone. Globally, these types of feedback amplify BVOC emission changes in 52% of reforested areas but suppress them elsewhere, intensifying spatial heterogeneity in aerosol climate effects. These divergent feedback pathways introduce strong spatial heterogeneity and non-linearity into the BSOA–climate response. Incorporating such biogeophysical modulation of BSOAs is essential for designing reforestation strategies that maximize climate mitigation benefits.},
  archive      = {J_NSR},
  author       = {Zhu, Jialei and Penner, Joyce E and Liu, Hao and Guo, Qinghao and Liu, Yaxin and Deng, Junjun and Zhao, Xi and Liu, Cong-Qiang and Fu, Pingqing},
  doi          = {10.1093/nsr/nwaf323},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf323},
  shortjournal = {Nat. Sci. Rev.},
  title        = {Reforestation-induced aerosol cooling effects divergently modulated by various types of biogeophysical feedback},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The origin and fate of subslab partial melts at convergent margins. <em>NSR</em>, <em>12</em>(10), nwaf314. (<a href='https://doi.org/10.1093/nsr/nwaf314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channelized thin seismic low-velocity zones (LVZs) are observed beneath subducting slabs, extending from the lithosphere-asthenosphere boundary to the mantle transition zone (MTZ). While LVZs above slabs are well-explained by slab dehydration and flux melting, the origin and fate of these enigmatic subslab LVZs—persisting far deeper than expected—remain elusive. Here we use geodynamic modeling to show that subduction-induced wet upwellings from a water-bearing MTZ that generates dehydration melting feed the base of the lithosphere forming seismically detected LVZs. A similar process may occur when a pre-existing partially molten layer atop the 410-km discontinuity is displaced upward in response to subduction. These thin partially molten layers are successively entrained alongside the subducting slab and recycled back to MTZ depths. This mechanism supports a globally widespread scenario where subduction-induced upwelling of a water-rich MTZ can account for the observed mantle heterogeneities. Since minute quantities of melts may dramatically reduce viscosity, this process is likely to have non-negligible implications for the Earth's dynamics and recycling of the lithosphere into the deep mantle.},
  archive      = {J_NSR},
  author       = {Yang, Jianfeng and Faccenda, Manuele and Chen, Ling and Wang, Xin and Shen, Hao and VanderBeek, Brandon P and Zhao, Liang},
  doi          = {10.1093/nsr/nwaf314},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf314},
  shortjournal = {Nat. Sci. Rev.},
  title        = {The origin and fate of subslab partial melts at convergent margins},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid indian ocean warming fuels more frequent extreme pre-flood season rainfall over southern china. <em>NSR</em>, <em>12</em>(10), nwaf298. (<a href='https://doi.org/10.1093/nsr/nwaf298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2024, southern China faced its worst flooding during the pre-flood season (April–June), the first major rainfall season in East Asia, with considerable socioeconomic consequences. This extreme flooding is fueled by the unprecedented warming in the Indian Ocean, with a decaying moderate El Niño in the Pacific contributing weakly. Alarmingly, similar pre-flood season flooding events have become increasingly frequent in southern China over recent decades, posing unexpected risks to local communities. We demonstrate that the recent rapid Indian Ocean warming enhances local convection efficiency, leading to more frequent intense pre-flood season rainfall. As sea surface temperature in the Indian Ocean continues to rise in a warming world, it becomes increasingly crucial to understand its role in shaping regional extreme weather patterns for future climate adaptation and disaster management.},
  archive      = {J_NSR},
  author       = {Hou, Ruiqin and Zhang, Wenjun and Hu, Suqiong and Xu, Rongrong},
  doi          = {10.1093/nsr/nwaf298},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf298},
  shortjournal = {Nat. Sci. Rev.},
  title        = {Rapid indian ocean warming fuels more frequent extreme pre-flood season rainfall over southern china},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cascading impacts of the maunder minimum on rainfall and society in the joseon dynasty. <em>NSR</em>, <em>12</em>(10), nwaf283. (<a href='https://doi.org/10.1093/nsr/nwaf283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human civilization's evolution is shaped by climate change, with solar energy input into the Earth's system as the primary external driver. This influence should be more pronounced during agricultural stages and periods of extreme solar activity. The late Joseon Dynasty of Korea serves as an ideal civilization sample of political continuity and stability, maintaining a 285-year-long meteorological diary and rainfall records with a temporal resolution of up to 2 hours, perfectly encompassing the Maunder Minimum (MM). Here we quantitatively reconstruct the rainfall patterns, revealing a rare, nearly century-long drought around the MM, accompanied by decadal climate fluctuations correlated to the sunspot cycle. Quantitative socio-environmental analyses further indicate that the convergence of cold, arid conditions and heightened climate instability ultimately precipitated cascading ecological and societal crises during the late Joseon Dynasty. Our findings offer new perspectives for understanding and addressing the impacts of future periods of extreme solar activity on modern civilization.},
  archive      = {J_NSR},
  author       = {Wang, Yuqi and Wei, Yong and Shi, Feng and Li, Xichen and Yao, Zhonghua and Yan, Limei and Yue, Yaochen and Yang, Shiling and Lin, Wei and Pan, Yongxin and Guo, Zhengtang},
  doi          = {10.1093/nsr/nwaf283},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf283},
  shortjournal = {Nat. Sci. Rev.},
  title        = {Cascading impacts of the maunder minimum on rainfall and society in the joseon dynasty},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medial preoptic CCKAR mediates anxiety and aggression induced by chronic emotional stress in male mice. <em>NSR</em>, <em>12</em>(10), nwaf152. (<a href='https://doi.org/10.1093/nsr/nwaf152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anxiety disorders frequently accompany aggression, with their co-occurrence predicting greater functional impairment and poor prognosis. Nevertheless, the underlying neural mechanisms remain elusive, primarily due to a lack of appropriate animal models. Here, we designed a chronic conspecific outsider stress (CCS) model in which male mice underwent perceived social threats and exhibited increased anxiety-like behaviors accompanied by aggression. CCS led to Fos activation and hyperexcitability of GABAergic neurons in the medial preoptic area (mPOA). Inhibition of mPOA GABAergic (mPOA Gad2 ) neurons rescued CCS-induced anxiety-like and aggressive behaviors, whereas activating these cells induced susceptibility to CCS. Moreover, CCS upregulated the mRNA and protein expression of the sexual-dimorphic gene, cholecystokinin A receptor (CCKAR)-encoding Cckar gene in the mPOA. Importantly, the knock-down and overexpression of CCKAR in the mPOA Gad2 neurons had alleviating and promoting effects on anxiety-like and aggressive behaviors, aligning with decreased and increased excitability by the anxiolytic CCKAR antagonist MK-329 and the anxiogenic CCKAR agonist A71623 in mPOA Gad2 neurons, respectively. Overall, our study characterizes a novel mouse model of anxiety disorders accompanied by aggression and the neuronal subpopulation and molecular mediator of the aberrant behaviors provide potential targets of intervention for anxiety disorders with aggression.},
  archive      = {J_NSR},
  author       = {Tang, Meng-Yu and Zhang, Yan-Yi and Lin, Lin and Wu, Lin-Lin and Hu, Meng-Ting and Tan, Li-Heng and Yu, Chen-Xi and Wang, Hao and Yu, Yan-Qin and Ding, Yu and Han, Jia-Xuan and Hu, Hailan and Li, Xiao-Ming and Lian, Hong},
  doi          = {10.1093/nsr/nwaf152},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf152},
  shortjournal = {Nat. Sci. Rev.},
  title        = {Medial preoptic CCKAR mediates anxiety and aggression induced by chronic emotional stress in male mice},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
