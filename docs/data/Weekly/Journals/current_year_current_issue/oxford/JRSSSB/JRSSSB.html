<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRSSSB</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrsssb">JRSSSB - 20</h2>
<ul>
<li><details>
<summary>
(2025). Correction to: Convexity and measures of statistical association. <em>JRSSSB</em>, <em>87</em>(4), 1307. (<a href='https://doi.org/10.1093/jrsssb/qkaf040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf040},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1307},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Convexity and measures of statistical association},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Parameterizing and simulating from causal models. <em>JRSSSB</em>, <em>87</em>(4), 1306. (<a href='https://doi.org/10.1093/jrsssb/qkaf039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf039},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1306},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Parameterizing and simulating from causal models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Consistent and fast inference in compartmental models of epidemics using poisson approximate likelihoods. <em>JRSSSB</em>, <em>87</em>(4), 1305. (<a href='https://doi.org/10.1093/jrsssb/qkaf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf013},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1305},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Consistent and fast inference in compartmental models of epidemics using poisson approximate likelihoods},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convexity and measures of statistical association. <em>JRSSSB</em>, <em>87</em>(4), 1281-1304. (<a href='https://doi.org/10.1093/jrsssb/qkaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent investigations on the measures of statistical association highlight essential properties such as zero-independence (the measure is zero if and only if the random variables are independent), monotonicity under information refinement, and max-functionality (the measure of association is maximal if and only if we are in the presence of a deterministic (noiseless) dependence). An open question concerns the reasons why measures of statistical associations satisfy one or more of those properties but not others. We show that convexity plays a central role in all properties. Convexity plus a form of strictness (that we are to define) are necessary and sufficient for zero-independence, and convexity and strict convexity on Dirac masses are necessary and sufficient for max-functionality. We apply the findings to study the families of measures of statistical association based on Csiszár divergences, optimal transport, kernels, as well as Chatterjee’s new correlation coefficient. We further discuss the role of convexity in guaranteeing the asymptotic unbiasedness of given data estimators, prove a central limit theorem for those estimators under independence, and show the rate of convergence under arbitrary dependence. We demonstrate the findings with numerical simulations in a multivariate response context.},
  archive      = {J_JRSSSB},
  author       = {Borgonovo, Emanuele and Figalli, Alessio and Ghosal, Promit and Plischke, Elmar and Savaré, Giuseppe},
  doi          = {10.1093/jrsssb/qkaf018},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1281-1304},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Convexity and measures of statistical association},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-resolution subsampling for linear classification with massive data. <em>JRSSSB</em>, <em>87</em>(4), 1260-1280. (<a href='https://doi.org/10.1093/jrsssb/qkaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling is one of the popular methods to balance statistical efficiency and computational efficiency in the big data era. Most approaches aim to select informative or representative sample points to achieve good overall information of the full data. The present work takes the view that sampling techniques are recommended for the region we focus on and summary measures are enough to collect the information for the rest according to a well-designed data partitioning. We propose a subsampling strategy that collects global information described by summary measures and local information obtained from selected subsample points. Thus, we call it multi-resolution subsampling. We show that the proposed method leads to a more efficient subsample-based estimator for general linear classification problems. Some asymptotic properties of the proposed method are established and connections to existing subsampling procedures are explored. Finally, we illustrate the proposed subsampling strategy via simulated and real-world examples.},
  archive      = {J_JRSSSB},
  author       = {Chen, Haolin and Dette, Holger and Yu, Jun},
  doi          = {10.1093/jrsssb/qkaf017},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1260-1280},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Multi-resolution subsampling for linear classification with massive data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence on the focal: Conformal prediction with selection-conditional coverage. <em>JRSSSB</em>, <em>87</em>(4), 1239-1259. (<a href='https://doi.org/10.1093/jrsssb/qkaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction builds marginally valid prediction intervals that cover the unknown outcome of a randomly drawn test point with a prescribed probability. However, in practice, data-driven methods are often used to identify specific test unit(s) of interest, requiring uncertainty quantification tailored to these focal units. In such cases, marginally valid conformal prediction intervals may fail to provide valid coverage for the focal unit(s) due to selection bias. This article presents a general framework for constructing a prediction set with finite-sample exact coverage, conditional on the unit being selected by a given procedure. The general form of our method accommodates arbitrary selection rules that are invariant to the permutation of the calibration units and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We also work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization-based selection, selection based on conformal p -values, and selection based on properties of preliminary conformal prediction sets. The performance of our methods is demonstrated via applications in drug discovery and health risk prediction.},
  archive      = {J_JRSSSB},
  author       = {Jin, Ying and Ren, Zhimei},
  doi          = {10.1093/jrsssb/qkaf016},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1239-1259},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Confidence on the focal: Conformal prediction with selection-conditional coverage},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased and consistent nested sampling via sequential monte carlo. <em>JRSSSB</em>, <em>87</em>(4), 1221-1238. (<a href='https://doi.org/10.1093/jrsssb/qkaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of sequential Monte Carlo methods which reformulates the essence of the nested sampling (NS) method of Skilling in terms of sequential Monte Carlo techniques. Two new algorithms are proposed: nested sampling via sequential Monte Carlo (NS-SMC) and adaptive nested sampling via sequential Monte Carlo (ANS-SMC). The new framework allows convergence results to be obtained in the setting when Markov chain Monte Carlo (MCMC) is used to produce new samples. An additional benefit is that marginal-likelihood (normalizing constant) estimates given by NS-SMC are unbiased. In contrast to NS, the analysis of our proposed algorithms does not require the (unrealistic) assumption that the simulated samples be independent. We show that a minor adjustment to our ANS-SMC algorithm recovers the original NS algorithm, which provides insights as to why NS seems to produce accurate estimates despite a typical violation of its assumptions. A numerical study is conducted where the performance of the proposed algorithms and temperature-annealed SMC is compared on challenging problems. Code for the experiments is made available online at https://github.com/LeahPrice/SMC-NS .},
  archive      = {J_JRSSSB},
  author       = {Salomone, Robert and South, Leah F and Drovandi, Christopher and Kroese, Dirk P and Johansen, Adam M},
  doi          = {10.1093/jrsssb/qkaf015},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1221-1238},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Unbiased and consistent nested sampling via sequential monte carlo},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential monte carlo testing by betting. <em>JRSSSB</em>, <em>87</em>(4), 1200-1220. (<a href='https://doi.org/10.1093/jrsssb/qkaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a Monte Carlo test, the observed dataset is fixed, and several resampled or permuted versions of the dataset are generated in order to test a null hypothesis that the original dataset is exchangeable with the resampled/permuted ones. Sequential Monte Carlo tests aim to save computational resources by generating these additional datasets sequentially one by one and potentially stopping early. While earlier tests yield valid inference at a particular prespecified stopping rule, our work develops a new anytime-valid Monte Carlo test that can be continuously monitored, yielding a p -value or e -value at any stopping time possibly not specified in advance. It generalizes the well-known method by Besag and Clifford, allowing it to stop at any time, but also encompasses new sequential Monte Carlo tests that tend to stop sooner under the null and alternative without compromising power. The core technical advance is the development of new test martingales for testing exchangeability against a very particular alternative based on a testing by betting technique. The proposed betting strategies are guided by the derivation of a simple log-optimal betting strategy, have closed-form expressions for the wealth process, provable guarantees on resampling risk, and display excellent power in practice.},
  archive      = {J_JRSSSB},
  author       = {Fischer, Lasse and Ramdas, Aaditya},
  doi          = {10.1093/jrsssb/qkaf014},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1200-1220},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Sequential monte carlo testing by betting},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework for cutting feedback within modularized bayesian inference. <em>JRSSSB</em>, <em>87</em>(4), 1171-1199. (<a href='https://doi.org/10.1093/jrsssb/qkaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard Bayesian inference enables building models that combine information from various sources, but this inference may not be reliable if components of the model are misspecified. Cut inference, a particular type of modularized Bayesian inference, is an alternative that splits a model into modules and cuts the feedback from any suspect module. Previous studies have focused on a two module case, but a more general definition of a ‘module’ remains unclear. We present a formal definition of a ‘module’ and discuss its properties. We formulate methods for identifying modules; determining the order of modules; and building the cut distribution that should be used for cut inference within an arbitrary directed acyclic graph structure. We justify the cut distribution by showing that it not only cuts the feedback but also is the best approximation to the joint distribution satisfying this condition in Kullback–Leibler divergence. We also extend cut inference for the two module case to a general multiple-module case via a sequential splitting technique and demonstrate this via illustrative applications.},
  archive      = {J_JRSSSB},
  author       = {Liu, Yang and Goudie, Robert J B},
  doi          = {10.1093/jrsssb/qkaf012},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1171-1199},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {A general framework for cutting feedback within modularized bayesian inference},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strong oracle guarantees for partial penalized tests of high-dimensional generalized linear models. <em>JRSSSB</em>, <em>87</em>(4), 1150-1170. (<a href='https://doi.org/10.1093/jrsssb/qkaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial penalized tests provide flexible approaches to testing linear hypotheses in high-dimensional generalized linear models. However, because the estimators used in these tests are local minimizers of potentially nonconvex folded-concave penalized objectives, the solutions one computes in practice may not coincide with the unknown local minima for which we have nice theoretical guarantees. To close this gap between theory and computation, we introduce local linear approximation (LLA) algorithms to compute the full and reduced model estimators for these tests and develop a theory specifically for the LLA solutions. We prove that our LLA algorithms converge to oracle estimators for the full and reduced models in two steps with overwhelming probability. We then leverage this strong oracle result and the asymptotic properties of the oracle estimators to show that the partial penalized test statistics evaluated at the LLA solutions are approximately chi-square in large samples, giving us guarantees for the tests using specific computed solutions and thereby closing the theoretical gap. In simulations, we find that our LLA tests closely agree with the oracle tests and compare favourably with alternative high-dimensional inference procedures. We demonstrate the flexibility of our LLA tests with two high-dimensional data applications.},
  archive      = {J_JRSSSB},
  author       = {Jacobson, Tate},
  doi          = {10.1093/jrsssb/qkaf010},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1150-1170},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Strong oracle guarantees for partial penalized tests of high-dimensional generalized linear models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian penalized empirical likelihood and markov chain monte carlo sampling. <em>JRSSSB</em>, <em>87</em>(4), 1127-1149. (<a href='https://doi.org/10.1093/jrsssb/qkaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce a novel methodological framework called Bayesian penalized empirical likelihood (BPEL), designed to address the computational challenges inherent in empirical likelihood (EL) approaches. Our approach has two primary objectives: (i) to enhance the inherent flexibility of EL in accommodating diverse model conditions, and (ii) to facilitate the use of well-established Markov Chain Monte Carlo sampling schemes as a convenient alternative to the complex optimization typically required for statistical inference using EL. To achieve the first objective, we propose a penalized approach that regularizes the Lagrange multipliers, significantly reducing the dimensionality of the problem while accommodating a comprehensive set of model conditions. For the second objective, our study designs and thoroughly investigates two popular sampling schemes within the BPEL context. We demonstrate that the BPEL framework is highly flexible and efficient, enhancing the adaptability and practicality of EL methods. Our study highlights the practical advantages of using sampling techniques over traditional optimization methods for EL problems, showing rapid convergence to the global optima of posterior distributions and ensuring the effective resolution of complex statistical inference challenges.},
  archive      = {J_JRSSSB},
  author       = {Chang, Jinyuan and Tang, Cheng Yong and Zhu, Yuanzheng},
  doi          = {10.1093/jrsssb/qkaf009},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1127-1149},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Bayesian penalized empirical likelihood and markov chain monte carlo sampling},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal prediction with conditional guarantees. <em>JRSSSB</em>, <em>87</em>(4), 1100-1126. (<a href='https://doi.org/10.1093/jrsssb/qkaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of constructing distribution-free prediction sets with finite-sample conditional guarantees. Prior work has shown that it is impossible to provide exact conditional coverage universally in finite samples. Thus, most popular methods only guarantee marginal coverage over the covariates or are restricted to a limited set of conditional targets, e.g. coverage over a finite set of prespecified subgroups. This paper bridges this gap by defining a spectrum of problems that interpolate between marginal and conditional validity. We motivate these problems by reformulating conditional coverage as coverage over a class of covariate shifts. When the target class of shifts is finite-dimensional, we show how to simultaneously obtain exact finite-sample coverage over all possible shifts. For example, given a collection of subgroups, our prediction sets guarantee coverage over each group. For more flexible, infinite-dimensional classes where exact coverage is impossible, we provide a procedure for quantifying the coverage errors of our algorithm. Moreover, by tuning interpretable hyperparameters, we allow the practitioner to control the size of these errors across shifts of interest. Our methods can be incorporated into existing split conformal inference pipelines, and thus can be used to quantify the uncertainty of modern black-box algorithms without distributional assumptions.},
  archive      = {J_JRSSSB},
  author       = {Gibbs, Isaac and Cherian, John J and Candès, Emmanuel J},
  doi          = {10.1093/jrsssb/qkaf008},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1100-1126},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Conformal prediction with conditional guarantees},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A conditioning tactic that increases design sensitivity in observational block designs. <em>JRSSSB</em>, <em>87</em>(4), 1085-1099. (<a href='https://doi.org/10.1093/jrsssb/qkaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an observational block design, there are I blocks of J individuals, typically with one treated individual and J − 1 controls; however, unlike a randomized block design, individuals were not randomly assigned to treatment or control. To be convincing, an observational block design must demonstrate that an ostensible treatment effect is not actually a consequence of small or moderate unmeasured biases of treatment assignment in the absence of a treatment effect. It is known that weighting to ignore blocks with a small range of responses increases the ability to distinguish a treatment effect from a bias in treatment assignment—that is, it increases the design sensitivity. Here, it is shown that a new tactic further increases design sensitivity. The new tactic involves a conditional statistic, such that blocks with moderately large ranges are considered conditionally given that the treated individual has either the largest or smallest response in the block. The new tactic is explored: (i) in terms of an asymptotic measure, the design sensitivity, (ii) in simulation of the power of a sensitivity analysis in finite samples, and (iii) in an example. Adaptive inference is briefly discussed. An R package weightedRank implements the method, contains the data, and reproduces the empirical results.},
  archive      = {J_JRSSSB},
  author       = {Rosenbaum, Paul R},
  doi          = {10.1093/jrsssb/qkaf007},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1085-1099},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {A conditioning tactic that increases design sensitivity in observational block designs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive experiments toward learning treatment effect heterogeneity. <em>JRSSSB</em>, <em>87</em>(4), 1055-1084. (<a href='https://doi.org/10.1093/jrsssb/qkaf006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding treatment effect heterogeneity has become an increasingly popular task in various fields, as it helps design personalized advertisements in e-commerce or targeted treatment in biomedical studies. However, most of the existing work in this research area focused on either analysing observational data based on strong causal assumptions or conducting post hoc analyses of randomized controlled trial data, and there has been limited effort dedicated to the design of randomized experiments specifically for uncovering treatment effect heterogeneity. In the manuscript, we develop a framework for designing and analysing response adaptive experiments toward better learning treatment effect heterogeneity. Concretely, we provide response adaptive experimental design frameworks that sequentially revise the data collection mechanism according to the accrued evidence during the experiment. Such design strategies allow for the identification of subgroups with the largest treatment effects with enhanced statistical efficiency. The proposed frameworks not only unify adaptive enrichment designs and response-adaptive randomization designs but also complement A/B test designs in e-commerce and randomized trial designs in clinical settings. We demonstrate the merit of our design with theoretical justifications and in simulation studies with synthetic e-commerce and clinical trial data.},
  archive      = {J_JRSSSB},
  author       = {Wei, Waverly and Ma, Xinwei and Wang, Jingshen},
  doi          = {10.1093/jrsssb/qkaf006},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1055-1084},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Adaptive experiments toward learning treatment effect heterogeneity},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric posterior corrections. <em>JRSSSB</em>, <em>87</em>(4), 1025-1054. (<a href='https://doi.org/10.1093/jrsssb/qkaf005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach to semiparametric inference using corrected posterior distributions. The method allows us to leverage the adaptivity, regularization, and predictive power of nonparametric Bayesian procedures to estimate low-dimensional functionals of interest without being restricted by the holistic Bayesian formalism. Starting from a conventional posterior on the whole data-generating distribution, we correct the marginal posterior for each functional of interest with the help of the Bayesian bootstrap. We provide conditions for the resulting one-step posterior to possess calibrated frequentist properties and specialize the results for several canonical examples: the integrated squared density, the mean of a missing-at-random outcome, and the average causal treatment effect on the treated. The procedure is computationally attractive, requiring only a simple, efficient postprocessing step that can be attached onto any arbitrary posterior sampling algorithm. Using the ACIC 2016 causal data analysis competition, we illustrate that our approach can outperform the existing state-of-the-art through the propagation of Bayesian uncertainty.},
  archive      = {J_JRSSSB},
  author       = {Yiu, Andrew and Fong, Edwin and Holmes, Chris and Rousseau, Judith},
  doi          = {10.1093/jrsssb/qkaf005},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1025-1054},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Semiparametric posterior corrections},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized empirical likelihood test for ultra-high dimensional means under general covariances. <em>JRSSSB</em>, <em>87</em>(4), 1001-1024. (<a href='https://doi.org/10.1093/jrsssb/qkaf004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a calibrated empirical likelihood test for ultra-high dimensional means that incorporates multiple projections. Under weak moment conditions on the distributions of data, we analyse all possible asymptotic distributions of the proposed test statistic in different scenarios. To determine the critical value and enhance test power, we employ the random symmetrization method based on the group of sign flips and use multiple selected projections. The test can still maintain the significance level asymptotically, even in the presence of heterogeneity in the data distribution. Moreover, the proposed test procedure allows for general covariance structures and ultra-high dimensional regimes. Further, the power function reveals the relation with the projection term in an asymptotic sense such that we can select suitable projections to achieve good power in various scenarios. A quasi-Newton algorithm is introduced to reduce the computational cost arising from the intensive optimizations required for computing empirical likelihood. Numerical studies evidence the promising performance of the proposed test compared with existing tests.},
  archive      = {J_JRSSSB},
  author       = {Chen, Yuexin and Zhu, Lixing and Xu, Wangli},
  doi          = {10.1093/jrsssb/qkaf004},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1001-1024},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Randomized empirical likelihood test for ultra-high dimensional means under general covariances},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmentation invariant manifold learning. <em>JRSSSB</em>, <em>87</em>(4), 978-1000. (<a href='https://doi.org/10.1093/jrsssb/qkaf003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is a widely used technique and an essential ingredient in the recent advance in self-supervised representation learning. By preserving the similarity between augmented data, the resulting data representation can improve various downstream analyses and achieve state-of-the-art performance in many applications. Despite the empirical effectiveness, most existing methods lack theoretical understanding under a general nonlinear setting. To fill this gap, we develop a statistical framework on a low-dimensional product manifold to model the data augmentation transformation. Under this framework, we introduce a new representation learning method called augmentation invariant manifold learning and design a computationally efficient algorithm by reformulating it as a stochastic optimization problem. Compared with existing self-supervised methods, the new method simultaneously exploits the manifold’s geometric structure and invariant property of augmented data and has an explicit theoretical guarantee. Our theoretical investigation characterizes the role of data augmentation in the proposed method and reveals why and how the data representation learned from augmented data can improve the k -nearest neighbour classifier in the downstream analysis, showing that a more complex data augmentation leads to more improvement in downstream analysis. Finally, numerical experiments on simulated and real data sets are presented to demonstrate the merit of the proposed method.},
  archive      = {J_JRSSSB},
  author       = {Wang, Shulei},
  doi          = {10.1093/jrsssb/qkaf003},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {978-1000},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Augmentation invariant manifold learning},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-phase rejective sampling and its asymptotic properties. <em>JRSSSB</em>, <em>87</em>(4), 957-977. (<a href='https://doi.org/10.1093/jrsssb/qkaf002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rejective sampling improves design and estimation efficiency of single-phase sampling when auxiliary information in a finite population is available. When such auxiliary information is unavailable, we propose to use two-phase rejective sampling (TPRS), which involves measuring auxiliary variables for the sample of units in the first phase, followed by the implementation of rejective sampling for the outcome in the second phase. We explore the asymptotic design properties of double expansion and regression estimators under TPRS. We show that TPRS enhances the efficiency of the double-expansion estimator, rendering it comparable to a regression estimator. We further refine the design to accommodate varying importance of covariates and extend it to multi-phase sampling. We start with the theory for the population mean and then extend the theory to parameters defined by general estimating equations. Our asymptotic results for TPRS immediately cover the existing single-phase rejective sampling, under which the asymptotic theory has not been fully established.},
  archive      = {J_JRSSSB},
  author       = {Yang, Shu and Ding, Peng},
  doi          = {10.1093/jrsssb/qkaf002},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {957-977},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Two-phase rejective sampling and its asymptotic properties},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analytic natural gradient updates for cholesky factor in gaussian variational approximation. <em>JRSSSB</em>, <em>87</em>(4), 930-956. (<a href='https://doi.org/10.1093/jrsssb/qkaf001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural gradients can improve convergence in stochastic variational inference significantly but inverting the Fisher information matrix is daunting in high dimensions. Moreover, in Gaussian variational approximation, natural gradient updates of the precision matrix do not ensure positive definiteness. To tackle this issue, we derive analytic natural gradient updates of the Cholesky factor of the covariance or precision matrix and consider sparsity constraints representing different posterior correlation structures. Stochastic normalized natural gradient ascent with momentum is proposed for implementation in generalized linear mixed models and deep neural networks.},
  archive      = {J_JRSSSB},
  author       = {Tan, Linda S L},
  doi          = {10.1093/jrsssb/qkaf001},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {930-956},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Analytic natural gradient updates for cholesky factor in gaussian variational approximation},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selecting informative conformal prediction sets with false coverage rate control. <em>JRSSSB</em>, <em>87</em>(4), 909-929. (<a href='https://doi.org/10.1093/jrsssb/qkae120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised learning, including regression and classification, conformal methods provide prediction sets for the outcome/label with finite sample coverage for any machine learning predictor. We consider here the case where such prediction sets come after a selection process. The selection process requires that the selected prediction sets be ‘informative’ in a well-defined sense. We consider both the classification and regression settings where the analyst may consider as informative only the sample with prediction sets small enough, excluding null values, or obeying other appropriate ‘monotone’ constraints. We develop a unified framework for building such informative conformal prediction sets while controlling the false coverage rate (FCR) on the selected sample. While conformal prediction sets after selection have been the focus of much recent literature in the field, the new introduced procedures, called InfoSP and InfoSCOP , are to our knowledge the first ones providing FCR control for informative prediction sets. We show the usefulness of our resulting procedures on real and simulated data.},
  archive      = {J_JRSSSB},
  author       = {Gazin, Ulysse and Heller, Ruth and Marandon, Ariane and Roquain, Etienne},
  doi          = {10.1093/jrsssb/qkae120},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {909-929},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Selecting informative conformal prediction sets with false coverage rate control},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
