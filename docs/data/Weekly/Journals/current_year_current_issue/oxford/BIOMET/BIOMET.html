<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMET</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomet">BIOMET - 15</h2>
<ul>
<li><details>
<summary>
(2025). Debiased learning of the causal net benefit with censored event time data. <em>BIOMET</em>, <em>112</em>(3), asaf051. (<a href='https://doi.org/10.1093/biomet/asaf051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cox regression is the default approach to evaluating the (relative) effect of two treatments on a survival endpoint. This standard framework has nonetheless been criticized for its canonical effect measure, the hazard ratio, having a subtle interpretation, thereby hindering policy-making. This in turn has prompted interest in other effects measures, such as the difference in restricted mean survival time, the net benefit and the win ratio, which have become increasingly popular. Developments in estimation and inference for the net benefit and win ratio proceed either under a semiparametric model, at the risk of bias due to model misspecification, or in a nonparametric model, but without the flexibility to adjust for covariates. In this paper, we overcome these challenges by introducing a scalar, model-free measure of conditional causal net benefit in terms of counterfactuals and developing a debiased estimator based on its efficient influence function. This estimator is root- n consistent and asymptotically model-free by flexibly enabling data-adaptive learning (e.g., machine learning) of the dependence of treatment, survival and censoring time on baseline covariates. By incorporating such covariates, the proposed estimators can improve the efficiency of randomized trial analyses, as well as correct for confounding and censoring bias in both randomized and observational studies. We also propose variations of the considered estimand (and estimator) that have a more favourable efficiency bound. The proposed method is illustrated by simulation studies and analysis of breast cancer data.},
  archive      = {J_BIOMET},
  author       = {Martinussen, Torben and Vansteelandt, Stijn},
  doi          = {10.1093/biomet/asaf051},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf051},
  shortjournal = {Biometrika},
  title        = {Debiased learning of the causal net benefit with censored event time data},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying and bounding the probability of necessity for causes of effects with ordinal outcomes. <em>BIOMET</em>, <em>112</em>(3), asaf049. (<a href='https://doi.org/10.1093/biomet/asaf049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the existing causal inference literature focuses on the forward-looking perspective by estimating effects of causes, the backward-looking perspective can provide insights into causes of effects. In backward-looking causal inference, the probability of necessity measures the probability that a certain event is caused by the treatment, given the observed treatment and outcome. Most existing results focus on binary outcomes. Motivated by applications with ordinal outcomes, we propose a general definition of the probability of necessity. However, identifying the probability of necessity is challenging because it involves the joint distribution of the potential outcomes. We propose the novel assumption of a monotonic incremental treatment effect to identify the probability of necessity with ordinal outcomes. We also discuss the testable implications of this key identification assumption. When it fails, we derive explicit formulas of the sharp large-sample bounds on the probability of necessity.},
  archive      = {J_BIOMET},
  author       = {Zhang, Chao and Geng, Zhi and Li, Wei and Ding, Peng},
  doi          = {10.1093/biomet/asaf049},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf049},
  shortjournal = {Biometrika},
  title        = {Identifying and bounding the probability of necessity for causes of effects with ordinal outcomes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudolikelihood estimators for graphical models: Existence and uniqueness. <em>BIOMET</em>, <em>112</em>(3), asaf045. (<a href='https://doi.org/10.1093/biomet/asaf045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical and sparse (inverse) covariance models have found widespread use in modern sample-starved high-dimensional applications. A part of their wide appeal stems from the significantly low sample sizes required for existence of the estimators, especially in comparison with the classical full covariance model. For undirected Gaussian graphical models, the minimum sample size required for the existence of maximum likelihood estimators had been an open question for almost half a century, and has recently been addressed in a series of works (Uhler, 2012; Ben-David, 2015; Gross & Sullivant, 2018). The very same question for pseudolikelihood estimators has remained unsolved ever since their introduction in the 1970s. Pseudolikelihood estimators have recently received renewed attention as they impose fewer restrictive assumptions and have better computational tractability, improved statistical performance and appropriateness in high-dimensional applications, thus renewing interest in this longstanding problem. In this paper, we undertake a comprehensive study of this open problem within the context of the two classes of pseudolikelihood methods proposed in the literature. We provide a precise answer to this question for both pseudolikelihood approaches and relate the corresponding solutions to their Gaussian counterparts.},
  archive      = {J_BIOMET},
  author       = {Roycraft, B and Rajaratnam, B},
  doi          = {10.1093/biomet/asaf045},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf045},
  shortjournal = {Biometrika},
  title        = {Pseudolikelihood estimators for graphical models: Existence and uniqueness},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent and scalable composite likelihood estimation of probit models with crossed random effects. <em>BIOMET</em>, <em>112</em>(3), asaf037. (<a href='https://doi.org/10.1093/biomet/asaf037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of crossed random effects models commonly incurs computational costs that grow faster than linearly in the sample size N ⁠ , often as fast as Ω ( N 3 / 2 ) ⁠ , making them unsuitable for large datasets. For non-Gaussian responses, integrating out the random effects to obtain a marginal likelihood poses significant challenges, especially for high-dimensional integrals for which the Laplace approximation may not be accurate. In this article we develop a composite likelihood approach to probit models that replaces the crossed random effects model with some hierarchical models that require only one-dimensional integrals. We show how to consistently estimate the crossed effects model parameters from the hierarchical model fits. We find that the computation scales linearly in the sample size. The method is illustrated by applying it to approximately five million observations from Stitch Fix, where the crossed effects formulation would require an integral of dimension larger than 700 000 ⁠ .},
  archive      = {J_BIOMET},
  author       = {Bellio, R and Ghosh, S and Owen, A B and Varin, C},
  doi          = {10.1093/biomet/asaf037},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf037},
  shortjournal = {Biometrika},
  title        = {Consistent and scalable composite likelihood estimation of probit models with crossed random effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive performance of power posteriors. <em>BIOMET</em>, <em>112</em>(3), asaf034. (<a href='https://doi.org/10.1093/biomet/asaf034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the impact of using tempered likelihoods in the production of posterior predictions. While the choice of temperature has an impact on predictive performance in small samples, we formally show that in moderate-to-large samples, tempering does not impact posterior predictions.},
  archive      = {J_BIOMET},
  author       = {McLatchie, Y and Fong, E and Frazier, D T and Knoblauch, J},
  doi          = {10.1093/biomet/asaf034},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf034},
  shortjournal = {Biometrika},
  title        = {Predictive performance of power posteriors},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bias correction of quadratic spectral estimators. <em>BIOMET</em>, <em>112</em>(3), asaf033. (<a href='https://doi.org/10.1093/biomet/asaf033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three cardinal, statistically consistent families of nonparametric estimators for the power spectral density of a time series are the lag-window, multitaper and Welch estimators. However, when estimating power spectral densities from a finite sample, each can be subject to nonignorable bias. Astfalck et al. (2024) developed a method that offers significant bias reduction for finite samples for Welch’s estimator, which this article extends to the larger family of quadratic estimators, thus providing similar theory for bias correction of lag-window and multitaper estimators as well as combinations thereof. Importantly, this theory may be used in conjunction with any and all tapers and lag-sequences designed for bias reduction, and so should be seen as an extension to valuable work in these fields, rather than a supplanting methodology. The order of computation is larger than the |$ {O}(n\log n) $| which is typical in spectral analyses, but it is not insurmountable in practice. Simulation studies support the theory with comparisons across variations of quadratic estimators.},
  archive      = {J_BIOMET},
  author       = {Astfalck, Lachlan C and Sykulski, Adam M and Cripps, Edward J},
  doi          = {10.1093/biomet/asaf033},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf033},
  shortjournal = {Biometrika},
  title        = {Bias correction of quadratic spectral estimators},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integer programming for learning directed acyclic graphs from nonidentifiable gaussian models. <em>BIOMET</em>, <em>112</em>(3), asaf032. (<a href='https://doi.org/10.1093/biomet/asaf032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of learning directed acyclic graphs from continuous observational data, generated according to a linear Gaussian structural equation model. State-of-the-art structure learning methods for this setting have at least one of the following shortcomings: (i) they cannot provide optimality guarantees and can suffer from learning suboptimal models; (ii) they rely on the stringent assumption that the noise is homoscedastic, and hence the underlying model is fully identifiable. We overcome these shortcomings and develop a computationally efficient mixed-integer programming framework for learning medium-sized problems that accounts for arbitrary heteroscedastic noise. We present an early stopping criterion under which we can terminate the branch-and-bound procedure to achieve an asymptotically optimal solution and establish the consistency of this approximate solution. In addition, we show via numerical experiments that our method outperforms state-of-the-art algorithms and is robust to noise heteroscedasticity, whereas the performance of some competing methods deteriorates under strong violations of the identifiability assumption. The software implementation of our method is available as the Python package micodag .},
  archive      = {J_BIOMET},
  author       = {Xu, Tong and Taeb, Armeen and Küçükyavuz, Simge and Shojaie, Ali},
  doi          = {10.1093/biomet/asaf032},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf032},
  shortjournal = {Biometrika},
  title        = {Integer programming for learning directed acyclic graphs from nonidentifiable gaussian models},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a turnkey approach for unbiased monte carlo estimation of smooth functions of expectations. <em>BIOMET</em>, <em>112</em>(3), asaf030. (<a href='https://doi.org/10.1093/biomet/asaf030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a smooth function f ⁠ , we develop a general approach to turn Monte Carlo samples with expectation m into an unbiased estimate of f ( m ) ⁠ . Specifically, we develop estimators that are based on randomly truncating the Taylor series expansion of f and estimating the coefficients of the truncated series. We derive their properties and propose a strategy to set their tuning parameters (which depend on m ⁠ ) automatically, with a view to making the whole approach simple to use. We develop our methods for the specific functions f ( x ) = log ⁡ x and f ( x ) = 1 / x ⁠ , as they arise in several statistical applications such as maximum likelihood estimation of latent variable models and Bayesian inference for unnormalized models. Detailed numerical studies are performed for a range of applications to determine how competitive and reliable the proposed approach is.},
  archive      = {J_BIOMET},
  author       = {Chopin, Nicolas and Crucinio, Francesca R and Singh, Sumeetpal S},
  doi          = {10.1093/biomet/asaf030},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf030},
  shortjournal = {Biometrika},
  title        = {Towards a turnkey approach for unbiased monte carlo estimation of smooth functions of expectations},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general form of covariate adjustment in clinical trials under covariate-adaptive randomization. <em>BIOMET</em>, <em>112</em>(3), asaf029. (<a href='https://doi.org/10.1093/biomet/asaf029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized clinical trials, adjusting for baseline covariates can improve credibility and efficiency for demonstrating and quantifying treatment effects. This article studies the augmented inverse propensity weighted estimator, which is a general form of covariate adjustment that uses linear, generalized linear and nonparametric or machine learning models for the conditional mean of the response given covariates. Under covariate-adaptive randomization, we establish general theorems that show a complete picture of the asymptotic normality, efficiency gain and applicability of augmented inverse propensity weighted estimators. In particular, we provide for the first time a rigorous theoretical justification of using machine learning methods with cross-fitting for dependent data under covariate-adaptive randomization. Based on the general theorems, we offer insights on the conditions for guaranteed efficiency gain and universal applicability under different randomization schemes, which also motivate a joint calibration strategy using some constructed covariates after applying augmented inverse propensity weighted estimators.},
  archive      = {J_BIOMET},
  author       = {Bannick, Marlena S and Shao, Jun and Liu, Jingyi and Du, Yu and Yi, Yanyao and Ye, Ting},
  doi          = {10.1093/biomet/asaf029},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf029},
  shortjournal = {Biometrika},
  title        = {A general form of covariate adjustment in clinical trials under covariate-adaptive randomization},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic factor analysis of high-dimensional recurrent events. <em>BIOMET</em>, <em>112</em>(3), asaf028. (<a href='https://doi.org/10.1093/biomet/asaf028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event time data arise in many studies, including in biomedicine, public health, marketing and social media analysis. High-dimensional recurrent event data involving many event types and observations have become prevalent with advances in information technology. This article proposes a semiparametric dynamic factor model for the dimension reduction of high-dimensional recurrent event data. The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies. A nearly rate-optimal smoothing-based estimator is proposed. An information criterion that consistently selects the number of factors is also developed. Simulation studies demonstrate the effectiveness of these inference tools. The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained.},
  archive      = {J_BIOMET},
  author       = {Chen, F and Chen, Y and Ying, Z and Zhou, K},
  doi          = {10.1093/biomet/asaf028},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf028},
  shortjournal = {Biometrika},
  title        = {Dynamic factor analysis of high-dimensional recurrent events},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving efficiency in transporting average treatment effects. <em>BIOMET</em>, <em>112</em>(3), asaf027. (<a href='https://doi.org/10.1093/biomet/asaf027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop flexible, semiparametric estimators of the average treatment effect transported to a new target population, which offer potential efficiency gains. Transport may be of value when the average treatment effect may differ across populations. We consider the setting where differences in the average treatment effect are due to differences in the distribution of effect modifiers, baseline covariates that modify the treatment effect. First, we propose a collaborative one-step semiparametric estimator that can improve efficiency. This approach does not require researchers to have knowledge about which covariates are effect modifiers and which differ in distribution between the populations, but it does require all covariates to be measured in the target population. Second, we propose two one-step semiparametric estimators that assume knowledge of which covariates are effect modifiers and which are both effect modifiers and differentially distributed between the populations. These estimators can be used even when not all covariates are observed in the target population; one estimator requires that only effect modifiers be observed, and the other requires that only those modifiers that are also differentially distributed be observed. We use simulations to compare the finite-sample performance of our proposed estimators and an existing semiparametric estimator of the transported average treatment effect, including in the presence of practical violations of the positivity assumption. Lastly, we apply our proposed estimators to a large-scale housing trial.},
  archive      = {J_BIOMET},
  author       = {Rudolph, K E and Williams, N T and Stuart, E A and Díaz, I},
  doi          = {10.1093/biomet/asaf027},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf027},
  shortjournal = {Biometrika},
  title        = {Improving efficiency in transporting average treatment effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general condition for bias attenuation by a nondifferentially mismeasured confounder. <em>BIOMET</em>, <em>112</em>(3), asaf026. (<a href='https://doi.org/10.1093/biomet/asaf026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world studies, the collected confounders may suffer from measurement error. Although mismeasurement of confounders is typically unintentional (originating from sources such as human oversight or imprecise machinery), deliberate mismeasurement also occurs and is becoming increasingly more common. For example, in the 2020 U.S. census, noise was added to measurements to assuage privacy concerns. Sensitive variables such as income or age are often partially censored and are only known up to a range of values. In such settings, obtaining valid estimates of the causal effect of a binary treatment can be impossible, as mismeasurement of confounders constitutes a violation of the no-unmeasured-confounding assumption. A natural question is whether the common practice of simply adjusting for the mismeasured confounder is justifiable. In this article, we answer this question in the affirmative and demonstrate that in many realistic scenarios not covered by previous literature, adjusting for the mismeasured confounders reduces bias compared to not adjusting.},
  archive      = {J_BIOMET},
  author       = {Zhang, Jeffrey and Lee, Junu},
  doi          = {10.1093/biomet/asaf026},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf026},
  shortjournal = {Biometrika},
  title        = {A general condition for bias attenuation by a nondifferentially mismeasured confounder},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: ‘Network cross-validation by edge sampling’. <em>BIOMET</em>, <em>112</em>(3), asaf023. (<a href='https://doi.org/10.1093/biomet/asaf023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asaf023},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf023},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Network cross-validation by edge sampling’},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning for piecewise-constant mean estimation: Optimality, ℓ1 and ℓ0 penalization. <em>BIOMET</em>, <em>112</em>(3), asaf018. (<a href='https://doi.org/10.1093/biomet/asaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study transfer learning for estimating piecewise-constant signals when source data, which may be relevant but disparate, are available in addition to target data. We first investigate transfer learning estimators that respectively employ ℓ 1 and ℓ 0 penalties for unisource data scenarios and then generalize these estimators to accommodate multisources. To further reduce estimation errors, especially when some sources significantly differ from the target, we introduce an informative source selection algorithm. We then examine these estimators with multisource selection and establish their minimax optimality. Unlike the common narrative in the transfer learning literature that the performance is enhanced through large source sample sizes, our approaches leverage higher observational frequencies and accommodate diverse frequencies across multiple sources. Our extensive numerical experiments show that the proposed transfer learning estimators significantly improve estimation performance compared to estimators that only use the target data.},
  archive      = {J_BIOMET},
  author       = {Wang, F and Yu, Y},
  doi          = {10.1093/biomet/asaf018},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf018},
  shortjournal = {Biometrika},
  title        = {Transfer learning for piecewise-constant mean estimation: Optimality, ℓ1 and ℓ0 penalization},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spike-and-slab prior for dimension selection in generalized linear network eigenmodels. <em>BIOMET</em>, <em>112</em>(3), asaf014. (<a href='https://doi.org/10.1093/biomet/asaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent space models are often used to model network data by embedding a network’s nodes into a low-dimensional latent space; however, choosing the dimension of this space remains a challenge. To this end, we begin by formalizing a class of latent space models we call generalized linear network eigenmodels that can model various edge types (binary, ordinal, nonnegative continuous) found in scientific applications. This model class subsumes the traditional eigenmodel by embedding it in a generalized linear model with an exponential dispersion family random component and fixes identifiability issues that hindered interpretability. We propose a Bayesian approach to dimension selection for generalized linear network eigenmodels based on an ordered spike-and-slab prior that provides improved dimension estimation and satisfies several appealing theoretical properties. We show that the model’s posterior is consistent and concentrates on low-dimensional models near the truth. We demonstrate our approach’s consistent dimension selection on simulated networks, and we use generalized linear network eigenmodels to study the effect of covariates on the formation of networks from biology, ecology and economics and the existence of residual latent structure.},
  archive      = {J_BIOMET},
  author       = {Loyal, Joshua D and Chen, Yuguo},
  doi          = {10.1093/biomet/asaf014},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf014},
  shortjournal = {Biometrika},
  title        = {A spike-and-slab prior for dimension selection in generalized linear network eigenmodels},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
