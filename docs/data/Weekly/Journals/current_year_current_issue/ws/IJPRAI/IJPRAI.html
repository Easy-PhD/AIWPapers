<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJPRAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijprai">IJPRAI - 13</h2>
<ul>
<li><details>
<summary>
(2025). Explainable modeling of quality anomaly traceability in industrial processes. <em>IJPRAI</em>, <em>39</em>(13), 2559018. (<a href='https://doi.org/10.1142/S0218001425590189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the defect traceability challenges caused by the complexity of modern industrial process operation mechanisms and high-dimensional parameter coupling, a quality anomaly diagnosis framework integrating interpretable machine learning and knowledge graph is proposed. The quantitative contributions of process parameters to quality indicators are assessed by constructing an architecture combining XGBoost prediction model and SHAP (SHapley Additive exPlanation) interpretable analysis. Structured storage of variable association patterns of historical data based on a knowledge graph. The K Nearest Neighbors (KNN) algorithm is further introduced to construct a data matching mechanism to achieve causal traceability of real-time anomalies through similarity-driven historical defect retrieval. The experimental results show that the method has a good ability to identify the root cause parameters of defects under multiple working condition scenarios, and provides a solution with both data-driven characteristics and knowledge interpretability for the quality control of complex industrial processes.},
  archive      = {J_IJPRAI},
  author       = {Biqing Wang},
  doi          = {10.1142/S0218001425590189},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2559018},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Explainable modeling of quality anomaly traceability in industrial processes},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization method of intelligent cleaning control strategy for cleaning of Rice–Wheat combine harvester. <em>IJPRAI</em>, <em>39</em>(13), 2559012. (<a href='https://doi.org/10.1142/S0218001425590128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key stage of the intelligent cleaning control strategy for rice–wheat combine harvesters is the initial setting of cleaning operation parameters. There is a lack of research on intelligent control methods that reveal the correlation between rice and wheat attributes such as variety, moisture content, and grass to grain ratio, and the initial values of cleaning device operating parameters, as well as the correlation between cleaning loss rate and cleaning impurity rate. This paper constructs an optimization model for the initial operation parameters of the cleaning, and based on the dynamic monitoring and control system of the cleaning’s operation quality and parameters, intelligently regulates the cleaning of the rice–wheat combine harvester. Field experiments and analysis demonstrated that the intelligent control system, guided by the initial parameter setting model, successfully stabilized the cleaning quality metrics (e.g. impurity rate and loss rate) of the rice–wheat combine harvester, thereby validating the effectiveness of the optimized cleaning control strategy based on the perceptual neural vision algorithm.},
  archive      = {J_IJPRAI},
  author       = {Zusheng Li and Yang Liu and Qing Jiang and Jing Zhang and YuQing Zhang and JiaHan Yu and ChangMin Zhan},
  doi          = {10.1142/S0218001425590128},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2559012},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Optimization method of intelligent cleaning control strategy for cleaning of Rice–Wheat combine harvester},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BC-MBINet: A novel architecture for accurate classification of breast cancer with microscopic biopsy images using deep convolutional neural networks. <em>IJPRAI</em>, <em>39</em>(13), 2557015. (<a href='https://doi.org/10.1142/S0218001425570150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) is the second most frequent malignancy, accounting for roughly 25% of all cases of cancer. BC is caused by genetic, epigenetic, and environmental factors, and their interaction too. The diagnosis of a BC is a critical step in the treatment process, and histopathological imaging is required to determine the type of illness. Identifying a disease is an important stage in the treatment process. However, this time-consuming task is exhausting, and people are prone to making mistakes that go unnoticed, making it difficult to determine the severity of the condition and this diagnosing step also relies on a pathologist’s expertise. In this paper, we have developed a novel BC with microscopic biopsy images network (BC-MBINet) model using deep convolutional neural networks. Feature extraction is handled by a sequence of convolutional layers, nonlinearity is handled using LeakyReLU activations, and learning is stabilized by batch normalization. A last Softmax layer is employed for binary classification into benign and malignant tumors, and dropout layers are included to decrease overfitting. The model achieves state-of-the-art accuracy and resilience in discriminating BC types by being trained on a publically available dataset of microscopic biopsy images. The proposed model is capable of classifying between the benign and malignant BC tumors with 99.04% accuracy. The model gives state-of-the-art results in its accuracy in classifying BC tumors into Benign or Malignant.},
  archive      = {J_IJPRAI},
  author       = {Kuljeet Singh and Amrit Sudershan and Sachin Kumar and Sourabh Shastri and Vibhakar Mansotra},
  doi          = {10.1142/S0218001425570150},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2557015},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {BC-MBINet: A novel architecture for accurate classification of breast cancer with microscopic biopsy images using deep convolutional neural networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CVD-M3NET: Cardiovascular disease stage classification via mamdani fuzzy-based modified mobile network. <em>IJPRAI</em>, <em>39</em>(13), 2557014. (<a href='https://doi.org/10.1142/S0218001425570149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globally, cardiovascular diseases (CVDs) remain an important cause of mortality, for early and precise diagnosis. Traditional machine learning (ML) models struggle with feature redundancy, data imbalance, and suboptimal performance due to inefficient feature selection and lack of robust deep learning (DL) architectures. To overcome these challenges, we propose a novel Cardiovascular Disease Stage Classification via the Mamdani Fuzzy-based Modified Mobile Network (CVD-M 3 Net) by integrating hybrid feature selection, DL network and Fuzzy logic for improved CVD stage classification. The proposed CVD-M 3 Net utilizes data from multiple sources including Cleveland, MIMIC-IV, UK Biobank, Statlog, Switzerland, Hungary, and VA Long Beach datasets. The Boruta-LASSO and Fast ICA techniques are used for feature selection by ensuring the retention of critical diagnostic attributes while eliminating irrelevant ones. The selected features undergo Z -score normalization for improved data consistency between the 15 features. The Modified MobileNet (MOMO-Net) is introduced with the integration of 1D convolutional and transformer layers in the MobileNet structure to categorize the tabular data for real-time CVD stage detection. The proposed CVD-M 3 Net utilizes the Mamdani fuzzy inference system (FIS) with a trapezoidal membership function to predict the CVD Stage Score (CSS). The efficiency of the proposed CVD-M 3 Net was estimated with the Accuracy, Sensitivity, Precision, Recall, and F1-score. From the experimental analysis, the proposed CVD-M 3 Net achieves an overall accuracy of 98.61% for efficient classification of CVD stages. The proposed CVD-M 3 Net increases the accuracy by 0.11%, 0.62%, 3.25%, and 0.38% better than ML algorithms, O-SBGC-LSTM, MaLCaDD, and DL-based CNN, respectively.},
  archive      = {J_IJPRAI},
  author       = {Lijetha Christopher Jaffrin and Visumathi James},
  doi          = {10.1142/S0218001425570149},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2557014},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {CVD-M3NET: Cardiovascular disease stage classification via mamdani fuzzy-based modified mobile network},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving brain tumor stage diagnosis with multi-stage hybrid classifier models. <em>IJPRAI</em>, <em>39</em>(13), 2557013. (<a href='https://doi.org/10.1142/S0218001425570137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To make informed clinical decisions and plan successful treatments, it is essential that magnetic resonance imaging (MRI) identify and diagnose brain tumors accurately. This research introduces a new method for multi-stage tumor detection and classification that uses a hybrid classifier (HC) to improve the accuracy of diagnoses. The first step of the suggested method is to identify possible tumor areas by obtaining features from grey-scale intensity analysis of successive pixels, which captures important differences. A systematic categorization technique relates these traits to distinct tumor stages. The HC uses rectilinear classifiers to analyze linear pixel changes and expanded classifiers to find bounded edges within regional pixel distributions. With this two-pronged strategy, tumors of any shape or size may be detected. Training classifiers on datasets with restricted edge characteristics and linear variation patterns allows for exact stage distinction, which is essential for learning. Compared to traditional classification approaches, the multi-stage framework greatly improves the capacity to detect complicated and nuanced tumor traits. Our powerful diagnostic approach permits precise and effective tumor identification and stage categorization to facilitate prompt treatments. The hybrid technique highlights its promise as a dependable, scalable solution to improve patient outcomes in medical imaging applications and advance brain tumor diagnosis.},
  archive      = {J_IJPRAI},
  author       = {S. R. Sowmiya and N. Sabiyath Fatima},
  doi          = {10.1142/S0218001425570137},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2557013},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Improving brain tumor stage diagnosis with multi-stage hybrid classifier models},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRGAN-based input enhancement and attention-guided YOLOv5s for real-time excavator pose estimation. <em>IJPRAI</em>, <em>39</em>(13), 2554014. (<a href='https://doi.org/10.1142/S021800142554014X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The measurement of excavator pose information is crucial for advancing intelligent excavator control systems. To address challenges in excavator pose detection — such as small pose measurement targets and blurred images — a high-speed, high-accuracy visual technology-based pose recognition algorithm, super-resolution input-YOLOv5s, was developed. This algorithm includes an image target designed specifically to facilitate the detection of the excavator arm’s pose angles. Excavator pose information is derived through the analysis of the target image data. The SRGAN model is used to enhance the quality of input data for YOLOv5, while attention mechanisms are introduced at the terminal stage of the backbone network. Focal loss is employed as the loss function to improve the detection accuracy and stability for small targets in complex construction environments, while also mitigating class imbalance. Experimental results demonstrate that the improved algorithm, SRI-YOLOv5s, achieved a detection speed of 59.20 FPS, with a mean average precision (mAP) of 89.46%, precision of 91.7%, and recall of 92.1%, outperforming the original model. The model’s real-time performance and robustness meet the requirements for excavator pose detection in practical environments.},
  archive      = {J_IJPRAI},
  author       = {Wangting Zeng and Qixiang Huang and Ke Wu and Xuedong Zhang and Bo Cui},
  doi          = {10.1142/S021800142554014X},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2554014},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {SRGAN-based input enhancement and attention-guided YOLOv5s for real-time excavator pose estimation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploration of simulation environments for visual question answering: 3D dense captioning for outdoor scenes. <em>IJPRAI</em>, <em>39</em>(13), 2552020. (<a href='https://doi.org/10.1142/S0218001425520202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel simulation framework tailored for Visual Question Answering (VQA) in complex outdoor environments, where existing datasets and systems fall short in semantic density and spatial reasoning. The proposed pipeline integrates procedural 3D scene generation, region-level dense captioning augmented with geometric priors, and structured scene graph construction to bridge vision and language understanding. A transformer-based reasoning module processes joint embeddings of questions and scene graphs to produce interpretable answers grounded in 3D semantics. The system supports diverse spatial and contextual queries and enables large-scale dataset synthesis with region-aligned captions and question–answer pairs. Extensive experiments across four datasets — Outdoor-Sim, SYNTHIA, StreetLearn, and ScanNet-VQA — demonstrate the framework’s superior performance over three strong baselines in accuracy, MRR, and convergence stability. Visualizations show robust alignment between captions, object layout, and semantic graphs. This study advances outdoor VQA by introducing a scalable, interpretable, and semantically grounded solution suitable for downstream robotics and scene understanding applications.},
  archive      = {J_IJPRAI},
  author       = {Renyue Wu and Kalama Bitur},
  doi          = {10.1142/S0218001425520202},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2552020},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Exploration of simulation environments for visual question answering: 3D dense captioning for outdoor scenes},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-assisted localized content-based image retrieval framework with multiple-instance learning algorithm. <em>IJPRAI</em>, <em>39</em>(13), 2552019. (<a href='https://doi.org/10.1142/S0218001425520196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional content-based image retrieval (CBIR) searches a database for pictures that match a single, unannotated query image. A comprehensive (or worldwide) perspective of the picture is essential for this kind of search. However, the intended picture content is frequently not global rather regional. Computer vision, pattern recognition, and CBIR are just a few of the applications that have long faced the “semantic gap” issue: how to bridge the gap between human perception of low-level semantic concepts and machine collection of high-level image pixels. In light of the recent achievements in deep learning research, there is hope for bridging the semantic gap. Therefore, the deep learning-assisted localized content-based image retrieval framework (DL-LCBIRF) was suggested in this study to rank photos in the database according to a similarity metric dependent upon specific areas inside the image. The first layer consists of “generic” descriptors that stand in for groups of feature vectors that are comparable and rotationally invariant. The combined probability of the frequencies of the “generic” descriptions over neighborhoods makes up the second layer. The proposed DL-LCBIR uses labelled images combined with a multiple-instance learning algorithm (MILA) to locate the target item and adjust the feature weights. This multi-modal probability is shown as a collection of “spatial frequency” clusters. It augments rotationally invariant statistical spatial constraints. In addition to enhancing the model’s performance, choosing a unique structure determines the model’s distinguishing features, which are common in good cases and seldom in negative ones.},
  archive      = {J_IJPRAI},
  author       = {P. Arulmozhi and R. Gopi},
  doi          = {10.1142/S0218001425520196},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2552019},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Deep learning-assisted localized content-based image retrieval framework with multiple-instance learning algorithm},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action space pruning for deep reinforcement learning in dou di zhu. <em>IJPRAI</em>, <em>39</em>(13), 2552014. (<a href='https://doi.org/10.1142/S0218001425520147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The card game Dou Di Zhu (competitive two-against-one game) presents a challenging multiplayer imperfect-information game problem due to its large action space. We developed a deep Monte Carlo (DMC) reinforcement learning (RL) framework called ASP-DouZero, which employs dynamic programming (DP) to prune the action space effectively, using statistical analysis results. The pruned action space was then applied to self-play data generation and neural network decision processes. We evaluated ASP-DouZero against the state-of-the-art DouZero framework under identical training conditions. Results showed the proposed approach achieved a 5% higher win rate in standardized matches after convergence while requiring 50% less training time on equivalent hardware. These findings demonstrate that action space pruning significantly improves decision-making performance and training efficiency in DMC-based approaches for Dou Di Zhu.},
  archive      = {J_IJPRAI},
  author       = {Shanglin Li and Jiabao Du and Yu Zhao and Tianle Xiang and Yulin Lan},
  doi          = {10.1142/S0218001425520147},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2552014},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Action space pruning for deep reinforcement learning in dou di zhu},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight object detection for real-time aerial video analysis on edge-deployed UAVs. <em>IJPRAI</em>, <em>39</em>(13), 2550025. (<a href='https://doi.org/10.1142/S0218001425500259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time object detection in aerial videos captured by Unmanned Aerial Vehicles (UAVs) plays a vital role in applications such as traffic surveillance, disaster response, and intelligent aerial operations. However, deploying deep learning models directly on UAV platforms faces significant challenges due to limited onboard computing capabilities and strict power constraints imposed by the aircraft’s propulsion system. The efficiency of the detection algorithm directly impacts not only processing latency but also the energy consumption and endurance of the UAVs flight dynamics. To address these challenges, this paper presents UAVDet, a lightweight object detection framework optimized for edge-deployed UAVs operating under propulsion-aware constraints. UAVDet comprises three modules: a Multi-Scale Lightweight Backbone (MSLB) for efficient multi-resolution feature extraction, a Temporal-Consistent Feature Alignment (TCFA) module that enhances inter-frame stability under aerial motion, and an Adaptive Context Enhancement Head (ACEH) for spatially and semantically precise detection. Extensive experiments on a custom UAV dataset demonstrate that UAVDet achieves a superior balance between detection accuracy and runtime efficiency. Moreover, its low computational overhead supports deployment on energy-constrained UAV platforms, reducing the burden on the propulsion system and enabling longer, smarter missions.},
  archive      = {J_IJPRAI},
  author       = {Hong Zhu and He Qin},
  doi          = {10.1142/S0218001425500259},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2550025},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Lightweight object detection for real-time aerial video analysis on edge-deployed UAVs},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic detection of obstacles in underground railway tracks for collision prevention. <em>IJPRAI</em>, <em>39</em>(13), 2550024. (<a href='https://doi.org/10.1142/S0218001425500247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned underground vehicles (UUVs) offer significant advantages for coal mine transportation. However, the subterranean environment is often complex and prone to unexpected obstacles, particularly pedestrians, which pose substantial safety risks. To mitigate these risks, we developed an automated collision avoidance system. Additionally, we established an information network enabling real-time map tracking of UUVs to facilitate efficient dispatch and operation. Advanced deep learning algorithms detect tracks, obstacles, and pedestrians. Within the track detection zone, the system employs the Hough transform to identify line segments. These segments are then assigned weighting factors based on clarity, clustering, and fitting techniques, enabling accurate reconstruction of the track’s left and right boundaries. Furthermore, a defined safety zone effectively assesses the distance between pedestrians and vehicles. In practical operation, UUVs autonomously trigger audible alarms or initiate braking when pedestrians enter a critical proximity, when other UUVs obstruct the roadway, or when vehicles approach from the opposite direction. This integrated system significantly enhances safety for underground coal mine transportation.},
  archive      = {J_IJPRAI},
  author       = {Kai Zhao and Xuenan Zhang and Liwei Chen},
  doi          = {10.1142/S0218001425500247},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2550024},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Automatic detection of obstacles in underground railway tracks for collision prevention},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human–Machine coupling study for fast visual target capture. <em>IJPRAI</em>, <em>39</em>(13), 2550023. (<a href='https://doi.org/10.1142/S0218001425500235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of computer vision, human–computer interaction, and intelligent sensing technologies, accurate line-of-sight (LOS) tracking has become a critical research topic. However, traditional gaze tracking systems face significant limitations under conditions of natural head movement. The direction of human vision is determined jointly by head posture and eye movement, and precise acquisition of head pose remains a key challenge in gaze tracking technology. To address the limitations imposed by head movement, this study aims to improve the robustness of gaze tracking in natural interaction scenarios. This paper proposes a novel head pose estimation method based on the 3D spatial information of facial feature points. The method utilizes stereo vision to reconstruct 3D feature points and employs a geometric model to calculate head pose, effectively decoupling head movement from eye movement in gaze tracking. In the experimental setup, two interaction modalities were tested: head pointing combined with key pressing, and gaze estimation combined with head pointing. Experimental results demonstrate that the proposed method achieves interaction latency between 100 ms and 200 ms, with a fast capture success rate of up to 85%, slightly outperforming traditional visual target acquisition algorithms. These results indicate the method’s superior responsiveness and stability under natural head movement conditions. The research provides an important technical foundation for achieving natural, efficient human–computer interaction in complex environments.},
  archive      = {J_IJPRAI},
  author       = {Wenbo Huang and Mingwei Zhao and Heng Zhang and Xiaoqiao Wang},
  doi          = {10.1142/S0218001425500235},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2550023},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Human–Machine coupling study for fast visual target capture},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AN advanced AI approach-based skin disease prediction system utilizing EN-QNN and grad-CAM in IoMT environment. <em>IJPRAI</em>, <em>39</em>(13), 2550013. (<a href='https://doi.org/10.1142/S0218001425500132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin acts as a natural shield, protecting the body from ultraviolet rays, extreme weather, and harmful chemicals. However, it can be affected by pollution, weakened immunity, and unhealthy lifestyles, leading to various skin diseases. Early detection of these conditions is crucial for timely treatment and better outcomes. Existing research has often overlooked skin diseases with similar visual characteristics, making it challenging to distinguish between different conditions using visual inspection alone. To address this, the paper proposes an AI-enabled prediction framework for skin disease prediction using EN-QNN and Grad-CAM. Initially, the images are collected using IoMT devices of skin diseases and undergo preprocessing, which includes resizing, noise removal and contrast enhancement using AK-CLAHE, followed by color analysis and segmentation using YCbCr and DF-U-Net. Morphological operations are then applied during post-processing. The shape and structure of lesions are analyzed using CMED. Meanwhile, using Grad-CAM, Contextual Information Analysis (CIA) is performed on preprocessed data. Concurrently, disease symptom prediction data (i.e. clinical data) are collected, and features are extracted from this data, including boundary localization and CIA. Finally, skin diseases are classified using EN-QNN. The proposed model achieved a high accuracy of 98.68051%, surpassing current techniques.},
  archive      = {J_IJPRAI},
  author       = {Bhavya Kadiyala and Sunil Kumar Alavilli and Rajani Priya Nippatla and Subramanyam Boyapati and Chaitanya Vasamsetty and Harleen Kaur},
  doi          = {10.1142/S0218001425500132},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {13},
  pages        = {2550013},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {AN advanced AI approach-based skin disease prediction system utilizing EN-QNN and grad-CAM in IoMT environment},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
