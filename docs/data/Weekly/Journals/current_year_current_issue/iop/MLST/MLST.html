<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLST</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mlst">MLST - 9</h2>
<ul>
<li><details>
<summary>
(2025). A high-performance and portable implementation of the SISSO method for CPUs and GPUs. <em>MLST</em>, <em>6</em>(4), 047001. (<a href='https://doi.org/10.1088/2632-2153/ae0ab3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sure-independence screening and sparsifying operator (SISSO) is an artificial intelligence (AI) method based on symbolic regression and compressed sensing widely used in materials science research. SISSO++ is its C++ implementation that employs MPI and OpenMP for parallelization, rendering it well-suited for high-performance computing (HPC) environments. As heterogeneous hardware becomes mainstream in the HPC and AI fields, we chose to port the SISSO++ code to GPUs using the Kokkos performance-portable library. Kokkos allows us to maintain a single codebase for both Nvidia and AMD GPUs, significantly reducing the maintenance effort. In this work, we summarize the necessary code changes we did to achieve hardware and performance portability. This is accompanied by performance benchmarks on Nvidia and AMD GPUs. We demonstrate the speedups obtained from using GPUs across the three most time-consuming parts of our code.},
  archive      = {J_MLST},
  author       = {Sebastian Eibl and Yi Yao and Matthias Scheffler and Markus Rampp and Luca M Ghiringhelli and Thomas A R Purcell},
  doi          = {10.1088/2632-2153/ae0ab3},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {047001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A high-performance and portable implementation of the SISSO method for CPUs and GPUs},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification in graph neural networks with shallow ensembles. <em>MLST</em>, <em>6</em>(4), 045007. (<a href='https://doi.org/10.1088/2632-2153/ae0bf0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine-learned potentials (MLPs) have revolutionized materials discovery by providing accurate and efficient predictions of molecular and material properties. Graph neural networks (GNNs) have emerged as a state-of-the-art approach due to their ability to capture complex atomic interactions. However, GNNs often produce unreliable predictions when encountering out-of-domain data and it is difficult to identify when that happens. To address this challenge, we explore uncertainty quantification (UQ) techniques, focusing on direct propagation of shallow ensembles (DPOSEs) as a computationally efficient alternative to deep ensembles. By integrating DPOSE into the SchNet model, we assess its ability to provide reliable uncertainty estimates across several density functional theory datasets, including QM9, OC20, and Gold dataset. Our findings often demonstrate that DPOSE successfully distinguishes between in-domain and out-of-domain samples, exhibiting higher uncertainty for unobserved molecule and material classes. This work highlights the potential of lightweight UQ methods in improving the robustness of GNN-based materials modeling and lays the foundation for future integration with active learning strategies.},
  archive      = {J_MLST},
  author       = {Tirtha Vinchurkar and Kareem Abdelmaqsoud and John R Kitchin},
  doi          = {10.1088/2632-2153/ae0bf0},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Uncertainty quantification in graph neural networks with shallow ensembles},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Super-resolving 3D nanostructures using artificially generated image data and spatial transport simulations. <em>MLST</em>, <em>6</em>(4), 045006. (<a href='https://doi.org/10.1088/2632-2153/ae0c55'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An approach for deploying stochastic three-dimensional (3D) models to generate microstructural 3D image data for training super-resolution networks is investigated for three different scaling factors \alpha\in\{2,4,8\} . The presented approach addresses the issue of scarcity in training data by training the networks only on artificial image data, generated by means of a stochastic 3D model that produces digital twins of the nanoporous inner structure of active particles in battery cathodes. In addition, the performance of super-resolution networks is investigated when complementing the input data, i.e. low-resolved microstructural 3D image data, with spatially resolved transport simulations. The performance of the trained networks is evaluated based on real tomographic image data, and quantified with respect to various geometric descriptors and effective transport properties. It turned out that the integration of transport simulations into the training of super-resolution networks showed an increase in performance for the scaling factors \alpha\in\{2,4\} , but a decrease in performance for α = 8. However, training the networks on artificial image data was effective in all cases.},
  archive      = {J_MLST},
  author       = {Orkun Furat and Phillip Gräfensteiner and Rishabh Saxena and Markus Osenberg and Matthias Neumann and Ingo Manke and Thomas Carraro and Volker Schmidt},
  doi          = {10.1088/2632-2153/ae0c55},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Super-resolving 3D nanostructures using artificially generated image data and spatial transport simulations},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling the power of multimodal large language models for radio astronomical image understanding and question answering. <em>MLST</em>, <em>6</em>(4), 045005. (<a href='https://doi.org/10.1088/2632-2153/ae0c56'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although multimodal large language models (MLLMs) have shown remarkable achievements across various scientific domains, their applications in radio astronomy remain largely unexplored. In this paper, we investigate the potential of MLLMs for image understanding and visual question answering (VQA) in radio astronomy. This can facilitate the use of MLLMs as AI assistants in both research and education by discerning and describing complex astronomical information in human-readable languages. However, general-purpose MLLMs show inferior performance in radio astronomy because they typically lack specialized knowledge. To bridge this gap, we construct a new VQA dataset, RadioAstroVQA, from open data repositories. Specifically, we transform data samples from different repositories into VQA examples by extracting questions based on task descriptions and observation reports associated with images and then composing their answers using ground-truth labels and captions. Furthermore, by leveraging the RadioAstroVQA dataset, we fine-tune two MLLMs of different parameter scales to specifically enhance their capacities for radio astronomical image classification and VQA tasks. Finally, we conduct extensive experiments to show that the fine-tuned MLLMs are capable of handling multiple types of radio astronomical images and generating customized textual output tailored to specific task needs. They achieve accuracy comparable to or even better than that of existing deep learning models for classification tasks. They also demonstrate significantly better performance on VQA tasks compared to several state-of-the-art MLLMs in general domains. These results confirm the potential of MLLMs to serve as specialized AI assistants in the field of radio astronomy.},
  archive      = {J_MLST},
  author       = {Fuyong Zhao and Yuyang Li and Zhenyu Liu and Panfeng Chen and Cunshi Wang and Jifeng Liu and Hui Li and Yanhao Wang},
  doi          = {10.1088/2632-2153/ae0c56},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unveiling the power of multimodal large language models for radio astronomical image understanding and question answering},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-fidelity learning for atomistic models via trainable data embeddings. <em>MLST</em>, <em>6</em>(4), 045004. (<a href='https://doi.org/10.1088/2632-2153/ae0d41'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach for end-to-end training of machine learning models for structure-property modeling on collections of datasets derived using different density functional theory functionals and basis sets. This approach overcomes the problem of data inconsistencies in the training of machine learning models on atomistic data. We rephrase the underlying problem as a multi-task learning scenario. We show that conditioning neural network-based models on trainable embedding vectors can effectively account for quantitative differences between methods. This allows for joint training on multiple datasets that would otherwise be incompatible. Therefore, this procedure circumvents the need for re-computations at a unified level of theory. Numerical experiments demonstrate that training on multiple reference methods enables transfer learning between tasks, resulting in even lower errors compared to training on separate tasks alone. Furthermore, we show that this approach can be used for multi-fidelity learning, improving data efficiency for the highest fidelity by an order of magnitude. To test scalability, we train a single model on a joint dataset compiled from ten disjoint subsets of the MultiXC-QM9 dataset generated by different reference methods. Again, we observe transfer learning effects that improve the model errors by a factor of 2 compared to training on each subset alone. We extend our investigation to machine learning force fields for material simulations. To this end, we incorporate trainable embedding vectors into the readout layer of a deep graph neural network (M3GNet) that is simultaneously trained on PBE and r2SCAN labels of the MatPES dataset. We observe that joint training on both fidelity levels reduces the amount of r2SCAN data required to achieve the accuracy of a single-fidelity model by a factor of 10.},
  archive      = {J_MLST},
  author       = {Rick Oerder and Gerrit Schmieden and Jan Hamaekers},
  doi          = {10.1088/2632-2153/ae0d41},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Multi-fidelity learning for atomistic models via trainable data embeddings},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anti-correlated noise in epoch-based stochastic gradient descent: Implications for weight variances in flat directions. <em>MLST</em>, <em>6</em>(4), 045003. (<a href='https://doi.org/10.1088/2632-2153/ae0ab2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) has become a cornerstone of neural network optimization due to its computational efficiency and generalization capabilities. However, the gradient noise introduced by SGD is often assumed to be uncorrelated over time, despite the common practice of epoch-based training where data is sampled without replacement. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum. Our main contributions are twofold: first, we calculate the exact autocorrelation of the noise during epoch-based training under the assumption that the noise is independent of small fluctuations in the weight vector, revealing that SGD noise is inherently anti-correlated over time. Second, we explore the influence of these anti-correlations on the variance of weight fluctuations. We find that for directions with curvature of the loss greater than a hyperparameter-dependent crossover value, the conventional predictions of isotropic weight variance under stationarity, based on uncorrelated and curvature-proportional noise, are recovered. Anti-correlations have negligible effect here. However, for relatively flat directions, the weight variance is significantly reduced, leading to a considerable decrease in loss fluctuations compared to the constant weight variance assumption. Furthermore, we present a numerical experiment where training with these anti-correlations enhances test performance, suggesting that the inherent noise structure induced by epoch-based training may play a role in finding flatter minima that generalize better.},
  archive      = {J_MLST},
  author       = {Marcel Kühn and Bernd Rosenow},
  doi          = {10.1088/2632-2153/ae0ab2},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Anti-correlated noise in epoch-based stochastic gradient descent: Implications for weight variances in flat directions},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum generative adversarial networks with dual generators. <em>MLST</em>, <em>6</em>(4), 045002. (<a href='https://doi.org/10.1088/2632-2153/ae0bf7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum generative adversarial networks (QGANs) have demonstrated strong capabilities in tasks like synthetic data generation and detecting anomalies. Recent developments have increasingly integrated traditional machine learning techniques to boost the performance of QGANs. Motivated by this progress, we propose an innovative QGAN architecture that incorporates a classical learning component and employs a dual-generator design. Our approach improves upon the traditional hybrid quantum–classical GAN structure and introduces a redesigned loss function tailored for the new model. Experiments on multiple datasets indicate that our method surpasses previous techniques in image generation quality, achieving a 1.38% average reduction in Fréchet inception distance scores compared to the current state-of-the-art, and improvements of 6.52%, 0.36%, and 0.38% in structural similarity index, cosine similarity, and peak signal-to-noise ratio metrics, respectively. Additionally, our architecture supports the generation of larger images (up to 78\,\times\,78 ), as verified on the CelebA dataset. Simulations conducted in noisy conditions further confirm the robustness and effectiveness of both the proposed architecture and loss function.},
  archive      = {J_MLST},
  author       = {Quangong Ma and Chaolong Hao and NianWen Si and Dan Qu},
  doi          = {10.1088/2632-2153/ae0bf7},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum generative adversarial networks with dual generators},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor network for anomaly detection in the latent space of proton collision events at the LHC. <em>MLST</em>, <em>6</em>(4), 045001. (<a href='https://doi.org/10.1088/2632-2153/ae0243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pursuit of discovering new phenomena at the Large Hadron Collider (LHC) requires constant innovation in algorithms and technologies. Tensor networks are mathematical models at the intersection of classical and quantum machine learning, which present a promising and efficient alternative for tackling these challenges. In this study, we propose a tensor network-based strategy for anomaly detection at the LHC and demonstrate its superior performance in identifying new phenomena compared to established quantum methods. Our model is a parameterized matrix product state with an isometric feature map, processing a latent representation of simulated LHC data generated by an autoencoder. Our results highlight the potential of tensor networks to enhance new-physics discovery.},
  archive      = {J_MLST},
  author       = {Ema Puljak and Maurizio Pierini and Artur Garcia-Saez},
  doi          = {10.1088/2632-2153/ae0243},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Tensor network for anomaly detection in the latent space of proton collision events at the LHC},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perspective on artificial intelligence for accelerated materials design (AI4Mat) workshops in 2024. <em>MLST</em>, <em>6</em>(4), 040201. (<a href='https://doi.org/10.1088/2632-2153/ae0d5d'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intersection of artificial intelligence and materials science has become increasingly interconnected, driving ambitious research initiatives across both fields. Since 2022, the AI for accelerated materials design (AI4Mat) workshops have provided a leading venue for showcasing cutting-edge advances in this emerging interdisciplinary domain while fostering critical discussions about the most pressing scientific and technical challenges. In 2024, AI4Mat hosted workshops at BOKU University and NeurIPS 2024, attracting researchers and practitioners from academia, industry, and government institutions worldwide. These workshops explored diverse research areas currently shaping the field, with participants engaging in comprehensive discussions that addressed the intersection’s most significant challenges from scientific, technical, and commercial perspectives. Through this holistic approach, AI4Mat’s 2024 workshops successfully illuminated the multifaceted nature of AI-driven materials research, highlighting both current achievements and future opportunities in this rapidly evolving field. In this article, the AI4Mat-2024 organizing committee presents key insights from our workshops and community discussions, outlining critical challenges in this emerging field while summarizing the latest advances in AI-accelerated materials design. We examine persistent challenges around data creation and reproducibility, alongside the growing commercial interest in developing new markets and optimization materials production processes at scale. The article also highlights significant research breakthroughs showcased at AI4Mat, including the application of large language models to accelerate materials science tasks, the development of sophisticated generative models for materials discovery, and the growing demand for interpretable AI methodologies that provide transparent insights into materials behavior.},
  archive      = {J_MLST},
  author       = {Santiago Miret and Marta Skreta and Geemi Wellawatte and Stefano Martiniani and N M Anoop Krishnan and George Karypis and Kevin Maik Jablonka},
  doi          = {10.1088/2632-2153/ae0d5d},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {040201},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Perspective on artificial intelligence for accelerated materials design (AI4Mat) workshops in 2024},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
