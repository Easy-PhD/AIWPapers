<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="eai">EAI - 15</h2>
<ul>
<li><details>
<summary>
(2025). Federated learning strategies for integrating composite meta-consistency loss with multi-head attention. <em>EAI</em>, <em>38</em>(4), 725-742. (<a href='https://doi.org/10.1177/30504554251340238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem of accuracy decline caused by feature redundancy, this paper designs a federated learning strategy that combines composite meta-consistency loss and multi-head attention. Firstly, this paper decorrelates the features based on the dual theory of constraints to eliminate redundant information, and improves the stability of the model through gradient-based regularization. Composite meta-consistency loss is constructed based on these two optimization methods. Experiments show that compared with the latest algorithms, the maximum accuracy of CIFAR-10 and Oxford-Pets in this paper is improved by 0.82% and 2.19%, respectively. After that, this paper introduces multi-head attention into the framework of federated learning. After capturing richer context information in the process of feature extraction, the combination of inner-layer update and outer-layer update of the meta-learning method enables the federated learning framework to effectively cope with the data distribution of different clients and finally accelerate the convergence speed. Compared with other algorithms, the average accuracy of the first 40 rounds in the MINIST, CIFAR-10 and CIFAR-100 data sets is higher. In CIFAR-10, SVHN, Oxford-Pets, taking Robust-HDP as the benchmark, the speedup ratio reaches 1.5, 1.42, and 1.34, respectively, which is faster than other algorithms.},
  archive      = {J_EAI},
  author       = {Afei Li and Xiaolei Yang and Li Ma and Lu Yu and Liyu Hao and Yongshan Liu},
  doi          = {10.1177/30504554251340238},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {725-742},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Federated learning strategies for integrating composite meta-consistency loss with multi-head attention},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASNet: Multi-attention sparse network for light detection and ranging (LIDAR)-based three-dimensional (3D) object detection. <em>EAI</em>, <em>38</em>(4), 707-724. (<a href='https://doi.org/10.1177/30504554251340241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that pillar-based detectors perform better in terms of both accuracy and speed, but these detectors perform poorly for detecting small objects such as pedestrians and cyclists. To solve this problem, we propose a highly efficient pillar-based model named MASNet, which mainly consists of a pillar mix attention (PMA) module, an attention-pooling operation, and a focal sparse network (FSN) module. The PMA module encodes the pillar features by fusion of the point-wise attention module and the channel-wise attention module. The attention-pooling operation aggregates the attention-encoded pillar features in a more comprehensive way to obtain the most expressive pillar features. In addition, the FSN module exploits the intrinsic sparsity of the data by introducing focal sparse convolution, which enriches the learned pillar features in the foreground without adding redundant pillars in other regions. On the KITTI three-dimensional (3D) Object Detection Benchmark, it achieves a 3D average precision of (77.81%, 60.30%, and 53.92%) in easy, moderate, and hard levels, which outperforms other pillar-based methods for the detection of cyclists. Additionally, our method is only 0.52% lower than the top-ranked method (pillar feature network (PIFENet)) on the KITTI Bird's Eye View pedestrian leaderboard, but our inference speed reaches 41 frames per second ahead of PIFENet by 57.69%.},
  archive      = {J_EAI},
  author       = {Fuqiang You and H Ziheng Zhang and Hao Chen},
  doi          = {10.1177/30504554251340241},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {707-724},
  shortjournal = {Eur. Artif. Intell.},
  title        = {MASNet: Multi-attention sparse network for light detection and ranging (LIDAR)-based three-dimensional (3D) object detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal coupling prompt learning for image classification tasks. <em>EAI</em>, <em>38</em>(4), 684-706. (<a href='https://doi.org/10.1177/30504554251335569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, vision-language pretraining (VLP) models have become a crucial driving force in the advancement of artificial intelligence. Besides, studies such as contrastive language-image pretraining (CLIP) have demonstrated that incorporating prompt learning within VLP models can significantly enhance the performance of downstream tasks. However, we believe that CLIP’s visual encoder suffers from feature extraction bias in image classification tasks, which is because of the uneven quantity and distribution of image features CLIP learned between the pretraining and fine-tuning stages. This can be further summarized as an inherent bias in feature extraction for differently distributed samples during the pretraining phase. To address the above problem this paper proposes (i) text-semantic hierarchical injection prompt learning method, which constructs self-attention layers and prompt mapping structures and injects text semantic features into the visual encoder layer by layer to generate visual prompt features and (ii) visual-semantic attention interactive prompt learning method, which further integrates text embeddings with the output features of the visual encoder through cross-attention and constructs instance-level text prompt features for each image. Based on the two above methods, this paper further proposes the multimodal coupling prompt learning CLIP (MCPL-CLIP) to enhance CLIP’s performance in image classification tasks. Experiments conducted on 15 image classification datasets demonstrate that MCPL-CLIP outperforms baseline models such as MaPLe, CoCoOp, and CoOp in cross-dataset transfer, domain generalization, and base-to-novel class generalization tasks, showcasing its superior text semantic representation and visual feature extraction capabilities.},
  archive      = {J_EAI},
  author       = {Yufei Liu and Hua Cheng and Yiquan Fang and Yiming Pan and Zehong Qian and Xiaoning Chen},
  doi          = {10.1177/30504554251335569},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {684-706},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Multimodal coupling prompt learning for image classification tasks},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KANDiff: Low-light image enhancement based on diffusion models. <em>EAI</em>, <em>38</em>(4), 666-683. (<a href='https://doi.org/10.1177/30504554251342571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement technique aims to improve the contrast and brightness of low-light images. Diffusion models, attributed to adeptness at capturing intricate details, have achieved good results in image enhancement, but there are problems such as inadequate estimation of noise characteristics and the emergence of color bias in the enhanced images. To address the aforementioned problems, this paper proposes a diffusion models-based method for low-light image enhancement, termed KANDiff. In the diffusion model architecture, this paper adds the nonlinear learnable activation function Kolmogorov–Arnold network to the noise estimation network U-Net to generate higher quality enhanced images. Additionally, KANDiff mitigates color bias in the enhanced images through a joint loss function and employs a patch-based image restoration strategy to significantly enhance the model generalization capability. The experimental results show that the KANDiff algorithm proposed in this paper can achieve high-quality image enhancement and achieve better enhancement effects compared to other algorithms.},
  archive      = {J_EAI},
  author       = {Yuanxin Ren and Minghui Yue and Yuxuan He and Liye Zhang},
  doi          = {10.1177/30504554251342571},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {666-683},
  shortjournal = {Eur. Artif. Intell.},
  title        = {KANDiff: Low-light image enhancement based on diffusion models},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid biogeography-based optimization algorithm for distributed assembly permutation flowshop scheduling problem. <em>EAI</em>, <em>38</em>(4), 649-665. (<a href='https://doi.org/10.1177/30504554251347434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributed assembly permutation flowshop scheduling problem (DAPFSP) has become a significant research topic in recent years. However, most existing studies rely on approximation search algorithms, such as metaheuristic algorithms, to address the high complexity of DAPFSP. To overcome the limitations of instability and poor search capability in these algorithms, this study proposes a hybrid biogeography-based optimization (HBBO) algorithm to solve DAPFSP with the objective of minimizing makespan. Two factory allocation methods are employed to generate initial solutions, reducing the possibility of redundant solutions. A crossover operator is utilized to improve the quality of partial solutions during the global search phase of HBBO. Additionally, four migration rate models are comprehensively tested as alternatives to commonly used models. Two neighborhood search methods are incorporated as the local search strategy for HBBO. The Metropolis acceptance criterion is employed to retain some inferior solutions, thereby enhancing solution diversity and preventing the algorithm from getting trapped in local optima. A series of experiments are conducted, and the computational results demonstrate that HBBO delivers promising results for DAPFSP.},
  archive      = {J_EAI},
  author       = {Long Cheng and Lei Wang and JingCao Cai and Kongfu Hu and Yuan Xiong and Qiangqiang Xia},
  doi          = {10.1177/30504554251347434},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {649-665},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A hybrid biogeography-based optimization algorithm for distributed assembly permutation flowshop scheduling problem},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-aspect graph representation feature integration for recommender dialogue system. <em>EAI</em>, <em>38</em>(4), 630-648. (<a href='https://doi.org/10.1177/30504554251347451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation-based dialogue systems aim to capture user preferences via interactive conversations for personalized recommendations. While existing studies focus on modeling user preferences, real-time dialog scenarios face challenges in balancing historical conversation contexts and immediate interests. This study proposes MGIRD, a multi-aspect graph representation approach integrating ordinary graphs and hypergraphs. We use graph structures to model users’ current interests and hypergraphs for historical conversation features, while incorporating historical behaviors in the recommendation module to balance context relevance. A novel item selection mechanism is introduced during dialog generation to naturally integrate recommended items. Experiments on Chinese TG-Redial and English Redial datasets show MGIRD outperforms most state-of-the-art methods in recommendation accuracy and dialog diversity, validating its effectiveness in enhancing recommendation quality and conversational fluency.},
  archive      = {J_EAI},
  author       = {Shi Li and Qing Yang Bai},
  doi          = {10.1177/30504554251347451},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {630-648},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Multi-aspect graph representation feature integration for recommender dialogue system},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic graph anomaly detection model combining dual behavior contrast. <em>EAI</em>, <em>38</em>(4), 617-629. (<a href='https://doi.org/10.1177/30504554251347752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current dynamic graph anomaly detection models learn multibehavior patterns for abnormal edges poorly and rely too much on the differences in long-term snapshots. Aiming at the above problems, combine dual behavior contrast dynamic graph anomaly detection model is proposed. Firstly, a dual behavior learning module is designed, where the role-based behavior learning submodule constructs graphlet degree vector by identifying four self-isomorphic orbits to capture deep structural features, while the attribute-based behavior learning submodule obtains attribute vectors through graph convolutional network. Then, the results are combined in the dynamic edge representation module to form the dynamic representations of edges to capture dual behavior patterns. Lastly, the anomaly detection module is designed to detect newly generated edges by combining contrastive learning with gated recurrent unit. We conduct experiments from four perspectives: anomaly detection accuracy, parameter sensitivity, robustness of module variants, and model runtime efficiency. The results demonstrate that the model achieves a peak accuracy of 92.05% in the task of dynamic edge anomaly detection.},
  archive      = {J_EAI},
  author       = {Jian Feng and Xiaotian Zhao},
  doi          = {10.1177/30504554251347752},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {617-629},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Dynamic graph anomaly detection model combining dual behavior contrast},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action recognition based virtual panorama live broadcasting system. <em>EAI</em>, <em>38</em>(4), 601-616. (<a href='https://doi.org/10.1177/30504554251347439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the significant success of transformers in natural language processing, an increasing number of researchers are introducing them into the field of computer vision, particularly for action recognition. As a crucial task in video understanding, action recognition has significant applications in live broadcasting, autonomous driving, and medical diagnostics. The attention mechanisms in transformers mimicking human visual attention allocation, thereby enhancing the processing capabilities and comprehension of long video sequences. However, they often overlook the aggregation of multiscale detail features and the hierarchical representations of early visual information. Additionally, attention networks are computationally intensive and parameter-heavy, complicating model training and extending inference times, rendering them unsuitable for real-time applications. To crack these nuts, we propose a lightweight multiscale action recognition model based on convolutional enhancement block (ConvEB) and multiscale average pooling encoder. The ConvEB aims to establish long-range dependencies among multiscale local features in the early stages of the network, providing effective inductive biases for the attention network to compensate for the loss of detailed information. Moreover, we introduce a parallel pooling mixer to replace the original attention mixer, ensuring model lightweight while maintaining recognition accuracy. Finally, we deploy this model in the construction of a virtual panorama live broadcasting system. Experimental results demonstrate that our action recognition algorithm achieves competitive performance, and the constructed panoramic system basically meets the needs of daily live broadcasting.},
  archive      = {J_EAI},
  author       = {Lichuan Geng and Zihao Zhao and Qiaohong Hou},
  doi          = {10.1177/30504554251347439},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {601-616},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Action recognition based virtual panorama live broadcasting system},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BSNet: Boundary-location network based on deep multi-scale modulation for camouflaged object detection. <em>EAI</em>, <em>38</em>(4), 581-600. (<a href='https://doi.org/10.1177/30504554251328322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to identify objects seamlessly embedded in the surrounding environment. Due to the high inherent similarity between the texture of the camouflaged object and its complex background, making COD far more challenging than traditional target detection. To solve these problems, we propose a method that uses holistic boundary information to optimize COD through a two-stage strategy. Specifically, the feature enhancement module is initially implemented to refine features at different scales and emphasize boundary details of camouflaged entities. Then, our network employs a boundary localization module to guide low-level local edge features through high-level global semantic. Furthermore, the boundary-embedded feature aggregation module is introduced to achieve cross-level fusion of multi-scale features, by embedding and effectively activating boundary information, which reduces the interference from cluttered backgrounds. Extensive experiments on four benchmark datasets demonstrate that our proposed model outperforms the other 17 state-of-the-art COD methods. The source code and results of our method are available at https://github.com/WObaibai/BSNet .},
  archive      = {J_EAI},
  author       = {Yuhong Chen and Meng Dai and Qing Zhang and Jiayun Wu},
  doi          = {10.1177/30504554251328322},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {581-600},
  shortjournal = {Eur. Artif. Intell.},
  title        = {BSNet: Boundary-location network based on deep multi-scale modulation for camouflaged object detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and implementation of a lightweight online human behavior recognition method for nighttime surveillance videos. <em>EAI</em>, <em>38</em>(4), 562-580. (<a href='https://doi.org/10.1177/30504554251347452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to study human behavior recognition techniques in nighttime near-infrared surveillance videos and address the lack of lightweight behavior recognition networks in the field of nighttime near-infrared public monitoring. The goal is to develop a method suitable for lightweight edge processing devices. Due to the absence of a nighttime infrared video behavior recognition dataset, we have constructed a custom dataset for human behavior in near-infrared monitoring. This dataset involved 1630 video samples categorized into 10 classes. To overcome the challenge posed by large models and high device requirements, we propose a behavior recognition algorithm based on lightweight two-dimensional convolutional neural networks. This algorithm can adapt to low-quality near-infrared surveillance videos. Besides, it's able to focus on different temporal features and avoid interference from static background frames in long videos. The proposed method achieves an accuracy of 92.3% by using the self-built dataset captured by an active infrared camera and exhibits a processing speed of 35ms per video on the AGX Xavier device. Compared to popular lightweight algorithms like MoViNet-A0 and X3D-XS, this algorithm achieves higher accuracy and shorter processing time under similar model computational complexity.},
  archive      = {J_EAI},
  author       = {Mingrui Liu and Xiaogang Wang and Jiayi Zhou and Keyu Chen and Rui Song},
  doi          = {10.1177/30504554251347452},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {562-580},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Design and implementation of a lightweight online human behavior recognition method for nighttime surveillance videos},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-branch hybrid visual networks and hierarchical adaptive fusion strategy: An effective multimodal fake news detection model. <em>EAI</em>, <em>38</em>(4), 543-561. (<a href='https://doi.org/10.1177/30504554251351227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current fake news detection models do not adequately extract fine-grained image features and also ignore the important impact of shallow text features on the results. In addition, the fusion methods are too simple and do not take into account the different importance of various information sources on the final detection results. To address these limitations, we propose a named Dual-branch Hybrid Visual Networks and Hierarchical Adaptive Fusion Strategy Model (DVHAM). Specifically, we design a dual-branch hybrid network based on transformer and convolutional neural network architecture. This network not only considers the global features of the image but also fully incorporates the local details of the image. In addition, we incorporate the hierarchical information of text to construct a hierarchical adaptive dynamic fusion module. The module employs a paired multihead attention mechanism and an adaptive adjustment strategy based on a gating mechanism. This design enables the model to capture and utilize the complementarity and correlation between the semantic and visual information at different levels in the text model. Simultaneously, it adaptively fuses the modal interaction features containing different information for the final detection task. DVHAM achieves an accuracy of 91.5%, 92.4%, and 90.6% on the Weibo, TWITTER, and PHEME datasets, respectively. This proves the effectiveness of DVHAM in the field of fake news detection.},
  archive      = {J_EAI},
  author       = {Xian Fu and Zhuzhu Zhang and Yu Sun and Tianrun Wu and Hui Zhang and Yaqiang Cao and Qi Cheng and Ningning Zhang},
  doi          = {10.1177/30504554251351227},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {543-561},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Dual-branch hybrid visual networks and hierarchical adaptive fusion strategy: An effective multimodal fake news detection model},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ME2FNet: Muti-level edge-enhanced fusion network for camouflaged object detection. <em>EAI</em>, <em>38</em>(4), 530-542. (<a href='https://doi.org/10.1177/30504554251351219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is an emerging research direction in computer vision in recent years, aiming to segment objects that are visually integrated with the background, which is a valuable task and has attracted increasing interest from researchers. Since camouflaged objects are integrated with their surroundings, their boundaries are also very blurred, and it becomes an important issue in COD to segment the edges of the objects accurately and completely. To address the above issues, in this article, we propose a novel multi-level edge-enhanced fusion for camouflaged object detection network (ME 2 FNet). Specifically, we design a residual texture enhanced module to obtain more refined features from the noise-filled backbone features. Then, we design an edge extraction module (EEM), which aims to extract effective edge semantic information from low-level features and high-level features by a simple local channel attention mechanism. Finally, we design a boundary-guided fusion module, which aims to fuse the previously obtained prior information. It can fuse the edge information extracted by EEM with the features at different levels of the backbone network, and guide the learning under the supervision of ground truth. At the same time, it fuses the high-level global information with the features at different levels, so that the final predicted edge is clearer and the overall structure is more complete. Extensive experiments on three challenging benchmark datasets have shown that ME 2 FNet outperforms multiple leading-edge models in recent years and achieves advanced results under four widely used evaluation metrics.},
  archive      = {J_EAI},
  author       = {Xuwei Tong and Guangjian Zhang and Yuhao Yang},
  doi          = {10.1177/30504554251351219},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {530-542},
  shortjournal = {Eur. Artif. Intell.},
  title        = {ME2FNet: Muti-level edge-enhanced fusion network for camouflaged object detection},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging context-sensitivity for user-centered explainability. <em>EAI</em>, <em>38</em>(4), 496-529. (<a href='https://doi.org/10.1177/30504554251331568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread integration of artificial intelligence (AI) into our daily lives has spurred an escalating demand for explainable AI (XAI). This demand is particularly pronounced in critical domains such as healthcare and finance, where understanding the decision-making processes of AI models is paramount. Despite noteworthy strides in XAI, prevailing approaches often neglect the crucial dimension of context, resulting in explanations that are challenging to comprehend and act upon for different stakeholders. This paper advocates for a paradigm shift towards context-sensitive explainability, tailoring explanations to users’ specific needs and understanding promoting inclusivity and accessibility. We propose a novel context taxonomy and a versatile framework, “ConEX” for developing context-sensitive explanations using any state-of-the-art post hoc explainer. Our empirical user study highlights diverse preferences for contextualization levels, emphasizing the importance of catering to these preferences to build trust and satisfaction in AI systems. Our contributions extend beyond the theoretical realm, offering practical guidance for developing context-sensitive explanations that are tailored to the specific needs of diverse stakeholders. By embracing context-sensitive explainability, we can unlock the true potential of AI, fostering trust, transparency, and informed decision-making across various domains.},
  archive      = {J_EAI},
  author       = {Yasmeen Khaled and Nourhan Ehab},
  doi          = {10.1177/30504554251331568},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {496-529},
  shortjournal = {Eur. Artif. Intell.},
  title        = {Leveraging context-sensitivity for user-centered explainability},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A methodology and a platform for high-quality rich personal data collection#. <em>EAI</em>, <em>38</em>(4), 474-495. (<a href='https://doi.org/10.1177/30504554251333615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, pervasive use of sensors in smart devices, e.g., phones, watches, medical devices, has increased dramatically the availability of personal data. However, existing research on data collection primarily focuses on the objective view of reality, as provided, for instance, by sensors, often neglecting the integration of subjective human input, as provided, for instance, by user answers to questionnaires. This limits substantially the exploitability of the collected data. In this paper, we present a methodology and a platform designed for the collection of a combination of large-scale sensor data and qualitative human feedback. The methodology has been designed to be deployed on top, and enrich functionalities of an existing data collection APP, called iLog, which has been used in large scale, worldwide data collection experiments. The main goal is to put the key actors involved in an experiment, i.e., the researcher in charge, the participant, and iLog in better control of the experiment itself, thus improving the quality and richness of the data collected. The novel functionalities of the resulting platform are: (i) a time-wise representation of the situational context within which the data collection is performed, (ii) an explicit representation of the temporal context within which the data collection is performed, (iii) a calendar-based dashboard for the real-time monitoring of the data collection context(s), and, (iv) a mechanism for the run-time revision of the data collection plan. The practicality and utility of the proposed functionalities are demonstrated in a case study involving 350 University students.},
  archive      = {J_EAI},
  author       = {Ivan Kayongo and Leonardo Malcotti and Haonan Zhao and Fausto Giunchiglia},
  doi          = {10.1177/30504554251333615},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {474-495},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A methodology and a platform for high-quality rich personal data collection#},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A conversational agent that learns to be aligned with the moral value of respect. <em>EAI</em>, <em>38</em>(4), 457-473. (<a href='https://doi.org/10.1177/30504554241311168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Videogame developers typically conduct user experience surveys to gather feedback from users once they have played. Nevertheless, as users may not recall all the details once finished, we propose an ethical conversational agent that respectfully conducts the survey during gameplay. To achieve this without hindering user’s engagement, we resort to reinforcement learning and an ethical embedding algorithm. Specifically, we transform the learning environment so that it guarantees that the agent learns to be respectful (i.e. aligned with the moral value of respect) while pursuing its individual objective of eliciting as much feedback information as possible. When applying this approach to a simple videogame, our comparative tests between the two agents (ethical and unethical) empirically demonstrate that endowing a survey-oriented conversational agent with this moral value of respect avoids disturbing user’s engagement while still pursuing its individual objective, which is to gather as much information as possible.},
  archive      = {J_EAI},
  author       = {Eric Roselló-Marín and Inmaculada Rodríguez and Maite Lopez-Sanchez and Manel Rodríguez-Soto and Juan Antonio Rodríguez-Aguilar},
  doi          = {10.1177/30504554241311168},
  journal      = {The European Journal on Artificial Intelligence},
  month        = {11},
  number       = {4},
  pages        = {457-473},
  shortjournal = {Eur. Artif. Intell.},
  title        = {A conversational agent that learns to be aligned with the moral value of respect},
  volume       = {38},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
