<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJRR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijrr">IJRR - 15</h2>
<ul>
<li><details>
<summary>
(2025). MultiSCOPE: Disambiguating in-hand object poses with proprioception and sequential interactions. <em>IJRR</em>, <em>44</em>(10-11), 1920-1938. (<a href='https://doi.org/10.1177/02783649251315757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint estimation of grasped object pose and extrinsic contacts is central to robust and dexterous manipulation. In this paper, we introduce MultiSCOPE, a state-estimation algorithm that leverages sequential frictional contacts (e.g., pokes) to jointly estimate contact locations and grasped object poses using exclusively proprioception and tactile feedback. Our method addresses the problem of reducing object pose uncertainty by using two complementary particle filters over a series of actions: one to estimate contact location (CPFGrasp) and another to estimate object poses (SCOPE). Our method addresses uncertainty in both robot proprioception and force-torque measurements, which is important for estimating in-hand object pose in the real world. We implement and evaluate our approach on simulated and real-world single-arm and dual-arm robotic systems. We demonstrate that by bringing two objects into contact several times, the robots can infer contact location and object poses simultaneously.},
  archive      = {J_IJRR},
  author       = {Andrea Sipos and Nima Fazeli},
  doi          = {10.1177/02783649251315757},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1920-1938},
  shortjournal = {Int. J. Robot. Res.},
  title        = {MultiSCOPE: Disambiguating in-hand object poses with proprioception and sequential interactions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DABA: Decentralized and accelerated large-scale bundle adjustment. <em>IJRR</em>, <em>44</em>(10-11), 1892-1919. (<a href='https://doi.org/10.1177/02783649241309968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scaling to arbitrarily large bundle adjustment problems requires data and compute to be distributed across multiple devices. Centralized methods in prior works are only able to solve small or medium size problems due to overhead in computation and communication. In this paper, we present a fully decentralized method that alleviates computation and communication bottlenecks to solve arbitrarily large bundle adjustment problems. We achieve this by reformulating the reprojection error and deriving a novel surrogate function that decouples optimization variables from different devices. This function makes it possible to use majorization minimization techniques and reduces bundle adjustment to independent optimization subproblems that can be solved in parallel. Moreover, an efficient closed-form warm start strategy has been presented that always improves bundle adjustment estimates. We further apply Nesterov’s acceleration and adaptive restart to improve convergence while maintaining its theoretical guarantees. Despite limited peer-to-peer communication, our method has provable convergence to first-order critical points under mild conditions. On extensive benchmarks with public datasets, our method converges much faster than decentralized baselines with similar memory usage and communication load. Compared to centralized baselines using a single device, our method, while being decentralized, yields more accurate solutions with significant speedups of up to 953.7x over C e r e s and 174.6x over D e e p L M . Code: https://github.com/facebookresearch/ DABA .},
  archive      = {J_IJRR},
  author       = {Taosha Fan and Joseph Ortiz and Ming Hsiao and Maurizio Monge and Jing Dong and Todd D Murphey and Mustafa Mukadam},
  doi          = {10.1177/02783649241309968},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1892-1919},
  shortjournal = {Int. J. Robot. Res.},
  title        = {DABA: Decentralized and accelerated large-scale bundle adjustment},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FurnitureBench: Reproducible real-world benchmark for long-horizon complex manipulation. <em>IJRR</em>, <em>44</em>(10-11), 1863-1891. (<a href='https://doi.org/10.1177/02783649241304789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL), imitation learning (IL), and task and motion planning (TAMP) have demonstrated impressive performance across various robotic manipulation tasks. However, these approaches have been limited to learning simple behaviors in current real-world manipulation benchmarks, such as pushing or pick-and-place. To enable more complex, long-horizon behaviors of an autonomous robot, we propose to focus on real-world furniture assembly, a complex, long-horizon robotic manipulation task that requires addressing many current robotic manipulation challenges. We present FurnitureBench, a reproducible real-world furniture assembly benchmark aimed at providing a low barrier for entry and being easily reproducible, so that researchers across the world can reliably test their algorithms and compare them against prior work. For ease of use, we provide 200+ hours of pre-collected data (5000+ demonstrations), 3D printable furniture models, a robotic environment setup guide, and systematic task initialization. Furthermore, we provide FurnitureSim, a fast and realistic simulator of FurnitureBench. We benchmark the performance of offline RL, IL, and offline-to-online RL algorithms on our assembly tasks and demonstrate the need to improve such algorithms to be able to solve our tasks in the real world, providing ample opportunities for future research.},
  archive      = {J_IJRR},
  author       = {Minho Heo and Youngwoon Lee and Doohyun Lee and Joseph J Lim},
  doi          = {10.1177/02783649241304789},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1863-1891},
  shortjournal = {Int. J. Robot. Res.},
  title        = {FurnitureBench: Reproducible real-world benchmark for long-horizon complex manipulation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-euclidean motion planning with graphs of geodesically convex sets. <em>IJRR</em>, <em>44</em>(10-11), 1840-1862. (<a href='https://doi.org/10.1177/02783649241302419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing optimal, collision-free trajectories for high-dimensional systems is a challenging and important problem. Sampling-based planners struggle with the dimensionality, whereas trajectory optimizers may get stuck in local minima due to inherent nonconvexities in the optimization landscape. The use of mixed-integer programming to encapsulate these nonconvexities and find globally optimal trajectories has recently shown great promise, thanks in part to tight convex relaxations and efficient approximation strategies that greatly reduce runtimes. These approaches were previously limited to Euclidean configuration spaces, precluding their use with mobile bases or continuous revolute joints. In this paper, we handle such scenarios by modeling configuration spaces as Riemannian manifolds, and we describe a reduction procedure for the zero-curvature case to a mixed-integer convex optimization problem. We further present a method for obtaining approximate solutions via piecewise-linear approximations that is applicable to manifolds of arbitrary curvature. We demonstrate our results on various robot platforms, including producing efficient collision-free trajectories for a PR2 bimanual mobile manipulator.},
  archive      = {J_IJRR},
  author       = {Thomas Cohn and Mark Petersen and Max Simchowitz and Russ Tedrake},
  doi          = {10.1177/02783649241302419},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1840-1862},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Non-euclidean motion planning with graphs of geodesically convex sets},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating strategies enabling novice users to teach plannable hierarchical tasks to robots. <em>IJRR</em>, <em>44</em>(10-11), 1814-1839. (<a href='https://doi.org/10.1177/02783649241301075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from demonstration (LfD) seeks to democratize robotics by enabling non-experts to intuitively program robots to perform novel skills through human task demonstration. Yet, LfD is challenging under a task and motion planning (TAMP) setting, as solving long-horizon manipulation tasks requires the use of hierarchical abstractions. Prior work has studied mechanisms for eliciting demonstrations that include hierarchical specifications for robotics applications but has not examined whether non-roboticist end-users are capable of providing such hierarchical demonstrations without explicit training from a roboticist for each task. We characterize whether, how, and which users can do so. Finding that the result is negative, we develop a series of training domains that successfully enable users to provide demonstrations that exhibit hierarchical abstractions. Our first experiment shows that fewer than half (35.71%) of our subjects provide demonstrations with hierarchical abstractions when not primed. Our second experiment demonstrates that users fail to teach the robot with adequately detailed TAMP abstractions, when not shown a video demonstration of an expert’s teaching strategy. Our experiments reveal the need for fundamentally different approaches in LfD to enable end-users to teach robots generalizable long-horizon tasks without being coached by experts at every step. Toward this goal, we developed and evaluated a set of TAMP domains for LfD in a third study. Positively, we find that experience obtained in different, training domains enables users to provide demonstrations with useful, plannable abstractions on new, test domains just as well as providing a video prescribing an expert’s teaching strategy in the new domain.},
  archive      = {J_IJRR},
  author       = {Nina Moorman and Aman Singh and Manisha Natarajan and Erin Hedlund-Botti and Mariah Schrum and Chuxuan Yang and Lakshmi Seelam and Matthew C. Gombolay and Nakul Gopalan},
  doi          = {10.1177/02783649241301075},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1814-1839},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Investigating strategies enabling novice users to teach plannable hierarchical tasks to robots},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convex geometric motion planning of multi-body systems on lie groups via variational integrators and sparse moment relaxation. <em>IJRR</em>, <em>44</em>(10-11), 1784-1813. (<a href='https://doi.org/10.1177/02783649241296160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports a novel result: with proper robot models based on geometric mechanics, one can formulate the kinodynamic motion planning problems for rigid body systems as exact polynomial optimization problems. Due to the nonlinear rigid body dynamics, the motion planning problem for rigid body systems is nonconvex. Existing global optimization-based methods do not parameterize 3D rigid body motion efficiently; thus, they do not scale well to long-horizon planning problems. We use Lie groups as the configuration space and apply the variational integrator to formulate the forced rigid body dynamics as quadratic polynomials. Then, we leverage Lasserre’s hierarchy of moment relaxation to obtain the globally optimal solution via semidefinite programming. By leveraging the sparsity of the motion planning problem, the proposed algorithm has linear complexity with respect to the planning horizon. This paper demonstrates that the proposed method can provide globally optimal solutions or certificates of infeasibility at the second-order relaxation for 3D drone landing using full dynamics and inverse kinematics for serial manipulators. Moreover, we extend the algorithms to multi-body systems via the constrained variational integrators. The testing cases on cart-pole and drone with cable-suspended load suggest that the proposed algorithms can provide rank-one optimal solutions or nontrivial initial guesses. Finally, we propose strategies to speed up the computation, including an alternative formulation using quaternion, which provides empirically tight relaxations for the drone landing problem at the first-order relaxation.},
  archive      = {J_IJRR},
  author       = {Sangli Teng and Ashkan Jasour and Ram Vasudevan and Maani Ghaffari},
  doi          = {10.1177/02783649241296160},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1784-1813},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Convex geometric motion planning of multi-body systems on lie groups via variational integrators and sparse moment relaxation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAM-RL: Sensing-aware model-based reinforcement learning via differentiable physics-based simulation and rendering. <em>IJRR</em>, <em>44</em>(10-11), 1767-1783. (<a href='https://doi.org/10.1177/02783649241284653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based reinforcement learning is recognized with the potential to be significantly more sample efficient than model-free reinforcement learning. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of model-based reinforcement learning in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectiveness of SAM-RL via extensive experiments. Videos are available on our project webpage.},
  archive      = {J_IJRR},
  author       = {Jun Lv and Yunhai Feng and Cheng Zhang and Shuang Zhao and Lin Shao and Cewu Lu},
  doi          = {10.1177/02783649241284653},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1767-1783},
  shortjournal = {Int. J. Robot. Res.},
  title        = {SAM-RL: Sensing-aware model-based reinforcement learning via differentiable physics-based simulation and rendering},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Morphological symmetries in robotics. <em>IJRR</em>, <em>44</em>(10-11), 1743-1766. (<a href='https://doi.org/10.1177/02783649241282422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive framework for studying and leveraging morphological symmetries in robotic systems. These are intrinsic properties of the robot’s morphology, frequently observed in animal biology and robotics, which stem from the replication of kinematic structures and the symmetrical distribution of mass. We illustrate how these symmetries extend to the robot’s state space and both proprioceptive and exteroceptive sensor measurements, resulting in the equivariance of the robot’s equations of motion and optimal control policies. Thus, we recognize morphological symmetries as a relevant and previously unexplored physics-informed geometric prior, with significant implications for both data-driven and analytical methods used in modeling, control, estimation and design in robotics. For data-driven methods, we demonstrate that morphological symmetries can enhance the sample efficiency and generalization of machine learning models through data augmentation, or by applying equivariant/invariant constraints on the model’s architecture. In the context of analytical methods, we employ abstract harmonic analysis to decompose the robot’s dynamics into a superposition of lower-dimensional, independent dynamics. We substantiate our claims with both synthetic and real-world experiments conducted on bipedal and quadrupedal robots. Lastly, we introduce the repository MorphoSymm to facilitate the practical use of the theory and applications outlined in this work.},
  archive      = {J_IJRR},
  author       = {Daniel Ordoñez Apraez and Giulio Turrisi and Vladimir Kostic and Mario Martin and Antonio Agudo and Francesc Moreno-Noguer and Massimiliano Pontil and Claudio Semini and Carlos Mastalli},
  doi          = {10.1177/02783649241282422},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1743-1766},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Morphological symmetries in robotics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robot learning on the job: Human-in-the-loop autonomy and learning during deployment. <em>IJRR</em>, <em>44</em>(10-11), 1727-1742. (<a href='https://doi.org/10.1177/02783649241273901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of computing powers and recent advances in deep learning, we have witnessed impressive demonstrations of novel robot capabilities in research settings. Nonetheless, these learning systems exhibit brittle generalization and require excessive training data for practical tasks. To harness the capabilities of state-of-the-art robot learning models while embracing their imperfections, we present Sirius, a principled framework for humans and robots to collaborate through a division of work. In this framework, partially autonomous robots are tasked with handling a major portion of decision-making where they work reliably; meanwhile, human operators monitor the process and intervene in challenging situations. Such a human–robot team ensures safe deployments in complex tasks. Further, we introduce a new learning algorithm to improve the policy’s performance on the data collected from the task executions. The core idea is re-weighing training samples with approximated human trust and optimizing the policies with weighted behavioral cloning. We evaluate Sirius in simulation and on real hardware, showing that Sirius consistently outperforms baselines over a collection of contact-rich manipulation tasks, achieving an 8% boost in simulation and 27% on real hardware than the state-of-the-art methods in policy success rate, with twice faster convergence and 85% memory size reduction. Videos and more details are available at https://ut-austin-rpl.github.io/sirius/ .},
  archive      = {J_IJRR},
  author       = {Huihan Liu and Soroush Nasiriany and Lance Zhang and Zhiyao Bao and Yuke Zhu},
  doi          = {10.1177/02783649241273901},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1727-1742},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Robot learning on the job: Human-in-the-loop autonomy and learning during deployment},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantically controllable augmentations for generalizable robot learning. <em>IJRR</em>, <em>44</em>(10-11), 1705-1726. (<a href='https://doi.org/10.1177/02783649241273686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalization to unseen real-world scenarios for robot manipulation requires exposure to diverse datasets during training. However, collecting large real-world datasets is intractable due to high operational costs. For robot learning to generalize despite these challenges, it is essential to leverage sources of data or priors beyond the robot’s direct experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a data source. These generative models encompass a broad range of real-world scenarios beyond a robot’s direct experience and can synthesize novel synthetic experiences that expose robotic agents to additional world priors aiding real-world generalization at no extra cost. In particular, our approach leverages pre-trained generative models as an effective tool for data augmentation. We propose a generative augmentation framework for semantically controllable augmentations and rapidly multiplying robot datasets while inducing rich variations that enable real-world generalization. Based on diverse augmentations of robot data, we show how scalable robot manipulation policies can be trained and deployed both in simulation and in unseen real-world environments such as kitchens and table-tops. By demonstrating the effectiveness of image-text generative models in diverse real-world robotic applications, our generative augmentation framework provides a scalable and efficient path for boosting generalization in robot learning at no extra human cost.},
  archive      = {J_IJRR},
  author       = {Zoey Chen and Zhao Mandi and Homanga Bharadhwaj and Mohit Sharma and Shuran Song and Abhishek Gupta and Vikash Kumar},
  doi          = {10.1177/02783649241273686},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1705-1726},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Semantically controllable augmentations for generalizable robot learning},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion policy: Visuomotor policy learning via action diffusion. <em>IJRR</em>, <em>44</em>(10-11), 1684-1704. (<a href='https://doi.org/10.1177/02783649241273668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot’s visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 15 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details are available (diffusion-policy.cs.columbia.edu).},
  archive      = {J_IJRR},
  author       = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},
  doi          = {10.1177/02783649241273668},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1684-1704},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Diffusion policy: Visuomotor policy learning via action diffusion},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-optimal ergodic search: Multiscale coverage in minimum time. <em>IJRR</em>, <em>44</em>(10-11), 1664-1683. (<a href='https://doi.org/10.1177/02783649241273597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Search and exploration capabilities are essential for robots to inspect hazardous areas, support scientific expeditions in extreme environments, and potentially save human lives in natural disasters. The variability of scale in these problems requires robots to reason about time alongside their dynamics and sensor capabilities to effectively assess and explore for information. Recent advances in ergodic search methods have shown promise in supporting trajectory planning for exploration in continuous, multiscale environments with dynamics consideration. However, these methods are still limited by their inability to effectively reason about and adapt the time to explore in response to their environment. This ability is crucial for adapting exploration to variable-resolution information-gathering tasks. To address this limitation, this paper poses the time-optimal ergodic search problem and investigates solutions for fast, multiscale, and adaptive robotic exploration trajectories. The problem is formulated as a minimum-time problem with an ergodic inequality constraint whose upper bound specifies the amount of coverage needed. We show the existence of optimal solutions using Pontryagin’s conditions of optimality, and we demonstrate effective, minimum-time coverage numerically through a direct transcription optimization approach. The efficacy of the approach in generating time-optimal search trajectories is demonstrated in simulation under several nonlinear dynamic constraints, and in a physical experiment using a drone in a cluttered environment. We find that constraints such as obstacle avoidance are readily integrated into our formulation, and we show through an ablation study the flexibility of search capabilities at various scales. Last, we contribute a receding-horizon formulation of time-optimal ergodic search for sensor-driven information-gathering and demonstrate improved adaptive sampling capabilities in localization tasks.},
  archive      = {J_IJRR},
  author       = {Dayi Ethan Dong and Henry Berger and Ian Abraham},
  doi          = {10.1177/02783649241273597},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1664-1683},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Time-optimal ergodic search: Multiscale coverage in minimum time},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soft yet secure: Exploring membrane buckling for achieving a versatile grasp with a rotation-driven squeezing gripper. <em>IJRR</em>, <em>44</em>(10-11), 1648-1663. (<a href='https://doi.org/10.1177/02783649241272120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, there is a growing demand for versatile robotic grippers that facilitate adaptive grasping across a range of objects, characterized by diverse attributes such as shapes, sizes, and mechanical properties. This research introduces a new soft gripper, denoted as ROSE ( RO tation-based S queezing Gripp E r), dedicated to the torsional buckling phenomenon to achieve its grasping capability. The inherent gripping pattern, formed through a single rotational actuation, enables ROSE to dynamically accommodate various objects (relatively smaller than the diameter of ROSE), even in challenging environments like an oil container, without a complicated controller. Experimental tests demonstrate that ROSE exhibits substantial gripping force, reaching up to more than 300 N in the specific setup, and a remarkable payload-to-weight ratio. Especially, ROSE can maintain mechanical integrity through long-term open-close operation cycles in a specific condition. In this paper, we also introduce non-linear simulations for the elaboration of behaviors of ROSE concerning different morphologies, encompassing geometry configurations and material properties. The findings highlight a significant correlation between morphological features and grasping performance, leading to the refinement of a dependable version of ROSE. This refined version was subsequently assessed through experimental trials in crop harvesting tasks, where ROSE demonstrated high success rates in picking both individual and clustered crops, regardless of their soft or stiff characteristics. Project’s website with videos: https://sites.google.com/view/rosesoftgripper .},
  archive      = {J_IJRR},
  author       = {Khoi Thanh Nguyen and Nhan Huu Nguyen and Van Anh Ho},
  doi          = {10.1177/02783649241272120},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1648-1663},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Soft yet secure: Exploring membrane buckling for achieving a versatile grasp with a rotation-driven squeezing gripper},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and robust learned single-view depth-aided monocular visual-inertial initialization. <em>IJRR</em>, <em>44</em>(10-11), 1619-1647. (<a href='https://doi.org/10.1177/02783649241262452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In monocular visual-inertial navigation, it is desirable to initialize the system as quickly and robustly as possible. A state-of-the-art initialization method typically constructs a linear system to find a closed-form solution using the image features and inertial measurements and then refines the states with a nonlinear optimization. These methods generally require a few seconds of data, which however can be expedited (less than a second) by adding constraints from a robust but only up-to-scale monocular depth network in the nonlinear optimization. To further accelerate this process, in this work, we leverage the scale-less depth measurements instead in the linear initialization step that is performed prior to the nonlinear one, which only requires a single depth image for the first frame. Importantly, we show that the typical estimation of all feature states independently in the closed-form solution can be modeled as estimating only the scale and bias parameters of the learned depth map. As such, our formulation enables building a smaller minimal problem than the state of the art, which can be seamlessly integrated into RANSAC for robust estimation. Experiments show that our method has state-of-the-art initialization performance in simulation as well as on popular real-world datasets (TUM-VI, and EuRoC MAV). For the TUM-VI dataset in simulation as well as real-world, we demonstrate the superior initialization performance with only a 0.3 s window of data, which is the smallest ever reported, and validate that our method can initialize more often, robustly, and accurately in different challenging scenarios.},
  archive      = {J_IJRR},
  author       = {Nathaniel Merrill and Patrick Geneva and Saimouli Katragadda and Chuchu Chen and Guoquan Huang},
  doi          = {10.1177/02783649241262452},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1619-1647},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Fast and robust learned single-view depth-aided monocular visual-inertial initialization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial. <em>IJRR</em>, <em>44</em>(10-11), 1617-1618. (<a href='https://doi.org/10.1177/02783649251367502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJRR},
  author       = {Kostas Bekris and Kris Hauser and Sylvia Herbert and Jingjin Yu},
  doi          = {10.1177/02783649251367502},
  journal      = {The International Journal of Robotics Research},
  month        = {9},
  number       = {10-11},
  pages        = {1617-1618},
  shortjournal = {Int. J. Robot. Res.},
  title        = {Editorial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
