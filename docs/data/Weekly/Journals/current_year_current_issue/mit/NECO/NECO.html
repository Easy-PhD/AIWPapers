<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco">NECO - 6</h2>
<ul>
<li><details>
<summary>
(2025). Sequential learning in the dense associative memory. <em>NECO</em>, <em>37</em>(10), 1877-1924. (<a href='https://doi.org/10.1162/neco.a.20'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential learning involves learning tasks in a sequence and proves challenging for most neural networks. Biological neural networks regularly succeed at the sequential learning challenge and are even capable of transferring knowledge both forward and backward between tasks. Artificial neural networks often totally fail to transfer performance between tasks and regularly suffer from degraded performance or catastrophic forgetting on previous tasks. Models of associative memory have been used to investigate the discrepancy between biological and artificial neural networks due to their biological ties and inspirations, of which the Hopfield network is the most studied model. The dense associative memory (DAM), or modern Hopfield network, generalizes the Hopfield network, allowing for greater capacities and prototype learning behaviors while still retaining the associative memory structure. We give a substantial review of the sequential learning space with particular respect to the Hopfield network and associative memories. We present the first published benchmarks of sequential learning in the DAM using various sequential learning techniques and analyze the results of the sequential learning to demonstrate previously unseen transitions in the behavior of the DAM. This letter also discusses the departure from biological plausibility that may affect the utility of the DAM as a tool for studying biological neural networks. We present our findings, including the effectiveness of a range of state-of-the-art sequential learning methods when applied to the DAM, and use these methods to further the understanding of DAM properties and behaviors.},
  archive      = {J_NECO},
  author       = {McAlister, Hayden and Robins, Anthony and Szymanski, Lech},
  doi          = {10.1162/neco.a.20},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1877-1924},
  shortjournal = {Neural Comput.},
  title        = {Sequential learning in the dense associative memory},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distance-based logistic matrix factorization. <em>NECO</em>, <em>37</em>(10), 1863-1876. (<a href='https://doi.org/10.1162/neco.a.25'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix factorization is a central paradigm in matrix completion and collaborative filtering. Low-rank factorizations have been extremely successful in reconstructing and generalizing high-dimensional data in a wide variety of machine learning problems from drug-target discovery to music recommendations. Virtually all proposed matrix factorization techniques use the dot product between latent factor vectors to reconstruct the original matrix. We propose a reformulation of the widely used logistic matrix factorization in which we use the distance, rather than the dot product, to measure similarity between latent factors. We show that this measure of similarity, which can draw nonlinear decision boundaries and respect triangle inequalities between points, has more expressive power and modeling capacity. The distance-based model implemented in Euclidean and hyperbolic space outperforms previous formulations of logistic matrix factorization on three different biological test problems with disparate structure and statistics. In particular, we show that a distance-based factorization (1) generalizes better to test data, (2) achieves optimal performance at lower factor space dimension, and (3) clusters data better in the latent factor space.},
  archive      = {J_NECO},
  author       = {Praturu, Anoop and Sharpee, Tatyana O.},
  doi          = {10.1162/neco.a.25},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1863-1876},
  shortjournal = {Neural Comput.},
  title        = {Distance-based logistic matrix factorization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid reweighting of sensory inputs and predictions in visual perception. <em>NECO</em>, <em>37</em>(10), 1853-1862. (<a href='https://doi.org/10.1162/neco.a.26'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A striking perceptual phenomenon has recently been described wherein people report seeing abrupt jumps in the location of a smoothly moving object (“position resets”). Here, we show that this phenomenon can be understood within the framework of recursive Bayesian estimation as arising from transient gain changes, temporarily prioritizing sensory input over predictive beliefs. From this perspective, position resets reveal a capacity for rapid adaptive precision weighting in human visual perception and offer a possible test bed within which to study the timing and flexibility of sensory gain control.},
  archive      = {J_NECO},
  author       = {Turner, William and Kwon, Oh-Sang and Kim, Minwoo J.B. and Hogendoorn, Hinze},
  doi          = {10.1162/neco.a.26},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1853-1862},
  shortjournal = {Neural Comput.},
  title        = {Rapid reweighting of sensory inputs and predictions in visual perception},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer models for signal processing: Scaled dot-product attention implements constrained filtering. <em>NECO</em>, <em>37</em>(10), 1839-1852. (<a href='https://doi.org/10.1162/neco.a.29'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable success of the transformer machine learning architecture for processing language sequences far exceeds the performance of classical signal processing methods. A unique component of transformer models is the scaled dot-product attention (SDPA) layer, which does not appear to have an analog in prior signal processing algorithms. Here, we show that SDPA operates using a novel principle that projects the current state estimate onto the space spanned by prior estimates. We show that SDPA, when used for causal recursive state estimation, implements constrained state estimation in circumstances where the constraint is unknown and may be time varying. Since constraints in high-dimensional space may represent the complex relationships that define nonlinear signals and models, this suggests that the SDPA layer and transformer models leverage constrained estimation to achieve their success. This also suggests that transformers and the SPDA layer could be a computational model for previously unexplained capabilities of human behavior.},
  archive      = {J_NECO},
  author       = {Sanger, Terence D.},
  doi          = {10.1162/neco.a.29},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1839-1852},
  shortjournal = {Neural Comput.},
  title        = {Transformer models for signal processing: Scaled dot-product attention implements constrained filtering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Firing rate models as associative memory: Synaptic design for robust retrieval. <em>NECO</em>, <em>37</em>(10), 1807-1838. (<a href='https://doi.org/10.1162/neco.a.28'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firing rate models are dynamical systems widely used in applied and theoretical neuroscience to describe local cortical dynamics in neuronal populations. By providing a macroscopic perspective of neuronal activity, these models are essential for investigating oscillatory phenomena, chaotic behavior, and associative memory processes. Despite their widespread use, the application of firing rate models to associative memory networks has received limited mathematical exploration, and most existing studies are focused on specific models. Conversely, well-established associative memory designs, such as Hopfield networks, lack key biologically relevant features intrinsic to firing rate models, including positivity and interpretable synaptic matrices reflecting the action of long-term potentiation and long-term depression. To address this gap, we propose a general framework that ensures the emergence of rescaled memory patterns as stable equilibria in the firing rate dynamics. Furthermore, we analyze the conditions under which the memories are locally and globally asymptotically stable, providing insights into constructing biologically plausible and robust systems for associative memory retrieval.},
  archive      = {J_NECO},
  author       = {Betteti, Simone and Baggio, Giacomo and Bullo, Francesco and Zampieri, Sandro},
  doi          = {10.1162/neco.a.28},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1807-1838},
  shortjournal = {Neural Comput.},
  title        = {Firing rate models as associative memory: Synaptic design for robust retrieval},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity deconstrains component limitations in sensorimotor control. <em>NECO</em>, <em>37</em>(10), 1783-1806. (<a href='https://doi.org/10.1162/neco.a.24'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human sensorimotor control is remarkably fast and accurate at the system level despite severe speed-accuracy trade-offs at the component level. The discrepancy between the contrasting speed-accuracy trade-offs at these two levels is a paradox. Meanwhile, speed accuracy trade-offs, heterogeneity, and layered architectures are ubiquitous in nerves, skeletons, and muscles, but they have only been studied in isolation using domain-specific models. In this article, we develop a mechanistic model for how component speed-accuracy trade-offs constrain sensorimotor control that is consistent with Fitts’ law for reaching. The model suggests that diversity among components deconstrains the limitations of individual components in sensorimotor control. Such diversity-enabled sweet spots (DESSs) are ubiquitous in nature, explaining why large heterogeneities exist in the components of biological systems and how natural selection routinely evolves systems with fast and accurate responses using imperfect components.},
  archive      = {J_NECO},
  author       = {Nakahira, Yorie and Liu, Quanying and Deng, Xiyu and Sejnowski, Terrence J. and Doyle, John C.},
  doi          = {10.1162/neco.a.24},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1783-1806},
  shortjournal = {Neural Comput.},
  title        = {Diversity deconstrains component limitations in sensorimotor control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
