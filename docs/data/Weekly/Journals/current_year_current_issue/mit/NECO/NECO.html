<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco">NECO - 4</h2>
<ul>
<li><details>
<summary>
(2025). Feature normalization prevents collapse of noncontrastive learning dynamics. <em>NECO</em>, <em>37</em>(11), 2079-2124. (<a href='https://doi.org/10.1162/neco.a.27'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning is a self-supervised representation learning framework where two positive views generated through data augmentation are made similar by an attraction force in a data representation space, while a repulsive force makes them far from negative examples. Noncontrastive learning, represented by BYOL and SimSiam, gets rid of negative examples and improves computational efficiency. While learned representations may collapse into a single point due to the lack of the repulsive force at first sight, Tian et al. ( 2021 ) revealed through learning dynamics analysis that the representations can avoid collapse if data augmentation is sufficiently stronger than regularization. However, their analysis does not take into account commonly used feature normalization, a normalizer before measuring the similarity of representations, and hence excessively strong regularization may still collapse the dynamics, an unnatural behavior under the presence of feature normalization. Therefore, we extend the previous theory based on the L2 loss by considering the cosine loss instead, which involves feature normalization. We show that the cosine loss induces sixth-order dynamics (while the L2 loss induces a third-order one), in which a stable equilibrium dynamically emerges even if there are only collapsed solutions with given initial parameters. Thus, we offer a new understanding that feature normalization plays an important role in robustly preventing the dynamics collapse.},
  archive      = {J_NECO},
  author       = {Bao, Han},
  doi          = {10.1162/neco.a.27},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2079-2124},
  shortjournal = {Neural Comput.},
  title        = {Feature normalization prevents collapse of noncontrastive learning dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling higher-order interactions in sparse and heavy-tailed neural population activity. <em>NECO</em>, <em>37</em>(11), 2011-2078. (<a href='https://doi.org/10.1162/neco.a.35'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons process sensory stimuli efficiently, showing sparse yet highly variable ensemble spiking activity involving structured higher-order interactions. Notably, while neural populations are mostly silent, they occasionally exhibit highly synchronous activity, resulting in sparse and heavy-tailed spike-count distributions. However, its mechanistic origin—specifically, what types of nonlinear properties in individual neurons induce such population-level patterns—remains unclear. In this study, we derive sufficient conditions under which the joint activity of homogeneous binary neurons generates sparse and widespread population firing rate distributions in infinitely large networks. We then propose a subclass of exponential family distributions that satisfy this condition. This class incorporates structured higher-order interactions with alternating signs and shrinking magnitudes, along with a base-measure function that offsets distributional concentration, giving rise to parameter-dependent sparsity and heavy-tailed population firing rate distributions. Analysis of recurrent neural networks that recapitulate these distributions reveals that individual neurons possess threshold-like nonlinearity, followed by supralinear activation that jointly facilitates sparse and synchronous population activity. These nonlinear features resemble those in modern Hopfield networks, suggesting a connection between widespread population activity and the network’s memory capacity. The theory establishes sparse and heavy-tailed distributions for binary patterns, forming a foundation for developing energy-efficient spike-based learning machines.},
  archive      = {J_NECO},
  author       = {Rodríguez-Domínguez, Ulises and Shimazaki, Hideaki},
  doi          = {10.1162/neco.a.35},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2011-2078},
  shortjournal = {Neural Comput.},
  title        = {Modeling higher-order interactions in sparse and heavy-tailed neural population activity},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Encoding of numerosity with robustness to object and scene identity in biologically inspired object recognition networks. <em>NECO</em>, <em>37</em>(11), 1975-2010. (<a href='https://doi.org/10.1162/neco.a.30'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Number sense, the ability to rapidly estimate object quantities in a visual scene without precise counting, is a crucial cognitive capacity found in humans and many other animals. Recent studies have identified artificial neurons tuned to numbers of items in biologically inspired vision models, even before training, and proposed these artificial neural networks as candidate models for the emergence of number sense in the brain. But real-world numerosity perception requires abstraction from the properties of individual objects and their contexts, unlike the simplified dot patterns used in previous studies. Using novel synthetically generated photorealistic stimuli, we show that deep convolutional neural networks optimized for object recognition encode information on approximate numerosity across diverse objects and scene types, which could be linearly read out from distributed activity patterns of later convolutional layers of different network architectures tested. In contrast, untrained networks with random weights failed to represent numerosity with abstractness to other visual properties and instead captured mainly low-level visual features. Our findings emphasize the importance of using complex, naturalistic stimuli to investigate mechanisms of number sense in both biological and artificial systems, and they suggest that the capacity of untrained networks to account for early-life numerical abilities should be reassessed. They further point to a possible, so far underappreciated, contribution of the brain's ventral visual pathway to representing numerosity with abstractness to other high-level visual properties.},
  archive      = {J_NECO},
  author       = {Chapalain, Thomas and Thirion, Bertrand and Eger, Evelyn},
  doi          = {10.1162/neco.a.30},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {1975-2010},
  shortjournal = {Neural Comput.},
  title        = {Encoding of numerosity with robustness to object and scene identity in biologically inspired object recognition networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A chimera model for motion anticipation in the retina and the primary visual cortex. <em>NECO</em>, <em>37</em>(11), 1925-1974. (<a href='https://doi.org/10.1162/neco.a.34'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a mean field model of the primary visual cortex (V1), connected to a realistic retina model, to study the impact of the retina on motion anticipation. We first consider the case where the retina does not itself provide anticipation—which is then only triggered by a cortical mechanism, the “anticipation by latency”—and unravel the effects of the retinal input amplitude, of stimulus features such as speed and contrast and of the size of cortical extensions and fiber conduction speed. Then we explore the changes in the cortical wave of anticipation when V1 is triggered by retina-driven anticipatory mechanisms: gain control and lateral inhibition by amacrine cells. Here, we show how retinal and cortical anticipation combine to provide an efficient processing where the simulated cortical response is in advance over the moving object that triggers this response, compensating the delays in visual processing.},
  archive      = {J_NECO},
  author       = {Emonet, Jérôme and Souihel, Selma and Chavane, Frédéric and Destexhe, Alain and Volo, Matteo di and Cessac, Bruno},
  doi          = {10.1162/neco.a.34},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {1925-1974},
  shortjournal = {Neural Comput.},
  title        = {A chimera model for motion anticipation in the retina and the primary visual cortex},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
