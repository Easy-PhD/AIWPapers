<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>mit</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alj">ALJ - 9</h2>
<ul>
<li><details>
<summary>
(2025). Automating the search for artificial life with foundation models. <em>ALJ</em>, <em>31</em>(3), 368-396. (<a href='https://doi.org/10.1162/ARTL.a.8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial and error to discover the configurations of lifelike simulations. This article presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called automated search for Artificial Life (ASAL), (a) finds simulations that produce target phenomena, (b) discovers simulations that generate temporally open-ended novelty, and (c) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates, including Boids, Particle Life, the Game of Life, Lenia, and neural cellular automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids life-forms, as well as cellular automata that are open-ended like Conway’s Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.},
  archive      = {J_ALJ},
  author       = {Kumar, Akarsh and Lu, Chris and Kirsch, Louis and Tang, Yujin and Stanley, Kenneth O. and Isola, Phillip and Ha, David},
  doi          = {10.1162/ARTL.a.8},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {368-396},
  shortjournal = {Artif. Life},
  title        = {Automating the search for artificial life with foundation models},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive distinctions as a language for cognitive science: Comparing methods of description in a model of referential communication. <em>ALJ</em>, <em>31</em>(3), 345-367. (<a href='https://doi.org/10.1162/artl_a_00475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An analysis of the language we use in scientific practice is critical to developing more rigorous and sound methodologies. This article argues that how certain methods of description are commonly employed in cognitive science risks obscuring important features of an agent’s cognition. We propose to make explicit a method of description whereby the concept of cognitive distinctions is the core principle. A model of referential communication is developed and analyzed as a platform to compare methods of description. We demonstrate that cognitive distinctions, realized in a graph theoretic formalism, better describe the behavior and perspective of a simple model agent than other, less systematic or natural language–dependent methods. We then consider how different descriptions relate to one another in the broader methodological framework of minimally cognitive behavior. Finally, we explore the consequences of, and challenges for, cognitive distinctions as a useful concept and method in the tool kit of cognitive scientists.},
  archive      = {J_ALJ},
  author       = {Gaul, Thomas M. and Izquierdo, Eduardo J.},
  doi          = {10.1162/artl_a_00475},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {345-367},
  shortjournal = {Artif. Life},
  title        = {Cognitive distinctions as a language for cognitive science: Comparing methods of description in a model of referential communication},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behaviour diversity in a walking and climbing centipede-like virtual creature. <em>ALJ</em>, <em>31</em>(3), 321-344. (<a href='https://doi.org/10.1162/artl_a_00476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot controllers are often optimized for a single robot in a single environment. This approach proves brittle, as such a controller will often fail to produce sensible behavior for a new morphology or environment. In comparison, animal gaits are robust and versatile. By observing animals, and attempting to extract general principles of locomotion from their movement, we aim to design a single, decentralized controller applicable to diverse morphologies and environments. The controller implements the three components of (a) undulation, (b) peristalsis, and (c) leg motion, which we believe are the essential elements in most animal gaits. This work is a first step toward a general controller. Accordingly, the controller has been evaluated on a limited range of simulated centipede-like robot morphologies. The centipede is chosen as inspiration because it moves using both body contractions and legged locomotion. For a controller to work in qualitatively different settings, it must also be able to exhibit qualitatively different behaviors. We find that six different modes of locomotion emerge from our controller in response to environmental and morphological changes. We also find that different parts of the centipede model can exhibit different modes of locomotion, simultaneously, based on local morphological features. This controller can potentially aid in the design or evolution of robots, by quickly testing the potential of a morphology, or be used to get insights about underlying locomotion principles in the centipede.},
  archive      = {J_ALJ},
  author       = {Norstein, Emma Stensby and Yasui, Kotaro and Kano, Takeshi and Ishiguro, Akio and Glette, Kyrre},
  doi          = {10.1162/artl_a_00476},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {321-344},
  shortjournal = {Artif. Life},
  title        = {Behaviour diversity in a walking and climbing centipede-like virtual creature},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benefit game 2.0: Alien seaweed Swarms—Exploring the interplay of human activity and environmental sustainability. <em>ALJ</em>, <em>31</em>(3), 304-320. (<a href='https://doi.org/10.1162/artl_a_00468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Benefit Game 2.0, a multiscreen Artificial Life gameplay installation. Saccharina latissima , a seaweed species economically beneficial to humans but threatened by overexploitation, motivates the creation of this artwork. Technically, the authors create an underwater virtual ecosystem consisting of a seaweed swarm and symbiotic fungi, created using procedural content generation via machine learning and rule-based methods. Moreover, the work features a unique cybernetic loop structure, incorporating audience observation and game token interactions. This virtual system is also symbolically influenced in real time by indoor carbon dioxide measurements, serving as an artistic metaphor for the broader impacts of climate change. This integration with the physical game machine underscores the fragile relationship between human activities and the environment under severe global climate change and immerses the audience in the challenging balance between sustainability and profit seeking in this context.},
  archive      = {J_ALJ},
  author       = {Fei, Dan-Lu and Wu, Zi-Wei and Zhang, Kang},
  doi          = {10.1162/artl_a_00468},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {304-320},
  shortjournal = {Artif. Life},
  title        = {Benefit game 2.0: Alien seaweed Swarms—Exploring the interplay of human activity and environmental sustainability},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complexity, artificial life, and artificial intelligence. <em>ALJ</em>, <em>31</em>(3), 289-303. (<a href='https://doi.org/10.1162/artl_a_00462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scientific fields of complexity, Artificial Life (ALife), and artificial intelligence (AI) share commonalities: historic, conceptual, methodological, and philosophical. Although their origins trace back to the 1940s birth of cybernetics, they were able to develop properly only as modern information technology became available. In this perspective, I offer a personal (and thus biased) account of the expectations and limitations of these fields, some of which have their roots in the limits of formal systems. I use interactions, self-organization, emergence, and balance to compare different aspects of complexity, ALife, and AI. Even when the trajectory of the article is influenced by my personal experience, the general questions posed (which outweigh the answers) will, I hope, be useful in aligning efforts in these fields toward overcoming—or accepting—their limits.},
  archive      = {J_ALJ},
  author       = {Gershenson, Carlos},
  doi          = {10.1162/artl_a_00462},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {289-303},
  shortjournal = {Artif. Life},
  title        = {Complexity, artificial life, and artificial intelligence},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolvability in artificial development of large, complex structures and the principle of terminal addition. <em>ALJ</em>, <em>31</em>(3), 276-288. (<a href='https://doi.org/10.1162/artl_a_00460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epigenetic tracking (ET) is a model of development that is capable of generating diverse, arbitrary, complex three-dimensional cellular structures starting from a single cell. The generated structures have a level of complexity (in terms of the number of cells) comparable to multicellular biological organisms. In this article, we investigate the evolvability of the development of a complex structure inspired by the “French flag” problem: an “Italian Anubis” (a three-dimensional, doglike figure patterned in three colors). Genes during development are triggered in ET at specific developmental stages, and the fitness of individuals during simulated evolution is calculated after a certain stage. When this evaluation stage was allowed to evolve, genes that were triggered at later stages of development tended to be incorporated into the genome later during evolutionary runs. This suggests the emergence of the property of terminal addition in this system. When the principle of terminal addition was explicitly incorporated into ET, and was the sole mechanism for introducing morphological innovation, evolvability improved markedly, leading to the development of structures much more closely approximating the target at a much lower computational cost.},
  archive      = {J_ALJ},
  author       = {Fontana, Alessandro and Wróbel, Borys},
  doi          = {10.1162/artl_a_00460},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {276-288},
  shortjournal = {Artif. Life},
  title        = {Evolvability in artificial development of large, complex structures and the principle of terminal addition},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous evolution in the NK treadmill model. <em>ALJ</em>, <em>31</em>(3), 256-275. (<a href='https://doi.org/10.1162/artl_a_00467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The NK fitness landscape is a well-known model with which to study evolutionary dynamics in landscapes of different ruggedness. However, the model is static, and genomes are typically small, allowing observations over only a short adaptive period. Here we introduce an extension to the model that allows the experimenter to set the velocity at which the landscape changes independently from other parameters, such as the ruggedness or the mutation rate. We find that, similar to the previously observed complexity catastrophe, where evolution comes to a halt when environments become too complex due to overly high degrees of epistasis, here the same phenomenon occurs when changes happen too rapidly. Our expanded model also preserves essential properties of the static NK landscape, allowing for proper comparisons between static and dynamic landscapes.},
  archive      = {J_ALJ},
  author       = {Mehra, Priyanka and Hintze, Arend},
  doi          = {10.1162/artl_a_00467},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {256-275},
  shortjournal = {Artif. Life},
  title        = {Continuous evolution in the NK treadmill model},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neurons as autoencoders. <em>ALJ</em>, <em>31</em>(3), 250-255. (<a href='https://doi.org/10.1162/artl_c_00461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter presents the idea that neural backpropagation is exploiting dendritic processing to enable individual neurons to perform autoencoding. Using a very simple connection weight search heuristic and artificial neural network model, the effects of interleaving autoencoding for each neuron in a hidden layer of a feedforward network are explored. This is contrasted with the equivalent standard layered approach to autoencoding. It is shown that such individualized processing is not detrimental and can improve network learning.},
  archive      = {J_ALJ},
  author       = {Bull, Larry},
  doi          = {10.1162/artl_c_00461},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {250-255},
  shortjournal = {Artif. Life},
  title        = {Neurons as autoencoders},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A word from the editors. <em>ALJ</em>, <em>31</em>(3), 249. (<a href='https://doi.org/10.1162/ARTL.e.11'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue of contributed articles, we are pleased to share with you a diversity of ALife research.In his letter “Neurons as Autoencoders,” Bull brings together ideas from backpropagation, dendritic computing, the NK model, and autoencoders to augment standard models of neuronal autoencoding. The NK model reappears in the article “Continuous Evolution in the NK Treadmill Model,” in which Mehra and Hintze extend the originally static model to allow it to change in a parameterized manner and investigate how evolution behaves at different rates of change. In “Evolvability in Artificial Development of Large Complex Structures and the Principle of Terminal Addition,” Fontana and Wróbel examine evo-devo processes and the emergence of modification to the later stages of a developmental process.In a historical and philosophical perspective, Gershenson offers a personal account that compares various aspects of, and raises questions about, the closely related topics of “Complexity, Artificial Life, and Artificial Intelligence.”Games are an interesting subtopic in ALife, useful both for understanding complex systems and for outreach. In “Benefit Game 2.0: Alien Seaweed Swarms—Exploring the Interplay of Human Activity and Environmental Sustainability,” Fei, Wu, and Zhang present their immersive seaweed ecosystem simulation game that responds not only to audience input but also to the real-world environmental state. Moving from plant to animal behavior, in “Behaviour Diversity in a Walking and Climbing Centipede-Like Virtual Creature,” Norstein et al. take inspiration from arthropod multilegged locomotion to help develop robotic controllers robust to morphological and environmental variation.In their methodological article “Cognitive Distinctions as a Language for Cognitive Science: Comparing Methods of Description in a Model of Referential Communication,” Gaul and Izquierdo examine the effect of choices of language and terminology on theoretical frameworks in cognitive science and use a model to expose the consequences. In another methodological work, “Automating the Search for Artificial Life With Foundation Models,” Kumar et al. exploit concepts of artificial intelligence’s foundation models to develop a new approach to searching for lifelike behaviors, applied to a range of ALife simulations.We thank all the authors for the time and care they took to perform their research and present their work. We hope you enjoy the results!},
  archive      = {J_ALJ},
  author       = {Dorin, Alan and Stepney, Susan},
  doi          = {10.1162/ARTL.e.11},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {249},
  shortjournal = {Artif. Life},
  title        = {A word from the editors},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="coli">COLI - 9</h2>
<ul>
<li><details>
<summary>
(2025). Natural language processing RELIES on linguistics. <em>COLI</em>, <em>51</em>(3), 1009-1032. (<a href='https://doi.org/10.1162/coli_a_00560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES , which encapsulates six major facets where linguistics contributes to NLP: R esources, E valuation, L ow-resource settings, I nterpretability, E xplanation, and the S tudy of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-à-vis systems of human language.},
  archive      = {J_COLI},
  author       = {Opitz, Juri and Wein, Shira and Schneider, Nathan},
  doi          = {10.1162/coli_a_00560},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1009-1032},
  shortjournal = {Comput. Lingu.},
  title        = {Natural language processing RELIES on linguistics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated essay scoring. <em>COLI</em>, <em>51</em>(3), 1005-1008. (<a href='https://doi.org/10.1162/coli_r_00513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated essay scoring is concerned with the development of language technologies that make it possible for an essay—or any piece of writing for that matter—to be evaluated or scored by a computer. These technologies find their utility primarily in the context of educational measurement, where they serve a dual purpose. On the one hand, they provide crucial support to educators and institutions, facilitating the assessment of students’ writing skills and content knowledge. A good example is the TOEFL iBT®, the Internet-based Test of English as a Foreign Language, administered by the Educational Testing Service and widely adopted by institutions worldwide. On the other hand, these technologies benefit writers themselves, including students, by offering a platform to assess and enhance their writing skills. One illustrative tool for this purpose is the Write &amp; Improve software developed at the University of Cambridge.The field of automated essay scoring emerged in the pioneering era of artificial intelligence and computational linguistics, with the inception of Ellis Page’s Project Essay Grade system at the University of Connecticut in 1964, and the subsequent publication of his seminal article “The Imminence of…Grading Essays by Computer” in 1966. Page’s automated scoring system is seen by the authors of this book as one of the first concrete applications of natural language processing, after the Audrey system for speech recognition and the Georgetown–IBM demonstration of machine translation. But just like speech recognition and machine translation, the industrialization of automated essay scoring mostly gained momentum in the 1990s. This decade saw an increase in richly annotated language data and corpora, which enabled the use of statistical and supervised machine learning in developing essay-scoring systems. Pearson’s Intelligent Essay Assessor™, released in 1998, is a prominent example of this evolution. A year later, in 1999, the e-rater® scoring engine was launched by the Educational Testing Service, commonly abbreviated as ETS, a private organization overseeing several standardized tests and high-stakes examinations such as TOEFL® and GRE®.This book is written by Beata Beigman Klebanov and Nitin Madnani, two researchers at ETS with many years of research experience and numerous peer-reviewed publications in the field of automated essay scoring. Their book provides a concise yet indispensable introduction to the field. After this short introduction, the eager reader is invited to take a deep dive into the scientific literature, encompassing slightly over half of the book’s content, approximately 115 pages. The literature review primarily caters to computational linguists and NLP practitioners, as it delves comprehensively into diverse machine learning models—ranging from linear regression to artificial neural networks—and intricate linguistic features. These features include general indicators of writing quality, such as text organization, coherence, and grammaticality, as well as genre-specific features, such as argument structure in argumentative essays. Furthermore, the reader not only gains insight into the extensive research carried out at ETS throughout the years but also delves into their technical expertise through a series of guided experiments with RSMTool—an open-source tool developed by the second author and colleagues. In addition, the authors also provide insight into a scalable and production-ready computer architecture used to build ETS products such as c-rater™, e-rater®, Language Muse®, and Writing Mentor™.The book is divided into five parts counting 13 chapters in total. The first part contains an introductory chapter in which the authors introduce the reader to Page’s aforementioned seminal 1966 article. Chapter 1 enumerates several arguments made by Page in favor of automated essay scoring, including its educational need, computational feasibility, high quality, and low cost. The chapter then sets forth four challenges associated with automated essay scoring. Two among them are the evaluation of original and source-based writing. Original writing, which reflects an author’s unique voice and thus stands out from existing and conventional works, poses a challenge because its originality could be overlooked or even under-evaluated by a computer. Source-based writing, a form of writing that reviews main points from external sources, presents a different problem because the assessment focuses on the correctness of content rather than measuring writing skills and essay quality. Another challenge is avoiding potential gaming strategies that test takers may employ to inflate their scores. A final challenge is automated feedback, as the effectiveness of providing linguistic and stylistic commentary on written work is not always proven.The second part contains two chapters covering a series of guided experiments and a set of best practices when building an automated essay scoring system. Chapter 2 provides a step-by-step guide for building an automated scoring system with supervised machine learning. First, the reader learns the usual engineering setup, including the use of a standard dataset (viz., the Automated Student Assessment Prize competition), an interpretable machine learning model (viz., linear regression), and a set of basic features derived from a scoring rubric. The authors also introduce the reader to their RSMTool. Then, the reader is guided through a series of ten experiments illustrating the incremental development (experiments 1–5) and evaluation (experiments 6–10) of the machine learning model. In each experiment, the reader is taught an important lesson such as feature fairness. For each experiment, the authors refer the reader to a report (i.e., correction key) available online.Chapter 3 provides some best practices for building an automated essay scoring system. First, the authors identify potentially conflicting perspectives between NLP developers and other stakeholders, including end users, subject matter experts, and business units. The authors then describe three natural use cases for automatic essay scoring, where it is added to a pre-existing assessment, developed concurrently with a new assessment, or implemented in a classroom. The authors provide some practical considerations and concrete actions that are well thought out and will appeal to many different readers.The third part contains five chapters that provide an up-to-date overview of the scientific literature and a more detailed account of the concepts introduced in the previous chapters. Chapter 4 describes various statistical models, including linear regression, latent semantic analysis, support vector machines, random forests, ensemble methods, and neural networks. For each of these models, the authors describe their mathematical underpinnings and explain how they can be used to score an essay. The chapter pays special attention to recent deep-learning architectures for automated essay scoring.Chapters 5 and 6 describe computational features that capture various aspects of the writing construct and the scoring rubric. Chapter 5 deals with general features. The features are organized into three classes: discourse features aiming to capture essay organization, development, and coherence; content features related to vocabulary use and topicality; and conventional features based on grammatical error detection. Chapter 6 then gives an in-depth overview of computational features that pertain to specific writing genres. The chapter is focused on four genres: argumentative writing, narrative writing, source-based writing, and reflective writing. Argumentative writing involves defending a particular position on a given topic (e.g., why technology should be integrated into education), presenting several claims as to why this position is valid, and supporting these claims with premises and evidence. Narrative writing involves telling a story that describes, for example, the historical evolution of technology in education. Source-based writing involves summarizing and comparing key points from external sources to compose an informed essay, which, for example, reviews the effectiveness of integrating technology into education based on scholarly sources. Finally, reflective writing involves examining personal experiences, such as teachers describing their experiences integrating technology into the classroom and reflecting on important lessons learned from this experience. This detailed overview of different writing genres also interestingly introduces the reader to research from related fields, such as computational argumentation and text summarization.Chapters 7 and 8 address two concerns in setting up real-world applications of automated essay scoring. Chapter 7 deals with the issues of reliability, scalability, and flexibility when deploying a scoring system at a large scale. The chapter describes and illustrates an Apache Storm architecture implemented in several ETS systems. Chapter 8 is about evaluating construct validity and fairness. When deploying an essay scoring system for high-stakes testing, fundamental issues arise when the system fails to measure the construct, assigns scores influenced by factors irrelevant to the measured construct, or is biased towards specific personal characteristics in the population of test takers. If an essay scoring system overlooks important features or favors particular writing styles or cultural representations, it undermines the validity and fairness of high-stakes assessments.The fourth part contains four chapters examining some broader challenges introduced in the first chapter and which remain to be solved for automated scoring. Chapter 9 deals with the automated generation of useful feedback on writing. The authors review several existing feedback systems and discuss how to define and evaluate the usefulness of feedback. Chapter 10 focuses on evaluating essay content, which is separate from assessing essay quality. Content scoring emphasizes measuring test-takers’ content knowledge, prioritizing these elements over writing skills. This evaluation can adopt either a reference-based or a response-based approach. Reference-based scoring involves comparing responses to a set of predefined reference answers, while response-based scoring independently assesses the response content. The chapter primarily explores the latter, investigating computational features and models tailored for this approach. Chapter 11 deals with another task related to but different from essay scoring, namely the automated scoring of spontaneous speech. After a brief account of the challenges with automated speech recognition, the authors review three sets of features for speech scoring: the delivery and fluency of spontaneous speech, vocabulary and grammar use, and topic development. The authors also contrast features relevant for scoring speech with those relevant for scoring writing. Lastly, chapter 12 examines several gaming strategies test-takers could use to fool the automated scoring system into giving a higher score. The authors review four types of strategies: the unnecessary use of shell language, the artificial generation of essays, the submission of off-topic responses, and the use of canned responses or plagiarized essays.The fifth and final part of the book contains a concluding chapter. The authors revisit the desiderata put forth by Ellis Page in his 1966 publication and summarize the overall achievements and remaining challenges in this respect. In addition, the authors discuss other challenging aspects that Page did not envision, such as the present-day ubiquity of technology, dealing with multiple languages, and setting up high-stakes tests that are valid, defensible, and fair.In sum, the book offers an excellent introduction to and deepening of the field of automated essay scoring. The book is well-structured and easy to read. Throughout the book, the authors provide thoughtful insights and practical advice based on their many years of experience at ETS. Compared to other books on the subject, the book offers a valuable combination of practical lessons and scientific deepening. By the end of the book, the reader has acquired a broad knowledge of the possibilities, challenges, and practical concerns involved with the automated scoring of student writing.},
  archive      = {J_COLI},
  author       = {Tack, Anaïs},
  doi          = {10.1162/coli_r_00513},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1005-1008},
  shortjournal = {Comput. Lingu.},
  title        = {Automated essay scoring},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey of cultural awareness in language models: Text and beyond. <em>COLI</em>, <em>51</em>(3), 907-1004. (<a href='https://doi.org/10.1162/COLI.a.14'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive, going beyond multilinguality and building on findings from psychology and anthropology. In this article, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking definitions of culture from the anthropology and psychology literature as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of human–computer interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature. 1},
  archive      = {J_COLI},
  author       = {Pawar, Siddhesh and Park, Junyeong and Jin, Jiho and Arora, Arnav and Myung, Junho and Yadav, Srishti and Haznitrama, Faiz Ghifari and Song, Inhwa and Oh, Alice and Augenstein, Isabelle},
  doi          = {10.1162/COLI.a.14},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {907-1004},
  shortjournal = {Comput. Lingu.},
  title        = {Survey of cultural awareness in language models: Text and beyond},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models are biased because they are large language models. <em>COLI</em>, <em>51</em>(3), 885-906. (<a href='https://doi.org/10.1162/coli_a_00558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This position paper’s primary goal is to provoke thoughtful discussion about the relationship between bias and fundamental properties of large language models (LLMs). I do this by seeking to convince the reader that harmful biases are an inevitable consequence arising from the design of any large language model as LLMs are currently formulated. To the extent that this is true, it suggests that the problem of harmful bias cannot be properly addressed without a serious reconsideration of AI driven by LLMs, going back to the foundational assumptions underlying their design.},
  archive      = {J_COLI},
  author       = {Resnik, Philip},
  doi          = {10.1162/coli_a_00558},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {885-906},
  shortjournal = {Comput. Lingu.},
  title        = {Large language models are biased because they are large language models},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics. <em>COLI</em>, <em>51</em>(3), 843-883. (<a href='https://doi.org/10.1162/coli_a_00543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate two essential challenges in the context of hierarchical topic modeling (HTM)—(i) the impact of data representation and (ii) topic evaluation. The data representation directly influences the performance of the topic generation, and the impact of new representations such as contextual embeddings in this task has been under-investigated. Topic evaluation , responsible for driving the advances in the field, assesses the overall quality of the topic generation process. HTM studies exploit the exact topic modeling (TM) evaluation metrics as traditional TM to measure the quality of topics. One significant result of our work is demonstrating that the HTM’s hierarchical nature demands novel ways of evaluating the quality of topics. As our main contribution, we propose two new topic quality metrics to assess the topical quality of the hierarchical structures. Uniqueness considers topic topological consistency, while the Semantic Hierarchical Structure (SHS) captures the semantic relatedness of the hierarchies. We also present an additional advance to the state-of-the-art by proposing the c-CluHTM. To the best of our knowledge, c-CluHTM is the first method that exploits contextual embeddings into NMF in HTM tasks. c-CluHTM enhances the topics’ semantics while preserving the hierarchical structure. We perform an experimental evaluation, and our results demonstrate the superiority of our proposal with gains between 12% and 21%, regarding NPMI and Coherence over the best baselines. Regarding the newly proposed metrics, our results reveal that Uniqueness and SHS can capture relevant information about the structure of the hierarchical topics that traditional metrics cannot.},
  archive      = {J_COLI},
  author       = {Viegas, Felipe and Pereira, Antonio and Cunha, Washington and França, Celso and Andrade, Claudio and Tuler, Elisa and Rocha, Leonardo and Gonçalves, Marcos André},
  doi          = {10.1162/coli_a_00543},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {843-883},
  shortjournal = {Comput. Lingu.},
  title        = {Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The emergence of chunking structures with hierarchical RNN. <em>COLI</em>, <em>51</em>(3), 815-841. (<a href='https://doi.org/10.1162/coli_a_00545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Natural Language Processing (NLP), predicting linguistic structures, such as parsing and chunking, has mostly relied on manual annotations of syntactic structures. This article introduces an unsupervised approach to chunking, a syntactic task that involves grouping words in a non-hierarchical manner. We present a Hierarchical Recurrent Neural Network (HRNN) designed to model word-to-chunk and chunk-to-sentence compositions. Our approach involves a two-stage training process: pretraining with an unsupervised parser and finetuning on downstream NLP tasks. Experiments on multiple datasets reveal a notable improvement of unsupervised chunking performance in both pretraining and finetuning stages. Interestingly, we observe that the emergence of the chunking structure is transient during the neural model’s downstream-task training. This study contributes to the advancement of unsupervised syntactic structure discovery and opens avenues for further research in linguistic theory. 1},
  archive      = {J_COLI},
  author       = {Wu, Zijun and Deshmukh, Anup Anand and Wu, Yongkang and Lin, Jimmy and Mou, Lili},
  doi          = {10.1162/coli_a_00545},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {815-841},
  shortjournal = {Comput. Lingu.},
  title        = {The emergence of chunking structures with hierarchical RNN},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tokenization changes meaning in large language models: Evidence from chinese. <em>COLI</em>, <em>51</em>(3), 785-814. (<a href='https://doi.org/10.1162/coli_a_00557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models segment many words into multiple tokens, and there is mixed evidence as to whether tokenization affects how state-of-the-art models represent meanings. Chinese characters present an opportunity to investigate this issue: They contain semantic radicals, which often convey useful information; characters with the same semantic radical tend to begin with the same one or two bytes (when using UTF-8 encodings); and tokens are common strings of bytes, so characters with the same radical often begin with the same token. This study asked GPT-4, GPT-4o, and Llama 3 whether characters contain the same semantic radical, elicited semantic similarity ratings, and conducted odd-one-out tasks (i.e., which character is not like the others). In all cases, misalignment between tokens and radicals systematically corrupted representations of Chinese characters. In experiments comparing characters represented by single tokens to multi-token characters, the models were less accurate for single-token characters, which suggests that segmenting words into fewer, longer tokens obscures valuable information in word form and will not resolve the problems introduced by tokenization. In experiments with 12 European languages, misalignment between tokens and suffixes systematically corrupted categorization of words by all three models, which suggests that the tendency to treat malformed tokens like linguistic units is pervasive.},
  archive      = {J_COLI},
  author       = {Haslett, David A.},
  doi          = {10.1162/coli_a_00557},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {785-814},
  shortjournal = {Comput. Lingu.},
  title        = {Tokenization changes meaning in large language models: Evidence from chinese},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniASA: A unified generative framework for argument structure analysis. <em>COLI</em>, <em>51</em>(3), 739-784. (<a href='https://doi.org/10.1162/coli_a_00553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation is a fundamental human activity that involves reasoning and persuasion, which also serves as the basis for the development of AI systems capable of complex reasoning. In NLP, to better understand human argumentation, argument structure analysis aims to identify argument components, such as claims and premises, and their relations from free text. It encompasses a variety of divergent tasks, such as end-to-end argument mining, argument pair extraction, and argument quadruplet extraction. Existing methods are usually tailored to only one specific argument structure analysis task, overlooking the inherent connections among different tasks. We observe that the fundamental goal of these tasks is similar: identifying argument components and their interrelations. Motivated by this, we present a unified generative framework for argument structure analysis (UniASA). It can uniformly address multiple argument structure analysis tasks in a sequence-to-sequence manner. Further, we enhance UniASA with a multi-view learning strategy based on subtask decomposition. We conduct experiments on seven datasets across three tasks. The results indicate that UniASA can address these tasks uniformly and achieve performance that is either superior to or comparable with the previous state-of-the-art methods. Also, we show that UniASA can be effectively integrated with large language models, such as Llama, through fine-tuning or in-context learning.},
  archive      = {J_COLI},
  author       = {Bao, Jianzhu and Jing, Mohan and Dong, Kuicai and Sun, Aixin and Sun, Yang and Xu, Ruifeng},
  doi          = {10.1162/coli_a_00553},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {739-784},
  shortjournal = {Comput. Lingu.},
  title        = {UniASA: A unified generative framework for argument structure analysis},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graded suspiciousness of adversarial texts to humans. <em>COLI</em>, <em>51</em>(3), 705-738. (<a href='https://doi.org/10.1162/coli_a_00555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples pose a significant challenge to deep neural networks across both image and text domains, with the intent to degrade model performance through carefully altered inputs. Adversarial texts, however, are distinct from adversarial images due to their requirement for semantic similarity and the discrete nature of the textual contents. This study delves into the concept of human suspiciousness, a quality distinct from the traditional focus on imperceptibility found in image-based adversarial examples, where adversarial changes are often desired to be indistinguishable to the human eye even when placed side by side with originals. Although this is generally not possible with text, textual adversarial content must still often remain undetected or non-suspicious to human readers. Even when the text’s purpose is to deceive NLP systems or bypass filters, the text is often expected to be natural to read. In this research, we expand the study of human suspiciousness by analyzing how individuals perceive adversarial texts. We gather and publish a novel dataset of Likert-scale human evaluations on the suspiciousness of adversarial sentences, crafted by four widely used adversarial attack methods and assess their correlation with the human ability to detect machine-generated alterations. Additionally, we develop a regression-based model to predict levels of suspiciousness and establish a baseline for future research in reducing the suspiciousness in adversarial text generation. We also demonstrate how the regressor-generated suspicious scores can be incorporated into adversarial generation methods to produce texts that are less likely to be perceived as computer-generated.},
  archive      = {J_COLI},
  author       = {Tonni, Shakila Mahjabin and Faustini, Pedro and Dras, Mark},
  doi          = {10.1162/coli_a_00555},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {705-738},
  shortjournal = {Comput. Lingu.},
  title        = {Graded suspiciousness of adversarial texts to humans},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ecj">ECJ - 5</h2>
<ul>
<li><details>
<summary>
(2025). On the use of the doubly stochastic matrix models for the quadratic assignment problem. <em>ECJ</em>, <em>33</em>(3), 425-457. (<a href='https://doi.org/10.1162/evco_a_00369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Permutation problems have captured the attention of the combinatorial optimization community for decades due to the challenge they pose. Although their solutions are naturally encoded as permutations, in each problem, the information to be used to optimize them can vary substantially. In this paper, we consider the Quadratic Assignment Problem (QAP) as a case study, and propose using Doubly Stochastic Matrices (DSMs) under the framework of Estimation of Distribution Algorithms. To that end, we design efficient learning and sampling schemes that enable an effective iterative update of the probability model. Conducted experiments on commonly adopted benchmarks for the QAP prove doubly stochastic matrices to be preferred to the other four models for permutations, both in terms of effectiveness and computational efficiency. Moreover, additional analyses performed on the structure of the QAP and the Linear Ordering Problem (LOP) show that DSMs are good to deal with assignment problems, but they have interesting capabilities to deal also with ordering problems such as the LOP. The paper concludes with a description of the potential uses of DSMs for other optimization paradigms, such as genetic algorithms or model-based gradient search.},
  archive      = {J_ECJ},
  author       = {Santucci, Valentino and Ceberio, Josu},
  doi          = {10.1162/evco_a_00369},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {425-457},
  shortjournal = {Evol. Comput.},
  title        = {On the use of the doubly stochastic matrix models for the quadratic assignment problem},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P-NP instance decomposition based on the fourier transform for solving the linear ordering problem. <em>ECJ</em>, <em>33</em>(3), 395-423. (<a href='https://doi.org/10.1162/evco_a_00368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fourier transform over finite groups has proved to be a useful tool for analyzing combinatorial optimization problems. However, few heuristic and metaheuristic algorithms have been proposed in the literature that utilize the information provided by this technique to guide the search process. In this work, we attempt to address this research gap by considering the case study of the Linear Ordering Problem (LOP). Based on the Fourier transform, we propose an instance decomposition strategy that divides any LOP instance into the sum of two LOP instances associated with a P and an NP-Hard optimization problem. By linearly aggregating the instances obtained from the decomposition, it is possible to create artificial instances with modified proportions of the P and NP-Hard components. Conducted experiments show that increasing the weight of the P component leads to a less rugged fitness landscape suitable for local search-based optimization. We take advantage of this phenomenon by presenting a new metaheuristic algorithm called P-Descent Search (PDS). The proposed method, first, optimizes a surrogate instance with a high proportion of the P component, and then, gradually increases the weight of the NP-Hard component until the original instance is reached. The multi-start version of PDS shows a promising and predictable performance that appears to be correlated to specific characteristics of the problem, which could open the door to an automatic tuning of its hyperparameters.},
  archive      = {J_ECJ},
  author       = {Benavides, Xabier and Hernando, Leticia and Ceberio, Josu and Lozano, Jose A.},
  doi          = {10.1162/evco_a_00368},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {395-423},
  shortjournal = {Evol. Comput.},
  title        = {P-NP instance decomposition based on the fourier transform for solving the linear ordering problem},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing monotone chance-constrained submodular functions using evolutionary multiobjective algorithms. <em>ECJ</em>, <em>33</em>(3), 363-393. (<a href='https://doi.org/10.1162/evco_a_00360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world optimization problems can be stated in terms of submodular functions. Furthermore, these real-world problems often involve uncertainties which may lead to the violation of given constraints. A lot of evolutionary multiobjective algorithms following the Pareto optimization approach have recently been analyzed and applied to submodular problems with different types of constraints. We present a first runtime analysis of evolutionary multiobjective algorithms based on Pareto optimization for chance-constrained submodular functions. Here the constraint involves stochastic components and the constraint can only be violated with a small probability of α ⁠ . We investigate the classical GSEMO algorithm for two different bi-objective formulations using tail bounds to determine the feasibility of solutions. We show that the algorithm GSEMO obtains the same worst case performance guarantees for monotone submodular functions as recently analyzed greedy algorithms for the case of uniform IID weights and uniformly distributed weights with the same dispersion when using the appropriate bi-objective formulation. As part of our investigations, we also point out situations where the use of tail bounds in the first bi-objective formulation can prevent GSEMO from obtaining good solutions in the case of uniformly distributed weights with the same dispersion if the objective function is submodular but non-monotone due to a single element impacting monotonicity. Furthermore, we investigate the behavior of the evolutionary multiobjective algorithms GSEMO, NSGA-II, and SPEA2 on different submodular chance-constrained network problems. Our experimental results show that the use of evolutionary multiobjective algorithms leads to significant performance improvements compared to state-of-the-art greedy algorithms for submodular optimization.},
  archive      = {J_ECJ},
  author       = {Neumann, Aneta and Neumann, Frank},
  doi          = {10.1162/evco_a_00360},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {363-393},
  shortjournal = {Evol. Comput.},
  title        = {Optimizing monotone chance-constrained submodular functions using evolutionary multiobjective algorithms},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Genetic programming for automatically evolving multiple features to classification. <em>ECJ</em>, <em>33</em>(3), 335-362. (<a href='https://doi.org/10.1162/evco_a_00359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing classification on high-dimensional data poses a significant challenge due to the huge search space. Moreover, complex feature interactions introduce an additional obstacle. The problems can be addressed by using feature selection to select relevant features or feature construction to construct a small set of high-level features. However, performing feature selection or feature construction might only make the feature set suboptimal. To remedy this problem, this study investigates the use of genetic programming for simultaneous feature selection and feature construction in addressing different classification tasks. The proposed approach is tested on 16 datasets and compared with seven methods including both feature selection and feature construction techniques. The results show that the obtained feature sets with the constructed and/or selected features can significantly increase the classification accuracy and reduce the dimensionality of the datasets. Further analysis reveals the complementarity of the obtained features leading to the promising classification performance of the proposed method.},
  archive      = {J_ECJ},
  author       = {Wang, Peng and Xue, Bing and Liang, Jing and Zhang, Mengjie},
  doi          = {10.1162/evco_a_00359},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {335-362},
  shortjournal = {Evol. Comput.},
  title        = {Genetic programming for automatically evolving multiple features to classification},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogates of scalarization functions. <em>ECJ</em>, <em>33</em>(3), 309-334. (<a href='https://doi.org/10.1162/evco_a_00354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, computationally intensive multiobjective optimization problems have been efficiently solved by surrogate-assisted multiobjective evolutionary algorithms. However, most of those algorithms can handle no more than 200 decision variables. As the number of decision variables increases further, unreliable surrogate models will result in a dramatic deterioration of their performance, which makes large-scale expensive multiobjective optimization challenging. To address this challenge, we develop a large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogate models of scalarization functions. The proposed algorithm (termed LDS-AF) reduces the dimension of the original decision space based on principal component analysis, and then directly approximates the scalarization functions in a decomposition-based multiobjective evolutionary algorithm. With the help of a two-stage modeling strategy and convergence control strategy, LDS-AF can keep a good balance between convergence and diversity, and achieve a promising performance without being trapped in a local optimum prematurely. The experimental results on a set of test instances have demonstrated its superiority over eight state-of-the-art algorithms on multiobjective optimization problems with up to 1,000 decision variables using only 500 real function evaluations.},
  archive      = {J_ECJ},
  author       = {Gu, Haoran and Wang, Handing and He, Cheng and Yuan, Bo and Jin, Yaochu},
  doi          = {10.1162/evco_a_00354},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {309-334},
  shortjournal = {Evol. Comput.},
  title        = {Large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogates of scalarization functions},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jmlr">JMLR - 1</h2>
<ul>
<li><details>
<summary>
(2025). Linear separation capacity of self-supervised representation learning. <em>JMLR</em>, <em>26</em>(194), 1-48. (<a href='https://jmlr.org/papers/v26/24-2032.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. Our theoretical analysis further underscores that the performance of downstream linear classifiers primarily hinges on the linear separability of data representations rather than the size of the labeled data set, reaffirming the viability of constructing efficient classifiers with limited labeled data amid an expansive unlabeled data set.},
  archive      = {J_JMLR},
  author       = {Shulei Wang},
  journal      = {Journal of Machine Learning Research},
  number       = {194},
  pages        = {1-48},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Linear separation capacity of self-supervised representation learning},
  url          = {https://jmlr.org/papers/v26/24-2032.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="neco">NECO - 6</h2>
<ul>
<li><details>
<summary>
(2025). Fast multigroup gaussian process factor models. <em>NECO</em>, <em>37</em>(9), 1709-1782. (<a href='https://doi.org/10.1162/neco.a.22'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes are now commonly used in dimensionality reduction approaches tailored to neuroscience, especially to describe changes in high-dimensional neural activity over time. As recording capabilities expand to include neuronal populations across multiple brain areas, cortical layers, and cell types, interest in extending gaussian process factor models to characterize multipopulation interactions has grown. However, the cubic runtime scaling of current methods with the length of experimental trials and the number of recorded populations (groups) precludes their application to large-scale multipopulation recordings. Here, we improve this scaling from cubic to linear in both trial length and group number. We present two approximate approaches to fitting multigroup gaussian process factor models based on inducing variables and the frequency domain. Empirically, both methods achieved orders of magnitude speed-up with minimal impact on statistical performance, in simulation and on neural recordings of hundreds of neurons across three brain areas. The frequency domain approach, in particular, consistently provided the greatest runtime benefits with the fewest trade-offs in statistical performance. We further characterize the estimation biases introduced by the frequency domain approach and demonstrate effective strategies to mitigate them. This work enables a powerful class of analysis techniques to keep pace with the growing scale of multipopulation recordings, opening new avenues for exploring brain function.},
  archive      = {J_NECO},
  author       = {Gokcen, Evren and Jasper, Anna I. and Kohn, Adam and Machens, Christian K. and Yu, Byron M.},
  doi          = {10.1162/neco.a.22},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1709-1782},
  shortjournal = {Neural Comput.},
  title        = {Fast multigroup gaussian process factor models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From function to implementation: Exploring degeneracy in evolved artificial agents. <em>NECO</em>, <em>37</em>(9), 1677-1708. (<a href='https://doi.org/10.1162/neco.a.19'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Degeneracy—the ability of different structures to perform the same function—is a fundamental feature of biological systems, contributing to their robustness and evolvability. However, the ubiquity of degeneracy in systems generated through adaptive processes complicates our understanding of the behavioral and computational strategies they employ. In this study, we investigated degeneracy in simple computational agents, known as Markov brains, trained using an artificial evolution algorithm to solve a spatial navigation task with or without associative memory. We analyzed degeneracy at three levels: behavioral, structural, and computational, with a focus on the last. Using information-theoretical concepts, Tononi et al. (1999) proposed a functional measure of degeneracy within biological networks. Here, we extended this approach to compare degeneracy across multiple networks. Using information-theoretical tools and causal analysis, we explored the computational strategies of the evolved agents and quantified their computational degeneracy. Our findings reveal a hierarchy of degenerate solutions, from varied behaviors to diverse structures and computations. Even agents with identical evolved behaviors demonstrated different underlying structures and computations. These results underscore the pervasive nature of degeneracy in neural networks, blurring the lines between the algorithmic and implementation levels in adaptive systems, and highlight the importance of advanced analytical tools to understand their complex behaviors.},
  archive      = {J_NECO},
  author       = {Hu, Zhimin and Cingiler, Oğulcan and Bohm, Clifford and Albantakis, Larissa},
  doi          = {10.1162/neco.a.19},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1677-1708},
  shortjournal = {Neural Comput.},
  title        = {From function to implementation: Exploring degeneracy in evolved artificial agents},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward generalized entropic sparsification for convolutional neural networks. <em>NECO</em>, <em>37</em>(9), 1648-1676. (<a href='https://doi.org/10.1162/neco.a.21'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are reported to be overparametrized. The search for optimal (minimal) and sufficient architecture is an NP-hard problem: if the network has N neurons, then there are 2 N possibilities to connect them—and therefore 2 N possible architectures and 2 N Boolean hyperparameters to encode them. Selecting the best possible hyperparameter out of them becomes an N p -hard problem since 2 N grows in N faster then any polynomial N p ⁠ . Here, we introduce a layer-by-layer data-driven pruning method based on the mathematical idea aiming at a computationally scalable entropic relaxation of the pruning problem. The sparse subnetwork is found from the pretrained (full) CNN using the network entropy minimization as a sparsity constraint. This allows deploying a numerically scalable algorithm with a sublinear scaling cost. The method is validated on several benchmarks (architectures): on MNIST (LeNet), resulting in sparsity of 55% to 84% and loss in accuracy of just 0.1% to 0.5%, and on CIFAR-10 (VGG-16, ResNet18), resulting in sparsity of 73% to 89% and loss in accuracy of 0.1% to 0.5%.},
  archive      = {J_NECO},
  author       = {Barisin, Tin and Horenko, Illia},
  doi          = {10.1162/neco.a.21},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1648-1676},
  shortjournal = {Neural Comput.},
  title        = {Toward generalized entropic sparsification for convolutional neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring stimulus information transfer between neural populations through the communication subspace. <em>NECO</em>, <em>37</em>(9), 1600-1647. (<a href='https://doi.org/10.1162/neco.a.17'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensory processing arises from the communication between neural populations across multiple brain areas. While the widespread presence of neural response variability shared throughout a neural population limits the amount of stimulus-related information those populations can accurately represent, how this variability affects the interareal communication of sensory information is unknown. We propose a mathematical framework to understand the impact of neural population response variability on sensory information transmission. We combine linear Fisher information, a metric connecting stimulus representation and variability, with the framework of communication subspaces, which suggests that functional mappings between cortical populations are low-dimensional relative to the space of population activity patterns. From this, we partition Fisher information depending on the alignment between the population covariance and the mean tuning direction projected onto the communication subspace or its orthogonal complement. We provide mathematical and numerical analyses of our proposed decomposition of Fisher information and examine theoretical scenarios that demonstrate how to leverage communication subspaces for flexible routing and gating of stimulus information. This work will provide researchers investigating interareal communication with a theoretical lens through which to understand sensory information transmission and guide experimental design.},
  archive      = {J_NECO},
  author       = {Weiss, Oren and Coen-Cagli, Ruben},
  doi          = {10.1162/neco.a.17},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1600-1647},
  shortjournal = {Neural Comput.},
  title        = {Measuring stimulus information transfer between neural populations through the communication subspace},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the architectural biases of the cortical microcircuit. <em>NECO</em>, <em>37</em>(9), 1551-1599. (<a href='https://doi.org/10.1162/neco.a.23'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cortex plays a crucial role in various perceptual and cognitive functions, driven by its basic unit, the canonical cortical microcircuit. Yet, we remain short of a framework that definitively explains the structure-function relationships of this fundamental neuroanatomical motif. To better understand how physical substrates of cortical circuitry facilitate their neuronal dynamics, we employ a computational approach using recurrent neural networks and representational analyses. We examine the differences manifested by the inclusion and exclusion of biologically motivated interareal laminar connections on the computational roles of different neuronal populations in the microcircuit of hierarchically related areas throughout learning. Our findings show that the presence of feedback connections correlates with the functional modularization of cortical populations in different layers and provides the microcircuit with a natural inductive bias to differentiate expected and unexpected inputs at initialization, which we justify mathematically. Furthermore, when testing the effects of training the microcircuit and its variants with a predictive-coding-inspired strategy, we find that doing so helps better encode noisy stimuli in areas of the cortex that receive feedback, all of which combine to suggest evidence for a predictive-coding mechanism serving as an intrinsic operative logic in the cortex.},
  archive      = {J_NECO},
  author       = {Balwani, Aishwarya and Cho, Suhee and Choi, Hannah},
  doi          = {10.1162/neco.a.23},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1551-1599},
  shortjournal = {Neural Comput.},
  title        = {Exploring the architectural biases of the cortical microcircuit},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic pathways of modulation enable robust task packing within neural dynamics. <em>NECO</em>, <em>37</em>(9), 1529-1550. (<a href='https://doi.org/10.1162/neco.a.18'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how brain networks learn and manage multiple tasks simultaneously is of interest in both neuroscience and artificial intelligence. In this regard, a recent research thread in theoretical neuroscience has focused on how recurrent neural network models and their internal dynamics enact multitask learning. To manage different tasks requires a mechanism to convey information about task identity or context into the model, which from a biological perspective may involve mechanisms of neuromodulation. In this study, we use recurrent network models to probe the distinctions between two forms of contextual modulation of neural dynamics, at the level of neuronal excitability and at the level of synaptic strength. We characterize these mechanisms in terms of their functional outcomes, focusing on their robustness to context ambiguity and, relatedly, their efficiency with respect to packing multiple tasks into finite-size networks. We also demonstrate the distinction between these mechanisms at the level of the neuronal dynamics they induce. Together, these characterizations indicate complementarity and synergy in how these mechanisms act, potentially over many timescales, toward enhancing the robustness of multitask learning.},
  archive      = {J_NECO},
  author       = {Vedovati, Giacomo and Ching, ShiNung},
  doi          = {10.1162/neco.a.18},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1529-1550},
  shortjournal = {Neural Comput.},
  title        = {Synergistic pathways of modulation enable robust task packing within neural dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="netn">NETN - 9</h2>
<ul>
<li><details>
<summary>
(2025). Alterations in topology, cost, and dynamics of gamma-band EEG functional networks in a preclinical model of traumatic brain injury. <em>NETN</em>, <em>9</em>(3), 1013-1038. (<a href='https://doi.org/10.1162/netn.a.21'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traumatic brain injury (TBI) is a major cause of disability leading to multiple sequelae in cognitive, sensory, and physical domains, including posttraumatic epilepsy. Despite extensive research, our understanding of its impact on macroscopic brain circuitry remains incomplete. We analyzed electrophysiological functional connectomes in the gamma band from an animal model of blast-induced TBI over multiple time points after injury. We revealed differences in small-world propensity and rich-club structure compared with age-matched controls, indicating functional reorganization following injury. We further investigated cost-efficiency trade-offs, propose a computationally efficient normalization procedure for quantifying the cost of spatially embedded networks that controls for connectivity strength differences, and observed dynamic changes across the injury timeline. To explore potential links between altered network topology and epileptic activity, we employed a brain-wide computational model of seizure dynamics and attribute brain reorganization to a homeostatic mechanism of activity regulation with the potential unintended consequence of driving generalized seizures. Finally, we demonstrated post-injury hyperexcitability that manifests as an increase in sound-evoked response amplitudes at the cortical level. Our work characterizes, for the first time, gamma-band functional network reorganization in a model of brain injury and proposes potential causes of these changes, thus identifying targets for future therapeutic interventions. Traumatic brain injury (TBI) is a prevalent neurological disorder caused by factors such as accidents, contact sports, or military conflict. While animal studies have provided detailed insights into the molecular and cellular mechanisms underlying TBI, and clinical research has revealed its effects on large-scale brain network organization, these findings often lack integration. Our research seeks to address this gap by examining large-scale brain activity in an animal model of TBI. By analyzing brain network structures, we identify reorganization, highlighting novel connections to weight gain and posttraumatic epilepsy. Further exploration using this model may deepen our understanding of cross-scale mechanisms and inform the development of therapeutic interventions.},
  archive      = {J_NETN},
  author       = {Tsikonofilos, Konstantinos and Bruyns-Haylett, Michael and May, Hazel G. and Donat, Cornelius K. and Kozlov, Andriy S.},
  doi          = {10.1162/netn.a.21},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {1013-1038},
  shortjournal = {Netw. Neuroscience},
  title        = {Alterations in topology, cost, and dynamics of gamma-band EEG functional networks in a preclinical model of traumatic brain injury},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Idiosyncrasy and generalizability of contraceptive- and hormone-related functional connectomes across the menstrual cycle. <em>NETN</em>, <em>9</em>(3), 990-1012. (<a href='https://doi.org/10.1162/netn.a.20'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroendocrinology has received little attention in human neuroscience research, resulting in a dearth of knowledge surrounding potent and dynamic modulators of cognition and behavior, as well as brain structure and function. This work addresses one such phenomenon by studying functional connectomics related to ovarian hormone fluctuations throughout the adult menstrual cycle. To do so, we used fMRI and hormone assessments from two dense, longitudinal datasets to assess variations in functional connectivity with respect to endogenous and exogenous endocrine factors throughout the menstrual cycle. First, we replicated prior findings that common, group-level, and individual-specific factors have similar relative contributions to functional brain network organization. Second, we found widespread connectivity related to hormonal contraceptive (HC) use, in addition to sparser estradiol- and progesterone-related connectivity. Differential generalizability of these connectivity patterns suggests progestin-specific impacts on functional brain organization in HC users. These results provide novel insight into within-individual changes in brain organization across the menstrual cycle and the extent to which these changes are shared between individuals, illuminating understudied phenomena in reproductive health and important information for all neuroimaging studies that include participants who menstruate. Endocrine modulation of brain function across the menstrual cycle is poorly understood. Human neuroimaging research on the menstrual cycle has long relied on group differences and/or coarse, within-individual cycle stage differences, overlooking considerable individual differences in brain organization, the menstrual cycle, and hormone concentrations. Here, we take a multidataset approach to identify the idiosyncratic contraceptive- and hormone-related functional connectivity from within-individual neuroendocrine dynamics and then test the generalizability of this connectivity to other individuals. In doing so, we identified idiosyncratic hormone-responsive functional connectivity that is somewhat generalizable to other individuals, although this generalizability is complicated by hormonal contraceptive use, potentially reflecting differential connectivity between contraceptive formulations. Thus, this work illuminates individual similarities and differences in neuroendocrine dynamics across the menstrual cycle.},
  archive      = {J_NETN},
  author       = {Bottenhorn, Katherine L. and Salo, Taylor and Jacobs, Emily G. and Pritschet, Laura and Taylor, Caitlin and Herting, Megan M. and Laird, Angela R.},
  doi          = {10.1162/netn.a.20},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {990-1012},
  shortjournal = {Netw. Neuroscience},
  title        = {Idiosyncrasy and generalizability of contraceptive- and hormone-related functional connectomes across the menstrual cycle},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure–function coupling using fixel-based analysis and functional magnetic resonance imaging in alzheimer’s disease and mild cognitive impairment. <em>NETN</em>, <em>9</em>(3), 969-989. (<a href='https://doi.org/10.1162/netn_a_00461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional MRI (fMRI) and diffusion-weighted imaging (DWI) help explore correlations between structural connectivity (SC) and functional connectivity (FC; SC–FC coupling). Studies on mild cognitive impairment (MCI) and Alzheimer’s disease (AD) observed coupling disruptions, co-occurring with cognitive decline. Advanced “fixel-based” analyses improved DWI’s accuracy in assessing microstructural and macrostructural features of white matter (WM), but previous aging coupling studies commonly defined SC via tensor-based tractography and streamline counts, thereby missing fiber-specific information. We investigated different types of fixel-FC coupling and their relation to cognition in 392 participants (Age mean = 73; 207 females) from the ADNI. Two hundred twenty-five controls, 142 MCI, and 25 AD with diffusion-weighted and resting-state fMRI scans were analyzed. Structural connectomes were constructed using average fixel metrics (fiber density (FD), fiber-bundle cross-section log, and combined [FDC]) as edges. SC–FC coupling for each SC metric was calculated at overall network, edge, and node levels. Overall DMN, node- and edge-specific coupling differences were found across SC measures and groups. DMN nodal coupling significantly predicted Mini-Mental Status Examination score and verbal memory. In conclusion, different types of fixel-based coupling alterations can be observed across the neurocognitive aging spectrum, in particular, FD–FC and FDC–FC coupling between DMN regions are associated with cognitive functioning. Structure–function coupling is the correlation between white matter structures connecting pairs of brain regions (structural connectivity) and their concomitant brain activity (functional connectivity). “fixel-based” diffusion-weighted imaging techniques improve the accuracy of white matter structure estimation and provide information on microstructural and macrostructural changes to brain networks. Using structural connectomes based on fixel-based weights, coupling was found to be altered in Alzheimer’s disease and mild cognitive impairment, especially in the default mode network. Default mode network coupling may be relevant to verbal memory and cognitive decline across aging, particularly for coupling driven by microstructural features of white matter.},
  archive      = {J_NETN},
  author       = {Billaud, Charly Hugo Alexandre and Yu, Junhong and for the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1162/netn_a_00461},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {969-989},
  shortjournal = {Netw. Neuroscience},
  title        = {Structure–function coupling using fixel-based analysis and functional magnetic resonance imaging in alzheimer’s disease and mild cognitive impairment},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metastable dynamics emerge from local excitatory–inhibitory homeostasis in the cortex at rest. <em>NETN</em>, <em>9</em>(3), 938-968. (<a href='https://doi.org/10.1162/netn_a_00460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamics of the human cortex are highly metastable, driving the spontaneous exploration of network states. This metastability depends on circuit-level edge-of-bifurcation dynamics, which emerge from firing-rate control through multiple mechanisms of excitatory–inhibitory (E–I) homeostasis. However, it is unclear how these contribute to the metastability of cortical networks. We propose that individual mechanisms of the E–I homeostasis contribute uniquely to the emergence of resting-state dynamics and test this hypothesis in a large-scale model of the human cortex. We show that empirical connectivity and dynamics can only be reproduced when accounting for multiple mechanisms of the E–I homeostasis. More specifically, while the homeostasis of excitation and inhibition enhances metastability, the regulation of intrinsic excitability ensures moderate synchrony, maximizing functional complexity. Furthermore, the modulation bifurcation modulation by the homeostasis of excitation and intrinsic excitability compensates for strong input fluctuations in connector hubs. Importantly, this only occurs in models accounting for local gamma oscillations, suggesting a relationship between E–I balance, gamma rhythms, and metastable dynamics. Altogether, our results show that cortical networks self-organize toward maximal metastability through the multifactor homeostasis of E–I balance. Therefore, the benefits of combining multiple homeostatic mechanisms transcend the circuit level, supporting the metastable dynamics of large-scale cortical networks. Experimental studies have consistently shown that cortical circuits maintain a precise homeostasis of excitatory–inhibitory (E–I) balance, thereby optimizing local dynamics. While it is well established that multiple homeostatic mechanisms are involved in this local regulation, it remains unclear how each contributes to the large-scale dynamics of cortical networks. This study presents evidence that, through the E–I homeostasis, the cortex can self-organize toward a regime of highly complex and metastable spontaneous dynamics. Crucially, we demonstrate that this results from the synergistic action of multiple homeostatic mechanisms. Our findings advance our understanding of the E–I homeostasis as a process of self-organization, demonstrating its key role in the maintenance of metastable dynamics in large-scale cortical networks.},
  archive      = {J_NETN},
  author       = {Páscoa dos Santos, Francisco and Verschure, Paul F. M. J.},
  doi          = {10.1162/netn_a_00460},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {938-968},
  shortjournal = {Netw. Neuroscience},
  title        = {Metastable dynamics emerge from local excitatory–inhibitory homeostasis in the cortex at rest},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain connectome from neuronal morphology. <em>NETN</em>, <em>9</em>(3), 913-937. (<a href='https://doi.org/10.1162/netn_a_00458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-subject morphological brain networks derived from cross-feature correlation of macroscopic MRI-derived morphological measures provide an important means for studying the brain connectome. However, the validity of this approach remains to be confirmed at the microscopic level. Here, we constructed morphological brain networks at the single-cell level by extending features from macroscopic morphological measures to microscopic descriptions of neuronal morphology. We demonstrated the feasibility and generalizability of the method using neurons in the somatosensory cortex of a rat, neurons over the whole brain of a mouse, and neurons in the middle temporal gyrus (MTG) of a human. We found that interneuron morphological similarity was higher for intra- than interclass connections, depended on cytoarchitectonic, chemoarchitectonic, and laminar classification of neurons (rat), differed between regions with different evolutionary timelines (mouse), and correlated with neuronal axonal projections (mouse). Furthermore, highly connected hub neurons were disproportionately from superficial layers (rat), inhibitory neurons (rat), and subcortical regions (mouse), and exhibited unique morphology. Finally, we demonstrated a more segregated, less integrated, and economic network architecture with worse resistance to targeted attacks for neurons in human MTG than neurons in a mouse’s primary visual cortex. Overall, our method provides an alternative avenue to study neuronal wiring diagrams in brains. The brain is a highly complex network spanning multiple spatial scales, yet the organization of brain networks at the single-cell level remains poorly understood. Here, we constructed microscopic morphological brain networks by assessing interneuron similarity based on neuronal morphology for different species. We found that interneuron morphological similarity was correlated with neuronal axonal projections, dependent on neuronal affiliation with respect to cellular and molecular architecture, laminar positioning, and brain area location, and capable of uncovering cross-species differences. Our method complements existing methodology aimed at mapping wiring diagrams in brains at the microscopic level.},
  archive      = {J_NETN},
  author       = {Jin, Suhui and Li, Junle and Wang, Jinhui},
  doi          = {10.1162/netn_a_00458},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {913-937},
  shortjournal = {Netw. Neuroscience},
  title        = {Brain connectome from neuronal morphology},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly estimating individual and group networks from fMRI data. <em>NETN</em>, <em>9</em>(3), 896-912. (<a href='https://doi.org/10.1162/netn_a_00457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In fMRI research, graphical models are used to uncover complex patterns of relationships between brain regions. Connectivity-based fMRI studies typically analyze nested data; raw observations, for example, BOLD responses, are nested within participants, which are nested within populations, for example, healthy controls. Often, studies ignore the nested structure and analyze participants either individually or in aggregate. This overlooks the distinction between within-participant and between-participant variance, which can lead to poor generalizability of results because group-level effects do not necessarily reflect effects for each member of the group and, at worst, risk paradoxical results where group-level effects are opposite to individual-level effects (e.g., Kievit, Frankenhuis, Waldorp, & Borsboom, 2013 ; Robinson, 2009 ; Simpson, 1951 ). To address these concerns, we propose a multilevel approach to model the fMRI networks, using a Gaussian graphical model at the individual level and a Curie-Weiss graphical model at the group level. Simulations show that our method outperforms individual or aggregate analysis in edge retrieval. We apply the proposed multilevel approach to resting-state fMRI data of 724 healthy participants, examining both their commonalities and individual differences. We not only recover the seven previously found resting-state networks at the group level but also observe considerable heterogeneity in the individual-level networks. Finally, we discuss the necessity of a multilevel approach, additional challenges, and possible future extensions.},
  archive      = {J_NETN},
  author       = {van den Bergh, Don and Douw, Linda and van der Pal, Zarah and Blanken, Tessa F. and Schrantee, Anouk and Marsman, Maarten},
  doi          = {10.1162/netn_a_00457},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {896-912},
  shortjournal = {Netw. Neuroscience},
  title        = {Jointly estimating individual and group networks from fMRI data},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The exponential distance rule-based network model predicts topology and reveals functionally relevant properties of the drosophila projectome. <em>NETN</em>, <em>9</em>(3), 869-895. (<a href='https://doi.org/10.1162/netn_a_00455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying structural brain networks has witnessed significant advancement in recent decades. Findings revealed a geometric principle, the exponential distance rule (EDR) showing that the number of neurons decreases exponentially with the length of their axons. This neuron-level information was used to build a region-level EDR network model that was able to explain various characteristics of interareal cortical networks in macaques, mice, and rats. The complete connectome of the Drosophila has recently been mapped providing information also about the network of neuropils (projectome). A recent study demonstrated the presence of the EDR in the Drosophila . In our study, we first revisit the EDR itself and precisely measure the characteristic decay rate. Next, we demonstrate that the EDR model effectively accounts for numerous binary and weighted properties of the projectome. Our study illustrates that the EDR model is a suitable null model for analyzing networks of brain regions, as it captures properties of region-level networks in very different species. The importance of the null model lies in its ability to facilitate the identification of functionally significant features not caused by inevitable geometric constraints, as we illustrate with the pronounced asymmetry of connection weights important for functional hierarchy. Recent advancements in the structural brain network analysis have revealed the exponential distance rule (EDR), which shows that neuron numbers decrease exponentially with their axon length. This neuron-level insight led to a region-level EDR network model that explained properties of interareal cortical networks in macaques, mice, and rats. Recently, the complete connectome of the Drosophila has been mapped and the presence of the EDR has also been confirmed. Our research revisits the EDR, measures its decay rate, and demonstrates that the EDR model effectively accounts for various properties of the network of brain regions, serving as an appropriate null model for a structural brain network analysis. Its importance as a null model lies in its ability to facilitate the identification of functionally significant features not caused by inevitable geometric constraints.},
  archive      = {J_NETN},
  author       = {Péntek, Balázs and Ercsey-Ravasz, Mária},
  doi          = {10.1162/netn_a_00455},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {869-895},
  shortjournal = {Netw. Neuroscience},
  title        = {The exponential distance rule-based network model predicts topology and reveals functionally relevant properties of the drosophila projectome},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint estimation of source dynamics and interactions from MEG data. <em>NETN</em>, <em>9</em>(3), 842-868. (<a href='https://doi.org/10.1162/netn_a_00453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current techniques to estimate directed functional connectivity from magnetoencephalography (MEG) signals involve two sequential steps: (a) estimation of the sources and their amplitude time series from the MEG data and (b) estimation of directed interactions between the source time series. However, such a sequential approach is not optimal as it leads to spurious connectivity due to spatial leakage. Here, we present an algorithm to jointly estimate the source and connectivity parameters using Bayesian filtering. We refer to this new algorithm as JEDI-MEG (Joint Estimation of source Dynamics and Interactions from MEG data). By formulating a state-space model for the locations and amplitudes of a given number of sources, we show that estimation of their connections can be reduced to a system identification problem. Using simulated MEG data, we show that the joint approach provides a more accurate reconstruction of connectivity parameters than the conventional two-step approach. Using real MEG responses to visually presented faces in 16 subjects, we also demonstrate that our method gives source and connectivity estimates that are both physiologically plausible and largely consistent across subjects. In conclusion, the proposed joint estimation approach outperforms the traditional two-step approach in determining functional connectivity in MEG data. Functional connectivity is currently estimated from electromagnetic brain signals such as magnetoencephalography (MEG) in two consecutive steps: First, the inverse problem is solved to estimate the locations and temporal dynamics of brain sources. Second, connectivity metrics are computed between these estimated sources. This approach suffers from the limitation that the information provided by the connectivity structure is not exploited in the estimation of source activity and vice versa. Here, we present a novel algorithm, utilizing Bayesian filtering, to jointly estimate the source and connectivity parameters to overcome this limitation. Compared with state-of-the-art two-step approaches, our method provides a more accurate reconstruction of the connectivity parameters, which we demonstrate using a standard connectivity benchmark simulation and an electrocorticography-based simulation of MEG data. We also applied our method to real MEG responses (open-access dataset) to visually presented faces in 16 subjects, and the results show that our approach provides source and connectivity estimates that are both physiologically plausible and largely consistent across subjects. Overall, this work contributes to methodological advances in estimating functional connectivity from MEG data.},
  archive      = {J_NETN},
  author       = {Puthanmadam Subramaniyam, Narayan and Tronarp, Filip and Särkkä, Simo and Parkkonen, Lauri},
  doi          = {10.1162/netn_a_00453},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {842-868},
  shortjournal = {Netw. Neuroscience},
  title        = {Joint estimation of source dynamics and interactions from MEG data},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Longitudinal changes in MEG-based brain network topology of ALS patients with cognitive/behavioral impairment—An exploratory study. <em>NETN</em>, <em>9</em>(3), 824-841. (<a href='https://doi.org/10.1162/netn_a_00450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amyotrophic lateral sclerosis (ALS) with only motor impairment (ALS-pure motor) and the behavioral variant of frontotemporal dementia (bvFTD) are hypothesized to represent extreme ends of a disease spectrum, which encompasses ALS with cognitive/behavioral impairment (ALSci/bi). In this longitudinal magnetoencephalography (MEG) study, we investigated changes in brain network topology of ALSci/bi over time as compared with ALS-pure motor and bvFTD patients. Resting-state MEG was recorded in ALS-pure motor ( n = 9), ALSci/bi ( n = 16), and bvFTD ( n = 16) at baseline and 5-month follow-up, projected to source space. The corrected version of the amplitude envelope correlation was applied to compute frequency-band-specific functional connectivity between brain regions, from which the backbone of the functional networks was constructed using the minimum spanning tree (MST) approach. Reference MSTs were computed based on the functional connectivity matrices for ALS-pure motor and bvFTD, against which the networks of ALSci/bi were compared. We showed that, at baseline, networks in the theta band of ALSci/bi patients were more similar to ALS-pure motor than bvFTD. At follow-up, ALSci/bi patients’ beta-band network similarity had moved away from ALS-pure motor and resembled bvFTD. In conclusion, our findings suggest that brain networks of ALSci/bi patients move along the ALS-bvFTD spectrum over time, from ALS-pure motor to bvFTD-like topology. In this longitudinal magnetoencephalography (MEG) study, we explored changes in brain network topology in amyotrophic lateral sclerosis (ALS) patients with cognitive/behavioral impairment (ALSci/bi), compared with the two extreme ends of the ALS behavioral variant of frontotemporal dementia (ALS-bvFTD) spectrum (ALS with only motor impairment and bvFTD patients without ALS). We recorded resting-state MEG at baseline and 5-month follow-up, analyzing frequency-band-specific functional connectivity and constructing the networks’ backbone using the minimum spanning tree (MST). Our findings indicate that at baseline, ALSci/bi networks were more similar to ALS-pure motor than bvFTD in the theta band. At follow-up, ALSci/bi networks shifted toward bvFTD-like topology in the beta band. This suggests that ALSci/bi patients’ brain networks evolve along the ALS-bvFTD spectrum, highlighting potential implications for disease progression.},
  archive      = {J_NETN},
  author       = {Govaarts, Rosanne and Scheijbeler, Elliz P. and Beeldman, Emma and Fraschini, Matteo and Griffa, Alessandra and Engels, Marjolein M. A. and van der Kooi, Anneke J. and Pijnenburg, Yolande A. L. and de Visser, Marianne and Stam, Cornelis J. and Raaphorst, Joost and Hillebrand, Arjan},
  doi          = {10.1162/netn_a_00450},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {824-841},
  shortjournal = {Netw. Neuroscience},
  title        = {Longitudinal changes in MEG-based brain network topology of ALS patients with cognitive/behavioral impairment—An exploratory study},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="tacl">TACL - 5</h2>
<ul>
<li><details>
<summary>
(2025). Explanatory summarization with discourse-driven planning. <em>TACL</em>, <em>13</em>, 1146-1170. (<a href='https://doi.org/10.1162/TACL.a.30'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination. The project information is available at https://dongqi.me/projects/ExpSum .},
  archive      = {J_TACL},
  author       = {Liu, Dongqi and Yu, Xi and Demberg, Vera and Lapata, Mirella},
  doi          = {10.1162/TACL.a.30},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1146-1170},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Explanatory summarization with discourse-driven planning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARE: Diverse visual question answering with robustness evaluation. <em>TACL</em>, <em>13</em>, 1121-1145. (<a href='https://doi.org/10.1162/TACL.a.29'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Language Models (VLMs) extend remarkable capabilities of text-only large language models and vision-only models, being able to learn from and process multi-modal vision-text input. While modern VLMs perform well on a number of standard image classification and image-text matching tasks, they still struggle with a number of crucial vision-language (VL) reasoning abilities such as counting and spatial reasoning. Moreover, while they might be very brittle to small variations in instructions and/or evaluation protocols, existing benchmarks fail to evaluate their robustness (or rather the lack of it). In order to couple challenging VL scenarios with comprehensive robustness evaluation, we introduce DARE , D iverse Visual Question A nswering with R obustness E valuation, a carefully created and curated multiple-choice VQA benchmark. DARE evaluates VLM performance on five diverse categories and includes four robustness-oriented evaluations based on the variations of prompts, the subsets of answer options, the output format, and the number of correct answers. Among a spectrum of other findings, we report that state-of-the-art VLMs still struggle with questions in most categories and are unable to consistently deliver their peak performance across the tested robustness evaluations. Consequently, our work calls for the systematic addition of robustness evaluations in future VLM research.},
  archive      = {J_TACL},
  author       = {Sterz, Hannah and Pfeiffer, Jonas and Vulić, Ivan},
  doi          = {10.1162/TACL.a.29},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1121-1145},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {DARE: Diverse visual question answering with robustness evaluation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing and adapting large language models for few-shot multilingual NLU: Are we there yet?. <em>TACL</em>, <em>13</em>, 1096-1120. (<a href='https://doi.org/10.1162/TACL.a.33'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised fine-tuning (SFT), supervised instruction tuning (SIT), and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning. ICL has gained popularity recently with the advent of LLMs due to its versatile simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on a variety of high- and low-resource languages over five different NLU tasks, and a myriad of language and domain setups. Importantly, performance is only one aspect of the comparison, where we also analyze and discuss the approaches through the optics of their computational, inference and financial costs. Some of the highlighted findings concern an excellent trade-off between performance and resource requirements/cost for SIT. We further analyze the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve accordingly and remains limited, especially for low-resource languages.},
  archive      = {J_TACL},
  author       = {Razumovskaia, Evgeniia and Vulić, Ivan and Korhonen, Anna},
  doi          = {10.1162/TACL.a.33},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1096-1120},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Analyzing and adapting large language models for few-shot multilingual NLU: Are we there yet?},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BenCzechMark: A czech-centric multitask and multimetric benchmark for large language models with duel scoring mechanism. <em>TACL</em>, <em>13</em>, 1068-1095. (<a href='https://doi.org/10.1162/TACL.a.32'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present B en C zech M ark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its duel scoring system is grounded in statistical significance theory and uses aggregation across tasks inspired by social preference theory. Our benchmark encompasses 50 challenging tasks, with corresponding test datasets, primarily in native Czech, with 14 newly collected ones. These tasks span 8 categories and cover diverse domains, including historical Czech news, essays from pupils or language learners, and spoken word. Furthermore, we collect and clean BUT-Large Czech Collection, the largest publicly available clean Czech language corpus, and use it for (i) contamination analysis and (ii) continuous pretraining of the first Czech-centric 7B language model with Czech-specific tokenization. We use our model as a baseline for comparison with publicly available multilingual models. Lastly, we release and maintain a leaderboard with existing 50 model submissions, where new model submissions can be made at https://huggingface.co/spaces/CZLC/BenCzechMark .},
  archive      = {J_TACL},
  author       = {Fajcik, Martin and Docekal, Martin and Dolezal, Jan and Ondrej, Karel and Beneš, Karel and Kapsa, Jan and Smrz, Pavel and Polok, Alexander and Hradis, Michal and Neverilova, Zuzana and Horak, Ales and Sabol, Radoslav and Stefanik, Michal and Jirkovsky, Adam and Adamczyk, David and Hyner, Petr and Hula, Jan and Kydlicek, Hynek},
  doi          = {10.1162/TACL.a.32},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1068-1095},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {BenCzechMark: A czech-centric multitask and multimetric benchmark for large language models with duel scoring mechanism},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KEFT: Knowledge-enhanced fine-tuning for large language models in domain-specific question answering. <em>TACL</em>, <em>13</em>, 1056-1067. (<a href='https://doi.org/10.1162/TACL.a.31'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of large language models (LLMs) has opened up promising opportunities for their downstream applications in question-answering (QA), such as ChatGPT, ChatGLM, etc. However, such LLMs do not perform very well in domain-specific QA tasks without fine-tuning. But directly fine-tuning LLMs on domain-specific corpus data may lead to catastrophic forgetting, causing the LLMs to lose their general language capability. To address this problem, we propose the Knowledge-Enhanced Fine-Tuning (KEFT) method, an unsupervised fine-tuning approach to enhance the knowledge capability of LLMs in domain-specific QA tasks while preserving their general language capability. KEFT leverages the inherent language comprehension of pre-trained LLMs to generate synthetic-QA datasets from domain-specific corpus data autonomously for fine-tuning, and adopts a Low-Rank Adaptation (LoRA) method to further alleviate over-fitting. Furthermore, to enhance the representation of domain-specific knowledge, we introduce a knowledge-enhanced fine-tuning loss function, which encourages the model to learn the knowledge-question connection, thereby generating natural and knowledgeable answers. Our evaluations across multiple domain-specific datasets demonstrate that KEFT surpasses state-of-the-art fine-tuning approaches, enhancing the performance of various LLMs in QA tasks in both English and Chinese languages.},
  archive      = {J_TACL},
  author       = {Li, Haiyun and Zhang, Jixin and Shen, Hua and Cheng, Ke and Huang, Xiaofeng},
  doi          = {10.1162/TACL.a.31},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1056-1067},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {KEFT: Knowledge-enhanced fine-tuning for large language models in domain-specific question answering},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="tmlr">TMLR - 39</h2>
<ul>
<li><details>
<summary>
(2025). MoReact: Generating reactive motion from textual descriptions. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=4zuT73heqm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and generating human reactions poses a significant challenge with broad applications for computer vision and human-computer interaction. Existing methods either treat multiple individuals as a single entity, directly generating interactions, or rely solely on one person's motion to generate the other's reaction, failing to integrate the rich semantic information that underpins human interactions. Yet, these methods often fall short in adaptive responsiveness, \ie, the ability to accurately respond to diverse and dynamic interaction scenarios. Recognizing this gap, our work introduces an approach tailored to address the limitations of existing models by focusing on text-driven human reaction generation. Our model specifically generates realistic motion sequences for individuals that responding to the other's actions based on a descriptive text of the interaction scenario. The goal is to produce motion sequences that not only complement the opponent's movements but also semantically fit the described interactions. To achieve this, we present MoReact, a diffusion-based method designed to disentangle the generation of global trajectories and local motions sequentially. This approach stems from the observation that generating global trajectories first is crucial for guiding local motion, ensuring better alignment with given action and text. Furthermore, we introduce a novel interaction loss to enhance the realism of generated close interactions. Our experiments, utilizing data adapted from a two-person motion dataset, demonstrate the efficacy of our approach for this novel task, which is capable of producing realistic, diverse, and controllable reactions that not only closely match the movements of the counterpart but also adhere to the textual guidance. Please find our webpage at https://xiyan-xu.github.io/MoReactWebPage.},
  archive      = {J_TMLR},
  author       = {Xiyan Xu and Sirui Xu and Yu-Xiong Wang and Liangyan Gui},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {MoReact: Generating reactive motion from textual descriptions},
  url          = {https://openreview.net/forum?id=4zuT73heqm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning equivalence classes of bayesian network structures with GFlowNet. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=FAcc7oAdaa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the causal graph underlying a system is essential for enabling causal inference, particularly in fields such as medicine and genetics. Identifying a causal Directed Acyclic Graph (DAG) from observational data alone is challenging because multiple DAGs can encode the same set of conditional independencies. These equivalent DAGs form a Markov Equivalence Class (MEC), which is represented by a Completed Partially Directed Acyclic Graph (CPDAG). Effectively approximating the CPDAG is crucial because it facilitates narrowing down the set of possible causal graphs underlying the data. We introduce CPDAG-GFN, a novel approach that uses a Generative Flow Network (GFlowNet) to learn a posterior distribution over CPDAGs. From this distribution, we sample high-reward CPDAG candidates that approximate the ground truth, with rewards determined by a score function that quantifies how well each graph fits the data. Additionally, CPDAG-GFN incorporates a sparsity-preferring filter to enhance the set of CPDAG candidates and improve their alignment with the ground truth. Experimental results on both simulated and real-world datasets demonstrate that CPDAG-GFN performs competitively with established methods for learning CPDAG candidates from observational data.},
  archive      = {J_TMLR},
  author       = {Michelle Liu and Zhaocheng Zhu and Olexa Bilaniuk and Emmanuel Bengio},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Learning equivalence classes of bayesian network structures with GFlowNet},
  url          = {https://openreview.net/forum?id=FAcc7oAdaa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SELU: Self-learning embodied multimodal large language models in unknown environments. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=G5gROx8AVi'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multimodal large language models (MLLMs) have demonstrated strong visual understanding and decision-making capabilities, enabling the exploration of autonomously improving MLLMs in unknown environments. However, external feedback like human or environmental feedback is not always available. To address this challenge, existing methods primarily focus on enhancing the decision-making capabilities of MLLMs through voting and scoring mechanisms, while little effort has been paid to improving the environmental comprehension of MLLMs in unknown environments. To fully unleash the self-learning potential of MLLMs, we propose a novel actor-critic self-learning paradigm, dubbed SELU, inspired by the actor-critic paradigm in reinforcement learning. The critic employs self-asking and hindsight relabeling to extract knowledge from interaction trajectories collected by the actor, thereby augmenting its environmental comprehension. Simultaneously, the actor is improved by the self-feedback provided by the critic, enhancing its decision-making. We evaluate our method in the AI2-THOR and VirtualHome environments, and SELU achieves critic improvements of approximately 28% and 30%, and actor improvements of about 20% and 24% via self-learning.},
  archive      = {J_TMLR},
  author       = {Boyu Li and Haobin Jiang and Ziluo Ding and Xinrun Xu and Haoran Li and Dongbin Zhao and Zongqing Lu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {SELU: Self-learning embodied multimodal large language models in unknown environments},
  url          = {https://openreview.net/forum?id=G5gROx8AVi},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behaviour discovery and attribution for explainable reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=JbHtpOIH9l'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building trust in reinforcement learning (RL) agents requires understanding why they make certain decisions, especially in high-stakes applications like robotics, healthcare, and finance. Existing explainability methods often focus on single states or entire trajectories, either providing only local, step-wise insights or attributing decisions to coarse, episodelevel summaries. Both approaches miss the recurring strategies and temporally extended patterns that actually drive agent behavior across multiple decisions. We address this gap by proposing a fully offline, reward-free framework for behavior discovery and segmentation, enabling the attribution of actions to meaningful and interpretable behavior segments that capture recurring patterns appearing across multiple trajectories. Our method identifies coherent behavior clusters from state-action sequences and attributes individual actions to these clusters for fine-grained, behavior-centric explanations. Evaluations on four diverse offline RL environments show that our approach discovers meaningful behaviors and outperforms trajectory-level baselines in fidelity, human preference, and cluster coherence. Our code is publicly available.},
  archive      = {J_TMLR},
  author       = {Rishav Rishav and Somjit Nath and Vincent Michalski and Samira Ebrahimi Kahou},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Behaviour discovery and attribution for explainable reinforcement learning},
  url          = {https://openreview.net/forum?id=JbHtpOIH9l},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlowBench: Benchmarking optical flow estimation methods for reliability and generalization. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Kh4bj6YDNm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical flow estimation is a crucial computer vision task often applied to safety-critical real-world scenarios like autonomous driving and medical imaging. While optical flow estimation accuracy has greatly benefited from the emergence of deep learning, learning-based methods are also known for their lack of generalization and reliability. However, reliability is paramount when optical flow methods are employed in the real world, where safety is essential. Furthermore, a deeper understanding of the robustness and reliability of learning-based optical flow estimation methods is still lacking, hindering the research community from building methods safe for real-world deployment. Thus, we propose FlowBench, a robustness benchmark and evaluation tool for learning-based optical flow methods. FlowBench facilitates streamlined research into the reliability of optical flow methods by benchmarking their robustness to adversarial attacks and out-of-distribution samples. With FlowBench, we benchmark 57 checkpoints across 3 datasets under 9 diverse adversarial attacks and 23 established common corruptions, making it the most comprehensive robustness analysis of optical flow methods to date. Across this wide range of methods, we consistently find that methods with state-of-the-art performance on established standard benchmarks lack reliability and generalization ability. Moreover, we find interesting correlations between the performance, reliability, and generalization ability of optical flow estimation methods, under various lenses such as design choices used, number of parameters, etc. The open-source code and weights for FlowBench are available in this GitHub repository: https://github.com/shashankskagnihotri/FlowBench.},
  archive      = {J_TMLR},
  author       = {Shashank Agnihotri and Julian Yuya Caspary and Luca Schwarz and Xinyan Gao and Jenny Schmalfuss and Andres Bruhn and Margret Keuper},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {FlowBench: Benchmarking optical flow estimation methods for reliability and generalization},
  url          = {https://openreview.net/forum?id=Kh4bj6YDNm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication cost reduction for subgraph counting under local differential privacy via hash functions. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=N1J236mepp'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We suggest the use of hash functions to cut down the communication costs when counting subgraphs under edge local differential privacy. While various algorithms exist for computing graph statistics --- including the count of subgraphs --- under the edge local differential privacy, many suffer with high communication costs, making them less efficient for large graphs. Though data compression is a typical approach in differential privacy, its application in local differential privacy requires a form of compression that every node can reproduce. In our study, we introduce linear congruence hashing. Leveraging amplification by sub-sampling, with a sampling size of $s$, our method can cut communication costs by a factor of $s^2$, albeit at the cost of increasing variance in the published graph statistic by a factor of $s$. The experimental results indicate that, when matched for communication costs, our method achieves a reduction in the $\ell_2$-error by up to 1000 times for triangle counts and by up to $10^3$ times for 4-cycles counts compared to the performance of leading algorithms.},
  archive      = {J_TMLR},
  author       = {Quentin Hillebrand and Vorapong Suppakitpaisarn and Tetsuo Shibuya},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Communication cost reduction for subgraph counting under local differential privacy via hash functions},
  url          = {https://openreview.net/forum?id=N1J236mepp},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging fully-observable solutions for improved partially-observable offline reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=e9p4TDPy6A'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline reinforcement learning (RL) is a popular learning framework for control problems where online interactions with the environment are expensive, risky, or otherwise impractical. Existing offline RL methods commonly assume full observability of the state, and therefore there is a lack of offline RL methods that are specialized for the more general case of partially-observable control. To address this gap, we propose Cross-Observability Conservative Q-Learning (CO-CQL), an offline RL algorithm for partially-observable control that leverages fully-observable expert policies in an asymmetric learning setting. To motivate the use of fully-observable experts for partially-observable control, we formalize Cross-Observability Optimality Ratio (COOR), a theoretical measure of cross-observability that quantifies the benefit of learning asymmetrically from a fully-observable expert, and Cross-Observability Approximation Ratio (COAR), an estimation of COOR computable from trained policies. Our empirical evaluation on a wide variety of partially-observable challenges demonstrates that CO-CQL is able to exploit the guidance of fully-observable experts to outperform other state-of-the-art offline algorithms.},
  archive      = {J_TMLR},
  author       = {Chulabhaya Wijesundara and Andrea Baisero and Gregory David Castanon and Alan S Carlin and Robert Platt and Christopher Amato},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Leveraging fully-observable solutions for improved partially-observable offline reinforcement learning},
  url          = {https://openreview.net/forum?id=e9p4TDPy6A},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased loss functions for multilabel classification with missing labels. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=hMq1hUhLqp'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers binary and multilabel classification problems in a setting where labels are missing independently and with a known rate. Missing labels are a ubiquitous phenomenon in extreme multi-label classification (XMC) tasks, such as matching Wikipedia articles to a small subset out of the hundreds of thousands of possible tags, where no human annotator can possibly check the validity of all the negative samples. For this reason, propensity-scored precision---an unbiased estimate for precision-at-k under a known noise model---has become one of the standard metrics in XMC. Few methods take this problem into account already during the training phase, and all of these are limited to loss functions that can be decomposed into a sum of contributions from each individual label. A typical approach to training is to reduce the multilabel problem into a series of binary or multiclass problems, and it has been shown that if the surrogate task should be consistent for optimizing recall, the resulting loss function is not decomposable over labels. Therefore, this paper develops unbiased estimators for generic, potentially non-decomposable loss functions. These estimators suffer from increased variance and may lead to ill-posed optimization problems, which we address by switching to convex upper-bounds. The theoretical considerations are further supplemented by an experimental study showing that the switch to unbiased estimators significantly alters the bias-variance trade-off and thus requires stronger regularization.},
  archive      = {J_TMLR},
  author       = {Erik Schultheis and Rohit Babbar},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Unbiased loss functions for multilabel classification with missing labels},
  url          = {https://openreview.net/forum?id=hMq1hUhLqp},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical test for saliency maps of graph neural networks via selective inference. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=5NkXTCVa7F'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have gained prominence for their ability to process graph-structured data across various domains. However, interpreting GNN decisions remains a significant challenge, leading to the adoption of saliency maps for identifying salient subgraphs composed of influential nodes and edges. Despite their utility, the reliability of GNN saliency maps has been questioned, particularly in terms of their robustness to input noise. In this study, we propose a statistical testing framework to rigorously evaluate the significance of saliency maps. Our main contribution lies in addressing the inflation of the Type I error rate caused by double-dipping of data, leveraging the framework of Selective Inference. Our method provides statistically valid $p$-values while controlling the Type I error rate, ensuring that identified salient subgraphs contain meaningful information rather than random artifacts. The method is applicable to a variety of saliency methods with piecewise linearity (e.g., Class Activation Mapping). We validate our method on synthetic and real-world datasets, demonstrating its capability in assessing the reliability of GNN interpretations.},
  archive      = {J_TMLR},
  author       = {Shuichi Nishino and Tomohiro Shiraishi and Teruyuki Katsuoka and Ichiro Takeuchi},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Statistical test for saliency maps of graph neural networks via selective inference},
  url          = {https://openreview.net/forum?id=5NkXTCVa7F},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Certified robustness to data poisoning in gradient-based training. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=9WHifn9ZVX'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern machine learning pipelines leverage large amounts of public data, making it infeasible to guarantee data quality and leaving models open to poisoning and backdoor attacks. Provably bounding the behavior of learning algorithms under such attacks remains an open problem. In this work, we address this challenge by developing the first framework providing provable guarantees on the behavior of models trained with potentially manipulated data without modifying the model or learning algorithm. In particular, our framework certifies robustness against untargeted and targeted poisoning, as well as backdoor attacks, for bounded and unbounded manipulations of the training inputs and labels. Our method leverages convex relaxations to over-approximate the set of all possible parameter updates for a given poisoning threat model, allowing us to bound the set of all reachable parameters for any gradient-based learning algorithm. Given this set of parameters, we provide bounds on worst-case behavior, including model performance and backdoor success rate. We demonstrate our approach on multiple real-world datasets from applications including energy consumption, medical imaging, and autonomous driving.},
  archive      = {J_TMLR},
  author       = {Philip Sosnin and Mark Niklas Mueller and Maximilian Baader and Calvin Tsay and Matthew Robert Wicker},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Certified robustness to data poisoning in gradient-based training},
  url          = {https://openreview.net/forum?id=9WHifn9ZVX},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HalluEntity: Benchmarking and understanding entity-level hallucination detection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=494k7e9R5D'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hallucinated generation through uncertainty estimation. However, these approaches predominantly operate at the sentence or paragraph level, failing to pinpoint specific spans or entities responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabricated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HalluEntity, which annotates hallucination at the entity level. Based on the dataset, we comprehensively evaluate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our experimental results show that uncertainty estimation approaches focusing on individual token probabilities tend to over-predict hallucinations, while context-aware methods show better but still suboptimal performance. Through an in-depth qualitative study, we identify relationships between hallucination tendencies and linguistic properties and highlight important directions for future research. HalluEntity: https://huggingface.co/datasets/samuelyeh/HalluEntity},
  archive      = {J_TMLR},
  author       = {Min-Hsuan Yeh and Max Kamachee and Seongheon Park and Yixuan Li},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {HalluEntity: Benchmarking and understanding entity-level hallucination detection},
  url          = {https://openreview.net/forum?id=494k7e9R5D},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). System-2 mathematical reasoning via enriched instruction tuning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Cl9Uox031k'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving complex mathematical problems via system-2 reasoning is a natural human skill, yet it remains a significant challenge for current large language models (LLMs). We identify the scarcity of deliberate multi-step reasoning data as a primary limiting factor. To this end, we introduce Enriched Instruction Tuning (EIT), a method that enriches existing human-annotated mathematical datasets by augmenting human-annotated data with AI-generated feedback to create fine-grained reasoning trajectories. These datasets are then used to fine-tune open-source LLMs, enhancing their mathematical reasoning abilities without reliance on any symbolic verification program. Concretely, EIT is composed of two critical steps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step (ERS). The former generates a high-level plan that breaks down complex instructions into a sequence of simpler objectives, while ERS fills in reasoning contexts often overlooked by human annotators, creating a smoother reasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods that generate reasoning chains only depending on LLM's internal knowledge, our method leverages human-annotated initial answers as ``meta-knowledge'' to help LLMs generate more detailed and precise reasoning processes, leading to a more trustworthy LLM expert for complex mathematical problems. In experiments, EIT achieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing state-of-the-art fine-tuning and prompting methods, and even matching the performance of tool-augmented methods.},
  archive      = {J_TMLR},
  author       = {Huanqia Cai and Yijun Yang and Zhifeng Li},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {System-2 mathematical reasoning via enriched instruction tuning},
  url          = {https://openreview.net/forum?id=Cl9Uox031k},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning from human feedback with active queries. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=EScatQaRxz'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ instance-dependent regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of DPO, establishing it as a data-efficient alternative to DPO. The codes are available at https://github.com/jkx19/ActiveQuery.},
  archive      = {J_TMLR},
  author       = {Kaixuan Ji and Jiafan He and Quanquan Gu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Reinforcement learning from human feedback with active queries},
  url          = {https://openreview.net/forum?id=EScatQaRxz},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tree search for language model agents. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=QF0N3x2XVm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments showcase the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute.},
  archive      = {J_TMLR},
  author       = {Jing Yu Koh and Stephen Marcus McAleer and Daniel Fried and Ruslan Salakhutdinov},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Tree search for language model agents},
  url          = {https://openreview.net/forum?id=QF0N3x2XVm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effect of random learning rate: Theoretical analysis of SGD dynamics in non-convex optimization via stationary distribution. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=RPtKkNx9ZK'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a variant of the stochastic gradient descent (SGD) with a random learning rate and reveal its convergence properties. SGD is a widely used stochastic optimization algorithm in machine learning, especially deep learning. Numerous studies reveal the convergence properties of SGD and its theoretically favorable variants. Among these, the analysis of convergence using a stationary distribution of updated parameters provides generalizable results. However, to obtain a stationary distribution, the update direction of the parameters must not degenerate, which limits the applicable variants of SGD. In this study, we consider a novel SGD variant, Poisson SGD, which has degenerated parameter update directions and instead utilizes a random learning rate. Consequently, we demonstrate that a distribution of a parameter updated by Poisson SGD converges to a stationary distribution under weak assumptions on a loss function. Based on this, we further show that Poisson SGD finds global minima in non-convex optimization problems and also evaluate the generalization error using this method. As a proof technique, we approximate the distribution by Poisson SGD with that of the bouncy particle sampler (BPS) and derive its stationary distribution, using the theoretical advance of the piece-wise deterministic Markov process (PDMP).},
  archive      = {J_TMLR},
  author       = {Naoki Yoshida and Shogo Nakakita and Masaaki Imaizumi},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Effect of random learning rate: Theoretical analysis of SGD dynamics in non-convex optimization via stationary distribution},
  url          = {https://openreview.net/forum?id=RPtKkNx9ZK},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning the language of protein structure. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=SRRPQIOS4w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning and \emph{de novo} generation of proteins are pivotal computational biology tasks. Whilst natural language processing (NLP) techniques have proven highly effective for protein sequence modelling, structure modelling presents a complex challenge, primarily due to its continuous and three-dimensional nature. Motivated by this discrepancy, we introduce an approach using a vector-quantized autoencoder that effectively tokenizes protein structures into discrete representations. This method transforms the continuous, complex space of protein structures into a manageable, discrete format with a codebook ranging from 4096 to 64000 tokens, achieving high-fidelity reconstructions with backbone root mean square deviations (RMSD) of approximately 1-4 \AA. To demonstrate the efficacy of our learned representations, we show that a simple GPT model trained on our codebooks can generate novel, diverse, and designable protein structures. Our approach not only provides representations of protein structure, but also mitigates the challenges of disparate modal representations and sets a foundation for seamless, multi-modal integration, enhancing the capabilities of computational methods in protein design.},
  archive      = {J_TMLR},
  author       = {Jérémie DONA and Benoit Gaujac and Timothy Atkinson and Liviu Copoiu and Thomas Pierrot and Thomas D Barrett},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Learning the language of protein structure},
  url          = {https://openreview.net/forum?id=SRRPQIOS4w},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond ordinary lipschitz constraints: Differentially private optimization with TNC. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=SZCygcrGng'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study Stochastic Convex Optimization in Differential Privacy model (DP-SCO). Unlike previous studies, here we assume the population risk function satisfies the Tsybakov Noise Condition (TNC) with some parameter $\theta>1$, where the Lipschitz constant of the loss could be extremely large or even unbounded, but the $\ell_2$-norm gradient of the loss has bounded $k$-th moment with $k\geq 2$. For the Lipschitz case with $\theta\geq 2$, we first propose an $(\epsilon, \delta)$-DP algorithms whose utility bound is $\tilde{O}\left(\left(\tilde{r}_{2k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\epsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$ in high probability, where $n$ is the sample size, $d$ is the model dimension, and $\tilde{r}_{2k}$ is a term that only depends on the $2k$-th moment of the gradient. It is notable that such an upper bound is independent of the Lipschitz constant. We then extend to the case where $\theta\geq \bar{\theta}> 1$ for some known constant $\bar{\theta}$. Moreover, when the privacy budget $\epsilon$ is small enough, we show an upper bound of $\tilde{O}\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\epsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$ even if the loss function is not Lipschitz. For the lower bound, we show that for any $\theta\geq 2$, the private minimax rate for $\rho$-zero Concentrated Differential Privacy is lower bounded by $\Omega\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\sqrt{\rho}}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$.},
  archive      = {J_TMLR},
  author       = {Difei Xu and Meng Ding and Zihang Xiang and Jinhui Xu and Di Wang},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Beyond ordinary lipschitz constraints: Differentially private optimization with TNC},
  url          = {https://openreview.net/forum?id=SZCygcrGng},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complementarity: Toward better metrics and optimizing data efficiency in LLMs. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=feAbrMXGMh'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalist Large Language Models (LLMs) are trained with an immense amount of data from across different domains. However, not all data contribute to model performance equally, and prioritizing data quality can improve domain-specific performance. We suggest that quality is not merely an independent feature of datasets, but rather the manner in which data samples interfere with or complement one another. Furthermore, existing performance metrics for language models are computationally expensive, while also frequently suffering from being mathematically ill-defined and poorly suited to generative AI. Toward improving general performance while reducing the amount of training data, and quantifying how data contributes to downstream tasks vis-a-vis their relation with other data, we introduce a new metric, Complementarity. We first establish a strong correlation between Complementarity and domain-specific task performance. Without reliance on heavy instruction-tuning and text scraping, Complementarity is significantly less expensive to compute and is applicable to a wide variety of potential target domains. Most interestingly, we demonstrate that the Complementarity taken over a training validation set provides a better predictor of generalization to future test sets than directly measuring performance on a test validation set. With this, we introduce an algorithm that carefully selects the data to fine-tune upon, leading to a high-performing fine-tuned generalist model while using only a fraction of the data, and without requiring data from the test domain. Overall, Complementarity may serve as a key metric in future analysis of data utility and design of datasets, and help facilitate the goal of a truly generalist model.},
  archive      = {J_TMLR},
  author       = {Roy Siegelmann},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Complementarity: Toward better metrics and optimizing data efficiency in LLMs},
  url          = {https://openreview.net/forum?id=feAbrMXGMh},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the limitations of layer synchronization in spiking neural networks. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=mfmAVwtMIk'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural-network processing in machine learning applications relies on layer synchronization. This is practiced even in artificial Spiking Neural Networks (SNNs), which are touted as consistent with neurobiology, in spite of processing in the brain being in fact asynchronous. A truly asynchronous system however would allow all neurons to evaluate concurrently their threshold and emit spikes upon receiving any presynaptic current. Omitting layer synchronization is potentially beneficial, for latency and energy efficiency, but asynchronous execution of models previously trained with layer synchronization may entail a mismatch in network dynamics and performance. We present and quantify this problem, and show that models trained with layer synchronization either perform poorly in absence of the synchronization, or fail to benefit from any energy and latency reduction, when such a mechanism is in place. We then explore a potential solution direction, based on a generalization of backpropagation-based training that integrates knowledge about an asynchronous execution scheduling strategy, for learning models suitable for asynchronous processing. We experiment with 2 asynchronous neuron execution scheduling strategies in datasets that encode spatial and temporal information, and we show the potential of asynchronous processing to use less spikes (up to 50\%), complete inference faster (up to 2x), and achieve competitive or even better accuracy (up to $\sim$10\% higher). Our exploration affirms that asynchronous event-based AI processing can be indeed more efficient, but we need to rethink how we train our SNN models to benefit from it. (Source code available at: \url{https://github.com/RoelMK/asynctorch})},
  archive      = {J_TMLR},
  author       = {Roel Koopman and Amirreza Yousefzadeh and Mahyar Shahsavari and Guangzhi Tang and Manolis Sifalakis},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Exploring the limitations of layer synchronization in spiking neural networks},
  url          = {https://openreview.net/forum?id=mfmAVwtMIk},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple noises in diffusion model for semi-supervised multi-domain translation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=vYdT26kDYM'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address the challenge of multi-domain translation, where the objective is to learn mappings between arbitrary configurations of domains within a defined set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for three domains) without the need for separate models for each specific translation configuration, enabling more efficient and flexible domain translation. We introduce Multi-Domain Diffusion (MDD), a method with dual purposes: i) reconstructing any missing views for new data objects, and ii) enabling learning in semi-supervised contexts with arbitrary supervision configurations. MDD achieves these objectives by exploiting the noise formulation of diffusion models, specifically modeling one noise level per domain. Similar to existing domain translation approaches, MDD learns the translation between any combination of domains. However, unlike prior work, our formulation inherently handles semi-supervised learning without modification by representing missing views as noise in the diffusion process. We evaluate our approach through domain translation experiments on BL3NDT, a multi-domain synthetic dataset designed for challenging semantic domain inversion, the BraTS2020 dataset, and the CelebAMask-HQ dataset.},
  archive      = {J_TMLR},
  author       = {Tsiry Mayet and Simon Bernard and Romain HÉRAULT and Clement Chatelain},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Multiple noises in diffusion model for semi-supervised multi-domain translation},
  url          = {https://openreview.net/forum?id=vYdT26kDYM},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous language model interpolation yields dynamic and controllable text generation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=xD9Nu2Wah4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications. In particular, linear interpolation between model parameters forms the backbone for many recent approaches to adapting models to user preferences. While the existing literature on LLM adaptation primarily focuses on finding methods that optimize for some set of performance criteria or user preferences, here we instead seek to better understand and characterize the behavior of dense, continuous interpolation between models. Specifically, we use low-rank updates to fine-tune a base model to various different domains, yielding a set of anchor models with distinct generation profiles. Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull. We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes simultaneously. We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case. Our results suggest that parameter merging facilitates flexible model adaptation due to its predictable behavior within the full interpolation region.},
  archive      = {J_TMLR},
  author       = {Sara Kangaslahti and David Alvarez-Melis},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Continuous language model interpolation yields dynamic and controllable text generation},
  url          = {https://openreview.net/forum?id=xD9Nu2Wah4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auto-regressive vs flow-matching: A comparative study of modeling paradigms for text-to-music generation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=xXc5DeaBYw'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly in many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and identify which design choices influence performance the most. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: auto-regressive decoding and conditional flow-matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM},
  archive      = {J_TMLR},
  author       = {Or Tal and Felix Kreuk and Yossi Adi},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Auto-regressive vs flow-matching: A comparative study of modeling paradigms for text-to-music generation},
  url          = {https://openreview.net/forum?id=xXc5DeaBYw},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean-field RL for large-scale unit-capacity pickup-and-delivery problems. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=E8JRswdyDR'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving large-scale vehicle routing problems (VRPs) is NP-hard and poses a computational challenge in numerous applications such as logistics. Meanwhile, mean-field control (MFC) provides a tractable and rigorous approach to controlling many agents. We provide a solution to pickup-and-delivery VRPs via scalable MFC. In combination with reinforcement learning (RL) and clustering, our MFC approach efficiently scales to large-scale VRPs. We perform a theoretical analysis of our MFC-based approximation, giving convergence results for large VRP instances and error bounds for clustering-based approximations. We verify our algorithms on different datasets and compare them against solutions such as OR-Tools, PyVRP and heuristics, showing scalability in terms of speed for mean-field methods, for the first time in discrete optimization. Overall, our work establishes a novel synthesis of MFC-based RL techniques, vehicle routing problems and clustering approximations, to solve a hard discrete optimization problem of practical use in a scalable way.},
  archive      = {J_TMLR},
  author       = {Kai Cui and Sharif Azem and Christian Fabian and Kirill Kuroptev and Ramin Khalili and Osama Abboud and Florian Steinke and Heinz Koeppl},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Mean-field RL for large-scale unit-capacity pickup-and-delivery problems},
  url          = {https://openreview.net/forum?id=E8JRswdyDR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust uncertainty quantification for quantile treatment effects in sequential decision making. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=F0BwbieVws'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider multi-stage sequential decision making, where the treatment at any stage may depend on the subject’s entire treatment and covariate history. We introduce a general framework for doubly robust uncertainty quantification for the quantiles of cumulative outcomes under a sequential treatment rule. While previous studies focused on mean effects, quantile effects offer unique insights into the distributional properties and are more robust for heavy-tailed outcomes. It is known that, doubly robust inference is significantly more challenging and largely unexplored for quantile treatment effects. More importantly, for mean effects, doubly robust estimation does not ensure doubly robust inference. Our approach first provides a doubly robust estimator for any quantile of interest based on pre-collected data, achieving semi-parametric efficiency. We then propose a novel doubly robust estimator for the asymptotic variance, enabling the construction of a doubly robust confidence interval. To overcome the challenges in parameter-dependent nuisance functions, we leverage deep conditional generative learning techniques. We demonstrate advantages of our approach via both simulation and real data from a short video platform. Additionally, we observe that our proposed approach leads to another mean effect estimator that outperforms existing estimators with heavy-tailed outcomes.},
  archive      = {J_TMLR},
  author       = {Yang Xu and Chengchun Shi and Shikai Luo and Lan Wang and Rui Song},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Doubly robust uncertainty quantification for quantile treatment effects in sequential decision making},
  url          = {https://openreview.net/forum?id=F0BwbieVws},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The overcooked generalisation challenge: Evaluating cooperation with novel partners in unknown environments using unsupervised environment design. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=K2KtcMlW6j'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Overcooked Generalisation Challenge (OGC) – a new benchmark for evaluating reinforcement learning (RL) agents on their ability to cooperate with unknown partners in unfamiliar environments. Existing work typically evaluated cooperative RL only in their training environment or with their training partners, thus seriously limiting our ability to understand agents’ generalisation capacity – an essential requirement for future collaboration with humans. The OGC extends Overcooked-AI to support dual curriculum design (DCD). It is fully GPU-accelerated, open-source, and integrated into the minimax DCD benchmark suite. Compared to prior DCD benchmarks, where designers manipulate only minimal elements of the environment, OGC introduces a significantly richer design space: full kitchen layouts with multiple objects that require the designer to account for interaction dynamics between agents. We evaluate state-of-the-art DCD algorithms alongside scalable neural architectures and find that current methods fail to produce agents that generalise effectively to novel layouts and unfamiliar partners. Our results indicate that both agents and curriculum designers struggle with the joint challenge of partner and environment generalisation. These findings establish OGC as a demanding testbed for cooperative generalisation and highlight key directions for future research. We open-source our code.},
  archive      = {J_TMLR},
  author       = {Constantin Ruhdorfer and Matteo Bortoletto and Anna Penzkofer and Andreas Bulling},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {The overcooked generalisation challenge: Evaluating cooperation with novel partners in unknown environments using unsupervised environment design},
  url          = {https://openreview.net/forum?id=K2KtcMlW6j},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving inverse problems using diffusion with iterative colored renoising. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=RZv8FcQDPW'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relatively poor, especially early in the reverse process, and so we propose a new approach that iteratively reestimates and ``renoises'' the estimate several times per diffusion step. This iterative approach, which we call Fast Iterative REnoising (FIRE), injects colored noise that is shaped to ensure that the pre-trained diffusion model always sees white noise, in accordance with how it was trained. We then embed FIRE into the DDIM reverse process and show that the resulting ``DDfire'' offers state-of-the-art accuracy and runtime on several linear inverse problems, as well as phase retrieval.},
  archive      = {J_TMLR},
  author       = {Matthew C Bendel and Saurav K Shastri and Rizwan Ahmad and Philip Schniter},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Solving inverse problems using diffusion with iterative colored renoising},
  url          = {https://openreview.net/forum?id=RZv8FcQDPW},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COMMA: A communicative multimodal multi-agent benchmark. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=TIGQIem1na'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advances of multimodal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a novel puzzle benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of multimodal puzzles, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. Our findings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like o4-mini. Many chain of thought reasoning models such as R1-Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent collaboration, indicating a potential growth area in their communication abilities.},
  archive      = {J_TMLR},
  author       = {Timothy Ossowski and Danyal Maqbool and Jixuan Chen and Zefan Cai and Tyler J. Bradshaw and Junjie Hu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {COMMA: A communicative multimodal multi-agent benchmark},
  url          = {https://openreview.net/forum?id=TIGQIem1na},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-positive multi-label learning with label cardinality. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=XEPPXH2nKu'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study learning a multi-label classifier from partially labeled data, where each instance has only a single positive label. We explain how auxiliary information available on the label cardinality, the number of positive labels per instance, can be used for improving such methods. We consider auxiliary information of varying granularity, ranging from knowing just the maximum number of labels over all instances to knowledge on the distribution of label cardinalities and even the exact cardinality of each instance. We introduce methods leveraging the different types of auxiliary information, study how close to the fully labeled accuracy we can get under different scenarios, and show that an easy-to-implement method only assuming the knowledge of the maximum cardinality is comparable to the state-of-the-art single-positive multi-label learning methods when using the same base model. Our implementation is publicly available at https://github.com/shayangharib/SPMLL_with_Label_Cardinality.},
  archive      = {J_TMLR},
  author       = {Shayan Gharib and Pierre-Alexandre Murena and Arto Klami},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Single-positive multi-label learning with label cardinality},
  url          = {https://openreview.net/forum?id=XEPPXH2nKu},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text to stealthy adversarial face masks. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=XYqCx026AI'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated that modern facial recognition systems, which are based on deep neural networks, are vulnerable to adversarial attacks, including the use of accessories, makeup patterns, or precision lighting. However, developing attacks that are both robust (resilient to changes in viewing angles and environmental conditions) and stealthy (do not attract suspicion by, for example, incorporating obvious facial features) remains a significant challenge. In this context, we introduce a novel diffusion-based method (DAFR) capable of generating robust and stealthy face masks for dodging recognition systems (where the system fails to identify the attacker). Specifically our approach is capable of producing high-fidelity printable textures using the guidance of textual prompts to determine the style. This method can also be adapted for impersonation purposes, where the system misidentifies the attacker as a specific other individual. Finally, we address a gap in the existing literature by presenting a comprehensive benchmark (FAAB) for evaluating adversarial accessories in three dimensions, assessing their robustness and stealthiness.},
  archive      = {J_TMLR},
  author       = {Ben Lewis and Thomas Moyse and James Parkinson and Elizabeth Telford and Callum Whitfield and Ranko Lazic},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Text to stealthy adversarial face masks},
  url          = {https://openreview.net/forum?id=XYqCx026AI},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LAPP: Large language model feedback for preference-driven reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=cq76wx7T9F'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Large Language Model-Assisted Preference Prediction (LAPP), a novel framework for robot learning that enables efficient, customizable, and expressive behavior acquisition with minimum human effort. Unlike prior approaches that rely heavily on reward engineering, human demonstrations, motion capture, or expensive pairwise preference labels, LAPP leverages large language models (LLMs) to automatically generate preference labels from raw state-action trajectories collected during reinforcement learning (RL). These labels are used to train an online preference predictor, which in turn guides the policy optimization process toward satisfying high-level behavioral specifications provided by humans. Our key technical contribution is the integration of LLMs into the RL feedback loop through trajectory-level preference prediction, enabling robots to acquire complex skills including subtle control over gait patterns and rhythmic timing. We evaluate LAPP on a diverse set of quadruped locomotion and dexterous manipulation tasks and show that it achieves efficient learning, higher final performance, faster adaptation, and precise control of high-level behaviors. Notably, LAPP enables robots to master highly dynamic and expressive tasks such as quadruped backflips, which remain out of reach for standard LLM-generated or handcrafted rewards. Our results highlight LAPP as a promising direction for scalable preference-driven robot learning.},
  archive      = {J_TMLR},
  author       = {Pingcheng Jian and Xiao Wei and Yanbaihui Liu and Samuel A. Moore and Michael M. Zavlanos and Boyuan Chen},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {LAPP: Large language model feedback for preference-driven reinforcement learning},
  url          = {https://openreview.net/forum?id=cq76wx7T9F},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Studying memorization of large language models using answers to stack overflow questions. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ddocn44Kaq'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are capable of answering many software related questions and supporting developers by generating code snippets. These capabilities originate from training on massive amounts of data from the Internet, including information from Stack Overflow. This raises the question whether answers to software related questions are simply memorized from the training data, which might raise problems as this often requires attribution (e.g., CC-BY license), sharing with a similar license (e.g., GPL licenses) or may even be prohibited (proprietary license). To study this, we compare responses to questions from Stack Overflow for questions that were known during LLM pre-training and questions that were not included in the pre-training data. We then calculate the overlap both with answers marked as accepted on Stack Overflow as well as other texts we can find on the internet. We further explore the impact of the popularity of programming languages, the complexity of the prompts used, and the randomization of the text generation process on the memorization of answers to Stack Overflow. We find that many generated answers are to some degree collages of memorized content and that this does not dependent on whether the questions were seen during training or not. However, many of the memorized snippets are common phrases or code and, therefore, not copyrightable. Still, we also have clear evidence that copyright violation happens and is likely when LLMs are used at large scales.},
  archive      = {J_TMLR},
  author       = {Laura Caspari and Alexander Trautsch and Michael Granitzer and Steffen Herbold},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Studying memorization of large language models using answers to stack overflow questions},
  url          = {https://openreview.net/forum?id=ddocn44Kaq},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HandsOnVLM: Vision-language models for hand-object interaction prediction. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ehhMFjKnWm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we predict future interaction trajectories of human hands in a scene given high-level colloquial task specifications in the form of natural language? In this paper, we extend the classic hand trajectory prediction task to several tasks involving explicit and implicit language queries. Our proposed tasks require extensive understanding of human daily activities and reasoning abilities about what is happening next given cues from the current scene. We also develop new benchmarks to evaluate the proposed two tasks, Vanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We enable solving these tasks by integrating high-level world knowledge and reasoning capabilities of Vision-Language Models (VLMs) with the auto-regressive nature of low-level ego-centric hand trajectories. Our model, HandsOnVLM is a novel VLM that can generate textual responses and produce future hand trajectories through natural-language conversations. Our experiments show that HandsOnVLM outperforms existing task-specific methods and other VLM baselines on proposed tasks, and demonstrates its ability to effectively utilize world knowledge for reasoning about low-level human hand trajectories based on the provided context. More details can be found at https://www.chenbao.tech/handsonvlm/.},
  archive      = {J_TMLR},
  author       = {Chen Bao and Jiarui Xu and Xiaolong Wang and Abhinav Gupta and Homanga Bharadhwaj},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {HandsOnVLM: Vision-language models for hand-object interaction prediction},
  url          = {https://openreview.net/forum?id=ehhMFjKnWm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discrete audio tokens: More than a survey!. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=eqNchtvc6v'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks. They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.},
  archive      = {J_TMLR},
  author       = {Pooneh Mousavi and Gallil Maimon and Adel Moumen and Darius Petermann and Jiatong Shi and Haibin Wu and Haici Yang and Anastasia Kuznetsova and Artem Ploujnikov and Ricard Marxer and Bhuvana Ramabhadran and Benjamin Elizalde and Loren Lugosch and Jinyu Li and Cem Subakan and Phil Woodland and Minje Kim and Hung-yi Lee and Shinji Watanabe and Yossi Adi and Mirco Ravanelli},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Discrete audio tokens: More than a survey!},
  url          = {https://openreview.net/forum?id=eqNchtvc6v},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the learned look-ahead behavior of chess neural networks. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=np4Bg2zIxL'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the look-ahead capabilities of chess-playing neural networks, specifically focusing on the Leela Chess Zero policy network. We build on the work of Jenner et al. (2024) by analyzing the model's ability to consider future moves and alternative sequences beyond the immediate next move. Our findings reveal that the network's look-ahead behavior is highly context-dependent, varying significantly based on the specific chess position. We demonstrate that the model can process information about board states up to seven moves ahead, utilizing similar internal mechanisms across different future time steps. Additionally, we provide evidence that the network considers multiple possible move sequences rather than focusing on a single line of play. These results offer new insights into the emergence of sophisticated look-ahead capabilities in neural networks trained on strategic tasks, contributing to our understanding of AI reasoning in complex domains. Our work also showcases the effectiveness of interpretability techniques in uncovering cognitive-like processes in artificial intelligence systems.},
  archive      = {J_TMLR},
  author       = {Diogo Cruz},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Understanding the learned look-ahead behavior of chess neural networks},
  url          = {https://openreview.net/forum?id=np4Bg2zIxL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlowKac: An efficient neural fokker-planck solver using temporal normalizing flows and the feynman-kac formula. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=paeyQFa5or'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving the Fokker-Planck equation for high-dimensional complex dynamical systems remains a pivotal yet challenging task due to the intractability of analytical solutions and the limitations of traditional numerical methods. In this work, we present FlowKac, a novel approach that reformulates the Fokker-Planck equation using the Feynman-Kac formula, allowing to query the solution at a given point via the expected values of stochastic paths. A key innovation of FlowKac lies in its adaptive stochastic sampling scheme which significantly reduces the computational complexity while maintaining high accuracy. This sampling technique, coupled with a time-indexed normalizing flow, designed for capturing time-evolving probability densities, enables robust sampling of collocation points, resulting in a flexible and mesh-free solver. This formulation mitigates the curse of dimensionality and enhances computational efficiency and accuracy, which is particularly crucial for applications that inherently require dimensions beyond the conventional three. We validate the robustness and scalability of our method through various experiments on a range of stochastic differential equations, demonstrating significant improvements over existing techniques.},
  archive      = {J_TMLR},
  author       = {Naoufal EL BEKRI and Lucas Drumetz and Franck Vermet},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {FlowKac: An efficient neural fokker-planck solver using temporal normalizing flows and the feynman-kac formula},
  url          = {https://openreview.net/forum?id=paeyQFa5or},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient object-centric representation learning using masked generative modeling. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=t9KvOYPeL3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning object-centric representations from visual inputs in an unsupervised manner has drawn focus to solve more complex tasks, such as reasoning and reinforcement learning. However, current state-of-the-art methods, relying on autoregressive transformers or diffusion models to generate scenes from object-centric representations, suffer from computational inefficiency due to their sequential or iterative nature. This computational bottleneck limits their practical application and hinders scaling to more complex downstream tasks. To overcome this, we propose MOGENT, an efficient object-centric learning framework based on masked generative modeling. MOGENT conditions a masked bidirectional transformer on learned object slots and employs a parallel iterative decoding scheme to generate scenes, enabling efficient compositional generation. Experiments show that MOGENT significantly improves computational efficiency, accelerating the generation process by up to 67x and 17x compared to autoregressive models and diffusion-based models, respectively. Importantly, the efficiency is attained while maintaining strong or competitive performance on object segmentation and compositional generation tasks.},
  archive      = {J_TMLR},
  author       = {Akihiro Nakano and Masahiro Suzuki and Yutaka Matsuo},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Efficient object-centric representation learning using masked generative modeling},
  url          = {https://openreview.net/forum?id=t9KvOYPeL3},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedComLoc: Communication-efficient distributed training of sparse and quantized models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=vYQPLytQsj'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is Local Training, which involves running multiple local stochastic gradient descent iterations between communication phases. Our work is inspired by the innovative Scaffnew algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc (Federated Compressed and Local Training), integrating practical and effective compression into Scaffnew to further enhance communication efficiency. Extensive experiments, using the popular Top-K compressor and quantization, demonstrate its prowess in substantially reducing communication overheads in heterogeneous settings.},
  archive      = {J_TMLR},
  author       = {Kai Yi and Georg Meinhardt and Laurent Condat and Peter Richtárik},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {FedComLoc: Communication-efficient distributed training of sparse and quantized models},
  url          = {https://openreview.net/forum?id=vYQPLytQsj},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label embedding via low-coherence matrices. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=vrcWXcr4On'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label embedding is a framework for multiclass classification problems where each label is represented by a distinct vector of some fixed dimension, and training involves matching model output to the vector representing the correct label. While label embedding has been successfully applied in extreme classification and zero-shot learning, and offers both computational and statistical advantages, its theoretical foundations remain poorly understood. This work presents an analysis of label embedding in the context of extreme multiclass classification, where the number of classes $C$ is very large. We present an excess risk bound that reveals a trade-off between computational and statistical efficiency, quantified via the coherence of the embedding matrix. We further show that under the Massart noise condition, the statistical penalty for label embedding vanishes with sufficiently low coherence. Our analysis supports an algorithm that is simple, scalable, and easily parallelizable, and experimental results demonstrate its effectiveness in large-scale applications.},
  archive      = {J_TMLR},
  author       = {Jianxin Zhang and Clayton Scott},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Label embedding via low-coherence matrices},
  url          = {https://openreview.net/forum?id=vrcWXcr4On},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNR-pruning: Sparsity-aware pruning via dying neuron reactivation in convolutional neural networks. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ymUjGCNPYa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we challenge the conventional view of dead neurons—neurons that cease to activate—during deep neural network training. Traditionally regarded as problematic due to their association with optimization challenges and reduced model adaptability over training epochs, dead neurons are often seen as a hindrance. However, we present a novel perspective, demonstrating that they can be effectively leveraged to enhance network sparsity. Specifically, we propose DNR-Pruning, dying neuron reactivation based sparsity-aware pruning approach for convolutional neural networks (CNNs) that exploits the behavior of individual neurons during training. Through a systematic exploration of hyperparameter configurations, we show that dying neurons can be harnessed to improve pruning algorithms. Our method dynamically monitors the occurrence of dying neurons, enabling adaptive sparsification throughout CNN training. Extensive experiments on diverse datasets demonstrate that DNR-Pruning outperforms existing sparsity-aware pruning techniques while achieving competitive results compared to state-of-the-art methods. These findings suggest that dying neurons can serve as an efficient mechanism for network compression and resource optimization in CNNs, opening new avenues for more efficient and high-performance deep learning models.},
  archive      = {J_TMLR},
  author       = {Boyuan Wang and Richard Jiang},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {DNR-pruning: Sparsity-aware pruning via dying neuron reactivation in convolutional neural networks},
  url          = {https://openreview.net/forum?id=ymUjGCNPYa},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
