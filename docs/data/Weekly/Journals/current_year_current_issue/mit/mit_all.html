<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>mit</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alj">ALJ - 9</h2>
<ul>
<li><details>
<summary>
(2025). Automating the search for artificial life with foundation models. <em>ALJ</em>, <em>31</em>(3), 368-396. (<a href='https://doi.org/10.1162/ARTL.a.8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial and error to discover the configurations of lifelike simulations. This article presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called automated search for Artificial Life (ASAL), (a) finds simulations that produce target phenomena, (b) discovers simulations that generate temporally open-ended novelty, and (c) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates, including Boids, Particle Life, the Game of Life, Lenia, and neural cellular automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids life-forms, as well as cellular automata that are open-ended like Conway’s Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.},
  archive      = {J_ALJ},
  author       = {Kumar, Akarsh and Lu, Chris and Kirsch, Louis and Tang, Yujin and Stanley, Kenneth O. and Isola, Phillip and Ha, David},
  doi          = {10.1162/ARTL.a.8},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {368-396},
  shortjournal = {Artif. Life},
  title        = {Automating the search for artificial life with foundation models},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive distinctions as a language for cognitive science: Comparing methods of description in a model of referential communication. <em>ALJ</em>, <em>31</em>(3), 345-367. (<a href='https://doi.org/10.1162/artl_a_00475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An analysis of the language we use in scientific practice is critical to developing more rigorous and sound methodologies. This article argues that how certain methods of description are commonly employed in cognitive science risks obscuring important features of an agent’s cognition. We propose to make explicit a method of description whereby the concept of cognitive distinctions is the core principle. A model of referential communication is developed and analyzed as a platform to compare methods of description. We demonstrate that cognitive distinctions, realized in a graph theoretic formalism, better describe the behavior and perspective of a simple model agent than other, less systematic or natural language–dependent methods. We then consider how different descriptions relate to one another in the broader methodological framework of minimally cognitive behavior. Finally, we explore the consequences of, and challenges for, cognitive distinctions as a useful concept and method in the tool kit of cognitive scientists.},
  archive      = {J_ALJ},
  author       = {Gaul, Thomas M. and Izquierdo, Eduardo J.},
  doi          = {10.1162/artl_a_00475},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {345-367},
  shortjournal = {Artif. Life},
  title        = {Cognitive distinctions as a language for cognitive science: Comparing methods of description in a model of referential communication},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behaviour diversity in a walking and climbing centipede-like virtual creature. <em>ALJ</em>, <em>31</em>(3), 321-344. (<a href='https://doi.org/10.1162/artl_a_00476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot controllers are often optimized for a single robot in a single environment. This approach proves brittle, as such a controller will often fail to produce sensible behavior for a new morphology or environment. In comparison, animal gaits are robust and versatile. By observing animals, and attempting to extract general principles of locomotion from their movement, we aim to design a single, decentralized controller applicable to diverse morphologies and environments. The controller implements the three components of (a) undulation, (b) peristalsis, and (c) leg motion, which we believe are the essential elements in most animal gaits. This work is a first step toward a general controller. Accordingly, the controller has been evaluated on a limited range of simulated centipede-like robot morphologies. The centipede is chosen as inspiration because it moves using both body contractions and legged locomotion. For a controller to work in qualitatively different settings, it must also be able to exhibit qualitatively different behaviors. We find that six different modes of locomotion emerge from our controller in response to environmental and morphological changes. We also find that different parts of the centipede model can exhibit different modes of locomotion, simultaneously, based on local morphological features. This controller can potentially aid in the design or evolution of robots, by quickly testing the potential of a morphology, or be used to get insights about underlying locomotion principles in the centipede.},
  archive      = {J_ALJ},
  author       = {Norstein, Emma Stensby and Yasui, Kotaro and Kano, Takeshi and Ishiguro, Akio and Glette, Kyrre},
  doi          = {10.1162/artl_a_00476},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {321-344},
  shortjournal = {Artif. Life},
  title        = {Behaviour diversity in a walking and climbing centipede-like virtual creature},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benefit game 2.0: Alien seaweed Swarms—Exploring the interplay of human activity and environmental sustainability. <em>ALJ</em>, <em>31</em>(3), 304-320. (<a href='https://doi.org/10.1162/artl_a_00468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Benefit Game 2.0, a multiscreen Artificial Life gameplay installation. Saccharina latissima , a seaweed species economically beneficial to humans but threatened by overexploitation, motivates the creation of this artwork. Technically, the authors create an underwater virtual ecosystem consisting of a seaweed swarm and symbiotic fungi, created using procedural content generation via machine learning and rule-based methods. Moreover, the work features a unique cybernetic loop structure, incorporating audience observation and game token interactions. This virtual system is also symbolically influenced in real time by indoor carbon dioxide measurements, serving as an artistic metaphor for the broader impacts of climate change. This integration with the physical game machine underscores the fragile relationship between human activities and the environment under severe global climate change and immerses the audience in the challenging balance between sustainability and profit seeking in this context.},
  archive      = {J_ALJ},
  author       = {Fei, Dan-Lu and Wu, Zi-Wei and Zhang, Kang},
  doi          = {10.1162/artl_a_00468},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {304-320},
  shortjournal = {Artif. Life},
  title        = {Benefit game 2.0: Alien seaweed Swarms—Exploring the interplay of human activity and environmental sustainability},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complexity, artificial life, and artificial intelligence. <em>ALJ</em>, <em>31</em>(3), 289-303. (<a href='https://doi.org/10.1162/artl_a_00462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scientific fields of complexity, Artificial Life (ALife), and artificial intelligence (AI) share commonalities: historic, conceptual, methodological, and philosophical. Although their origins trace back to the 1940s birth of cybernetics, they were able to develop properly only as modern information technology became available. In this perspective, I offer a personal (and thus biased) account of the expectations and limitations of these fields, some of which have their roots in the limits of formal systems. I use interactions, self-organization, emergence, and balance to compare different aspects of complexity, ALife, and AI. Even when the trajectory of the article is influenced by my personal experience, the general questions posed (which outweigh the answers) will, I hope, be useful in aligning efforts in these fields toward overcoming—or accepting—their limits.},
  archive      = {J_ALJ},
  author       = {Gershenson, Carlos},
  doi          = {10.1162/artl_a_00462},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {289-303},
  shortjournal = {Artif. Life},
  title        = {Complexity, artificial life, and artificial intelligence},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolvability in artificial development of large, complex structures and the principle of terminal addition. <em>ALJ</em>, <em>31</em>(3), 276-288. (<a href='https://doi.org/10.1162/artl_a_00460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epigenetic tracking (ET) is a model of development that is capable of generating diverse, arbitrary, complex three-dimensional cellular structures starting from a single cell. The generated structures have a level of complexity (in terms of the number of cells) comparable to multicellular biological organisms. In this article, we investigate the evolvability of the development of a complex structure inspired by the “French flag” problem: an “Italian Anubis” (a three-dimensional, doglike figure patterned in three colors). Genes during development are triggered in ET at specific developmental stages, and the fitness of individuals during simulated evolution is calculated after a certain stage. When this evaluation stage was allowed to evolve, genes that were triggered at later stages of development tended to be incorporated into the genome later during evolutionary runs. This suggests the emergence of the property of terminal addition in this system. When the principle of terminal addition was explicitly incorporated into ET, and was the sole mechanism for introducing morphological innovation, evolvability improved markedly, leading to the development of structures much more closely approximating the target at a much lower computational cost.},
  archive      = {J_ALJ},
  author       = {Fontana, Alessandro and Wróbel, Borys},
  doi          = {10.1162/artl_a_00460},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {276-288},
  shortjournal = {Artif. Life},
  title        = {Evolvability in artificial development of large, complex structures and the principle of terminal addition},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous evolution in the NK treadmill model. <em>ALJ</em>, <em>31</em>(3), 256-275. (<a href='https://doi.org/10.1162/artl_a_00467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The NK fitness landscape is a well-known model with which to study evolutionary dynamics in landscapes of different ruggedness. However, the model is static, and genomes are typically small, allowing observations over only a short adaptive period. Here we introduce an extension to the model that allows the experimenter to set the velocity at which the landscape changes independently from other parameters, such as the ruggedness or the mutation rate. We find that, similar to the previously observed complexity catastrophe, where evolution comes to a halt when environments become too complex due to overly high degrees of epistasis, here the same phenomenon occurs when changes happen too rapidly. Our expanded model also preserves essential properties of the static NK landscape, allowing for proper comparisons between static and dynamic landscapes.},
  archive      = {J_ALJ},
  author       = {Mehra, Priyanka and Hintze, Arend},
  doi          = {10.1162/artl_a_00467},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {256-275},
  shortjournal = {Artif. Life},
  title        = {Continuous evolution in the NK treadmill model},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neurons as autoencoders. <em>ALJ</em>, <em>31</em>(3), 250-255. (<a href='https://doi.org/10.1162/artl_c_00461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter presents the idea that neural backpropagation is exploiting dendritic processing to enable individual neurons to perform autoencoding. Using a very simple connection weight search heuristic and artificial neural network model, the effects of interleaving autoencoding for each neuron in a hidden layer of a feedforward network are explored. This is contrasted with the equivalent standard layered approach to autoencoding. It is shown that such individualized processing is not detrimental and can improve network learning.},
  archive      = {J_ALJ},
  author       = {Bull, Larry},
  doi          = {10.1162/artl_c_00461},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {250-255},
  shortjournal = {Artif. Life},
  title        = {Neurons as autoencoders},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A word from the editors. <em>ALJ</em>, <em>31</em>(3), 249. (<a href='https://doi.org/10.1162/ARTL.e.11'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue of contributed articles, we are pleased to share with you a diversity of ALife research.In his letter “Neurons as Autoencoders,” Bull brings together ideas from backpropagation, dendritic computing, the NK model, and autoencoders to augment standard models of neuronal autoencoding. The NK model reappears in the article “Continuous Evolution in the NK Treadmill Model,” in which Mehra and Hintze extend the originally static model to allow it to change in a parameterized manner and investigate how evolution behaves at different rates of change. In “Evolvability in Artificial Development of Large Complex Structures and the Principle of Terminal Addition,” Fontana and Wróbel examine evo-devo processes and the emergence of modification to the later stages of a developmental process.In a historical and philosophical perspective, Gershenson offers a personal account that compares various aspects of, and raises questions about, the closely related topics of “Complexity, Artificial Life, and Artificial Intelligence.”Games are an interesting subtopic in ALife, useful both for understanding complex systems and for outreach. In “Benefit Game 2.0: Alien Seaweed Swarms—Exploring the Interplay of Human Activity and Environmental Sustainability,” Fei, Wu, and Zhang present their immersive seaweed ecosystem simulation game that responds not only to audience input but also to the real-world environmental state. Moving from plant to animal behavior, in “Behaviour Diversity in a Walking and Climbing Centipede-Like Virtual Creature,” Norstein et al. take inspiration from arthropod multilegged locomotion to help develop robotic controllers robust to morphological and environmental variation.In their methodological article “Cognitive Distinctions as a Language for Cognitive Science: Comparing Methods of Description in a Model of Referential Communication,” Gaul and Izquierdo examine the effect of choices of language and terminology on theoretical frameworks in cognitive science and use a model to expose the consequences. In another methodological work, “Automating the Search for Artificial Life With Foundation Models,” Kumar et al. exploit concepts of artificial intelligence’s foundation models to develop a new approach to searching for lifelike behaviors, applied to a range of ALife simulations.We thank all the authors for the time and care they took to perform their research and present their work. We hope you enjoy the results!},
  archive      = {J_ALJ},
  author       = {Dorin, Alan and Stepney, Susan},
  doi          = {10.1162/ARTL.e.11},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {249},
  shortjournal = {Artif. Life},
  title        = {A word from the editors},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="coli">COLI - 9</h2>
<ul>
<li><details>
<summary>
(2025). Natural language processing RELIES on linguistics. <em>COLI</em>, <em>51</em>(3), 1009-1032. (<a href='https://doi.org/10.1162/coli_a_00560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES , which encapsulates six major facets where linguistics contributes to NLP: R esources, E valuation, L ow-resource settings, I nterpretability, E xplanation, and the S tudy of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-à-vis systems of human language.},
  archive      = {J_COLI},
  author       = {Opitz, Juri and Wein, Shira and Schneider, Nathan},
  doi          = {10.1162/coli_a_00560},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1009-1032},
  shortjournal = {Comput. Lingu.},
  title        = {Natural language processing RELIES on linguistics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated essay scoring. <em>COLI</em>, <em>51</em>(3), 1005-1008. (<a href='https://doi.org/10.1162/coli_r_00513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated essay scoring is concerned with the development of language technologies that make it possible for an essay—or any piece of writing for that matter—to be evaluated or scored by a computer. These technologies find their utility primarily in the context of educational measurement, where they serve a dual purpose. On the one hand, they provide crucial support to educators and institutions, facilitating the assessment of students’ writing skills and content knowledge. A good example is the TOEFL iBT®, the Internet-based Test of English as a Foreign Language, administered by the Educational Testing Service and widely adopted by institutions worldwide. On the other hand, these technologies benefit writers themselves, including students, by offering a platform to assess and enhance their writing skills. One illustrative tool for this purpose is the Write &amp; Improve software developed at the University of Cambridge.The field of automated essay scoring emerged in the pioneering era of artificial intelligence and computational linguistics, with the inception of Ellis Page’s Project Essay Grade system at the University of Connecticut in 1964, and the subsequent publication of his seminal article “The Imminence of…Grading Essays by Computer” in 1966. Page’s automated scoring system is seen by the authors of this book as one of the first concrete applications of natural language processing, after the Audrey system for speech recognition and the Georgetown–IBM demonstration of machine translation. But just like speech recognition and machine translation, the industrialization of automated essay scoring mostly gained momentum in the 1990s. This decade saw an increase in richly annotated language data and corpora, which enabled the use of statistical and supervised machine learning in developing essay-scoring systems. Pearson’s Intelligent Essay Assessor™, released in 1998, is a prominent example of this evolution. A year later, in 1999, the e-rater® scoring engine was launched by the Educational Testing Service, commonly abbreviated as ETS, a private organization overseeing several standardized tests and high-stakes examinations such as TOEFL® and GRE®.This book is written by Beata Beigman Klebanov and Nitin Madnani, two researchers at ETS with many years of research experience and numerous peer-reviewed publications in the field of automated essay scoring. Their book provides a concise yet indispensable introduction to the field. After this short introduction, the eager reader is invited to take a deep dive into the scientific literature, encompassing slightly over half of the book’s content, approximately 115 pages. The literature review primarily caters to computational linguists and NLP practitioners, as it delves comprehensively into diverse machine learning models—ranging from linear regression to artificial neural networks—and intricate linguistic features. These features include general indicators of writing quality, such as text organization, coherence, and grammaticality, as well as genre-specific features, such as argument structure in argumentative essays. Furthermore, the reader not only gains insight into the extensive research carried out at ETS throughout the years but also delves into their technical expertise through a series of guided experiments with RSMTool—an open-source tool developed by the second author and colleagues. In addition, the authors also provide insight into a scalable and production-ready computer architecture used to build ETS products such as c-rater™, e-rater®, Language Muse®, and Writing Mentor™.The book is divided into five parts counting 13 chapters in total. The first part contains an introductory chapter in which the authors introduce the reader to Page’s aforementioned seminal 1966 article. Chapter 1 enumerates several arguments made by Page in favor of automated essay scoring, including its educational need, computational feasibility, high quality, and low cost. The chapter then sets forth four challenges associated with automated essay scoring. Two among them are the evaluation of original and source-based writing. Original writing, which reflects an author’s unique voice and thus stands out from existing and conventional works, poses a challenge because its originality could be overlooked or even under-evaluated by a computer. Source-based writing, a form of writing that reviews main points from external sources, presents a different problem because the assessment focuses on the correctness of content rather than measuring writing skills and essay quality. Another challenge is avoiding potential gaming strategies that test takers may employ to inflate their scores. A final challenge is automated feedback, as the effectiveness of providing linguistic and stylistic commentary on written work is not always proven.The second part contains two chapters covering a series of guided experiments and a set of best practices when building an automated essay scoring system. Chapter 2 provides a step-by-step guide for building an automated scoring system with supervised machine learning. First, the reader learns the usual engineering setup, including the use of a standard dataset (viz., the Automated Student Assessment Prize competition), an interpretable machine learning model (viz., linear regression), and a set of basic features derived from a scoring rubric. The authors also introduce the reader to their RSMTool. Then, the reader is guided through a series of ten experiments illustrating the incremental development (experiments 1–5) and evaluation (experiments 6–10) of the machine learning model. In each experiment, the reader is taught an important lesson such as feature fairness. For each experiment, the authors refer the reader to a report (i.e., correction key) available online.Chapter 3 provides some best practices for building an automated essay scoring system. First, the authors identify potentially conflicting perspectives between NLP developers and other stakeholders, including end users, subject matter experts, and business units. The authors then describe three natural use cases for automatic essay scoring, where it is added to a pre-existing assessment, developed concurrently with a new assessment, or implemented in a classroom. The authors provide some practical considerations and concrete actions that are well thought out and will appeal to many different readers.The third part contains five chapters that provide an up-to-date overview of the scientific literature and a more detailed account of the concepts introduced in the previous chapters. Chapter 4 describes various statistical models, including linear regression, latent semantic analysis, support vector machines, random forests, ensemble methods, and neural networks. For each of these models, the authors describe their mathematical underpinnings and explain how they can be used to score an essay. The chapter pays special attention to recent deep-learning architectures for automated essay scoring.Chapters 5 and 6 describe computational features that capture various aspects of the writing construct and the scoring rubric. Chapter 5 deals with general features. The features are organized into three classes: discourse features aiming to capture essay organization, development, and coherence; content features related to vocabulary use and topicality; and conventional features based on grammatical error detection. Chapter 6 then gives an in-depth overview of computational features that pertain to specific writing genres. The chapter is focused on four genres: argumentative writing, narrative writing, source-based writing, and reflective writing. Argumentative writing involves defending a particular position on a given topic (e.g., why technology should be integrated into education), presenting several claims as to why this position is valid, and supporting these claims with premises and evidence. Narrative writing involves telling a story that describes, for example, the historical evolution of technology in education. Source-based writing involves summarizing and comparing key points from external sources to compose an informed essay, which, for example, reviews the effectiveness of integrating technology into education based on scholarly sources. Finally, reflective writing involves examining personal experiences, such as teachers describing their experiences integrating technology into the classroom and reflecting on important lessons learned from this experience. This detailed overview of different writing genres also interestingly introduces the reader to research from related fields, such as computational argumentation and text summarization.Chapters 7 and 8 address two concerns in setting up real-world applications of automated essay scoring. Chapter 7 deals with the issues of reliability, scalability, and flexibility when deploying a scoring system at a large scale. The chapter describes and illustrates an Apache Storm architecture implemented in several ETS systems. Chapter 8 is about evaluating construct validity and fairness. When deploying an essay scoring system for high-stakes testing, fundamental issues arise when the system fails to measure the construct, assigns scores influenced by factors irrelevant to the measured construct, or is biased towards specific personal characteristics in the population of test takers. If an essay scoring system overlooks important features or favors particular writing styles or cultural representations, it undermines the validity and fairness of high-stakes assessments.The fourth part contains four chapters examining some broader challenges introduced in the first chapter and which remain to be solved for automated scoring. Chapter 9 deals with the automated generation of useful feedback on writing. The authors review several existing feedback systems and discuss how to define and evaluate the usefulness of feedback. Chapter 10 focuses on evaluating essay content, which is separate from assessing essay quality. Content scoring emphasizes measuring test-takers’ content knowledge, prioritizing these elements over writing skills. This evaluation can adopt either a reference-based or a response-based approach. Reference-based scoring involves comparing responses to a set of predefined reference answers, while response-based scoring independently assesses the response content. The chapter primarily explores the latter, investigating computational features and models tailored for this approach. Chapter 11 deals with another task related to but different from essay scoring, namely the automated scoring of spontaneous speech. After a brief account of the challenges with automated speech recognition, the authors review three sets of features for speech scoring: the delivery and fluency of spontaneous speech, vocabulary and grammar use, and topic development. The authors also contrast features relevant for scoring speech with those relevant for scoring writing. Lastly, chapter 12 examines several gaming strategies test-takers could use to fool the automated scoring system into giving a higher score. The authors review four types of strategies: the unnecessary use of shell language, the artificial generation of essays, the submission of off-topic responses, and the use of canned responses or plagiarized essays.The fifth and final part of the book contains a concluding chapter. The authors revisit the desiderata put forth by Ellis Page in his 1966 publication and summarize the overall achievements and remaining challenges in this respect. In addition, the authors discuss other challenging aspects that Page did not envision, such as the present-day ubiquity of technology, dealing with multiple languages, and setting up high-stakes tests that are valid, defensible, and fair.In sum, the book offers an excellent introduction to and deepening of the field of automated essay scoring. The book is well-structured and easy to read. Throughout the book, the authors provide thoughtful insights and practical advice based on their many years of experience at ETS. Compared to other books on the subject, the book offers a valuable combination of practical lessons and scientific deepening. By the end of the book, the reader has acquired a broad knowledge of the possibilities, challenges, and practical concerns involved with the automated scoring of student writing.},
  archive      = {J_COLI},
  author       = {Tack, Anaïs},
  doi          = {10.1162/coli_r_00513},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1005-1008},
  shortjournal = {Comput. Lingu.},
  title        = {Automated essay scoring},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey of cultural awareness in language models: Text and beyond. <em>COLI</em>, <em>51</em>(3), 907-1004. (<a href='https://doi.org/10.1162/COLI.a.14'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive, going beyond multilinguality and building on findings from psychology and anthropology. In this article, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking definitions of culture from the anthropology and psychology literature as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of human–computer interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature. 1},
  archive      = {J_COLI},
  author       = {Pawar, Siddhesh and Park, Junyeong and Jin, Jiho and Arora, Arnav and Myung, Junho and Yadav, Srishti and Haznitrama, Faiz Ghifari and Song, Inhwa and Oh, Alice and Augenstein, Isabelle},
  doi          = {10.1162/COLI.a.14},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {907-1004},
  shortjournal = {Comput. Lingu.},
  title        = {Survey of cultural awareness in language models: Text and beyond},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models are biased because they are large language models. <em>COLI</em>, <em>51</em>(3), 885-906. (<a href='https://doi.org/10.1162/coli_a_00558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This position paper’s primary goal is to provoke thoughtful discussion about the relationship between bias and fundamental properties of large language models (LLMs). I do this by seeking to convince the reader that harmful biases are an inevitable consequence arising from the design of any large language model as LLMs are currently formulated. To the extent that this is true, it suggests that the problem of harmful bias cannot be properly addressed without a serious reconsideration of AI driven by LLMs, going back to the foundational assumptions underlying their design.},
  archive      = {J_COLI},
  author       = {Resnik, Philip},
  doi          = {10.1162/coli_a_00558},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {885-906},
  shortjournal = {Comput. Lingu.},
  title        = {Large language models are biased because they are large language models},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics. <em>COLI</em>, <em>51</em>(3), 843-883. (<a href='https://doi.org/10.1162/coli_a_00543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate two essential challenges in the context of hierarchical topic modeling (HTM)—(i) the impact of data representation and (ii) topic evaluation. The data representation directly influences the performance of the topic generation, and the impact of new representations such as contextual embeddings in this task has been under-investigated. Topic evaluation , responsible for driving the advances in the field, assesses the overall quality of the topic generation process. HTM studies exploit the exact topic modeling (TM) evaluation metrics as traditional TM to measure the quality of topics. One significant result of our work is demonstrating that the HTM’s hierarchical nature demands novel ways of evaluating the quality of topics. As our main contribution, we propose two new topic quality metrics to assess the topical quality of the hierarchical structures. Uniqueness considers topic topological consistency, while the Semantic Hierarchical Structure (SHS) captures the semantic relatedness of the hierarchies. We also present an additional advance to the state-of-the-art by proposing the c-CluHTM. To the best of our knowledge, c-CluHTM is the first method that exploits contextual embeddings into NMF in HTM tasks. c-CluHTM enhances the topics’ semantics while preserving the hierarchical structure. We perform an experimental evaluation, and our results demonstrate the superiority of our proposal with gains between 12% and 21%, regarding NPMI and Coherence over the best baselines. Regarding the newly proposed metrics, our results reveal that Uniqueness and SHS can capture relevant information about the structure of the hierarchical topics that traditional metrics cannot.},
  archive      = {J_COLI},
  author       = {Viegas, Felipe and Pereira, Antonio and Cunha, Washington and França, Celso and Andrade, Claudio and Tuler, Elisa and Rocha, Leonardo and Gonçalves, Marcos André},
  doi          = {10.1162/coli_a_00543},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {843-883},
  shortjournal = {Comput. Lingu.},
  title        = {Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The emergence of chunking structures with hierarchical RNN. <em>COLI</em>, <em>51</em>(3), 815-841. (<a href='https://doi.org/10.1162/coli_a_00545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Natural Language Processing (NLP), predicting linguistic structures, such as parsing and chunking, has mostly relied on manual annotations of syntactic structures. This article introduces an unsupervised approach to chunking, a syntactic task that involves grouping words in a non-hierarchical manner. We present a Hierarchical Recurrent Neural Network (HRNN) designed to model word-to-chunk and chunk-to-sentence compositions. Our approach involves a two-stage training process: pretraining with an unsupervised parser and finetuning on downstream NLP tasks. Experiments on multiple datasets reveal a notable improvement of unsupervised chunking performance in both pretraining and finetuning stages. Interestingly, we observe that the emergence of the chunking structure is transient during the neural model’s downstream-task training. This study contributes to the advancement of unsupervised syntactic structure discovery and opens avenues for further research in linguistic theory. 1},
  archive      = {J_COLI},
  author       = {Wu, Zijun and Deshmukh, Anup Anand and Wu, Yongkang and Lin, Jimmy and Mou, Lili},
  doi          = {10.1162/coli_a_00545},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {815-841},
  shortjournal = {Comput. Lingu.},
  title        = {The emergence of chunking structures with hierarchical RNN},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tokenization changes meaning in large language models: Evidence from chinese. <em>COLI</em>, <em>51</em>(3), 785-814. (<a href='https://doi.org/10.1162/coli_a_00557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models segment many words into multiple tokens, and there is mixed evidence as to whether tokenization affects how state-of-the-art models represent meanings. Chinese characters present an opportunity to investigate this issue: They contain semantic radicals, which often convey useful information; characters with the same semantic radical tend to begin with the same one or two bytes (when using UTF-8 encodings); and tokens are common strings of bytes, so characters with the same radical often begin with the same token. This study asked GPT-4, GPT-4o, and Llama 3 whether characters contain the same semantic radical, elicited semantic similarity ratings, and conducted odd-one-out tasks (i.e., which character is not like the others). In all cases, misalignment between tokens and radicals systematically corrupted representations of Chinese characters. In experiments comparing characters represented by single tokens to multi-token characters, the models were less accurate for single-token characters, which suggests that segmenting words into fewer, longer tokens obscures valuable information in word form and will not resolve the problems introduced by tokenization. In experiments with 12 European languages, misalignment between tokens and suffixes systematically corrupted categorization of words by all three models, which suggests that the tendency to treat malformed tokens like linguistic units is pervasive.},
  archive      = {J_COLI},
  author       = {Haslett, David A.},
  doi          = {10.1162/coli_a_00557},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {785-814},
  shortjournal = {Comput. Lingu.},
  title        = {Tokenization changes meaning in large language models: Evidence from chinese},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniASA: A unified generative framework for argument structure analysis. <em>COLI</em>, <em>51</em>(3), 739-784. (<a href='https://doi.org/10.1162/coli_a_00553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation is a fundamental human activity that involves reasoning and persuasion, which also serves as the basis for the development of AI systems capable of complex reasoning. In NLP, to better understand human argumentation, argument structure analysis aims to identify argument components, such as claims and premises, and their relations from free text. It encompasses a variety of divergent tasks, such as end-to-end argument mining, argument pair extraction, and argument quadruplet extraction. Existing methods are usually tailored to only one specific argument structure analysis task, overlooking the inherent connections among different tasks. We observe that the fundamental goal of these tasks is similar: identifying argument components and their interrelations. Motivated by this, we present a unified generative framework for argument structure analysis (UniASA). It can uniformly address multiple argument structure analysis tasks in a sequence-to-sequence manner. Further, we enhance UniASA with a multi-view learning strategy based on subtask decomposition. We conduct experiments on seven datasets across three tasks. The results indicate that UniASA can address these tasks uniformly and achieve performance that is either superior to or comparable with the previous state-of-the-art methods. Also, we show that UniASA can be effectively integrated with large language models, such as Llama, through fine-tuning or in-context learning.},
  archive      = {J_COLI},
  author       = {Bao, Jianzhu and Jing, Mohan and Dong, Kuicai and Sun, Aixin and Sun, Yang and Xu, Ruifeng},
  doi          = {10.1162/coli_a_00553},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {739-784},
  shortjournal = {Comput. Lingu.},
  title        = {UniASA: A unified generative framework for argument structure analysis},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graded suspiciousness of adversarial texts to humans. <em>COLI</em>, <em>51</em>(3), 705-738. (<a href='https://doi.org/10.1162/coli_a_00555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples pose a significant challenge to deep neural networks across both image and text domains, with the intent to degrade model performance through carefully altered inputs. Adversarial texts, however, are distinct from adversarial images due to their requirement for semantic similarity and the discrete nature of the textual contents. This study delves into the concept of human suspiciousness, a quality distinct from the traditional focus on imperceptibility found in image-based adversarial examples, where adversarial changes are often desired to be indistinguishable to the human eye even when placed side by side with originals. Although this is generally not possible with text, textual adversarial content must still often remain undetected or non-suspicious to human readers. Even when the text’s purpose is to deceive NLP systems or bypass filters, the text is often expected to be natural to read. In this research, we expand the study of human suspiciousness by analyzing how individuals perceive adversarial texts. We gather and publish a novel dataset of Likert-scale human evaluations on the suspiciousness of adversarial sentences, crafted by four widely used adversarial attack methods and assess their correlation with the human ability to detect machine-generated alterations. Additionally, we develop a regression-based model to predict levels of suspiciousness and establish a baseline for future research in reducing the suspiciousness in adversarial text generation. We also demonstrate how the regressor-generated suspicious scores can be incorporated into adversarial generation methods to produce texts that are less likely to be perceived as computer-generated.},
  archive      = {J_COLI},
  author       = {Tonni, Shakila Mahjabin and Faustini, Pedro and Dras, Mark},
  doi          = {10.1162/coli_a_00555},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {705-738},
  shortjournal = {Comput. Lingu.},
  title        = {Graded suspiciousness of adversarial texts to humans},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ecj">ECJ - 5</h2>
<ul>
<li><details>
<summary>
(2025). On the use of the doubly stochastic matrix models for the quadratic assignment problem. <em>ECJ</em>, <em>33</em>(3), 425-457. (<a href='https://doi.org/10.1162/evco_a_00369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Permutation problems have captured the attention of the combinatorial optimization community for decades due to the challenge they pose. Although their solutions are naturally encoded as permutations, in each problem, the information to be used to optimize them can vary substantially. In this paper, we consider the Quadratic Assignment Problem (QAP) as a case study, and propose using Doubly Stochastic Matrices (DSMs) under the framework of Estimation of Distribution Algorithms. To that end, we design efficient learning and sampling schemes that enable an effective iterative update of the probability model. Conducted experiments on commonly adopted benchmarks for the QAP prove doubly stochastic matrices to be preferred to the other four models for permutations, both in terms of effectiveness and computational efficiency. Moreover, additional analyses performed on the structure of the QAP and the Linear Ordering Problem (LOP) show that DSMs are good to deal with assignment problems, but they have interesting capabilities to deal also with ordering problems such as the LOP. The paper concludes with a description of the potential uses of DSMs for other optimization paradigms, such as genetic algorithms or model-based gradient search.},
  archive      = {J_ECJ},
  author       = {Santucci, Valentino and Ceberio, Josu},
  doi          = {10.1162/evco_a_00369},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {425-457},
  shortjournal = {Evol. Comput.},
  title        = {On the use of the doubly stochastic matrix models for the quadratic assignment problem},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P-NP instance decomposition based on the fourier transform for solving the linear ordering problem. <em>ECJ</em>, <em>33</em>(3), 395-423. (<a href='https://doi.org/10.1162/evco_a_00368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fourier transform over finite groups has proved to be a useful tool for analyzing combinatorial optimization problems. However, few heuristic and metaheuristic algorithms have been proposed in the literature that utilize the information provided by this technique to guide the search process. In this work, we attempt to address this research gap by considering the case study of the Linear Ordering Problem (LOP). Based on the Fourier transform, we propose an instance decomposition strategy that divides any LOP instance into the sum of two LOP instances associated with a P and an NP-Hard optimization problem. By linearly aggregating the instances obtained from the decomposition, it is possible to create artificial instances with modified proportions of the P and NP-Hard components. Conducted experiments show that increasing the weight of the P component leads to a less rugged fitness landscape suitable for local search-based optimization. We take advantage of this phenomenon by presenting a new metaheuristic algorithm called P-Descent Search (PDS). The proposed method, first, optimizes a surrogate instance with a high proportion of the P component, and then, gradually increases the weight of the NP-Hard component until the original instance is reached. The multi-start version of PDS shows a promising and predictable performance that appears to be correlated to specific characteristics of the problem, which could open the door to an automatic tuning of its hyperparameters.},
  archive      = {J_ECJ},
  author       = {Benavides, Xabier and Hernando, Leticia and Ceberio, Josu and Lozano, Jose A.},
  doi          = {10.1162/evco_a_00368},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {395-423},
  shortjournal = {Evol. Comput.},
  title        = {P-NP instance decomposition based on the fourier transform for solving the linear ordering problem},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing monotone chance-constrained submodular functions using evolutionary multiobjective algorithms. <em>ECJ</em>, <em>33</em>(3), 363-393. (<a href='https://doi.org/10.1162/evco_a_00360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world optimization problems can be stated in terms of submodular functions. Furthermore, these real-world problems often involve uncertainties which may lead to the violation of given constraints. A lot of evolutionary multiobjective algorithms following the Pareto optimization approach have recently been analyzed and applied to submodular problems with different types of constraints. We present a first runtime analysis of evolutionary multiobjective algorithms based on Pareto optimization for chance-constrained submodular functions. Here the constraint involves stochastic components and the constraint can only be violated with a small probability of α ⁠ . We investigate the classical GSEMO algorithm for two different bi-objective formulations using tail bounds to determine the feasibility of solutions. We show that the algorithm GSEMO obtains the same worst case performance guarantees for monotone submodular functions as recently analyzed greedy algorithms for the case of uniform IID weights and uniformly distributed weights with the same dispersion when using the appropriate bi-objective formulation. As part of our investigations, we also point out situations where the use of tail bounds in the first bi-objective formulation can prevent GSEMO from obtaining good solutions in the case of uniformly distributed weights with the same dispersion if the objective function is submodular but non-monotone due to a single element impacting monotonicity. Furthermore, we investigate the behavior of the evolutionary multiobjective algorithms GSEMO, NSGA-II, and SPEA2 on different submodular chance-constrained network problems. Our experimental results show that the use of evolutionary multiobjective algorithms leads to significant performance improvements compared to state-of-the-art greedy algorithms for submodular optimization.},
  archive      = {J_ECJ},
  author       = {Neumann, Aneta and Neumann, Frank},
  doi          = {10.1162/evco_a_00360},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {363-393},
  shortjournal = {Evol. Comput.},
  title        = {Optimizing monotone chance-constrained submodular functions using evolutionary multiobjective algorithms},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Genetic programming for automatically evolving multiple features to classification. <em>ECJ</em>, <em>33</em>(3), 335-362. (<a href='https://doi.org/10.1162/evco_a_00359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing classification on high-dimensional data poses a significant challenge due to the huge search space. Moreover, complex feature interactions introduce an additional obstacle. The problems can be addressed by using feature selection to select relevant features or feature construction to construct a small set of high-level features. However, performing feature selection or feature construction might only make the feature set suboptimal. To remedy this problem, this study investigates the use of genetic programming for simultaneous feature selection and feature construction in addressing different classification tasks. The proposed approach is tested on 16 datasets and compared with seven methods including both feature selection and feature construction techniques. The results show that the obtained feature sets with the constructed and/or selected features can significantly increase the classification accuracy and reduce the dimensionality of the datasets. Further analysis reveals the complementarity of the obtained features leading to the promising classification performance of the proposed method.},
  archive      = {J_ECJ},
  author       = {Wang, Peng and Xue, Bing and Liang, Jing and Zhang, Mengjie},
  doi          = {10.1162/evco_a_00359},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {335-362},
  shortjournal = {Evol. Comput.},
  title        = {Genetic programming for automatically evolving multiple features to classification},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogates of scalarization functions. <em>ECJ</em>, <em>33</em>(3), 309-334. (<a href='https://doi.org/10.1162/evco_a_00354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, computationally intensive multiobjective optimization problems have been efficiently solved by surrogate-assisted multiobjective evolutionary algorithms. However, most of those algorithms can handle no more than 200 decision variables. As the number of decision variables increases further, unreliable surrogate models will result in a dramatic deterioration of their performance, which makes large-scale expensive multiobjective optimization challenging. To address this challenge, we develop a large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogate models of scalarization functions. The proposed algorithm (termed LDS-AF) reduces the dimension of the original decision space based on principal component analysis, and then directly approximates the scalarization functions in a decomposition-based multiobjective evolutionary algorithm. With the help of a two-stage modeling strategy and convergence control strategy, LDS-AF can keep a good balance between convergence and diversity, and achieve a promising performance without being trapped in a local optimum prematurely. The experimental results on a set of test instances have demonstrated its superiority over eight state-of-the-art algorithms on multiobjective optimization problems with up to 1,000 decision variables using only 500 real function evaluations.},
  archive      = {J_ECJ},
  author       = {Gu, Haoran and Wang, Handing and He, Cheng and Yuan, Bo and Jin, Yaochu},
  doi          = {10.1162/evco_a_00354},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {309-334},
  shortjournal = {Evol. Comput.},
  title        = {Large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogates of scalarization functions},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jmlr">JMLR - 1</h2>
<ul>
<li><details>
<summary>
(2025). Linear separation capacity of self-supervised representation learning. <em>JMLR</em>, <em>26</em>(194), 1-48. (<a href='https://jmlr.org/papers/v26/24-2032.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. Our theoretical analysis further underscores that the performance of downstream linear classifiers primarily hinges on the linear separability of data representations rather than the size of the labeled data set, reaffirming the viability of constructing efficient classifiers with limited labeled data amid an expansive unlabeled data set.},
  archive      = {J_JMLR},
  author       = {Shulei Wang},
  journal      = {Journal of Machine Learning Research},
  number       = {194},
  pages        = {1-48},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Linear separation capacity of self-supervised representation learning},
  url          = {https://jmlr.org/papers/v26/24-2032.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="neco">NECO - 4</h2>
<ul>
<li><details>
<summary>
(2025). Feature normalization prevents collapse of noncontrastive learning dynamics. <em>NECO</em>, <em>37</em>(11), 2079-2124. (<a href='https://doi.org/10.1162/neco.a.27'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning is a self-supervised representation learning framework where two positive views generated through data augmentation are made similar by an attraction force in a data representation space, while a repulsive force makes them far from negative examples. Noncontrastive learning, represented by BYOL and SimSiam, gets rid of negative examples and improves computational efficiency. While learned representations may collapse into a single point due to the lack of the repulsive force at first sight, Tian et al. ( 2021 ) revealed through learning dynamics analysis that the representations can avoid collapse if data augmentation is sufficiently stronger than regularization. However, their analysis does not take into account commonly used feature normalization, a normalizer before measuring the similarity of representations, and hence excessively strong regularization may still collapse the dynamics, an unnatural behavior under the presence of feature normalization. Therefore, we extend the previous theory based on the L2 loss by considering the cosine loss instead, which involves feature normalization. We show that the cosine loss induces sixth-order dynamics (while the L2 loss induces a third-order one), in which a stable equilibrium dynamically emerges even if there are only collapsed solutions with given initial parameters. Thus, we offer a new understanding that feature normalization plays an important role in robustly preventing the dynamics collapse.},
  archive      = {J_NECO},
  author       = {Bao, Han},
  doi          = {10.1162/neco.a.27},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2079-2124},
  shortjournal = {Neural Comput.},
  title        = {Feature normalization prevents collapse of noncontrastive learning dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling higher-order interactions in sparse and heavy-tailed neural population activity. <em>NECO</em>, <em>37</em>(11), 2011-2078. (<a href='https://doi.org/10.1162/neco.a.35'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons process sensory stimuli efficiently, showing sparse yet highly variable ensemble spiking activity involving structured higher-order interactions. Notably, while neural populations are mostly silent, they occasionally exhibit highly synchronous activity, resulting in sparse and heavy-tailed spike-count distributions. However, its mechanistic origin—specifically, what types of nonlinear properties in individual neurons induce such population-level patterns—remains unclear. In this study, we derive sufficient conditions under which the joint activity of homogeneous binary neurons generates sparse and widespread population firing rate distributions in infinitely large networks. We then propose a subclass of exponential family distributions that satisfy this condition. This class incorporates structured higher-order interactions with alternating signs and shrinking magnitudes, along with a base-measure function that offsets distributional concentration, giving rise to parameter-dependent sparsity and heavy-tailed population firing rate distributions. Analysis of recurrent neural networks that recapitulate these distributions reveals that individual neurons possess threshold-like nonlinearity, followed by supralinear activation that jointly facilitates sparse and synchronous population activity. These nonlinear features resemble those in modern Hopfield networks, suggesting a connection between widespread population activity and the network’s memory capacity. The theory establishes sparse and heavy-tailed distributions for binary patterns, forming a foundation for developing energy-efficient spike-based learning machines.},
  archive      = {J_NECO},
  author       = {Rodríguez-Domínguez, Ulises and Shimazaki, Hideaki},
  doi          = {10.1162/neco.a.35},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2011-2078},
  shortjournal = {Neural Comput.},
  title        = {Modeling higher-order interactions in sparse and heavy-tailed neural population activity},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Encoding of numerosity with robustness to object and scene identity in biologically inspired object recognition networks. <em>NECO</em>, <em>37</em>(11), 1975-2010. (<a href='https://doi.org/10.1162/neco.a.30'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Number sense, the ability to rapidly estimate object quantities in a visual scene without precise counting, is a crucial cognitive capacity found in humans and many other animals. Recent studies have identified artificial neurons tuned to numbers of items in biologically inspired vision models, even before training, and proposed these artificial neural networks as candidate models for the emergence of number sense in the brain. But real-world numerosity perception requires abstraction from the properties of individual objects and their contexts, unlike the simplified dot patterns used in previous studies. Using novel synthetically generated photorealistic stimuli, we show that deep convolutional neural networks optimized for object recognition encode information on approximate numerosity across diverse objects and scene types, which could be linearly read out from distributed activity patterns of later convolutional layers of different network architectures tested. In contrast, untrained networks with random weights failed to represent numerosity with abstractness to other visual properties and instead captured mainly low-level visual features. Our findings emphasize the importance of using complex, naturalistic stimuli to investigate mechanisms of number sense in both biological and artificial systems, and they suggest that the capacity of untrained networks to account for early-life numerical abilities should be reassessed. They further point to a possible, so far underappreciated, contribution of the brain's ventral visual pathway to representing numerosity with abstractness to other high-level visual properties.},
  archive      = {J_NECO},
  author       = {Chapalain, Thomas and Thirion, Bertrand and Eger, Evelyn},
  doi          = {10.1162/neco.a.30},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {1975-2010},
  shortjournal = {Neural Comput.},
  title        = {Encoding of numerosity with robustness to object and scene identity in biologically inspired object recognition networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A chimera model for motion anticipation in the retina and the primary visual cortex. <em>NECO</em>, <em>37</em>(11), 1925-1974. (<a href='https://doi.org/10.1162/neco.a.34'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a mean field model of the primary visual cortex (V1), connected to a realistic retina model, to study the impact of the retina on motion anticipation. We first consider the case where the retina does not itself provide anticipation—which is then only triggered by a cortical mechanism, the “anticipation by latency”—and unravel the effects of the retinal input amplitude, of stimulus features such as speed and contrast and of the size of cortical extensions and fiber conduction speed. Then we explore the changes in the cortical wave of anticipation when V1 is triggered by retina-driven anticipatory mechanisms: gain control and lateral inhibition by amacrine cells. Here, we show how retinal and cortical anticipation combine to provide an efficient processing where the simulated cortical response is in advance over the moving object that triggers this response, compensating the delays in visual processing.},
  archive      = {J_NECO},
  author       = {Emonet, Jérôme and Souihel, Selma and Chavane, Frédéric and Destexhe, Alain and Volo, Matteo di and Cessac, Bruno},
  doi          = {10.1162/neco.a.34},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {1925-1974},
  shortjournal = {Neural Comput.},
  title        = {A chimera model for motion anticipation in the retina and the primary visual cortex},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="netn">NETN - 6</h2>
<ul>
<li><details>
<summary>
(2025). Reproducibility of resting-state functional connectivity in healthy aging and brain injury: A mini-multiverse analysis. <em>NETN</em>, <em>9</em>(3), 1154-1175. (<a href='https://doi.org/10.1162/netn_a_00459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resting-state functional connectivity (RSFC) methods are the most widely applied tools in the network neurosciences, but their reliability remains an active area of study. We use back-to-back 10-min resting-state scans in a healthy aging ( n = 41) and traumatic brain injury (TBI) sample ( n = 45) composed of older adults to assess the replicability of RSFC using a “mini” multiverse approach. The goal was to evaluate the reproducibility of commonly used graph metrics and determine if aging and moderate-severe TBI influences RSFC reliability using intraclass correlation coefficients (ICCs). There is clear evidence for reliable results in aging and TBI. Global network metrics such as within-network connectivity and segregation were most reliable whereas other whole-brain connectivity estimates (e.g., clustering coefficient, eigenvector centrality) were least reliable. Analysis of canonical networks revealed the default mode and salience networks as most reliable. There was a notable influence of motion scrubbing on ICCs, with diminished reliability proportional to the number of volumes removed. Choice of brain atlas had a modest effect on findings. Overall, RSFC reproducibility is preserved in older adults and after significant neurological compromise. We also identify a subset of graph metrics and canonical networks with promising reliability. In this paper, we examine the reproducibility of resting-state functional connectivity in healthy aging and traumatic brain injury (TBI). We use a mini-verse approach to determine if workflows have a significant effect on resting-state functional connectivity (RSFC) reliability. While the study of RSFC reliability has been previously examined (e.g., in healthy, young adults), it is lacking in the aging and TBI literatures. To our knowledge, this is the first study of back-to-back (consecutive) scans that examine RSFC reliability in healthy aging and TBI. In brief, these data and these analyses do not currently exist in the literature. RSFC reliability remains a vital area of investigation in healthy aging and clinical samples. We believe there are critical contributions that can be made by this paper.},
  archive      = {J_NETN},
  author       = {Mullin, Hollie A. and Carpenter, Catherine M. and Cwiek, Andrew P. and Lan, Gloria and Chase, Spencer O. and Carter, Emily E. and Vervoordt, Samantha M. and Rabinowitz, Amanda and Venkatesan, Umesh and Hillary, Frank G.},
  doi          = {10.1162/netn_a_00459},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1154-1175},
  shortjournal = {Netw. Neuroscience},
  title        = {Reproducibility of resting-state functional connectivity in healthy aging and brain injury: A mini-multiverse analysis},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing dynamic functional connectivity subnetwork contributions in narrative classification with shapley values. <em>NETN</em>, <em>9</em>(3), 1138-1153. (<a href='https://doi.org/10.1162/NETN.a.25'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional connectivity derived from functional magnetic resonance imaging (fMRI) data has been increasingly used to study brain activity. In this study, we model brain dynamic functional connectivity during narrative tasks as a temporal brain network and employ a machine learning model to classify in a supervised setting the modality (audio, movie), the content (airport, restaurant situations) of narratives, and both combined. Leveraging Shapley values, we analyze subnetwork contributions within Yeo parcellations (7- and 17-subnetworks) to explore their involvement in narrative modality and comprehension. This work represents the first application of this approach to functional aspects of the brain, validated by existing literature, and provides novel insights at the whole-brain level. Our findings suggest that schematic representations in narratives may not depend solely on preexisting knowledge of the top-down process to guide perception and understanding, but may also emerge from a bottom-up process driven by the temporal parietal subnetwork. This study investigates how different brain subnetworks contribute to processing narratives. We used a machine learning model to analyze fMRI data from participants listening to or watching narratives that varied in modality (audio or movie) and thematic content (airport or restaurant). Our model accurately classified these different narrative aspects, and by using Shapley values, we identified the subnetworks most crucial for each classification. Consistent with existing neuroscience knowledge, our findings highlight the distinct roles of these subnetworks in narrative comprehension. This study provides a powerful approach for investigating brain function across various domains.},
  archive      = {J_NETN},
  author       = {Rossi, Aurora and Aeschlimann, Yanis and Natale, Emanuele and Deslauriers-Gauthier, Samuel and Dominey, Peter Ford},
  doi          = {10.1162/NETN.a.25},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1138-1153},
  shortjournal = {Netw. Neuroscience},
  title        = {Characterizing dynamic functional connectivity subnetwork contributions in narrative classification with shapley values},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-guided generative adversarial networks for synthesizing and augmenting structural connectivity matrices for connectivity-based prediction. <em>NETN</em>, <em>9</em>(3), 1110-1137. (<a href='https://doi.org/10.1162/NETN.a.24'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent machine learning techniques have improved connectome-based predictions by modeling complex dependencies between brain connectivity and cognitive traits. However, they typically require large datasets that are costly and time-consuming to collect. To address this, we propose Task-guided generative adversarial network (GAN) II, a novel data augmentation method that uses GANs to expand sample sizes in connectome-based prediction tasks. Our method incorporates a task-guided branch within the Wasserstein GAN framework, specifically designed to synthesize structural connectivity matrices and improve prediction accuracy by capturing task-relevant features. We evaluated Task-guided GAN II on the prediction of fluid intelligence using the NIMH Health Research Volunteer Dataset. Results showed that data augmentation improved prediction accuracy. To further assess whether augmentation can substitute for increasing actual collected sample sizes, we conducted additional validation using the Human Connectome Project WU-Minn S1200 dataset. Task-guided GAN II improved prediction performance with limited real data, with gains of up to twofold augmentation observed. However, excessive augmentation did not result in further improvements, suggesting that augmentation complements, but does not fully replace, real data augmentation. These results suggest that Task-guided GAN II is a promising tool for harnessing small datasets in human connectomics research, improving predictive modeling where large-scale data collection is impractical.},
  archive      = {J_NETN},
  author       = {Yamamoto, Tatsuya and Sugiura, Tomoki and Hiroyasu, Tomoyuki and Hiwa, Satoru},
  doi          = {10.1162/NETN.a.24},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1110-1137},
  shortjournal = {Netw. Neuroscience},
  title        = {Task-guided generative adversarial networks for synthesizing and augmenting structural connectivity matrices for connectivity-based prediction},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stable brain PET metabolic networks using a multiple sampling scheme. <em>NETN</em>, <em>9</em>(3), 1087-1109. (<a href='https://doi.org/10.1162/NETN.a.23'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interregional communication within the human brain is essential for maintaining functional integrity. A promising approach for investigating how brain regions communicate relies on the assumption that the brain operates as a complex network. In this context, positron emission tomography (PET) images have been suggested as a valuable source for understanding brain networks. However, such networks are typically assembled through direct computation without accounting for outliers, impacting the reliability of group representative networks. In this study, we used brain [ 18 F]fluoro-2-deoxyglucose PET data from 1,227 individuals in the Alzheimer’s disease (AD) continuum from the Alzheimer’s Disease Neuroimaging Initiative cohort to develop a novel method for constructing stable metabolic brain networks that are resilient to spurious data points. Our multiple sampling scheme generates brain networks with greater stability compared with conventional approaches. The proposed method is robust to imbalanced datasets and requires 50% fewer subjects to achieve stability than the conventional method. We further validated the approach in an independent AD cohort ( n = 114) from São Paulo, Brazil (Faculdade de Medicina da Universidade de São Paulo). This innovative method is flexible and improves the robustness of metabolic brain network analyses, supporting better insights into brain connectivity and resilience to data variability across multiple radiotracers for both health and disease.},
  archive      = {J_NETN},
  author       = {Schu, Guilherme and Limberger, Christian and Brum, Wagner S. and De Bastiani, Marco Antônio and Rodrigues, Yuri Elias and de Azeredo, Julio Cesar and Pascoal, Tharick A. and Benedet, Andrea L. and Mathotaarachchi, Sulantha and Rosa-Neto, Pedro and Almeida, Jorge and de Paula Faria, Daniele and de Souza Duran, Fábio Luiz and Buchpiguel, Carlos Alberto and Coutinho, Artur Martins and Busatto, Geraldo F. and Zimmer, Eduardo R. and for the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1162/NETN.a.23},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1087-1109},
  shortjournal = {Netw. Neuroscience},
  title        = {Stable brain PET metabolic networks using a multiple sampling scheme},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial correlation as a tool for mapping functional-structural correspondence in human brain connectivity. <em>NETN</em>, <em>9</em>(3), 1065-1086. (<a href='https://doi.org/10.1162/NETN.a.22'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain structure-function coupling has been studied in health and disease by many different researchers in recent years. Most of the studies have estimated functional connectivity matrices as correlation coefficients between different brain areas, despite well-known disadvantages compared with partial correlation connectivity matrices. Indeed, partial correlation represents a more sensible model for structural connectivity since, under a Gaussian approximation, it accounts only for direct dependencies between brain areas. Motivated by this and following previous results by different authors, we investigate structure-function coupling using partial correlation matrices of functional magnetic resonance imaging brain activity time series under various regularization (also known as noise-cleaning) algorithms. We find that, across different algorithms and conditions, partial correlation provides a higher match with structural connectivity retrieved from density-weighted imaging data than standard correlation, and this occurs at both subject and population levels. Importantly, we also show that regularization and thresholding are crucial for this match to emerge. Finally, we assess neurogenetic associations in relation to structure-function coupling, which presents promising opportunities to further advance research in the field of network neuroscience, particularly concerning brain disorders. A precise understanding of how brain structure and function interact is fundamentally relevant to understanding disease. For the functional representation, most of the previous research has used correlation methods, which have limitations. Our study explores a different approach called partial correlation methods, which more accurately reflect the brain’s direct connections. We found that partial correlation aligns better with the brain’s structural connectivity than standard methods, both in individuals and groups. Additionally, we identified promising links between brain connectivity and genetics, offering new insights into brain disorders. Our work highlights the importance of using advanced connectivity methods to improve our understanding of the brain’s structure-function relationship, paving the way for future research in brain health and disease.},
  archive      = {J_NETN},
  author       = {Santucci, Francesca and Jimenez-Marin, Antonio and Gabrielli, Andrea and Bonifazi, Paolo and Ibáñez-Berganza, Miguel and Gili, Tommaso and Cortes, Jesus M.},
  doi          = {10.1162/NETN.a.22},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1065-1086},
  shortjournal = {Netw. Neuroscience},
  title        = {Partial correlation as a tool for mapping functional-structural correspondence in human brain connectivity},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic fluctuations of intrinsic brain activity are associated with consistent topological patterns in puberty and are biomarkers of neural maturation. <em>NETN</em>, <em>9</em>(3), 1039-1064. (<a href='https://doi.org/10.1162/netn_a_00452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic brain dynamics play a fundamental role in cognitive function, but their development is incompletely understood. We investigated pubertal changes in temporal fluctuations of intrinsic network topologies (focusing on the strongest connections and coordination patterns) and signals, in an early longitudinal sample from the Adolescent Brain Cognitive Development (ABCD) study, with resting-state fMRI ( n = 4,099 at baseline; n = 3,376 at follow-up [median age = 10.0 (1.1) and 12.0 (1.1) years; n = 2,116 with both assessments]). Reproducible, inverse associations between low-frequency signal and topological fluctuations were estimated ( p < 0.05, β = −0.20 to −0.02, 95% confidence interval (CI) = [−0.23, −0.001]). Signal (but not topological) fluctuations increased in somatomotor and prefrontal areas with pubertal stage ( p < 0.03, β = 0.06–0.07, 95% CI = [0.03, 0.11]), but decreased in orbitofrontal, insular, and cingulate cortices, as well as cerebellum, hippocampus, amygdala, and thalamus ( p < 0.05, β = −0.09 to −0.03, 95% CI = [−0.15, −0.001]). Higher temporal signal and topological variability in spatially distributed regions were estimated in girls. In racial/ethnic minorities, several associations between signal and topological fluctuations were in the opposite direction of those in the entire sample, suggesting potential racial differences. Our findings indicate that during puberty, intrinsic signal dynamics change significantly in developed and developing brain regions, but their strongest coordination patterns may already be sufficiently developed and remain temporally consistent. We have investigated pubertal changes in intrinsic signal and network dynamics, estimated from resting-state fMRI in a sample of youth from the ABCD study. We have identified reproducible, inverse associations between low-frequency signal and topological fluctuations, as well as pubertal changes in intrinsic signal dynamics but not topological patterns of strongly connected networks. We have also identified sex differences in these dynamics and negative associations with body mass index (BMI). Several associations between signal and topological fluctuations were in the opposite direction in racial/ethnic minorities compared with those in the entire sample. Our findings indicate that intrinsic signal dynamics change significantly in developed and developing brain regions during puberty, but their strongest synchronization patterns may already be sufficiently developed prior to puberty and are dynamically reproducible.},
  archive      = {J_NETN},
  author       = {Lim, Jethro and Cooper, Kaitlynn and Stamoulis, Catherine},
  doi          = {10.1162/netn_a_00452},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1039-1064},
  shortjournal = {Netw. Neuroscience},
  title        = {Dynamic fluctuations of intrinsic brain activity are associated with consistent topological patterns in puberty and are biomarkers of neural maturation},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="tacl">TACL - 1</h2>
<ul>
<li><details>
<summary>
(2025). A systematic review of NLP for dementia: Tasks, datasets, and opportunities. <em>TACL</em>, <em>13</em>, 1204-1244. (<a href='https://doi.org/10.1162/TACL.a.35'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The close link between cognitive decline and language has fostered long-standing collaboration between the NLP and medical communities in dementia research. To examine this, we reviewed over 240 papers applying NLP to dementia-related efforts, drawing from medical, technological, and NLP-focused literature. We identify key research areas, including dementia detection, linguistic biomarker extraction, caregiver support, and patient assistance, showing that half of all papers focus solely on dementia detection using clinical data. Yet, many directions remain unexplored, such as artificially degraded language models, synthetic data, digital twins, and more. We highlight gaps and opportunities around trust, scientific rigor, applicability, and cross-community collaboration. We raise ethical dilemmas in the field, and highlight the diverse datasets encountered throughout our review including recorded, written, structured, spontaneous, synthetic, clinical, social media–based, and more. This review aims to inspire more creative, impactful, and rigorous research on NLP for dementia.},
  archive      = {J_TACL},
  author       = {Peled-Cohen, Lotem and Reichart, Roi},
  doi          = {10.1162/TACL.a.35},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {10},
  pages        = {1204-1244},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A systematic review of NLP for dementia: Tasks, datasets, and opportunities},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="tmlr">TMLR - 40</h2>
<ul>
<li><details>
<summary>
(2025). Contextual combinatorial bandits with changing action sets via gaussian processes. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=2RgfAY3jnI'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a contextual bandit problem with a combinatorial action set and time-varying base arm availability. At the beginning of each round, the agent observes the set of available base arms and their contexts and then selects an action that is a feasible subset of the set of available base arms to maximize its cumulative reward in the long run. We assume that the mean outcomes of base arms are samples from a Gaussian Process \rev{(GP)} indexed by the context set ${\cal X}$, and the expected reward is Lipschitz continuous in expected base arm outcomes. For this setup, we propose an algorithm called Optimistic Combinatorial Learning and Optimization with Kernel Upper Confidence Bounds (O'CLOK-UCB) and prove that it incurs $\tilde{O}(\sqrt{\lambda^*(K)KT\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)} )$ regret with high probability, where $\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)$ is the maximum information gain associated with the sets of base arm contexts $\mathcal{X}_t$ that appeared in the first $T$ rounds, $K$ is the maximum cardinality of any feasible action over all rounds, and $\lambda^*(K)$ is the maximum eigenvalue of all covariance matrices of selected actions up to time $T$, which is a function of $K$. To dramatically speed up the algorithm, we also propose a variant of O'CLOK-UCB that uses sparse GPs. Finally, we experimentally show that both algorithms exploit inter-base arm outcome correlation and vastly outperform the previous state-of-the-art UCB-based algorithms in realistic setups.},
  archive      = {J_TMLR},
  author       = {Andi Nika and Sepehr Elahi and Cem Tekin},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Contextual combinatorial bandits with changing action sets via gaussian processes},
  url          = {https://openreview.net/forum?id=2RgfAY3jnI},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial robustness of graph transformers. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=4xK0vjxTWL'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies have shown that Message-Passing Graph Neural Networks (MPNNs) are highly susceptible to adversarial attacks. In contrast, despite the increasing importance of Graph Transformers (GTs), their robustness properties are unexplored. We close this gap and design the first adaptive attacks for GTs. In particular, we provide general design principles for strong gradient-based attacks on GTs w.r.t. structure perturbations and instantiate our attack framework for five representative and popular GT architectures. Specifically, we study GTs with specialized attention mechanisms and Positional Encodings (PEs) based on pairwise shortest paths, random walks, and the Laplacian spectrum. We evaluate our attacks on multiple tasks and perturbation models, including structure perturbations for node and graph classification, and node injection for graph classification. Our results reveal that GTs can be catastrophically fragile in many cases. Addressing this vulnerability, we show how our adaptive attacks can be effectively used for adversarial training, substantially improving robustness.},
  archive      = {J_TMLR},
  author       = {Philipp Foth and Lukas Gosch and Simon Geisler and Leo Schwinn and Stephan Günnemann},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Adversarial robustness of graph transformers},
  url          = {https://openreview.net/forum?id=4xK0vjxTWL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting B2T: Discovering and mitigating visual biases through keyword explanations. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=5GS1q65pv6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to reproduce and extend the findings of "Discovering and Mitigating Visual Biases through Keyword Explanation" by Kim et al.(2024). The paper proposes the B2T framework, which detects and mitigates visual biases by extracting keywords from generated captions. By identifying biases in datasets, B2T contributes to the prevention of discriminatory behavior in vision-language models. We aim to investigate the five key claims from the original paper, namely that B2T (i) is able to identify whether a word represents a bias, (ii) can extract these keywords from captions of mispredicted images, (iii) outperforms other bias discovery models, (iv) can improve CLIP zero-shot prompting with the discovered keywords, and (v) identifies labeling errors in a dataset. To reproduce their results, we use the publicly available codebase and our re-implementations. Our findings confirm the first three claims and partially validate the fourth. We reject the fifth claim, due to the failure to identify pertinent labeling errors. Finally, we enhance the original work by optimizing the efficiency of the implementation, and assessing the generalizability of B2T on a new dataset.},
  archive      = {J_TMLR},
  author       = {Faissal El Kayouhi and Aïda Asma and Joey Laarhoven and Fiona Nagelhout},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Revisiting B2T: Discovering and mitigating visual biases through keyword explanations},
  url          = {https://openreview.net/forum?id=5GS1q65pv6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Capsule network projectors are equivariant and invariant learners. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=7owCO3qskH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning invariant representations has been the longstanding approach to self-supervised learning. However, recently progress has been made in preserving equivariant properties in representations, yet do so with highly prescribed architectures. In this work, we propose an invariant-equivariant self-supervised architecture that employs Capsule Networks (CapsNets), which have been shown to capture equivariance with respect to novel viewpoints. We demonstrate that the use of CapsNets in equivariant self-supervised architectures achieves improved downstream performance on equivariant tasks with higher efficiency and fewer network parameters. To accommodate the architectural changes of CapsNets, we introduce a new objective function based on entropy minimisation. This approach, which we name CapsIE (Capsule Invariant Equivariant Network), achieves state-of-the-art performance on the equivariant rotation tasks on the 3DIEBench dataset compared to prior equivariant SSL methods, while performing competitively against supervised counterparts. Our results demonstrate the ability of CapsNets to learn complex and generalised representations for large-scale, multi-task datasets compared to previous CapsNet benchmarks.},
  archive      = {J_TMLR},
  author       = {Miles Everett and Aiden Durrant and Mingjun Zhong and Georgios Leontidis},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Capsule network projectors are equivariant and invariant learners},
  url          = {https://openreview.net/forum?id=7owCO3qskH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent trajectory: A new framework for deep actor-critic reinforcement learning with uncertainty quantification. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=8B74xdaRHa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification in deep learning is challenging due to the complexity of deep neural networks. This challenge is particularly pronounced in deep reinforcement learning (RL), where agents interact with stochastic environments. In deep actor-critic RL, this challenge is further exacerbated due to the interdependence between the actor and critic updates. Existing uncertainty quantification methods for RL are predominantly developed within the Bayesian framework. While these methods estimate the uncertainty of the value function, their confidence intervals are often misleading, with the coverage rate frequently falling well below the nominal level. To address this issue, we introduce a novel deep RL framework that treats transition trajectories as latent variables. Leveraging this framework, we propose an adaptive Stochastic Gradient Markov Chain Monte Carlo algorithm to train deep actor-critic models, which naturally accounts for the interdependence between the actor and critic updates. We provide theoretical guarantees for the convergence of the proposed method and offer empirical evidence for its effectiveness in uncertainty quantification of the value function. The proposed latent trajectory framework is highly flexible, allowing for the integration of advanced RL strategies to further enhance deep actor-critic learning.},
  archive      = {J_TMLR},
  author       = {Frank Shih and Faming Liang},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Latent trajectory: A new framework for deep actor-critic reinforcement learning with uncertainty quantification},
  url          = {https://openreview.net/forum?id=8B74xdaRHa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D2 actor critic: Diffusion actor meets distributional critic. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=8KbstCUXhH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce D2AC, a new model-free reinforcement learning (RL) algorithm designed to train expressive diffusion policies online effectively. At its core is a policy improvement objective that avoids the high variance of typical policy gradients and the complexity of backpropagation through time. This stable learning process is critically enabled by our second contribution: a robust distributional critic, which we design through a fusion of distributional RL and clipped double Q-learning. The resulting algorithm is highly effective, achieving state-of-the-art performance on a benchmark of eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains, spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard benchmarks, we also evaluate a biologically motivated predator-prey task to examine the behavioral robustness and generalization capacity of our approach.},
  archive      = {J_TMLR},
  author       = {Lunjun Zhang and Shuo Han and Hanrui Lyu and Bradly C. Stadie},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {D2 actor critic: Diffusion actor meets distributional critic},
  url          = {https://openreview.net/forum?id=8KbstCUXhH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Illusion or algorithm? investigating memorization, emergence, and symbolic processing in in-context learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=10QqO1tM1H'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere "memorization" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.},
  archive      = {J_TMLR},
  author       = {Jingcheng Niu and Subhabrata Dutta and Ahmed Elshabrawy and Harish Tayyar Madabushi and Iryna Gurevych},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Illusion or algorithm? investigating memorization, emergence, and symbolic processing in in-context learning},
  url          = {https://openreview.net/forum?id=10QqO1tM1H},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blending adversarial training and representation-conditional purification via aggregation improves adversarial robustness. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=40BXthYscW'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a novel adversarial defence mechanism for image classification - CARSO - blending the paradigms of adversarial training and adversarial purification in a synergistic robustness-enhancing way. The method builds upon an adversarially-trained classifier, and learns to map its internal representation associated with a potentially perturbed input onto a distribution of tentative clean reconstructions. Multiple samples from such distribution are classified by the same adversarially-trained model, and a carefully chosen aggregation of its outputs finally constitutes the robust prediction of interest. Experimental evaluation by a well-established benchmark of strong adaptive attacks, across different image datasets, shows that CARSO is able to defend itself against adaptive end-to-end white-box attacks devised for stochastic defences. With a modest clean accuracy penalty, our method improves by a significant margin the state-of-the-art for Cifar-10, Cifar-100, and TinyImageNet-200 $\ell_\infty$ robust classification accuracy against AutoAttack.},
  archive      = {J_TMLR},
  author       = {Emanuele Ballarin and Alessio ansuini and Luca Bortolussi},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Blending adversarial training and representation-conditional purification via aggregation improves adversarial robustness},
  url          = {https://openreview.net/forum?id=40BXthYscW},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven discovery of PDEs via the adjoint method. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Az3mJ4d1eT'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form and formulate a PDE-constrained optimization problem aimed at minimizing the error of the PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations), allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, we consider a family of temporal parameterized PDEs that encompass linear, nonlinear, and spatial derivative candidate terms, and elegantly derive the corresponding adjoint equations. We show the efficacy of the proposed approach in identifying the form of the PDE up to machine accuracy, enabling the accurate discovery of PDEs from data. We also compare its performance with the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND \cite{rudy2017data} among others, on both smooth and noisy data sets. Even though the proposed adjoint method relies on forward/backward solvers, it outperforms PDE-FIND in the limit of large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter.},
  archive      = {J_TMLR},
  author       = {Mohsen Sadr and Tony Tohme and KAMAL YOUCEF-TOUMI},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Data-driven discovery of PDEs via the adjoint method},
  url          = {https://openreview.net/forum?id=Az3mJ4d1eT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal horizons in forecasting: A performance-learnability trade-off. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=BeudQIxT1R'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training autoregressive models to forecast dynamical systems, a critical question arises: how far into the future should the model be trained to predict for optimal performance? In this work, we address this question by analyzing the relationship between the geometry of the loss landscape and the training time horizon. Using dynamical systems theory, we prove that loss minima for long horizons generalize well to short-term forecasts, whereas minima found on short horizons result in worse long-term predictions. However, we also prove that the loss landscape becomes rougher as the training horizon grows, making long-horizon training inherently challenging. We validate our theory through numerical experiments and discuss practical implications for selecting training horizons. Our results provide a principled foundation for hyperparameter optimization in autoregressive forecasting models.},
  archive      = {J_TMLR},
  author       = {Pau Vilimelis Aceituno and Jack William Miller and Noah Marti and Youssef Farag and Victor Boussange},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Temporal horizons in forecasting: A performance-learnability trade-off},
  url          = {https://openreview.net/forum?id=BeudQIxT1R},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VSCoDe: Visual-augmentation selection for contrastive decoding. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=CqSyPc9W7Y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive performance of recent Large Vision-Language Models (LVLMs), these models often produce inaccurate responses. To address this issue, previous studies have aimed to reduce hallucinations by using contrastive decoding (CD) with modified images, such as cropping objects related to query or adding noise, thereby contrasting with the original image. However, these methods have several limitations. First, employing fixed visual augmentation, such as adding noise, is a simple approach but too rigid to contrast on various queries. Conversely, using semantics in queries or images by leveraging external models can adaptively generate contrastive images, but it entails significant additional costs. To address these shortcomings, we explore using pre-defined visual augmentations to enable flexible adaptation to each query without relying on external models. We observe that each query achieves different contrasts through different visual augmentations. Based on this, we propose a novel method called VSCoDe, Visual-Augmentation Selection for Contrastive Decoding, which adaptively selects augmentations using a proposed distance metric to identify those with higher contrast. Our empirical evaluations demonstrate that VSCoDe outperforms previous methods and enhances the quality of various vision-language tasks without additional training or reliance on external models.},
  archive      = {J_TMLR},
  author       = {Sihyeon Kim and Boryeong Cho and Sangmin Bae and Sumyeong Ahn and Se-Young Yun},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {VSCoDe: Visual-augmentation selection for contrastive decoding},
  url          = {https://openreview.net/forum?id=CqSyPc9W7Y},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized smooth stochastic variational inequalities: Almost sure convergence and convergence rates. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=EjqSpbUBWU'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on solving a stochastic variational inequality (SVI) problem under relaxed smoothness assumption for a class of structured non-monotone operators. The SVI problem has attracted significant interest in the machine learning community due to its immediate application to adversarial training and multi-agent reinforcement learning. In many such applications, the resulting operators do not satisfy the smoothness assumption. To address this issue, we focus on a weaker generalized smoothness assumption called $\alpha$-symmetric. Under $p$-quasi sharpness and $\alpha$-symmetric assumptions on the operator, we study clipped projection (gradient descent-ascent) and clipped Korpelevich (extragradient) methods. For these clipped methods, we provide the first almost-sure convergence results without making any assumptions on the boundedness of either the stochastic operator or the stochastic samples. We also provide the first in-expectation unbiased convergence rate results for these methods under a relaxed smoothness assumption for $\alpha \leq \frac{1}{2}$.},
  archive      = {J_TMLR},
  author       = {Daniil Vankov and Angelia Nedich and Lalitha Sankar},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Generalized smooth stochastic variational inequalities: Almost sure convergence and convergence rates},
  url          = {https://openreview.net/forum?id=EjqSpbUBWU},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EL-clustering: Combining upper- and lower-bounded clusterings for equitable load constraints. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=EkjDfnJ1gU'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of an ordinary clustering algorithm may yield a clustering output where the number of points per cluster (cluster size) varies significantly. In settings where the centers correspond to facilities that provide a service, this can be highly undesirable as the cluster size is essentially the service load for a facility. While prior work has considered imposing either a lower bound on the cluster sizes or an upper bound, imposing both bounds simultaneously has seen limited work, especially for the $k$-median objective, despite its strong practical motivation. In this paper, we solve the \emph{equitable load} (\EL{}) clustering problem where we minimize the $k$-median objective subject to the cluster sizes not exceeding an upper bound or falling below a lower bound. We solve this problem using a modular approach. Specifically, given a clustering solution that satisfies the lower bound constraints and another that satisfies the upper bound constraints, we introduce a combination algorithm which essentially combines both solutions to produce one that satisfies both constraints simultaneously at the expense of a bounded degradation in the $k$-median objective and a slight violation of the upper bound. Our combination algorithm runs in $O(k^3+n)$ time, where $n$ is the number of points and is faster than standard $k$-median algorithms that satisfy either the lower or upper bound constraints. Interestingly, our results can be generalized to various other clustering objectives, including the $k$-means objective. We also do empirical evaluation for $k$-Median objective on benchmark datasets to show that both, the cost as well as the violation factor are significantly smaller in practice than the theoretical worst-case guarantees\footnote{https://github.com/0-rudra-0/el-clustering}.},
  archive      = {J_TMLR},
  author       = {Rajni Dabas and Neelima Gupta and Rudra Bhardwaj and Sapna Grover},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {EL-clustering: Combining upper- and lower-bounded clusterings for equitable load constraints},
  url          = {https://openreview.net/forum?id=EkjDfnJ1gU},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From novelty to imitation: Self-distilled rewards for offline reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=F5K94JI2Jb'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline Reinforcement Learning (RL) aims to learn effective policies from a static dataset without requiring further agent environment interactions. However, its practical adoption is often hindered by the need for explicit reward annotations, which can be costly to engineer or difficult to obtain retrospectively. To address this, we propose ReLOAD (Reinforcement Learning with Offline Reward Annotation via Distillation), a novel reward annotation framework for offline RL. Unlike existing methods that depend on complex alignment procedures, our approach adapts Random Network Distillation (RND) to generate intrinsic rewards from expert demonstrations using a simple yet effective embedding discrepancy measure. First, we train a predictor network to mimic a fixed target network’s embeddings based on expert state transitions. Later, the prediction error between these networks serves as a reward signal for each transition in the static dataset. This mechanism provides a structured reward signal without requiring handcrafted reward annotations. We provide a formal theoretical construct that provides insights into how RND prediction errors effectively serve as intrinsic rewards by distinguishing expert-like transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables robust offline policy learning and achieves performance competitive with traditional reward-annotated methods.},
  archive      = {J_TMLR},
  author       = {Gaurav Chaudhary and Laxmidhar Behera},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {From novelty to imitation: Self-distilled rewards for offline reinforcement learning},
  url          = {https://openreview.net/forum?id=F5K94JI2Jb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An architecture built for federated learning: Addressing data heterogeneity through adaptive normalization-free feature recalibration. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=GtdYFLsblb'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a decentralized collaborative training paradigm preserving stakeholders’ data ownership while improving performance and generalization. However, statistical heterogeneity among client datasets degrades system performance. To address this issue, we propose Adaptive Normalization-free Feature Recalibration (ANFR), a model architecture-level approach that combines weight standardization and channel attention to combat heterogeneous data in FL. ANFR leverages weight standardization to avoid mismatched client statistics and inconsistent averaging, ensuring robustness under heterogeneity, and channel attention to produce learnable scaling factors for feature maps, suppressing inconsistencies across clients due to heterogeneity. We demonstrate that combining these techniques boosts model performance beyond their individual contributions, by improving class selectivity and channel attention weight distribution. ANFR works with any aggregation method, supports both global and personalized FL, and adds minimal overhead. Furthermore, when training with differential privacy, ANFR achieves an appealing balance between privacy and utility, enabling strong privacy guarantees without sacrificing performance. By integrating weight standardization and channel attention in the backbone model, ANFR offers a novel and versatile approach to the challenge of statistical heterogeneity. Extensive experiments show ANFR consistently outperforms established baselines across various aggregation methods, datasets, and heterogeneity conditions. Code is provided at https://github.com/siomvas/ANFR.},
  archive      = {J_TMLR},
  author       = {Vasilis Siomos and Jonathan Passerat-Palmbach and Giacomo Tarroni},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {An architecture built for federated learning: Addressing data heterogeneity through adaptive normalization-free feature recalibration},
  url          = {https://openreview.net/forum?id=GtdYFLsblb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially private clustered federated learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=JSsko0a4yr'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL), which is a decentralized machine learning (ML) approach, often incorporates differential privacy (DP) to provide rigorous data privacy guarantees to clients. Previous works attempted to address high structured data heterogeneity in vanilla FL settings through clustering clients (a.k.a clustered FL), but these methods remain sensitive and prone to errors, further exacerbated by the DP noise. This vulnerability makes the previous methods inappropriate for differentially private FL (DPFL) under structured data heterogeneity. To address this gap, we propose an algorithm for differentially private clustered FL, which is robust to the DP noise in the system and identifies the underlying clients’ clusters correctly. To this end, we propose to cluster clients based on both their model updates and training loss values. Furthermore, for clustering clients’ model updates at the end of the first round, our proposed approach addresses the server’s uncertainties by employing large batch sizes as well as Gaussian Mixture Models (GMM) to reduce the impact of DP and stochastic noise and avoid potential clustering errors. We provide theoretical analysis to justify our approach and evaluate it across diverse data distributions and privacy budgets. Our experimental results show the approach’s effectiveness in addressing high structured data heterogeneity in DPFL.},
  archive      = {J_TMLR},
  author       = {Saber Malekmohammadi and Afaf Taik and Golnoosh Farnadi},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Differentially private clustered federated learning},
  url          = {https://openreview.net/forum?id=JSsko0a4yr},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On diffusion posterior sampling via sequential monte carlo for zero-shot scaffolding of protein motifs. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=KXRYY7iwqh'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of diffusion models, new proteins can be generated at an unprecedented rate. The motif scaffolding problem requires steering this generative process to yield proteins with a desirable functional substructure called a motif. While models have been trained to take the motif as conditional input, recent techniques in diffusion posterior sampling can be leveraged as zero-shot alternatives whose approximations can be corrected with sequential Monte Carlo (SMC) algorithms. In this work, we introduce a new set of guidance potentials for describing scaffolding tasks and solve them by adapting SMC-aided diffusion posterior samplers with an unconditional model, Genie, as a prior. In single motif problems, we find that (i) the proposed potentials perform comparably, if not better, than the conventional masking approach, (ii) samplers based on reconstruction guidance outperform their replacement method counterparts, and (iii) measurement tilted proposals and twisted targets improve performance substantially. Furthermore, as a demonstration, we provide solutions to two multi-motif problems by pairing reconstruction guidance with an SE(3)-invariant potential. We also produce designable internally symmetric monomers with a guidance potential for point symmetry constraints. Our code is available at: https://github.com/matsagad/mres-project.},
  archive      = {J_TMLR},
  author       = {James Matthew Young and O. Deniz Akyildiz},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On diffusion posterior sampling via sequential monte carlo for zero-shot scaffolding of protein motifs},
  url          = {https://openreview.net/forum?id=KXRYY7iwqh},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic primal-dual double block-coordinate for two- way partial AUC maximization. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=M3kibBFP4q'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-way partial AUC (TPAUC) is a critical performance metric for binary classification with imbalanced data, as it focuses on specific ranges of the true positive rate (TPR) and false positive rate (FPR). However, stochastic algorithms for TPAUC optimization remain under-explored, with existing methods either limited to approximated TPAUC loss functions or burdened by sub-optimal complexities. To overcome these limitations, we introduce two innovative stochastic primal-dual double block-coordinate algorithms for TPAUC maximization. These algorithms utilize stochastic block-coordinate updates for both the primal and dual variables, catering to both convex and non-convex settings. We provide theoretical convergence rate analyses, demonstrating significant improvements over prior approaches. Our experimental results, based on multiple benchmark datasets, validate the superior performance of our algorithms, showcasing faster convergence and better generalization. This work advances the state of the art in TPAUC optimization and offers practical tools for real-world machine learning applications.},
  archive      = {J_TMLR},
  author       = {Linli Zhou and Bokun Wang and My T. Thai and Tianbao Yang},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Stochastic primal-dual double block-coordinate for two- way partial AUC maximization},
  url          = {https://openreview.net/forum?id=M3kibBFP4q},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning time-series representations by hierarchical uniformity-tolerance latent balancing. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=NTmVEAiyB5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose TimeHUT, a novel method for learning time-series representations by hierarchical uniformity-tolerance balancing of contrastive representations. Our method uses two distinct losses to learn strong representations with the aim of striking an effective balance between uniformity and tolerance in the embedding space. First, TimeHUT uses a hierarchical setup to learn both instance-wise and temporal information from input time-series. Next, we integrate a temperature scheduler within the vanilla contrastive loss to balance the uniformity and tolerance characteristics of the embeddings. Additionally, a hierarchical angular margin loss enforces instance-wise and temporal contrast losses, creating geometric margins between positive and negative pairs of temporal sequences. This approach improves the coherence of positive pairs and their separation from the negatives, enhancing the capture of temporal dependencies within a time-series sample. We evaluate our approach on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and multivariate classification, as well as Yahoo and KPI datasets for anomaly detection. The results demonstrate that TimeHUT outperforms prior methods by considerable margins on classification, while obtaining competitive results for anomaly detection. Finally, detailed sensitivity and ablation studies are performed to evaluate different components and hyperparameters of our method.},
  archive      = {J_TMLR},
  author       = {Amin Jalali and Milad Soltany and Michael Greenspan and Ali Etemad},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Learning time-series representations by hierarchical uniformity-tolerance latent balancing},
  url          = {https://openreview.net/forum?id=NTmVEAiyB5},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale targeted cause discovery via learning from simulated data. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=NVgy29IQw8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel machine learning approach for inferring causal variables of a target variable from observations. Our focus is on directly inferring a set of causal factors without requiring full causal graph reconstruction, which is computationally challenging in large-scale systems. The identified causal set consists of all potential regulators of the target variable under experimental settings, enabling efficient regulation through intervention. To achieve this, we train a neural network using supervised learning on simulated data to infer causality. By employing a subsampled-ensemble inference strategy, our approach scales with linear complexity in the number of variables, efficiently scaling up to thousands of variables. Empirical results demonstrate superior performance in identifying causal relationships within large-scale gene regulatory networks, outperforming existing methods that emphasize full-graph discovery. We validate our model's generalization capability across out-of-distribution graph structures and generating mechanisms, including gene regulatory networks of E. coli and the human K562 cell line.},
  archive      = {J_TMLR},
  author       = {Jang-Hyun Kim and Claudia Skok Gibbs and Sangdoo Yun and Hyun Oh Song and Kyunghyun Cho},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Large-scale targeted cause discovery via learning from simulated data},
  url          = {https://openreview.net/forum?id=NVgy29IQw8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSC: Posterior sampling-based compression. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=OsqgU6Jz4t'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have transformed the landscape of image generation and now show remarkable potential for image compression. Most of the recent diffusion-based compression methods require training and are tailored for a specific bit-rate. In this work, we propose Posterior Sampling-based Compression (PSC) -- a zero-shot compression method that leverages a pre-trained diffusion model as its sole neural network component, thus enabling the use of diverse, publicly available models without additional training. Our approach is inspired by transform coding methods, which encode the image in some pre-chosen transform domain. However, PSC constructs a transform that is adaptive to the image. This is done by employing a zero-shot diffusion-based posterior sampler so as to progressively construct the rows of the transform matrix. Each new chunk of rows is chosen to reduce the uncertainty about the image given the quantized measurements collected thus far. Importantly, the same adaptive scheme can be replicated at the decoder, thus avoiding the need to encode the transform itself. We demonstrate that even with basic quantization and entropy coding, PSC's performance is comparable to established training-based methods in terms of rate, distortion, and perceptual quality. This is while providing greater flexibility, allowing to choose at inference time any desired rate or distortion.},
  archive      = {J_TMLR},
  author       = {Noam Elata and Tomer Michaeli and Michael Elad},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {PSC: Posterior sampling-based compression},
  url          = {https://openreview.net/forum?id=OsqgU6Jz4t},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coreset-driven re-labeling: Tackling noisy annotations with noise-free gradients. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Tk78vb2Qd7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale datasets invariably contain annotation noise. Re-labeling methods have been developed to handle annotation noise in large-scale datasets. Though various methodologies to alleviate annotation noise have been developed, these are particularly time-consuming and computationally intensive. The requirement of high computational power and longer time duration can be drastically reduced by selecting a representative coreset. In this work, we adapt a noise-free gradient-based coreset selection method towards re-labeling applications for noisy datasets with erroneous labels. We introduce ‘confidence score’ to the coreset selection method to cater for the presence of noisy labels. Through extensive evaluation over CIFAR-100N, Web Vision, and ImageNet-1K Datasets, we demonstrate that our method outperforms the SOTA coreset selection for re-labeling methods (DivideMix and SOP+). We have provided the codebase at URL.},
  archive      = {J_TMLR},
  author       = {Saumyaranjan Mohanty and Konda Reddy Mopuri},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Coreset-driven re-labeling: Tackling noisy annotations with noise-free gradients},
  url          = {https://openreview.net/forum?id=Tk78vb2Qd7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A case for library-level $k$-means binning in histogram gradient-boosted trees. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=UaTrLLspJa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern Gradient Boosted Decision Trees (GBDTs) accelerate split finding with histogram-based binning, which reduces complexity from $O(N\log N)$ to $O(N)$ by aggregating gradients into fixed-size bins. However, the predominant quantile binning strategy—designed to distribute data points evenly among bins—may overlook critical boundary values that could enhance predictive performance. In this work, we consider a novel approach that replaces quantile binning with a $k$-means discretizer initialized with quantile bins, and justify the swap with a proof showing how, for any $L$-Lipschitz function, k-means maximizes the worst-case explained variance of Y obtained when treating all values in a given bin as equivalent. We test this swap against quantile and uniform binning on 33 OpenML datasets plus synthetics that control for modality, skew, and bin budget. Across 18 regression datasets, k-means shows no statistically significant losses at the 5% level and wins in three cases—most strikingly a 55% MSE drop on one particularly skewed dataset—even though k-means' mean reciprocal rank (MRR) is slightly lower (0.65 vs 0.72). On the 15 classification datasets the two methods are statistically tied (MRR 0.70 vs 0.68) with gaps $\leq$0.2 pp. Synthetic experiments confirm consistently large MSE gains—typically $>$20% and rising to 90% as outlier magnitude increases or bin budget drops. We find that k-means keeps error on par with exhaustive (no-binning) splitting when extra cuts add little value, yet still recovers key split points that quantile overlooks. As such, we advocate for a built-in bin_method=$k$-means flag, especially in regression tasks and in tight-budget settings such as the 32–64-bin GPU regime—because it is a "safe default" with large upside, yet adds only a one-off, cacheable overhead ($\approx$ 3.5s per feature to bin 10M rows on one Apple M1 thread).},
  archive      = {J_TMLR},
  author       = {Asher Labovich},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {A case for library-level $k$-means binning in histogram gradient-boosted trees},
  url          = {https://openreview.net/forum?id=UaTrLLspJa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical comparison of membership inference attacks in deep transfer learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=UligTUCgdt'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of powerful large-scale foundation models, the training paradigm is increasingly shifting from from-scratch training to transfer learning. This enables high utility training with small, domain-specific datasets typical in sensitive applications. Membership inference attacks (MIAs) provide an empirical estimate of the privacy leakage by machine learning models. Yet, prior assessments of MIAs against models fine-tuned with transfer learning rely on a small subset of possible attacks. We address this by comparing performance of diverse MIAs in transfer learning settings to help practitioners identify the most efficient attacks for privacy risk evaluation. We find that attack efficacy decreases with the increase in training data for score-based MIAs. We find that there is no one MIA which captures all privacy risks in models trained with transfer learning. While the Likelihood Ratio Attack (LiRA) demonstrates superior performance across most experimental scenarios, the Inverse Hessian Attack (IHA) proves to be more effective against models fine-tuned on PatchCamelyon dataset in high data regime.},
  archive      = {J_TMLR},
  author       = {Yuxuan Bai and Gauri Pradhan and Marlon Tobaben and Antti Honkela},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Empirical comparison of membership inference attacks in deep transfer learning},
  url          = {https://openreview.net/forum?id=UligTUCgdt},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating spatial information into goal-conditioned hierarchical reinforcement learning via graph representations. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=a7Bx4s5gA8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.},
  archive      = {J_TMLR},
  author       = {Shuyuan Zhang and Zihan Wang and Xiao-Wen Chang and Doina Precup},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Incorporating spatial information into goal-conditioned hierarchical reinforcement learning via graph representations},
  url          = {https://openreview.net/forum?id=a7Bx4s5gA8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pref-GUIDE: Continual policy learning from real-time human feedback via preference-based learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=dWGUwidXDm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training reinforcement learning agents with human feedback is crucial when task objectives are difficult to specify through dense reward functions. While prior methods rely on offline trajectory comparisons to elicit human preferences, such data is unavailable in online learning scenarios where agents must adapt on the fly. Recent approaches address this by collecting real-time scalar feedback to guide agent behavior and train reward models for continued learning after human feedback becomes unavailable. However, scalar feedback is often noisy and inconsistent, limiting the accuracy and generalization of learned rewards. We propose Pref-GUIDE, a framework that transforms real-time scalar feedback into preference-based data to improve reward model learning for continual policy training. Pref-GUIDE Individual mitigates temporal inconsistency by comparing agent behaviors within short windows and filtering ambiguous feedback. Pref-GUIDE Voting further enhances robustness by aggregating reward models across a population of users to form consensus preferences. Across three challenging environments, Pref-GUIDE significantly outperforms scalar-feedback baselines, with the voting variant exceeding even expert-designed dense rewards. By reframing scalar feedback as structured preferences with population feedback, Pref-GUIDE, offers a scalable and principled approach for harnessing human input in online reinforcement learning.},
  archive      = {J_TMLR},
  author       = {Zhengran Ji and Boyuan Chen},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Pref-GUIDE: Continual policy learning from real-time human feedback via preference-based learning},
  url          = {https://openreview.net/forum?id=dWGUwidXDm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decomposed direct preference optimization for structure-based drug design. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=dwSpo5DRk8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have achieved promising results for Structure-Based Drug Design (SBDD). Nevertheless, high-quality protein subpocket and ligand data are relatively scarce, which hinders the models’ generation capabilities. Recently, Direct Preference Optimization (DPO) has emerged as a pivotal tool for aligning generative models with human preferences. In this paper, we propose DecompDpo, a structure-based optimization method aligns diffusion models with pharmaceutical needs using multi-granularity preference pairs. DecompDpo introduces decomposition into the optimization objectives and obtains preference pairs at the molecule or decomposed substructure level based on each objective’s decomposability. Additionally, DecompDpo introduces a physics-informed energy term to ensure reasonable molecular conformations in the optimization results. Notably, DecompDpo can be effectively used for two main purposes: (1) fine-tuning pretrained diffusion models for molecule generation across various protein families, and (2) molecular optimization given a specific protein subpocket after generation. Extensive experiments on the CrossDocked2020 benchmark show that DecompDpo significantly improves model performance, achieving up to 98.5% Med. High Affinity and a 43.9% success rate for molecule generation, and 100% Med. High Affinity and a 52.1% success rate for targeted molecule optimization.},
  archive      = {J_TMLR},
  author       = {Xiwei Cheng and Xiangxin Zhou and Yuwei Yang and Yu Bao and Quanquan Gu},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Decomposed direct preference optimization for structure-based drug design},
  url          = {https://openreview.net/forum?id=dwSpo5DRk8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does unsupervised domain adaptation improve the robustness of amortized bayesian inference? a systematic evaluation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ewgLuvnEw6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in silico and practice. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications -- a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA to increase the robustness of ABI.},
  archive      = {J_TMLR},
  author       = {Lasse Elsemüller and Valentin Pratz and Mischa von Krause and Andreas Voss and Paul-Christian Bürkner and Stefan T. Radev},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Does unsupervised domain adaptation improve the robustness of amortized bayesian inference? a systematic evaluation},
  url          = {https://openreview.net/forum?id=ewgLuvnEw6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-NN as a simple and effective estimator of transferability. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=hGlkjP1zHc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How well can one expect transfer learning to work in a new setting where the domain is shifted, the task is different, and the architecture changes? Many transfer learning metrics have been proposed to answer this question. But how accurate are their predictions in a realistic new setting? We conducted an extensive evaluation involving over 42,000 experiments comparing 23 transferability metrics across 16 different datasets to assess their ability to predict transfer performance. Our findings reveal that none of the existing metrics perform well across the board. However, we find that a simple k-nearest neighbor evaluation -- as is commonly used to evaluate feature quality for self-supervision -- not only surpasses existing metrics, but also offers better computational efficiency and ease of implementation.},
  archive      = {J_TMLR},
  author       = {Moein Sorkhei and Christos Matsoukas and Johan Fredin Haslum and Emir Konuk and Kevin Smith},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {K-NN as a simple and effective estimator of transferability},
  url          = {https://openreview.net/forum?id=hGlkjP1zHc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond marginals: Learning joint spatio-temporal patterns for multivariate anomaly detection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=iETTv1okjX'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to improve anomaly detection (AD) by incorporating the time-varying non-linear spatio-temporal correlations of the multi-variate time series data in the modeling process. In multivariate AD, the simultaneous deviation of multiple nodes from their expected behavior can indicate an anomaly, even if no individual node shows a clearly abnormal pattern. In many existing approaches, time series variables are assumed to be (conditionally) independent, which oversimplifies real-world interactions. Our approach addresses this by modeling joint dependencies using a copula-based framework, which decouples the modeling of marginal distributions, temporal dynamics, and inter-variable dependencies. We use a transformer encoder to capture temporal patterns, and to model spatial (inter-variable) dependencies, we integrate a copula. Both components are trained jointly in a latent space using a self-supervised contrastive learning objective to learn meaningful feature representations to separate normal and anomaly samples.},
  archive      = {J_TMLR},
  author       = {Padmaksha Roy and Almuatazbellah Boker and Lamine Mili},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Beyond marginals: Learning joint spatio-temporal patterns for multivariate anomaly detection},
  url          = {https://openreview.net/forum?id=iETTv1okjX},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse-to-sparse training of diffusion models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=iRupdoPLJa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models (DMs) are a powerful type of generative models that have achieved state-of-the-art results in various image synthesis tasks and have shown potential in other domains, such as natural language processing and temporal data modeling. Despite their stable training dynamics and ability to produce diverse high-quality samples, DMs are notorious for requiring significant computational resources, both in the training and inference stages. Previous work has focused mostly on increasing the efficiency of model inference. This paper introduces, for the first time, the paradigm of sparse-to-sparse training to DMs, with the aim of improving both training and inference efficiency. We focus on unconditional generation and train sparse DMs from scratch (Latent Diffusion and ChiroDiff) on six datasets using three different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of sparsity in model performance. Our experiments show that sparse DMs are able to match and often outperform their Dense counterparts, while substantially reducing the number of trainable parameters and FLOPs. We also identify safe and effective values to perform sparse-to-sparse training of DMs.},
  archive      = {J_TMLR},
  author       = {Inês Cardoso Oliveira and Decebal Constantin Mocanu and Luis A. Leiva},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Sparse-to-sparse training of diffusion models},
  url          = {https://openreview.net/forum?id=iRupdoPLJa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral clustering and labeling for crowdsourcing with inherently distinct task types. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=jVQjtzcvAc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dawid-Skene model is the most widely assumed model in the analysis of crowdsourcing algorithms that estimate ground-truth labels from noisy worker responses. In this work, we are motivated by crowdsourcing applications where workers have distinct skill sets and their accuracy additionally depends on a task's type. Focusing on the case where there are two types of tasks, we propose a spectral method to partition tasks into two groups such that a worker has the same reliability for all tasks within a group. Our analysis reveals a separability condition such that task types can be perfectly recovered if the number of workers $n$ scales logarithmically with the number of tasks $d$. Numerical experiments show how clustering tasks by type before estimating ground-truth labels enhances the performance of crowdsourcing algorithms in practical applications.},
  archive      = {J_TMLR},
  author       = {Saptarshi Mandal and Seo Taek Kong and Dimitrios Katselis and R. Srikant},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Spectral clustering and labeling for crowdsourcing with inherently distinct task types},
  url          = {https://openreview.net/forum?id=jVQjtzcvAc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Activation sharding for scalable training of large models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=kQCuMcEneq'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite fast progress, efficiently training large language models (LLMs) in extremely long contexts remains challenging. Existing methods fall back to training LLMs with short contexts (up to a few thousand tokens) and use inference time techniques when evaluating on very long contexts (above 1M tokens). Training on very long contexts is limited by GPU memory availability and the prohibitively long training times it requires on state-of-the-art hardware. Meanwhile, many real-life applications require training/fine-tuning with long context on specific tasks. Such applications include, for example, augmenting the context with various sources of raw reference information for extraction, summarization, or fact reconciliation tasks. We propose adjoint sharding, a novel technique that comprises sharding gradient calculation during training to reduce memory requirements by orders of magnitude, making training on very long contexts computationally tractable. At the core of our adjoint sharding algorithm lies the adjoint method, which efficiently computes gradients that are provably equivalent to the gradients computed using standard backpropagation. We also propose truncated adjoint sharding to accelerate the algorithm while maintaining performance. We provide a distributed and a parallel-computing version of adjoint sharding to speed up training and to show that adjoint sharding is compatible with these standard memory-reduction techniques. Empirical results show the proposed adjoint sharding algorithm reduces memory usage by up to 3$\times$ on a large language model with 1.27B parameters on 1M context length training. This reduction in memory usage allows increasing the maximum context length of training a 1.27B parameter model from 35K tokens to above 100K tokens on a training infrastructure composed of five AWS P4 instances.},
  archive      = {J_TMLR},
  author       = {Xingzi Xu and Amir Tavanaei and Kavosh Asadi and Karim Bouyarmane},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Activation sharding for scalable training of large models},
  url          = {https://openreview.net/forum?id=kQCuMcEneq},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable and responsible foundation models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=nLJZh4M6S5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.},
  archive      = {J_TMLR},
  author       = {Xinyu Yang and Junlin Han and Rishi Bommasani and Jinqi Luo and Wenjie Qu and Wangchunshu Zhou and Adel Bibi and Xiyao Wang and Jaehong Yoon and Elias Stengel-Eskin and Shengbang Tong and Lingfeng Shen and Rafael Rafailov and Runjia Li and Zhaoyang Wang and Yiyang Zhou and Chenhang Cui and Yu Wang and Wenhao Zheng and Huichi Zhou and Jindong Gu and Zhaorun Chen and Peng Xia and Tony Lee and Thomas P Zollo and Vikash Sehwag and Jixuan Leng and Jiuhai Chen and Yuxin Wen and Huan Zhang and Zhun Deng and Linjun Zhang and Pavel Izmailov and Pang Wei Koh and Yulia Tsvetkov and Andrew Gordon Wilson and Jiaheng Zhang and James Zou and Cihang Xie and Hao Wang and Philip Torr and Julian McAuley and David Alvarez-Melis and Florian Tramèr and Kaidi Xu and Suman Jana and Chris Callison-Burch and Rene Vidal and Filippos Kokkinos and Mohit Bansal and Beidi Chen and Huaxiu Yao},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Reliable and responsible foundation models},
  url          = {https://openreview.net/forum?id=nLJZh4M6S5},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlearning misalignment for personalized LLM adaptation via instance-response-dependent discrepancies. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=njE3swFBMc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Large Language Models (LLMs) have revolutionized chatbot interactions, they often fall short of aligning responses with the nuanced preferences of individual users, a challenge rooted in the inherently subjective and proprietary nature of those preferences. Consequently, prompt-based learning, though effective in enhancing factual accuracy due to its emphasis on universal correctness, remains insufficient for achieving accurate personalised response alignment. Because user preferences vary widely across individuals and contexts, aligning responses requires a more personalized and context-aware approach. To address this limitation, we propose Consistent Marginalization (CM), a novel framework that aims to unlearn misalignment by constructing a personalised memory bank of instance-response-dependent discrepancies, built from a small set of user preference samples. This personalised memory bank equips LLMs with the ability to understand, recall, and adapt to individual preferences, enabling more consistent and personalized responses. Evaluated across a diverse range of domain-specific datasets and model architectures, CM yields notable improvements in response alignment and robustness. We believe Consistent Marginalization represents a valuable step toward enabling LLMs to become genuinely personable and adaptive conversational agents by understanding user preferences and generating responses that are better aligned with individual user expectations.},
  archive      = {J_TMLR},
  author       = {Cheng Chen and Atsushi Nitanda and Ivor Tsang},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Unlearning misalignment for personalized LLM adaptation via instance-response-dependent discrepancies},
  url          = {https://openreview.net/forum?id=njE3swFBMc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dependency-aware maximum likelihood estimation for active learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=qDVDSXXGK1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning aims to efficiently build a labeled training set by strategically selecting samples to query labels from annotators. In this sequential process, each sample acquisition influences subsequent selections, causing dependencies among samples in the labeled set. However, these dependencies are overlooked during the model parameter estimation stage when updating the model using Maximum Likelihood Estimation (MLE), a conventional method that assumes independent and identically distributed (i.i.d.) data. We propose Dependency-aware MLE (DMLE), which corrects MLE within the active learning framework by addressing sample dependencies typically neglected due to the i.i.d. assumption, ensuring consistency with active learning principles in the model parameter estimation process. This improved method achieves superior performance across multiple benchmark datasets, reaching higher performance in earlier cycles compared to conventional MLE. Specifically, we observe average accuracy improvements of 6%, 8.6%, and 10.5% for k=1, k=5, and k=10 respectively, after collecting the first 100 samples, where entropy is the acquisition function and k is the query batch size acquired at every active learning cycle.},
  archive      = {J_TMLR},
  author       = {Beyza Kalkanli and Tales Imbiriba and Stratis Ioannidis and Deniz Erdogmus and Jennifer Dy},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Dependency-aware maximum likelihood estimation for active learning},
  url          = {https://openreview.net/forum?id=qDVDSXXGK1},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing utility and privacy: Dynamically private SGD with random projection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=u6OSRdkAwl'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic optimization is a pivotal enabler in modern machine learning, producing effective models for various tasks. However, several existing works have shown that model parameters and gradient information are susceptible to privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy concerns, its static noise mechanism impacts the error bounds for model performance. Additionally, with the exponential increase in model parameters, efficient learning of these models using stochastic optimizers has become more challenging. To address these concerns, we introduce the Dynamically Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we combine two important ideas: (i) dynamic differential privacy (DDP) with automatic gradient clipping and (ii) random projection with SGD, allowing dynamic adjustment of the tradeoff between utility and privacy of the model. It exhibits provably sub-linear convergence rates across different objective functions, matching the best available rate. The theoretical analysis further suggests that DDP leads to better utility at the cost of privacy, while random projection enables more efficient model learning. Extensive experiments across diverse datasets show that D2P2-SGD remarkably enhances accuracy while maintaining privacy. Our code is available here.},
  archive      = {J_TMLR},
  author       = {Zhanhong Jiang and Md Zahid Hasan and Nastaran Saadati and Aditya Balu and Chao Liu and Soumik Sarkar},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Balancing utility and privacy: Dynamically private SGD with random projection},
  url          = {https://openreview.net/forum?id=u6OSRdkAwl},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MGPATH: A vision-language model with multi-granular prompt learning for few-shot whole slide pathology classification. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=u7U81JLGjH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model’s ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this https://github.com/HauschildLab/MGPATH.},
  archive      = {J_TMLR},
  author       = {Anh-Tien Nguyen and Duy Minh Ho Nguyen and Nghiem Tuong Diep and Trung Quoc Nguyen and Nhat Ho and Jacqueline Michelle Metsch and Miriam Cindy Maurer and Daniel Sonntag and Hanibal Bohnenberger and Anne-Christin Hauschild},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {MGPATH: A vision-language model with multi-granular prompt learning for few-shot whole slide pathology classification},
  url          = {https://openreview.net/forum?id=u7U81JLGjH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HybridFlow: Quantification of aleatoric and epistemic uncertainty with a single hybrid model. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=xRiEdSyVjY'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification is critical for ensuring robustness in high-stakes machine learning applications. We introduce HybridFlow, a modular hybrid architecture that unifies the modeling of aleatoric and epistemic uncertainty by combining a Conditional Masked Autoregressive normalizing flow for estimating aleatoric uncertainty with a flexible probabilistic predictor for epistemic uncertainty. The framework supports integration with any probabilistic model class, allowing users to easily adapt HybridFlow to existing architectures without sacrificing predictive performance. HybridFlow improves upon previous uncertainty quantification frameworks across a range of regression tasks, such as depth estimation, a collection of regression benchmarks, and a scientific case study of ice sheet emulation. We also provide empirical results of the quantified uncertainty, showing that the uncertainty quantified by HybridFlow is calibrated and better aligns with model error than existing methods for quantifying aleatoric and epistemic uncertainty. HybridFlow addresses a key challenge in Bayesian deep learning, unifying aleatoric and epistemic uncertainty modeling in a single robust framework.},
  archive      = {J_TMLR},
  author       = {Peter Van Katwyk and Karianne Bergen},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {HybridFlow: Quantification of aleatoric and epistemic uncertainty with a single hybrid model},
  url          = {https://openreview.net/forum?id=xRiEdSyVjY},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chimera: State space models beyond sequences. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=yv0TUssepk'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based deep learning methods have emerged as the standard approach to model diverse data such as sequences, images, and graphs. These methods rely on self-attention, which treats data as an unordered set of elements. This ignores the neighborhood structure or graph topology of the data and requires the use of inductive biases, such as position embeddings in sequences and images, and random walks in graphs, to incorporate topology. However, developing bespoke inductive biases for each task requires significant effort and can also introduce side-effects hindering generalization. In this work, we introduce Chimera, a unified model that directly incorporates the data topology in a principled way, obviating the need for domain-specific biases. Central to Chimera is the observation that state-space models---which naturally do not require position embeddings---can be generalized to capture any general graph topology. Our experiments demonstrate the versatility of our approach---Chimera achieves strong performance across the domains of language, vision, and graphs, outperforming BERT on GLUE by 0.7 points, ViT on ImageNet-1k by 2.6%, and all the baselines on the Long Range Graph Benchmark. Our results validate Chimera's principled methodological contributions and affirm the long-held belief that data topology is a powerful inductive bias across modalities. We further propose algorithmic optimizations to improve Chimera's efficiency while maintaining performance: 1) For the subclass of Directed Acyclic Graphs we show that Chimera can be implemented as a linear time recurrence. 2) For general graphs, we relax the method with a simple mathematical approximation, achieving Transformer's quadratic complexity without relying on domain-specific biases.},
  archive      = {J_TMLR},
  author       = {Aakash Lahoti and Tanya Marwah and Ratish Puduppully and Albert Gu},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Chimera: State space models beyond sequences},
  url          = {https://openreview.net/forum?id=yv0TUssepk},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
