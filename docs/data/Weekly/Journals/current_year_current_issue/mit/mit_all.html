<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>mit</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alj">ALJ - 9</h2>
<ul>
<li><details>
<summary>
(2025). Automating the search for artificial life with foundation models. <em>ALJ</em>, <em>31</em>(3), 368-396. (<a href='https://doi.org/10.1162/ARTL.a.8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial and error to discover the configurations of lifelike simulations. This article presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called automated search for Artificial Life (ASAL), (a) finds simulations that produce target phenomena, (b) discovers simulations that generate temporally open-ended novelty, and (c) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates, including Boids, Particle Life, the Game of Life, Lenia, and neural cellular automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids life-forms, as well as cellular automata that are open-ended like Conway’s Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.},
  archive      = {J_ALJ},
  author       = {Kumar, Akarsh and Lu, Chris and Kirsch, Louis and Tang, Yujin and Stanley, Kenneth O. and Isola, Phillip and Ha, David},
  doi          = {10.1162/ARTL.a.8},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {368-396},
  shortjournal = {Artif. Life},
  title        = {Automating the search for artificial life with foundation models},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive distinctions as a language for cognitive science: Comparing methods of description in a model of referential communication. <em>ALJ</em>, <em>31</em>(3), 345-367. (<a href='https://doi.org/10.1162/artl_a_00475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An analysis of the language we use in scientific practice is critical to developing more rigorous and sound methodologies. This article argues that how certain methods of description are commonly employed in cognitive science risks obscuring important features of an agent’s cognition. We propose to make explicit a method of description whereby the concept of cognitive distinctions is the core principle. A model of referential communication is developed and analyzed as a platform to compare methods of description. We demonstrate that cognitive distinctions, realized in a graph theoretic formalism, better describe the behavior and perspective of a simple model agent than other, less systematic or natural language–dependent methods. We then consider how different descriptions relate to one another in the broader methodological framework of minimally cognitive behavior. Finally, we explore the consequences of, and challenges for, cognitive distinctions as a useful concept and method in the tool kit of cognitive scientists.},
  archive      = {J_ALJ},
  author       = {Gaul, Thomas M. and Izquierdo, Eduardo J.},
  doi          = {10.1162/artl_a_00475},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {345-367},
  shortjournal = {Artif. Life},
  title        = {Cognitive distinctions as a language for cognitive science: Comparing methods of description in a model of referential communication},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behaviour diversity in a walking and climbing centipede-like virtual creature. <em>ALJ</em>, <em>31</em>(3), 321-344. (<a href='https://doi.org/10.1162/artl_a_00476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot controllers are often optimized for a single robot in a single environment. This approach proves brittle, as such a controller will often fail to produce sensible behavior for a new morphology or environment. In comparison, animal gaits are robust and versatile. By observing animals, and attempting to extract general principles of locomotion from their movement, we aim to design a single, decentralized controller applicable to diverse morphologies and environments. The controller implements the three components of (a) undulation, (b) peristalsis, and (c) leg motion, which we believe are the essential elements in most animal gaits. This work is a first step toward a general controller. Accordingly, the controller has been evaluated on a limited range of simulated centipede-like robot morphologies. The centipede is chosen as inspiration because it moves using both body contractions and legged locomotion. For a controller to work in qualitatively different settings, it must also be able to exhibit qualitatively different behaviors. We find that six different modes of locomotion emerge from our controller in response to environmental and morphological changes. We also find that different parts of the centipede model can exhibit different modes of locomotion, simultaneously, based on local morphological features. This controller can potentially aid in the design or evolution of robots, by quickly testing the potential of a morphology, or be used to get insights about underlying locomotion principles in the centipede.},
  archive      = {J_ALJ},
  author       = {Norstein, Emma Stensby and Yasui, Kotaro and Kano, Takeshi and Ishiguro, Akio and Glette, Kyrre},
  doi          = {10.1162/artl_a_00476},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {321-344},
  shortjournal = {Artif. Life},
  title        = {Behaviour diversity in a walking and climbing centipede-like virtual creature},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benefit game 2.0: Alien seaweed Swarms—Exploring the interplay of human activity and environmental sustainability. <em>ALJ</em>, <em>31</em>(3), 304-320. (<a href='https://doi.org/10.1162/artl_a_00468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Benefit Game 2.0, a multiscreen Artificial Life gameplay installation. Saccharina latissima , a seaweed species economically beneficial to humans but threatened by overexploitation, motivates the creation of this artwork. Technically, the authors create an underwater virtual ecosystem consisting of a seaweed swarm and symbiotic fungi, created using procedural content generation via machine learning and rule-based methods. Moreover, the work features a unique cybernetic loop structure, incorporating audience observation and game token interactions. This virtual system is also symbolically influenced in real time by indoor carbon dioxide measurements, serving as an artistic metaphor for the broader impacts of climate change. This integration with the physical game machine underscores the fragile relationship between human activities and the environment under severe global climate change and immerses the audience in the challenging balance between sustainability and profit seeking in this context.},
  archive      = {J_ALJ},
  author       = {Fei, Dan-Lu and Wu, Zi-Wei and Zhang, Kang},
  doi          = {10.1162/artl_a_00468},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {304-320},
  shortjournal = {Artif. Life},
  title        = {Benefit game 2.0: Alien seaweed Swarms—Exploring the interplay of human activity and environmental sustainability},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complexity, artificial life, and artificial intelligence. <em>ALJ</em>, <em>31</em>(3), 289-303. (<a href='https://doi.org/10.1162/artl_a_00462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scientific fields of complexity, Artificial Life (ALife), and artificial intelligence (AI) share commonalities: historic, conceptual, methodological, and philosophical. Although their origins trace back to the 1940s birth of cybernetics, they were able to develop properly only as modern information technology became available. In this perspective, I offer a personal (and thus biased) account of the expectations and limitations of these fields, some of which have their roots in the limits of formal systems. I use interactions, self-organization, emergence, and balance to compare different aspects of complexity, ALife, and AI. Even when the trajectory of the article is influenced by my personal experience, the general questions posed (which outweigh the answers) will, I hope, be useful in aligning efforts in these fields toward overcoming—or accepting—their limits.},
  archive      = {J_ALJ},
  author       = {Gershenson, Carlos},
  doi          = {10.1162/artl_a_00462},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {289-303},
  shortjournal = {Artif. Life},
  title        = {Complexity, artificial life, and artificial intelligence},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolvability in artificial development of large, complex structures and the principle of terminal addition. <em>ALJ</em>, <em>31</em>(3), 276-288. (<a href='https://doi.org/10.1162/artl_a_00460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epigenetic tracking (ET) is a model of development that is capable of generating diverse, arbitrary, complex three-dimensional cellular structures starting from a single cell. The generated structures have a level of complexity (in terms of the number of cells) comparable to multicellular biological organisms. In this article, we investigate the evolvability of the development of a complex structure inspired by the “French flag” problem: an “Italian Anubis” (a three-dimensional, doglike figure patterned in three colors). Genes during development are triggered in ET at specific developmental stages, and the fitness of individuals during simulated evolution is calculated after a certain stage. When this evaluation stage was allowed to evolve, genes that were triggered at later stages of development tended to be incorporated into the genome later during evolutionary runs. This suggests the emergence of the property of terminal addition in this system. When the principle of terminal addition was explicitly incorporated into ET, and was the sole mechanism for introducing morphological innovation, evolvability improved markedly, leading to the development of structures much more closely approximating the target at a much lower computational cost.},
  archive      = {J_ALJ},
  author       = {Fontana, Alessandro and Wróbel, Borys},
  doi          = {10.1162/artl_a_00460},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {276-288},
  shortjournal = {Artif. Life},
  title        = {Evolvability in artificial development of large, complex structures and the principle of terminal addition},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous evolution in the NK treadmill model. <em>ALJ</em>, <em>31</em>(3), 256-275. (<a href='https://doi.org/10.1162/artl_a_00467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The NK fitness landscape is a well-known model with which to study evolutionary dynamics in landscapes of different ruggedness. However, the model is static, and genomes are typically small, allowing observations over only a short adaptive period. Here we introduce an extension to the model that allows the experimenter to set the velocity at which the landscape changes independently from other parameters, such as the ruggedness or the mutation rate. We find that, similar to the previously observed complexity catastrophe, where evolution comes to a halt when environments become too complex due to overly high degrees of epistasis, here the same phenomenon occurs when changes happen too rapidly. Our expanded model also preserves essential properties of the static NK landscape, allowing for proper comparisons between static and dynamic landscapes.},
  archive      = {J_ALJ},
  author       = {Mehra, Priyanka and Hintze, Arend},
  doi          = {10.1162/artl_a_00467},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {256-275},
  shortjournal = {Artif. Life},
  title        = {Continuous evolution in the NK treadmill model},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neurons as autoencoders. <em>ALJ</em>, <em>31</em>(3), 250-255. (<a href='https://doi.org/10.1162/artl_c_00461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter presents the idea that neural backpropagation is exploiting dendritic processing to enable individual neurons to perform autoencoding. Using a very simple connection weight search heuristic and artificial neural network model, the effects of interleaving autoencoding for each neuron in a hidden layer of a feedforward network are explored. This is contrasted with the equivalent standard layered approach to autoencoding. It is shown that such individualized processing is not detrimental and can improve network learning.},
  archive      = {J_ALJ},
  author       = {Bull, Larry},
  doi          = {10.1162/artl_c_00461},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {250-255},
  shortjournal = {Artif. Life},
  title        = {Neurons as autoencoders},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A word from the editors. <em>ALJ</em>, <em>31</em>(3), 249. (<a href='https://doi.org/10.1162/ARTL.e.11'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue of contributed articles, we are pleased to share with you a diversity of ALife research.In his letter “Neurons as Autoencoders,” Bull brings together ideas from backpropagation, dendritic computing, the NK model, and autoencoders to augment standard models of neuronal autoencoding. The NK model reappears in the article “Continuous Evolution in the NK Treadmill Model,” in which Mehra and Hintze extend the originally static model to allow it to change in a parameterized manner and investigate how evolution behaves at different rates of change. In “Evolvability in Artificial Development of Large Complex Structures and the Principle of Terminal Addition,” Fontana and Wróbel examine evo-devo processes and the emergence of modification to the later stages of a developmental process.In a historical and philosophical perspective, Gershenson offers a personal account that compares various aspects of, and raises questions about, the closely related topics of “Complexity, Artificial Life, and Artificial Intelligence.”Games are an interesting subtopic in ALife, useful both for understanding complex systems and for outreach. In “Benefit Game 2.0: Alien Seaweed Swarms—Exploring the Interplay of Human Activity and Environmental Sustainability,” Fei, Wu, and Zhang present their immersive seaweed ecosystem simulation game that responds not only to audience input but also to the real-world environmental state. Moving from plant to animal behavior, in “Behaviour Diversity in a Walking and Climbing Centipede-Like Virtual Creature,” Norstein et al. take inspiration from arthropod multilegged locomotion to help develop robotic controllers robust to morphological and environmental variation.In their methodological article “Cognitive Distinctions as a Language for Cognitive Science: Comparing Methods of Description in a Model of Referential Communication,” Gaul and Izquierdo examine the effect of choices of language and terminology on theoretical frameworks in cognitive science and use a model to expose the consequences. In another methodological work, “Automating the Search for Artificial Life With Foundation Models,” Kumar et al. exploit concepts of artificial intelligence’s foundation models to develop a new approach to searching for lifelike behaviors, applied to a range of ALife simulations.We thank all the authors for the time and care they took to perform their research and present their work. We hope you enjoy the results!},
  archive      = {J_ALJ},
  author       = {Dorin, Alan and Stepney, Susan},
  doi          = {10.1162/ARTL.e.11},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {249},
  shortjournal = {Artif. Life},
  title        = {A word from the editors},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="coli">COLI - 9</h2>
<ul>
<li><details>
<summary>
(2025). Natural language processing RELIES on linguistics. <em>COLI</em>, <em>51</em>(3), 1009-1032. (<a href='https://doi.org/10.1162/coli_a_00560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES , which encapsulates six major facets where linguistics contributes to NLP: R esources, E valuation, L ow-resource settings, I nterpretability, E xplanation, and the S tudy of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-à-vis systems of human language.},
  archive      = {J_COLI},
  author       = {Opitz, Juri and Wein, Shira and Schneider, Nathan},
  doi          = {10.1162/coli_a_00560},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1009-1032},
  shortjournal = {Comput. Lingu.},
  title        = {Natural language processing RELIES on linguistics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated essay scoring. <em>COLI</em>, <em>51</em>(3), 1005-1008. (<a href='https://doi.org/10.1162/coli_r_00513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated essay scoring is concerned with the development of language technologies that make it possible for an essay—or any piece of writing for that matter—to be evaluated or scored by a computer. These technologies find their utility primarily in the context of educational measurement, where they serve a dual purpose. On the one hand, they provide crucial support to educators and institutions, facilitating the assessment of students’ writing skills and content knowledge. A good example is the TOEFL iBT®, the Internet-based Test of English as a Foreign Language, administered by the Educational Testing Service and widely adopted by institutions worldwide. On the other hand, these technologies benefit writers themselves, including students, by offering a platform to assess and enhance their writing skills. One illustrative tool for this purpose is the Write &amp; Improve software developed at the University of Cambridge.The field of automated essay scoring emerged in the pioneering era of artificial intelligence and computational linguistics, with the inception of Ellis Page’s Project Essay Grade system at the University of Connecticut in 1964, and the subsequent publication of his seminal article “The Imminence of…Grading Essays by Computer” in 1966. Page’s automated scoring system is seen by the authors of this book as one of the first concrete applications of natural language processing, after the Audrey system for speech recognition and the Georgetown–IBM demonstration of machine translation. But just like speech recognition and machine translation, the industrialization of automated essay scoring mostly gained momentum in the 1990s. This decade saw an increase in richly annotated language data and corpora, which enabled the use of statistical and supervised machine learning in developing essay-scoring systems. Pearson’s Intelligent Essay Assessor™, released in 1998, is a prominent example of this evolution. A year later, in 1999, the e-rater® scoring engine was launched by the Educational Testing Service, commonly abbreviated as ETS, a private organization overseeing several standardized tests and high-stakes examinations such as TOEFL® and GRE®.This book is written by Beata Beigman Klebanov and Nitin Madnani, two researchers at ETS with many years of research experience and numerous peer-reviewed publications in the field of automated essay scoring. Their book provides a concise yet indispensable introduction to the field. After this short introduction, the eager reader is invited to take a deep dive into the scientific literature, encompassing slightly over half of the book’s content, approximately 115 pages. The literature review primarily caters to computational linguists and NLP practitioners, as it delves comprehensively into diverse machine learning models—ranging from linear regression to artificial neural networks—and intricate linguistic features. These features include general indicators of writing quality, such as text organization, coherence, and grammaticality, as well as genre-specific features, such as argument structure in argumentative essays. Furthermore, the reader not only gains insight into the extensive research carried out at ETS throughout the years but also delves into their technical expertise through a series of guided experiments with RSMTool—an open-source tool developed by the second author and colleagues. In addition, the authors also provide insight into a scalable and production-ready computer architecture used to build ETS products such as c-rater™, e-rater®, Language Muse®, and Writing Mentor™.The book is divided into five parts counting 13 chapters in total. The first part contains an introductory chapter in which the authors introduce the reader to Page’s aforementioned seminal 1966 article. Chapter 1 enumerates several arguments made by Page in favor of automated essay scoring, including its educational need, computational feasibility, high quality, and low cost. The chapter then sets forth four challenges associated with automated essay scoring. Two among them are the evaluation of original and source-based writing. Original writing, which reflects an author’s unique voice and thus stands out from existing and conventional works, poses a challenge because its originality could be overlooked or even under-evaluated by a computer. Source-based writing, a form of writing that reviews main points from external sources, presents a different problem because the assessment focuses on the correctness of content rather than measuring writing skills and essay quality. Another challenge is avoiding potential gaming strategies that test takers may employ to inflate their scores. A final challenge is automated feedback, as the effectiveness of providing linguistic and stylistic commentary on written work is not always proven.The second part contains two chapters covering a series of guided experiments and a set of best practices when building an automated essay scoring system. Chapter 2 provides a step-by-step guide for building an automated scoring system with supervised machine learning. First, the reader learns the usual engineering setup, including the use of a standard dataset (viz., the Automated Student Assessment Prize competition), an interpretable machine learning model (viz., linear regression), and a set of basic features derived from a scoring rubric. The authors also introduce the reader to their RSMTool. Then, the reader is guided through a series of ten experiments illustrating the incremental development (experiments 1–5) and evaluation (experiments 6–10) of the machine learning model. In each experiment, the reader is taught an important lesson such as feature fairness. For each experiment, the authors refer the reader to a report (i.e., correction key) available online.Chapter 3 provides some best practices for building an automated essay scoring system. First, the authors identify potentially conflicting perspectives between NLP developers and other stakeholders, including end users, subject matter experts, and business units. The authors then describe three natural use cases for automatic essay scoring, where it is added to a pre-existing assessment, developed concurrently with a new assessment, or implemented in a classroom. The authors provide some practical considerations and concrete actions that are well thought out and will appeal to many different readers.The third part contains five chapters that provide an up-to-date overview of the scientific literature and a more detailed account of the concepts introduced in the previous chapters. Chapter 4 describes various statistical models, including linear regression, latent semantic analysis, support vector machines, random forests, ensemble methods, and neural networks. For each of these models, the authors describe their mathematical underpinnings and explain how they can be used to score an essay. The chapter pays special attention to recent deep-learning architectures for automated essay scoring.Chapters 5 and 6 describe computational features that capture various aspects of the writing construct and the scoring rubric. Chapter 5 deals with general features. The features are organized into three classes: discourse features aiming to capture essay organization, development, and coherence; content features related to vocabulary use and topicality; and conventional features based on grammatical error detection. Chapter 6 then gives an in-depth overview of computational features that pertain to specific writing genres. The chapter is focused on four genres: argumentative writing, narrative writing, source-based writing, and reflective writing. Argumentative writing involves defending a particular position on a given topic (e.g., why technology should be integrated into education), presenting several claims as to why this position is valid, and supporting these claims with premises and evidence. Narrative writing involves telling a story that describes, for example, the historical evolution of technology in education. Source-based writing involves summarizing and comparing key points from external sources to compose an informed essay, which, for example, reviews the effectiveness of integrating technology into education based on scholarly sources. Finally, reflective writing involves examining personal experiences, such as teachers describing their experiences integrating technology into the classroom and reflecting on important lessons learned from this experience. This detailed overview of different writing genres also interestingly introduces the reader to research from related fields, such as computational argumentation and text summarization.Chapters 7 and 8 address two concerns in setting up real-world applications of automated essay scoring. Chapter 7 deals with the issues of reliability, scalability, and flexibility when deploying a scoring system at a large scale. The chapter describes and illustrates an Apache Storm architecture implemented in several ETS systems. Chapter 8 is about evaluating construct validity and fairness. When deploying an essay scoring system for high-stakes testing, fundamental issues arise when the system fails to measure the construct, assigns scores influenced by factors irrelevant to the measured construct, or is biased towards specific personal characteristics in the population of test takers. If an essay scoring system overlooks important features or favors particular writing styles or cultural representations, it undermines the validity and fairness of high-stakes assessments.The fourth part contains four chapters examining some broader challenges introduced in the first chapter and which remain to be solved for automated scoring. Chapter 9 deals with the automated generation of useful feedback on writing. The authors review several existing feedback systems and discuss how to define and evaluate the usefulness of feedback. Chapter 10 focuses on evaluating essay content, which is separate from assessing essay quality. Content scoring emphasizes measuring test-takers’ content knowledge, prioritizing these elements over writing skills. This evaluation can adopt either a reference-based or a response-based approach. Reference-based scoring involves comparing responses to a set of predefined reference answers, while response-based scoring independently assesses the response content. The chapter primarily explores the latter, investigating computational features and models tailored for this approach. Chapter 11 deals with another task related to but different from essay scoring, namely the automated scoring of spontaneous speech. After a brief account of the challenges with automated speech recognition, the authors review three sets of features for speech scoring: the delivery and fluency of spontaneous speech, vocabulary and grammar use, and topic development. The authors also contrast features relevant for scoring speech with those relevant for scoring writing. Lastly, chapter 12 examines several gaming strategies test-takers could use to fool the automated scoring system into giving a higher score. The authors review four types of strategies: the unnecessary use of shell language, the artificial generation of essays, the submission of off-topic responses, and the use of canned responses or plagiarized essays.The fifth and final part of the book contains a concluding chapter. The authors revisit the desiderata put forth by Ellis Page in his 1966 publication and summarize the overall achievements and remaining challenges in this respect. In addition, the authors discuss other challenging aspects that Page did not envision, such as the present-day ubiquity of technology, dealing with multiple languages, and setting up high-stakes tests that are valid, defensible, and fair.In sum, the book offers an excellent introduction to and deepening of the field of automated essay scoring. The book is well-structured and easy to read. Throughout the book, the authors provide thoughtful insights and practical advice based on their many years of experience at ETS. Compared to other books on the subject, the book offers a valuable combination of practical lessons and scientific deepening. By the end of the book, the reader has acquired a broad knowledge of the possibilities, challenges, and practical concerns involved with the automated scoring of student writing.},
  archive      = {J_COLI},
  author       = {Tack, Anaïs},
  doi          = {10.1162/coli_r_00513},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1005-1008},
  shortjournal = {Comput. Lingu.},
  title        = {Automated essay scoring},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey of cultural awareness in language models: Text and beyond. <em>COLI</em>, <em>51</em>(3), 907-1004. (<a href='https://doi.org/10.1162/COLI.a.14'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive, going beyond multilinguality and building on findings from psychology and anthropology. In this article, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking definitions of culture from the anthropology and psychology literature as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of human–computer interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature. 1},
  archive      = {J_COLI},
  author       = {Pawar, Siddhesh and Park, Junyeong and Jin, Jiho and Arora, Arnav and Myung, Junho and Yadav, Srishti and Haznitrama, Faiz Ghifari and Song, Inhwa and Oh, Alice and Augenstein, Isabelle},
  doi          = {10.1162/COLI.a.14},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {907-1004},
  shortjournal = {Comput. Lingu.},
  title        = {Survey of cultural awareness in language models: Text and beyond},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models are biased because they are large language models. <em>COLI</em>, <em>51</em>(3), 885-906. (<a href='https://doi.org/10.1162/coli_a_00558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This position paper’s primary goal is to provoke thoughtful discussion about the relationship between bias and fundamental properties of large language models (LLMs). I do this by seeking to convince the reader that harmful biases are an inevitable consequence arising from the design of any large language model as LLMs are currently formulated. To the extent that this is true, it suggests that the problem of harmful bias cannot be properly addressed without a serious reconsideration of AI driven by LLMs, going back to the foundational assumptions underlying their design.},
  archive      = {J_COLI},
  author       = {Resnik, Philip},
  doi          = {10.1162/coli_a_00558},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {885-906},
  shortjournal = {Comput. Lingu.},
  title        = {Large language models are biased because they are large language models},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics. <em>COLI</em>, <em>51</em>(3), 843-883. (<a href='https://doi.org/10.1162/coli_a_00543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate two essential challenges in the context of hierarchical topic modeling (HTM)—(i) the impact of data representation and (ii) topic evaluation. The data representation directly influences the performance of the topic generation, and the impact of new representations such as contextual embeddings in this task has been under-investigated. Topic evaluation , responsible for driving the advances in the field, assesses the overall quality of the topic generation process. HTM studies exploit the exact topic modeling (TM) evaluation metrics as traditional TM to measure the quality of topics. One significant result of our work is demonstrating that the HTM’s hierarchical nature demands novel ways of evaluating the quality of topics. As our main contribution, we propose two new topic quality metrics to assess the topical quality of the hierarchical structures. Uniqueness considers topic topological consistency, while the Semantic Hierarchical Structure (SHS) captures the semantic relatedness of the hierarchies. We also present an additional advance to the state-of-the-art by proposing the c-CluHTM. To the best of our knowledge, c-CluHTM is the first method that exploits contextual embeddings into NMF in HTM tasks. c-CluHTM enhances the topics’ semantics while preserving the hierarchical structure. We perform an experimental evaluation, and our results demonstrate the superiority of our proposal with gains between 12% and 21%, regarding NPMI and Coherence over the best baselines. Regarding the newly proposed metrics, our results reveal that Uniqueness and SHS can capture relevant information about the structure of the hierarchical topics that traditional metrics cannot.},
  archive      = {J_COLI},
  author       = {Viegas, Felipe and Pereira, Antonio and Cunha, Washington and França, Celso and Andrade, Claudio and Tuler, Elisa and Rocha, Leonardo and Gonçalves, Marcos André},
  doi          = {10.1162/coli_a_00543},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {843-883},
  shortjournal = {Comput. Lingu.},
  title        = {Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The emergence of chunking structures with hierarchical RNN. <em>COLI</em>, <em>51</em>(3), 815-841. (<a href='https://doi.org/10.1162/coli_a_00545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Natural Language Processing (NLP), predicting linguistic structures, such as parsing and chunking, has mostly relied on manual annotations of syntactic structures. This article introduces an unsupervised approach to chunking, a syntactic task that involves grouping words in a non-hierarchical manner. We present a Hierarchical Recurrent Neural Network (HRNN) designed to model word-to-chunk and chunk-to-sentence compositions. Our approach involves a two-stage training process: pretraining with an unsupervised parser and finetuning on downstream NLP tasks. Experiments on multiple datasets reveal a notable improvement of unsupervised chunking performance in both pretraining and finetuning stages. Interestingly, we observe that the emergence of the chunking structure is transient during the neural model’s downstream-task training. This study contributes to the advancement of unsupervised syntactic structure discovery and opens avenues for further research in linguistic theory. 1},
  archive      = {J_COLI},
  author       = {Wu, Zijun and Deshmukh, Anup Anand and Wu, Yongkang and Lin, Jimmy and Mou, Lili},
  doi          = {10.1162/coli_a_00545},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {815-841},
  shortjournal = {Comput. Lingu.},
  title        = {The emergence of chunking structures with hierarchical RNN},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tokenization changes meaning in large language models: Evidence from chinese. <em>COLI</em>, <em>51</em>(3), 785-814. (<a href='https://doi.org/10.1162/coli_a_00557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models segment many words into multiple tokens, and there is mixed evidence as to whether tokenization affects how state-of-the-art models represent meanings. Chinese characters present an opportunity to investigate this issue: They contain semantic radicals, which often convey useful information; characters with the same semantic radical tend to begin with the same one or two bytes (when using UTF-8 encodings); and tokens are common strings of bytes, so characters with the same radical often begin with the same token. This study asked GPT-4, GPT-4o, and Llama 3 whether characters contain the same semantic radical, elicited semantic similarity ratings, and conducted odd-one-out tasks (i.e., which character is not like the others). In all cases, misalignment between tokens and radicals systematically corrupted representations of Chinese characters. In experiments comparing characters represented by single tokens to multi-token characters, the models were less accurate for single-token characters, which suggests that segmenting words into fewer, longer tokens obscures valuable information in word form and will not resolve the problems introduced by tokenization. In experiments with 12 European languages, misalignment between tokens and suffixes systematically corrupted categorization of words by all three models, which suggests that the tendency to treat malformed tokens like linguistic units is pervasive.},
  archive      = {J_COLI},
  author       = {Haslett, David A.},
  doi          = {10.1162/coli_a_00557},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {785-814},
  shortjournal = {Comput. Lingu.},
  title        = {Tokenization changes meaning in large language models: Evidence from chinese},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniASA: A unified generative framework for argument structure analysis. <em>COLI</em>, <em>51</em>(3), 739-784. (<a href='https://doi.org/10.1162/coli_a_00553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation is a fundamental human activity that involves reasoning and persuasion, which also serves as the basis for the development of AI systems capable of complex reasoning. In NLP, to better understand human argumentation, argument structure analysis aims to identify argument components, such as claims and premises, and their relations from free text. It encompasses a variety of divergent tasks, such as end-to-end argument mining, argument pair extraction, and argument quadruplet extraction. Existing methods are usually tailored to only one specific argument structure analysis task, overlooking the inherent connections among different tasks. We observe that the fundamental goal of these tasks is similar: identifying argument components and their interrelations. Motivated by this, we present a unified generative framework for argument structure analysis (UniASA). It can uniformly address multiple argument structure analysis tasks in a sequence-to-sequence manner. Further, we enhance UniASA with a multi-view learning strategy based on subtask decomposition. We conduct experiments on seven datasets across three tasks. The results indicate that UniASA can address these tasks uniformly and achieve performance that is either superior to or comparable with the previous state-of-the-art methods. Also, we show that UniASA can be effectively integrated with large language models, such as Llama, through fine-tuning or in-context learning.},
  archive      = {J_COLI},
  author       = {Bao, Jianzhu and Jing, Mohan and Dong, Kuicai and Sun, Aixin and Sun, Yang and Xu, Ruifeng},
  doi          = {10.1162/coli_a_00553},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {739-784},
  shortjournal = {Comput. Lingu.},
  title        = {UniASA: A unified generative framework for argument structure analysis},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graded suspiciousness of adversarial texts to humans. <em>COLI</em>, <em>51</em>(3), 705-738. (<a href='https://doi.org/10.1162/coli_a_00555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples pose a significant challenge to deep neural networks across both image and text domains, with the intent to degrade model performance through carefully altered inputs. Adversarial texts, however, are distinct from adversarial images due to their requirement for semantic similarity and the discrete nature of the textual contents. This study delves into the concept of human suspiciousness, a quality distinct from the traditional focus on imperceptibility found in image-based adversarial examples, where adversarial changes are often desired to be indistinguishable to the human eye even when placed side by side with originals. Although this is generally not possible with text, textual adversarial content must still often remain undetected or non-suspicious to human readers. Even when the text’s purpose is to deceive NLP systems or bypass filters, the text is often expected to be natural to read. In this research, we expand the study of human suspiciousness by analyzing how individuals perceive adversarial texts. We gather and publish a novel dataset of Likert-scale human evaluations on the suspiciousness of adversarial sentences, crafted by four widely used adversarial attack methods and assess their correlation with the human ability to detect machine-generated alterations. Additionally, we develop a regression-based model to predict levels of suspiciousness and establish a baseline for future research in reducing the suspiciousness in adversarial text generation. We also demonstrate how the regressor-generated suspicious scores can be incorporated into adversarial generation methods to produce texts that are less likely to be perceived as computer-generated.},
  archive      = {J_COLI},
  author       = {Tonni, Shakila Mahjabin and Faustini, Pedro and Dras, Mark},
  doi          = {10.1162/coli_a_00555},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {705-738},
  shortjournal = {Comput. Lingu.},
  title        = {Graded suspiciousness of adversarial texts to humans},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ecj">ECJ - 5</h2>
<ul>
<li><details>
<summary>
(2025). On the use of the doubly stochastic matrix models for the quadratic assignment problem. <em>ECJ</em>, <em>33</em>(3), 425-457. (<a href='https://doi.org/10.1162/evco_a_00369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Permutation problems have captured the attention of the combinatorial optimization community for decades due to the challenge they pose. Although their solutions are naturally encoded as permutations, in each problem, the information to be used to optimize them can vary substantially. In this paper, we consider the Quadratic Assignment Problem (QAP) as a case study, and propose using Doubly Stochastic Matrices (DSMs) under the framework of Estimation of Distribution Algorithms. To that end, we design efficient learning and sampling schemes that enable an effective iterative update of the probability model. Conducted experiments on commonly adopted benchmarks for the QAP prove doubly stochastic matrices to be preferred to the other four models for permutations, both in terms of effectiveness and computational efficiency. Moreover, additional analyses performed on the structure of the QAP and the Linear Ordering Problem (LOP) show that DSMs are good to deal with assignment problems, but they have interesting capabilities to deal also with ordering problems such as the LOP. The paper concludes with a description of the potential uses of DSMs for other optimization paradigms, such as genetic algorithms or model-based gradient search.},
  archive      = {J_ECJ},
  author       = {Santucci, Valentino and Ceberio, Josu},
  doi          = {10.1162/evco_a_00369},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {425-457},
  shortjournal = {Evol. Comput.},
  title        = {On the use of the doubly stochastic matrix models for the quadratic assignment problem},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P-NP instance decomposition based on the fourier transform for solving the linear ordering problem. <em>ECJ</em>, <em>33</em>(3), 395-423. (<a href='https://doi.org/10.1162/evco_a_00368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fourier transform over finite groups has proved to be a useful tool for analyzing combinatorial optimization problems. However, few heuristic and metaheuristic algorithms have been proposed in the literature that utilize the information provided by this technique to guide the search process. In this work, we attempt to address this research gap by considering the case study of the Linear Ordering Problem (LOP). Based on the Fourier transform, we propose an instance decomposition strategy that divides any LOP instance into the sum of two LOP instances associated with a P and an NP-Hard optimization problem. By linearly aggregating the instances obtained from the decomposition, it is possible to create artificial instances with modified proportions of the P and NP-Hard components. Conducted experiments show that increasing the weight of the P component leads to a less rugged fitness landscape suitable for local search-based optimization. We take advantage of this phenomenon by presenting a new metaheuristic algorithm called P-Descent Search (PDS). The proposed method, first, optimizes a surrogate instance with a high proportion of the P component, and then, gradually increases the weight of the NP-Hard component until the original instance is reached. The multi-start version of PDS shows a promising and predictable performance that appears to be correlated to specific characteristics of the problem, which could open the door to an automatic tuning of its hyperparameters.},
  archive      = {J_ECJ},
  author       = {Benavides, Xabier and Hernando, Leticia and Ceberio, Josu and Lozano, Jose A.},
  doi          = {10.1162/evco_a_00368},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {395-423},
  shortjournal = {Evol. Comput.},
  title        = {P-NP instance decomposition based on the fourier transform for solving the linear ordering problem},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing monotone chance-constrained submodular functions using evolutionary multiobjective algorithms. <em>ECJ</em>, <em>33</em>(3), 363-393. (<a href='https://doi.org/10.1162/evco_a_00360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world optimization problems can be stated in terms of submodular functions. Furthermore, these real-world problems often involve uncertainties which may lead to the violation of given constraints. A lot of evolutionary multiobjective algorithms following the Pareto optimization approach have recently been analyzed and applied to submodular problems with different types of constraints. We present a first runtime analysis of evolutionary multiobjective algorithms based on Pareto optimization for chance-constrained submodular functions. Here the constraint involves stochastic components and the constraint can only be violated with a small probability of α ⁠ . We investigate the classical GSEMO algorithm for two different bi-objective formulations using tail bounds to determine the feasibility of solutions. We show that the algorithm GSEMO obtains the same worst case performance guarantees for monotone submodular functions as recently analyzed greedy algorithms for the case of uniform IID weights and uniformly distributed weights with the same dispersion when using the appropriate bi-objective formulation. As part of our investigations, we also point out situations where the use of tail bounds in the first bi-objective formulation can prevent GSEMO from obtaining good solutions in the case of uniformly distributed weights with the same dispersion if the objective function is submodular but non-monotone due to a single element impacting monotonicity. Furthermore, we investigate the behavior of the evolutionary multiobjective algorithms GSEMO, NSGA-II, and SPEA2 on different submodular chance-constrained network problems. Our experimental results show that the use of evolutionary multiobjective algorithms leads to significant performance improvements compared to state-of-the-art greedy algorithms for submodular optimization.},
  archive      = {J_ECJ},
  author       = {Neumann, Aneta and Neumann, Frank},
  doi          = {10.1162/evco_a_00360},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {363-393},
  shortjournal = {Evol. Comput.},
  title        = {Optimizing monotone chance-constrained submodular functions using evolutionary multiobjective algorithms},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Genetic programming for automatically evolving multiple features to classification. <em>ECJ</em>, <em>33</em>(3), 335-362. (<a href='https://doi.org/10.1162/evco_a_00359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing classification on high-dimensional data poses a significant challenge due to the huge search space. Moreover, complex feature interactions introduce an additional obstacle. The problems can be addressed by using feature selection to select relevant features or feature construction to construct a small set of high-level features. However, performing feature selection or feature construction might only make the feature set suboptimal. To remedy this problem, this study investigates the use of genetic programming for simultaneous feature selection and feature construction in addressing different classification tasks. The proposed approach is tested on 16 datasets and compared with seven methods including both feature selection and feature construction techniques. The results show that the obtained feature sets with the constructed and/or selected features can significantly increase the classification accuracy and reduce the dimensionality of the datasets. Further analysis reveals the complementarity of the obtained features leading to the promising classification performance of the proposed method.},
  archive      = {J_ECJ},
  author       = {Wang, Peng and Xue, Bing and Liang, Jing and Zhang, Mengjie},
  doi          = {10.1162/evco_a_00359},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {335-362},
  shortjournal = {Evol. Comput.},
  title        = {Genetic programming for automatically evolving multiple features to classification},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogates of scalarization functions. <em>ECJ</em>, <em>33</em>(3), 309-334. (<a href='https://doi.org/10.1162/evco_a_00354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, computationally intensive multiobjective optimization problems have been efficiently solved by surrogate-assisted multiobjective evolutionary algorithms. However, most of those algorithms can handle no more than 200 decision variables. As the number of decision variables increases further, unreliable surrogate models will result in a dramatic deterioration of their performance, which makes large-scale expensive multiobjective optimization challenging. To address this challenge, we develop a large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogate models of scalarization functions. The proposed algorithm (termed LDS-AF) reduces the dimension of the original decision space based on principal component analysis, and then directly approximates the scalarization functions in a decomposition-based multiobjective evolutionary algorithm. With the help of a two-stage modeling strategy and convergence control strategy, LDS-AF can keep a good balance between convergence and diversity, and achieve a promising performance without being trapped in a local optimum prematurely. The experimental results on a set of test instances have demonstrated its superiority over eight state-of-the-art algorithms on multiobjective optimization problems with up to 1,000 decision variables using only 500 real function evaluations.},
  archive      = {J_ECJ},
  author       = {Gu, Haoran and Wang, Handing and He, Cheng and Yuan, Bo and Jin, Yaochu},
  doi          = {10.1162/evco_a_00354},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {309-334},
  shortjournal = {Evol. Comput.},
  title        = {Large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogates of scalarization functions},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jmlr">JMLR - 1</h2>
<ul>
<li><details>
<summary>
(2025). Linear separation capacity of self-supervised representation learning. <em>JMLR</em>, <em>26</em>(194), 1-48. (<a href='https://jmlr.org/papers/v26/24-2032.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. Our theoretical analysis further underscores that the performance of downstream linear classifiers primarily hinges on the linear separability of data representations rather than the size of the labeled data set, reaffirming the viability of constructing efficient classifiers with limited labeled data amid an expansive unlabeled data set.},
  archive      = {J_JMLR},
  author       = {Shulei Wang},
  journal      = {Journal of Machine Learning Research},
  number       = {194},
  pages        = {1-48},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Linear separation capacity of self-supervised representation learning},
  url          = {https://jmlr.org/papers/v26/24-2032.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="neco">NECO - 6</h2>
<ul>
<li><details>
<summary>
(2025). Sequential learning in the dense associative memory. <em>NECO</em>, <em>37</em>(10), 1877-1924. (<a href='https://doi.org/10.1162/neco.a.20'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential learning involves learning tasks in a sequence and proves challenging for most neural networks. Biological neural networks regularly succeed at the sequential learning challenge and are even capable of transferring knowledge both forward and backward between tasks. Artificial neural networks often totally fail to transfer performance between tasks and regularly suffer from degraded performance or catastrophic forgetting on previous tasks. Models of associative memory have been used to investigate the discrepancy between biological and artificial neural networks due to their biological ties and inspirations, of which the Hopfield network is the most studied model. The dense associative memory (DAM), or modern Hopfield network, generalizes the Hopfield network, allowing for greater capacities and prototype learning behaviors while still retaining the associative memory structure. We give a substantial review of the sequential learning space with particular respect to the Hopfield network and associative memories. We present the first published benchmarks of sequential learning in the DAM using various sequential learning techniques and analyze the results of the sequential learning to demonstrate previously unseen transitions in the behavior of the DAM. This letter also discusses the departure from biological plausibility that may affect the utility of the DAM as a tool for studying biological neural networks. We present our findings, including the effectiveness of a range of state-of-the-art sequential learning methods when applied to the DAM, and use these methods to further the understanding of DAM properties and behaviors.},
  archive      = {J_NECO},
  author       = {McAlister, Hayden and Robins, Anthony and Szymanski, Lech},
  doi          = {10.1162/neco.a.20},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1877-1924},
  shortjournal = {Neural Comput.},
  title        = {Sequential learning in the dense associative memory},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distance-based logistic matrix factorization. <em>NECO</em>, <em>37</em>(10), 1863-1876. (<a href='https://doi.org/10.1162/neco.a.25'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix factorization is a central paradigm in matrix completion and collaborative filtering. Low-rank factorizations have been extremely successful in reconstructing and generalizing high-dimensional data in a wide variety of machine learning problems from drug-target discovery to music recommendations. Virtually all proposed matrix factorization techniques use the dot product between latent factor vectors to reconstruct the original matrix. We propose a reformulation of the widely used logistic matrix factorization in which we use the distance, rather than the dot product, to measure similarity between latent factors. We show that this measure of similarity, which can draw nonlinear decision boundaries and respect triangle inequalities between points, has more expressive power and modeling capacity. The distance-based model implemented in Euclidean and hyperbolic space outperforms previous formulations of logistic matrix factorization on three different biological test problems with disparate structure and statistics. In particular, we show that a distance-based factorization (1) generalizes better to test data, (2) achieves optimal performance at lower factor space dimension, and (3) clusters data better in the latent factor space.},
  archive      = {J_NECO},
  author       = {Praturu, Anoop and Sharpee, Tatyana O.},
  doi          = {10.1162/neco.a.25},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1863-1876},
  shortjournal = {Neural Comput.},
  title        = {Distance-based logistic matrix factorization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid reweighting of sensory inputs and predictions in visual perception. <em>NECO</em>, <em>37</em>(10), 1853-1862. (<a href='https://doi.org/10.1162/neco.a.26'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A striking perceptual phenomenon has recently been described wherein people report seeing abrupt jumps in the location of a smoothly moving object (“position resets”). Here, we show that this phenomenon can be understood within the framework of recursive Bayesian estimation as arising from transient gain changes, temporarily prioritizing sensory input over predictive beliefs. From this perspective, position resets reveal a capacity for rapid adaptive precision weighting in human visual perception and offer a possible test bed within which to study the timing and flexibility of sensory gain control.},
  archive      = {J_NECO},
  author       = {Turner, William and Kwon, Oh-Sang and Kim, Minwoo J.B. and Hogendoorn, Hinze},
  doi          = {10.1162/neco.a.26},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1853-1862},
  shortjournal = {Neural Comput.},
  title        = {Rapid reweighting of sensory inputs and predictions in visual perception},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer models for signal processing: Scaled dot-product attention implements constrained filtering. <em>NECO</em>, <em>37</em>(10), 1839-1852. (<a href='https://doi.org/10.1162/neco.a.29'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable success of the transformer machine learning architecture for processing language sequences far exceeds the performance of classical signal processing methods. A unique component of transformer models is the scaled dot-product attention (SDPA) layer, which does not appear to have an analog in prior signal processing algorithms. Here, we show that SDPA operates using a novel principle that projects the current state estimate onto the space spanned by prior estimates. We show that SDPA, when used for causal recursive state estimation, implements constrained state estimation in circumstances where the constraint is unknown and may be time varying. Since constraints in high-dimensional space may represent the complex relationships that define nonlinear signals and models, this suggests that the SDPA layer and transformer models leverage constrained estimation to achieve their success. This also suggests that transformers and the SPDA layer could be a computational model for previously unexplained capabilities of human behavior.},
  archive      = {J_NECO},
  author       = {Sanger, Terence D.},
  doi          = {10.1162/neco.a.29},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1839-1852},
  shortjournal = {Neural Comput.},
  title        = {Transformer models for signal processing: Scaled dot-product attention implements constrained filtering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Firing rate models as associative memory: Synaptic design for robust retrieval. <em>NECO</em>, <em>37</em>(10), 1807-1838. (<a href='https://doi.org/10.1162/neco.a.28'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firing rate models are dynamical systems widely used in applied and theoretical neuroscience to describe local cortical dynamics in neuronal populations. By providing a macroscopic perspective of neuronal activity, these models are essential for investigating oscillatory phenomena, chaotic behavior, and associative memory processes. Despite their widespread use, the application of firing rate models to associative memory networks has received limited mathematical exploration, and most existing studies are focused on specific models. Conversely, well-established associative memory designs, such as Hopfield networks, lack key biologically relevant features intrinsic to firing rate models, including positivity and interpretable synaptic matrices reflecting the action of long-term potentiation and long-term depression. To address this gap, we propose a general framework that ensures the emergence of rescaled memory patterns as stable equilibria in the firing rate dynamics. Furthermore, we analyze the conditions under which the memories are locally and globally asymptotically stable, providing insights into constructing biologically plausible and robust systems for associative memory retrieval.},
  archive      = {J_NECO},
  author       = {Betteti, Simone and Baggio, Giacomo and Bullo, Francesco and Zampieri, Sandro},
  doi          = {10.1162/neco.a.28},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1807-1838},
  shortjournal = {Neural Comput.},
  title        = {Firing rate models as associative memory: Synaptic design for robust retrieval},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity deconstrains component limitations in sensorimotor control. <em>NECO</em>, <em>37</em>(10), 1783-1806. (<a href='https://doi.org/10.1162/neco.a.24'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human sensorimotor control is remarkably fast and accurate at the system level despite severe speed-accuracy trade-offs at the component level. The discrepancy between the contrasting speed-accuracy trade-offs at these two levels is a paradox. Meanwhile, speed accuracy trade-offs, heterogeneity, and layered architectures are ubiquitous in nerves, skeletons, and muscles, but they have only been studied in isolation using domain-specific models. In this article, we develop a mechanistic model for how component speed-accuracy trade-offs constrain sensorimotor control that is consistent with Fitts’ law for reaching. The model suggests that diversity among components deconstrains the limitations of individual components in sensorimotor control. Such diversity-enabled sweet spots (DESSs) are ubiquitous in nature, explaining why large heterogeneities exist in the components of biological systems and how natural selection routinely evolves systems with fast and accurate responses using imperfect components.},
  archive      = {J_NECO},
  author       = {Nakahira, Yorie and Liu, Quanying and Deng, Xiyu and Sejnowski, Terrence J. and Doyle, John C.},
  doi          = {10.1162/neco.a.24},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1783-1806},
  shortjournal = {Neural Comput.},
  title        = {Diversity deconstrains component limitations in sensorimotor control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="netn">NETN - 9</h2>
<ul>
<li><details>
<summary>
(2025). Alterations in topology, cost, and dynamics of gamma-band EEG functional networks in a preclinical model of traumatic brain injury. <em>NETN</em>, <em>9</em>(3), 1013-1038. (<a href='https://doi.org/10.1162/netn.a.21'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traumatic brain injury (TBI) is a major cause of disability leading to multiple sequelae in cognitive, sensory, and physical domains, including posttraumatic epilepsy. Despite extensive research, our understanding of its impact on macroscopic brain circuitry remains incomplete. We analyzed electrophysiological functional connectomes in the gamma band from an animal model of blast-induced TBI over multiple time points after injury. We revealed differences in small-world propensity and rich-club structure compared with age-matched controls, indicating functional reorganization following injury. We further investigated cost-efficiency trade-offs, propose a computationally efficient normalization procedure for quantifying the cost of spatially embedded networks that controls for connectivity strength differences, and observed dynamic changes across the injury timeline. To explore potential links between altered network topology and epileptic activity, we employed a brain-wide computational model of seizure dynamics and attribute brain reorganization to a homeostatic mechanism of activity regulation with the potential unintended consequence of driving generalized seizures. Finally, we demonstrated post-injury hyperexcitability that manifests as an increase in sound-evoked response amplitudes at the cortical level. Our work characterizes, for the first time, gamma-band functional network reorganization in a model of brain injury and proposes potential causes of these changes, thus identifying targets for future therapeutic interventions. Traumatic brain injury (TBI) is a prevalent neurological disorder caused by factors such as accidents, contact sports, or military conflict. While animal studies have provided detailed insights into the molecular and cellular mechanisms underlying TBI, and clinical research has revealed its effects on large-scale brain network organization, these findings often lack integration. Our research seeks to address this gap by examining large-scale brain activity in an animal model of TBI. By analyzing brain network structures, we identify reorganization, highlighting novel connections to weight gain and posttraumatic epilepsy. Further exploration using this model may deepen our understanding of cross-scale mechanisms and inform the development of therapeutic interventions.},
  archive      = {J_NETN},
  author       = {Tsikonofilos, Konstantinos and Bruyns-Haylett, Michael and May, Hazel G. and Donat, Cornelius K. and Kozlov, Andriy S.},
  doi          = {10.1162/netn.a.21},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {1013-1038},
  shortjournal = {Netw. Neuroscience},
  title        = {Alterations in topology, cost, and dynamics of gamma-band EEG functional networks in a preclinical model of traumatic brain injury},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Idiosyncrasy and generalizability of contraceptive- and hormone-related functional connectomes across the menstrual cycle. <em>NETN</em>, <em>9</em>(3), 990-1012. (<a href='https://doi.org/10.1162/netn.a.20'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroendocrinology has received little attention in human neuroscience research, resulting in a dearth of knowledge surrounding potent and dynamic modulators of cognition and behavior, as well as brain structure and function. This work addresses one such phenomenon by studying functional connectomics related to ovarian hormone fluctuations throughout the adult menstrual cycle. To do so, we used fMRI and hormone assessments from two dense, longitudinal datasets to assess variations in functional connectivity with respect to endogenous and exogenous endocrine factors throughout the menstrual cycle. First, we replicated prior findings that common, group-level, and individual-specific factors have similar relative contributions to functional brain network organization. Second, we found widespread connectivity related to hormonal contraceptive (HC) use, in addition to sparser estradiol- and progesterone-related connectivity. Differential generalizability of these connectivity patterns suggests progestin-specific impacts on functional brain organization in HC users. These results provide novel insight into within-individual changes in brain organization across the menstrual cycle and the extent to which these changes are shared between individuals, illuminating understudied phenomena in reproductive health and important information for all neuroimaging studies that include participants who menstruate. Endocrine modulation of brain function across the menstrual cycle is poorly understood. Human neuroimaging research on the menstrual cycle has long relied on group differences and/or coarse, within-individual cycle stage differences, overlooking considerable individual differences in brain organization, the menstrual cycle, and hormone concentrations. Here, we take a multidataset approach to identify the idiosyncratic contraceptive- and hormone-related functional connectivity from within-individual neuroendocrine dynamics and then test the generalizability of this connectivity to other individuals. In doing so, we identified idiosyncratic hormone-responsive functional connectivity that is somewhat generalizable to other individuals, although this generalizability is complicated by hormonal contraceptive use, potentially reflecting differential connectivity between contraceptive formulations. Thus, this work illuminates individual similarities and differences in neuroendocrine dynamics across the menstrual cycle.},
  archive      = {J_NETN},
  author       = {Bottenhorn, Katherine L. and Salo, Taylor and Jacobs, Emily G. and Pritschet, Laura and Taylor, Caitlin and Herting, Megan M. and Laird, Angela R.},
  doi          = {10.1162/netn.a.20},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {990-1012},
  shortjournal = {Netw. Neuroscience},
  title        = {Idiosyncrasy and generalizability of contraceptive- and hormone-related functional connectomes across the menstrual cycle},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure–function coupling using fixel-based analysis and functional magnetic resonance imaging in alzheimer’s disease and mild cognitive impairment. <em>NETN</em>, <em>9</em>(3), 969-989. (<a href='https://doi.org/10.1162/netn_a_00461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional MRI (fMRI) and diffusion-weighted imaging (DWI) help explore correlations between structural connectivity (SC) and functional connectivity (FC; SC–FC coupling). Studies on mild cognitive impairment (MCI) and Alzheimer’s disease (AD) observed coupling disruptions, co-occurring with cognitive decline. Advanced “fixel-based” analyses improved DWI’s accuracy in assessing microstructural and macrostructural features of white matter (WM), but previous aging coupling studies commonly defined SC via tensor-based tractography and streamline counts, thereby missing fiber-specific information. We investigated different types of fixel-FC coupling and their relation to cognition in 392 participants (Age mean = 73; 207 females) from the ADNI. Two hundred twenty-five controls, 142 MCI, and 25 AD with diffusion-weighted and resting-state fMRI scans were analyzed. Structural connectomes were constructed using average fixel metrics (fiber density (FD), fiber-bundle cross-section log, and combined [FDC]) as edges. SC–FC coupling for each SC metric was calculated at overall network, edge, and node levels. Overall DMN, node- and edge-specific coupling differences were found across SC measures and groups. DMN nodal coupling significantly predicted Mini-Mental Status Examination score and verbal memory. In conclusion, different types of fixel-based coupling alterations can be observed across the neurocognitive aging spectrum, in particular, FD–FC and FDC–FC coupling between DMN regions are associated with cognitive functioning. Structure–function coupling is the correlation between white matter structures connecting pairs of brain regions (structural connectivity) and their concomitant brain activity (functional connectivity). “fixel-based” diffusion-weighted imaging techniques improve the accuracy of white matter structure estimation and provide information on microstructural and macrostructural changes to brain networks. Using structural connectomes based on fixel-based weights, coupling was found to be altered in Alzheimer’s disease and mild cognitive impairment, especially in the default mode network. Default mode network coupling may be relevant to verbal memory and cognitive decline across aging, particularly for coupling driven by microstructural features of white matter.},
  archive      = {J_NETN},
  author       = {Billaud, Charly Hugo Alexandre and Yu, Junhong and for the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1162/netn_a_00461},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {969-989},
  shortjournal = {Netw. Neuroscience},
  title        = {Structure–function coupling using fixel-based analysis and functional magnetic resonance imaging in alzheimer’s disease and mild cognitive impairment},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metastable dynamics emerge from local excitatory–inhibitory homeostasis in the cortex at rest. <em>NETN</em>, <em>9</em>(3), 938-968. (<a href='https://doi.org/10.1162/netn_a_00460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamics of the human cortex are highly metastable, driving the spontaneous exploration of network states. This metastability depends on circuit-level edge-of-bifurcation dynamics, which emerge from firing-rate control through multiple mechanisms of excitatory–inhibitory (E–I) homeostasis. However, it is unclear how these contribute to the metastability of cortical networks. We propose that individual mechanisms of the E–I homeostasis contribute uniquely to the emergence of resting-state dynamics and test this hypothesis in a large-scale model of the human cortex. We show that empirical connectivity and dynamics can only be reproduced when accounting for multiple mechanisms of the E–I homeostasis. More specifically, while the homeostasis of excitation and inhibition enhances metastability, the regulation of intrinsic excitability ensures moderate synchrony, maximizing functional complexity. Furthermore, the modulation bifurcation modulation by the homeostasis of excitation and intrinsic excitability compensates for strong input fluctuations in connector hubs. Importantly, this only occurs in models accounting for local gamma oscillations, suggesting a relationship between E–I balance, gamma rhythms, and metastable dynamics. Altogether, our results show that cortical networks self-organize toward maximal metastability through the multifactor homeostasis of E–I balance. Therefore, the benefits of combining multiple homeostatic mechanisms transcend the circuit level, supporting the metastable dynamics of large-scale cortical networks. Experimental studies have consistently shown that cortical circuits maintain a precise homeostasis of excitatory–inhibitory (E–I) balance, thereby optimizing local dynamics. While it is well established that multiple homeostatic mechanisms are involved in this local regulation, it remains unclear how each contributes to the large-scale dynamics of cortical networks. This study presents evidence that, through the E–I homeostasis, the cortex can self-organize toward a regime of highly complex and metastable spontaneous dynamics. Crucially, we demonstrate that this results from the synergistic action of multiple homeostatic mechanisms. Our findings advance our understanding of the E–I homeostasis as a process of self-organization, demonstrating its key role in the maintenance of metastable dynamics in large-scale cortical networks.},
  archive      = {J_NETN},
  author       = {Páscoa dos Santos, Francisco and Verschure, Paul F. M. J.},
  doi          = {10.1162/netn_a_00460},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {938-968},
  shortjournal = {Netw. Neuroscience},
  title        = {Metastable dynamics emerge from local excitatory–inhibitory homeostasis in the cortex at rest},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain connectome from neuronal morphology. <em>NETN</em>, <em>9</em>(3), 913-937. (<a href='https://doi.org/10.1162/netn_a_00458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-subject morphological brain networks derived from cross-feature correlation of macroscopic MRI-derived morphological measures provide an important means for studying the brain connectome. However, the validity of this approach remains to be confirmed at the microscopic level. Here, we constructed morphological brain networks at the single-cell level by extending features from macroscopic morphological measures to microscopic descriptions of neuronal morphology. We demonstrated the feasibility and generalizability of the method using neurons in the somatosensory cortex of a rat, neurons over the whole brain of a mouse, and neurons in the middle temporal gyrus (MTG) of a human. We found that interneuron morphological similarity was higher for intra- than interclass connections, depended on cytoarchitectonic, chemoarchitectonic, and laminar classification of neurons (rat), differed between regions with different evolutionary timelines (mouse), and correlated with neuronal axonal projections (mouse). Furthermore, highly connected hub neurons were disproportionately from superficial layers (rat), inhibitory neurons (rat), and subcortical regions (mouse), and exhibited unique morphology. Finally, we demonstrated a more segregated, less integrated, and economic network architecture with worse resistance to targeted attacks for neurons in human MTG than neurons in a mouse’s primary visual cortex. Overall, our method provides an alternative avenue to study neuronal wiring diagrams in brains. The brain is a highly complex network spanning multiple spatial scales, yet the organization of brain networks at the single-cell level remains poorly understood. Here, we constructed microscopic morphological brain networks by assessing interneuron similarity based on neuronal morphology for different species. We found that interneuron morphological similarity was correlated with neuronal axonal projections, dependent on neuronal affiliation with respect to cellular and molecular architecture, laminar positioning, and brain area location, and capable of uncovering cross-species differences. Our method complements existing methodology aimed at mapping wiring diagrams in brains at the microscopic level.},
  archive      = {J_NETN},
  author       = {Jin, Suhui and Li, Junle and Wang, Jinhui},
  doi          = {10.1162/netn_a_00458},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {913-937},
  shortjournal = {Netw. Neuroscience},
  title        = {Brain connectome from neuronal morphology},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly estimating individual and group networks from fMRI data. <em>NETN</em>, <em>9</em>(3), 896-912. (<a href='https://doi.org/10.1162/netn_a_00457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In fMRI research, graphical models are used to uncover complex patterns of relationships between brain regions. Connectivity-based fMRI studies typically analyze nested data; raw observations, for example, BOLD responses, are nested within participants, which are nested within populations, for example, healthy controls. Often, studies ignore the nested structure and analyze participants either individually or in aggregate. This overlooks the distinction between within-participant and between-participant variance, which can lead to poor generalizability of results because group-level effects do not necessarily reflect effects for each member of the group and, at worst, risk paradoxical results where group-level effects are opposite to individual-level effects (e.g., Kievit, Frankenhuis, Waldorp, & Borsboom, 2013 ; Robinson, 2009 ; Simpson, 1951 ). To address these concerns, we propose a multilevel approach to model the fMRI networks, using a Gaussian graphical model at the individual level and a Curie-Weiss graphical model at the group level. Simulations show that our method outperforms individual or aggregate analysis in edge retrieval. We apply the proposed multilevel approach to resting-state fMRI data of 724 healthy participants, examining both their commonalities and individual differences. We not only recover the seven previously found resting-state networks at the group level but also observe considerable heterogeneity in the individual-level networks. Finally, we discuss the necessity of a multilevel approach, additional challenges, and possible future extensions.},
  archive      = {J_NETN},
  author       = {van den Bergh, Don and Douw, Linda and van der Pal, Zarah and Blanken, Tessa F. and Schrantee, Anouk and Marsman, Maarten},
  doi          = {10.1162/netn_a_00457},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {896-912},
  shortjournal = {Netw. Neuroscience},
  title        = {Jointly estimating individual and group networks from fMRI data},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The exponential distance rule-based network model predicts topology and reveals functionally relevant properties of the drosophila projectome. <em>NETN</em>, <em>9</em>(3), 869-895. (<a href='https://doi.org/10.1162/netn_a_00455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying structural brain networks has witnessed significant advancement in recent decades. Findings revealed a geometric principle, the exponential distance rule (EDR) showing that the number of neurons decreases exponentially with the length of their axons. This neuron-level information was used to build a region-level EDR network model that was able to explain various characteristics of interareal cortical networks in macaques, mice, and rats. The complete connectome of the Drosophila has recently been mapped providing information also about the network of neuropils (projectome). A recent study demonstrated the presence of the EDR in the Drosophila . In our study, we first revisit the EDR itself and precisely measure the characteristic decay rate. Next, we demonstrate that the EDR model effectively accounts for numerous binary and weighted properties of the projectome. Our study illustrates that the EDR model is a suitable null model for analyzing networks of brain regions, as it captures properties of region-level networks in very different species. The importance of the null model lies in its ability to facilitate the identification of functionally significant features not caused by inevitable geometric constraints, as we illustrate with the pronounced asymmetry of connection weights important for functional hierarchy. Recent advancements in the structural brain network analysis have revealed the exponential distance rule (EDR), which shows that neuron numbers decrease exponentially with their axon length. This neuron-level insight led to a region-level EDR network model that explained properties of interareal cortical networks in macaques, mice, and rats. Recently, the complete connectome of the Drosophila has been mapped and the presence of the EDR has also been confirmed. Our research revisits the EDR, measures its decay rate, and demonstrates that the EDR model effectively accounts for various properties of the network of brain regions, serving as an appropriate null model for a structural brain network analysis. Its importance as a null model lies in its ability to facilitate the identification of functionally significant features not caused by inevitable geometric constraints.},
  archive      = {J_NETN},
  author       = {Péntek, Balázs and Ercsey-Ravasz, Mária},
  doi          = {10.1162/netn_a_00455},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {869-895},
  shortjournal = {Netw. Neuroscience},
  title        = {The exponential distance rule-based network model predicts topology and reveals functionally relevant properties of the drosophila projectome},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint estimation of source dynamics and interactions from MEG data. <em>NETN</em>, <em>9</em>(3), 842-868. (<a href='https://doi.org/10.1162/netn_a_00453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current techniques to estimate directed functional connectivity from magnetoencephalography (MEG) signals involve two sequential steps: (a) estimation of the sources and their amplitude time series from the MEG data and (b) estimation of directed interactions between the source time series. However, such a sequential approach is not optimal as it leads to spurious connectivity due to spatial leakage. Here, we present an algorithm to jointly estimate the source and connectivity parameters using Bayesian filtering. We refer to this new algorithm as JEDI-MEG (Joint Estimation of source Dynamics and Interactions from MEG data). By formulating a state-space model for the locations and amplitudes of a given number of sources, we show that estimation of their connections can be reduced to a system identification problem. Using simulated MEG data, we show that the joint approach provides a more accurate reconstruction of connectivity parameters than the conventional two-step approach. Using real MEG responses to visually presented faces in 16 subjects, we also demonstrate that our method gives source and connectivity estimates that are both physiologically plausible and largely consistent across subjects. In conclusion, the proposed joint estimation approach outperforms the traditional two-step approach in determining functional connectivity in MEG data. Functional connectivity is currently estimated from electromagnetic brain signals such as magnetoencephalography (MEG) in two consecutive steps: First, the inverse problem is solved to estimate the locations and temporal dynamics of brain sources. Second, connectivity metrics are computed between these estimated sources. This approach suffers from the limitation that the information provided by the connectivity structure is not exploited in the estimation of source activity and vice versa. Here, we present a novel algorithm, utilizing Bayesian filtering, to jointly estimate the source and connectivity parameters to overcome this limitation. Compared with state-of-the-art two-step approaches, our method provides a more accurate reconstruction of the connectivity parameters, which we demonstrate using a standard connectivity benchmark simulation and an electrocorticography-based simulation of MEG data. We also applied our method to real MEG responses (open-access dataset) to visually presented faces in 16 subjects, and the results show that our approach provides source and connectivity estimates that are both physiologically plausible and largely consistent across subjects. Overall, this work contributes to methodological advances in estimating functional connectivity from MEG data.},
  archive      = {J_NETN},
  author       = {Puthanmadam Subramaniyam, Narayan and Tronarp, Filip and Särkkä, Simo and Parkkonen, Lauri},
  doi          = {10.1162/netn_a_00453},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {842-868},
  shortjournal = {Netw. Neuroscience},
  title        = {Joint estimation of source dynamics and interactions from MEG data},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Longitudinal changes in MEG-based brain network topology of ALS patients with cognitive/behavioral impairment—An exploratory study. <em>NETN</em>, <em>9</em>(3), 824-841. (<a href='https://doi.org/10.1162/netn_a_00450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amyotrophic lateral sclerosis (ALS) with only motor impairment (ALS-pure motor) and the behavioral variant of frontotemporal dementia (bvFTD) are hypothesized to represent extreme ends of a disease spectrum, which encompasses ALS with cognitive/behavioral impairment (ALSci/bi). In this longitudinal magnetoencephalography (MEG) study, we investigated changes in brain network topology of ALSci/bi over time as compared with ALS-pure motor and bvFTD patients. Resting-state MEG was recorded in ALS-pure motor ( n = 9), ALSci/bi ( n = 16), and bvFTD ( n = 16) at baseline and 5-month follow-up, projected to source space. The corrected version of the amplitude envelope correlation was applied to compute frequency-band-specific functional connectivity between brain regions, from which the backbone of the functional networks was constructed using the minimum spanning tree (MST) approach. Reference MSTs were computed based on the functional connectivity matrices for ALS-pure motor and bvFTD, against which the networks of ALSci/bi were compared. We showed that, at baseline, networks in the theta band of ALSci/bi patients were more similar to ALS-pure motor than bvFTD. At follow-up, ALSci/bi patients’ beta-band network similarity had moved away from ALS-pure motor and resembled bvFTD. In conclusion, our findings suggest that brain networks of ALSci/bi patients move along the ALS-bvFTD spectrum over time, from ALS-pure motor to bvFTD-like topology. In this longitudinal magnetoencephalography (MEG) study, we explored changes in brain network topology in amyotrophic lateral sclerosis (ALS) patients with cognitive/behavioral impairment (ALSci/bi), compared with the two extreme ends of the ALS behavioral variant of frontotemporal dementia (ALS-bvFTD) spectrum (ALS with only motor impairment and bvFTD patients without ALS). We recorded resting-state MEG at baseline and 5-month follow-up, analyzing frequency-band-specific functional connectivity and constructing the networks’ backbone using the minimum spanning tree (MST). Our findings indicate that at baseline, ALSci/bi networks were more similar to ALS-pure motor than bvFTD in the theta band. At follow-up, ALSci/bi networks shifted toward bvFTD-like topology in the beta band. This suggests that ALSci/bi patients’ brain networks evolve along the ALS-bvFTD spectrum, highlighting potential implications for disease progression.},
  archive      = {J_NETN},
  author       = {Govaarts, Rosanne and Scheijbeler, Elliz P. and Beeldman, Emma and Fraschini, Matteo and Griffa, Alessandra and Engels, Marjolein M. A. and van der Kooi, Anneke J. and Pijnenburg, Yolande A. L. and de Visser, Marianne and Stam, Cornelis J. and Raaphorst, Joost and Hillebrand, Arjan},
  doi          = {10.1162/netn_a_00450},
  journal      = {Network Neuroscience},
  month        = {7},
  number       = {3},
  pages        = {824-841},
  shortjournal = {Netw. Neuroscience},
  title        = {Longitudinal changes in MEG-based brain network topology of ALS patients with cognitive/behavioral impairment—An exploratory study},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="tacl">TACL - 6</h2>
<ul>
<li><details>
<summary>
(2025). Active knowledge structuring for large language models in materials science text mining. <em>TACL</em>, <em>13</em>, 1186-1203. (<a href='https://doi.org/10.1162/TACL.a.36'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) offer a promising alternative to traditional Materials Science Text Mining (MSTM) by reducing the need for extensive data labeling and fine-tuning. However, existing zero-/few-shot methods still face limitations in aligning with personalized needs in scientific discovery. To address this, we propose ClassMATe, an active knowledge structuring approach for MSTM. Specifically, we first propose a class definition stylization method to structure knowledge, enabling explicit clustering of latent material knowledge in LLMs for enhanced inference. To align with the scientists’ needs, we propose an active needs refining strategy that iteratively clarifies needs by learning from uncertainty-aware hard samples of LLMs, further refining the knowledge structuring. Extensive experiments on seven tasks and eight datasets show that ClassMATe, as a plug-and-play method, achieves performance comparable to supervised learning without requiring fine-tuning or extra knowledge base, highlighting the potential to bridge the gap between LLMs’ latent knowledge and real-world scientific applications. 1},
  archive      = {J_TACL},
  author       = {Zhang, Xin and Yuan, Jingling and Zhang, Peiliang and Liu, Jia and Li, Lin},
  doi          = {10.1162/TACL.a.36},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1186-1203},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Active knowledge structuring for large language models in materials science text mining},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explanatory summarization with discourse-driven planning. <em>TACL</em>, <em>13</em>, 1146-1170. (<a href='https://doi.org/10.1162/TACL.a.30'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination. The project information is available at https://dongqi.me/projects/ExpSum .},
  archive      = {J_TACL},
  author       = {Liu, Dongqi and Yu, Xi and Demberg, Vera and Lapata, Mirella},
  doi          = {10.1162/TACL.a.30},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1146-1170},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Explanatory summarization with discourse-driven planning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARE: Diverse visual question answering with robustness evaluation. <em>TACL</em>, <em>13</em>, 1121-1145. (<a href='https://doi.org/10.1162/TACL.a.29'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Language Models (VLMs) extend remarkable capabilities of text-only large language models and vision-only models, being able to learn from and process multi-modal vision-text input. While modern VLMs perform well on a number of standard image classification and image-text matching tasks, they still struggle with a number of crucial vision-language (VL) reasoning abilities such as counting and spatial reasoning. Moreover, while they might be very brittle to small variations in instructions and/or evaluation protocols, existing benchmarks fail to evaluate their robustness (or rather the lack of it). In order to couple challenging VL scenarios with comprehensive robustness evaluation, we introduce DARE , D iverse Visual Question A nswering with R obustness E valuation, a carefully created and curated multiple-choice VQA benchmark. DARE evaluates VLM performance on five diverse categories and includes four robustness-oriented evaluations based on the variations of prompts, the subsets of answer options, the output format, and the number of correct answers. Among a spectrum of other findings, we report that state-of-the-art VLMs still struggle with questions in most categories and are unable to consistently deliver their peak performance across the tested robustness evaluations. Consequently, our work calls for the systematic addition of robustness evaluations in future VLM research.},
  archive      = {J_TACL},
  author       = {Sterz, Hannah and Pfeiffer, Jonas and Vulić, Ivan},
  doi          = {10.1162/TACL.a.29},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1121-1145},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {DARE: Diverse visual question answering with robustness evaluation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing and adapting large language models for few-shot multilingual NLU: Are we there yet?. <em>TACL</em>, <em>13</em>, 1096-1120. (<a href='https://doi.org/10.1162/TACL.a.33'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised fine-tuning (SFT), supervised instruction tuning (SIT), and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning. ICL has gained popularity recently with the advent of LLMs due to its versatile simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on a variety of high- and low-resource languages over five different NLU tasks, and a myriad of language and domain setups. Importantly, performance is only one aspect of the comparison, where we also analyze and discuss the approaches through the optics of their computational, inference and financial costs. Some of the highlighted findings concern an excellent trade-off between performance and resource requirements/cost for SIT. We further analyze the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve accordingly and remains limited, especially for low-resource languages.},
  archive      = {J_TACL},
  author       = {Razumovskaia, Evgeniia and Vulić, Ivan and Korhonen, Anna},
  doi          = {10.1162/TACL.a.33},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1096-1120},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Analyzing and adapting large language models for few-shot multilingual NLU: Are we there yet?},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BenCzechMark: A czech-centric multitask and multimetric benchmark for large language models with duel scoring mechanism. <em>TACL</em>, <em>13</em>, 1068-1095. (<a href='https://doi.org/10.1162/TACL.a.32'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present B en C zech M ark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its duel scoring system is grounded in statistical significance theory and uses aggregation across tasks inspired by social preference theory. Our benchmark encompasses 50 challenging tasks, with corresponding test datasets, primarily in native Czech, with 14 newly collected ones. These tasks span 8 categories and cover diverse domains, including historical Czech news, essays from pupils or language learners, and spoken word. Furthermore, we collect and clean BUT-Large Czech Collection, the largest publicly available clean Czech language corpus, and use it for (i) contamination analysis and (ii) continuous pretraining of the first Czech-centric 7B language model with Czech-specific tokenization. We use our model as a baseline for comparison with publicly available multilingual models. Lastly, we release and maintain a leaderboard with existing 50 model submissions, where new model submissions can be made at https://huggingface.co/spaces/CZLC/BenCzechMark .},
  archive      = {J_TACL},
  author       = {Fajcik, Martin and Docekal, Martin and Dolezal, Jan and Ondrej, Karel and Beneš, Karel and Kapsa, Jan and Smrz, Pavel and Polok, Alexander and Hradis, Michal and Neverilova, Zuzana and Horak, Ales and Sabol, Radoslav and Stefanik, Michal and Jirkovsky, Adam and Adamczyk, David and Hyner, Petr and Hula, Jan and Kydlicek, Hynek},
  doi          = {10.1162/TACL.a.32},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1068-1095},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {BenCzechMark: A czech-centric multitask and multimetric benchmark for large language models with duel scoring mechanism},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KEFT: Knowledge-enhanced fine-tuning for large language models in domain-specific question answering. <em>TACL</em>, <em>13</em>, 1056-1067. (<a href='https://doi.org/10.1162/TACL.a.31'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of large language models (LLMs) has opened up promising opportunities for their downstream applications in question-answering (QA), such as ChatGPT, ChatGLM, etc. However, such LLMs do not perform very well in domain-specific QA tasks without fine-tuning. But directly fine-tuning LLMs on domain-specific corpus data may lead to catastrophic forgetting, causing the LLMs to lose their general language capability. To address this problem, we propose the Knowledge-Enhanced Fine-Tuning (KEFT) method, an unsupervised fine-tuning approach to enhance the knowledge capability of LLMs in domain-specific QA tasks while preserving their general language capability. KEFT leverages the inherent language comprehension of pre-trained LLMs to generate synthetic-QA datasets from domain-specific corpus data autonomously for fine-tuning, and adopts a Low-Rank Adaptation (LoRA) method to further alleviate over-fitting. Furthermore, to enhance the representation of domain-specific knowledge, we introduce a knowledge-enhanced fine-tuning loss function, which encourages the model to learn the knowledge-question connection, thereby generating natural and knowledgeable answers. Our evaluations across multiple domain-specific datasets demonstrate that KEFT surpasses state-of-the-art fine-tuning approaches, enhancing the performance of various LLMs in QA tasks in both English and Chinese languages.},
  archive      = {J_TACL},
  author       = {Li, Haiyun and Zhang, Jixin and Shen, Hua and Cheng, Ke and Huang, Xiaofeng},
  doi          = {10.1162/TACL.a.31},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1056-1067},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {KEFT: Knowledge-enhanced fine-tuning for large language models in domain-specific question answering},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="tmlr">TMLR - 88</h2>
<ul>
<li><details>
<summary>
(2025). MoReact: Generating reactive motion from textual descriptions. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=4zuT73heqm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and generating human reactions poses a significant challenge with broad applications for computer vision and human-computer interaction. Existing methods either treat multiple individuals as a single entity, directly generating interactions, or rely solely on one person's motion to generate the other's reaction, failing to integrate the rich semantic information that underpins human interactions. Yet, these methods often fall short in adaptive responsiveness, \ie, the ability to accurately respond to diverse and dynamic interaction scenarios. Recognizing this gap, our work introduces an approach tailored to address the limitations of existing models by focusing on text-driven human reaction generation. Our model specifically generates realistic motion sequences for individuals that responding to the other's actions based on a descriptive text of the interaction scenario. The goal is to produce motion sequences that not only complement the opponent's movements but also semantically fit the described interactions. To achieve this, we present MoReact, a diffusion-based method designed to disentangle the generation of global trajectories and local motions sequentially. This approach stems from the observation that generating global trajectories first is crucial for guiding local motion, ensuring better alignment with given action and text. Furthermore, we introduce a novel interaction loss to enhance the realism of generated close interactions. Our experiments, utilizing data adapted from a two-person motion dataset, demonstrate the efficacy of our approach for this novel task, which is capable of producing realistic, diverse, and controllable reactions that not only closely match the movements of the counterpart but also adhere to the textual guidance. Please find our webpage at https://xiyan-xu.github.io/MoReactWebPage.},
  archive      = {J_TMLR},
  author       = {Xiyan Xu and Sirui Xu and Yu-Xiong Wang and Liangyan Gui},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {MoReact: Generating reactive motion from textual descriptions},
  url          = {https://openreview.net/forum?id=4zuT73heqm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning equivalence classes of bayesian network structures with GFlowNet. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=FAcc7oAdaa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the causal graph underlying a system is essential for enabling causal inference, particularly in fields such as medicine and genetics. Identifying a causal Directed Acyclic Graph (DAG) from observational data alone is challenging because multiple DAGs can encode the same set of conditional independencies. These equivalent DAGs form a Markov Equivalence Class (MEC), which is represented by a Completed Partially Directed Acyclic Graph (CPDAG). Effectively approximating the CPDAG is crucial because it facilitates narrowing down the set of possible causal graphs underlying the data. We introduce CPDAG-GFN, a novel approach that uses a Generative Flow Network (GFlowNet) to learn a posterior distribution over CPDAGs. From this distribution, we sample high-reward CPDAG candidates that approximate the ground truth, with rewards determined by a score function that quantifies how well each graph fits the data. Additionally, CPDAG-GFN incorporates a sparsity-preferring filter to enhance the set of CPDAG candidates and improve their alignment with the ground truth. Experimental results on both simulated and real-world datasets demonstrate that CPDAG-GFN performs competitively with established methods for learning CPDAG candidates from observational data.},
  archive      = {J_TMLR},
  author       = {Michelle Liu and Zhaocheng Zhu and Olexa Bilaniuk and Emmanuel Bengio},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Learning equivalence classes of bayesian network structures with GFlowNet},
  url          = {https://openreview.net/forum?id=FAcc7oAdaa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SELU: Self-learning embodied multimodal large language models in unknown environments. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=G5gROx8AVi'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multimodal large language models (MLLMs) have demonstrated strong visual understanding and decision-making capabilities, enabling the exploration of autonomously improving MLLMs in unknown environments. However, external feedback like human or environmental feedback is not always available. To address this challenge, existing methods primarily focus on enhancing the decision-making capabilities of MLLMs through voting and scoring mechanisms, while little effort has been paid to improving the environmental comprehension of MLLMs in unknown environments. To fully unleash the self-learning potential of MLLMs, we propose a novel actor-critic self-learning paradigm, dubbed SELU, inspired by the actor-critic paradigm in reinforcement learning. The critic employs self-asking and hindsight relabeling to extract knowledge from interaction trajectories collected by the actor, thereby augmenting its environmental comprehension. Simultaneously, the actor is improved by the self-feedback provided by the critic, enhancing its decision-making. We evaluate our method in the AI2-THOR and VirtualHome environments, and SELU achieves critic improvements of approximately 28% and 30%, and actor improvements of about 20% and 24% via self-learning.},
  archive      = {J_TMLR},
  author       = {Boyu Li and Haobin Jiang and Ziluo Ding and Xinrun Xu and Haoran Li and Dongbin Zhao and Zongqing Lu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {SELU: Self-learning embodied multimodal large language models in unknown environments},
  url          = {https://openreview.net/forum?id=G5gROx8AVi},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behaviour discovery and attribution for explainable reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=JbHtpOIH9l'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building trust in reinforcement learning (RL) agents requires understanding why they make certain decisions, especially in high-stakes applications like robotics, healthcare, and finance. Existing explainability methods often focus on single states or entire trajectories, either providing only local, step-wise insights or attributing decisions to coarse, episodelevel summaries. Both approaches miss the recurring strategies and temporally extended patterns that actually drive agent behavior across multiple decisions. We address this gap by proposing a fully offline, reward-free framework for behavior discovery and segmentation, enabling the attribution of actions to meaningful and interpretable behavior segments that capture recurring patterns appearing across multiple trajectories. Our method identifies coherent behavior clusters from state-action sequences and attributes individual actions to these clusters for fine-grained, behavior-centric explanations. Evaluations on four diverse offline RL environments show that our approach discovers meaningful behaviors and outperforms trajectory-level baselines in fidelity, human preference, and cluster coherence. Our code is publicly available.},
  archive      = {J_TMLR},
  author       = {Rishav Rishav and Somjit Nath and Vincent Michalski and Samira Ebrahimi Kahou},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Behaviour discovery and attribution for explainable reinforcement learning},
  url          = {https://openreview.net/forum?id=JbHtpOIH9l},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlowBench: Benchmarking optical flow estimation methods for reliability and generalization. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Kh4bj6YDNm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical flow estimation is a crucial computer vision task often applied to safety-critical real-world scenarios like autonomous driving and medical imaging. While optical flow estimation accuracy has greatly benefited from the emergence of deep learning, learning-based methods are also known for their lack of generalization and reliability. However, reliability is paramount when optical flow methods are employed in the real world, where safety is essential. Furthermore, a deeper understanding of the robustness and reliability of learning-based optical flow estimation methods is still lacking, hindering the research community from building methods safe for real-world deployment. Thus, we propose FlowBench, a robustness benchmark and evaluation tool for learning-based optical flow methods. FlowBench facilitates streamlined research into the reliability of optical flow methods by benchmarking their robustness to adversarial attacks and out-of-distribution samples. With FlowBench, we benchmark 57 checkpoints across 3 datasets under 9 diverse adversarial attacks and 23 established common corruptions, making it the most comprehensive robustness analysis of optical flow methods to date. Across this wide range of methods, we consistently find that methods with state-of-the-art performance on established standard benchmarks lack reliability and generalization ability. Moreover, we find interesting correlations between the performance, reliability, and generalization ability of optical flow estimation methods, under various lenses such as design choices used, number of parameters, etc. The open-source code and weights for FlowBench are available in this GitHub repository: https://github.com/shashankskagnihotri/FlowBench.},
  archive      = {J_TMLR},
  author       = {Shashank Agnihotri and Julian Yuya Caspary and Luca Schwarz and Xinyan Gao and Jenny Schmalfuss and Andres Bruhn and Margret Keuper},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {FlowBench: Benchmarking optical flow estimation methods for reliability and generalization},
  url          = {https://openreview.net/forum?id=Kh4bj6YDNm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication cost reduction for subgraph counting under local differential privacy via hash functions. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=N1J236mepp'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We suggest the use of hash functions to cut down the communication costs when counting subgraphs under edge local differential privacy. While various algorithms exist for computing graph statistics --- including the count of subgraphs --- under the edge local differential privacy, many suffer with high communication costs, making them less efficient for large graphs. Though data compression is a typical approach in differential privacy, its application in local differential privacy requires a form of compression that every node can reproduce. In our study, we introduce linear congruence hashing. Leveraging amplification by sub-sampling, with a sampling size of $s$, our method can cut communication costs by a factor of $s^2$, albeit at the cost of increasing variance in the published graph statistic by a factor of $s$. The experimental results indicate that, when matched for communication costs, our method achieves a reduction in the $\ell_2$-error by up to 1000 times for triangle counts and by up to $10^3$ times for 4-cycles counts compared to the performance of leading algorithms.},
  archive      = {J_TMLR},
  author       = {Quentin Hillebrand and Vorapong Suppakitpaisarn and Tetsuo Shibuya},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Communication cost reduction for subgraph counting under local differential privacy via hash functions},
  url          = {https://openreview.net/forum?id=N1J236mepp},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging fully-observable solutions for improved partially-observable offline reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=e9p4TDPy6A'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline reinforcement learning (RL) is a popular learning framework for control problems where online interactions with the environment are expensive, risky, or otherwise impractical. Existing offline RL methods commonly assume full observability of the state, and therefore there is a lack of offline RL methods that are specialized for the more general case of partially-observable control. To address this gap, we propose Cross-Observability Conservative Q-Learning (CO-CQL), an offline RL algorithm for partially-observable control that leverages fully-observable expert policies in an asymmetric learning setting. To motivate the use of fully-observable experts for partially-observable control, we formalize Cross-Observability Optimality Ratio (COOR), a theoretical measure of cross-observability that quantifies the benefit of learning asymmetrically from a fully-observable expert, and Cross-Observability Approximation Ratio (COAR), an estimation of COOR computable from trained policies. Our empirical evaluation on a wide variety of partially-observable challenges demonstrates that CO-CQL is able to exploit the guidance of fully-observable experts to outperform other state-of-the-art offline algorithms.},
  archive      = {J_TMLR},
  author       = {Chulabhaya Wijesundara and Andrea Baisero and Gregory David Castanon and Alan S Carlin and Robert Platt and Christopher Amato},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Leveraging fully-observable solutions for improved partially-observable offline reinforcement learning},
  url          = {https://openreview.net/forum?id=e9p4TDPy6A},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased loss functions for multilabel classification with missing labels. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=hMq1hUhLqp'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers binary and multilabel classification problems in a setting where labels are missing independently and with a known rate. Missing labels are a ubiquitous phenomenon in extreme multi-label classification (XMC) tasks, such as matching Wikipedia articles to a small subset out of the hundreds of thousands of possible tags, where no human annotator can possibly check the validity of all the negative samples. For this reason, propensity-scored precision---an unbiased estimate for precision-at-k under a known noise model---has become one of the standard metrics in XMC. Few methods take this problem into account already during the training phase, and all of these are limited to loss functions that can be decomposed into a sum of contributions from each individual label. A typical approach to training is to reduce the multilabel problem into a series of binary or multiclass problems, and it has been shown that if the surrogate task should be consistent for optimizing recall, the resulting loss function is not decomposable over labels. Therefore, this paper develops unbiased estimators for generic, potentially non-decomposable loss functions. These estimators suffer from increased variance and may lead to ill-posed optimization problems, which we address by switching to convex upper-bounds. The theoretical considerations are further supplemented by an experimental study showing that the switch to unbiased estimators significantly alters the bias-variance trade-off and thus requires stronger regularization.},
  archive      = {J_TMLR},
  author       = {Erik Schultheis and Rohit Babbar},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Unbiased loss functions for multilabel classification with missing labels},
  url          = {https://openreview.net/forum?id=hMq1hUhLqp},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical test for saliency maps of graph neural networks via selective inference. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=5NkXTCVa7F'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have gained prominence for their ability to process graph-structured data across various domains. However, interpreting GNN decisions remains a significant challenge, leading to the adoption of saliency maps for identifying salient subgraphs composed of influential nodes and edges. Despite their utility, the reliability of GNN saliency maps has been questioned, particularly in terms of their robustness to input noise. In this study, we propose a statistical testing framework to rigorously evaluate the significance of saliency maps. Our main contribution lies in addressing the inflation of the Type I error rate caused by double-dipping of data, leveraging the framework of Selective Inference. Our method provides statistically valid $p$-values while controlling the Type I error rate, ensuring that identified salient subgraphs contain meaningful information rather than random artifacts. The method is applicable to a variety of saliency methods with piecewise linearity (e.g., Class Activation Mapping). We validate our method on synthetic and real-world datasets, demonstrating its capability in assessing the reliability of GNN interpretations.},
  archive      = {J_TMLR},
  author       = {Shuichi Nishino and Tomohiro Shiraishi and Teruyuki Katsuoka and Ichiro Takeuchi},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Statistical test for saliency maps of graph neural networks via selective inference},
  url          = {https://openreview.net/forum?id=5NkXTCVa7F},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Certified robustness to data poisoning in gradient-based training. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=9WHifn9ZVX'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern machine learning pipelines leverage large amounts of public data, making it infeasible to guarantee data quality and leaving models open to poisoning and backdoor attacks. Provably bounding the behavior of learning algorithms under such attacks remains an open problem. In this work, we address this challenge by developing the first framework providing provable guarantees on the behavior of models trained with potentially manipulated data without modifying the model or learning algorithm. In particular, our framework certifies robustness against untargeted and targeted poisoning, as well as backdoor attacks, for bounded and unbounded manipulations of the training inputs and labels. Our method leverages convex relaxations to over-approximate the set of all possible parameter updates for a given poisoning threat model, allowing us to bound the set of all reachable parameters for any gradient-based learning algorithm. Given this set of parameters, we provide bounds on worst-case behavior, including model performance and backdoor success rate. We demonstrate our approach on multiple real-world datasets from applications including energy consumption, medical imaging, and autonomous driving.},
  archive      = {J_TMLR},
  author       = {Philip Sosnin and Mark Niklas Mueller and Maximilian Baader and Calvin Tsay and Matthew Robert Wicker},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Certified robustness to data poisoning in gradient-based training},
  url          = {https://openreview.net/forum?id=9WHifn9ZVX},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HalluEntity: Benchmarking and understanding entity-level hallucination detection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=494k7e9R5D'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hallucinated generation through uncertainty estimation. However, these approaches predominantly operate at the sentence or paragraph level, failing to pinpoint specific spans or entities responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabricated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HalluEntity, which annotates hallucination at the entity level. Based on the dataset, we comprehensively evaluate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our experimental results show that uncertainty estimation approaches focusing on individual token probabilities tend to over-predict hallucinations, while context-aware methods show better but still suboptimal performance. Through an in-depth qualitative study, we identify relationships between hallucination tendencies and linguistic properties and highlight important directions for future research. HalluEntity: https://huggingface.co/datasets/samuelyeh/HalluEntity},
  archive      = {J_TMLR},
  author       = {Min-Hsuan Yeh and Max Kamachee and Seongheon Park and Yixuan Li},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {HalluEntity: Benchmarking and understanding entity-level hallucination detection},
  url          = {https://openreview.net/forum?id=494k7e9R5D},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). System-2 mathematical reasoning via enriched instruction tuning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Cl9Uox031k'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving complex mathematical problems via system-2 reasoning is a natural human skill, yet it remains a significant challenge for current large language models (LLMs). We identify the scarcity of deliberate multi-step reasoning data as a primary limiting factor. To this end, we introduce Enriched Instruction Tuning (EIT), a method that enriches existing human-annotated mathematical datasets by augmenting human-annotated data with AI-generated feedback to create fine-grained reasoning trajectories. These datasets are then used to fine-tune open-source LLMs, enhancing their mathematical reasoning abilities without reliance on any symbolic verification program. Concretely, EIT is composed of two critical steps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step (ERS). The former generates a high-level plan that breaks down complex instructions into a sequence of simpler objectives, while ERS fills in reasoning contexts often overlooked by human annotators, creating a smoother reasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods that generate reasoning chains only depending on LLM's internal knowledge, our method leverages human-annotated initial answers as ``meta-knowledge'' to help LLMs generate more detailed and precise reasoning processes, leading to a more trustworthy LLM expert for complex mathematical problems. In experiments, EIT achieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing state-of-the-art fine-tuning and prompting methods, and even matching the performance of tool-augmented methods.},
  archive      = {J_TMLR},
  author       = {Huanqia Cai and Yijun Yang and Zhifeng Li},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {System-2 mathematical reasoning via enriched instruction tuning},
  url          = {https://openreview.net/forum?id=Cl9Uox031k},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning from human feedback with active queries. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=EScatQaRxz'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ instance-dependent regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of DPO, establishing it as a data-efficient alternative to DPO. The codes are available at https://github.com/jkx19/ActiveQuery.},
  archive      = {J_TMLR},
  author       = {Kaixuan Ji and Jiafan He and Quanquan Gu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Reinforcement learning from human feedback with active queries},
  url          = {https://openreview.net/forum?id=EScatQaRxz},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tree search for language model agents. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=QF0N3x2XVm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments showcase the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute.},
  archive      = {J_TMLR},
  author       = {Jing Yu Koh and Stephen Marcus McAleer and Daniel Fried and Ruslan Salakhutdinov},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Tree search for language model agents},
  url          = {https://openreview.net/forum?id=QF0N3x2XVm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effect of random learning rate: Theoretical analysis of SGD dynamics in non-convex optimization via stationary distribution. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=RPtKkNx9ZK'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a variant of the stochastic gradient descent (SGD) with a random learning rate and reveal its convergence properties. SGD is a widely used stochastic optimization algorithm in machine learning, especially deep learning. Numerous studies reveal the convergence properties of SGD and its theoretically favorable variants. Among these, the analysis of convergence using a stationary distribution of updated parameters provides generalizable results. However, to obtain a stationary distribution, the update direction of the parameters must not degenerate, which limits the applicable variants of SGD. In this study, we consider a novel SGD variant, Poisson SGD, which has degenerated parameter update directions and instead utilizes a random learning rate. Consequently, we demonstrate that a distribution of a parameter updated by Poisson SGD converges to a stationary distribution under weak assumptions on a loss function. Based on this, we further show that Poisson SGD finds global minima in non-convex optimization problems and also evaluate the generalization error using this method. As a proof technique, we approximate the distribution by Poisson SGD with that of the bouncy particle sampler (BPS) and derive its stationary distribution, using the theoretical advance of the piece-wise deterministic Markov process (PDMP).},
  archive      = {J_TMLR},
  author       = {Naoki Yoshida and Shogo Nakakita and Masaaki Imaizumi},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Effect of random learning rate: Theoretical analysis of SGD dynamics in non-convex optimization via stationary distribution},
  url          = {https://openreview.net/forum?id=RPtKkNx9ZK},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning the language of protein structure. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=SRRPQIOS4w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning and \emph{de novo} generation of proteins are pivotal computational biology tasks. Whilst natural language processing (NLP) techniques have proven highly effective for protein sequence modelling, structure modelling presents a complex challenge, primarily due to its continuous and three-dimensional nature. Motivated by this discrepancy, we introduce an approach using a vector-quantized autoencoder that effectively tokenizes protein structures into discrete representations. This method transforms the continuous, complex space of protein structures into a manageable, discrete format with a codebook ranging from 4096 to 64000 tokens, achieving high-fidelity reconstructions with backbone root mean square deviations (RMSD) of approximately 1-4 \AA. To demonstrate the efficacy of our learned representations, we show that a simple GPT model trained on our codebooks can generate novel, diverse, and designable protein structures. Our approach not only provides representations of protein structure, but also mitigates the challenges of disparate modal representations and sets a foundation for seamless, multi-modal integration, enhancing the capabilities of computational methods in protein design.},
  archive      = {J_TMLR},
  author       = {Jérémie DONA and Benoit Gaujac and Timothy Atkinson and Liviu Copoiu and Thomas Pierrot and Thomas D Barrett},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Learning the language of protein structure},
  url          = {https://openreview.net/forum?id=SRRPQIOS4w},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond ordinary lipschitz constraints: Differentially private optimization with TNC. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=SZCygcrGng'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study Stochastic Convex Optimization in Differential Privacy model (DP-SCO). Unlike previous studies, here we assume the population risk function satisfies the Tsybakov Noise Condition (TNC) with some parameter $\theta>1$, where the Lipschitz constant of the loss could be extremely large or even unbounded, but the $\ell_2$-norm gradient of the loss has bounded $k$-th moment with $k\geq 2$. For the Lipschitz case with $\theta\geq 2$, we first propose an $(\epsilon, \delta)$-DP algorithms whose utility bound is $\tilde{O}\left(\left(\tilde{r}_{2k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\epsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$ in high probability, where $n$ is the sample size, $d$ is the model dimension, and $\tilde{r}_{2k}$ is a term that only depends on the $2k$-th moment of the gradient. It is notable that such an upper bound is independent of the Lipschitz constant. We then extend to the case where $\theta\geq \bar{\theta}> 1$ for some known constant $\bar{\theta}$. Moreover, when the privacy budget $\epsilon$ is small enough, we show an upper bound of $\tilde{O}\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\epsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$ even if the loss function is not Lipschitz. For the lower bound, we show that for any $\theta\geq 2$, the private minimax rate for $\rho$-zero Concentrated Differential Privacy is lower bounded by $\Omega\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\sqrt{\rho}}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$.},
  archive      = {J_TMLR},
  author       = {Difei Xu and Meng Ding and Zihang Xiang and Jinhui Xu and Di Wang},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Beyond ordinary lipschitz constraints: Differentially private optimization with TNC},
  url          = {https://openreview.net/forum?id=SZCygcrGng},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complementarity: Toward better metrics and optimizing data efficiency in LLMs. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=feAbrMXGMh'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalist Large Language Models (LLMs) are trained with an immense amount of data from across different domains. However, not all data contribute to model performance equally, and prioritizing data quality can improve domain-specific performance. We suggest that quality is not merely an independent feature of datasets, but rather the manner in which data samples interfere with or complement one another. Furthermore, existing performance metrics for language models are computationally expensive, while also frequently suffering from being mathematically ill-defined and poorly suited to generative AI. Toward improving general performance while reducing the amount of training data, and quantifying how data contributes to downstream tasks vis-a-vis their relation with other data, we introduce a new metric, Complementarity. We first establish a strong correlation between Complementarity and domain-specific task performance. Without reliance on heavy instruction-tuning and text scraping, Complementarity is significantly less expensive to compute and is applicable to a wide variety of potential target domains. Most interestingly, we demonstrate that the Complementarity taken over a training validation set provides a better predictor of generalization to future test sets than directly measuring performance on a test validation set. With this, we introduce an algorithm that carefully selects the data to fine-tune upon, leading to a high-performing fine-tuned generalist model while using only a fraction of the data, and without requiring data from the test domain. Overall, Complementarity may serve as a key metric in future analysis of data utility and design of datasets, and help facilitate the goal of a truly generalist model.},
  archive      = {J_TMLR},
  author       = {Roy Siegelmann},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Complementarity: Toward better metrics and optimizing data efficiency in LLMs},
  url          = {https://openreview.net/forum?id=feAbrMXGMh},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the limitations of layer synchronization in spiking neural networks. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=mfmAVwtMIk'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural-network processing in machine learning applications relies on layer synchronization. This is practiced even in artificial Spiking Neural Networks (SNNs), which are touted as consistent with neurobiology, in spite of processing in the brain being in fact asynchronous. A truly asynchronous system however would allow all neurons to evaluate concurrently their threshold and emit spikes upon receiving any presynaptic current. Omitting layer synchronization is potentially beneficial, for latency and energy efficiency, but asynchronous execution of models previously trained with layer synchronization may entail a mismatch in network dynamics and performance. We present and quantify this problem, and show that models trained with layer synchronization either perform poorly in absence of the synchronization, or fail to benefit from any energy and latency reduction, when such a mechanism is in place. We then explore a potential solution direction, based on a generalization of backpropagation-based training that integrates knowledge about an asynchronous execution scheduling strategy, for learning models suitable for asynchronous processing. We experiment with 2 asynchronous neuron execution scheduling strategies in datasets that encode spatial and temporal information, and we show the potential of asynchronous processing to use less spikes (up to 50\%), complete inference faster (up to 2x), and achieve competitive or even better accuracy (up to $\sim$10\% higher). Our exploration affirms that asynchronous event-based AI processing can be indeed more efficient, but we need to rethink how we train our SNN models to benefit from it. (Source code available at: \url{https://github.com/RoelMK/asynctorch})},
  archive      = {J_TMLR},
  author       = {Roel Koopman and Amirreza Yousefzadeh and Mahyar Shahsavari and Guangzhi Tang and Manolis Sifalakis},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Exploring the limitations of layer synchronization in spiking neural networks},
  url          = {https://openreview.net/forum?id=mfmAVwtMIk},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple noises in diffusion model for semi-supervised multi-domain translation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=vYdT26kDYM'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address the challenge of multi-domain translation, where the objective is to learn mappings between arbitrary configurations of domains within a defined set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for three domains) without the need for separate models for each specific translation configuration, enabling more efficient and flexible domain translation. We introduce Multi-Domain Diffusion (MDD), a method with dual purposes: i) reconstructing any missing views for new data objects, and ii) enabling learning in semi-supervised contexts with arbitrary supervision configurations. MDD achieves these objectives by exploiting the noise formulation of diffusion models, specifically modeling one noise level per domain. Similar to existing domain translation approaches, MDD learns the translation between any combination of domains. However, unlike prior work, our formulation inherently handles semi-supervised learning without modification by representing missing views as noise in the diffusion process. We evaluate our approach through domain translation experiments on BL3NDT, a multi-domain synthetic dataset designed for challenging semantic domain inversion, the BraTS2020 dataset, and the CelebAMask-HQ dataset.},
  archive      = {J_TMLR},
  author       = {Tsiry Mayet and Simon Bernard and Romain HÉRAULT and Clement Chatelain},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Multiple noises in diffusion model for semi-supervised multi-domain translation},
  url          = {https://openreview.net/forum?id=vYdT26kDYM},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous language model interpolation yields dynamic and controllable text generation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=xD9Nu2Wah4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications. In particular, linear interpolation between model parameters forms the backbone for many recent approaches to adapting models to user preferences. While the existing literature on LLM adaptation primarily focuses on finding methods that optimize for some set of performance criteria or user preferences, here we instead seek to better understand and characterize the behavior of dense, continuous interpolation between models. Specifically, we use low-rank updates to fine-tune a base model to various different domains, yielding a set of anchor models with distinct generation profiles. Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull. We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes simultaneously. We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case. Our results suggest that parameter merging facilitates flexible model adaptation due to its predictable behavior within the full interpolation region.},
  archive      = {J_TMLR},
  author       = {Sara Kangaslahti and David Alvarez-Melis},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Continuous language model interpolation yields dynamic and controllable text generation},
  url          = {https://openreview.net/forum?id=xD9Nu2Wah4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auto-regressive vs flow-matching: A comparative study of modeling paradigms for text-to-music generation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=xXc5DeaBYw'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly in many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and identify which design choices influence performance the most. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: auto-regressive decoding and conditional flow-matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM},
  archive      = {J_TMLR},
  author       = {Or Tal and Felix Kreuk and Yossi Adi},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Auto-regressive vs flow-matching: A comparative study of modeling paradigms for text-to-music generation},
  url          = {https://openreview.net/forum?id=xXc5DeaBYw},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean-field RL for large-scale unit-capacity pickup-and-delivery problems. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=E8JRswdyDR'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving large-scale vehicle routing problems (VRPs) is NP-hard and poses a computational challenge in numerous applications such as logistics. Meanwhile, mean-field control (MFC) provides a tractable and rigorous approach to controlling many agents. We provide a solution to pickup-and-delivery VRPs via scalable MFC. In combination with reinforcement learning (RL) and clustering, our MFC approach efficiently scales to large-scale VRPs. We perform a theoretical analysis of our MFC-based approximation, giving convergence results for large VRP instances and error bounds for clustering-based approximations. We verify our algorithms on different datasets and compare them against solutions such as OR-Tools, PyVRP and heuristics, showing scalability in terms of speed for mean-field methods, for the first time in discrete optimization. Overall, our work establishes a novel synthesis of MFC-based RL techniques, vehicle routing problems and clustering approximations, to solve a hard discrete optimization problem of practical use in a scalable way.},
  archive      = {J_TMLR},
  author       = {Kai Cui and Sharif Azem and Christian Fabian and Kirill Kuroptev and Ramin Khalili and Osama Abboud and Florian Steinke and Heinz Koeppl},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Mean-field RL for large-scale unit-capacity pickup-and-delivery problems},
  url          = {https://openreview.net/forum?id=E8JRswdyDR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust uncertainty quantification for quantile treatment effects in sequential decision making. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=F0BwbieVws'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider multi-stage sequential decision making, where the treatment at any stage may depend on the subject’s entire treatment and covariate history. We introduce a general framework for doubly robust uncertainty quantification for the quantiles of cumulative outcomes under a sequential treatment rule. While previous studies focused on mean effects, quantile effects offer unique insights into the distributional properties and are more robust for heavy-tailed outcomes. It is known that, doubly robust inference is significantly more challenging and largely unexplored for quantile treatment effects. More importantly, for mean effects, doubly robust estimation does not ensure doubly robust inference. Our approach first provides a doubly robust estimator for any quantile of interest based on pre-collected data, achieving semi-parametric efficiency. We then propose a novel doubly robust estimator for the asymptotic variance, enabling the construction of a doubly robust confidence interval. To overcome the challenges in parameter-dependent nuisance functions, we leverage deep conditional generative learning techniques. We demonstrate advantages of our approach via both simulation and real data from a short video platform. Additionally, we observe that our proposed approach leads to another mean effect estimator that outperforms existing estimators with heavy-tailed outcomes.},
  archive      = {J_TMLR},
  author       = {Yang Xu and Chengchun Shi and Shikai Luo and Lan Wang and Rui Song},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Doubly robust uncertainty quantification for quantile treatment effects in sequential decision making},
  url          = {https://openreview.net/forum?id=F0BwbieVws},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The overcooked generalisation challenge: Evaluating cooperation with novel partners in unknown environments using unsupervised environment design. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=K2KtcMlW6j'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Overcooked Generalisation Challenge (OGC) – a new benchmark for evaluating reinforcement learning (RL) agents on their ability to cooperate with unknown partners in unfamiliar environments. Existing work typically evaluated cooperative RL only in their training environment or with their training partners, thus seriously limiting our ability to understand agents’ generalisation capacity – an essential requirement for future collaboration with humans. The OGC extends Overcooked-AI to support dual curriculum design (DCD). It is fully GPU-accelerated, open-source, and integrated into the minimax DCD benchmark suite. Compared to prior DCD benchmarks, where designers manipulate only minimal elements of the environment, OGC introduces a significantly richer design space: full kitchen layouts with multiple objects that require the designer to account for interaction dynamics between agents. We evaluate state-of-the-art DCD algorithms alongside scalable neural architectures and find that current methods fail to produce agents that generalise effectively to novel layouts and unfamiliar partners. Our results indicate that both agents and curriculum designers struggle with the joint challenge of partner and environment generalisation. These findings establish OGC as a demanding testbed for cooperative generalisation and highlight key directions for future research. We open-source our code.},
  archive      = {J_TMLR},
  author       = {Constantin Ruhdorfer and Matteo Bortoletto and Anna Penzkofer and Andreas Bulling},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {The overcooked generalisation challenge: Evaluating cooperation with novel partners in unknown environments using unsupervised environment design},
  url          = {https://openreview.net/forum?id=K2KtcMlW6j},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving inverse problems using diffusion with iterative colored renoising. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=RZv8FcQDPW'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relatively poor, especially early in the reverse process, and so we propose a new approach that iteratively reestimates and ``renoises'' the estimate several times per diffusion step. This iterative approach, which we call Fast Iterative REnoising (FIRE), injects colored noise that is shaped to ensure that the pre-trained diffusion model always sees white noise, in accordance with how it was trained. We then embed FIRE into the DDIM reverse process and show that the resulting ``DDfire'' offers state-of-the-art accuracy and runtime on several linear inverse problems, as well as phase retrieval.},
  archive      = {J_TMLR},
  author       = {Matthew C Bendel and Saurav K Shastri and Rizwan Ahmad and Philip Schniter},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Solving inverse problems using diffusion with iterative colored renoising},
  url          = {https://openreview.net/forum?id=RZv8FcQDPW},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COMMA: A communicative multimodal multi-agent benchmark. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=TIGQIem1na'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advances of multimodal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a novel puzzle benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of multimodal puzzles, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. Our findings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like o4-mini. Many chain of thought reasoning models such as R1-Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent collaboration, indicating a potential growth area in their communication abilities.},
  archive      = {J_TMLR},
  author       = {Timothy Ossowski and Danyal Maqbool and Jixuan Chen and Zefan Cai and Tyler J. Bradshaw and Junjie Hu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {COMMA: A communicative multimodal multi-agent benchmark},
  url          = {https://openreview.net/forum?id=TIGQIem1na},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-positive multi-label learning with label cardinality. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=XEPPXH2nKu'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study learning a multi-label classifier from partially labeled data, where each instance has only a single positive label. We explain how auxiliary information available on the label cardinality, the number of positive labels per instance, can be used for improving such methods. We consider auxiliary information of varying granularity, ranging from knowing just the maximum number of labels over all instances to knowledge on the distribution of label cardinalities and even the exact cardinality of each instance. We introduce methods leveraging the different types of auxiliary information, study how close to the fully labeled accuracy we can get under different scenarios, and show that an easy-to-implement method only assuming the knowledge of the maximum cardinality is comparable to the state-of-the-art single-positive multi-label learning methods when using the same base model. Our implementation is publicly available at https://github.com/shayangharib/SPMLL_with_Label_Cardinality.},
  archive      = {J_TMLR},
  author       = {Shayan Gharib and Pierre-Alexandre Murena and Arto Klami},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Single-positive multi-label learning with label cardinality},
  url          = {https://openreview.net/forum?id=XEPPXH2nKu},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text to stealthy adversarial face masks. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=XYqCx026AI'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated that modern facial recognition systems, which are based on deep neural networks, are vulnerable to adversarial attacks, including the use of accessories, makeup patterns, or precision lighting. However, developing attacks that are both robust (resilient to changes in viewing angles and environmental conditions) and stealthy (do not attract suspicion by, for example, incorporating obvious facial features) remains a significant challenge. In this context, we introduce a novel diffusion-based method (DAFR) capable of generating robust and stealthy face masks for dodging recognition systems (where the system fails to identify the attacker). Specifically our approach is capable of producing high-fidelity printable textures using the guidance of textual prompts to determine the style. This method can also be adapted for impersonation purposes, where the system misidentifies the attacker as a specific other individual. Finally, we address a gap in the existing literature by presenting a comprehensive benchmark (FAAB) for evaluating adversarial accessories in three dimensions, assessing their robustness and stealthiness.},
  archive      = {J_TMLR},
  author       = {Ben Lewis and Thomas Moyse and James Parkinson and Elizabeth Telford and Callum Whitfield and Ranko Lazic},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Text to stealthy adversarial face masks},
  url          = {https://openreview.net/forum?id=XYqCx026AI},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LAPP: Large language model feedback for preference-driven reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=cq76wx7T9F'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Large Language Model-Assisted Preference Prediction (LAPP), a novel framework for robot learning that enables efficient, customizable, and expressive behavior acquisition with minimum human effort. Unlike prior approaches that rely heavily on reward engineering, human demonstrations, motion capture, or expensive pairwise preference labels, LAPP leverages large language models (LLMs) to automatically generate preference labels from raw state-action trajectories collected during reinforcement learning (RL). These labels are used to train an online preference predictor, which in turn guides the policy optimization process toward satisfying high-level behavioral specifications provided by humans. Our key technical contribution is the integration of LLMs into the RL feedback loop through trajectory-level preference prediction, enabling robots to acquire complex skills including subtle control over gait patterns and rhythmic timing. We evaluate LAPP on a diverse set of quadruped locomotion and dexterous manipulation tasks and show that it achieves efficient learning, higher final performance, faster adaptation, and precise control of high-level behaviors. Notably, LAPP enables robots to master highly dynamic and expressive tasks such as quadruped backflips, which remain out of reach for standard LLM-generated or handcrafted rewards. Our results highlight LAPP as a promising direction for scalable preference-driven robot learning.},
  archive      = {J_TMLR},
  author       = {Pingcheng Jian and Xiao Wei and Yanbaihui Liu and Samuel A. Moore and Michael M. Zavlanos and Boyuan Chen},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {LAPP: Large language model feedback for preference-driven reinforcement learning},
  url          = {https://openreview.net/forum?id=cq76wx7T9F},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Studying memorization of large language models using answers to stack overflow questions. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ddocn44Kaq'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are capable of answering many software related questions and supporting developers by generating code snippets. These capabilities originate from training on massive amounts of data from the Internet, including information from Stack Overflow. This raises the question whether answers to software related questions are simply memorized from the training data, which might raise problems as this often requires attribution (e.g., CC-BY license), sharing with a similar license (e.g., GPL licenses) or may even be prohibited (proprietary license). To study this, we compare responses to questions from Stack Overflow for questions that were known during LLM pre-training and questions that were not included in the pre-training data. We then calculate the overlap both with answers marked as accepted on Stack Overflow as well as other texts we can find on the internet. We further explore the impact of the popularity of programming languages, the complexity of the prompts used, and the randomization of the text generation process on the memorization of answers to Stack Overflow. We find that many generated answers are to some degree collages of memorized content and that this does not dependent on whether the questions were seen during training or not. However, many of the memorized snippets are common phrases or code and, therefore, not copyrightable. Still, we also have clear evidence that copyright violation happens and is likely when LLMs are used at large scales.},
  archive      = {J_TMLR},
  author       = {Laura Caspari and Alexander Trautsch and Michael Granitzer and Steffen Herbold},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Studying memorization of large language models using answers to stack overflow questions},
  url          = {https://openreview.net/forum?id=ddocn44Kaq},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HandsOnVLM: Vision-language models for hand-object interaction prediction. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ehhMFjKnWm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we predict future interaction trajectories of human hands in a scene given high-level colloquial task specifications in the form of natural language? In this paper, we extend the classic hand trajectory prediction task to several tasks involving explicit and implicit language queries. Our proposed tasks require extensive understanding of human daily activities and reasoning abilities about what is happening next given cues from the current scene. We also develop new benchmarks to evaluate the proposed two tasks, Vanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We enable solving these tasks by integrating high-level world knowledge and reasoning capabilities of Vision-Language Models (VLMs) with the auto-regressive nature of low-level ego-centric hand trajectories. Our model, HandsOnVLM is a novel VLM that can generate textual responses and produce future hand trajectories through natural-language conversations. Our experiments show that HandsOnVLM outperforms existing task-specific methods and other VLM baselines on proposed tasks, and demonstrates its ability to effectively utilize world knowledge for reasoning about low-level human hand trajectories based on the provided context. More details can be found at https://www.chenbao.tech/handsonvlm/.},
  archive      = {J_TMLR},
  author       = {Chen Bao and Jiarui Xu and Xiaolong Wang and Abhinav Gupta and Homanga Bharadhwaj},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {HandsOnVLM: Vision-language models for hand-object interaction prediction},
  url          = {https://openreview.net/forum?id=ehhMFjKnWm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discrete audio tokens: More than a survey!. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=eqNchtvc6v'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks. They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.},
  archive      = {J_TMLR},
  author       = {Pooneh Mousavi and Gallil Maimon and Adel Moumen and Darius Petermann and Jiatong Shi and Haibin Wu and Haici Yang and Anastasia Kuznetsova and Artem Ploujnikov and Ricard Marxer and Bhuvana Ramabhadran and Benjamin Elizalde and Loren Lugosch and Jinyu Li and Cem Subakan and Phil Woodland and Minje Kim and Hung-yi Lee and Shinji Watanabe and Yossi Adi and Mirco Ravanelli},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Discrete audio tokens: More than a survey!},
  url          = {https://openreview.net/forum?id=eqNchtvc6v},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the learned look-ahead behavior of chess neural networks. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=np4Bg2zIxL'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the look-ahead capabilities of chess-playing neural networks, specifically focusing on the Leela Chess Zero policy network. We build on the work of Jenner et al. (2024) by analyzing the model's ability to consider future moves and alternative sequences beyond the immediate next move. Our findings reveal that the network's look-ahead behavior is highly context-dependent, varying significantly based on the specific chess position. We demonstrate that the model can process information about board states up to seven moves ahead, utilizing similar internal mechanisms across different future time steps. Additionally, we provide evidence that the network considers multiple possible move sequences rather than focusing on a single line of play. These results offer new insights into the emergence of sophisticated look-ahead capabilities in neural networks trained on strategic tasks, contributing to our understanding of AI reasoning in complex domains. Our work also showcases the effectiveness of interpretability techniques in uncovering cognitive-like processes in artificial intelligence systems.},
  archive      = {J_TMLR},
  author       = {Diogo Cruz},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Understanding the learned look-ahead behavior of chess neural networks},
  url          = {https://openreview.net/forum?id=np4Bg2zIxL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlowKac: An efficient neural fokker-planck solver using temporal normalizing flows and the feynman-kac formula. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=paeyQFa5or'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving the Fokker-Planck equation for high-dimensional complex dynamical systems remains a pivotal yet challenging task due to the intractability of analytical solutions and the limitations of traditional numerical methods. In this work, we present FlowKac, a novel approach that reformulates the Fokker-Planck equation using the Feynman-Kac formula, allowing to query the solution at a given point via the expected values of stochastic paths. A key innovation of FlowKac lies in its adaptive stochastic sampling scheme which significantly reduces the computational complexity while maintaining high accuracy. This sampling technique, coupled with a time-indexed normalizing flow, designed for capturing time-evolving probability densities, enables robust sampling of collocation points, resulting in a flexible and mesh-free solver. This formulation mitigates the curse of dimensionality and enhances computational efficiency and accuracy, which is particularly crucial for applications that inherently require dimensions beyond the conventional three. We validate the robustness and scalability of our method through various experiments on a range of stochastic differential equations, demonstrating significant improvements over existing techniques.},
  archive      = {J_TMLR},
  author       = {Naoufal EL BEKRI and Lucas Drumetz and Franck Vermet},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {FlowKac: An efficient neural fokker-planck solver using temporal normalizing flows and the feynman-kac formula},
  url          = {https://openreview.net/forum?id=paeyQFa5or},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient object-centric representation learning using masked generative modeling. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=t9KvOYPeL3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning object-centric representations from visual inputs in an unsupervised manner has drawn focus to solve more complex tasks, such as reasoning and reinforcement learning. However, current state-of-the-art methods, relying on autoregressive transformers or diffusion models to generate scenes from object-centric representations, suffer from computational inefficiency due to their sequential or iterative nature. This computational bottleneck limits their practical application and hinders scaling to more complex downstream tasks. To overcome this, we propose MOGENT, an efficient object-centric learning framework based on masked generative modeling. MOGENT conditions a masked bidirectional transformer on learned object slots and employs a parallel iterative decoding scheme to generate scenes, enabling efficient compositional generation. Experiments show that MOGENT significantly improves computational efficiency, accelerating the generation process by up to 67x and 17x compared to autoregressive models and diffusion-based models, respectively. Importantly, the efficiency is attained while maintaining strong or competitive performance on object segmentation and compositional generation tasks.},
  archive      = {J_TMLR},
  author       = {Akihiro Nakano and Masahiro Suzuki and Yutaka Matsuo},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Efficient object-centric representation learning using masked generative modeling},
  url          = {https://openreview.net/forum?id=t9KvOYPeL3},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedComLoc: Communication-efficient distributed training of sparse and quantized models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=vYQPLytQsj'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is Local Training, which involves running multiple local stochastic gradient descent iterations between communication phases. Our work is inspired by the innovative Scaffnew algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc (Federated Compressed and Local Training), integrating practical and effective compression into Scaffnew to further enhance communication efficiency. Extensive experiments, using the popular Top-K compressor and quantization, demonstrate its prowess in substantially reducing communication overheads in heterogeneous settings.},
  archive      = {J_TMLR},
  author       = {Kai Yi and Georg Meinhardt and Laurent Condat and Peter Richtárik},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {FedComLoc: Communication-efficient distributed training of sparse and quantized models},
  url          = {https://openreview.net/forum?id=vYQPLytQsj},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label embedding via low-coherence matrices. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=vrcWXcr4On'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label embedding is a framework for multiclass classification problems where each label is represented by a distinct vector of some fixed dimension, and training involves matching model output to the vector representing the correct label. While label embedding has been successfully applied in extreme classification and zero-shot learning, and offers both computational and statistical advantages, its theoretical foundations remain poorly understood. This work presents an analysis of label embedding in the context of extreme multiclass classification, where the number of classes $C$ is very large. We present an excess risk bound that reveals a trade-off between computational and statistical efficiency, quantified via the coherence of the embedding matrix. We further show that under the Massart noise condition, the statistical penalty for label embedding vanishes with sufficiently low coherence. Our analysis supports an algorithm that is simple, scalable, and easily parallelizable, and experimental results demonstrate its effectiveness in large-scale applications.},
  archive      = {J_TMLR},
  author       = {Jianxin Zhang and Clayton Scott},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Label embedding via low-coherence matrices},
  url          = {https://openreview.net/forum?id=vrcWXcr4On},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNR-pruning: Sparsity-aware pruning via dying neuron reactivation in convolutional neural networks. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ymUjGCNPYa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we challenge the conventional view of dead neurons—neurons that cease to activate—during deep neural network training. Traditionally regarded as problematic due to their association with optimization challenges and reduced model adaptability over training epochs, dead neurons are often seen as a hindrance. However, we present a novel perspective, demonstrating that they can be effectively leveraged to enhance network sparsity. Specifically, we propose DNR-Pruning, dying neuron reactivation based sparsity-aware pruning approach for convolutional neural networks (CNNs) that exploits the behavior of individual neurons during training. Through a systematic exploration of hyperparameter configurations, we show that dying neurons can be harnessed to improve pruning algorithms. Our method dynamically monitors the occurrence of dying neurons, enabling adaptive sparsification throughout CNN training. Extensive experiments on diverse datasets demonstrate that DNR-Pruning outperforms existing sparsity-aware pruning techniques while achieving competitive results compared to state-of-the-art methods. These findings suggest that dying neurons can serve as an efficient mechanism for network compression and resource optimization in CNNs, opening new avenues for more efficient and high-performance deep learning models.},
  archive      = {J_TMLR},
  author       = {Boyuan Wang and Richard Jiang},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {DNR-pruning: Sparsity-aware pruning via dying neuron reactivation in convolutional neural networks},
  url          = {https://openreview.net/forum?id=ymUjGCNPYa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An information-theoretic lower bound on the generalization error of autoencoders. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=0esF0M467w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying the limitations of classical neural network architectures is a critically underexplored area of machine learning research. Deriving lower bounds on the optimal performance of these architectures can facilitate improved neural architecture search and overfitting detection. We present an information-theoretic lower bound on the generalization mean squared error of autoencoders with sigmoid activation functions. Through the Estimation Error and Differential Entropy (EEDE) inequality for continuous random vectors, we derive this lower bound, which provides a new perspective on the inherent limitations and capabilities of autoencoders. Our analysis extends to the examination of how this lower bound is influenced by various architectural features and data distribution characteristics. This study enriches our theoretical understanding of autoencoders and has substantial practical implications for their design, optimization, and application in the field of deep learning.},
  archive      = {J_TMLR},
  author       = {Shyam Venkatasubramanian and Sean Moushegian and Ahmed Aloui and Vahid Tarokh},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {An information-theoretic lower bound on the generalization error of autoencoders},
  url          = {https://openreview.net/forum?id=0esF0M467w},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Min-max optimisation for nonconvex-nonconcave functions using a random zeroth-order extragradient algorithm. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=1bxY1uAXyr'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the performance of the random Gaussian smoothing Zeroth-Order ExtraGradient (ZO-EG) scheme considering deterministic min-max optimisation problems with possibly NonConvex-NonConcave (NC-NC) objective functions. We consider both unconstrained and constrained, differentiable and non-differentiable settings. We discuss the min-max problem from the point of view of variational inequalities. For the unconstrained problem, we establish the convergence of the ZO-EG algorithm to the neighbourhood of an $\epsilon$-stationary point of the NC-NC objective function, whose radius can be controlled under a variance reduction scheme, along with its complexity. For the constrained problem, we introduce the new notion of proximal variational inequalities and give examples of functions satisfying this property. Moreover, we prove analogous results to the unconstrained case for the constrained problem. For the non-differentiable case, we prove the convergence of the ZO-EG algorithm to a neighbourhood of an $\epsilon$-stationary point of the smoothed version of the objective function, where the radius of the neighbourhood can be controlled, which can be related to the ($\delta,\epsilon$)-Goldstein stationary point of the original objective function.},
  archive      = {J_TMLR},
  author       = {Amir Ali Farzin and Yuen-Man Pun and Philipp Braun and Antoine Lesage-Landry and Youssef Diouane and Iman Shames},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Min-max optimisation for nonconvex-nonconcave functions using a random zeroth-order extragradient algorithm},
  url          = {https://openreview.net/forum?id=1bxY1uAXyr},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey of video diffusion models: Foundations, implementations, and applications. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=2ODDBObKjH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in diffusion models have revolutionized video generation, offering superior temporal consistency and visual quality compared to traditional generative adversarial networks-based approaches. While this emerging field shows tremendous promise in applications, it faces significant challenges in motion consistency, computational efficiency, and ethical considerations. This survey provides a comprehensive review of diffusion-based video generation, examining its evolution, technical foundations, and practical applications. We present a systematic taxonomy of current methodologies, analyze architectural innovations and optimization strategies, and investigate applications across low-level vision tasks such as denoising and super-resolution. Additionally, we explore the synergies between diffusion-based video generation and related domains, including video representation learning, question answering, and retrieval. Compared to the existing surveys (Lei et al., 2024a;b; Melniket al., 2024; Cao et al., 2023; Xing et al., 2024c) which focus on specific aspects of video generation, such as human video synthesis (Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our work provides a broader, more updated, and more fine-grained perspective on diffusion-based approaches with a special section for evaluation metrics, industry solutions, and training engineering techniques in video generation. This survey serves as a foundational resource for researchers and practitioners working at the intersection of diffusion models and video generation, providing insights into both the theoretical frameworks and practical implementations that drive this rapidly evolving field.},
  archive      = {J_TMLR},
  author       = {Yimu Wang and Xuye Liu and Wei Pang and Li Ma and Shuai Yuan and Paul Debevec and Ning Yu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Survey of video diffusion models: Foundations, implementations, and applications},
  url          = {https://openreview.net/forum?id=2ODDBObKjH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GROOD: GRadient-aware out-of-distribution detection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=2V7itvvMVJ'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) detection is crucial for ensuring the reliability of deep learning models in real-world applications. Existing methods typically focus on feature representations or output-space analysis, often assuming a distribution over these spaces or leveraging gradient norms with respect to model parameters. However, these approaches struggle to distinguish near-OOD samples and often require extensive hyper-parameter tuning, limiting their practicality. In this work, we propose GRadient-aware Out-Of-Distribution detection (GROOD), a method that derives an OOD prototype from synthetic samples and computes class prototypes directly from In-distribution (ID) training data. By analyzing the gradients of a nearest-class-prototype loss function concerning an artificial OOD prototype, our approach achieves a clear separation between in-distribution and OOD samples. Experimental evaluations demonstrate that gradients computed from the OOD prototype enhance the distinction between ID and OOD data, surpassing established baselines in robustness, particularly on ImageNet-1k. These findings highlight the potential of gradient-based methods and prototype-driven approaches in advancing OOD detection within deep neural networks.},
  archive      = {J_TMLR},
  author       = {Mostafa ElAraby and Sabyasachi Sahoo and Yann Pequignot and Paul Novello and Liam Paull},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {GROOD: GRadient-aware out-of-distribution detection},
  url          = {https://openreview.net/forum?id=2V7itvvMVJ},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning robust representations for visual reinforcement learning via task-relevant mask sampling. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=2rxNDxHwtn'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans excel at isolating relevant information from noisy data to predict the behavior of dynamic systems, effectively disregarding non-informative, temporally-correlated noise. In contrast, existing visual reinforcement learning algorithms face challenges in generating noise-free predictions within high-dimensional, noise-saturated environments, especially when trained on world models featuring realistic background noise extracted from natural video streams. We propose Task Relevant Mask Sampling (TRMS), a novel approach for identifying task-specific and reward-relevant masks. TRMS utilizes existing segmentation models as a masking prior, which is subsequently followed by a mask selector that dynamically identifies subset of masks at each timestep, selecting those most probable to contribute to task-specific rewards. To mitigate the high computational cost associated with these masking priors, a lightweight student network is trained in parallel. This network learns to perform masking independently and replaces the Segment Anything Model~(SAM)-based teacher network after a brief initial phase (<10-25% of total training). TRMS enhances the generalization capabilities of Soft Actor-Critic agents under distractions, achieves better performance on the RL-Vigen benchmark, which includes challenging variants of the DeepMind Control Suite, Dexterous Manipulation and Quadruped Locomotion tasks.},
  archive      = {J_TMLR},
  author       = {Vedant Dave and Ozan Özdenizci and Elmar Rueckert},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Learning robust representations for visual reinforcement learning via task-relevant mask sampling},
  url          = {https://openreview.net/forum?id=2rxNDxHwtn},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LIT-LVM: Structured regularization for interaction terms in linear predictors using latent variable models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=3uW5nxESu1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some of the simplest, yet most frequently used predictors in statistics and machine learning use weighted linear combinations of features. Such linear predictors can model non-linear relationships between features by adding interaction terms corresponding to the products of all pairs of features. We consider the problem of accurately estimating coefficients for interaction terms in linear predictors. We hypothesize that the coefficients for different interaction terms have an approximate low-dimensional structure and represent each feature by a latent vector in a low-dimensional space. This low-dimensional representation can be viewed as a structured regularization approach that further mitigates overfitting in high-dimensional settings beyond standard regularizers such as the lasso and elastic net. We demonstrate that our approach, called LIT-LVM, achieves superior prediction accuracy compared to the elastic net, hierarchical lasso, and factorization machines on a wide variety of simulated and real data, particularly when the number of interaction terms is high compared to the number of samples. LIT-LVM also provides low-dimensional latent representations for features that are useful for visualizing and analyzing their relationships.},
  archive      = {J_TMLR},
  author       = {Mohammadreza Nemati and Zhipeng Huang and Kevin S. Xu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {LIT-LVM: Structured regularization for interaction terms in linear predictors using latent variable models},
  url          = {https://openreview.net/forum?id=3uW5nxESu1},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double machine learning based structure identification from temporal data. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=4iHAoFVM2K'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the causes of time-series data is a fundamental task in many applications, spanning from finance to earth sciences or bio-medical applications. Common approaches for this task are based on vector auto-regression, and they do not take into account unknown confounding between potential causes. However, in settings with many potential causes and noisy data, these approaches may be substantially biased. Furthermore, potential causes may be correlated in practical applications or even contain cycles. To address these challenges, we propose a new double machine learning based method for structure identification from temporal data (DR-SIT). We provide theoretical guarantees, showing that our method asymptotically recovers the true underlying causal structure. Our analysis extends to cases where the potential causes have cycles, and they may even be confounded. We further perform extensive experiments to showcase the superior performance of our method. Code: https://github.com/sdi1100041/TMLR_submission_DR_SIT},
  archive      = {J_TMLR},
  author       = {Emmanouil Angelis and Francesco Quinzan and Ashkan Soleymani and Patrick Jaillet and Stefan Bauer},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Double machine learning based structure identification from temporal data},
  url          = {https://openreview.net/forum?id=4iHAoFVM2K},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Byzantine-robust and hessian-free federated bilevel optimization. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=5trmyvtkeo'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, Byzantine robust algorithms to solve a minimization problem in the Federated setup have received significant attention. Most of the existing works consider the problem of byzantine-robustness for single-level optimization or consider the federated bilevel optimization without Byzantine nodes. However, problem formulation such as federated bilevel optimization in the presence of byzantine nodes is unexplored. Recognizing the gap, for the first time, we propose a computationally efficient and robust algorithm for solving Federated Bilevel Optimization with Byzantine (FedBOB) nodes that: \One Work under the assumption that the data across nodes are heterogeneous (non-iid), \2 Consider the lower-level objective is non-convex and satisfies the Polyak-\L ojasiewicz (PL)-inequality, and \3 Is fully first-order and does not rely on second order information. We achieve this by reformulating the federated bilevel problem into a single penalty problem. We provide the theoretical performance of the proposed algorithm and experimentally corroborate our theoretical findings.},
  archive      = {J_TMLR},
  author       = {Shruti P Maralappanavar and Bharath B N},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Byzantine-robust and hessian-free federated bilevel optimization},
  url          = {https://openreview.net/forum?id=5trmyvtkeo},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate bayesian neural operators: Uncertainty quantification for parametric PDEs. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=6WvIkYsMA8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural operators are a type of deep architecture that learns to solve (i.e. learns the nonlinear solution operator of) partial differential equations (PDEs). The current state of the art for these models does not provide explicit uncertainty quantification. This is arguably even more of a problem for this kind of tasks than elsewhere in machine learning, because the dynamical systems typically described by PDEs often exhibit subtle, multiscale structure that makes errors hard to spot by humans. In this work, we first provide a mathematically detailed Bayesian formulation of the ``shallow'' (linear) version of neural operators in the formalism of Gaussian processes. We then extend this analytic treatment to general deep neural operators—specifically, graph neural operators—using approximate methods from Bayesian deep learning, enabling them to incorporate uncertainty quantification. As a result, our approach is able to identify cases, and provide structured uncertainty estimates, where the neural operator fails to predict well.},
  archive      = {J_TMLR},
  author       = {Emilia Magnani and Nicholas Krämer and Runa Eschenhagen and Lorenzo Rosasco and Philipp Hennig},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Approximate bayesian neural operators: Uncertainty quantification for parametric PDEs},
  url          = {https://openreview.net/forum?id=6WvIkYsMA8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding emergent in-context learning from a kernel regression perspective. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=6rD50Q6yYz'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing a kernel-regression perspective of understanding LLMs' ICL behaviors when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context demonstrations grows. Then, we empirically investigate the in-context behaviors of language models. We find that during ICL, the attention and hidden features in LLMs match the behaviors of a kernel regression. Finally, our theory provides insights into multiple phenomena observed in the ICL field: why retrieving demonstrative samples similar to test samples can help, why ICL performance is sensitive to the output formats, and why ICL accuracy benefits from selecting in-distribution and representative samples.},
  archive      = {J_TMLR},
  author       = {Chi Han and Ziqi Wang and Han Zhao and Heng Ji},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Understanding emergent in-context learning from a kernel regression perspective},
  url          = {https://openreview.net/forum?id=6rD50Q6yYz},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudo-asynchronous local SGD: Robust and efficient data-parallel training. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=8VTrvS5vN7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following AI scaling trends, frontier models continue to grow in size and continue to be trained on larger datasets. Training these models requires huge investments in exascale computational resources, which has in turn driven developtment of distributed deep learning methods. Data parallelism is an essential approach to speed up training, but it requires frequent global communication between workers, which can bottleneck training at the largest scales. In this work, we propose a method called Pseudo-Asynchronous Local SGD (PALSGD) to improve the efficiency of data-parallel training. PALSGD is an extension of Local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023), designed to further reduce communication frequency by introducing a pseudo-synchronization mechanism. PALSGD allows the use of longer synchronization intervals compared to standard Local SGD. Despite the reduced communication frequency, the pseudo-synchronization approach ensures that model consistency is maintained, leading to performance results comparable to those achieved with more frequent synchronization. Furthermore, we provide a theoretical analysis of PALSGD, establishing its convergence and deriving its convergence rate. This analysis offers insights into the algorithm's behavior and performance guarantees. We evaluated PALSGD on image classification and language modeling tasks. Our results show that PALSGD achieves better performance in less time compared to existing methods like Distributed Data Parallel (DDP), and DiLoCo. Notably, PALSGD trains 18.4% faster than DDP on ImageNet-1K with ResNet-50, 24.4% faster than DDP on TinyStories with GPT-Neo-125M, and 21.1% faster than DDP on TinyStories with GPT-Neo-8M.},
  archive      = {J_TMLR},
  author       = {Hiroki Naganuma and Xinzhi Zhang and Man-Chung Yue and Ioannis Mitliagkas and Russell J. Hewett and Philipp Andre Witte and Yin Tat Lee},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Pseudo-asynchronous local SGD: Robust and efficient data-parallel training},
  url          = {https://openreview.net/forum?id=8VTrvS5vN7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural causal circuits: Probabilistic circuits climbing all rungs of pearl's ladder of causation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=25XyUTICdZ'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity and vastness of our world can require large models with numerous variables. Unfortunately, coming up with a model that is both accurate and able to provide predictions in a reasonable amount of time can prove difficult. One possibility to help overcome such problems is sum-product networks (SPNs), probabilistic models with the ability to tractably perform inference in linear time. In this paper, we extend SPNs' capabilities to the field of causality and introduce the family of structural causal circuits (SCCs), a type of SPNs capable of answering causal questions. Starting from conventional SPNs, we ``climb the ladder of causation'' and show how SCCs can represent not only observational but also interventional and counterfactual problems. We demonstrate successful application in different settings, ranging from simple binary variables to physics-based simulations.},
  archive      = {J_TMLR},
  author       = {Florian Peter Busch and Moritz Willig and Matej Zečević and Kristian Kersting and Devendra Singh Dhami},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Structural causal circuits: Probabilistic circuits climbing all rungs of pearl's ladder of causation},
  url          = {https://openreview.net/forum?id=25XyUTICdZ},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open problems in mechanistic interpretability. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=91H76m9Z94'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mechanistic interpretability aims to understand the computational mechanisms underlying neural networks' capabilities in order to accomplish concrete scientific and engineering goals. Progress in this field thus promises to provide greater assurance over AI system behavior and shed light on exciting scientific questions about the nature of intelligence. Despite recent progress toward these goals, there are many open problems in the field that require solutions before many scientific and practical benefits can be realized: Our methods require both conceptual and practical improvements to reveal deeper insights; we must figure out how best to apply our methods in pursuit of specific goals; and the field must grapple with socio-technical challenges that influence and are influenced by our work. This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.},
  archive      = {J_TMLR},
  author       = {Lee Sharkey and Bilal Chughtai and Joshua Batson and Jack Lindsey and Jeffrey Wu and Lucius Bushnaq and Nicholas Goldowsky-Dill and Stefan Heimersheim and Alejandro Ortega and Joseph Isaac Bloom and Stella Biderman and Adrià Garriga-Alonso and Arthur Conmy and Neel Nanda and Jessica Mary Rumbelow and Martin Wattenberg and Nandi Schoots and Joseph Miller and William Saunders and Eric J Michaud and Stephen Casper and Max Tegmark and David Bau and Eric Todd and Atticus Geiger and Mor Geva and Jesse Hoogland and Daniel Murfet and Thomas McGrath},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Open problems in mechanistic interpretability},
  url          = {https://openreview.net/forum?id=91H76m9Z94},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model guidance via robust feature attribution. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=AVAHxDSqUu'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlling the patterns a model learns is essential to preventing reliance on irrelevant or misleading features. Such reliance on irrelevant features, often called shortcut features, has been observed across domains, including medical imaging and natural language processing, where it may lead to real-world harms. A common mitigation strategy leverages annotations (provided by humans or machines) indicating which features are relevant or irrelevant. These annotations are compared to model explanations, typically in the form of feature salience, and used to guide the loss function during training. Unfortunately, recent works have demonstrated that feature salience methods are unreliable and therefore offer a poor signal to optimize. In this work, we propose a simplified objective that simultaneously optimizes for explanation robustness and mitigation of shortcut learning. Unlike prior objectives with similar aims, we demonstrate theoretically why our approach ought to be more effective. Across a comprehensive series of experiments, we show that our approach consistently reduces test-time misclassifications by 20\% compared to state-of-the-art methods. We also extend prior experimental settings to include natural language processing tasks. Additionally, we conduct novel ablations that yield practical insights, including the relative importance of annotation quality over quantity. Code for our method and experiments is available at: https://github.com/Mihneaghitu/ModelGuidanceViaRobustFeatureAttribution.},
  archive      = {J_TMLR},
  author       = {Mihnea Ghitu and Vihari Piratla and Matthew Robert Wicker},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Model guidance via robust feature attribution},
  url          = {https://openreview.net/forum?id=AVAHxDSqUu},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two is better than one: Aligned representation pairs for anomaly detection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Bt0zdsnWYc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection focuses on identifying samples that deviate from the norm. Discovering informative representations of normal samples is crucial to detecting anomalies effectively. Recent self-supervised methods have successfully learned such representations by employing prior knowledge about anomalies to create synthetic outliers during training. However, we often do not know what to expect from unseen data in specialized real-world applications. In this work, we address this limitation with our new approach, Con2, which leverages prior knowledge about symmetries in normal samples to observe the data in different contexts. Con2 consists of two parts: Context Contrasting clusters representations according to their context, while Content Alignment encourages the model to capture semantic information by aligning the positions of normal samples across clusters. The resulting representation space allows us to detect anomalies as outliers of the learned context clusters. We demonstrate the benefit of this approach in extensive experiments on specialized medical datasets, outperforming competitive baselines based on self-supervised learning and pretrained models and presenting competitive performance on natural imaging benchmarks.},
  archive      = {J_TMLR},
  author       = {Alain Ryser and Thomas M. Sutter and Alexander Marx and Julia E Vogt},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Two is better than one: Aligned representation pairs for anomaly detection},
  url          = {https://openreview.net/forum?id=Bt0zdsnWYc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling multiple descents in unsupervised autoencoders. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=FqfHDs6unx'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The phenomenon of double descent has challenged the traditional bias-variance trade-off in supervised learning but remains unexplored in unsupervised learning, with some studies arguing for its absence. In this study, we first demonstrate analytically that double descent does not occur in linear unsupervised autoencoders (AEs). In contrast, we show for the first time that both double and triple descent can be observed with nonlinear AEs across various data models and architectural designs. We examine the effects of partial sample and feature noise and highlight the critical role of bottleneck size in shaping the double descent curve. Through extensive experiments on both synthetic and real datasets, we uncover model-wise, epoch-wise, and sample-wise double descent across several data types and architectures. Our findings indicate that over-parameterized models not only improve reconstruction but also enhance performance in downstream tasks such as anomaly detection and domain adaptation, highlighting their practical value in complex real-world scenarios.},
  archive      = {J_TMLR},
  author       = {Kobi Rahimi and Yehonathan Refael and Tom Tirer and Ofir Lindenbaum},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Unveiling multiple descents in unsupervised autoencoders},
  url          = {https://openreview.net/forum?id=FqfHDs6unx},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unifi3D: A study on 3D representations for generation and reconstruction in a common framework. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=GQpTWpXILA'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following rapid advancements in text and image generation, research has increasingly shifted towards 3D generation. Unlike the well-established pixel-based representation in images, 3D representations remain diverse and fragmented, encompassing a wide variety of approaches such as voxel grids, neural radiance fields, signed distance functions, point clouds, or octrees, each offering distinct advantages and limitations. In this work, we present a unified evaluation framework designed to assess the performance of 3D representations in reconstruction and generation. We compare these representations based on multiple criteria: quality, computational efficiency, and generalization performance. Beyond standard model benchmarking, our experiments aim to derive best practices over all steps involved in the 3D generation pipeline, including preprocessing, mesh reconstruction, compression with autoencoders, and generation. Our findings highlight that reconstruction errors significantly impact overall performance, underscoring the need to evaluate generation and reconstruction jointly. We provide insights that can inform the selection of suitable 3D models for various applications, facilitating the development of more robust and application-specific solutions in 3D generation. The code for our framework is available at https://github.com/isl-org/unifi3d.},
  archive      = {J_TMLR},
  author       = {Nina Wiedemann and Sainan Liu and Quentin Leboutet and Katelyn Gao and Benjamin Ummenhofer and Michael Paulitsch and Kai Yuan},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Unifi3D: A study on 3D representations for generation and reconstruction in a common framework},
  url          = {https://openreview.net/forum?id=GQpTWpXILA},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-1-to-G: Taming pretrained 2D diffusion model for direct 3D generation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=GVizav9Zf8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in 2D image generation have achieved remarkable quality, largely driven by the capacity of diffusion models and the availability of large-scale datasets. However, direct 3D generation is still constrained by the scarcity and lower fidelity of 3D datasets. In this paper, we introduce Zero-1-to-G, a novel approach that addresses this problem by enabling direct single-view generation on Gaussian splats using pretrained 2D diffusion models. Our key insight is that Gaussian splats, a 3D representation, can be decomposed into multi-view images encoding different attributes. This reframes the challenging task of direct 3D generation within a 2D diffusion framework, allowing us to leverage the rich priors of pretrained 2D diffusion models. To incorporate 3D awareness, we introduce cross-view and cross-attribute attention layers, which capture complex correlations and enforce 3D consistency across generated splats. This makes Zero-1-to-G the first direct image-to-3D generative model to effectively utilize pretrained 2D diffusion priors, enabling efficient training and improved generalization to unseen objects. Extensive experiments on both synthetic and in-the-wild datasets demonstrate superior performance in 3D object generation, offering a new approach to high-quality 3D generation.},
  archive      = {J_TMLR},
  author       = {Xuyi Meng and Chen Wang and Jiahui Lei and Kostas Daniilidis and Jiatao Gu and Lingjie Liu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Zero-1-to-G: Taming pretrained 2D diffusion model for direct 3D generation},
  url          = {https://openreview.net/forum?id=GVizav9Zf8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low compute unlearning via sparse representations. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=GyKXzmk43s'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine unlearning, which involves erasing knowledge about a \emph{forget set} from a trained model, can prove to be costly and infeasible using existing techniques. We propose a low-compute unlearning technique based on a discrete representational bottleneck. We show that the proposed technique efficiently unlearns the forget set and incurs negligible damage to the model's performance on the rest of the dataset. We evaluate the proposed technique on the problem of class unlearning using four datasets: CIFAR-10, CIFAR-100, LACUNA-100 and ImageNet-1k. We compare the proposed technique to SCRUB, a state-of-the-art approach which uses knowledge distillation for unlearning. Across all four datasets, the proposed technique performs as well as, if not better than SCRUB while incurring almost no computational cost.},
  archive      = {J_TMLR},
  author       = {Vedant Shah and Frederik Träuble and Ashish Malik and Hugo Larochelle and Michael Curtis Mozer and Sanjeev Arora and Yoshua Bengio and Anirudh Goyal},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Low compute unlearning via sparse representations},
  url          = {https://openreview.net/forum?id=GyKXzmk43s},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRDT3: Diffusion-refined decision test-time training model. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=I6zjLhIzgh'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision Transformer (DT), a trajectory modelling method, has shown competitive performance compared to traditional offline reinforcement learning (RL) approaches on various classic control tasks. However, it struggles to learn optimal policies from suboptimal, reward-labelled trajectories. In this study, we explore the use of conditional generative modelling to facilitate trajectory stitching given its high-quality data generation ability. Additionally, recent advancements in Recurrent Neural Networks (RNNs) have shown their linear complexity and competitive sequence modelling performance over Transformers. We leverage the Test-Time Training (TTT) layer, an RNN that updates hidden states during testing, to model trajectories in the form of DT. We introduce a unified framework, called Diffusion-Refined Decision TTT (DRDT3), to achieve performance beyond DT models. Specifically, we propose the Decision TTT (DT3) module, which harnesses the sequence modelling strengths of both self-attention and the TTT layer to capture recent contextual information and make coarse action predictions. DRDT3 iteratively refines the coarse action predictions through the generative diffusion model, progressively moving closer to the optimal actions. We further integrate DT3 with the diffusion model using a unified optimization objective. With experiments on multiple tasks in the D4RL benchmark, our DT3 model without diffusion refinement demonstrates improved performance over standard DT, while DRDT3 further achieves superior results compared to state-of-the-art DT-based and offline RL methods.},
  archive      = {J_TMLR},
  author       = {Xingshuai Huang and Di Wu and Benoit Boulet},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {DRDT3: Diffusion-refined decision test-time training model},
  url          = {https://openreview.net/forum?id=I6zjLhIzgh},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification in retrieval augmented question answering. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=JLkgI0h7wy'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieval augmented Question Answering (QA) helps QA models overcome knowledge gaps by incorporating retrieved evidence, typically a set of passages, alongside the question at test time. Previous studies show that this approach improves QA performance and reduces hallucinations, without, however, assessing whether the retrieved passages are indeed useful at answering correctly. In this work, we propose to quantify the uncertainty of a QA model via estimating the utility of the passages it is provided with. We train a lightweight neural model to predict passage utility for a target QA model and show that while simple information theoretic metrics can predict answer correctness up to a certain extent, our approach efficiently approximates or outperforms more expensive sampling-based methods.},
  archive      = {J_TMLR},
  author       = {Laura Perez-Beltrachini and Mirella Lapata},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Uncertainty quantification in retrieval augmented question answering},
  url          = {https://openreview.net/forum?id=JLkgI0h7wy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Amdahl’s law for LLMs: A throughput-centric analysis of extreme LLM quantization. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=JtrQJJQYpP'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of 1-bit large language models (LLMs) has sparked significant interest, promising substantial efficiency gains through extreme quantization. However, these benefits are inherently limited by the portion of the model that can be quantized. Specifically, 1-bit quantization typically targets only the projection layers, while the attention mechanisms remain in higher precision, potentially creating significant throughput bottlenecks. To address this, we present an adaptation of Amdahl's Law specifically tailored to the LLMs, offering a quantitative framework for understanding the throughput limits of extreme quantization. Our analysis reveals how improvements in quantization can deliver substantial throughput gains, but only to the extent that they address critical throughput-constrained sections of the model. Through extensive experiments across diverse model architectures and hardware platforms, we highlight key trade-offs and performance ceilings, providing a roadmap for future research aimed at maximizing LLM throughput through more holistic quantization strategies.},
  archive      = {J_TMLR},
  author       = {Jinendra Malekar and Ramtin Zand},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Amdahl’s law for LLMs: A throughput-centric analysis of extreme LLM quantization},
  url          = {https://openreview.net/forum?id=JtrQJJQYpP},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomous imagination: Closed-loop decomposition of visual-to-textual conversion in visual reasoning for multimodal large language models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=MI4yIBLprs'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under pure textual modality, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning tasks by decomposing them into simpler sub-problems. However, Multimodal Large Language Models (MLLMs) still struggle with some seemingly straightforward visual tasks, such as counting and solving jigsaw puzzles. We argue that these tasks challenge the ability of {\it visual-to-textual conversion}, where MLLMs convert visual information perceived from the input scene, to textual information for further reasoning and generating the answer. If the complexity of the visual input is beyond the perceptual capability of the MLLMs, without decomposing this conversion process, simply scaling inference-time reasoning cannot solve the task because it repeatedly encounters the same perceptual bottleneck. We propose an approach, {\it autonomous imagination}, to enable MLLMs to iteratively modify visual inputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate visual states, decomposing visual-to-textual conversion into closed-loop visual modification steps. We show that, without any retraining, MLLMs can now solve tasks initially beyond their perceptual capability, highlighting that closed-loop visual modification can be an effective way of decomposing the visual reasoning task into solvable substeps. Our code and data are released at https://future-item.github.io/autoimagine-site/.},
  archive      = {J_TMLR},
  author       = {Jingming Liu and Yumeng Li and Boyuan Xiao and Yichang Jian and Ziang Qin and Tianjia Shao and Yao-Xiang Ding and Kun Zhou},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Autonomous imagination: Closed-loop decomposition of visual-to-textual conversion in visual reasoning for multimodal large language models},
  url          = {https://openreview.net/forum?id=MI4yIBLprs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A noise-corrected langevin algorithm and sampling by half-denoising. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=QGtXn5GtfK'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Langevin algorithm is a classic method for sampling from a given pdf in a real space. In its basic version, it only requires knowledge of the gradient of the log-density, also called the score function. However, in deep learning, it is often easier to learn the so-called "noisy-data score function", i.e. the gradient of the log-density of noisy data, more precisely when Gaussian noise is added to the data. Such an estimate is biased and complicates the use of the Langevin method. Here, we propose a noise-corrected version of the Langevin algorithm, where the bias due to noisy data is removed, at least regarding first-order terms. Unlike diffusion models, our algorithm only needs to know the noisy-data score function for one single noise level. We further propose a simple special case which has an interesting intuitive interpretation of iteratively adding noise the data and then attempting to remove half of that noise.},
  archive      = {J_TMLR},
  author       = {Aapo Hyvarinen},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {A noise-corrected langevin algorithm and sampling by half-denoising},
  url          = {https://openreview.net/forum?id=QGtXn5GtfK},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RouteFinder: Towards foundation models for vehicle routing problems. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=QzGLoaOPiY'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces RouteFinder, a comprehensive foundation model framework to tackle different Vehicle Routing Problem (VRP) variants. Our core idea is that a foundation model for VRPs should be able to represent variants by treating each as a subset of a generalized problem equipped with different attributes. We propose a unified VRP environment capable of efficiently handling any combination of these attributes. The RouteFinder model leverages a modern transformer-based encoder and global attribute embeddings to improve task representation. Additionally, we introduce two reinforcement learning techniques to enhance multi-task performance: mixed batch training, which enables training on different variants at once, and multi-variant reward normalization to balance different reward scales. Finally, we propose efficient adapter layers that enable fine-tuning for new variants with unseen attributes. Extensive experiments on 48 VRP variants show RouteFinder outperforms recent state-of-the-art learning methods. Our code is publicly available at https://github.com/ai4co/routefinder.},
  archive      = {J_TMLR},
  author       = {Federico Berto and Chuanbo Hua and Nayeli Gast Zepeda and André Hottung and Niels Wouda and Leon Lan and Junyoung Park and Kevin Tierney and Jinkyoo Park},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {RouteFinder: Towards foundation models for vehicle routing problems},
  url          = {https://openreview.net/forum?id=QzGLoaOPiY},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M4GN: Mesh-based multi-segment hierarchical graph network for dynamic simulations. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=R3vDbqWa1v'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh-based graph neural networks (GNNs) have become effective surrogates for PDE simulations, yet their deep message passing incurs high cost and over‑smoothing on large, long‑range meshes; hierarchical GNNs shorten propagation paths but still face two key obstacles: (i) building coarse graphs that respect mesh topology, geometry, and physical discontinuities, and (ii) maintaining fine-scale accuracy without sacrificing the speed gained from coarsening. We tackle these challenges with M4GN—a three‑tier, segment‑centric hierarchical network. M4GN begins with a hybrid segmentation strategy that pairs a fast graph partitioner with a superpixel‑style refinement guided by modal‑decomposition features, producing contiguous segments of dynamically consistent nodes. These segments are encoded by a permutation‑invariant aggregator, avoiding the order sensitivity and quadratic cost of aggregation approaches used in prior works. The resulting information bridges a micro‑level GNN—which captures local dynamics—and a macro‑level transformer that reasons efficiently across segments, achieving a principled balance between accuracy and efficiency. Evaluated on multiple representative benchmark datasets, M4GN improves prediction accuracy by up to 56\% while achieving up to 22\% faster inference than state‑of‑the‑art baselines.},
  archive      = {J_TMLR},
  author       = {Bo Lei and Victor M Castillo and Yeping Hu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {M4GN: Mesh-based multi-segment hierarchical graph network for dynamic simulations},
  url          = {https://openreview.net/forum?id=R3vDbqWa1v},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoNNect: Connectivity-based regularization for structural pruning of neural networks. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=RIZCe7BuEp'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning encompasses a range of techniques aimed at increasing the sparsity of neural networks (NNs). These techniques can generally be framed as minimizing a loss function subject to an $L_0$ norm constraint. This paper introduces CoNNect, a novel differentiable regularizer for sparse NN training that ensures connectivity between input and output layers. We prove that CoNNect approximates $L_0$ regularization, while preserving essential network structure and preventing the emergence of fragmented or poorly connected subnetworks. Moreover, CoNNect is easily integrated within established structural pruning strategies. Numerical experiments demonstrate that CoNNect can improve classical pruning strategies and enhance state-of-the-art one-shot pruners, such as DepGraph and LLM-pruner.},
  archive      = {J_TMLR},
  author       = {Christian P.C. Franssen and Jinyang Jiang and Yijie Peng and Bernd Heidergott},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {CoNNect: Connectivity-based regularization for structural pruning of neural networks},
  url          = {https://openreview.net/forum?id=RIZCe7BuEp},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to rank with top-$K$ fairness. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=SSPCc39XvO'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness in ranking models is crucial, as disparities in exposure can disproportionately affect protected groups. Most fairness-aware ranking systems focus on ensuring comparable average exposure for groups across the entire ranked list, which may not fully address real-world concerns. For example, when a ranking model is used for allocating resources among candidates or disaster hotspots, decision-makers often prioritize only the top-$K$ ranked items, while the ranking beyond top-$K$ becomes less relevant. In this paper, we propose a list-wise learning-to-rank framework that addresses the issues of inequalities in top-$K$ rankings at training time. Specifically, we propose a top-$K$ exposure disparity measure that extends the classic exposure disparity metric in a ranked list. We then learn a ranker to balance relevance and fairness in top-$K$ rankings. Since direct top-$K$ selection is computationally expensive for a large number of items, we transform the non-differentiable selection process into a differentiable objective function and develop efficient stochastic optimization algorithms to achieve both high accuracy and sufficient fairness. Extensive experiments demonstrate that our method outperforms existing methods.},
  archive      = {J_TMLR},
  author       = {Boyang Zhang and Quanqi Hu and Mingxuan Sun and Qihang Lin and Tianbao Yang},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Learning to rank with top-$K$ fairness},
  url          = {https://openreview.net/forum?id=SSPCc39XvO},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solution augmentation for ARC problems using GFlowNet: A probabilistic exploration approach. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ULCOhBgGzy'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the core challenges in building general reasoning systems lies in generating diverse, human-aligned solution trajectories—different yet valid paths by which a problem can be solved. Prior approaches often rely on handcrafted templates, rule-based augmentations, or human demonstrations, which are limited in scalability and stylistic diversity. To address this, we explore the use of Generative Flow Networks (GFlowNets) for automated solution augmentation in reasoning tasks. We propose a framework that learns to generate diverse reasoning trajectories with probabilities proportional to their quality, guided by a human-inspired reward function and a novel geometric forward policy. This enables the generation of multiple plausible solution paths without relying on manual supervision. Moreover, our method supports efficient test-time augmentation from input-output examples alone, without access to ground-truth programs or external demonstrations—making it suitable for zero-shot settings. We evaluate our framework on the Abstraction and Reasoning Corpus (ARC-AGI), a benchmark designed to test compositional and abstract reasoning. Our results show that GFlowNets can effectively explore the space of valid reasoning processes, producing a variety of plausible reasoning trajectories, similar to how different individuals might solve the same problem using different intermediate steps. These trajectories are generated at scale-over 100k per task in under an hour, and follow a logarithmic yield trend, enabling practical tradeoffs between augmentation volume and novelty. Furthermore, fine-tuning a large language model (LLaMA 3.1 Instruct 8B) on these synthetic trajectories leads to a 28.6% improvement in reasoning accuracy on ARC tasks, demonstrating the downstream utility of our method. These findings suggest that GFlowNets offer a promising foundation for modeling structured reasoning in automated trajectory generation. Our code is here: https://github.com/GIST-DSLab/GFN_to_ARC},
  archive      = {J_TMLR},
  author       = {Sanha Hwang and Seungpil Lee and Sejin Kim and Sundong Kim},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Solution augmentation for ARC problems using GFlowNet: A probabilistic exploration approach},
  url          = {https://openreview.net/forum?id=ULCOhBgGzy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wolf: Dense video captioning with a world summarization framework. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Z1dH7hao7p'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Wolf, a WOrLd summarization Framework for accurate video captioning. Wolf is an automated captioning framework that adopts a mixture-of-experts approach, leveraging complementary strengths of Vision Language Models (VLMs). By utilizing both image and video models, our framework captures different levels of information and summarizes them efficiently. Our approach can be applied to enhance video understanding, auto-labeling, and captioning. To evaluate caption quality, we introduce CapScore, an LLM-based metric to assess the similarity and quality of generated captions compared to the ground truth captions. We further build four human-annotated datasets in three domains: autonomous driving, general scenes, and robotics, to facilitate comprehensive comparisons. We show that Wolf achieves superior captioning performance compared to state-of-the-art approaches from the research community (VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For instance, in comparison with GPT-4V, Wolf improves CapScore (caption quality) by 55.6% and CapScore (caption similarity) by 77.4% on challenging driving videos. Finally, we establish a benchmark for video captioning and introduce a leaderboard, aiming to accelerate advancements in video understanding, captioning, and data alignment.},
  archive      = {J_TMLR},
  author       = {Boyi Li and Ligeng Zhu and Ran Tian and Shuhan Tan and Yuxiao Chen and Yao Lu and Yin Cui and Sushant Veer and Max Ehrlich and Jonah Philion and Xinshuo Weng and Fuzhao Xue and Linxi Fan and Yuke Zhu and Jan Kautz and Andrew Tao and Ming-Yu Liu and Sanja Fidler and Boris Ivanovic and Trevor Darrell and Jitendra Malik and Song Han and Marco Pavone},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Wolf: Dense video captioning with a world summarization framework},
  url          = {https://openreview.net/forum?id=Z1dH7hao7p},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeedleBench: Evaluating LLM retrieval and reasoning across varying information densities. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=cEvmIKsRw0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability of large language models to handle long-context information plays a crucial role across various real-world applications. Existing methods for evaluating long-context abilities often rely either on real-world long texts, making it difficult to exclude the influence of models' inherent knowledge, or introduce large amounts of irrelevant filler content to artificially reach target lengths, reducing the relevance and effectiveness of assessments. To address these limitations, we introduce NeedleBench, a comprehensive synthetic framework designed to assess retrieval and reasoning performance in bilingual long-context tasks with adaptive context lengths (e.g., 32k, 128k, and beyond). NeedleBench systematically embeds key data points at varying depths to rigorously test models' capabilities in diverse settings. Tasks within NeedleBench are categorized into two distinct scenarios: information-sparse, characterized by minimal relevant details embedded within extensive irrelevant text to simulate simpler real-world retrieval tasks; and information-dense, implemented as the Ancestral Trace Challenge, where relevant information is continuously distributed throughout the context to simulate more complex real-world reasoning tasks. Our experiments show that, while recent reasoning models such as Deepseek-R1 and OpenAI's o3 have demonstrated strong performance on mathematical reasoning benchmarks, they still struggle to generalize their reasoning abilities and perform poorly on our information-dense tasks, frequently encountering difficulties with continuous retrieval and reasoning even at relatively shorter context lengths.Furthermore, we identify and characterize a phenomenon termed `under-thinking', wherein models prematurely conclude their reasoning processes despite the availability of relevant information. NeedleBench thus provides critical insights and targeted evaluation tools essential for understanding and improving the long-context capabilities of LLMs. All codes and resources are publicly available at https://github.com/open-compass/opencompass.},
  archive      = {J_TMLR},
  author       = {Mo Li and Songyang Zhang and Taolin Zhang and Haodong Duan and Yunxin Liu and Kai Chen},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {NeedleBench: Evaluating LLM retrieval and reasoning across varying information densities},
  url          = {https://openreview.net/forum?id=cEvmIKsRw0},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaGFN: Exploring distant modes with adapted metadynamics for continuous GFlowNets. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=dtyNeemB7A'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Flow Networks (GFlowNets) are a class of generative models that sample objects in proportion to a specified reward function through a learned policy. They can be trained either on-policy or off-policy, needing a balance between exploration and exploitation for fast convergence to a target distribution. While exploration strategies for discrete GFlowNets have been studied, exploration in the continuous case remains to be investigated, despite the potential for novel exploration algorithms due to the local connectedness of continuous domains. Here, we introduce Adapted Metadynamics, a variant of metadynamics that can be applied to arbitrary black-box reward functions on continuous domains. We use Adapted Metadynamics as an exploration strategy for continuous GFlowNets. We show several continuous domains where the resulting algorithm, MetaGFN, accelerates convergence to the target distribution and discovers more distant reward modes than previous off-policy exploration strategies used for training GFlowNets.},
  archive      = {J_TMLR},
  author       = {Dominic Phillips and Flaviu Cipcigan},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {MetaGFN: Exploring distant modes with adapted metadynamics for continuous GFlowNets},
  url          = {https://openreview.net/forum?id=dtyNeemB7A},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simplifying knowledge transfer in pretrained models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=eQ9AVtDaP3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pretrained models are ubiquitous in the current deep learning landscape, offering strong results on a broad range of tasks. Recent works have shown that models differing in various design choices exhibit categorically diverse generalization behavior, resulting in one model grasping distinct data-specific insights unavailable to the other. In this paper, we propose to leverage large publicly available model repositories as an auxiliary source of model improvements. We introduce a data partitioning strategy where pretrained models autonomously adopt either the role of a student, seeking knowledge, or that of a teacher, imparting knowledge. Experiments across various tasks demonstrate the effectiveness of our proposed approach. In image classification, we improved the performance of ViT-B by approximately 1.4\% through bidirectional knowledge transfer with ViT-T. For semantic segmentation, our method boosted all evaluation metrics by enabling knowledge transfer both within and across backbone architectures. In video saliency prediction, our approach achieved a new state-of-the-art. We further extend our approach to knowledge transfer between multiple models, leading to considerable performance improvements for all model participants.},
  archive      = {J_TMLR},
  author       = {Siddharth Jain and Shyamgopal Karthik and Vineet Gandhi},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Simplifying knowledge transfer in pretrained models},
  url          = {https://openreview.net/forum?id=eQ9AVtDaP3},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hallucination detection on a budget: Efficient bayesian estimation of semantic entropy. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=j2N2RuNdbC'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting whether an LLM hallucinates is an important research challenge. One promising way of doing so is to estimate the semantic entropy (Farquhar et al., 2024) of the distribution of generated sequences. We propose a new algorithm for doing that, with two main advantages. First, due to us taking the Bayesian approach, we achieve a much better quality of semantic entropy estimates for a given budget of samples from the LLM. Second, we are able to tune the number of samples adaptively so that `harder' contexts receive more samples. We demonstrate empirically that our approach systematically beats the baselines, requiring only 53% of samples used by Farquhar et al. (2024) to achieve the same quality of hallucination detection as measured by AUROC. Moreover, quite counterintuitively, our estimator is useful even with just one sample from the LLM.},
  archive      = {J_TMLR},
  author       = {Kamil Ciosek and Nicolò Felicioni and Sina Ghiassian},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Hallucination detection on a budget: Efficient bayesian estimation of semantic entropy},
  url          = {https://openreview.net/forum?id=j2N2RuNdbC},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing vision backbones for dense prediction with dense attentive probing. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=neMAx4uBlh'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paradigm of pretraining a backbone on a large set of (often unlabeled) images has gained popularity. The quality of the resulting features is commonly measured by freezing the backbone and training different task heads on top of it. However, current evaluations cover only classifications of whole images or require complex dense task heads which introduce a large number of parameters and add their own inductive biases. In this work, we propose dense attentive probing, a parameter-efficient readout method for dense prediction on arbitrary backbones – independent of the size and resolution of their feature volume. To this end, we extend cross-attention with distance-based masks of learnable sizes. We employ this method to evaluate 18 common backbones on dense predictions tasks in three dimensions: instance awareness, local semantics and spatial understanding. We find that DINOv2 outperforms all other backbones tested – including those supervised with masks and language – across all three task categories. Furthermore, our analysis suggests that self-supervised pretraining tends to yield features that separate object instances better than vision-language models. Code is available at http://eckerlab.org/code/deap.},
  archive      = {J_TMLR},
  author       = {Timo Lüddecke and Alexander S. Ecker},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Characterizing vision backbones for dense prediction with dense attentive probing},
  url          = {https://openreview.net/forum?id=neMAx4uBlh},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global optimization algorithm through high-resolution sampling. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=r3VEA1AWY5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an optimization algorithm that can identify a global minimum of a potentially nonconvex smooth function with high probability, assuming the Gibbs measure of the potential satisfies a logarithmic Sobolev inequality. Our contribution is twofold: on the one hand we propose said global optimization method, which is built on an oracle sampling algorithm producing arbitrarily accurate samples from a given Gibbs measure. On the other hand, we propose a new sampling algorithm, drawing inspiration from both overdamped and underdamped Langevin dynamics, as well as from the high-resolution differential equation known for its acceleration in deterministic settings. While the focus of the paper is primarily theoretical, we demonstrate the effectiveness of our algorithms on the Rastrigin function, where it outperforms recent approaches.},
  archive      = {J_TMLR},
  author       = {Daniel Cortild and Claire Delplancke and Nadia Oudjane and Juan Peypouquet},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Global optimization algorithm through high-resolution sampling},
  url          = {https://openreview.net/forum?id=r3VEA1AWY5},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mutual information perspective on multiple latent variable generative models for positive view generation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=uaj8ZL2PtK'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image generation, Multiple Latent Variable Generative Models (MLVGMs) employ multiple latent variables to gradually shape the final images, from global characteristics to finer and local details (e.g., StyleGAN, NVAE), emerging as powerful tools for diverse applications. Yet their generative dynamics remain only empirically observed, without a systematic understanding of each latent variable's impact. In this work, we propose a novel framework that quantifies the contribution of each latent variable using Mutual Information (MI) as a metric. Our analysis reveals that current MLVGMs often underutilize some latent variables, and provides actionable insights for their use in downstream applications. With this foundation, we introduce a method for generating synthetic data for Self-Supervised Contrastive Representation Learning (SSCRL). By leveraging the hierarchical and disentangled variables of MLVGMs, our approach produces diverse and semantically meaningful views without the need for real image data. Additionally, we introduce a Continuous Sampling (CS) strategy, where the generator dynamically creates new samples during SSCRL training, greatly increasing data variability. Our comprehensive experiments demonstrate the effectiveness of these contributions, showing that MLVGMs' generated views compete on par with or even surpass views generated from real data. This work establishes a principled approach to understanding and exploiting MLVGMs, advancing both generative modeling and self-supervised learning. Code and pre-trained models at: https://github.com/SerezD/mi_ml_gen},
  archive      = {J_TMLR},
  author       = {Dario Serez and Marco Cristani and Alessio Del Bue and Vittorio Murino and Pietro Morerio},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {A mutual information perspective on multiple latent variable generative models for positive view generation},
  url          = {https://openreview.net/forum?id=uaj8ZL2PtK},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond instance consistency: Investigating view diversity in self-supervised learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=urWCU3YMA0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) conventionally relies on the instance consistency paradigm, assuming that different views of the same image can be treated as positive pairs. However, this assumption breaks down for non-iconic data, where different views may contain distinct objects or semantic information. In this paper, we investigate the effectiveness of SSL when instance consistency is not guaranteed. Through extensive ablation studies, we demonstrate that SSL can still learn meaningful representations even when positive pairs lack strict instance consistency. Furthermore, our analysis further reveals that increasing view diversity, by enforcing zero overlapping or using smaller crop scales, can enhance downstream performance on classification and dense prediction tasks. However, excessive diversity is found to reduce effectiveness, suggesting an optimal range for view diversity. To quantify this, we adopt the Earth Mover’s Distance (EMD) as an estimator to measure mutual information between views, finding that moderate EMD values correlate with improved SSL learning, providing insights for future SSL framework design. We validate our findings across a range of settings, highlighting their robustness and applicability on diverse data sources.},
  archive      = {J_TMLR},
  author       = {Huaiyuan Qin and Muli Yang and Siyuan Hu and Peng Hu and Yu Zhang and Chen Gong and Hongyuan Zhu},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Beyond instance consistency: Investigating view diversity in self-supervised learning},
  url          = {https://openreview.net/forum?id=urWCU3YMA0},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wasserstein convergence of score-based generative models under semiconvexity and discontinuous gradients. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=vS9iVRB7XF'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Score-based Generative Models (SGMs) approximate a data distribution by perturbing it with Gaussian noise and subsequently denoising it via a learned reverse diffusion process. These models excel at modeling complex data distributions and generating diverse samples, achieving state-of-the-art performance across domains such as computer vision, audio generation, reinforcement learning, and computational biology. Despite their empirical success, existing Wasserstein-2 convergence analysis typically assume strong regularity conditions--such as smoothness or strict log-concavity of the data distribution--that are rarely satisfied in practice. In this work, we establish the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs targeting semiconvex distributions with potentially discontinuous gradients. Our upper bounds are explicit and sharp in key parameters, achieving optimal dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of order one. The framework accommodates a wide class of practically relevant distributions, including symmetric modified half-normal distributions, Gaussian mixtures, double-well potentials, and elastic net potentials. By leveraging semiconvexity without requiring smoothness assumptions on the potential such as differentiability, our results substantially broaden the theoretical foundations of SGMs, bridging the gap between empirical success and rigorous guarantees in non-smooth, complex data regimes.},
  archive      = {J_TMLR},
  author       = {Stefano Bruno and Sotirios Sabanis},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Wasserstein convergence of score-based generative models under semiconvexity and discontinuous gradients},
  url          = {https://openreview.net/forum?id=vS9iVRB7XF},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taxonomy, opportunities, and challenges of representation engineering for large language models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=2U1KIfmaU9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation Engineering (RepE) is a novel paradigm for controlling the behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune the model, RepE directly manipulates the model's internal representations. As a result, it may offer more effective, interpretable, data-efficient, and flexible control over models' behavior. We present the first comprehensive survey of RepE for LLMs, reviewing the rapidly growing literature to address key questions: What RepE methods exist and how do they differ? For what concepts and problems has RepE been applied? What are the strengths and weaknesses of RepE compared to other methods? To answer these, we propose a unified framework describing RepE as a pipeline comprising representation identification, operationalization, and control. We posit that while RepE methods offer significant potential, challenges remain, including managing multiple concepts, ensuring reliability, and preserving models' performance. Towards improving RepE, we identify opportunities for experimental and methodological improvements and construct a guide for best practices.},
  archive      = {J_TMLR},
  author       = {Jan Wehner and Sahar Abdelnabi and Daniel Chee Hian Tan and David Krueger and Mario Fritz},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Taxonomy, opportunities, and challenges of representation engineering for large language models},
  url          = {https://openreview.net/forum?id=2U1KIfmaU9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local distribution-based adaptive oversampling for imbalanced regression. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=6qYTR9iJdm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced regression occurs when continuous target variables have skewed distributions, creating sparse regions that are difficult for machine learning models to predict accurately. This issue particularly affects neural networks, which often struggle with imbalanced data. While class imbalance in classification has been extensively studied, imbalanced regression remains relatively unexplored, with few effective solutions. Existing approaches often rely on arbitrary thresholds to categorize samples as rare or frequent, ignoring the continuous nature of target distributions. These methods can produce synthetic samples that fail to improve model performance and may discard valuable information through undersampling. To address these limitations, we propose LDAO (Local Distribution-based Adaptive Oversampling), a novel data-level approach that avoids categorizing individual samples as rare or frequent. Instead, LDAO learns the global distribution structure by decomposing the dataset into a mixture of local distributions, each preserving its statistical characteristics. LDAO then models and samples from each local distribution independently before merging them into a balanced training set. LDAO achieves a balanced representation across the entire target range while preserving the inherent statistical structure within each local distribution. In extensive evaluations on 45 imbalanced datasets, LDAO outperforms state-of-the-art oversampling methods on both frequent and rare target values, demonstrating its effectiveness for addressing the challenge of imbalanced regression.},
  archive      = {J_TMLR},
  author       = {Shayan Alahyari and Mike Domaratzki},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Local distribution-based adaptive oversampling for imbalanced regression},
  url          = {https://openreview.net/forum?id=6qYTR9iJdm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized orders of magnitude for scalable, parallel, high-dynamic-range computation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=SUuzb0SOGu'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many domains, from deep learning to finance, require compounding real numbers over long sequences, often leading to catastrophic numerical underflow or overflow. We introduce generalized orders of magnitude (GOOMs), a principled extension of traditional orders of magnitude that incorporates floating-point numbers as a special case, and which in practice enables stable computation over significantly larger dynamic ranges of real numbers than previously possible. We implement GOOMs, along with an efficient custom parallel prefix scan, to support native execution on parallel hardware such as GPUs. We demonstrate that our implementation of GOOMs outperforms traditional approaches with three representative experiments, all of which were previously considered impractical or impossible, and now become possible and practical: (1) compounding real matrix products {\em far} beyond standard floating-point limits; (2) estimating spectra of Lyapunov exponents in parallel, {\em orders of magnitude faster} than with previous methods, applying a novel selective-resetting method to prevent state colinearity; and (3) capturing long-range dependencies in deep recurrent neural networks with {\em non-diagonal recurrent states, computed in parallel via a prefix scan, without requiring any form of stabilization}. Our results show that our implementation of GOOMs, combined with efficient parallel scanning, offers a scalable and numerically robust alternative to conventional floating-point numbers for high-dynamic-range applications.},
  archive      = {J_TMLR},
  author       = {Franz A. Heinsen and Leo Kozachkov},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Generalized orders of magnitude for scalable, parallel, high-dynamic-range computation},
  url          = {https://openreview.net/forum?id=SUuzb0SOGu},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLMs can learn self-restraint through iterative self-reflection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=SvKPfchVKX'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to be deployed safely, Large Language Models (LLMs) must be capable of dynamically adapting their behavior based on their level of knowledge and uncertainty associated with specific topics. This adaptive behavior, which we refer to as self-restraint, is non-trivial to teach since it depends on the internal knowledge of an LLM. By default, LLMs are trained to maximize the next token likelihood, which does not teach the model to modulate its answer based on its level of uncertainty. In order to learn self-restraint, we devise a utility function that can encourage the model to produce responses only when its level of confidence is above a user-specified target accuracy $\rho^*$. This utility function can be used to score generation of different length and abstention. To optimize this function, we introduce ReSearch, a process of ``self-reflection'' consisting of iterative self-prompting and self-evaluation. We use the ReSearch algorithm to generate synthetic data on which we finetune our models. ReSearch elegantly incorporates the ability to abstain by augmenting the samples generated by the model during the search procedure with an answer expressing abstention. Compared to their original versions, our resulting models generate fewer hallucinations overall at no additional inference cost, for both known and unknown topics, as the model learns to selectively restrain itself. In addition, we show that our iterative search is more efficient as a function of tokens than naive search. Finally, we show that by modifying the target accuracy $\rho^*$, our trained models exhibit different behaviors.},
  archive      = {J_TMLR},
  author       = {Alexandre Piché and Aristides Milios and Dzmitry Bahdanau and Christopher Pal},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {LLMs can learn self-restraint through iterative self-reflection},
  url          = {https://openreview.net/forum?id=SvKPfchVKX},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segmenting text and learning their rewards for improved RLHF in language model. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=YhLlqD0UNi'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning from human feedback (RLHF) has been widely adopted to align language models (LMs) with human preference. Previous RLHF works typically take a bandit formulation, which, though intuitive, ignores the sequential nature of LM generation and can suffer from the sparse reward issue. While recent works propose dense token-level RLHF, treating each token as an action may be oversubtle to proper reward assignment. In this paper, we seek to get the best of both by training and utilizing a segment-level reward model, which assigns a reward to each semantically complete text segment that spans over a short sequence of tokens. For reward learning, our method allows dynamic text segmentation and compatibility with standard sequence-preference datasets. For effective RL-based LM training against segment reward, we generalize the classical scalar bandit reward normalizers into location-aware normalizer functions and interpolate the segment reward for further densification. Our method performs competitively on three popular RLHF benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation studies are conducted to further demonstrate our method.},
  archive      = {J_TMLR},
  author       = {Yueqin Yin and Shentao Yang and Yujia Xie and Ziyi Yang and Yuting Sun and Hany Hassan Awadalla and Weizhu Chen and Mingyuan Zhou},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Segmenting text and learning their rewards for improved RLHF in language model},
  url          = {https://openreview.net/forum?id=YhLlqD0UNi},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VColRL: Learn to solve the vertex coloring problem using reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=a9AQRieTne'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vertex Coloring Problem (VCP) is a fundamental NP-hard problem with applications in wireless networks, compiler design, scheduling, etc. We present VColRL, a deep reinforcement learning (DRL) framework that learns to color graphs quickly by leveraging a reduction-based approach that progressively reduces the graph at each step. The core novelty of VColRL is a new Markov Decision Process (MDP) formulation tailored for VCP that assigns colors to multiple vertices at each step, incorporates a rollback mechanism to revert all conflicting vertices to the undecided state, and employs a reward function designed to minimize the highest-indexed color used. Experiments on synthetic and benchmark graphs show that VColRL improves color usage over optimization solvers and prior learning-based methods, remains competitive with search-based heuristics and metaheuristics, and achieves fast runtime, while generalizing well to diverse graph families despite being trained only on synthetic graphs from a single family.},
  archive      = {J_TMLR},
  author       = {Abhinav Anand and Subrahmanya Swamy Peruru and Amitangshu Pal},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {VColRL: Learn to solve the vertex coloring problem using reinforcement learning},
  url          = {https://openreview.net/forum?id=a9AQRieTne},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curvature diversity-driven deformation and domain alignment for point cloud. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ePXWnH7rGk'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation is crucial for point cloud learning due to geometric variations across different generation methods and sensors. To tackle this challenge, we propose Curvature Diversity-Driven Nuclear-Norm Wasserstein Domain Alignment (CDND). We first introduce a Curvature Diversity-driven Deformation Reconstruction (CurvRec) task, enabling the model to extract salient features from semantically rich regions of a given point cloud. We then propose a theoretical framework for Deformation-based Nuclear-norm Wasserstein Discrepancy (D-NWD), extending the Nuclear-norm Wasserstein Discrepancy to original and deformed samples. Our theoretical analysis demonstrates that D-NWD is effective for any deformation method. Empirical experiment results show that our CDND achieves state-of-the-art performance by a noticeable margin over existing approaches.},
  archive      = {J_TMLR},
  author       = {Mengxi Wu and Hao Huang and Yi Fang and Mohammad Rostami},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Curvature diversity-driven deformation and domain alignment for point cloud},
  url          = {https://openreview.net/forum?id=ePXWnH7rGk},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simple and nearly-optimal sampling for rank-1 tensor completion via gauss-jordan. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ggAphfUt1J'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the sample and computational complexity of the rank-1 tensor completion problem in $\otimes_{i=1}^{N} \mathbb{R}^{d}$, given a uniformly sampled subset of entries. We present a characterization of the problem which reduces to solving a pair of random linear systems. For example, when $N$ is a constant, we prove it requires no more than $m = O(d^2 \log d)$ samples and runtime $O(md^2)$. Moreover, we show that a broad class of algorithms require $\Omega(d\log d)$ samples, even under higher rank scenarios. In contrast, existing upper bounds on the sample complexity are at least as large as $d^{1.5} \mu^{\Omega(1)} \log^{\Omega(1)} d$, where $\mu$ can be $\Theta(d)$ in the worst case. Prior work obtained these looser guarantees in higher rank versions of our problem, and tend to involve more complicated algorithms.},
  archive      = {J_TMLR},
  author       = {Alejandro Gomez-Leos and Oscar Lopez},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Simple and nearly-optimal sampling for rank-1 tensor completion via gauss-jordan},
  url          = {https://openreview.net/forum?id=ggAphfUt1J},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain graph anomaly detection via test-time training with homophily-guided self-supervision. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=sB3LqdOlNb'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Anomaly Detection (GAD) has demonstrated great effectiveness in identifying unusual patterns within graph-structured data. However, while labeled anomalies are often scarce in emerging applications, existing supervised GAD approaches are either ineffective or not applicable when moved across graph domains due to distribution shifts and heterogeneous feature spaces. To address these challenges, we present GADT3, a novel test-time training framework for cross-domain GAD. GADT3 combines supervised and self-supervised learning during training while adapting to a new domain during test time using only self-supervised learning by leveraging a homophily-based affinity score that captures domain-invariant properties of anomalies. Our framework introduces four key innovations to cross-domain GAD: an effective self-supervision scheme, an attention-based mechanism that dynamically learns edge importance weights during message passing, domain-specific encoders for handling heterogeneous features, and class-aware regularization to address imbalance. Experiments across multiple cross-domain settings demonstrate that GADT3 significantly outperforms existing approaches, achieving average improvements of over 8.2\% in AUROC and AUPRC compared to the best competing model.},
  archive      = {J_TMLR},
  author       = {Delaram Pirhayatifard and Arlei Silva},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Cross-domain graph anomaly detection via test-time training with homophily-guided self-supervision},
  url          = {https://openreview.net/forum?id=sB3LqdOlNb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The accuracy cost of weakness: A theoretical analysis of fixed-segment weak labeling for events in time. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=tTw8wXBQ18'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate labels are critical for deriving robust machine learning models. Labels are used to train supervised learning models and to evaluate most machine learning paradigms. In this paper, we model the accuracy and cost of a common weak labeling process where annotators assign presence or absence labels to fixed-length data segments for a given event class. The annotator labels a segment as "present" if it sufficiently covers an event from that class, e.g., a birdsong sound event in audio data. We analyze how the segment length affects the label accuracy and the required number of annotations, and compare this fixed-length labeling approach with an oracle method that uses the true event activations to construct the segments. Furthermore, we quantify the gap between these methods and verify that in most realistic scenarios the oracle method is better than the fixed-length labeling method in both accuracy and cost. Our findings provide a theoretical justification for adaptive weak labeling strategies that mimic the oracle process, and a foundation for optimizing weak labeling processes in sequence labeling tasks.},
  archive      = {J_TMLR},
  author       = {John Martinsson and Olof Mogren and Tuomas Virtanen and Maria Sandsten},
  journal      = {Transactions on Machine Learning Research},
  month        = {9},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {The accuracy cost of weakness: A theoretical analysis of fixed-segment weak labeling for events in time},
  url          = {https://openreview.net/forum?id=tTw8wXBQ18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
