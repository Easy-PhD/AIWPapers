<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="or">OR - 32</h2>
<ul>
<li><details>
<summary>
(2025). Acknowledgment to referees (2024). <em>OR</em>, <em>73</em>(4), iii. (<a href='https://doi.org/10.1287/opre.2025.apprec.v73.n4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OR},
  doi          = {10.1287/opre.2025.apprec.v73.n4},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {iii},
  shortjournal = {Oper. Res.},
  title        = {Acknowledgment to referees (2024)},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Off-line estimation of controlled markov chains: Minimaxity and sample complexity. <em>OR</em>, <em>73</em>(4), 2281-2295. (<a href='https://doi.org/10.1287/opre.2023.0046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study a natural nonparametric estimator of the transition probability matrices of a finite controlled Markov chain. We consider an off-line setting with a fixed data set of size m , collected using a so-called logging policy. We develop sample complexity bounds for the estimator and establish conditions for minimaxity. Our statistical bounds depend on the logging policy through its mixing properties. We show that achieving a particular statistical risk bound involves a subtle and interesting trade-off between the strength of the mixing properties and the number of samples. We demonstrate the validity of our results under various examples, such as ergodic Markov chains; weakly ergodic inhomogeneous Markov chains; and controlled Markov chains with nonstationary Markov, episodic, and greedy controls. Lastly, we use these sample complexity bounds to establish concomitant ones for off-line evaluation of stationary Markov control policies. Funding: I. Banerjee was supported in part by the Ross-Lynn fellowship and McLean scholarship at Purdue University. H. Honnappa was partly supported by the National Science Foundation [Grants CAREER/2143752, DMS/1812197 and DMS/2153915]. V. Rao was supported by the National Science Foundation [Grants RI/1816499 and DMS/1812197]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0046 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0046},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2281-2295},
  shortjournal = {Oper. Res.},
  title        = {Off-line estimation of controlled markov chains: Minimaxity and sample complexity},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convex chance-constrained programs with wasserstein ambiguity. <em>OR</em>, <em>73</em>(4), 2264-2280. (<a href='https://doi.org/10.1287/opre.2021.0709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chance constraints yield nonconvex feasible regions in general. In particular, when the uncertain parameters are modeled by a Wasserstein ball, existing studies showed that the distributionally robust (pessimistic) chance constraint admits a mixed-integer conic representation. This paper identifies sufficient conditions that lead to convex feasible regions of chance constraints with Wasserstein ambiguity. First, when uncertainty arises from the right-hand side of a pessimistic joint chance constraint, we show that the ensuing feasible region is convex if the Wasserstein ball is centered around a log-concave distribution (or, more generally, an α -concave distribution with α ≥ − 1 ). In addition, we propose a block coordinate ascent algorithm and prove its convergence to global optimum, as well as the rate of convergence. Second, when uncertainty arises from the left-hand side of a pessimistic two-sided chance constraint, we show the convexity if the Wasserstein ball is centered around an elliptical and star unimodal distribution. In addition, we propose a family of second-order conic inner approximations, and we bound their approximation error and prove their asymptotic exactness. Furthermore, we extend the convexity results to optimistic chance constraints. Funding: This work was supported by the National Science Foundation [Grants ECCS-1845980, OIA-2119691, and OIA-1946391] and the Air Force Office of Scientific Research [Grant FA9550-23-1-0323]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0709 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0709},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2264-2280},
  shortjournal = {Oper. Res.},
  title        = {Convex chance-constrained programs with wasserstein ambiguity},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic control of service systems with returns: Application to design of postdischarge hospital readmission prevention programs. <em>OR</em>, <em>73</em>(4), 2242-2263. (<a href='https://doi.org/10.1287/opre.2022.0066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a control problem for queueing systems in which customers may return for additional episodes of service after their initial service completion. At each service completion epoch, the decision maker can choose to reduce the probability of return for the departing customer but at a cost that is convex increasing in the amount of reduction in the return probability. Other costs are incurred as customers wait in the queue and every time they return for service. Our primary motivation comes from postdischarge quality improvement interventions (e.g., follow-up phone calls, outpatient appointments) frequently used in a variety of healthcare settings to reduce unplanned hospital readmissions. Our objective is to understand how the cost of interventions should be balanced with the reductions in congestion and service costs. To this end, we consider a fluid approximation of the queueing system and characterize the structure of optimal long-run average and bias-optimal transient control policies for the fluid model. Our structural results motivate the design of intuitive surge protocols whereby different intensities of interventions (corresponding to different levels of reduction in the return probability) are provided based on the congestion in the system. Through extensive simulation experiments, we study the performance of the fluid policy for the stochastic system and identify parameter regimes in which it leads to significant cost savings compared with a fixed long-run average optimal policy that ignores holding costs and a simple policy that uses the highest level of intervention whenever the queue is nonempty. In particular, we find that, in a parameter regime relevant to our motivating application, dynamically adjusting the intensity of interventions could result in up to 25.4% reduction in long-run average cost and 33.7% in finite-horizon costs compared with the simple aggressive policy. Funding: V. Sarhangian was supported by the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2018-04518]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0066 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0066},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2242-2263},
  shortjournal = {Oper. Res.},
  title        = {Dynamic control of service systems with returns: Application to design of postdischarge hospital readmission prevention programs},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strong optimal classification trees. <em>OR</em>, <em>73</em>(4), 2223-2241. (<a href='https://doi.org/10.1287/opre.2021.0034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are among the most popular machine learning models and are used routinely in applications ranging from revenue management and medicine to bioinformatics. In this paper, we consider the problem of learning optimal binary classification trees with univariate splits. Literature on the topic has burgeoned in recent years, motivated both by the empirical suboptimality of heuristic approaches and the tremendous improvements in mixed-integer optimization (MIO) technology. Yet, existing MIO-based approaches from the literature do not leverage the power of MIO to its full extent: they rely on weak formulations, resulting in slow convergence and large optimality gaps. To fill this gap in the literature, we propose an intuitive flow-based MIO formulation for learning optimal binary classification trees. Our formulation can accommodate side constraints to enable the design of interpretable and fair decision trees. Moreover, we show that our formulation has a stronger linear optimization relaxation than existing methods in the case of binary data. We exploit the decomposable structure of our formulation and max-flow/min-cut duality to derive a Benders’ decomposition method to speed-up computation. We propose a tailored procedure for solving each decomposed subproblem that provably generates facets of the feasible set of the MIO as constraints to add to the main problem. We conduct extensive computational experiments on standard benchmark data sets on which we show that our proposed approaches are 29 times faster than state-of-the-art MIO-based techniques and improve out-of-sample performance by up to 8%. Funding: P. Vayanos and S. Aghaei gratefully acknowledge support from the Hilton C. Foundation, the Homeless Policy Research Institute, the Home for Good foundation under the “C.E.S. Triage Tool Research & Refinement” grant. P. Vayanos is funded in part by the National Science Foundation, under CAREER [Grant 2046230]. She is grateful for this support. A. Gómez is funded in part by the National Science Foundation under [Grants 1930582 and 2006762]. Supplemental Material: The online appendix and code files are available at https://doi.org/10.1287/opre.2021.0034 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0034},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2223-2241},
  shortjournal = {Oper. Res.},
  title        = {Strong optimal classification trees},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonadaptive stochastic score classification and explainable half-space evaluation. <em>OR</em>, <em>73</em>(4), 2204-2222. (<a href='https://doi.org/10.1287/opre.2023.0431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential testing problems involve a complex system with several components, each of which is “working” with some independent probability. The outcome of each component can be determined by performing a test, which incurs some cost. The overall system status is given by a function f of the outcomes of its components. The goal is to evaluate this function f by performing tests at the minimum expected cost. Although there has been extensive prior work on this topic, provable approximation bounds are mainly limited to simple functions, like “ k -out-of- n ” and half-spaces. We consider significantly more general “score classification” functions, and we provide the first constant-factor approximation algorithm (improving over a previous logarithmic approximation ratio). Moreover, our policy is nonadaptive; it just involves performing tests in an a priori fixed order. We also consider the related half-space evaluation problem, where we want to evaluate some function on d half-spaces (e.g., the intersection of half-spaces). We show that our approach provides an O ( d 2 log d ) -approximation algorithm for this problem. Our algorithms also extend to the setting of “batched” tests, where multiple tests can be performed simultaneously while incurring an extra setup cost. Finally, we perform computational experiments that demonstrate the practical performance of our algorithm for score classification. We observe that, for most instances, the cost of our algorithm is within a factor of 1.5 of an information-theoretic lower bound on the optimal value. Funding: This work was supported by the Division of Computing and Communication Foundations [Grants CCF-2006778 and CCF-2006953] and the Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-1940766].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0431},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2204-2222},
  shortjournal = {Oper. Res.},
  title        = {Nonadaptive stochastic score classification and explainable half-space evaluation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logarithmic regret in multisecretary and online linear programs with continuous valuations. <em>OR</em>, <em>73</em>(4), 2188-2203. (<a href='https://doi.org/10.1287/opre.2022.0036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I use empirical processes to study how the shadow prices of a linear program that allocates an endowment of n β ∈ R m resources to n customers behave as n → ∞ . I show the shadow prices (i) adhere to a concentration of measure, (ii) converge to a multivariate normal under central-limit-theorem scaling, and (iii) have a variance that decreases like Θ ( 1 / n ) . I use these results to prove that the expected regret in an online linear program is Θ ( log n ) , both when the customer variable distribution is known upfront and must be learned on the fly. This result tightens the sharpest known upper bound from O ( log n log log n ) to O ( log n ) , and it extends the Ω ( log n ) lower bound known for single-dimensional problems to the multidimensional setting. I illustrate my new techniques with a simple analysis of a multisecretary problem. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0036 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0036},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2188-2203},
  shortjournal = {Oper. Res.},
  title        = {Logarithmic regret in multisecretary and online linear programs with continuous valuations},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardness of pricing routes for two-stage stochastic vehicle routing problems with scenarios. <em>OR</em>, <em>73</em>(4), 2177-2187. (<a href='https://doi.org/10.1287/opre.2023.0569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem with stochastic demands (VRPSD) generalizes the classic vehicle routing problem by considering customer demands as random variables. Similar to other vehicle routing variants, state-of-the-art algorithms for the VRPSD are often based on set-partitioning formulations, which require efficient routines for the associated pricing problems. However, all of these set partitioning–based approaches have strong assumptions on the correlation between the demands of random variables (e.g., no correlation), a simplification that diverges from real-world settings where correlations frequently exist. In contrast, there is a significant effort in the stochastic programming community to solve problems where the uncertainty is modeled with a finite set of scenarios. This approach can approximate more diverse distributions via sampling and is particularly appealing in data-driven contexts where historical data are readily available. To fill this gap, we focus on the VRPSD with demands given by scenarios. We show that for any route relaxation (where repeated visits are allowed in a route) and any approximation of the recourse cost that satisfies some mild assumptions, the VRPSD pricing problem is still strongly N P -hard. This provides a very strong argument for the difficulty of developing efficient column generation–based algorithms for the VRPSD with demands following an empirical probability distribution of scenarios. Funding: This work was supported by Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2020-04030].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0569},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2177-2187},
  shortjournal = {Oper. Res.},
  title        = {Hardness of pricing routes for two-stage stochastic vehicle routing problems with scenarios},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving blockchain consistency bound by assigning weights to random blocks. <em>OR</em>, <em>73</em>(4), 2156-2176. (<a href='https://doi.org/10.1287/opre.2022.0463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchains based on the celebrated Nakamoto consensus protocol have shown promise in several applications, including cryptocurrencies. However, these blockchains have inherent scalability limits caused by the protocol’s consensus properties. In particular, the consistency property demonstrates a tight trade-off between block production speed and the system’s security in terms of resisting adversarial attacks. As such, this paper proposes a novel method called Ironclad, which improves the blockchain consistency bound by assigning a different weight to randomly selected blocks. We apply our method to the original Nakamoto protocol and rigorously prove that such a combination can significantly improve the consistency bound by analyzing the fundamental consensus properties. This kind of improvement enables a much faster block production rate than the original Nakamoto protocol but with the same security guarantee. Funding: This work was supported in part by the WeBank-Hong Kong University of Science and Technology Joint Lab [Project WEB19EG01-M]. The research of J. Zhang was supported in part by the Hong Kong Research Grants Council [Grants 16208120 and 16214121]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0463 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0463},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2156-2176},
  shortjournal = {Oper. Res.},
  title        = {Improving blockchain consistency bound by assigning weights to random blocks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A short and general duality proof for wasserstein distributionally robust optimization. <em>OR</em>, <em>73</em>(4), 2146-2155. (<a href='https://doi.org/10.1287/opre.2023.0135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general duality result for Wasserstein distributionally robust optimization that holds for any Kantorovich transport cost, measurable loss function, and nominal probability distribution. Assuming an interchangeability principle inherent in existing duality results, our proof only uses one-dimensional convex analysis. Furthermore, we demonstrate that the interchangeability principle holds if and only if certain measurable projection and weak measurable selection conditions are satisfied. To illustrate the broader applicability of our approach, we provide a rigorous treatment of duality results in distributionally robust Markov decision processes and distributionally robust multistage stochastic programming. Additionally, we extend our analysis to other problems such as infinity-Wasserstein distributionally robust optimization, risk-averse optimization, and globalized distributionally robust counterpart. Funding: L. Zhang acknowledges the support of Xunyu Zhou and the Nie Center for Intelligent Asset Management at Columbia University. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0135 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0135},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2146-2155},
  shortjournal = {Oper. Res.},
  title        = {A short and general duality proof for wasserstein distributionally robust optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-adaptive local decision rules. <em>OR</em>, <em>73</em>(4), 2125-2145. (<a href='https://doi.org/10.1287/opre.2023.0564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For parameterized mixed-binary optimization problems, we construct local decision rules that prescribe near-optimal courses of action across a set of parameter values. The decision rules stem from solving risk-adaptive training problems over classes of continuous, possibly nonlinear mappings. In asymptotic and nonasymptotic analysis, we establish that the decision rules prescribe near-optimal decisions locally for the actual problems without relying on linearity, convexity, or smoothness. The development also accounts for practically important aspects such as inexact function evaluations, solution tolerances in training problems, regularization, and reformulations to solver-friendly models. The decision rules also furnish a means to carry out sensitivity and stability analysis for broad classes of parameterized optimization problems. We develop a decomposition algorithm for solving the resulting training problems and demonstrate its ability to generate quality decision rules on a nonlinear binary optimization model from search theory. Funding: J. O. Royset is supported in part by the Office of Naval Research (ONR) [Grant N000142412277]. M. A. Lejeune was supported by the National Science Foundation [Grants DMS-2318519 and ECCS-2114100], and the Office of Naval Research [Grant N00014-22-1-2649]. Supplemental Material: The computer code and data that support the findings of this study and the online appendix are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0564 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0564},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2125-2145},
  shortjournal = {Oper. Res.},
  title        = {Risk-adaptive local decision rules},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic allocation of reusable resources: Logarithmic regret in overloaded networks. <em>OR</em>, <em>73</em>(4), 2097-2124. (<a href='https://doi.org/10.1287/opre.2022.0429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of dynamically allocating reusable resources to customers of n types. There are d pools of resources and a finite number of units from each resource. If a customer request is accepted, the decision maker collects a type-dependent reward, and the customer occupies, for a random service time, one unit from each resource in a set of these. Upon service completion, these resource units become available for future allocation. This is a loss network: requests that are not accepted leave immediately. The decision maker’s objective is to maximize the long-run average reward subject to the resource capacity constraint. A natural linear programming (LP) relaxation of the problem serves as an upper bound on the performance of any policy. We identify a condition that generalizes the notion of overload in single-resource networks (i.e., when d = 1 ). The LP guides our construction of a threshold policy. In this policy, the number of thresholds equals the number of resource types (hence, does not depend on the number of customer types). These thresholds are applied to a “corrected” headcount process. In the case of a single resource, the corrected head count is the number of resource units that are occupied. We prove that, in overloaded networks, the additive loss (or regret) of this policy benchmarked against the LP upper bound is logarithmic in the total arrival volume in the high-customer-volume, many-resource-units, asymptotic regime. No policy can achieve sublogarithmic regret. Simulations showcase the performance of the proposed policy. Funding: X. Xie and I. Gurvich were supported by an Amazon Research Award and DRAGONS – Dynamic Resource Allocation Gains for Operational Networked Sharing, Department of Defense (Army) [Grant STTR A18B-T007]. S. Küçükyavuz was supported by an Office of Naval Research [Grant N00014-22-1-2602]. Supplemental Material: Data and code files are available at https://doi.org/10.1287/opre.2022.0429 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0429},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2097-2124},
  shortjournal = {Oper. Res.},
  title        = {Dynamic allocation of reusable resources: Logarithmic regret in overloaded networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal regularized online allocation by adaptive re-solving. <em>OR</em>, <em>73</em>(4), 2079-2096. (<a href='https://doi.org/10.1287/opre.2022.0486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a dual-based algorithm framework for solving the regularized online resource allocation problems, which have potentially nonconcave cumulative rewards, hard resource constraints, and a nonseparable regularizer. Under a strategy of adaptively updating the resource constraints, the proposed framework only requests approximate solutions to the empirical dual problems up to a certain accuracy and yet delivers an optimal logarithmic regret under a locally second-order growth condition. Surprisingly, a delicate analysis of the dual objective function enables us to eliminate the notorious log-log factor in regret bound. The flexible framework renders renowned and computationally fast algorithms immediately applicable, for example, dual stochastic gradient descent. Additionally, an infrequent re-solving scheme is proposed, which significantly reduces computational demands without compromising the optimal regret performance. A worst-case square-root regret lower bound is established if the resource constraints are not adaptively updated during dual optimization, which underscores the critical role of adaptive dual variable update. Comprehensive numerical experiments demonstrate the merits of the proposed algorithm framework. Funding: This work was supported by the National Foreign Expert Project [G2022030026], the Research Grants Council, and the University Grants Committee [Grants GRF 16211220, GRF 16300121, and GRF 16301622]. Supplemental Material: The online appendices and code files are available at https://doi.org/10.1287/opre.2022.0486 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0486},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2079-2096},
  shortjournal = {Oper. Res.},
  title        = {Optimal regularized online allocation by adaptive re-solving},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically optimal clearing control of backlogs in multiclass processing systems. <em>OR</em>, <em>73</em>(4), 2061-2078. (<a href='https://doi.org/10.1287/opre.2022.0570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a dynamic scheduling problem for a processing system facing the problem of optimally clearing a large backlog of unsatisfied demand from several classes of customers (or jobs). We formulate the problem as a multiclass queueing model with a large initial queue and arrival rates that approximately equal the system’s processing capacity. The goal is to find a scheduling policy that minimizes a holding-and-abandonment cost during the transient period in which the system is considered congested. Because computing an exact solution to the optimal-control problem is infeasible, we develop a unified asymptotic approximation that covers, in particular, the conventional and the many-server heavy-traffic regimes. In addition to the generality and flexibility of our unified asymptotic framework, we also prove a strong form of asymptotic optimality, under which the costs converge in expectation and in probability. In particular, for the special two-class case, we prove that a static priority policy, which follows a discounted c μ / θ rule, is asymptotically optimal. When there are more than two classes of customers, we show that any admissible control that follows the best-effort rule, which gives the lowest priority to one of the classes according to the discounted c μ / θ ordering, becomes asymptotically optimal after some relatively short time period. Finally, using heuristic arguments and insights from our analyses, we propose scheduling policies that build on the best-effort rule. An extensive numerical study shows that those proposed policies are effective and provides guidance as to when to use either policy in practice. Funding: O. Perry and L. Yu were partially supported by NSF [Grants CMMI 1763100 and CMMI 2006350]. L. Yu is currently supported by NSFC [Grants 72201153, 72242106, and 72394361]. Supplemental Material: The online appendix and code are available at https://doi.org/10.1287/opre.2022.0570 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0570},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2061-2078},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically optimal clearing control of backlogs in multiclass processing systems},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal dynamic mechanism under customer search. <em>OR</em>, <em>73</em>(4), 2045-2060. (<a href='https://doi.org/10.1287/opre.2022.0136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the seller’s revenue-maximizing mechanism in the face of a customer who searches for outside alternatives over a finite horizon. The customer’s utility from searches is modeled as a general function—referred to as the recall function—of the past search outcomes. Without observing the customer’s valuation of the product or any realization of search outcomes, the seller can propose and commit to a contract with the customer before the search process begins. Under a general recall function, we show that the optimal strategy for the seller is to offer a menu of American options consisting of deposits and strike prices. In the case in which the customer can only recall a few recent outside alternatives, we further establish that, under the optimal mechanism, customers with low valuation search for outside alternatives without engaging with the seller, whereas high-valuation customers exercise the option immediately, effectively turning the option into an exploding offer. Customers with intermediate valuation only exercise the option, if ever, at the end of the search horizon. Whereas a longer search horizon or smaller search cost both increase the customer’s utility from searches, they have different impacts on the seller’s revenue. More search opportunities lead to an exponential decrease in the seller’s revenue, and in the limit, the optimal mechanism converges to a posted price mechanism. In contrast, as the search cost increases, the seller’s revenue may initially decrease and then increase. In the extreme case in which the search cost exceeds the average value of outside alternatives, the customer’s sequential search problem reduces to strategically timing purchases of the seller’s product. Our optimal mechanism, in this case, reduces to making a single exploding offer with a monopoly price. Funding: This work is funded in part by the Ministry of Education, Singapore, under its 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0136 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0136},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2045-2060},
  shortjournal = {Oper. Res.},
  title        = {Optimal dynamic mechanism under customer search},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boundary effects in the diffusion of new products on cartesian networks. <em>OR</em>, <em>73</em>(4), 2026-2044. (<a href='https://doi.org/10.1287/opre.2022.0004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the effect of boundaries in the discrete Bass model on D -dimensional Cartesian networks. In two dimensions, this model describes the diffusion of new products that spread primarily by spatial peer effects, such as residential photovoltaic solar systems. We show analytically that nodes (residential units) that are located near the boundary are less likely to adopt than centrally located ones. This boundary effect is local and decays exponentially with the distance from the boundary. At the aggregate level, boundary effects reduce the overall adoption level. The magnitude of this reduction scales as 1 M 1 / D , where M is the number of nodes. Our analysis is supported by empirical evidence on the effect of boundaries on the adoption of solar. Funding: This material is based upon work supported by the Department of Energy (DOE) [Grant DE-EE0007657]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0004 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0004},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2026-2044},
  shortjournal = {Oper. Res.},
  title        = {Boundary effects in the diffusion of new products on cartesian networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic relocations in car-sharing networks. <em>OR</em>, <em>73</em>(4), 2010-2025. (<a href='https://doi.org/10.1287/opre.2021.0062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel dynamic car relocation policy for a car-sharing network with centralized control and uncertain, unbalanced demand. The policy is derived from a reformulation of the linear programming fluid model approximation of the dynamic problem. We project the full-dimensional fluid approximation onto the lower-dimensional space of relocation decisions only. This projection results in a characterization of the problem as n + 1 linear programs, where n is the number of nodes in the network. The reformulation uncovers structural properties that are interpretable using absorbing Markov chain concepts and allows us to write the gradient with respect to the relocation decisions in closed form. Our policy exploits these gradients to make dynamic car relocation decisions. We provide extensive numerical results on hundreds of random networks where our dynamic car relocation policy consistently outperforms the standard static policy. Our policy reduces the optimality gap in steady state by more than 23% on average. Also, in a short-term, time-varying setting, the lookahead version of our dynamic policy outperforms the static lookahead policy slightly more than in the time-homogeneous tests. Funding: This work was supported by the Natural Sciences and Engineering Research Council of Canada [Grants RGPGP-2015-00050 and RGPIN-2018-04561]. Supplemental Material: The computer code, data, and e-companion that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0062 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0062},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2010-2025},
  shortjournal = {Oper. Res.},
  title        = {Dynamic relocations in car-sharing networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Establishing convergence of infinite-server queues with batch arrivals to shot-noise processes. <em>OR</em>, <em>73</em>(4), 2002-2009. (<a href='https://doi.org/10.1287/opre.2023.0353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Across domains as diverse as communication channels, computing systems, and public health management, a myriad of real-world queueing systems receive batch arrivals of jobs or customers. In this work, we show that under a natural scaling regime, both the queue-length process and the workload process associated with a properly scaled sequence of infinite-server queueing systems with batch arrivals converge almost surely, uniformly on compact sets, to shot-noise processes. Given the applicability of these models, our relatively direct and accessible methodology may also be of independent interest, where we invoke the Glivenko–Cantelli theorem when the Strong Law of Large Numbers fails to hold for the queue-length batch scaling yet then, exploit the continuity of stationary excess distributions and the classic strong law when the Glivenko–Cantelli theorem fails to hold in the workload batch scaling. These results strengthen a convergence result recently established in the work of de Graaf et al. [de Graaf WF, Scheinhardt WR, Boucherie RJ (2017) Shot-noise fluid queues and infinite-server systems with batch arrivals. Performance Evaluation 116:143–155] in multiple ways, and furthermore, they provide new insight into how the queue-length and workload limits differ from one another.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0353},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2002-2009},
  shortjournal = {Oper. Res.},
  title        = {Establishing convergence of infinite-server queues with batch arrivals to shot-noise processes},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partition and prosper: Design and pricing of single bundle. <em>OR</em>, <em>73</em>(4), 1983-2001. (<a href='https://doi.org/10.1287/opre.2022.0465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product bundling is a widely used selling strategy among multiproduct firms, yet designing and pricing bundles optimally remain a complex challenge. This paper addresses this fundamental issue by exploring the selection and pricing of a single bundle from a range of products. For instance, in the single bundle with the rest (SBR) framework, the bundle is optimally chosen and priced, whereas the remaining products are offered individually, collectively maximizing profit. We show that the SBR optimization problem under multivariate normal valuations is polynomial-time solvable, provided that the associated covariance matrix can be decomposed into a positive diagonal matrix minus a positive semidefinite matrix of (small) fixed rank. Interestingly, we also show that the subproblem of SBR optimization, where individual product prices are predetermined, is N P -hard, even if customer valuations are independent. Building on these results, we use a Bayesian optimization (BO) algorithm combined with a conic integer programming reformulation to solve the general SBR optimization problem under correlated valuations. We further show that SBR is a constant approximation to more complex mechanisms in terms of profit performance. Extensive numerical results demonstrate that our BO algorithm has superior performance over baseline heuristics, and SBR achieves significantly higher profit than separate selling and grand bundling. Interestingly, simulation studies reveal that allowing customers the additional option to purchase products either as part of a bundle or individually enhances social welfare (i.e., increases both profit and customer surplus) compared with SBR, separate selling, and grand bundling. These findings highlight the potential benefits of bundle pricing strategies in achieving improved outcomes for both firms and customers. Funding : H. Sun is supported by the National Natural Science Foundation of China [Grant 72301168] and the Shanghai Pujiang Programme [Grant 23PJC062]. X. Li is supported by the Singapore Ministry of Education [Tier 1 Grant 23-0619-P0001 and Tier 1 Grant 24-0500-A0001] and the National Natural Science Foundation of China [Grant 72171156]. C.-P. Teo is supported by the National Research Foundation Singapore [Grant I2001E0059] and the Singapore Ministry of Education [Grant MOE-2019-T3-1-010]. C.-P. Teo is also supported by the Natural Science Foundation of Chongqing, China [Grant CSTB2022NSCQ-MSX1667]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2022.0465 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0465},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1983-2001},
  shortjournal = {Oper. Res.},
  title        = {Partition and prosper: Design and pricing of single bundle},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian and randomized clock auctions. <em>OR</em>, <em>73</em>(4), 1965-1982. (<a href='https://doi.org/10.1287/opre.2022.0421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a single-parameter mechanism design problem, a provider is looking to sell some service to a group of potential buyers. Each buyer i has a private value v i for receiving this service, but a feasibility constraint restricts which buyers can be simultaneously served. Recent work in economics introduced (deferred-acceptance) clock auctions as a superior class of auctions for this problem due to their transparency, simplicity, and strong incentive guarantees. Subsequent work focused on evaluating these auctions in terms of worst-case social welfare approximation, leading to strong impossibility results: Without prior information regarding buyers’ values, deterministic clock auctions cannot achieve bounded approximations, even for feasibility constraints comprising two maximal feasible sets. We demonstrate how to circumvent these negative results by leveraging prior information or randomization . In particular, we provide clock auctions that give an O ( log log k ) -approximation for arbitrary downward-closed feasibility constraints with k maximal feasible sets for three different information regimes. The more prior information we have access to, the simpler the proposed auctions. In addition, we propose a parametrization of the complexity of clock auctions, paving the way for exciting future research. Funding: This work was supported by the Simons Foundation [Grant 820931], the Science and Technology Innovation 2030 [Grant 2018AAA0100903], the National Natural Science Foundation of China [Grant 62150610500], the Central University Basic Research Fund of China, the National Science Foundation [Grants CCF-1755955 and CCF-2008280], the H2020 European Research Council [Grant 866132], and the Israel Science Foundation [Grant 317/17]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0421 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0421},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1965-1982},
  shortjournal = {Oper. Res.},
  title        = {Bayesian and randomized clock auctions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian mechanism design for blockchain transaction fee allocation. <em>OR</em>, <em>73</em>(4), 1944-1964. (<a href='https://doi.org/10.1287/opre.2024.0865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In blockchain systems, the design of transaction fee mechanisms (TFMs) is essential for stability and satisfaction for both miners and users. A recent work has proven the impossibility of collusion-proof mechanisms that achieve both nonzero miner revenue and Dominant Strategy Incentive Compatibility (DSIC) for users. However, a positive miner revenue is important in practice to motivate miners. To address this challenge, we consider a Bayesian game setting and relax the DSIC requirement for users to Bayesian Nash Incentive Compatibility (BNIC). In particular, we propose an auxiliary mechanism method that makes connections between BNIC and DSIC mechanisms. With the auxiliary mechanism method, we design a TFM based on the multinomial logit (MNL) choice model, and prove that the TFM has both BNIC and collusion-proof properties with an asymptotic constant-factor approximation of optimal miner revenue for i.i.d. bounded valuations. Our result breaks the zero-revenue barrier while preserving truthfulness and collusion-proof properties. Funding: X. Chen thanks the NSF [Grant IIS-1845444] for support. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2024.0865 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.0865},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1944-1964},
  shortjournal = {Oper. Res.},
  title        = {Bayesian mechanism design for blockchain transaction fee allocation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—On dynamic pricing with covariates. <em>OR</em>, <em>73</em>(4), 1932-1943. (<a href='https://doi.org/10.1287/opre.2021.0802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider dynamic pricing with covariates under a generalized linear demand model: A seller can dynamically adjust the price of a product over a horizon of T time periods, and at each time period t , the demand of the product is jointly determined by the price and an observable covariate vector x t ∈ R d through a generalized linear model with unknown coefficients. Most of the existing literature assumes the covariate vectors x t s are independently and identically distributed (i.i.d.); the few papers that relax this assumption either sacrifice model generality or yield suboptimal regret bounds. In this paper, we show that Upper Confidence Bound and Thompson sampling-based pricing algorithms can achieve an O ( d T log T ) regret upper bound without assuming any statistical structure on the covariates x t . Our upper bound on the regret matches the lower bound up to logarithmic factors. We thus show that (i) the i.i.d. assumption is not necessary for obtaining low regret, and (ii) the regret bound can be independent of the (inverse) minimum eigenvalue of the covariance matrix of the x t s, a quantity present in previous bounds. Moreover, we consider a constrained setting of the dynamic pricing problem where there is a limited and unreplenishable inventory, and we develop theoretical results that relate the best achievable algorithm performance to a variation measure with respect to the temporal distribution shift of the covariates. We also demonstrate the proposed algorithms’ performance with numerical experiments. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0802 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0802},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1932-1943},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On dynamic pricing with covariates},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Community-engaged school district design: A stream-based approach. <em>OR</em>, <em>73</em>(4), 1916-1931. (<a href='https://doi.org/10.1287/opre.2022.0621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehensive community engagement in public school district design is essential to create equitable and effective enrollment policies reflective of community needs and values. We revisit the school district design problem with a focus on codesigning with community partners. We introduce a new compact formulation that incorporates multiple decisions simultaneously by assigning students in each geographic unit to a set of schools (e.g., elementary, middle, and high schools, and schools with specialized programming) with a single composite variable, referred to as a “stream.” This formulation is computationally efficient and easily reconfigurable for evolving problem specifications that are endemic to community codesign. These features were essential in the district redesign process described in this paper, allowing the community to iteratively develop proposals to address inequities in access to education and improve the student assignment process. Funding: This work was supported by the National Science Foundation [Grant CMMI-1727744] and Northwestern University’s McCormick Catalyst Fund.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0621},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1916-1931},
  shortjournal = {Oper. Res.},
  title        = {Community-engaged school district design: A stream-based approach},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically optimal competitive ratio for online allocation of reusable resources. <em>OR</em>, <em>73</em>(4), 1897-1915. (<a href='https://doi.org/10.1287/opre.2021.0695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of online allocation (matching, budgeted allocations, and assortments) of reusable resources for which an adversarial sequence of resource requests is revealed over time and any allocated resource is used/rented for a stochastic duration drawn independently from a resource-dependent usage distribution. Previously, it was known that a greedy algorithm is 0.5-competitive against the clairvoyant benchmark that knows the entire sequence of requests in advance. We give a novel algorithm that is ( 1 − 1 / e ) -competitive for arbitrary usage distributions when the starting capacity of each resource is large and the usage distributions are known. This is the best achievable competitive ratio guarantee for the problem; that is, no online algorithm can have a better competitive ratio. We also give a distribution-oblivious online algorithm and show that it is ( 1 − 1 / e ) -competitive in special cases. At the heart of our algorithms is a new quantity that factors in the potential of reusability for each resource by (computationally) creating an asymmetry between identical units of the resource. We establish the performance guarantee for our algorithms by constructing a feasible solution to a novel system of inequalities that allows direct comparison with the clairvoyant benchmark instead of a linear programming relaxation of the benchmark. Our technique generalizes the primal-dual analysis framework for online resource allocation and may be of broader interest. Funding: This work was supported by Google (Google Research Scholar Program) and the Division of Civil, Mechanical and Manufacturing Innovation [Grants 1351838, 1636046, and 2340306]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0695 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0695},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1897-1915},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically optimal competitive ratio for online allocation of reusable resources},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic fair division with partial information. <em>OR</em>, <em>73</em>(4), 1876-1896. (<a href='https://doi.org/10.1287/opre.2023.0608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the fundamental problem of fairly and efficiently allocating T indivisible items among n agents with additive preferences. Items become available over a sequence of rounds, and every item must be allocated immediately and irrevocably before the next one arrives. Previous work shows that when the agents’ valuations for the items are drawn from known distributions, it is possible (under mild assumptions) to find allocations that are envy-free with high probability and Pareto efficient ex post. However, this requires that agents accurately report their values to the algorithm, which rarely happens in practice. We study a partial-information setting, where true item values are hidden from the algorithm and it is only possible to elicit ordinal information in the form of a ranking or pairwise comparison relative to prior items. When values are drawn from i.i.d. distributions, or correlated distributions consisting of a shared common value for each item with i.i.d. noise, we give an algorithm that is envy-free and ( 1 − ϵ ) -welfare-maximizing with high probability. We provide similar guarantees (envy-freeness and a constant approximation to welfare with high probability) even with minimally expressive queries that ask for a comparison with a single previous item. For independent but nonidentical agents, we obtain envy-freeness and a constant approximation to Pareto efficiency with high probability. Our results are asymptotically tight. A computational study shows that envy-freeness and efficiency can be achieved on practical time-horizons. Funding: D. Halpern is supported by the National Science Foundation Graduate Research Fellowship Program [Grant DGE1745303]. A. Psomas is supported in part by an NSF CAREER award (Division of Computing and Communication Foundations) [Grant CCF-2144208], a Google AI for Social Good award, and research awards from Google and Supra. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0608 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0608},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1876-1896},
  shortjournal = {Oper. Res.},
  title        = {Dynamic fair division with partial information},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On simple mechanisms for dependent items. <em>OR</em>, <em>73</em>(4), 1849-1875. (<a href='https://doi.org/10.1287/opre.2022.0552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of selling n heterogeneous items to a single buyer, whose values for different items are dependent. Under arbitrary dependence, others show that no simple mechanism can achieve a nonnegligible fraction of the optimal revenue even with only two items. We consider the setting where the buyer’s type is drawn from a correlated distribution that can be captured by a Markov random field (MRF), one of the most prominent frameworks for modeling high-dimensional distributions with structure. We show how the performance of simple mechanisms depends on some natural parameters of the MRF for several fundamental classes of the buyer’s valuations. Our results are based on the duality framework by of others and a new concentration inequality for XOR-of-OR-of-Singletons functions over dependent random variables. Funding: This research was supported by a Sloan Foundation Research Fellowship and the National Science Foundation [Award CCF-1942583 (CAREER)].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0552},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1849-1875},
  shortjournal = {Oper. Res.},
  title        = {On simple mechanisms for dependent items},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simultaneous column-and-row generation solution method for liner shipping network design. <em>OR</em>, <em>73</em>(4), 1825-1848. (<a href='https://doi.org/10.1287/opre.2020.0458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The liner shipping network design (LSND) problem involves creating regular ship rotations to transport containerized cargo between seaports. The objective is to maximize carrier profit by balancing revenue from satisfied demand against operating and transshipment costs. Finding an optimal solution is challenging because of complex rotation structures and joint decisions on fleet deployment, cargo routing, and rotation design. This work introduces a set partitioning–like formulation for LSND with transshipment costs, featuring an exponential number of variables and constraints. The formulation captures key service components, such as ship type, sailing speed, and frequency. Addressing transshipment costs requires numerous rotation-dependent variables and constraints, making even linear programming relaxation difficult to solve. To tackle this, we propose a simultaneous column-and-row generation (SCRG) solution method with novel speedup techniques. Integrating SCRG into a branch-and-price algorithm, we develop an exact method for LSND and test it on two variants with different rotation configurations. Extensive computational experiments demonstrate the method’s effectiveness and efficiency. In addition to advancing solution methods for LSND, this work enhances the SCRG-based method and expands its practical applications. Funding: This research was supported by the National Natural Science Foundation of China [Grants 72171147, 72031006] and the Research Grants Council of Hong Kong SAR, China [Grant 15221619]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2020.0458 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0458},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1825-1848},
  shortjournal = {Oper. Res.},
  title        = {A simultaneous column-and-row generation solution method for liner shipping network design},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint order fulfillment and inventory management in assemble-to-order generalized w systems. <em>OR</em>, <em>73</em>(4), 1805-1824. (<a href='https://doi.org/10.1287/opre.2021.0281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper characterizes joint order fulfillment and inventory policies for assemble-to-order generalized W systems, in which k products are assembled from a common component and k product-specific (dedicated) components. We consider a periodic-review system and focus on nested fulfillment policies, in which orders are fulfilled in decreasing order of profit margins. We prove that the optimal fulfillment policy of a two-product W system is nested. For systems with more than two products, although nested policies may not be optimal in general, we identify a sufficient condition for the optimal policy to be nested, and furthermore, we show that a nested policy is asymptotically optimal in a high-demand regime. Based on these results, we develop an asymptotically optimal strategy for the joint fulfillment and inventory decision under which the policy for demand fulfillment is static nested and that for inventory procurement is of the newsvendor type. In addition, we derive insights regarding the interactions between the component inventories and find that although the inventories of the common component and a particular dedicated component are complementary, those of different dedicated components may not be substitutable. Funding: The work of M. Yu was partially supported by the Hong Kong Research Grant Council [Grants 16502018 and 647312]. The work of S. Zheng was partially supported by the Hong Kong Research Grant Council [Grant 16502621]. The work of J. Chen was partially supported by the National Natural Science Foundation of China [Grants 72171202 and 72232007]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0281 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0281},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1805-1824},
  shortjournal = {Oper. Res.},
  title        = {Joint order fulfillment and inventory management in assemble-to-order generalized w systems},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Split liver transplantation: An analytical decision support model. <em>OR</em>, <em>73</em>(4), 1785-1804. (<a href='https://doi.org/10.1287/opre.2022.0131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Split liver transplantation (SLT) is a procedure that potentially saves two lives using one liver, increasing the total benefit derived from the limited number of donated livers available. SLT may also improve equity by giving transplant candidates who are physically smaller (including children) increased access to liver transplants. However, SLT is rarely used in the United States. To help quantify the benefits of increased SLT utilization and provide decision support tools, we introduce a deceased-donor liver allocation model with both efficiency and fairness objectives. We formulate our model as a multiqueue fluid system, incorporating the specifics of donor-recipient size matching and patients’ dynamically changing health conditions. Leveraging a novel decomposition result, we find the exact optimal matching procedure, enabling us to benchmark the performance of different allocation policies against the theoretical optimal. Numerical results, utilizing data from the Organ Procurement and Transplantation Network, show that increased utilization of SLT can significantly reduce patient deaths, increase total quality-adjusted life years, and improve fairness among different patient groups. Funding: This work was supported by Carnegie Mellon University Tepper’s Health Care Initiative Funding from 2022 to 2023. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0131 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0131},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1785-1804},
  shortjournal = {Oper. Res.},
  title        = {Split liver transplantation: An analytical decision support model},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard and soft defense against a sequence of aerial threats. <em>OR</em>, <em>73</em>(4), 1767-1784. (<a href='https://doi.org/10.1287/opre.2024.1025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of missiles and drones (hereafter referred to as threats) in attacks by both state and nonstate actors highlights the critical need for a robust defense system to counter these threats. We develop a combat model for the engagement between a Blue defender who is subject to repeated attacks by Red threats. The defender employs two types of defenses: hard interceptors, such as antiballistic missiles, and soft measures, such as directed-energy weapons and jamming. Employing strategies for these two types of defensive options are evaluated by a two-dimensional measure of effectiveness: expected number of leaking Red threats and the expected expenditure of hard interceptors. We define efficient frontiers on this two-dimensional space and identify defense strategies that compose these frontiers. Funding: This research is supported by funding from the Naval Postgraduate School, Naval Research Program [PE 0605853N/2098]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2024.1025 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1025},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1767-1784},
  shortjournal = {Oper. Res.},
  title        = {Hard and soft defense against a sequence of aerial threats},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unit commitment without commitment: A dynamic programming approach for managing an integrated energy system under uncertainty. <em>OR</em>, <em>73</em>(4), 1744-1766. (<a href='https://doi.org/10.1287/opre.2023.0546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though variability and uncertainty have always posed challenges for power systems, the increasing use of renewable energy sources has exacerbated these issues. At a vertically integrated utility, the system operator manages many generation units—renewable and otherwise—and storage units to ensure that the total energy produced matches contemporaneous demand. Current industry practice at these utilities involves solving “unit commitment” and “economic dispatch” optimization problems to choose production plans. These models, though complex, do not explicitly incorporate uncertainty. In this paper, we develop a dynamic programming approach to help system operators manage production under uncertainty. We formulate the problem as a stochastic dynamic program and use Lagrangian methods to decompose the system across units. The Lagrangian model relaxes the demand-matching constraint and introduces stochastic Lagrange multipliers that can be interpreted as prices representing the varying marginal value of energy production; each unit is then operated to maximize its own expected “profit” given these uncertain prices. These unit-specific value functions are then used to incorporate longer-term effects in dispatch decisions. The unit-specific value functions also provide a way to value generation and storage units in an uncertain environment. We develop relevant theory and demonstrate this dynamic approach using data from the Duke Energy Carolinas and Progress systems. Our numerical experiments demonstrate that this dynamic approach is computationally feasible at an industrial scale and can improve current practice. Specifically, our results suggest that this dynamic approach can reduce operational costs by about 2% on average in the present Duke Energy system and, in a “future” system with increased solar and storage capacity, can reduce operational costs by 4%–5% on average. Perhaps more strikingly, this dynamic approach, on average, performs within 0.2%–0.3% of production plans based on perfect foresight about future net demands. Funding: This work was supported by the Department of Energy Advanced Research Projects Agency - Energy [Grant DE-AR0001283, “A Grid that’s Risk-Aware for Clean Electricity (GRACE)”] and Fuqua and Tuck. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2023.0546 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0546},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1744-1766},
  shortjournal = {Oper. Res.},
  title        = {Unit commitment without commitment: A dynamic programming approach for managing an integrated energy system under uncertainty},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tailored base-surge policies in dual-sourcing inventory systems with demand learning. <em>OR</em>, <em>73</em>(4), 1723-1743. (<a href='https://doi.org/10.1287/opre.2022.0624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a periodic-review dual-sourcing inventory system in which the expedited supplier is faster and more costly, whereas the regular supplier is slower and cheaper. Under full demand distributional information, it is well known that the optimal policy is extremely complex but the celebrated Tailored Base-Surge (TBS) policy performs near optimally. Under such a policy, a constant order is placed at the regular source in each period, whereas the order placed at the expedited source follows a simple order-up-to rule. In this paper, we assume that the firm does not know the demand distribution a priori and makes adaptive inventory ordering decisions in each period based only on the past sales (a.k.a. censored demand) data. The standard performance measure is regret, which is the cost difference between a feasible learning algorithm and the clairvoyant (full-information) benchmark. When the benchmark is chosen to be the (full-information) best Tailored Base-Surge policy, we develop the first nonparametric learning algorithm that admits a regret bound of O ( T ( log T ) 3 log log T ) , which is provably tight up to a logarithmic factor. Leveraging the structure of this problem, our approach combines the power of bisection search and stochastic gradient descent and also involves a delicate high-probability coupling argument between our and the clairvoyant optimal system dynamics. Funding: The research of C. Shi is partially supported by an Amazon research award. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0624 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0624},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1723-1743},
  shortjournal = {Oper. Res.},
  title        = {Tailored base-surge policies in dual-sourcing inventory systems with demand learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
