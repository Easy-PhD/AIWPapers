<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>informs</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="deca">DECA - 4</h2>
<ul>
<li><details>
<summary>
(2025). Note: Ordinal equivalence of buying and selling prices for Information—When is constant risk attitude entailed?. <em>DECA</em>, <em>22</em>(3), 227-233. (<a href='https://doi.org/10.1287/deca.2024.0307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazen and Sounderpandian (1999) claim to have demonstrated that ordinal equivalence of the buying price and the selling price for information entails that the utility function must have constant risk attitude (i.e., be linear or exponential). However, there is no explicit indication of the scope of the ordinal equivalence assumed—is it within any particular decision problem , or within all decision problems sharing the same utility function ? Here we explore the difference between these two assumptions, providing counterexamples for the constant risk attitude assertion in the first case, and in the second case a proof that buy-sell ordinal equivalence within all decision problems sharing the same utility forces constant risk attitude. In the first case, the simplest counterexample arises from the assertion that more refined information sources have greater information values.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0307},
  journal      = {Decision Analysis},
  month        = {9},
  number       = {3},
  pages        = {227-233},
  shortjournal = {Decis. Anal.},
  title        = {Note: Ordinal equivalence of buying and selling prices for Information—When is constant risk attitude entailed?},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization scheduling of integrated park energy systems with hydrogen energy storage based on hybrid copula. <em>DECA</em>, <em>22</em>(3), 206-226. (<a href='https://doi.org/10.1287/deca.2024.0261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the energy utilization efficiency of the comprehensive energy park and the proportion of renewable energy grid connection, this paper first proposes to use the hybrid copula theory to construct a wind power and photovoltaics joint output model, reducing the uncertainty of wind power output and photovoltaic output. Secondly, this paper proposes the participation of hydrogen energy storage equipment in the power system scheduling of integrated energy parks. Hydrogen energy storage, as a clean, efficient, and sustainable carbon-free energy storage technology, can be used to mitigate the impact of wind power and photovoltaics output on the power grid. Finally, this paper proposes to construct an optimized scheduling model that considers economic optimization and proves the economic feasibility of the model through case analysis. The calculation results show that the comprehensive energy operating cost has been reduced by 14.15%, the proportion of thermal power output has been reduced by 32.54%, and the proportion of wind solar combined output has increased by 30.9%.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0261},
  journal      = {Decision Analysis},
  month        = {9},
  number       = {3},
  pages        = {206-226},
  shortjournal = {Decis. Anal.},
  title        = {Optimization scheduling of integrated park energy systems with hydrogen energy storage based on hybrid copula},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning-assisted decision-making methodology based on simplex weight generation for non-dominated alternative selection. <em>DECA</em>, <em>22</em>(3), 189-205. (<a href='https://doi.org/10.1287/deca.2024.0188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiobjective decision-making problems, it is common to encounter nondominated alternatives. In these situations, the decision-making process becomes complex, as each alternative offers better outcomes for some objectives and worse outcomes for others simultaneously. However, DMs still must choose a single alternative that provides an acceptable balance between the conflicting objectives, which can become exceedingly challenging. To address this scenario, our work introduces a decision-making framework aimed at supporting such decisions. Our proposed framework draws upon concepts from the field of Multi-Criteria Decision Making, and combines a novel simplex-like weight generation method with expert insights and machine learning data-driven procedures to establish an intuitive methodology that empowers DMs to select a single alternative from a range of alternatives. In this paper, we illustrate the effectiveness of our methodology through an example and two real-world decision cases from the oil and gas industry, each involving 128 alternatives and five distinct objectives. Funding: This work was supported by Equinor [Grant 2017/15736-3]; Fundação de Amparo à Pesquisa do Estado de São Paulo [Grant 2017/15736-3].},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0188},
  journal      = {Decision Analysis},
  month        = {9},
  number       = {3},
  pages        = {189-205},
  shortjournal = {Decis. Anal.},
  title        = {A machine learning-assisted decision-making methodology based on simplex weight generation for non-dominated alternative selection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile-parameterized distributions for expert knowledge elicitation. <em>DECA</em>, <em>22</em>(3), 169-188. (<a href='https://doi.org/10.1287/deca.2024.0219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a comprehensive overview of quantile-parameterized distributions (QPDs) as a tool for capturing expert predictions and parametric judgments. We survey a range of methods for constructing distributions that are parameterized by a set of quantile-probability pairs and describe an approach to generalizing them to enhance their tail flexibility. Furthermore, we explore the extension of QPDs to the multivariate setting, surveying the approaches to construct bivariate distributions, which can be adopted to obtain distributions with quantile-parameterized margins. Through this review and synthesis of the previously proposed methods, we aim to enhance the understanding and utilization of QPDs in various domains. Funding: U. Sahlin was funded by the Crafoord Foundation [ref 20200626]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/deca.2024.0219 .},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0219},
  journal      = {Decision Analysis},
  month        = {9},
  number       = {3},
  pages        = {169-188},
  shortjournal = {Decis. Anal.},
  title        = {Quantile-parameterized distributions for expert knowledge elicitation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijaa">IJAA - 6</h2>
<ul>
<li><details>
<summary>
(2025). Optimizing a heritage railway provider’s volunteer workforce allocation: The case of swanage railways. <em>IJAA</em>, <em>55</em>(4), 375-382. (<a href='https://doi.org/10.1287/inte.2024.0160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Heritage railways are railway operations that are not mainstream and serve as a reminiscence of past railways to build a cultural and national identity. They are among the main forms of heritage tourism. The heritage railway industry differs fundamentally from other forms of tourism and travel because of the significantly large fraction of volunteers in its workforce; additionally, heritage railways are of an extremely smaller scale than mainstream railways, with lower annual revenues. Although the regular railway industry extensively uses mathematical decision-making technologies for its daily operations, there is little evidence and few case studies demonstrating such value for heritage railways. Among the first of such studies, we present our experiences in using mathematical optimization models that improved the workforce allocation at a premier UK-based heritage railway company: Swanage Railways. Our collaboratively developed optimization models show four hours of reduction in weekly overtime for some employees during emergencies. If volunteers are efficiently integrated into the workforce, we find a reduction in overall workload by 26.7% for some of the existing employees. Finally, our models present a potential to reduce staffing costs by up to 35% if an hourly wage system is used instead of a fixed salary system. History: This paper was refereed.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0160},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {7-8},
  number   = {4},
  pages    = {375-382},
  title    = {Optimizing a heritage railway provider’s volunteer workforce allocation: The case of swanage railways},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-creating an analytical mindset at a financial technology platform. <em>IJAA</em>, <em>55</em>(4), 360-374. (<a href='https://doi.org/10.1287/inte.2024.0157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Collaborations between researchers and businesses can significantly benefit both parties. However, it can be challenging for researchers to form lasting ties with businesses. The challenge is particularly salient for early career researchers who have yet to build track records and for new technology ventures that must carefully allocate their scarce resources. Our purpose is to articulate the benefits and key success factors of such researcher-venture collaborations. We do so with a case study of a now-prominent European crowdinvesting platform with which we have collaborated since its nascence. Our collaboration yielded significant impact for academia by advancing scholarly knowledge and for the venture through both concrete and intangible benefits. In addition to data and insight, the collaboration also introduced new practices in the venture’s data-driven decision making. Building on our experiences and interviews with key managers, we propose seven key success factors for mutually beneficial researcher-venture collaborations: pursuing collaborations proactively, cultivating agility, being physically present, respecting business priorities, communicating results promptly, translating research to business language, and leveraging institutional backing. We offer practical guidance for early career researchers on how to build successful collaborations with technology ventures. The study provides insights for ventures into the benefits of collaborating with researchers. History: This paper was refereed. Funding: This work was supported by the Emil Aaltonen Foundation, the Finnish Cultural Foundation, the Finnish Foundation for Share Promotion, the Foundation for Aalto University Science and Technology, the Foundation for Economic Education, the Foundation for the Advancement of Finnish Securities Markets, the Foundation of Finnish Business College, Jenny and Antti Wihuri Foundation, Kaute Foundation, Marcus Wallenberg Foundation, Nasdaq Nordic Foundation, Paulo Foundation, Savings Banks Research Foundation, and the Support Foundation of the Helsinki School of Economics.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0157},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {7-8},
  number   = {4},
  pages    = {360-374},
  title    = {Co-creating an analytical mindset at a financial technology platform},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overbooking surgical blocks. <em>IJAA</em>, <em>55</em>(4), 344-359. (<a href='https://doi.org/10.1287/inte.2023.0063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Operating rooms (ORs) are some of the most expensive and critical resources for hospitals, and therefore OR access and utilization are of high importance. We developed a unique method to overbook surgeon blocks: assigning more OR block time than we have OR capacity available, to improve OR access. The challenge lies in managing the process of overbooking and ensuring fairness so that no one surgeon is more affected by overbooking compared with others. Using mixed-integer quadratic programming, we enable block overbooking while applying a consistent definition of fairness. This effort was implemented into routine operations through our simplified process, and it was deployed as a web tool for the surgical staff to manage and communicate decisions. Overbooking blocks contributed an additional 2% and 3.5% to OR utilization at the two campuses where it was deployed. This work enables surgeons, scheduling managers, and surgical leadership to manage operating room use beyond physical capacity, reliably and optimally. History: This paper was refereed.},
  archive  = {J},
  doi      = {10.1287/inte.2023.0063},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {7-8},
  number   = {4},
  pages    = {344-359},
  title    = {Overbooking surgical blocks},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A tutorial on teaching data analytics with generative AI. <em>IJAA</em>, <em>55</em>(4), 319-343. (<a href='https://doi.org/10.1287/inte.2023.0053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This tutorial addresses the challenge of incorporating large language models, such as ChatGPT, in a data analytics class. It details several new in-class and out-of-class teaching techniques enabled by artificial intelligence (AI). Here are three examples. Instructors can parallelize instruction by having students interact with different custom-made GPTs to learn different parts of an analysis and then teach each other what they learned from their GPTs. Instructors can turn problem sets into AI tutoring sessions: a custom-made GPT guides a student through the problems and the student uploads the chatlog for their homework submission. Instructors can assign different labs to each section of a class and have each section create AI assistants to help the other sections work through their labs. This tutorial advocates the natural language programming (NLP) paradigm, in which students articulate desired data transformations with a spoken language, such as English, and then use AI to generate the corresponding computer code. Students can wrangle data more effectively with NLP than with Excel. History: This paper was refereed.},
  archive  = {J},
  doi      = {10.1287/inte.2023.0053},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {7-8},
  number   = {4},
  pages    = {319-343},
  title    = {A tutorial on teaching data analytics with generative AI},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generic approach to optimize a blueprint schedule for multidisciplinary group therapy in rehabilitation care. <em>IJAA</em>, <em>55</em>(4), 296-318. (<a href='https://doi.org/10.1287/inte.2023.0070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Rehabilitation care consists of a wide variety of treatments organized in appointment series for individual and group therapy. Blueprint scheduling is a tactical approach to support appointment booking at the operational level. A blueprint schedule prescribes both the timing of group therapy sessions and the allocation of therapist and treatment room capacity to these sessions and thus defines which appointment types can be booked on which time slots. It facilitates providing treatment at the right time, by the right therapist(s), at a suitable location, and with a therapy load equally spread over day and week. This paper presents a structured generic approach for tactical capacity planning in multidisciplinary rehabilitation care. The generic approach includes optimization of a blueprint schedule using mixed-integer linear programming that was integrated in a decision support tool. We applied our approach in the rehabilitation center of Sint Maartenskliniek in Nijmegen, the Netherlands. The resulting blueprint schedule enables Sint Maartenskliniek to meet demand for rehabilitation care at the right time, by the right therapist(s), and at a suitable location while spreading the therapy load. Our approach can be applied in similar multidisciplinary care settings, potentially improving quality of care, quality of labor, and costs of care. History: This paper was refereed. Funding: This project has been supported by the Sint Maartenskliniek.},
  archive  = {J},
  doi      = {10.1287/inte.2023.0070},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {7-8},
  number   = {4},
  pages    = {296-318},
  title    = {A generic approach to optimize a blueprint schedule for multidisciplinary group therapy in rehabilitation care},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical investigation into the prevalence and impacts of complicating environmental factors in published Interfaces/IJAA projects. <em>IJAA</em>, <em>55</em>(4), 279-295. (<a href='https://doi.org/10.1287/inte.2024.0128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Previous research describes 10 contextual complications that exist in the application of applying analytical models and how they impact the models and modeling approaches themselves. These complications are pervasive and, because they affect the constructs of the modeler, must be better understood by practitioners who implement such models and researchers in order to increase the robustness, appropriateness, and usefulness of the models themselves. This research surveys the extent of the presence of these factors and the extent to which they affected modeling efforts in 76 different published applications via an author survey. It finds that the factors are pervasive and their importance to the appropriateness and success of the modeling efforts is high. Further, it finds a strong interaction factor between them, with underlying business and project constructs on which the factors align. As a result, it seems that a line of research geared toward identifying and overcoming these factors would aid in the application of analytical models and demonstrate the applied value of the profession. For practitioners, it is of high value to be aware of and consider these contextual factors when implementing models in order to improve their probability of success. History: This paper was refereed.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0128},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {7-8},
  number   = {4},
  pages    = {279-295},
  title    = {An empirical investigation into the prevalence and impacts of complicating environmental factors in published Interfaces/IJAA projects},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijds">IJDS - 5</h2>
<ul>
<li><details>
<summary>
(2025). Call for Papers—INFORMS journal on data science virtual special issue on the dual edge of AI: Catalyzing and challenging the future of energy systems. <em>IJDS</em>, <em>4</em>(3), iii-iv. (<a href='https://doi.org/10.1287/ijds.2026.cfp.v05.n1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2026.cfp.v05.n1},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {iii-iv},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Call for Papers—INFORMS journal on data science virtual special issue on the dual edge of AI: Catalyzing and challenging the future of energy systems},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based feature selection method under budget constraint for multiclass classification problems. <em>IJDS</em>, <em>4</em>(3), 265-282. (<a href='https://doi.org/10.1287/ijds.2024.0050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel graph-based method for budget-constrained feature selection (GB-BC-FS) in multiclass classification problems. The method identifies a subset of features that complement each other’s ability to distinguish between different classes, thereby utilizing the entire feature space while maintaining the model’s predictive performance and adhering to budget constraints on feature costs. This is achieved through an intuitive heuristic based on a scoring function, allowing users to calibrate the solution provided by GB-BC-FS. The calibration prioritizes selecting features with complementary qualities while minimizing the costs associated with feature collection, under constraint compliance. The approach is designed to handle practical limitations, making it suitable for applications where resources like cost and time are constrained. This not only improves computational efficiency but also aligns with broader implications related to optimizing resource utilization and ensuring practical applicability in data-driven industries. The effectiveness of GB-BC-FS was validated through extensive experimental analysis, including two comprehensive experiments with a real case study. These experiments demonstrated that GB-BC-FS significantly outperforms existing state-of-the-art approaches, achieving an average accuracy improvement of 10.4% and saving an average of 85.17% in run time compared with finding the optimal set of features, all while adhering to budget limits. Our code is fully documented and available online at https://github.com/davidlevinwork/gbfs/ . Funding: This work was supported by the Israeli Ministry of Innovation, Science and Technology [Grant 0004323]. Data Ethics & Reproducibility Note: The code capsule is available at https://github.com/davidlevinwork/gbfs/ and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2024.0050 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2024.0050},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {265-282},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Graph-based feature selection method under budget constraint for multiclass classification problems},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic neighbourhood components analysis. <em>IJDS</em>, <em>4</em>(3), 248-264. (<a href='https://doi.org/10.1287/ijds.2023.0018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance metric learning is a fundamental task in data mining and is known to enhance the performance of various distance-based algorithms. In this paper, we consider stochastic training data in which repeated feature vectors can belong to different classes, a scenario in which existing methods of metric learning are known to struggle. This type of data is common in stochastic simulations, where multidimensional, recurrent system states are subject to inherent randomness. Classification models on such high-resolution simulation-generated data play a critical role in real-time decision making across diverse applications. This paper presents and implements a stochastic version of the popular neighbourhood components analysis. We demonstrate its behaviour on stochastic data using simulation models and reveal its advantages when used for nearest neighbour classification. Meanwhile, the assumptions of stochastic labelling and repeated feature vectors extend to data from various domains, suggesting that the method can attain broad impact. For example, beyond its applications to system control and decision making with digital twin simulation, it may enhance the analysis of data from sensor networks, recommender systems, and crowdsourced platforms, where stochasticity and recurring feature patterns are typical. Funding: This work was supported by the Engineering and Physical Sciences Research Council–funded STOR-i Centre for Doctoral Training at Lancaster University [Grant EP/L015692/1]. In addition, Barry L. Nelson’s work was supported by the U.S. National Science Foundation [Grant DMS-1854562]. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.0189724.v5 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0018 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0018},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {248-264},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Stochastic neighbourhood components analysis},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating hidden epidemic: A bayesian spatiotemporal compartmental modeling approach. <em>IJDS</em>, <em>4</em>(3), 230-247. (<a href='https://doi.org/10.1287/ijds.2023.0020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efforts to mitigate public health crises have been complicated by unreported cases and the ever-changing trends of those monitored health events across geographic regions and socioeconomic cultures. To resolve both challenges, we propose a Bayesian spatiotemporal susceptible-exposed-infected-recovered-removed (BayST-SEIRD) framework that builds the hidden effects of neighboring communities, local features, and the reporting rates into its transmission mechanism. To alleviate the computational burdens embedded in a fully Bayesian algorithm, we propose an alternating approach that learns the compartmental structure and the spatial effects separately. With a simulation study, we show that this algorithm can accurately retrieve our designed system. Then, we apply BayST-SEIRD to model the coronavirus disease 2019 (COVID-19) dynamics in the metropolitan Atlanta area. We observe that most counties’ reporting rates were below 10% of the projected total infected population and that age and educational level are negatively correlated with the exposing rate, suggesting the needs for stronger incentives for COVID-19 testing and quarantine among the younger population. Importantly, BayST-SEIRD facilitates the reconstruction of actual case counts of the monitored subject among neighboring communities, which is critical to designing impactful public health policy interventions. Funding: This research was supported by the National Center for Advancing Translational Sciences of the National Institutes of Health under Award Number UL1TR002378. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The work of K. Paynabar was partially supported by the Fouts Family Chair. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6447675/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0020 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0020},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {230-247},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Estimating hidden epidemic: A bayesian spatiotemporal compartmental modeling approach},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Observational vs. experimental data when making automated decisions using machine learning. <em>IJDS</em>, <em>4</em>(3), 197-229. (<a href='https://doi.org/10.1287/ijds.2023.0012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decisions supported by machine learning often aim to improve outcomes through interventions, such as influencing purchasing behavior with ads or increasing customer retention with special offers. However, using observational data to estimate these effects can introduce confounding bias. Although experimental data can mitigate confounding, it is not always feasible to obtain and can be costly when it is. This paper presents theoretical results focusing on the impact of confounding on decision making, emphasizing that optimizing decisions often involves determining whether a causal effect exceeds a threshold rather than minimizing bias in the estimate. Consequently, models built with readily available but confounded data can sometimes yield decisions as good as or better than those based on costly, unconfounded data. This can occur when larger effects are more likely to be overestimated or when the benefits of larger, cheaper data sets outweigh the drawbacks of confounding. We validate the theoretical findings using benchmark data from the 2016 Atlantic Causal Inference Conference causal modeling competition, encompassing 77 scenarios and 7,700 data sets. We then introduce theoretical conditions, weaker than ignorability, that characterize when confounding preserves effect rankings. These conditions allow for empirical heuristic tests to assess whether observational data aligns with this structure. Finally, we apply our findings in a large-scale case study using advertising data, demonstrating how these insights can guide decision making in practice. Funding: This research, including Yanfang Hou’s contributions, was supported by the Research Grants Council [Grant 26500822]. The authors thank Ira Rennert and the New York University/Stern Fubon Center for support. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.6587526.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0012 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0012},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {197-229},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Observational vs. experimental data when making automated decisions using machine learning},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijoc">IJOC - 18</h2>
<ul>
<li><details>
<summary>
(2025). On discrete subproblems in integer optimal control with total variation regularization in two dimensions. <em>IJOC</em>, <em>37</em>(4), 1121-1141. (<a href='https://doi.org/10.1287/ijoc.2024.0680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze integer linear programs that we obtain after discretizing two-dimensional subproblems arising from a trust-region algorithm for mixed integer optimal control problems with total variation regularization. We discuss NP-hardness of the discretized problems and the connection to graph-based problems. We show that the underlying polyhedron exhibits structural restrictions in its vertices with regard to which variables can attain fractional values at the same time. Based on this property, we derive cutting planes by employing a relation to shortest-path and minimum bisection problems. We propose a branching rule and a primal heuristic which improves previously found feasible points. We validate the proposed tools with a numerical benchmark in a standard integer programming solver. We observe a significant speedup for medium-sized problems. Our results give hints for scaling toward larger instances in the future. History: Accepted by Andrea Lodi, Area Editor for Design & Analysis of Algorithms–Discrete. Funding: This work was supported by the Deutsche Forschungsgemeinschaft [Grant MA 10080/2-1]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0680 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0680 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0680},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1121-1141},
  shortjournal = {INFORMS J. Comput.},
  title        = {On discrete subproblems in integer optimal control with total variation regularization in two dimensions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approximation algorithm for minimum-weight (1,m)–Connected dominating set. <em>IJOC</em>, <em>37</em>(4), 1106-1120. (<a href='https://doi.org/10.1287/ijoc.2023.0306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a graph with nonnegative node weight. A vertex subset is called a CDS (connected dominating set) if every other node has at least one neighbor in the subset and the subset induces a connected subgraph. Furthermore, if every other node has at least m neighbors in the subset, then the node subset is called a ( 1 , m ) CDS. The minimum-weight ( 1 , m ) CDS problem aims at finding a ( 1 , m ) CDS with minimum total node weight. In this paper, we present a new polynomial-time approximation algorithm for this problem, which improves previous ratio by a factor of 2/3. History: Accepted by Erwin Pesch, Area Editor for Heuristic Search & Approximation Algorithms. Funding: This work was supported by the National Natural Science Foundation of China [Grant U20A2068] and the National Science Foundation [Grant III-1907472]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0306 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0306 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0306},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1106-1120},
  shortjournal = {INFORMS J. Comput.},
  title        = {A new approximation algorithm for minimum-weight (1,m)–Connected dominating set},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulating confidence intervals for conditional value-at-risk via least-squares metamodels. <em>IJOC</em>, <em>37</em>(4), 1087-1105. (<a href='https://doi.org/10.1287/ijoc.2023.0394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metamodeling techniques have been applied to approximate portfolio loss as a function of financial risk factors, thus producing point estimates of various measures of portfolio risk based on Monte Carlo samples. Rather than point estimates, this paper focuses on the construction of confidence intervals (CIs) for a widely used risk measure, the so-called conditional value-at-risk (CVaR), when the least-squares method (LSM) is employed as a metamodel in the point estimation. To do so, we first develop lower and upper bounds of CVaR and construct CIs for these bounds. Then, the lower end of the CI for the lower bound and the upper end of the CI for the upper bound together form a CI of CVaR with justifiable statistical guarantee, which accounts for both the metamodel error and the noises of Monte Carlo samples. The proposed CI procedure reuses the samples simulated for LSM point estimation, thus requiring no additional simulation budget. We demonstrate via numerical examples that the proposed procedure may lead to a CI with the desired coverage probability and a much smaller width than that of an existing CI in the literature. History: Accepted by Bruno Tuffin, Area Editor for Simulation. Funding: This research was supported by the National Natural Science Foundation of China (NNSFC) [Grants 72101260 and 72471232], the Research Grants Council of Hong Kong (RGC-HK) [General Research Fund Project 11508620], InnoHK Initiative, the Government of the HKSAR, and Laboratory for AI-Powered Financial Technologies, and NNSFC/RGC-HK Joint Research Scheme [Project N_CityU 105/21]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0394 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0394 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0394},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1087-1105},
  shortjournal = {INFORMS J. Comput.},
  title        = {Simulating confidence intervals for conditional value-at-risk via least-squares metamodels},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On interdicting dense clusters in a network. <em>IJOC</em>, <em>37</em>(4), 1069-1086. (<a href='https://doi.org/10.1287/ijoc.2023.0027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a vertex-weighted undirected graph with blocking costs of its vertices and edges, we seek a minimum cost subset of vertices and edges to block such that the weight of any γ -quasi-clique in the interdicted graph is at most some predefined threshold parameter. The value of γ ∈ ( 0 , 1 ] specifies the edge density of cohesive vertex groups of interest in the network. The considered weighted γ-quasi-clique interdiction problem can be viewed as a natural generalization of several variations of the clique blocker problem previously studied in the literature. From the application perspective, this setting is primarily motivated by the problem of disrupting adversarial (“dark”) networks (e.g., social or communication networks), where γ -quasi-cliques represent “tightly knit” groups of adversaries that we aim to dismantle. We first address the theoretical computational complexity of the problem. We then exploit some basic characterization of its feasible solutions to derive a linear integer programming (IP) formulation. This linear IP model can be solved using a lazy-fashioned branch-and-cut scheme. We also propose a combinatorial branch-and-bound algorithm for solving this problem. The computational performance of the developed exact solution schemes is studied using a test bed of randomly generated and real-life networks. Finally, some interesting insights and observations are also provided using a well-known example of a terrorist network. History: Accepted by Russel Bent, Area Editor for Network Optimization: Algorithms & Applications. Funding: The work of S. Butenko was partially supported by the Air Force Office of Scientific Research under Award FA9550-23-1-0300. The work of O. A. Prokopyev was partially supported by the Office of Naval Research under Award ONR N00014-22-1-2678. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0027 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0027 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ . The online appendix is available at https://doi.org/10.1287/ijoc.2023.0027 .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0027},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1069-1086},
  shortjournal = {INFORMS J. Comput.},
  title        = {On interdicting dense clusters in a network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An algorithm for clustering with confidence-based must-link and cannot-link constraints. <em>IJOC</em>, <em>37</em>(4), 1044-1068. (<a href='https://doi.org/10.1287/ijoc.2023.0419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study here the semisupervised k -clustering problem where information is available on whether pairs of objects are in the same or different clusters. This information is available either with certainty or with a limited level of confidence. We introduce the pair-wise confidence constraints clustering (PCCC) algorithm, which iteratively assigns objects to clusters while accounting for the information provided on the pairs of objects. Our algorithm uses integer programming for the assignment of objects, which allows us to include relationships as hard constraints that are guaranteed to be satisfied or as soft constraints that can be violated subject to a penalty. This flexibility distinguishes our algorithm from the state of the art, in which all pair-wise constraints are considered hard or all are considered soft. We developed an enhanced multistart approach and a model-size reduction technique for the integer program that contribute to the effectiveness and efficiency of the algorithm. Unlike existing algorithms, our algorithm scales to large-scale instances with up to 60,000 objects, 100 clusters, and millions of cannot-link constraints (which are the most challenging constraints to incorporate). We compare the PCCC algorithm with state-of-the-art approaches in an extensive computational study. Even though the PCCC algorithm is more general than the state-of-the-art approaches in its applicability, it outperforms the state-of-the-art approaches on instances with all hard or all soft constraints in terms of both run time and various metrics of solution quality. The code of the PCCC algorithm is publicly available on GitHub. History: Accepted by Ram Ramesh, Area Editor for Data Science and Machine Learning. Funding: The research of D. S. Hochbaum was supported by the AI Institute NSF Award [Grant 2112533]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0419 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0419 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0419},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1044-1068},
  shortjournal = {INFORMS J. Comput.},
  title        = {An algorithm for clustering with confidence-based must-link and cannot-link constraints},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient optimization model and tabu Search–Based global optimization approach for the continuous p-dispersion problem. <em>IJOC</em>, <em>37</em>(4), 1018-1043. (<a href='https://doi.org/10.1287/ijoc.2023.0089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous p -dispersion problems with and without boundary constraints are NP-hard optimization problems with numerous real-world applications, notably in facility location and circle packing, which are widely studied in mathematics and operations research. In this work, we concentrate on general cases with a nonconvex multiply connected region that are rarely studied in the literature due to their intractability and the absence of an efficient optimization model. Using the penalty function approach, we design a unified and almost everywhere differentiable optimization model for these complex problems and propose a tabu search–based global optimization (TSGO) algorithm for solving them. Computational results over a variety of benchmark instances show that the proposed model works very well, allowing popular local optimization methods (e.g., the quasi-Newton methods and the conjugate gradient methods) to reach high-precision solutions due to the differentiability of the model. These results further demonstrate that the proposed TSGO algorithm is very efficient and significantly outperforms several popular global optimization algorithms in the literature, improving the best-known solutions for several existing instances in a short computational time. Experimental analyses are conducted to show the influence of several key ingredients of the algorithm on computational performance. History: Accepted by Erwin Pesch, Area Editor for Heuristic Search & Approximation Algorithms. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72122006, 71821001, and 72471100] Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0089 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0089 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0089},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1018-1043},
  shortjournal = {INFORMS J. Comput.},
  title        = {An efficient optimization model and tabu Search–Based global optimization approach for the continuous p-dispersion problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fraud detection by integrating multisource heterogeneous presence-only data. <em>IJOC</em>, <em>37</em>(4), 998-1017. (<a href='https://doi.org/10.1287/ijoc.2023.0366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In credit fraud detection practice, certain fraudulent transactions often evade detection because of the hidden nature of fraudulent behavior. To address this issue, an increasing number of positive-unlabeled (PU) learning techniques have been employed by more and more financial institutions. However, most of these methods are designed for single data sets and do not take into account the heterogeneity of data when they are collected from different sources. In this paper, we propose an integrative PU learning method (I-PU) for pooling information from multiple heterogeneous PU data sets. A novel approach that penalizes group differences is developed to explicitly and automatically identify the cluster structures of coefficients across different data sets, thus offering a plausible interpretation of heterogeneity. Furthermore, we apply a bilevel selection method to detect the sparse structure at both the group level and within-group level. Theoretically, we show that our proposed estimator has the oracle property. Computationally, we design an expectation-maximization (EM) algorithm framework and propose an alternating direction method of multipliers (ADMM) algorithm to solve it. Simulation results show that our proposed method has better numerical performance in terms of variable selection, parameter estimation, and prediction ability. Finally, a real-world application showcases the effectiveness of our method in identifying distinct coefficient clusters and its superior prediction performance compared with direct data merging or separate modeling. This result also offers valuable insights for financial institutions in developing targeted fraud detection systems. History: Accepted by Ram Ramesh, Area Editor for Data Science & Machine Learning. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72071169, 72231005, 72233002, and 72471169], the Fundamental Research Funds for the Central Universities of China [Grant 20720231060], the National Social Science Fund of China [Grant 21&ZD146], and Shuimu Tsinghua Scholar Program. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0366 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0366 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0366},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {998-1017},
  shortjournal = {INFORMS J. Comput.},
  title        = {Fraud detection by integrating multisource heterogeneous presence-only data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain-independent dynamic programming and constraint programming approaches for assembly line balancing problems with setups. <em>IJOC</em>, <em>37</em>(4), 977-997. (<a href='https://doi.org/10.1287/ijoc.2024.0603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose domain-independent dynamic programming (DIDP) and constraint programming (CP) models to exactly solve type 1 and type 2 assembly line balancing problem with sequence-dependent setup times (SUALBPs). The goal is to assign tasks to assembly stations and to sequence these tasks within each station while satisfying precedence relations specified between a subset of task pairs. Each task has a given processing time and a setup time dependent on the previous task on the station to which the task is assigned. The sum of the processing and setup times of tasks assigned to each station constitute the station time and the maximum station time is called the cycle time. For the type 1 SUALBP (SUALBP-1), the objective is to minimize the number of stations, given a maximum cycle time. For the type 2 SUALBP (SUALBP-2), the objective is to minimize the cycle time, given the number of stations. On a set of diverse SUALBP instances, experimental results show that our approaches significantly outperform the state-of-the-art mixed integer programming models for SUALBP-1. For SUALBP-2, the DIDP model outperforms the state-of-the-art exact approach based on logic-based Benders decomposition. By closing 76 open instances for SUALBP-2, our results demonstrate the promise of DIDP for solving complex planning and scheduling problems. History: Accepted by Pascal Van Hentenryck, Area Editor for Computational Modeling: Methods and Analysis. Funding: This work was supported by Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2020-04039]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0603 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0603 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0603},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {977-997},
  shortjournal = {INFORMS J. Comput.},
  title        = {Domain-independent dynamic programming and constraint programming approaches for assembly line balancing problems with setups},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A peaceman-rachford splitting method for the protein side-chain positioning problem. <em>IJOC</em>, <em>37</em>(4), 962-976. (<a href='https://doi.org/10.1287/ijoc.2023.0094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the NP-hard protein side-chain positioning ( SCP ) problem, an important final task of protein structure prediction. We formulate the SCP as an integer quadratic program and derive its doubly nonnegative (DNN) (convex) relaxation. Strict feasibility fails for this DNN relaxation. We apply facial reduction to regularize the problem. This gives rise to a natural splitting of the variables. We then use a variation of the Peaceman-Rachford splitting method to solve the DNN relaxation. The resulting relaxation and rounding procedures provide strong approximate solutions. Empirical evidence shows that almost all our instances of this NP-hard SCP problem, taken from the Protein Data Bank, are solved to provable optimality . Our large problems correspond to solving a DNN relaxation with 2,883,601 binary variables to provable optimality. History: Accepted by Paul Brooks, Area Editor for Applications in Biology, Medicine, & Healthcare. Funding: This research was supported by the Natural Sciences and Engineering Research Council of Canada [Grants 50503-10827 and RGPIN-2016-04660]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0094 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0094 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0094},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {962-976},
  shortjournal = {INFORMS J. Comput.},
  title        = {A peaceman-rachford splitting method for the protein side-chain positioning problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting urban traffic states with sparse data using hankel temporal matrix factorization. <em>IJOC</em>, <em>37</em>(4), 945-961. (<a href='https://doi.org/10.1287/ijoc.2022.0197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting urban traffic states is crucial to transportation network monitoring and management, playing an important role in the decision-making process. Despite the substantial progress that has been made in developing accurate, efficient, and reliable algorithms for traffic forecasting, most existing approaches fail to handle sparsity, high-dimensionality, and nonstationarity in traffic time series and seldom consider the temporal dependence between traffic states. To address these issues, this work presents a Hankel temporal matrix factorization (HTMF) model using the Hankel matrix in the lower dimensional spaces under a matrix factorization framework. In particular, we consider an alternating minimization scheme to optimize the factor matrices in matrix factorization and the Hankel matrix in the lower dimensional spaces simultaneously. To perform traffic state forecasting, we introduce two efficient estimation processes on real-time incremental data, including an online imputation (i.e., reconstruct missing values) and an online forecasting (i.e., estimate future data points). Through extensive experiments on the real-world Uber movement speed data set in Seattle, Washington, we empirically demonstrate the superior forecasting performance of HTMF over several baseline models and highlight the advantages of HTMF for addressing sparsity, nonstationarity, and short time series. History: Accepted by Ram Ramesh, Area Editor for Data Science & Machine Learning. Funding: This research was supported by the Institute for Data Valorisation, the Interuniversity Research Centre on Enterprise Networks, Logistics and Transportation, the National Natural Science Foundation of China [Grants 12371456, 72101049, 72232001], the Sichuan Science and Technology Program [Grant 2024NSFJQ0038], and the Fundamental Research Funds for the Central Universities [Grant DUT23RC(3)045].},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2022.0197},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {945-961},
  shortjournal = {INFORMS J. Comput.},
  title        = {Forecasting urban traffic states with sparse data using hankel temporal matrix factorization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining precision boosting with LP iterative refinement for exact linear optimization. <em>IJOC</em>, <em>37</em>(4), 933-944. (<a href='https://doi.org/10.1287/ijoc.2023.0409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a combination of the two state-of-the-art algorithms for the exact solution of linear programs (LPs) over the rational numbers in practice, that is, without any roundoff errors or numerical tolerances. By integrating the method of precision boosting inside an LP iterative refinement loop, the combined algorithm is able to leverage the strengths of both methods: the speed of LP iterative refinement, in particular, in the majority of cases when a double-precision floating-point solver is able to compute approximate solutions with small errors, and the robustness of precision boosting whenever extended levels of precision become necessary. We compare the practical performance of the resulting algorithm with both pure methods on a large set of LPs and mixed-integer programs (MIPs). The results show that the combined algorithm solves more instances than a pure LP iterative refinement approach while being faster than pure precision boosting. When embedded in an exact branch-and-cut framework for MIPs, the combined algorithm is able to reduce the number of failed calls to the exact LP solver to zero while maintaining the speed of the pure LP iterative refinement approach. History: Accepted by Antonio Frangioni, Area Editor for Design and Analysis of Algorithms: Continuous. Funding: The work for this article has been conducted within the Research Campus Modal funded by the German Federal Ministry of Education and Research (BMBF) [Grants 05M14ZAM and 05M20ZBM].},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0409},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {933-944},
  shortjournal = {INFORMS J. Comput.},
  title        = {Combining precision boosting with LP iterative refinement for exact linear optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pairwise stability in weighted network formation games: Selection and computation. <em>IJOC</em>, <em>37</em>(4), 917-932. (<a href='https://doi.org/10.1287/ijoc.2024.0546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the selection and computation of pairwise stable networks when agents have differentiable and concave utility functions. We show that a pairwise stable network can be obtained by finding a Nash equilibrium of a noncooperative game played by the nodes and links in the network. Based on this observation, we introduce a logarithmic tracing procedure and a path-following algorithm for network formation games. We apply the algorithm to several models in the literature and make comparisons with two existing algorithms: a path-following algorithm based on the linear tracing procedure (LinTP) and a decompose and exhaustive search method (DaE). Numerical results indicate that the proposed method is more than four times as efficient as LinTP. Although DaE demonstrates exceptional efficiency for small-scale problems, our method outperforms it significantly for large-scale problems, where DaE may fail to find a solution. We also show that the decomposition technique of DaE can be used to further accelerate our algorithm for a special class of problems. History: Accepted by Antonio Frangioni, Area Editor for Design & Analysis of Algorithms–Continuous. Funding: This work was supported in part by the National Natural Science Foundation of China [Grants 12201289, 12122107, and 72394363/72394360], the Natural Science Foundation of Jiangsu Province [Grant BK20220754], the Guangdong Basic and Applied Basic Research Foundation [Grant 2021A1515110207], the Young Elite Scientists Sponsorship Program by CAST [Grant 2023QNRC001], and the Open Research Fund from the Guangdong Provincial Key Laboratory of Big Data Computing [Grant B10120210117-OF05]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0546 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0546 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0546},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {917-932},
  shortjournal = {INFORMS J. Comput.},
  title        = {Pairwise stability in weighted network formation games: Selection and computation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep stacking kernel machines for the data-driven multi-item, one-warehouse, multiretailer problems with backlog and lost sales. <em>IJOC</em>, <em>37</em>(4), 894-916. (<a href='https://doi.org/10.1287/ijoc.2022.0365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data-driven, multi-item, one-warehouse, multiretailer (OWMR) problem is examined by leveraging historical data and using machine learning methods to improve the ordering decisions in a two-echelon supply chain. A deep stacking kernel machine (DSKM) and its adaptive reweighting extension (ARW-DSKM), fusing deep learning and support vector machines, are developed for the data-driven, multi-item OWMR problems with backlog and lost sales. Considering the temporal network structure and the constraints connecting the subproblems for each item and each retailer, a Lagrange relaxation–based, trilevel, optimization algorithm and a greedy heuristic with good theoretical properties are developed to train the proposed DSKM and ARW-DSKM at acceptable computational costs. Empirical studies are conducted on two retail data sets, and the performances of the proposed methods and some benchmark methods are compared. The DSKM and the ARW-DSKM obtained the best results among the proposed and benchmark methods for the applications of ordering decisions with and without censored demands and with and without new items. Moreover, the implications in selecting suitable, that is, prediction-then-optimization and joint-prediction-and-optimization, frameworks, models/algorithms, and features are investigated. History: Accepted by Ram Ramesh, Area Editor for Data Science and Machine Learning. Funding: This work was supported by the National Natural Science Foundation of China [Grant 72371062]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2022.0365 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2022.0365 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2022.0365},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {894-916},
  shortjournal = {INFORMS J. Comput.},
  title        = {Deep stacking kernel machines for the data-driven multi-item, one-warehouse, multiretailer problems with backlog and lost sales},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clustering then estimation of spatio-temporal self-exciting processes. <em>IJOC</em>, <em>37</em>(4), 874-893. (<a href='https://doi.org/10.1287/ijoc.2022.0351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new estimation procedure for general spatio-temporal point processes that include a self-exciting feature. Estimating spatio-temporal self-exciting point processes with observed data is challenging, partly because of the difficulty in computing and optimizing the likelihood function. To circumvent this challenge, we employ a Poisson cluster representation for spatio-temporal self-exciting point processes to simplify the likelihood function and develop a new estimation procedure called “clustering-then-estimation” (CTE), which integrates clustering algorithms with likelihood-based estimation methods. Compared with the widely used expectation-maximization (EM) method, our approach separates the cluster structure inference of the data from the model selection. This has the benefit of reducing the risk of model misspecification. Our approach is computationally more efficient because it does not need to recursively solve optimization problems, which would be needed for EM. We also present asymptotic statistical results for our approach as theoretical support. Experimental results on several synthetic and real data sets illustrate the effectiveness of the proposed CTE procedure. History: Accepted by Ram Ramesh, Area Editor for Data Science & Machine Learning. Funding: J. Anderson is supported by NSF [Grant ECCS-2144634]. R. Righter is supported by the Ron Wolff Chaired Professorship. Z. Zheng is supported by NSF [Grant DMS-2220537]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2022.0351 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2022.0351 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2022.0351},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {874-893},
  shortjournal = {INFORMS J. Comput.},
  title        = {Clustering then estimation of spatio-temporal self-exciting processes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision making under cumulative prospect theory: An alternating direction method of multipliers. <em>IJOC</em>, <em>37</em>(4), 856-873. (<a href='https://doi.org/10.1287/ijoc.2023.0243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel numerical method for solving the problem of decision making under cumulative prospect theory (CPT), where the goal is to maximize utility subject to practical constraints, assuming only finite realizations of the associated distribution are available. Existing methods for CPT optimization rely on particular assumptions that may not hold in practice. To overcome this limitation, we present the first numerical method with a theoretical guarantee for solving CPT optimization using an alternating direction method of multipliers (ADMM). One of its subproblems involves optimization with the CPT utility subject to a chain constraint, which presents a significant challenge. To address this, we develop two methods for solving this subproblem. The first method uses dynamic programming, whereas the second method is a modified version of the pooling-adjacent-violators algorithm that incorporates the CPT utility function. Moreover, we prove the theoretical convergence of our proposed ADMM method and the two subproblem-solving methods. Finally, we conduct numerical experiments to validate our proposed approach and demonstrate how CPT’s parameters influence investor behavior, using real-world data. History: Accepted by Antonio Frangioni, Area Editor for Design & Analysis of Algorithms: Continuous. Funding: This research was supported by the National Natural Science Foundation of China [Grants 12171100, 71971083, and 72171138], the Natural Science Foundation of Shanghai [Grant 22ZR1405100], the Major Program of the National Natural Science Foundation of China [Grants 72394360, 72394364], the Program for Innovative Research Team of Shanghai University of Finance and Economics [Grant 2020110930], Fundamental Research Funds for the Central Universities, and the Open Research Fund of Key Laboratory of Advanced Theory and Application in Statistics and Data Science, Ministry of Education, East China Normal University. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0243 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0243 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0243},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {856-873},
  shortjournal = {INFORMS J. Comput.},
  title        = {Decision making under cumulative prospect theory: An alternating direction method of multipliers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Production planning under demand and endogenous supply uncertainty. <em>IJOC</em>, <em>37</em>(4), 831-855. (<a href='https://doi.org/10.1287/ijoc.2023.0067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of determining how much finished goods inventory to source from different capacitated facilities in order to maximize profits resulting from sales of such inventory. We consider a problem wherein there is uncertainty in demand for finished goods inventory and production yields at facilities. Further, we consider that uncertainty in production yields is endogenous, as it depends on both the facilities where a product is produced and the volumes produced at those facilities. We model the problem as a two stage stochastic program and propose an exact, Benders-based algorithm for solving instances of the problem. We prove the correctness of the algorithm and with an extensive computational study demonstrate that it outperforms known benchmarks. Finally, we establish the value in modeling uncertainty in both demands and production yields. History: Accepted by Andrea Lodi, Area Editor for Design & Analysis of Algorithms–Discrete. Supplemental Material: Software that implements the algorithms found in this paper, as well as the instances used in the computational study, can be found at Hewitt and Pantuso (2024) . The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0067 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0067 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0067},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {831-855},
  shortjournal = {INFORMS J. Comput.},
  title        = {Production planning under demand and endogenous supply uncertainty},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The electric vehicle routing and overnight charging scheduling problem on a multigraph. <em>IJOC</em>, <em>37</em>(4), 808-830. (<a href='https://doi.org/10.1287/ijoc.2023.0404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the electric vehicle (EV) routing and overnight charging scheduling problem, a fleet of EVs must serve the demand of a set of customers with time windows. The problem consists in finding a set of minimum cost routes and determining an overnight EV charging schedule that ensures the routes’ feasibility. Because (i) travel time and energy consumption are conflicting resources, (ii) the overnight charging operations take considerable time, and (iii) the charging infrastructure at the depot is limited, we model the problem on a multigraph where each arc between two vertices represents a path with a different resource consumption trade-off. To solve the problem, we design a branch-price-and-cut algorithm that implements state-of-the-art techniques, including the ng -path relaxation, subset-row inequalities, and a specialized labeling algorithm. We report computational results showing that the method solves to optimality instances with up to 50 customers. We also present experiments evaluating the benefits of modeling the problem on a multigraph rather than on the more classical 1-graph representation. History: Accepted by Andra Lodi, Area Editor for Design and Analysis of Algorithms—Discrete. Funding: This work was supported by the Natural Sciences and Engineering Research Council of Canada through the Discovery grants [Grant RGPIN-2023-03791]. It was also partially funded by HEC Montréal through the research professorship on Clean Transportation Analytics. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0404 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0404 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0404},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {808-830},
  shortjournal = {INFORMS J. Comput.},
  title        = {The electric vehicle routing and overnight charging scheduling problem on a multigraph},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient node selection policy for monte carlo tree search with neural networks. <em>IJOC</em>, <em>37</em>(4), 785-807. (<a href='https://doi.org/10.1287/ijoc.2023.0307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte Carlo tree search (MCTS) has been gaining increasing popularity, and the success of AlphaGo has prompted a new trend of incorporating a value network and a policy network constructed with neural networks into MCTS, namely, NN-MCTS. In this work, motivated by the shortcomings of the widely used upper confidence bounds applied to trees (UCT) policy, we formulate the node selection problem in NN-MCTS as a multistage ranking and selection (R&S) problem and propose a node selection policy that efficiently allocates a limited search budget to maximize the probability of correctly selecting the best action at the root state. The value and policy networks in NN-MCTS further improve the performance of the proposed node selection policy by providing prior knowledge and guiding the selection of the final action, respectively. Numerical experiments on two board games and an OpenAI task demonstrate that the proposed method outperforms the UCT policy used in AlphaGo Zero and MuZero, implying the potential of constructing node selection policies in NN-MCTS with R&S procedures. History: Accepted by Bruno Tuffin, Area Editor for Simulation. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72325007, 72250065, and 72022001], and a PKU-Boya Postdoctoral Fellowship 2406396158. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2023.0307 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2023.0307 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2023.0307},
  journal      = {INFORMS Journal on Computing},
  month        = {7-8},
  number       = {4},
  pages        = {785-807},
  shortjournal = {INFORMS J. Comput.},
  title        = {An efficient node selection policy for monte carlo tree search with neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ijoo">IJOO - 4</h2>
<ul>
<li><details>
<summary>
(2025). On the tradeoff between distributional belief and ambiguity: Conservatism, finite-sample guarantees, and asymptotic properties. <em>IJOO</em>, <em>7</em>(3), 240-263. (<a href='https://doi.org/10.1287/ijoo.2024.0047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and analyze a new data-driven tradeoff (TRO) approach for modeling uncertainty that serves as a middle ground between the optimistic approach, which adopts a distributional belief, and the pessimistic distributionally robust optimization approach, which hedges against distributional ambiguity. We equip the TRO model with a TRO ambiguity set characterized by a size parameter controlling the level of optimism and a shape parameter representing distributional ambiguity. We first show that constructing the TRO ambiguity set using a general star-shaped shape parameter with the empirical distribution as its star center is necessary and sufficient to guarantee the hierarchical structure of the sequence of TRO ambiguity sets. Then, we analyze the properties of the TRO model, including quantifying conservatism, quantifying bias and generalization error, and establishing asymptotic properties. Specifically, we show that the TRO model could generate a spectrum of decisions, ranging from optimistic to conservative decisions. Additionally, we show that it could produce an unbiased estimator of the true optimal value. Furthermore, we establish the almost-sure convergence of the optimal value and the set of optimal solutions of the TRO model to their true counterparts. We exemplify our theoretical results using an inventory control problem and a portfolio optimization problem. Funding: This material is based partially on work supported by the U.S. Department of Energy, Office of Energy Efficiency and Renewable Energy, specifically the Water Power Technology Office [Award DE-EE0009450]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/ijoo.2024.0047 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2024.0047},
  journal      = {INFORMS Journal on Optimization},
  month        = {Summer},
  number       = {3},
  pages        = {240-263},
  shortjournal = {INFORMS J. Optim.},
  title        = {On the tradeoff between distributional belief and ambiguity: Conservatism, finite-sample guarantees, and asymptotic properties},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing green energy systems. <em>IJOO</em>, <em>7</em>(3), 214-239. (<a href='https://doi.org/10.1287/ijoo.2024.0051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy production throughout the world is transitioning from fossil fuels to renewable sources such as wind power and solar power. This transition has been gradual—over half of the world’s electricity is still produced by coal, oil, and gas—but must accelerate to meet global emission targets. This paper examines the contributions that mathematical optimization and equilibrium models can make to help accelerate this transition. The models we catalog cover a range of physical scales and timescales. Our focus is on novel model formulations that can help overcome the challenges of the transition by unpicking the complexity inherent in many settings and quantifying the trade-offs that must be made when developing energy policy. Funding: This research was performed while the authors were participating in the Architecture of Green Energy Systems Program hosted by the University of Chicago, which is supported by the National Science Foundation [Grant DMS-1929348]. A. B. Philpott acknowledges support from MBIE Catalyst Fund New Zealand German Platform for Green Hydrogen Integration (HINT) [Grant UOCX2117].},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2024.0051},
  journal      = {INFORMS Journal on Optimization},
  month        = {Summer},
  number       = {3},
  pages        = {214-239},
  shortjournal = {INFORMS J. Optim.},
  title        = {Optimizing green energy systems},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tractable continuous approximations for constraint selection via cardinality minimization. <em>IJOO</em>, <em>7</em>(3), 195-213. (<a href='https://doi.org/10.1287/ijoo.2024.0038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a cardinality minimization problem that simultaneously minimizes an objective function and the cardinality of unsatisfied soft constraints. This paper proposes two continuous approximation methods that reformulate the discrete cardinality as complementarity constraints and difference-of-convex functions, respectively. We show that under suitable conditions, local and stationary solutions of the approximation problems recover local minimizers of the cardinality minimization problem. To demonstrate effectiveness, we apply the proposed methods to applications where violating as few preference conditions (or soft constraints) is desired. Our numerical study supports the use of methods based on our new approximations for cardinality minimization that produce comparable solutions with other benchmark formulations. Funding: The authors acknowledge the Office of Engaged Learning at Southern Methodist University for its support of the work of D. Troxell. The work of M. Ahn was partially supported by NSF CRII [Grant IIS-1948341].},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2024.0038},
  journal      = {INFORMS Journal on Optimization},
  month        = {Summer},
  number       = {3},
  pages        = {195-213},
  shortjournal = {INFORMS J. Optim.},
  title        = {Tractable continuous approximations for constraint selection via cardinality minimization},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robotic warehousing operations: A learn-then-optimize approach to large-scale neighborhood search. <em>IJOO</em>, <em>7</em>(3), 171-194. (<a href='https://doi.org/10.1287/ijoo.2024.0033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid deployment of robotics technologies requires dedicated optimization algorithms to manage large fleets of autonomous agents. This paper supports robotic parts-to-picker operations in warehousing by optimizing order–workstation assignments, item–pod assignments, and the schedule of order fulfillment at workstations. The model maximizes throughput, managing human workload at the workstations and congestion in the facility. We solve it via large-scale neighborhood search with a novel learn-then-optimize approach to subproblem generation. The algorithm relies on an off-line machine learning procedure to predict objective improvements based on subproblem features and an online optimization model to generate a new subproblem at each iteration. In collaboration with Amazon Robotics, we show that our model and algorithm generate much stronger solutions for practical problems than state-of-the-art approaches. In particular, our solution enhances the utilization of robotic fleets by coordinating robotic tasks for human operators to pick multiple items at once and by coordinating robotic routes to avoid congestion in the facility. Funding: This research was partially funded by Amazon.com Services LLC under award number 2D-12552417. Supplemental Material: The online appendix and code and data files are available at https://doi.org/10.1287/ijoo.2024.0033 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2024.0033},
  journal      = {INFORMS Journal on Optimization},
  month        = {Summer},
  number       = {3},
  pages        = {171-194},
  shortjournal = {INFORMS J. Optim.},
  title        = {Robotic warehousing operations: A learn-then-optimize approach to large-scale neighborhood search},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="isre">ISRE - 31</h2>
<ul>
<li><details>
<summary>
(2025). Research spotlights. <em>ISRE</em>, <em>36</em>(2), iv-xi. (<a href='https://doi.org/10.1287/isre.2025.resspot.v36.n2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2025.resspot.v36.n2},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {iv-xi},
  shortjournal = {Inf. Syst. Res.},
  title        = {Research spotlights},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). About our authors. <em>ISRE</em>, <em>36</em>(2), 1259-1267. (<a href='https://doi.org/10.1287/isre.2025.abtheau.v36.n2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2025.abtheau.v36.n2},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1259-1267},
  shortjournal = {Inf. Syst. Res.},
  title        = {About our authors},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Timely quality problem resolution in peer-production systems: The impact of bots, policy citations, and contributor experience. <em>ISRE</em>, <em>36</em>(2), 1242-1258. (<a href='https://doi.org/10.1287/isre.2020.0485'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although online peer-production systems have proven to be effective in producing high-quality content, their open call for participation makes them susceptible to ongoing quality problems. A key concern is that the problems should be addressed quickly to prevent low-quality content from remaining in place for extended periods. We examine the impacts of two control mechanisms, bots and policy citations, and the number of contributors, with and without prior experience in editing an article, on the cleanup time of 4,473 quality problem events in Wikipedia. We define cleanup time as the time it takes to resolve a quality problem once it has been detected in an article. Using an accelerated failure time model, we find that the number of bots editing an article during a quality problem event has no effect on cleanup time; that citing policies to justify edits during the event is associated with a longer cleanup time; and that more contributors, with or without prior experience in editing the article, are associated with a shorter cleanup time. We also find important interactions between each of the two control mechanisms and the number of different types of contributors. There is a marginal increase in cleanup time that is larger when an increase in the number of contributors is accompanied by fewer bots editing the article during a quality problem event. This interaction effect is more pronounced when increasing the number of contributors without prior experience in editing the article. Further, there is a marginal decrease in cleanup time that is larger when an increase in the number of contributors, with or without prior experience in editing the article, is accompanied by fewer policy citations. Taken together, our results show that the use of bots and policy citations as control mechanisms must be considered in conjunction with the number of contributors with and without prior experience in editing an article. Accordingly, the number of contributors and their experience alone may not explain important outcomes in peer production; it is also important to find an appropriate mix of different control mechanisms and types of contributors to address quality problems quickly. History: Yulin Fang, Senior Editor; Choon Ling Sia, Associate Editor. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2020.0485 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2020.0485},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1242-1258},
  shortjournal = {Inf. Syst. Res.},
  title        = {Timely quality problem resolution in peer-production systems: The impact of bots, policy citations, and contributor experience},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opening first-party app resources: Empirical evidence of free-riding. <em>ISRE</em>, <em>36</em>(2), 1228-1241. (<a href='https://doi.org/10.1287/isre.2021.0607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform owners are releasing their own apps on their platforms. These first-party apps (FPAs) typically leverage platform resources more effectively, competitively threatening rivals. Although the impact of FPAs on rivals’ innovation has been the subject of extensive study, the dominant view in previous research assumes that these FPAs are closed to third-party apps (TPAs). However, there is an increasing trend of FPAs opening their resources to TPAs, as they provide application programming interfaces (APIs) allowing TPAs to access their resources. Rivals still exist, as many TPAs choose not to have access to FPAs’ open resources because of their limited control over these resources. Does opening an FPA’s resources impact rivals’ innovation? The answer to the question is largely unknown. We exploit the release of the Apple Health Records API, a feature that opens Apple Health Records to TPAs, to design a quasi-experiment that investigates whether and how opening an FPA’s resources influence rivals’ innovations. Through several analyses, we conclude that opening an FPA’s resources to TPAs generates free-riding benefits for rivals. Moreover, these benefits mainly arise because of the growing presence of TPAs that do not adopt FPAs’ open resources in the market. We discuss the theoretical contributions and practical implications of our findings. History: Paul A. Pavlou, Senior Editor; Khim Yong Goh, Associate Editor. Funding: This work was funded in part through an endowed chair at the Sam M. Walton College of Business, University of Arkansas. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0607 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0607},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1228-1241},
  shortjournal = {Inf. Syst. Res.},
  title        = {Opening first-party app resources: Empirical evidence of free-riding},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Better is better? signaling paradoxes in performance-based advertising. <em>ISRE</em>, <em>36</em>(2), 1217-1227. (<a href='https://doi.org/10.1287/isre.2021.0419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertising signaling theory shows that costly advertising can serve as a credible signal of product quality (hereafter ad-signal ), provided that the effectiveness of advertising can be accurately measured. The rise of performance-based advertising, in which advertising is paid based on consumer actions, such as clicking on or purchasing through an ad link, presents challenges in measuring the effectiveness of advertising. Specifically, it becomes difficult to differentiate whether a consumer’s action is caused by ad-signal or the inherent product appeal. This leads to a critical question: how does this inaccuracy in evaluating advertising effectiveness affect the signaling role of advertising? This research analyzes how the inherent product reputation and the breadth of ad-signal reach impact the signaling role of advertising, uncovering two paradoxes under performance-based advertising: a higher product reputation does not necessarily help advertising to signal product quality (product reputation paradox), and a broader ad-signal reach can impede the signaling role of advertising (ad-signal reach paradox). We propose modified payment schemes to address both paradoxes. These insights contribute to advertising signaling theory and offer practical guidance for designing effective payment schemes under performance-based advertising. History: Yong Tan, Senior Editor. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72301265, 72171132].},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0419},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1217-1227},
  shortjournal = {Inf. Syst. Res.},
  title        = {Better is better? signaling paradoxes in performance-based advertising},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the dynamic and episodic nature of technostressors and their effects on cyberdeviance: A daily field investigation. <em>ISRE</em>, <em>36</em>(2), 1196-1216. (<a href='https://doi.org/10.1287/isre.2020.0273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technostress is an emerging topic in the information systems (IS) literature. Prior studies have mostly adopted a static approach to capturing snapshots of technostressors, without considering their dynamic and episodic nature. Daily technostress is often the product of discrete events encountered in the workplace that shape employees’ momentary affect and behaviors in situ. To lay the conceptual foundation for understanding deviant behaviors, this study integrates the self-regulation perspective into the transactional model of stress and conducts a contextualized, longitudinal, and daily investigation of how and when daily perceived technostressors affect employees’ daily cyberdeviant behaviors (i.e., cyberdeviance). In a time-lagged experience sampling study of 188 professionals who completed a survey three times a day for two weeks, we found that employees experiencing daily techno-overload and techno-invasion are likely to engage in daily cyberdeviance to cope with their daily exhaustion. We also examined the cross-level moderating effect of an individual trait (i.e., technology self-efficacy) on the strength of the within-person process model of daily cyberdeviance. Our multilevel analysis results showed that technology self-efficacy alleviated employees’ daily exhaustion induced by daily techno-overload. This study theoretically contributes to the IS literature by providing a theoretical explanation of the underlying mechanisms of techno-overload, techno-invasion, and employees’ cyberdeviance, in addition to using a within-person approach to understand the dynamic and episodic nature of these technostressors. History: Yulin Fang, Senior Editor; Zhenhui (Jack) Jiang, Associate Editor. Funding: This work was supported by MOE (Ministry of Education in China) Project of Humanities and Social Sciences [Grant 21YJA630007] and the Government of Spain and the European Regional Development Fund (European Union) [Research Projects PID2021-124725NB-I00, PID2021-124396NB-I00, and TED2021-130104B-I00]. This work was also supported by the Research Grants Council of the Hong Kong Special Administrative Region, China [Grant HKBU12500020]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2020.0273 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2020.0273},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1196-1216},
  shortjournal = {Inf. Syst. Res.},
  title        = {Understanding the dynamic and episodic nature of technostressors and their effects on cyberdeviance: A daily field investigation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Promoting security behaviors in remote work environments: Personal values shaping information security policy compliance. <em>ISRE</em>, <em>36</em>(2), 1183-1195. (<a href='https://doi.org/10.1287/isre.2021.0563'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity threats and information security policy (ISP) compliance are critical concerns for organizations. The recent trend of working from home has made individual characteristics more relevant in fostering ISP compliance. Whereas extant research has theorized and offered alternatives to induce ISP compliance, most studies have failed to consider the differences between onsite and remote workers to motivate compliance with ISPs and have instead focused on standard interventions without considering personal characteristics. One of the few models including factors related to individuals’ characteristics is the unified model of information security policy compliance (UMISPC). This paper extends the UMISPC by drawing on Schwartz’s universal theory of personal values. We propose the values construct as a robust representation of an individual’s motivations to comply with an ISP. We confirm that personal values are significant predictors of compliance with ISPs. Furthermore, a comparison between onsite and remote workers suggests that personal values are more relevant in remote work settings. Our findings shed light on the values and individual characteristics that are important motivators for ISP compliance and how they differ for onsite and remote workers. Our results suggest that people’s personal motivations should be considered in promoting organizations’ ISPs and that organizations’ interventions should be tailored by understanding what values motivate or hinder ISP compliance. History: Manju Ahuja, Senior Editor; Debabrata Dey, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0563 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0563},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1183-1195},
  shortjournal = {Inf. Syst. Res.},
  title        = {Promoting security behaviors in remote work environments: Personal values shaping information security policy compliance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of popularity cues and peer endorsements on assertive social media ads. <em>ISRE</em>, <em>36</em>(2), 1167-1182. (<a href='https://doi.org/10.1287/isre.2021.0606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms, such as Facebook, often display assertive call-to- action ads that encourage direct conversion actions, such as purchasing a product or installing an app. Additionally, these ads can display popularity signals (i.e., “likes”) and social endorsement from friends (i.e., friends’ “likes”). We examine the effectiveness of displaying these different signals on such ads in generating clicks through field tests conducted on Facebook. We collaborated with a mobile app company and conducted a call-to-action ad campaign on Facebook, targeting unique user groups with this type of ads for a mobile app. Our findings reveal that the overall number of “likes” associated with the ad does not impact the user’s decision to click the ad. Additionally, ads endorsed by friends have a lower clicking performance compared with those without such endorsement. Further analyses based on randomized laboratory studies and additional field tests using different products and apps confirm these results and demonstrate that the presence of these cues activates users’ persuasion knowledge of assertive call-to-action ads, resulting in a lower click performance. Furthermore, the negative impact of social cues is particularly pronounced when there is low similarity between users and their friends. These results have significant implications for the design of assertive call-to-action ads on social media platforms. History: Ram Gopal, Senior Editor; Wenjing Duan, Associate Editor. Funding: Funding was received from the McCombs Research Excellence Grant. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0606 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0606},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1167-1182},
  shortjournal = {Inf. Syst. Res.},
  title        = {The effect of popularity cues and peer endorsements on assertive social media ads},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of voice AI on digital commerce. <em>ISRE</em>, <em>36</em>(2), 1147-1166. (<a href='https://doi.org/10.1287/isre.2022.0140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voice-activated shopping assistants (voice AI), such as Amazon Alexa and Alibaba Tmall Genie, have been gaining popularity worldwide as a new channel for online shopping. In this paper, we analyze large-scale archival data of consumer-level purchase records from Alibaba, the world’s largest e-commerce platform, to empirically investigate how consumers’ adoption of Tmall Genie affects their consumption. The results show that the average consumer’s weekly spending the e-commerce platform increased by 16.6% within the first four months after adopting voice AI. Additionally, we explore specific product features that moderate the effect of Genie adoption by examining repeat purchases, product substitutability, and familiarity, supporting a mechanism that involves reducing information acquisition costs. The positive effects of Genie adoption remain significant on repeat purchases in the long term, although they attenuate over time. Furthermore, our analyses reveal that on average, the voice channel has a positive spillover effect on spending on the PC channel but no significant effect on the mobile channel. The channel dynamics are contingent on specific shopping contexts. Our results demonstrate that voice AI devices with shopping capabilities can enhance the growth of the affiliated e-commerce platform. As the first study to empirically examine the impact of voice AI adoption on e-commerce consumption, our paper provides valuable implications for e-commerce platforms and retailers leveraging voice-activated shopping. History: Deepa Mani, Senior Editor; Guodong (Gordon) Gao, Associate Editor. Funding: The authors acknowledge the research grant and support from the Marketing Science Institute (MSI). C. Sun acknowledges the support from National University of Singapore under the Start-Up Grant Scheme and the AI & Management (AIM) Research Center at Tsinghua SEM. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0140 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0140},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1147-1166},
  shortjournal = {Inf. Syst. Res.},
  title        = {The effect of voice AI on digital commerce},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Omnificence or differentiation? an empirical study of knowledge structure and career development of IT workers. <em>ISRE</em>, <em>36</em>(2), 1129-1146. (<a href='https://doi.org/10.1287/isre.2022.0634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amid the growing importance of information technology (IT) in the business landscape, the pivotal role of IT knowledge on the demand side of the labor market, at both industry and firm levels, is well documented. However, the important labor supply side concerning IT workers has remained largely unknown. This raises challenges about how IT professionals should strategically cultivate their IT knowledge structures toward a sustainable, well-compensated career path. This paper bridges this gap by examining how different types of IT knowledge structures of IT workers impact their salaries and job security over time. We theorize, define, and operationalize two new metrics to characterize the knowledge structures of an IT worker. Knowledge omnificence measures the breadth of an IT worker’s own knowledge structure, whereas knowledge differentiation assesses the extent of difference between one’s knowledge set and those of their peers. By analyzing extensive career data of IT workers from 2000 to 2016, we demonstrated that, on average, a high level of IT knowledge differentiation or omnificence yields positive economic returns for IT workers. However, there is an intriguing twist: such a positive relationship is not monotonic. The most advantageous strategy is to acquire IT knowledge at moderate levels of knowledge omnificence and differentiation. Further, our results revealed another new twist: to increase salary potential or pursue a better position, one should aim for knowledge omnificence, whereas those valuing job security should aim for knowledge differentiation. This aligns with our theoretical rationale that utilizes a structured framework, integrating the dynamic capability framework and the boundaryless career theory. Besides, we found that both knowledge omnificence and differentiation reduced gender disparity in the labor market. In particular, females benefited more, with a 14.74% (or 1.38%) increase in salary, from having a one-unit increase in knowledge omnificence (or differentiation). This study holds critical managerial implications for IT workers, firms, and policymakers. It emphasizes the importance of strategic management of IT knowledge structure in enabling IT workers to thrive in the dynamic and competitive IT job market. History: Manju Ahuja, Senior Editor; Yuliang Yao, Associate Editor. Funding: This work was supported by the National Natural Science Foundation of China [Grant 72272003]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0634 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0634},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1129-1146},
  shortjournal = {Inf. Syst. Res.},
  title        = {Omnificence or differentiation? an empirical study of knowledge structure and career development of IT workers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Less artificial, more intelligent: Understanding affinity, trustworthiness, and preference for digital humans. <em>ISRE</em>, <em>36</em>(2), 1096-1128. (<a href='https://doi.org/10.1287/isre.2022.0203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Companies are beginning to deploy highly realistic-looking digital human agents (DHAs) controlled by increasingly realistic artificial intelligence (AI) for online customer service tasks often performed by chatbots. We conducted four major experiments to examine users’ perceptions (trustworthiness, affinity, and willingness to work with) and behaviors while using DHA via a mixed-method approach with data from quantitative surveys, qualitative interviews, direct observations, and neurophysiological measurements. Four different DHAs were used in our experiments, which included commercial products from two different vendors (which proved to be immature) and two future-focused ones (where participants were successfully led to believe that the human-controlled digital human was controlled by AI). The first study compared user perceptions of a DHA, a chatbot, and a human agent from a written description and found few differences between the DHA and the chatbot. The second study compared perceptions after using a commercially available DHA and a chatbot. Most participants reported problems using a current production implementation of DHA, either finding it uncanny or robotic or having trouble conversing with it. The third and fourth studies used a plausible future-focused “Wizard of Oz” design by informing users that the DHA was controlled by AI when it was actually controlled by a human. Participants still preferred a human agent using video conferencing to the DHA, but after controlling for visual fidelity, we did not find evidence of differences between the human and the DHA. Current DHAs that have communication problems trigger greater affinity than chatbots but are otherwise similar to them. When the DHAs’ representation and communication ability match human ability, we failed to find differences between DHAs and human agents for simple customer service tasks. Our results also add to research on algorithm aversion and suggest that the anthropomorphic computer interfaces of DHA might alleviate algorithm aversion. History: Monideepa Tarafdar, Senior Editor; J. J. Po-An Hsieh, Associate Editor.},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0203},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1096-1128},
  shortjournal = {Inf. Syst. Res.},
  title        = {Less artificial, more intelligent: Understanding affinity, trustworthiness, and preference for digital humans},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Platform governance with algorithm-based content moderation: An empirical study on reddit. <em>ISRE</em>, <em>36</em>(2), 1078-1095. (<a href='https://doi.org/10.1287/isre.2021.0036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing volumes of participation in social media and online communities, content moderation has become an integral component of platform governance. Volunteer (human) moderators have thus far been the essential workforce for content moderation. Because volunteer-based content moderation faces challenges in achieving scalable, desirable, and sustainable moderation, many online platforms have recently started to adopt algorithm-based content moderation tools (bots). When bots are introduced into platform governance, it is unclear how volunteer moderators react in terms of their community-policing and -nurturing efforts. To understand the impacts of these increasingly popular bot moderators, we conduct an empirical study with data collected from 156 communities (subreddits) on Reddit. Based on a series of econometric analyses, we find that bots augment volunteer moderators by stimulating them to moderate a larger quantity of posts, and such effects are pronounced in larger communities. Specifically, volunteer moderators perform 20.9% more community policing, particularly over subjective rules. Moreover, in communities with larger sizes, volunteers also exert increased efforts in offering more explanations and suggestions after their community adopted bots. Notably, increases in activities are primarily driven by the increased need for nurturing efforts to accompany growth in subjective policing. Moreover, introducing bots to content moderation also improves the retention of volunteer moderators. Overall, we show that introducing algorithm-based content moderation into platform governance is beneficial for sustaining digital communities. History: Eric Zheng, Senior Editor; J.J. Hsieh, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0036 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0036},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1078-1095},
  shortjournal = {Inf. Syst. Res.},
  title        = {Platform governance with algorithm-based content moderation: An empirical study on reddit},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does david make a goliath? impact of rival’s expertise signals on online user engagement. <em>ISRE</em>, <em>36</em>(2), 1054-1077. (<a href='https://doi.org/10.1287/isre.2022.0282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How does information on a rival’s expertise influence the focal user’s engagement in online competitive settings? Online competitive settings differ fundamentally from other online contexts, such as e-commerce and social media platforms, explored in the prior work. This contextual distinction, we argue, is important as it determines the relationship between information about others and the focal user’s engagement. The relevance of this question relates to broader theoretical ambiguities concerning the effects of status and past performance as relative and absolute signals of expertise. Our findings are based on a field experiment in which we modified the multiround mobile two-player gaming app interface, randomly exposing players to rival’s status and past performance signals. We measure the focal user’s engagement regarding their decision to continue the game after each round. As a baseline, we find that the focal user is less likely to continue the game if the rival exhibits higher current performance. However, the rival’s status and past performance signals create strong contingencies wherein the principle effect of current performance is stronger if the rival has a high status or moderate past performance. Further, these contingent effects are partly predicated on the focal player’s motivation to compete. These findings offer several important implications for driving user engagement in online competitive settings and meaningfully advance our current understanding of the effects of status and past performance information on online engagement. History: Anandasivam Gopal, Senior Editor; Yili (Kevin) Hong, Associate Editor. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2022.0282 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0282},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1054-1077},
  shortjournal = {Inf. Syst. Res.},
  title        = {Does david make a goliath? impact of rival’s expertise signals on online user engagement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How to make my bug bounty cost-effective? a game-theoretical model. <em>ISRE</em>, <em>36</em>(2), 1031-1053. (<a href='https://doi.org/10.1287/isre.2021.0349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To mitigate the threats from malicious exploitation of vulnerabilities, an increasing number of organizations across different industries have started incorporating bug bounty programs (BBPs) in their vulnerability management cycles. Whereas a BBP attracts external security researchers to facilitate the discovery of vulnerabilities in organizations’ information technology systems, it also increases the risks after the vulnerabilities are discovered. To deal with the trade-offs, organizations need to understand how to design an optimal bounty and evaluate the total cost of a BBP depending on several key factors. The industry is motivated to understand how the bounty and total costs are impacted by (i) the characteristics of the organization (e.g., security posture and patching complexity), (ii) security researchers (e.g., the heterogeneity among security researchers and their number), and (iii) other factors such as the legal framework surrounding the BBP. However, because there is a lack of formal analyses regarding these issues, we use game-theoretical models to shed light on relevant questions and provide several useful results and managerial insights. First, although an organization’s patching complexity and the bounty act as substitutes, the relationship between security posture and the bounty is not necessarily substitutive or complementary. Furthermore, having a larger number of or more capable security researchers does not necessarily imply an increased bounty or lower total costs. Moreover, whereas the prevalent business belief is that an increased level of legal protection offered to the security researchers increases the cost of the BBP, we find that neither the cost of the BBP nor the offered bounty necessarily increases or decreases. This nuanced finding depends on different types of costs incurred because of the inherent vulnerability itself and costs related to possible leaks out of the BBP. Our study provides insights to security professionals, organizations, and policymakers in designing cost-effective BBPs. History: Ram Gopal, Senior Editor; Jianqing Chen, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0349 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0349},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1031-1053},
  shortjournal = {Inf. Syst. Res.},
  title        = {How to make my bug bounty cost-effective? a game-theoretical model},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does virtual reality help property sales? empirical evidence from a real estate platform. <em>ISRE</em>, <em>36</em>(2), 1011-1030. (<a href='https://doi.org/10.1287/isre.2021.9138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) has emerged as a transformative addition to real estate platforms, revolutionizing the presentation of property details. In contrast to traditional presentation technologies (e.g., pictures and videos), VR constructs an interactive three-dimensional (3D) environment for consumers to obtain property information. Prior literature mainly examined VR’s effects (e.g., vividness and interactivity) on selling low-involvement products such as books and apparel from a behavioral perspective. Real estate properties are typical high-involvement products, differing significantly in product value, importance, risk, and agent participation, necessitating comprehensive product information. We use a large-scale data set from a leading real estate platform to investigate VR’s influences on properties’ market outcomes (including the time-on-market and selling price) from an informative perspective. We find that VR is an “efficiency enhancer,” accelerating properties’ time-on-market rather than increasing properties’ selling price as a “market value influencer.” VR shows a greater acceleration effect for properties with higher product involvement, higher product quality, and lower agent service quality. This demonstrates VR’s ability in enhancing information richness to satisfy consumers’ information needs for a higher level of product involvement; establishing information credibility in discerning product quality; and serving as an alternative information source in lieu of low-quality agent services. We also disentangle the direct effect of VR Tour from the indirect effect of VR badge on consumers’ decision making to reveal VR’s effectiveness in product evaluation. This work contributes to the literature on nonimmersive VR and offers managerial implications, particularly in the context of property sales, high-involvement product marketing, and product presentation technologies in e-commerce. History: Giri Kumar Tayi, Senior Editor; Yili (Kevin) Hong, Associate Editor. Funding: Z. Yan was sponsored in part by the National Natural Science Foundation of China and Shanghai Pujiang Program [Grant 22PJC117]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2021.9138 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.9138},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {1011-1030},
  shortjournal = {Inf. Syst. Res.},
  title        = {Does virtual reality help property sales? empirical evidence from a real estate platform},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of situational achievement goals on online learning behavior: Results from field experiments. <em>ISRE</em>, <em>36</em>(2), 983-1010. (<a href='https://doi.org/10.1287/isre.2022.0353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their prevalence, online learning platforms have difficulty sustaining user motivation, resulting in low engagement and poor performance. Addressing this challenge, we study how these platforms can boost learner motivation by fostering an effective learning environment and inducing different situational goals. We conducted a randomized field experiment with behavioral interventions inspired by the achievement goal theory in a massive open online course with more than 2,000 learners from 171 countries. Using various econometric analyses, we estimate the effects of interventions based on three situational achievement goals: learning, performance-prove, and performance-avoidance. In contrast to the traditional (offline) education literature, which finds learning goals to be the most effective and performance goals to be inferior, we demonstrate that performance-prove goals are the most effective in enhancing online engagement and performance. We trace the roots of this finding to the differences in the structure of online and offline environments and the psychological needs of online learners. We uncover the underlying mechanism by empirically examining how situational achievement goals stimulate different achievement emotions necessary to maintain engagement in the learning process. We further find that the effectiveness of each goal depends on learners’ characteristics: prior performance and social activity. Learners with stronger prior performance benefit more from the performance-prove goal, whereas those with moderate performance levels gain from the performance-avoidance goal, and those with lower prior performance are positively influenced by the learning goal. Socially isolated learners respond best to performance goals. Through a second field experiment, we explore the optimal combination of situational goals. We find that combining learning and performance-prove goals leads to the highest learning outcomes. Our study offers theoretical contributions and practical implications for scholars and platform providers on how to effectively leverage situational achievement goals and related achievement emotions for improving online user outcomes. History: Karthik Kannan, Senior Editor; Mingfeng Lin, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0353 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0353},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {983-1010},
  shortjournal = {Inf. Syst. Res.},
  title        = {The impact of situational achievement goals on online learning behavior: Results from field experiments},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling the cost of free: How an ad-sponsored model affects serialized digital content creation. <em>ISRE</em>, <em>36</em>(2), 962-982. (<a href='https://doi.org/10.1287/isre.2023.0263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of a business model significantly impacts the success of digital content platforms, influencing both consumers and creators. Within the field of serialized digital content, establishing incentives that align with creators’ interests poses a notable challenge. Web novel platforms, which allow writers to produce and market their work online, have traditionally adopted a pay-per-view model, where readers pay for access to content. However, a recent trend has seen the introduction of an ad-sponsored model, aimed at attracting readers with free content. Although the ad-sponsored model expands reader accessibility, its effects on writers remain uncertain. Our study investigates the impacts of the ad-sponsored model on writers’ content creation efforts, leveraging a policy change within a leading web novel platform as a natural experiment. Our results indicate a general decrease in writers’ productivity under the ad-sponsored model. To uncover the underlying mechanism, we focus on the role of reader engagement and conduct a mediation analysis. Our results demonstrate that the ad-sponsored model is associated with less reader engagement, which leads to a subsequent decrease in writers’ content creation efforts. Further, we conduct a reader-level analysis and find a significant decline in engagement with writers among those who consume ad-sponsored content, providing additional evidence supporting our main findings. These results substantiate our hypothesis that diminished reader engagement stems from a perception of reduced “sunk costs” associated with free, ad-sponsored content compared with paid books. From a psychological standpoint, the sunk costs inherent in paid content motivate continued reader engagement and interaction with writers, whereas ad-sponsored reading removes such costs, thereby decreasing reader engagement. This study holds profound implications for digital content platforms, underscoring the necessity for platform managers to carefully evaluate the impacts of implementing the ad-sponsored model on both creators and consumers of serialized digital content. History: Param Singh, Senior Editor; Gordon Burtch, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2023.0263 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2023.0263},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {962-982},
  shortjournal = {Inf. Syst. Res.},
  title        = {Unveiling the cost of free: How an ad-sponsored model affects serialized digital content creation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond risk: A measure of distribution uncertainty. <em>ISRE</em>, <em>36</em>(2), 944-961. (<a href='https://doi.org/10.1287/isre.2022.0089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty, particularly distribution uncertainty (a.k.a. ambiguity), holds significant relevance in both academic research and practical applications. Much of the existing research, however, has concentrated primarily on addressing outcome uncertainty (or risk), frequently neglecting the aspect of distribution uncertainty. This research delves into distribution uncertainty, a critical yet often overlooked aspect of empirical research. We argue that there is a pressing need to integrate considerations of ambiguity directly into the development and implementation of data analytics models, calling for the promotion and wider use of a well-defined measure of ambiguity. We introduce a quantitative measure of ambiguity that surpasses conventional approaches by precisely capturing distribution uncertainty. We illustrate the properties and advantages of this measure, highlighting its ability to enhance empirical models, yield more reliable parameter estimates, and contribute to the decision-making process. Using decision making in the financial market as an example, we demonstrate the value of this ambiguity measure. This paper promotes a more nuanced understanding of uncertainty and offers implications for both research methodologies and practical risk management. History: Yong Tan, Senior Editor; Chad Ho, Associate Editor. Funding: This work was supported by the University Grants Committee Research Grants Council [Grants GRF 14500521, 165052947, 14501320, 14503818, and TRS:T31-604/18-N], the National Natural Science Foundation of China [Grants 72121001, 72301125], the Tsinghua University Initiative Scientific Research Program [Grant 2021THZWJC28], and the Shenzhen Stable Support Plan Program for Higher Education Institutions [Grant 20220815111555004]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0089 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0089},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {944-961},
  shortjournal = {Inf. Syst. Res.},
  title        = {Beyond risk: A measure of distribution uncertainty},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics of shared security in the cloud. <em>ISRE</em>, <em>36</em>(2), 916-943. (<a href='https://doi.org/10.1287/isre.2023.0256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud services exist under a shared security environment with a dynamic nature; users trade fixed costs for variable costs over time, and both cloud services providers (CSPs) and users contribute to overall security. We investigate the nature of shared security in a dynamic game where users’ security contributions and cloud usage figure into their CSP’s vulnerability. Furthermore, CSPs’ own security contribution takes into account both their users as well as competition with other CSPs. The Markov perfect equilibrium reveals the long-term time patterns of security of the cloud. In particular, we identify a novel form of time-path strategic complementary between usage and a CSP’s Markov state of security. This implies that cloud security is an unusual form of impure public good, whereby individual contributions bolstering a CSP’s security endow a selective incentive (private benefit) on others rather than on the contributor alone. Because this increases usage, CSP vulnerability increases over time. At the same time, CSP competition on security may lead to both welfare improvements for users and lock-in. History: Martin Bichler, Senior Editor; Yifan Dou, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2023.0256 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2023.0256},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {916-943},
  shortjournal = {Inf. Syst. Res.},
  title        = {Dynamics of shared security in the cloud},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stress from digital work: Toward a unified view of digital hindrance stressors. <em>ISRE</em>, <em>36</em>(2), 896-915. (<a href='https://doi.org/10.1287/isre.2022.0691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many models with various sets of hindrance technostressors. Researchers and practitioners face the challenge of selecting a model or mixing several models without guidance on their relative advantages and suitability for contemporary digital work. None of the existing models captures the full conceptual breadth of hindrance technostress, and the existing models typically offer suboptimal power to explain the negative psychological responses or outcomes of technostressors. We synthesize the fragmented works on hindrance technostressors and propose a unified hierarchical model of digital hindrance stressors. We provide an extensive and parsimonious measurement model with high predictive power. This work builds on technostress and occupational stress theory using a quantitative-dominant mixed-methods study. The empirical part of the study includes a qualitative prestudy and multiple surveys with more than 5,800 participants. The data support the modeling, validation, and benchmarking of the new models we introduce. We discuss the relative advantages of the models for research and practice and guide their selection. History: Jason Bennett Thatcher, Senior Editor; David (Jingjun) Xu, Associate Editor. Funding: This study was supported by the German Federal Ministry of Education and Research [Grant 02L16D030]. Open Access funding was enabled by the German Federal Ministry of Education and Research and the Ministry of Science, Research and the Arts (MWK) of Baden-Württemberg [Grant 16DHBKI002]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0691 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0691},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {896-915},
  shortjournal = {Inf. Syst. Res.},
  title        = {Stress from digital work: Toward a unified view of digital hindrance stressors},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HyperCARS: Using hyperbolic embeddings for generating hierarchical contextual situations in context-aware recommender systems. <em>ISRE</em>, <em>36</em>(2), 871-895. (<a href='https://doi.org/10.1287/isre.2022.0202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual situations, such as having dinner at a restaurant on Friday with the spouse, became a useful mechanism to represent context in context-aware recommender systems (CARS). Prior research has shown important advantages of using latent embedding representation approaches to model contextual information in the Euclidean space leading to better recommendations. However, these traditional approaches have major challenges with construction of proper embeddings of hierarchical structures of contextual information, as well as with interpretations of the obtained representations that would be useful for managers. To address these problems, we propose the HyperCARS method that models hierarchical contextual situations in the latent hyperbolic space. HyperCARS combines hyperbolic embeddings with hierarchical clustering to construct contextual situations, which allows to loosely couple the contextual modeling component with recommendation algorithms and therefore provides flexibility to use a broad range of previously developed recommendation algorithms. We demonstrate empirically that the proposed hyperbolic embedding approach better captures the hierarchical nature of context than its Euclidean counterpart and produces hierarchical contextual situations that are more distinct and better separated at multiple hierarchical levels. We also demonstrate that hyperbolic contextual situations lead to better context-aware recommendations in terms of standard recommendation metrics and to better interpretability of the resulting hierarchical contextual situations. Because hyperbolic embeddings can also be used in many other applications besides CARS, in this paper, we propose the latent embeddings representation framework that systematically classifies prior work on embeddings and identifies novel research streams for hyperbolic embeddings across information systems applications. History: Ahmed Abbasi, Senior Editor; Huimin Zhao, Associate Editor. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2022.0202 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0202},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {871-895},
  shortjournal = {Inf. Syst. Res.},
  title        = {HyperCARS: Using hyperbolic embeddings for generating hierarchical contextual situations in context-aware recommender systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clocking in or not? optimal design of a novel gamified business model in online learning. <em>ISRE</em>, <em>36</em>(2), 847-870. (<a href='https://doi.org/10.1287/isre.2021.0138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clocking-in cash-back ( CIC ), an emerging gamified business model in online learning, has recently garnered significant attention. CIC allows users to secure a full refund of the course fee through consecutive completion of specific tasks within a required time window. These tasks, known as clocking in , encompass activities such as daily assignments and sharing progress updates on social media. By employing this gamification system, the firm effectively monitors user efforts, categorizing them as winners or quitters based on clocking-in completion. Despite its growing popularity, this new business model has yet to be systematically analyzed in the literature. This paper fills this critical gap by examining how an online learning firm should set the optimal time window for its course and how the time window is affected by context-specific factors. We identify two opposing effects associated with extending the time window on users’ quitting time: the psychological-disutility increasing effect (negative) and the effort-cost decreasing effect (positive). Our results indicate that as quitters’ positive word-of-mouth (WOM) effects increase, there are cases in which the firm should opt for shortening the time window, primarily because of the psychological-disutility increasing effect outweighing the effort-cost decreasing effect. Furthermore, we find that it is not always beneficial for the firm to extend the time window when there is an increased presence of high-ability users. Additionally, we find that as the marginal content creation cost rises, instead of reducing the difficulty level of each task, the firm may find it more advantageous to raise the difficulty level by shortening the time window. Our findings provide valuable insights that online learning firms can utilize to enhance their design of the CIC mechanism. History: Anandasivam Gopal, Senior Editor; Yan Huang, Associate Editor. Funding: D. Liu’s research was supported in part by the National Natural Science Foundation of China [Grant NSFC-72071118] and WU Jiapei Award for Information Economics in 2019 [Grant M19100295]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0138 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0138},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {847-870},
  shortjournal = {Inf. Syst. Res.},
  title        = {Clocking in or not? optimal design of a novel gamified business model in online learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile apps, trading behaviors, and portfolio performance: Evidence from a quasi-experiment in china. <em>ISRE</em>, <em>36</em>(2), 828-846. (<a href='https://doi.org/10.1287/isre.2020.0616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile apps are among the most important and widely used financial technology (fintech) innovations in the brokerage industry. Surprisingly, despite their increasing economic importance and theoretical significance, few studies examine the effects of mobile app use on individual investors’ financial decisions and performance. This study seeks to understand how mobile apps influence investors’ trading behaviors and portfolio performance by using a proprietary longitudinal data set from December 2012 to November 2015 from a large securities company in China with a quasi-experimental setting to answer our research questions. We leverage the introduction of an app to identify the effect of mobile app adoption by using a sample of 20,665 investors. We use the generalized synthetic control method and find that mobile app adoption does not affect investors’ portfolio performance when one examines aggregate impacts using a binary indicator of mobile app use. Our analyses of the mechanisms indicate that adopting mobile apps results in a noticeable decrease in time constraints, a proxy for transaction friction, and a modest increase intrend-chasing bias, reflecting tendencies toward myopic decision making. Because the reduction in time constraints can benefit investors’ performance, the increase in trend-chasing can be detrimental to investors’ performance; our findings explain why mobile app adoption has no overall effect on portfolio performance. Further analyses of adopters’ postadoption behaviors provide interesting insights and show that the mobile app usage intensity has an inverted U–shaped relationship with portfolio performance. The results are robust to using different samples or excluding high market volatility periods and by using a variety of methods, such as propensity score matching, dynamic matching, stacked difference in differences, or an instrumental variable approach. We discuss the implications for research and practice. History: Eric Zheng, Senior Editor; Hailiang Chen, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2020.0616 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2020.0616},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {828-846},
  shortjournal = {Inf. Syst. Res.},
  title        = {Mobile apps, trading behaviors, and portfolio performance: Evidence from a quasi-experiment in china},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating platform-led affiliate marketing: Implications for content creation and platform profitability. <em>ISRE</em>, <em>36</em>(2), 802-827. (<a href='https://doi.org/10.1287/isre.2022.0620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, some user-generated content (UGC) platforms have introduced shopping features to generate additional commission-based revenue from in-app transactions. These platforms allow creators to produce shoppable content, where promoted products are tagged with affiliate links, and in return, they earn a percentage of the commission the creators receive from merchants (such a new business model is referred to as platform-led affiliate marketing ). Despite its growing popularity, the impact of this business model on the key stakeholders (i.e., UGC platforms, content creators, and content consumers) has yet to be systematically analyzed in the existing literature. This paper aims to address this significant literature gap by employing a game-theoretic model. Our findings demonstrate that platform-led affiliate marketing can create a win-win situation for UGC platforms, creators participating in affiliate marketing, and content consumers. Furthermore, we find that even creators not participating in affiliate marketing can sometimes benefit indirectly from other creators’ participation. On the other hand, surprisingly, we find that UGC platforms may not necessarily benefit from adopting this emerging business model because of potential losses in traffic revenue. These results carry significant managerial implications for UGC platforms regarding whether and how to leverage affiliate marketing. Moreover, our findings suggest that creators may benefit from reducing their production efforts in the face of intensified competition. More importantly, optimal production decisions for creators can vary significantly, depending on specific factors (e.g., an increase in the number of creators or content substitutability) contributing to heightened competition. These findings offer valuable insights that can guide creators in refining their production strategies. History: Karthik Kannan, Senior Editor; Martin Bichler, Associate Editor. Funding: D. Liu's research was supported in part by the National Natural Science Foundation of China [Grant NSFC-72071118] and WU Jiapei Award for Information Economics in 2019 [Grant M19100295]. S. Kumar thanks Temple Center for International Business Education and Research for partially supporting this research. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0620 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0620},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {802-827},
  shortjournal = {Inf. Syst. Res.},
  title        = {Navigating platform-led affiliate marketing: Implications for content creation and platform profitability},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time to stop? an empirical investigation on the consequences of canceling monetary incentives on a digital platform. <em>ISRE</em>, <em>36</em>(2), 781-801. (<a href='https://doi.org/10.1287/isre.2022.0017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital platforms commonly use monetary incentives to motivate users to perform specific tasks. Extant studies have shown the effects of introducing such monetary rewards on the outcomes of interest (e.g., participation and performance) on public platforms. However, little is known about the impact of canceling rewards (i.e., whether it simply reverses the effect of their introduction), and particularly less attention is paid to corporate platforms. Our study examines the impact of canceling monetary incentives using quasi-natural experiments on a corporate platform. Similar to prior studies focusing on public platforms, we find that introducing quantity-based monetary incentives increases participation (contribution quantity), but has no significant effect on performance (contribution quality). Yet, in contrast, our main empirical analysis reveals that canceling monetary incentives is not simply the reverse process of their introduction. In particular, compared with the increase in participation when monetary rewards were initially introduced, the cancellation of these rewards leads to a sharper decrease in participation. This suggests that canceling rewards has a net negative impact on participation. In addition, canceling monetary rewards also causes a significant decline in performance, which indicates that the effects of canceling and introducing rewards on performance are not simply the opposite of each other. Furthermore, we examine the heterogeneous responses of individuals with different self-motivation types and working competency levels to monetary incentives, highlighting the “asymmetry” between canceling and introducing incentives. We also discuss the similarities and differences between corporate and public platforms regarding the impact of monetary incentives. Our results provide important practical implications for enterprise information systems and general information systems regarding their design of platform strategies. History: Yong Tan, Senior Editor; Wenjing Duan, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0017 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0017},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {781-801},
  shortjournal = {Inf. Syst. Res.},
  title        = {Time to stop? an empirical investigation on the consequences of canceling monetary incentives on a digital platform},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning personalized privacy preference from public data. <em>ISRE</em>, <em>36</em>(2), 761-780. (<a href='https://doi.org/10.1287/isre.2023.0318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning consumers’ personalized privacy preferences is crucial for firms and policymakers to establish trust and compliance and guide effective policymaking. Existing approaches rely mostly on private information such as proprietary user behavior data and individual-level demographic and socio-economic factors, or require explicit user input, which can be invasive and burdensome, potentially leading to user dissatisfaction. Nowadays, individuals generate and share vast amounts of information about themselves in the public domain, which can provide a valuable multifaceted view of their behaviors, attitudes, and preferences. This information thus has the potential to provide valuable insights into individuals’ privacy preferences. In this study, we propose a novel framework to predict personalized privacy preference by leveraging a ubiquitous source of public data—social media posts. Deeply rooted in psychological and privacy theories, we use deep learning model and natural language processing algorithms to learn theory-driven psychosocial traits such as lifestyle, risk preference, personality, privacy-related economic preferences, linguistic styles, and more from social media posts. Interestingly, we find that psychosocial traits from public data provide greater predictive power than private information. Furthermore, we conduct multiple interpretability analyses to understand what drives the model’s performance. Finally, we demonstrate the practical value of our model and show that our framework can assist platforms and policymakers in forecasting the consequences of privacy policies. Overall, our framework provides managerial implications for enhancing consumer privacy control and trust, optimizing platform data management, and informing policymakers about better data privacy regulations. History: Ravi Bapna, Senior Editor; Heng Xu, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2023.0318 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2023.0318},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {761-780},
  shortjournal = {Inf. Syst. Res.},
  title        = {Learning personalized privacy preference from public data},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monitoring and home bias in global hiring: Evidence from an online labor platform. <em>ISRE</em>, <em>36</em>(2), 736-760. (<a href='https://doi.org/10.1287/isre.2021.0526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of remote work has accelerated the adoption of monitoring systems to keep track of worker behavior, especially on online labor platforms. In contrast to the existing literature that predominantly focuses on the effect of monitoring on productivity, this study investigates the impact of monitoring from the perspective of contractual governance. In principle, by enabling the detailed real-time observation of worker progress, the deployment of monitoring systems has the potential to improve contractual control and coordination, thereby reducing employers’ preferences for domestic workers (home bias). Leveraging the exogenous introduction of a monitoring system for time-based projects on a leading online labor platform, we employ a difference-in-differences model to estimate the impact of monitoring systems in reducing home bias. Our findings reveal that following the monitoring system’s introduction, the bias against foreign workers becomes substantially weaker and statistically insignificant, highlighting the overlooked role of monitoring systems in fostering a more level playing field for global workers. Our further analysis indicates that monitoring leads to a notable 15% increase in the hiring of foreign workers. Moreover, the decrease in home bias is more pronounced in high-routine projects or when employers lack prior positive experiences with foreign workers, two scenarios characterized by low external uncertainty and high internal uncertainty, respectively. Additionally, employers no longer exhibit a stronger home bias when workers have higher ratings, where the expected moral hazard risk is lower, nor when workers reside in the same time zone, where expected coordination costs are lower. These findings lend support to the effectiveness of monitoring systems in mitigating employers’ home bias through enhancing contractual control and coordination. Our findings provide important managerial implications for the design of online labor platforms. History: Deepa Mani, Senior Editor; Tianshu Sun, Associate Editor. Funding: This work was supported by an NET Institute Grant and the Robert Wood Johnson Foundation (RWJF) [Grant 74503]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2021.0526 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2021.0526},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {736-760},
  shortjournal = {Inf. Syst. Res.},
  title        = {Monitoring and home bias in global hiring: Evidence from an online labor platform},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deeper down the rabbit hole: How technology conspiracy beliefs emerge and foster a conspiracy mindset. <em>ISRE</em>, <em>36</em>(2), 709-735. (<a href='https://doi.org/10.1287/isre.2022.0494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conspiracy theories, which allege that powerful groups hatch malicious plots, are increasingly recognized as a threat to contemporary society. Although research acknowledges the role of information technology (IT) in spreading such theories, the understanding of conspiracy beliefs related to technology, their formation, and their effects remains limited. Building on theoretical insights on conspiracy beliefs and information systems (IS) research dealing with the impact of individuals’ perceptions of technology, we theorize on technology conspiracy beliefs. We define technology conspiracy beliefs as an individual’s endorsement of an unverified narrative that purports that an organization issuing technology is using that technology to secretly pursue evil goals. We then develop the TECONMIND (technology-conspiracy-mindset) model, which suggests that an individual’s perceptions of a technology’s characteristics (performance expectancy, effort expectancy, and perceived risks) and its issuer’s characteristics (perceived malevolence and perceived power) can lead to the formation of technology conspiracy beliefs. Moreover, the model proposes a reciprocal relationship between technology conspiracy beliefs and a broader conspiracy mindset. Accordingly, we expect technology conspiracy beliefs to foster a conspiracy mindset, which in turn, promotes further technology conspiracy beliefs. We provide empirical evidence for the prevalence of technology conspiracy beliefs and then test this model in two studies—a multiwave field study and an experiment—that support our central propositions. Our study contributes to IS research in three ways. First, by developing a conceptual understanding of technology conspiracy beliefs, we introduce an IT artifact-centered concept that allows researchers to explore a previously overlooked dark side of technology. Second, we establish a reciprocal relationship between technology conspiracy beliefs and conspiracy mindsets, indicating that the endorsement of technology conspiracy beliefs can set in motion a vicious cycle in which individuals increasingly interpret their environment using conspiracy theories. Third, we provide an initial understanding of which perceived technology and issuer characteristics make technologies prone to become objects of conspiracy beliefs. Our findings should sensitize technology developers and policymakers as to how their decisions can instigate or mitigate technology conspiracy beliefs, which have significant long-term societal consequences. History: Jason Thatcher, Senior Editor; David Xu, Associate Editor. Funding: This work was supported by the Volkswagen Foundation [Participatory surveillance through tracing apps; Grant 99796]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0494 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0494},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {709-735},
  shortjournal = {Inf. Syst. Res.},
  title        = {Deeper down the rabbit hole: How technology conspiracy beliefs emerge and foster a conspiracy mindset},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of geographic and social proximity on physicians: Evidence from the adoption of an online health community. <em>ISRE</em>, <em>36</em>(2), 690-708. (<a href='https://doi.org/10.1287/isre.2020.0663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although online doctor consultations are rapidly gaining popularity, physicians must actively participate in physician–patient interaction platforms to fully unlock their potential. Through a social influence lens, this study empirically investigates physician adoption behavior over time across regions in the diffusion of an online health community (OHC). We examined the impacts of geographically and socially close adopters and investigated the interaction of proximity influences and competition in adoption. We collected panel data on 21,654 physicians in 32 cities in three provinces in China from a large Chinese OHC. The results demonstrate that both geographic and social proximity facilitate adoption when local competition is low. However, as local competition increases, the impact of socially close prior adopters increases, whereas that of geographically close prior adopters decreases. This pattern becomes stronger for physicians with lower titles. History: Eric Zheng, Senior Editor; Huigang Liang, Associate Editor. Funding: J. Luo appreciates support from the National Natural Science Foundation of China [Grants 72293580, 72293585, 71832008, 72221001, 72231003]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2020.0663 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2020.0663},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {690-708},
  shortjournal = {Inf. Syst. Res.},
  title        = {The impact of geographic and social proximity on physicians: Evidence from the adoption of an online health community},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of the general data protection regulation on the global mobile app market: Digital trade implications of data protection and privacy regulations. <em>ISRE</em>, <em>36</em>(2), 669-689. (<a href='https://doi.org/10.1287/isre.2022.0421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although regional data protection and privacy regimes are often cited as major barriers to crossborder digital trade, mitigating consumer privacy concerns through regulations can potentially increase the demand for foreign digital products or services. This study presents empirical evidence on this issue by examining the impact of the General Data Protection Regulation (GDPR) on the global mobile app market. We construct a comprehensive data set of apps distributed by Apple’s App Store over the 26-month period covering the enactment of the GDPR and employ econometric models to analyze the regulation’s effects on app trade between country pairs. Contrary to assertions that regional data protection and privacy regulations impede digital trade and aggravate fragmentation, the empirical results demonstrate a significant increase in top-performing foreign apps compared with native ones in the European Union countries post-GDPR. We further conduct a series of analyses to explore the underlying mechanisms potentially driving these effects from both the supply and demand sides. Our findings lend support to the demand-side mechanism, whereby the GDPR helps alleviate consumer privacy concerns and provides reassurance in adopting foreign digital products. History: Eric Zheng, Senior Editor; Idris Adjerid, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.0421 .},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2022.0421},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {669-689},
  shortjournal = {Inf. Syst. Res.},
  title        = {Impact of the general data protection regulation on the global mobile app market: Digital trade implications of data protection and privacy regulations},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing next-generation multimethod research in information systems: A framework and some recommendations for authors and evaluators. <em>ISRE</em>, <em>36</em>(2), 647-668. (<a href='https://doi.org/10.1287/isre.2025.editorial.v36.n2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity of sociotechnical phenomena, proliferation of diverse data sources, expanding repertoire of research methods, and broadening multiparadigmatic awareness and competence have spurred a growing interest in multimethod research in the information systems (IS) field. This editorial recognizes and celebrates the value of integrating diverse methods and offers a framework to classify multimethod research using two dimensions: (a) methodological distance—the degree of difference (proximate or distant) between methods employed in terms of characteristics such as paradigmatic assumptions, techniques, and goals; and (b) nature of integration—the extent to which the methods are combined in loosely coupled (interlayered) or tightly coupled (intertwined) ways. These dimensions yield four types of multimethod research: assembly (proximate methods with interlayered integration), blend (proximate methods with intertwined integration), bridge (distant methods with interlayered integration), and fusion (distant methods with intertwined integration). We illustrate each archetype with studies published in leading IS journals. Building on these examples, we provide actionable guidance for authors on conducting and presenting multimethod research and also offer recommendations for evaluators of multimethod work. More broadly, we call on the IS community to embrace multimethod research not as an ad hoc stack of methods, but as a systematic strategy, aligning with these recommendations related to methodological distance and nature of integration, to produce a credible, revelatory, and rich body of knowledge on multifaceted IS phenomena.},
  archive      = {J_ISRE},
  doi          = {10.1287/isre.2025.editorial.v36.n2},
  journal      = {Information Systems Research},
  month        = {6},
  number       = {2},
  pages        = {647-668},
  shortjournal = {Inf. Syst. Res.},
  title        = {Advancing next-generation multimethod research in information systems: A framework and some recommendations for authors and evaluators},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ited">ITED - 10</h2>
<ul>
<li><details>
<summary>
(2025). Game—The freight transportation game: Operational challenges for carriers in online spot market platforms. <em>ITED</em>, <em>26</em>(1), 91-101. (<a href='https://doi.org/10.1287/ited.2024.0091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the freight transport spot market, carriers face several challenges, including handling small-sized loads, managing short lead times, and dealing with uncertainties that can disrupt their existing networks and capacity. In this context, auction platforms assist carriers in developing strategies to maintain productivity and service levels. This paper introduces the freight transportation game, an online auction-based platform specifically designed for the freight spot market. The game equips players, acting as carriers, with tools to develop transportation strategies, optimize routes, and propose competitive rates to maximize profits. Players engage in decision making and competition, with key performance indicators evaluating their decisions. The game includes two scenarios; the first helps students understand the operational challenges that carriers face in spot market auctions, whereas the second encourages cooperation to enhance performance and overcome obstacles. Designed for a three-hour session, the game can be integrated into courses on supply chain management or transportation systems. The practical learnings for students from the game include understanding the complexity of constructing transport plans, recognizing the challenges of using online auction-based platforms, and experiencing the difficulties of managing spot market requests despite their benefits. Funding: This work was supported by the Physical Internet Chair at Mines Paris, PSL University funded through an “Appel à Manifestation d’Intérêt” titled “Skills and Jobs of the Future” launched by la Caisse des Dépôts. Supplemental Material: Supplemental materials are available at https://doi.org/10.1287/ited.2024.0091 .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2024.0091},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {91-101},
  shortjournal = {Inf. Syst. Res.},
  title        = {Game—The freight transportation game: Operational challenges for carriers in online spot market platforms},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case—Potty parity: Stadium restroom design. <em>ITED</em>, <em>26</em>(1), 87-90. (<a href='https://doi.org/10.1287/ited.2023.0051cs'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on Diversity, Equity and Inclusion in OR/MS Classrooms.},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0051cs},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {87-90},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case—Potty parity: Stadium restroom design},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case Article—Potty parity: Stadium restroom design. <em>ITED</em>, <em>26</em>(1), 79-86. (<a href='https://doi.org/10.1287/ited.2023.0051ca'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the long wait times for women and the lack of accessibility for LGBTQ+ individuals when they use restrooms, this case provides a set of analytical tools to evaluate wait time disparity among users for different restroom configurations. A stadium manager who faces complaints about excessive restroom wait times aims to retrofit the restroom layout to improve both efficiency, measured in terms of wait time, and fairness, measured in terms of totalitarian and Rawlsian scores. Given that customers have diverse preferences over the use of restroom types, in three modules, students learn to (i) evaluate queuing parameters for a mix of heterogeneous populations, (ii) evaluate queuing metrics for various restroom layouts and discuss their wait time disparities, and (iii) evaluate and discuss the fairness of access to restroom facilities from a diversity, equity, and inclusion (DEI) perspective. By completing this case, students gain an understanding of service systems, learn about process flexibility concepts, and become familiar with DEI concepts and measures. The primary objectives of the case for students are to understand the trade-offs between efficiency and fairness, develop an understanding of multiobjective problems, and improve their skills in employing queuing concepts and tools. History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on Diversity, Equity and Inclusion in OR/MS Classrooms. Supplemental Material: The Teaching Note and Excel files are available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0051ca},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {79-86},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case Article—Potty parity: Stadium restroom design},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case—Optimizing food donation delivery for nonprofit company Logica&Co. <em>ITED</em>, <em>26</em>(1), 73-78. (<a href='https://doi.org/10.1287/ited.2023.0042cs'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case addresses a real-world challenge encountered by the nonprofit organization Logica&Co. It revolves around optimizing the logistics involved in collecting food donations from local businesses and delivering them to soup kitchens, utilizing a fleet of bike riders. The focus is identifying efficient strategies to minimize transportation and warehouse costs while maximizing the impact of the donations and private sponsor monetary contributions. The study includes tasks such as determining optimal bike and e-bike warehouse locations and managing the allocation of resources among riders and soup kitchen volunteers.},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0042cs},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {73-78},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case—Optimizing food donation delivery for nonprofit company Logica&Co},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case Article—Optimizing food donation delivery for the nonprofit company Logica&Co. <em>ITED</em>, <em>26</em>(1), 63-72. (<a href='https://doi.org/10.1287/ited.2023.0042ca'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents a case exercise that teaches students how to apply mathematical programming to a real-life context. The case deals with the management of the food donation supply chain. The case, using a project-based approach, proposes a realistic scenario that simulates the consulting interaction with a nonprofit company, Logica&Co, which acts as a two-sided platform connecting supply and demand. The objective is to define an effective strategy to collect and deliver food donations, using either bike or e-bike, from local businesses to soup kitchens, covering a semester-long timeframe. The form of the problem exhibits nonlinear characteristics, but the design allows for adjustable difficulty levels. Students can assess their performance during the class period thanks to an interactive offline tool, the SoS simulator , which is publicly available for download and can be customized by instructors. The case was proposed as a competitive group challenge for students of the bachelor’s or master’s program in management engineering at Sapienza University of Rome. However, given the embedded characteristics of flexibility, it can be easily adjusted for heterogeneous curricula of undergraduate- and graduate-level courses in engineering programs (this opportunity is extensively discussed in the Case Article and the Teaching Note). The students appreciated both the teaching methodology and the teamwork aspects and highlighted the utility of the SoS simulator tool. Supplemental Material: The Teaching Note and its supplemental material are available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0042ca},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {63-72},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case Article—Optimizing food donation delivery for the nonprofit company Logica&Co},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Infrastructure decision preferences and the influence of social justice education. <em>ITED</em>, <em>26</em>(1), 48-62. (<a href='https://doi.org/10.1287/ited.2023.0052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social justice considerations are critical in today’s engineering education. As an example, access to electricity will be crucial to the development of sub-Saharan Africa (SSA) by powering essential sectors and services that would yield economic growth and an improved quality of life. Because infrastructure investments are often allocated to urban areas, it is important for stakeholders to equally prioritize rural electrification in their efforts to achieve universal electrification. These stakeholders make decisions on allocating infrastructure investment based on their underlying values and preferences. It is therefore important to introduce and integrate ideas of fairness and social justice into the engineering curriculum. Our paper investigates the influence of social justice education on decision preferences among students tasked with planning electricity systems. We engaged graduate and undergraduate students in a discrete choice experiment to determine their preferences for equality. In our study, we find that an interactive social justice education module can be effective in increasing equality preferences among non–U.S.-citizen students by 22%. Among students with relatively low preferences for equality, we find that the education module was effective in increasing preference by 18% on average and up to 191%, which translated into more equitable resource allocation. As such, we show that an education intervention for stakeholders may be effective in improving equitable electrification in SSA. The preference elicitation and education module presented in this paper could be repurposed by instructors to include other contexts in their course content (e.g., money, water, housing, etc.) or to people wanting to educate decision makers about social justice. History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on Diversity, Equity and Inclusion in OR/MS Classrooms. Funding: This work was supported by the National Science Foundation [Grant 2121730].},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0052},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {48-62},
  shortjournal = {Inf. Syst. Res.},
  title        = {Infrastructure decision preferences and the influence of social justice education},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An agile approach to student consulting projects: Iteration and communication to improve decision making, presentations, and teamwork. <em>ITED</em>, <em>26</em>(1), 34-47. (<a href='https://doi.org/10.1287/ited.2023.0057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding consulting projects into the curriculum presents an effective means of providing students with experiential applied learning opportunities. However, creating, planning, and managing such projects can be challenging. This paper introduces a unique approach to managing consulting projects: Agile Project Management with Scrum. By incorporating a commitment to iteration and communication as the core of the project experience, Agile with Scrum fosters an impactful, realistic, and engaging student consulting experience. This approach enhances decision making, presentations, and team dynamics. This article discusses how one supply chain management course embedded Agile with Scrum into a client consulting project to convert a mediocre experiential learning opportunity into a transformative one. After describing Agile with Scrum and explaining its potential in the classroom, this paper discusses the consulting project before and after Agile; the results; the lessons learned; and the value created for students, clients, and faculty. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ited.2023.0057 .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0057},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {34-47},
  shortjournal = {Inf. Syst. Res.},
  title        = {An agile approach to student consulting projects: Iteration and communication to improve decision making, presentations, and teamwork},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An operations Research–Based teaching unit for grade 12: The ROAR experience, part III. <em>ITED</em>, <em>26</em>(1), 12-33. (<a href='https://doi.org/10.1287/ited.2023.0065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we finish describing the project and the experimentation of Ricerca Operativa Applicazioni Reali (ROAR; in English, Real Applications of Operations Research), a three-year project for higher secondary schools. ROAR is composed of three teaching units addressed to grades 10, 11, and 12, respectively. To improve students’ interest, motivation, and skills related to science, technology, engineering, and mathematics disciplines, ROAR integrates the teaching of mathematics and computer science through operations research. Its implementation started in 2021 in a grade 10 class at the scientific high school IIS Antonietti in Iseo (Brescia, Italy). We provided the details of the first two units in previous papers. Here, we focus on the third and last unit, carried out from October 2022 to January 2023, with the same students, then in a grade 12 class. Similarly to the first two units, we describe objectives, prerequisites, topics and methods, the organization of the lectures, digital technologies used, and a challenging final project that, this time, involved the manufacturer company Filtrec S.p.A. with a case. After analyzing the feedback from students, teachers, and practitioners engaged in the experimentation, we reflect on the entire experimentation and provide some insights to replicate a similar experience. Funding: G. Colajanni was partially supported by the research project Programma Ricerca di Ateneo UNICT 2020-22 linea 2, Ottimizzazione di Modelli di Network slIce 5G con UAV, the Italian Ministry of University and Research and the European Union for the Programma Operativo Nazionale project on Research and Innovation 2014-2020, D.M. 1062/2021, and the GNAMPA-INdAM Group. A. Raffaele was partially supported by the National Group for Scientific Computation. E. Taranto was partially supported by the National Group for Algebraic and Geometric Structures, and their Applications. Supplemental Material: The online appendices are available at https://doi.org/10.1287/ited.2023.0065 .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0065},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {12-33},
  shortjournal = {Inf. Syst. Res.},
  title        = {An operations Research–Based teaching unit for grade 12: The ROAR experience, part III},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case—Optimizing transportation and equity in a district zoning problem for ORville public schools. <em>ITED</em>, <em>26</em>(1), 8-11. (<a href='https://doi.org/10.1287/ited.2023.0046cs'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on Diversity, Equity and Inclusion in OR/MS Classrooms.},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0046cs},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {8-11},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case—Optimizing transportation and equity in a district zoning problem for ORville public schools},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case Article—Optimizing transportation and equity in a district zoning problem for ORville public schools. <em>ITED</em>, <em>26</em>(1), 1-7. (<a href='https://doi.org/10.1287/ited.2023.0046ca'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This case asks students to solve a multiobjective optimization decision problem applied to school district zoning. In the case, residential areas in a school district must be assigned to middle schools having capacity limits. The operational considerations on the decision are the weighted distances traveled between students’ residences and their assigned middle school campuses. The ethical considerations pertain to equity in education and are measured using the numbers of economically disadvantaged students assigned to each middle school. This problem, including its two objectives, is motivated by a real zoning problem faced by a school district. Publicly available data from the school district and its rezoning efforts are used to populate the case. To complete the case, students model and solve an optimization problem both with and without the ethical considerations included. In doing so, they observe how ethical considerations can change the outputs generated by the optimization model. Evidence is provided from student surveys to indicate how completion of the case impacts perceptions of the importance of and understanding of how to include ethical considerations in optimization modeling. Results indicate statistically significant increases in those perceptions. History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on Diversity, Equity and Inclusion in OR/MS Classrooms. Supplemental Material: The Teaching Note and DataAndMetricsDemo Excel workbook are available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0046ca},
  journal      = {INFORMS Transactions on Education},
  month        = {9},
  number       = {1},
  pages        = {1-7},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case Article—Optimizing transportation and equity in a district zoning problem for ORville public schools},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="mksc">MKSC - 14</h2>
<ul>
<li><details>
<summary>
(2025). Focus on authors. <em>MKSC</em>, <em>44</em>(4), 970-973. (<a href='https://doi.org/10.1287/mksc.2025.focusonaus.v44.4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2025.focusonaus.v44.4},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {970-973},
  shortjournal = {Market. Sci.},
  title        = {Focus on authors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart green nudging: Reducing product returns through digital footprints and causal machine learning. <em>MKSC</em>, <em>44</em>(4), 954-969. (<a href='https://doi.org/10.1287/mksc.2022.0393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In e-commerce, product returns have become a costly and escalating issue for retailers. Beyond the financial implications for businesses, product returns also lead to increased greenhouse gas emissions and the squandering of natural resources. Traditional approaches, such as charging customers for returns, have proven largely ineffective in curbing returns, thus calling for more nuanced strategies to tackle this issue. This paper investigates the effectiveness of informing consumers about the negative environmental consequences of product returns (“green nudging”) to curtail product returns through a large-scale randomized field experiment ( n = 117,304) conducted with a leading European fashion retailer’s online store. Our findings indicate that implementing green nudging can decrease product returns by 2.6% without negatively impacting sales. We then develop and assess a causal machine learning model designed to identify treatment heterogeneities and personalize green nudging (i.e., make nudging “smart”). Our off-policy evaluation indicates that this personalization can approximately double the success of green nudging. The study demonstrates the effectiveness of both subtle marketing interventions and personalization using causal machine learning in mitigating environmentally and economically harmful product returns, thus highlighting the feasibility of employing “Better Marketing for a Better World” approaches in a digital setting. History: Olivier Toubia served as the senior editor. This paper was selected as a finalist in the 2022 Gary L. Lilien ISMS-MSI Practice Prize Competition. Funding: Partial financial support was received from the Leibniz Institute for Financial Research SAFE and the Deutsche Forschungsgemeinschaft [Grant 449023539]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0393 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0393},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {954-969},
  shortjournal = {Market. Sci.},
  title        = {Smart green nudging: Reducing product returns through digital footprints and causal machine learning},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book bans in american libraries: Impact of politics on inclusive content consumption. <em>MKSC</em>, <em>44</em>(4), 933-953. (<a href='https://doi.org/10.1287/mksc.2024.0716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Banning of books has become increasingly prevalent and politically polarizing in the United States. Although the primary goal of these bans is to restrict access to books, conversations about the bans have garnered attention on a wider scale. This increased attention to bans can either have a chilling effect or can influence consumers to read the banned books. In this study, we use a novel, large-scale data set of U.S. library book circulations and evaluate the impact of high-profile book bans on the consumption of banned books. Using a staggered difference-in-differences design, we find that the circulations of banned books increased by 12%, on average, compared with comparable nonbanned titles after the ban. We also find that banning a book in a state leads to increased circulation in states without bans. We show that the increase in consumption is driven by books from lesser-known authors, suggesting that new and unknown authors stand to gain from the increasing consumer support. Additionally, our results demonstrate that books with higher visibility on social media following the ban see an increase in consumption, suggesting a pivotal role played by social media. Using patron-level data from the Seattle Public Library that include the borrower’s age, we provide suggestive evidence that the increase in readership in the aggregate data is driven, in part, by children reading a book more once it is banned. Using data on campaign emails sent to potential donors subscribed to politicians’ mailing lists, we show a significant increase in mentions of book ban-related topics in fundraising emails sent by Republican candidates. We also provide suggestive evidence on the impact of the rhetoric around these events on donations received by politicians. History: Tat Chan served as the senior editor. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2024.0716 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2024.0716},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {933-953},
  shortjournal = {Market. Sci.},
  title        = {Book bans in american libraries: Impact of politics on inclusive content consumption},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The promotional effects of live streams by twitch influencers. <em>MKSC</em>, <em>44</em>(4), 916-932. (<a href='https://doi.org/10.1287/mksc.2022.0400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the effect of video game live streaming on the popularity of broadcasted games. To this end, we collect novel high-frequency data from Twitch.tv , a major video game streaming platform, by monitoring live streams of 60,000 popular streamers every 10 minutes for eight months. To estimate the effect of live streaming, we leverage these high-frequency data and isolate plausibly exogenous within-day variation in the broadcast hours of top influencers. We find that the number of people watching a game in live streams increases the concurrent number of people playing it with an elasticity of 0.027, a moderate effect that dissipates within a few hours. Investigating the mechanisms behind live streaming effects, we find evidence that live streams make consumers aware of games by lesser-known publishers and reveal the quality and match value of games to consumers. Our back-of-the-envelope calculations suggest that, despite the general excitement about live stream promotions in this industry, only about one sixth of all games profit from sponsored live streams. History: Puneet Manchanda served as the senior editor. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0400 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0400},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {916-932},
  shortjournal = {Market. Sci.},
  title        = {The promotional effects of live streams by twitch influencers},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opposing influences of YouTube influencers: Purchase and usage effects in the video game industry. <em>MKSC</em>, <em>44</em>(4), 894-915. (<a href='https://doi.org/10.1287/mksc.2021.0242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influencers promote firms’ products by posting content such as videos on social media platforms. For entertainment products, these posts could substitute or complement demand for the original entertainment product. We study video games, the largest entertainment product category comprising one third of YouTube traffic, using a large daily panel data set on thousands of video games. Leveraging a supply shock on YouTube called the “Adpocalypse,” we measure the impact of influencer videos on purchase and usage of games. We provide plausibly causal evidence that, on average, influencer video posts substitute to video games for purchases but complement for usage. We also find that influencer effects differ across firms. Managers can use these results to align the influencer effects they face with their revenue models, such as using in-game purchases or a subscription model when facing complements on usage. History: Puneet Manchanda served as the senior editor. Funding: N. Li received funding from the Shanghai Pujiang Program, Shanghai Municipal Human Resources and Social Security Bureau [Grant 2020PJC111], for this research. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2021.0242 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2021.0242},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {894-915},
  shortjournal = {Market. Sci.},
  title        = {Opposing influences of YouTube influencers: Purchase and usage effects in the video game industry},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of voluntary labeling. <em>MKSC</em>, <em>44</em>(4), 874-893. (<a href='https://doi.org/10.1287/mksc.2023.0273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policymakers have mandated food labeling standards, for example, through the Nutrition Labeling and Education Act. However, many claims made by firms are voluntary, such as when they label products as containing “low calories” and “no high-fructose corn syrup (HFCS).” This paper examines whether the use of voluntary labels can help consumers make more informed choices, or if labels lead consumers to ignore other relevant product characteristics, resulting in choices that can be harmful. With theory providing both types of predictions, we empirically analyze firm- and consumer-side behavior, focusing on the specific case of the “no HFCS” label and the corresponding sugar content—an increasingly policy-relevant product characteristic—of such products. We first document common firm practices in the industry across various product categories where the use of the label is prevalent. We then examine consumer search and purchase behavior through an incentive-aligned experiment. We find that products with the “no HFCS” label are less healthy, containing more sugars, than others, and that certain subsets of consumers search less and buy nutritionally worse products in the presence of voluntary labels. Our findings inform both managers and policymakers about whether and how voluntary labels should be displayed. History: Tat Chan served as the senior editor. Funding: This work was supported by the Marketing Science Institute [Research Agreement Grant 4001628]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2023.0273 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0273},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {874-893},
  shortjournal = {Market. Sci.},
  title        = {The impact of voluntary labeling},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corruption and stooges in procurement. <em>MKSC</em>, <em>44</em>(4), 856-873. (<a href='https://doi.org/10.1287/mksc.2023.0078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If a buying firm employs an agent to conduct a procurement process, then sellers might be tempted to bribe the agent in order to win the contract. As a consequence, the procurement outcome can be inefficient (i.e., a low-quality product is purchased). This paper shows under which conditions sellers’ delegation to stooges mitigates or aggravates the inefficiency problem. Delegation will be efficiency-enhancing if the stooge’s and the delegating seller’s joint loss from detected corruption is sufficiently large compared with the respective loss of a non-delegating seller. Intuitively, the probability of inefficient procurement increases with the intensity of the bribing competition, and large losses mitigate this intensity. If all parties on the sell side that are involved in corruption suffer similar losses under delegation and nondelegation, then the condition for an efficiency-enhancing effect of stooges is clearly satisfied. The theoretical analysis also shows that, in contrast to the strategic-delegation literature, overall delegation is not an equilibrium outcome because bribing is a hidden action and sellers dislike leaving positive rents to stooges who are protected by limited liability. Finally, this paper derives the buying firm’s optimal reserve price and incentive pay for its agent to prevent an inefficient procurement outcome for sure. History: Anthony Dukes served as the senior editor. Funding: This study was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy—EXC 2126/1—390838866. The author certifies that he has no affiliations with or involvement in any organization or entity with any financial interest or nonfinancial interest in the subject matter or materials discussed in this article. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2023.0078 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0078},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {856-873},
  shortjournal = {Market. Sci.},
  title        = {Corruption and stooges in procurement},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning product characteristics and consumer preferences from search data. <em>MKSC</em>, <em>44</em>(4), 838-855. (<a href='https://doi.org/10.1287/mksc.2023.0118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key idea in demand estimation is to model products as bundles of characteristics. In this paper, we offer an approach for jointly learning latent product characteristics and consumer preferences from search data in order to predict demand more accurately. We combine data on consumers’ web-browsing histories and hotel price/quantity data to test this method in the hotel market. In two distinct applications, we show that closeness in latent characteristic space predicts competition, and parameters learned from search data substantially improve postmerger demand predictions. History: Catherine Tucker served as the senior editor. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2023.0118 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0118},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {838-855},
  shortjournal = {Market. Sci.},
  title        = {Learning product characteristics and consumer preferences from search data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Close a store to open a pandora’s box? the effects of store closure on sales, omnichannel shopping, and mobile app usage. <em>MKSC</em>, <em>44</em>(4), 820-837. (<a href='https://doi.org/10.1287/mksc.2022.0450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What are the causal effects of the closure of a store on a retail chain’s overall, offline, and online sales? We address this central research question using sales, shopper transaction, and mobile app usage data from a large retail chain that closed 34 stores across states in a month. We use the difference-in-differences approach, creating propensity score-matched control counties and controlling for selection. We examine potential moderators of these effects and underlying mechanisms using individual shopper-level and mobile app user-level data, including analyzing app engagement through topic modeling. Our findings reveal that closing a store opens a Pandora’s box in that it triggers significant net monthly sales loss of $209,317 (more than the average monthly sales of the closed store), representing 18.7% of the retailer’s net sales in the county with the closed store because of spillover effects on other channel purchases by the retailer’s customers in that county. The numbers of the retailer’s active customers, new customers, active app users, new app users, and their mobile app engagement all decline postclosure. Store closure has a negative spillover effect on even nearby shoppers who never shopped at the closed store. Loyal shoppers among nonvisitors to the closed store and app users are more tolerant of store closure than other shoppers. To mitigate adverse effects, retail chains can strategically choose stores closer to other stores in the chain, with a high percentage of in-store discounts and online sales, and a low value of product returns to close. Additionally, they can redirect shoppers in affected counties to the chain’s nearby stores and online (in particular, the mobile app) by offering discounts and promoting store and product information in the app. History: Catherine Tucker served as the senior editor. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0450 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0450},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {820-837},
  shortjournal = {Market. Sci.},
  title        = {Close a store to open a pandora’s box? the effects of store closure on sales, omnichannel shopping, and mobile app usage},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consumer search and product line length: The role of the consumer-product fit distribution. <em>MKSC</em>, <em>44</em>(4), 802-819. (<a href='https://doi.org/10.1287/mksc.2023.0462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More intense consumer search across firms may lead to both stronger price competition and a better match between customers and products. We show that the net result of these forces may lead to either shorter or longer product lines and higher or lower prices and profits depending on the distribution of product valuations across consumers. In the case of full market participation/coverage and many firms, we provide a simple function of value distribution (and discuss its relation to the hazard rate) that can guide managers in understanding the directional impact of consumer search costs on firms’ product line length. Lower search costs lead to longer product lines if the value distribution has a nonincreasing hazard rate (e.g., Exponential or Pareto distributions) but are a force toward shorter product lines under Normal, Logistic, or Gumbel-Minimum distributions. Gumbel-Maximum is the borderline case resulting in a net zero effect. Under Uniform distribution, prices and product lines are inverted-U shaped in the search costs. We separately consider the implications of consumers’ market participation decisions for firms’ product line strategy. History: Anthony Dukes served as the senior editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2023.0462 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0462},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {802-819},
  shortjournal = {Market. Sci.},
  title        = {Consumer search and product line length: The role of the consumer-product fit distribution},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating sparse spatial demand to manage crowdsourced supply in the sharing economy. <em>MKSC</em>, <em>44</em>(4), 777-801. (<a href='https://doi.org/10.1287/mksc.2022.0458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a structural approach to guide decisions regarding the acquisition, retention, and development of individual providers by a sharing economy platform that crowdsources supply, which we call provider relationship management. Taking the context of a French car-sharing platform for which we have historical data, we lay out a random coefficient logit model of spatial demand combined with a Bertrand model of price competition between providers. Sparsity brings challenges in demand estimation; we resolve them through an approximation that brings new insights on a recent model with Poisson consumer arrivals. We then perform counterfactuals to evaluate the incremental value brought by existing potential providers to the platform. The results show that ignoring externalities between providers leads to large biases: provider incremental values are overestimated by 40% on average, and customer scorings are substantially impacted, resulting in suboptimal reward allocation. We also evaluate the potential impact of an advertising campaign to illustrate how our approach can be used to target acquisitions in specific locations, and we study the impact of activities that may increase the value of existing providers through price and/or location changes. History: Tat Chan served as the senior editor. Funding: This work was supported by Investissements d’Avenir [Grant ANR-11-IDEX-0003/Labex Ecodec/ANR-11-LABX-0047] and Fondation HEC. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0458 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0458},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {777-801},
  shortjournal = {Market. Sci.},
  title        = {Estimating sparse spatial demand to manage crowdsourced supply in the sharing economy},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Buying from a competitor: A model of knowledge spillover and innovation. <em>MKSC</em>, <em>44</em>(4), 760-776. (<a href='https://doi.org/10.1287/mksc.2023.0148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many firms buy a production input from a competitor. However, managers often worry that this supply relationship may give their competitor valuable knowledge about new product innovations. We develop a two-period model in which a firm can buy an input from a competitor or a third party in each period. In order to innovate, the firm must invest in improving the input, which results in its supplier learning to produce a higher quality input. We show that buying from the competitor: (i) increases short-term profits by softening price competition and (ii) may reduce long-term profits by preventing investment in innovation. Our results imply that the classic holdup problem, which leads to underinvestment in innovation, becomes more severe when a firm buys from its competitor who benefits from knowledge spillover. History: Olivier Toubia served as the senior editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2023.0148 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0148},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {760-776},
  shortjournal = {Market. Sci.},
  title        = {Buying from a competitor: A model of knowledge spillover and innovation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frontiers: Does carrying news increase engagement with non-news content on social media platforms?. <em>MKSC</em>, <em>44</em>(4), 748-759. (<a href='https://doi.org/10.1287/mksc.2024.0993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of online news aggregators has intensified the debate over whether they should compensate news publishers for redistributing news content. Regulators worldwide are working to develop policies that balance the interests of both parties. However, there is limited understanding of the impact of carrying news on news aggregators, especially on their non-news content. Our research fills this gap by examining the impact of news on non-news user engagement and content generation on Facebook. We utilize a natural experiment—the Facebook Australia news shutdown in 2021—and apply a difference-in-differences approach to quantify these effects. We find that, in the short run, both user engagement and content generation of non-news content on Facebook decreased after the news shutdown. Moreover, the effects were more pronounced for non-news posts with negative sentiment and higher popularity as well as for non-news accounts that are more socially active, experienced, and verified. These results highlight the positive spillover effects of news on non-news content and provide timely and relevant insights for regulators, news publishers, social media platforms that carry news, and content creators on social media platforms. History: Olivier Toubia served as the senior editor. This paper was accepted through the Marketing Science : Frontiers review process. Funding and Competing Interests: All authors certify that they have no affiliation with or involvement in any organization or entity with any financial interest or nonfinancial interest in the subject matter or materials discussed in this paper. The authors have no funding to report. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2024.0993 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2024.0993},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {748-759},
  shortjournal = {Market. Sci.},
  title        = {Frontiers: Does carrying news increase engagement with non-news content on social media platforms?},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frontiers: Generative AI and personalized video advertisements. <em>MKSC</em>, <em>44</em>(4), 733-747. (<a href='https://doi.org/10.1287/mksc.2023.0494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the effectiveness of personalized video advertisements created using generative artificial intelligence (GenAI). We run a mobile ad targeting field experiment on WhatsApp in partnership with a leading direct-to-consumer e-commerce brand that sells eco-friendly sustainable products. We randomize users into receiving ads from one of three targeting conditions: (1) GenAI-based personalized video ads, (2) personalized image ads, and (3) generic nonpersonalized video ads. The first group is our main treatment, and the latter two serve as baselines. In the personalized treatment conditions, ad content is tailored to individual purchase histories, whereas in the generic treatment condition, a uniform brand message is delivered to all users. Our results show that GenAI-based personalized video ads increase engagement by six to nine percentage points over the baselines. These gains are robust across consumer demographics such as gender and location. We use back-of-the-envelope calculations to highlight substantial cost savings and productivity benefits of GenAI-based personalized ad campaigns. We discuss the implications of our findings for businesses and policymakers while noting the potential variation in effectiveness and generalizability of GenAI applications across marketing contexts. History: Olivier Toubia served as the senior editor. This paper was accepted through the Marketing Science : Frontiers review process. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2023.0494 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0494},
  journal      = {Marketing Science},
  month        = {7-8},
  number       = {4},
  pages        = {733-747},
  shortjournal = {Market. Sci.},
  title        = {Frontiers: Generative AI and personalized video advertisements},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="moor">MOOR - 30</h2>
<ul>
<li><details>
<summary>
(2025). Comparison between mean-variance and monotone mean-variance preferences under jump diffusion and stochastic factor model. <em>MOOR</em>, <em>50</em>(3), 2405-2432. (<a href='https://doi.org/10.1287/moor.2022.0331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper compares the optimal investment problems based on monotone mean-variance (MMV) and mean-variance (MV) preferences in a Lévy market with an untradable stochastic factor. It is an open question proposed by Trybuła and Zawisza. Using the dynamic programming and Lagrange multiplier methods, we get the Hamilton-Jacobi-Bellman-Isaacs (HJBI) and Hamilton-Jacobi-Bellman (HJB) equations corresponding to the two investment problems. The equations are transformed into a new-type parabolic equation, from which the optimal strategies under both preferences are derived. We prove that the two optimal strategies and value functions coincide if and only if an important market assumption holds. When the assumption is violated, MMV investors act differently from MV investors. Thus, we conclude that the difference between continuous-time MMV and MV portfolio selections is due to the discontinuity of the market. In addition, we derive the efficient frontier and analyze the economic impact of the jump diffusion risky asset. We also provide empirical evidence to demonstrate the validity of the assumption in real financial markets. Funding: This research was supported by National Natural Science Foundation of China [Grants 12271290 and 11871036].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0331},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2405-2432},
  shortjournal = {Math. Oper. Res.},
  title        = {Comparison between mean-variance and monotone mean-variance preferences under jump diffusion and stochastic factor model},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual solutions in convex stochastic optimization. <em>MOOR</em>, <em>50</em>(3), 2375-2404. (<a href='https://doi.org/10.1287/moor.2022.0270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies duality and optimality conditions for general convex stochastic optimization problems. The main result gives sufficient conditions for the absence of a duality gap and the existence of dual solutions in a locally convex space of random variables. It implies, in particular, the necessity of scenario-wise optimality conditions that are behind many fundamental results in operations research, stochastic optimal control, and financial mathematics. Our analysis builds on the theory of Fréchet spaces of random variables whose topological dual can be identified with the direct sum of another space of random variables and a space of singular functionals. The results are illustrated by deriving sufficient and necessary optimality conditions for several more specific problem classes. We obtain significant extensions to earlier models, for example, on stochastic optimal control, portfolio optimization, and mathematical programming.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0270},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2375-2404},
  shortjournal = {Math. Oper. Res.},
  title        = {Dual solutions in convex stochastic optimization},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Langevin dynamics based algorithm e-THεO POULA for stochastic optimization problems with discontinuous stochastic gradient. <em>MOOR</em>, <em>50</em>(3), 2333-2374. (<a href='https://doi.org/10.1287/moor.2022.0307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new Langevin dynamics based algorithm, called the extended tamed hybrid ε -order polygonal unadjusted Langevin algorithm (e-TH ε O POULA), to solve optimization problems with discontinuous stochastic gradients, which naturally appear in real-world applications such as quantile estimation, vector quantization, conditional value at risk (CVaR) minimization, and regularized optimization problems involving rectified linear unit (ReLU) neural networks. We demonstrate both theoretically and numerically the applicability of the e-TH ε O POULA algorithm. More precisely, under the conditions that the stochastic gradient is locally Lipschitz in average and satisfies a certain convexity at infinity condition, we establish nonasymptotic error bounds for e-TH ε O POULA in Wasserstein distances and provide a nonasymptotic estimate for the expected excess risk, which can be controlled to be arbitrarily small. Three key applications in finance and insurance are provided, namely, multiperiod portfolio optimization, transfer learning in multiperiod portfolio optimization, and insurance claim prediction, which involve neural networks with (Leaky)-ReLU activation functions. Numerical experiments conducted using real-world data sets illustrate the superior empirical performance of e-TH ε O POULA compared with SGLD (stochastic gradient Langevin dynamics), TUSLA (tamed unadjusted stochastic Langevin algorithm), adaptive moment estimation, and Adaptive Moment Estimation with a Strongly Non-Convex Decaying Learning Rate in terms of model accuracy. Funding: Financial support was provided by the Alan Turing Institute, London, under the Engineering and Physical Sciences Research Council [Grant EP/N510129/1]; the Ministry of Education of Singapore Academic Research Fund [Tier 2 Grant MOE-T2EP20222-0013]; the European Union’s Horizon 2020 Research and Innovation Programme [Marie Skłodowska-Curie Grant Agreement 801215]; the University of Edinburgh’s Data-Driven Innovation Programme, part of the Edinburgh and South East Scotland City Region Deal; an Institute of Information and Communications Technology Planning and Evaluation grant funded by the Korean Ministry of Science and ICT (MIST) [Grant 2020-0-01336]; the Artificial Intelligence Graduate School Program of the Ulsan National Institute of Science and Technology; a National Research Foundation of Korea grant funded by the Korean government (MSIT) [Grant RS-2023-00253002]; and the Guangzhou–Hong Kong University of Science and Technology (Guangzhou) Joint Funding Program [Grant 2024A03J0630].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0307},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2333-2374},
  shortjournal = {Math. Oper. Res.},
  title        = {Langevin dynamics based algorithm e-THεO POULA for stochastic optimization problems with discontinuous stochastic gradient},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A retrospective approximation approach for smooth stochastic optimization. <em>MOOR</em>, <em>50</em>(3), 2301-2332. (<a href='https://doi.org/10.1287/moor.2022.0136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic Gradient (SG) is the de facto iterative technique to solve stochastic optimization (SO) problems with a smooth (nonconvex) objective f and a stochastic first-order oracle. SG’s attractiveness is due in part to its simplicity of executing a single step along the negative subsampled gradient direction to update the incumbent iterate. In this paper, we question SG’s choice of executing a single step as opposed to multiple steps between subsample updates. Our investigation leads naturally to generalizing SG into Retrospective Approximation (RA), where, during each iteration, a “deterministic solver” executes possibly multiple steps on a subsampled deterministic problem and stops when further solving is deemed unnecessary from the standpoint of statistical efficiency. RA thus formalizes what is appealing for implementation—during each iteration, “plug in” a solver—for example, L-BFGS line search or Newton-CG— as is , and solve only to the extent necessary. We develop a complete theory using relative error of the observed gradients as the principal object, demonstrating that almost sure and L 1 consistency of RA are preserved under especially weak conditions when sample sizes are increased at appropriate rates. We also characterize the iteration and oracle complexity (for linear and sublinear solvers) of RA and identify a practical termination criterion leading to optimal complexity rates. To subsume nonconvex f , we present a certain “random central limit theorem” that incorporates the effect of curvature across all first-order critical points, demonstrating that the asymptotic behavior is described by a certain mixture of normals. The message from our numerical experiments is that the ability of RA to incorporate existing second-order deterministic solvers in a strategic manner might be important from the standpoint of dispensing with hyper-parameter tuning. Funding: R. Pasupathy received financial support from the Office of Naval Research [Grants N000141712295 and 13000991]. R. Bollapragada received financial support from the Lawrence Livermore National Laboratory and the National Science Foundation [Grant NSF DMS 2324643].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0136},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2301-2332},
  shortjournal = {Math. Oper. Res.},
  title        = {A retrospective approximation approach for smooth stochastic optimization},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The minimax property in infinite two-person win-lose games. <em>MOOR</em>, <em>50</em>(3), 2287-2300. (<a href='https://doi.org/10.1287/moor.2023.0352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore a version of the minimax theorem for two-person win-lose games with infinitely many pure strategies. In the countable case, we give a combinatorial condition on the game which implies the minimax property. In the general case, we prove that a game satisfies the minimax property along with all its subgames if and only if none of its subgames is isomorphic to the “larger number game.” This generalizes a recent theorem of Hanneke, Livni, and Moran. We also propose several applications of our results outside of game theory.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0352},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2287-2300},
  shortjournal = {Math. Oper. Res.},
  title        = {The minimax property in infinite two-person win-lose games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Envy-free division of multilayered cakes. <em>MOOR</em>, <em>50</em>(3), 2261-2286. (<a href='https://doi.org/10.1287/moor.2022.0350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dividing a multilayered cake under nonoverlapping constraints captures several scenarios (e.g., allocating multiple facilities over time where each agent can utilize at most one facility simultaneously). We establish the existence of an envy-free multidivision that is nonoverlapping and contiguous within each layer when the number of agents is a prime power, solving partially an open question by Hosseini et al. [Hosseini H, Igarashi A, Searns A (2020) Fair division of time: Multi-layered cake cutting. Proc. 29th Internat. Joint Conf. Artificial Intelligence (IJCAI) , 182–188; Hosseini H, Igarashi A, Searns A (2020) Fair division of time: Multi-layered cake cutting. Preprint, submitted April 28, http://arxiv.org/abs/2004.13397 ]. Our approach follows an idea proposed by Jojić et al. [Jojić D, Panina G, Živaljević R (2021) Splitting necklaces, with constraints. SIAM J. Discrete Math. 35(2):1268–1286] for envy-free divisions, relying on a general fixed-point theorem. We further design a fully polynomial-time approximation scheme for the two-layer, three-agent case, with monotone preferences. All results are actually established for divisions among groups of almost the same size. In the one-layer, three-group case, our algorithm is able to deal with any predetermined sizes, still with monotone preferences. For three groups, this provides an algorithmic version of a recent theorem by Segal-Halevi and Suksompong [Segal-Halevi E, Suksompong W (2021) How to cut a cake fairly: A generalization to groups. Amer. Math. Monthly 128(1):79–83]. Funding: This work was partially supported by the Japan Science and Technology Agency [Grant JPMJPR20C], Fusion Oriented REsearch for disruptive Science and Technology [Grant JPMJFR226O], and Exploratory Research for Advanced Technology [Grant JPMJER2301].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0350},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2261-2286},
  shortjournal = {Math. Oper. Res.},
  title        = {Envy-free division of multilayered cakes},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust online selection with uncertain offer acceptance. <em>MOOR</em>, <em>50</em>(3), 2226-2260. (<a href='https://doi.org/10.1287/moor.2023.0210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online advertising has motivated interest in online selection problems. Displaying ads to the right users benefits both the platform (e.g., via pay-per-click) and the advertisers (by increasing their reach). In practice, not all users click on displayed ads, while the platform’s algorithm may miss the users most disposed to do so. This mismatch decreases the platform’s revenue and the advertiser’s chances to reach the right customers. With this motivation, we propose a secretary problem where a candidate may or may not accept an offer according to a known probability p . Because we do not know the top candidate willing to accept an offer, the goal is to maximize a robust objective defined as the minimum over integers k of the probability of choosing one of the top k candidates, given that one of these candidates will accept an offer. Using Markov decision process theory, we derive a linear program for this max-min objective whose solution encodes an optimal policy. The derivation may be of independent interest, as it is generalizable and can be used to obtain linear programs for many online selection models. We further relax this linear program into an infinite counterpart, which we use to provide bounds for the objective and closed-form policies. For p ≥ p * ≈ 0.6 , an optimal policy is a simple threshold rule that observes the first p 1 / ( 1 − p ) fraction of candidates and subsequently makes offers to the best candidate observed so far. Funding: Financial support from the U.S. National Science Foundation [Grants CCF-2106444, CCF-1910423, and CMMI 1552479] is gratefully acknowledged.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0210},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2226-2260},
  shortjournal = {Math. Oper. Res.},
  title        = {Robust online selection with uncertain offer acceptance},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The core of housing markets from an agent’s perspective: Is it worth sprucing up your home?. <em>MOOR</em>, <em>50</em>(3), 2199-2225. (<a href='https://doi.org/10.1287/moor.2023.0092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study housing markets as introduced by Shapley and Scarf. We investigate the computational complexity of various questions regarding the situation of an agent a in a housing market H : we show that it is NP -hard to find an allocation in the core of H in which (i) a receives a certain house, (ii) a does not receive a certain house, or (iii) a receives a house other than a ’s own. We prove that the core of housing markets respects improvement in the following sense: given an allocation in the core of H in which agent a receives a house h , if the value of the house owned by a increases, then the resulting housing market admits an allocation in its core in which a receives either h or a house that a prefers to h ; moreover, such an allocation can be found efficiently. We further show an analogous result in the S table R oommates setting by proving that stable matchings in a one-sided market also respect improvement. Funding: This work was supported by the Hungarian Scientific Research Fund [Grants K124171, K128611]. I. Schlotter is supported by the Hungarian Academy of Sciences under its Momentum Programme (LP2021-2) and its János Bolyai Research Scholarship. The research reported in this paper and carried out by T. Fleiner at the Budapest University of Technology and Economics was supported by the “TKP2020, National Challenges Program” of the National Research Development and Innovation Office [BME NC TKP2020 and OTKA K143858] and by the Higher Education Excellence Program of the Ministry of Human Capacities in the frame of the Artificial Intelligence research area of the Budapest University of Technology and Economics (BME FIKP-MI/SC). P. Biró gratefully acknowledges financial support from the Hungarian Scientific Research Fund, OTKA [Grant K143858] and the Hungarian Academy of Sciences [Momentum Grant LP2021-2].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0092},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2199-2225},
  shortjournal = {Math. Oper. Res.},
  title        = {The core of housing markets from an agent’s perspective: Is it worth sprucing up your home?},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of a class of minimization problems lacking lower semicontinuity. <em>MOOR</em>, <em>50</em>(3), 2175-2198. (<a href='https://doi.org/10.1287/moor.2023.0295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimization of nonlower semicontinuous functions is a difficult topic that has been minimally studied. Among such functions is a Heaviside composite function that is the composition of a Heaviside function with a possibly nonsmooth multivariate function. Unifying a statistical estimation problem with hierarchical selection of variables and a sample average approximation of composite chance constrained stochastic programs, a Heaviside composite optimization problem is one whose objective and constraints are defined by sums of possibly nonlinear multiples of such composite functions. Via a pulled-out formulation, a pseudostationarity concept for a feasible point was introduced in an earlier work as a necessary condition for a local minimizer of a Heaviside composite optimization problem. The present paper extends this previous study in several directions: (a) showing that pseudostationarity is implied by (and thus, weaker than) a sharper subdifferential-based stationarity condition that we term epistationarity; (b) introducing a set-theoretic sufficient condition, which we term a local convexity-like property, under which an epistationary point of a possibly nonlower semicontinuous optimization problem is a local minimizer; (c) providing several classes of Heaviside composite functions satisfying this local convexity-like property; (d) extending the epigraphical formulation of a nonnegative multiple of a Heaviside composite function to a lifted formulation for arbitrarily signed multiples of the Heaviside composite function, based on which we show that an epistationary solution of the given Heaviside composite program with broad classes of B-differentiable component functions can in principle be approximately computed by surrogation methods. Funding: The work of Y. Cui was based on research supported by the National Science Foundation [Grants CCF-2153352, DMS-2309729, and CCF-2416172] and the National Institutes of Health [Grant 1R01CA287413-01]. The work of J.-S. Pang was based on research supported by the Air Force Office of Scientific Research [Grant FA9550-22-1-0045].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0295},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2175-2198},
  shortjournal = {Math. Oper. Res.},
  title        = {Analysis of a class of minimization problems lacking lower semicontinuity},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlated equilibria in large anonymous bayesian games. <em>MOOR</em>, <em>50</em>(3), 2157-2174. (<a href='https://doi.org/10.1287/moor.2023.0278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider multipopulation Bayesian games with a large number of players. Each player aims at minimizing a cost function that depends on this player’s own action, the distribution of players’ actions in all populations, and an unknown state parameter. We study the nonatomic limit versions of these games and introduce the concept of Bayes correlated Wardrop equilibrium, which extends the concept of Bayes correlated equilibrium to nonatomic games. We prove that Bayes correlated Wardrop equilibria are limits of action flows induced by Bayes correlated equilibria of the game with a large finite set of small players. For nonatomic games with complete information admitting a convex potential, we prove that the set of correlated and of coarse correlated Wardrop equilibria coincide with the set of probability distributions over Wardrop equilibria and that all equilibrium outcomes have the same costs. We get the following consequences. First, all flow distributions of (coarse) correlated equilibria in convex potential games with finitely many players converge to mixtures of Wardrop equilibria when the weight of each player tends to zero. Second, for any sequence of flows satisfying a no-regret property, its empirical distribution converges to the set of distributions over Wardrop equilibria, and the average cost converges to the unique Wardrop cost. Funding: This work was partially supported by European Cooperation in Science and Technology Action 16228 GAMENET. F. Koessler acknowledges the support of the Agence Nationale de la Recherche [Grant StratCom ANR-19-CE26-0010-01]. M. Scarsini acknowledges the support of the Gruppo Nazionale per l’Analisi Matematica, la Probabilità e le loro Applicazioni project [Grant CUP_E53C22001930001], the Ministero dell’Università e della Ricerca Progetti di Rilevante Interesse Nazionale [Grant 2022EKNE5K], and the European Union-Next Generation EU, component M4C2, investment 1.1 (Ministero dell’Università e della Ricerca Progetti di Rilevante Interesse Nazionale Piano Nazionale di Ripresa e Resilienza) [Grant P2022XT8C8]. T. Tomala gratefully acknowledges the support of the HEC foundation and Agence Nationale de la Recherche/Investissements d’Avenir [Grant ANR-11-IDEX-0003/Labex Ecodec/ANR-11-LABX-0047].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0278},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2157-2174},
  shortjournal = {Math. Oper. Res.},
  title        = {Correlated equilibria in large anonymous bayesian games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse integer programming is fixed-parameter tractable. <em>MOOR</em>, <em>50</em>(3), 2141-2156. (<a href='https://doi.org/10.1287/moor.2023.0162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the general integer programming problem where the number of variables n is a variable part of the input. We consider two natural parameters of the constraint matrix A : its numeric measure a and its sparsity measure d . We present an algorithm for solving integer programming in time g ( a , d ) poly ( n , L ) , where g is some computable function of the parameters a and d , and L is the binary encoding length of the input. In particular, integer programming is fixed-parameter tractable parameterized by a and d , and is solvable in polynomial time for every fixed a and d . Our results also extend to nonlinear separable convex objective functions. Funding: F. Eisenbrand, C. Hunkenschröder, and K.-M. Klein were supported by the Swiss National Science Foundation (SNSF) within the project “Convexity, geometry of numbers, and the complexity of integer programming” [Grant 163071]. A. Levin and S. Onn are partially supported by the Israel Science Foundation [Grant 308/18]. A. Levin is also partially supported by the Israel Science Foundation [Grant 1467/22]. S. Onn is also partially supported by the Dresner Chair at the Technion. M. Koutecký is partially supported by Charles University project UNCE 24/SCI/008, and by the project 22-22997S of the Grantová Agentura České Republiky (GA ČR).},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0162},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2141-2156},
  shortjournal = {Math. Oper. Res.},
  title        = {Sparse integer programming is fixed-parameter tractable},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a network centrality maximization game. <em>MOOR</em>, <em>50</em>(3), 2112-2140. (<a href='https://doi.org/10.1287/moor.2022.0251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a network formation game where n players, identified with the nodes of a directed graph to be formed, choose where to wire their outgoing links in order to maximize their PageRank centrality. Specifically, the action of every player i consists in the wiring of a predetermined number d i of directed out-links, and her utility is her own PageRank centrality in the network resulting from the actions of all players. We show that this is a potential game and that the best response correspondence always exhibits a local structure in that it is never convenient for a node i to link to other nodes that are at incoming distance more than d i from her. We then study the equilibria of this game determining necessary conditions for a graph to be a (strict, recurrent) Nash equilibrium. Moreover, in the homogeneous case, where players all have the same number d of out-links, we characterize the structure of the potential-maximizing equilibria, and in the special cases d = 1 and d = 2, we provide a complete classification of the set of (strict, recurrent) Nash equilibria. Our analysis shows in particular that the considered formation mechanism leads to the emergence of undirected and disconnected or loosely connected networks. Funding: This research was carried out within the framework of the Ministero dell’Università e della Ricerca (MIUR)-funded Progetto di Eccellenza of the Dipartimento di Scienze Matematiche G. L. Lagrange, Politecnico di Torino [CUP: E11G18000350001]. It received partial support from the MIUR-funded project PRIN 2017 “Advanced Network Control of Future Smart Grids” and from the Compagnia di San Paolo.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0251},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2112-2140},
  shortjournal = {Math. Oper. Res.},
  title        = {On a network centrality maximization game},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal ratcheting of dividends with capital injection. <em>MOOR</em>, <em>50</em>(3), 2073-2111. (<a href='https://doi.org/10.1287/moor.2023.0102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the optimal dividend problem with capital injection and ratcheting constraint with nondecreasing dividend payout rate. Capital injections are introduced in order to eliminate the possibility of bankruptcy. Under the Cramér–Lundberg risk model, the problem is formulated as a two-dimensional stochastic control problem. By applying the viscosity theory, we show that the value function is the unique viscosity solution to the associated Hamilton–Jacobi–Bellman equation. In order to obtain analytical results, we further study the problem with finite ratcheting constraint, where the dividend rate takes only a finite number of available values. We show that the value function under general ratcheting can be approximated arbitrarily closely by the one with finite ratcheting. Finally, we derive the expressions of value function when the threshold-type finite ratcheting dividend strategy with capital injection is applied, and we show the optimality of such a strategy under certain conditions of concavity. Numerical examples under various scenarios are provided at the end. Funding W. Wang was supported by the National Natural Science Foundation of China [Grants 12171405, 12271066, and 11661074] and the Fundamental Research Funds for the Central Universities of China [Grant 20720220044]. R. Xu was supported by the National Natural Science Foundation of China [Grants 12201506 and 12371468], the Natural Science Foundation of the Jiangsu Higher Education Institutions of China [Grant 21KJB110024], and Xi’an Jiaotong-Liverpool University Research Development Funding [Grant RDF-20-01-02].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0102},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2073-2111},
  shortjournal = {Math. Oper. Res.},
  title        = {Optimal ratcheting of dividends with capital injection},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the set of balanced games. <em>MOOR</em>, <em>50</em>(3), 2047-2072. (<a href='https://doi.org/10.1287/moor.2023.0379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the geometric structure of the set of cooperative transferable utility games having a nonempty core, characterized by Bondareva and Shapley as balanced games. We show that this set is a nonpointed polyhedral cone, and we find the set of its extremal rays and facets. This study is also done for the set of balanced games whose value for the grand coalition is fixed, which yields an affine nonpointed polyhedral cone. Finally, the case of nonnegative balanced games with fixed value for the grand coalition is tackled. This set is a convex polytope, with remarkable properties. We characterize its vertices and facets, study the adjacency structure of vertices, develop an algorithm for generating vertices in a random uniform way, and show that this polytope is combinatorial and its adjacency graph is Hamiltonian. Last, we give a characterization of the set of games having a core reduced to a singleton. Funding: This work was supported by the Spanish Government [Grant PID2021-124933NB-I00].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0379},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2047-2072},
  shortjournal = {Math. Oper. Res.},
  title        = {On the set of balanced games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parabolic regularity of spectral functions. <em>MOOR</em>, <em>50</em>(3), 2017-2046. (<a href='https://doi.org/10.1287/moor.2023.0010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is devoted to the study of the second-order variational analysis of spectral functions. It is well-known that spectral functions can be expressed as a composite function of symmetric functions and eigenvalue functions. We establish several second-order properties of spectral functions when their associated symmetric functions enjoy these properties. Our main attention is given to characterize parabolic regularity for this class of functions. It was observed recently that parabolic regularity can play a central rule in ensuring the validity of important second-order variational properties, such as twice epi-differentiability. We demonstrates that for convex spectral functions, their parabolic regularity amounts to that of their symmetric functions. As an important consequence, we calculate the second subderivative of convex spectral functions, which allows us to establish second-order optimality conditions for a class of matrix optimization problems. Funding: The research of A. Mohammadi is funded by a postdoctoral fellowship from Georgetown University. E. Sarabi is partially supported by the U.S. National Science Foundation [Grant DMS 2108546].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0010},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {2017-2046},
  shortjournal = {Math. Oper. Res.},
  title        = {Parabolic regularity of spectral functions},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compact extended formulations for low-rank functions with indicator variables. <em>MOOR</em>, <em>50</em>(3), 1992-2016. (<a href='https://doi.org/10.1287/moor.2021.0281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the mixed-integer epigraph of a special class of convex functions with nonconvex indicator constraints, which are often used to impose logical constraints on the support of the solutions. The class of functions we consider are defined as compositions of low-dimensional nonlinear functions with affine functions. Extended formulations describing the convex hull of such sets can easily be constructed via disjunctive programming although a direct application of this method often yields prohibitively large formulations, whose size is exponential in the number of variables. In this paper, we propose a new disjunctive representation of the sets under study, which leads to compact formulations with size exponential in the dimension of the nonlinear function but polynomial in the number of variables. Moreover, we show how to project out the additional variables for the case of dimension one, recovering or generalizing known results for the convex hulls of such sets (in the original space of variables). Our computational results indicate that the proposed approach can significantly improve the performance of solvers in structured problems. Funding: This work was supported by the National Science Foundation Division of Computing and Communication Foundations [Grant 2006762].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0281},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1992-2016},
  shortjournal = {Math. Oper. Res.},
  title        = {Compact extended formulations for low-rank functions with indicator variables},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information design and sharing in supply chains. <em>MOOR</em>, <em>50</em>(3), 1965-1991. (<a href='https://doi.org/10.1287/moor.2023.0008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the interplay between inventory replenishment policies and information sharing in the context of a two-tier supply chain with a single supplier and a single retailer serving an independent and identically distributed Gaussian market demand. We investigate how the retailer’s inventory policy impacts the supply chain’s cumulative expected long-term average inventory costs C in two extreme information-sharing cases: (a) full information sharing and (b) no information sharing. To find the retailer’s inventory policy that minimizes C , we formulate an infinite-dimensional optimization problem whose decision variables are the MA( ∞ ) coefficients that characterize a stationary ordering policy. Under full information sharing, the optimization problem admits a simple solution and the optimal policy is given by an MA(1) process. On the other hand, to solve the optimization problem under no information sharing, we reformulate the optimization from its time domain formulation to an equivalent z -transform formulation in which the decision variables correspond to elements of the Hardy space H 2 . This alternative representation allows us to use a number of results from H 2 theory to compute the optimal value of C and characterize a sequence of ϵ -optimal inventory policies under some mild technical conditions. By comparing the optimal solution under full information sharing and no information sharing, we derive a number of important practical takeaways. For instance, we show that there is value in information sharing if and only if the retailer’s optimal policy under full information sharing is not invertible with respect to the sequence of demand shocks. Furthermore, we derive a fundamental mathematical identity that reveals the value of information sharing by exploiting the canonical Smirnov–Beurling inner–outer factorization of the retailer’s orders when viewed as an element of H 2 . We also show that the value of information sharing can grow unboundedly when the cumulative supply chain costs are dominated by the supplier’s inventory costs. Funding: R. Caldentey acknowledges the University of Chicago Booth School of Business for financial support. Supplemental Material: The online appendix is available at https://doi.org/10.1287/moor.2023.0008 .},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0008},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1965-1991},
  shortjournal = {Math. Oper. Res.},
  title        = {Information design and sharing in supply chains},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty propagation and dynamic robust risk measures. <em>MOOR</em>, <em>50</em>(3), 1939-1964. (<a href='https://doi.org/10.1287/moor.2023.0267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for quantifying propagation of uncertainty arising in a dynamic setting. Specifically, we define dynamic uncertainty sets designed explicitly for discrete stochastic processes over a finite time horizon. These dynamic uncertainty sets capture the uncertainty surrounding stochastic processes and models, accounting for factors such as distributional ambiguity. Examples of uncertainty sets include those induced by the Wasserstein distance and f -divergences. We further define dynamic robust risk measures as the supremum of all candidates’ risks within the uncertainty set. In an axiomatic way, we discuss conditions on the uncertainty sets that lead to well-known properties of dynamic robust risk measures, such as convexity and coherence. Furthermore, we discuss the necessary and sufficient properties of dynamic uncertainty sets that lead to time-consistencies of dynamic robust risk measures. We find that uncertainty sets stemming from f -divergences lead to strong time-consistency whereas the Wasserstein distance results in a new time-consistent notion of weak recursiveness. Moreover, we show that a dynamic robust risk measure is strong time-consistent or weak recursive if and only if it admits a recursive representation of one-step conditional robust risk measures arising from static uncertainty sets. Funding: M. Mailhot and S. M. Pesenti acknowledge support from the Canadian Statistical Sciences Institute (CANSSI) and from the Natural Sciences and Engineering Research Council of Canada [Grants RGPIN-2015-05447, DGECR-2020-00333, and RGPIN-2020-04289]. M. R. Moresco thanks the Horizon Postdoctoral Fellowship for the support.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0267},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1939-1964},
  shortjournal = {Math. Oper. Res.},
  title        = {Uncertainty propagation and dynamic robust risk measures},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Erratum to: Linear convergence of dual coordinate descent on nonpolyhedral convex problems. <em>MOOR</em>, <em>50</em>(3), 1935-1938. (<a href='https://doi.org/10.1287/moor.2024.0500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our proof of linear convergence for Dykstra’s algorithm was erroneous, and in fact, there even exists a counterexample showing that the result is false.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2024.0500},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1935-1938},
  shortjournal = {Math. Oper. Res.},
  title        = {Erratum to: Linear convergence of dual coordinate descent on nonpolyhedral convex problems},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair shares: Feasibility, domination, and incentives. <em>MOOR</em>, <em>50</em>(3), 1901-1934. (<a href='https://doi.org/10.1287/moor.2022.0257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider fair allocation of indivisible goods to n equally entitled agents. Every agent i has a valuation function v i from some given class of valuation functions. A share s is a function that maps ( v i , n ) to a nonnegative value. A share is feasible if for every allocation instance, there is an allocation that gives every agent i a bundle that is acceptable with respect to v i , one of value at least her share value s ( v i , n ) . We introduce the following concepts. A share is self-maximizing if reporting the true valuation maximizes the minimum true value of a bundle that is acceptable with respect to the report. A share s ρ-dominates another share s ′ if s ( v i , n ) ≥ ρ · s ′ ( v i , n ) for every valuation function. We initiate a systematic study of feasible and self-maximizing shares and a systematic study of ρ -domination relation between shares, presenting both positive and negative results. Funding: The research of M. Babaioff is supported in part by a Golda Meir Fellowship. The research of U. Feige is supported in part by the Israel Science Foundation [Grant 1122/22].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0257},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1901-1934},
  shortjournal = {Math. Oper. Res.},
  title        = {Fair shares: Feasibility, domination, and incentives},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An augmented lagrangian approach to conically constrained nonmonotone variational inequality problems. <em>MOOR</em>, <em>50</em>(3), 1868-1900. (<a href='https://doi.org/10.1287/moor.2023.0167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider a nonmonotone (mixed) variational inequality (VI) model with (nonlinear) convex conic constraints. Through developing an equivalent Lagrangian function-like primal-dual saddle point system for the VI model in question, we introduce an augmented Lagrangian primal-dual method, called ALAVI (Augmented Lagrangian Approach to Variational Inequality) in the paper, for solving a general constrained VI model. Under an assumption, called the primal-dual variational coherence condition in the paper, we prove the convergence of ALAVI. Next, we show that many existing generalized monotonicity properties are sufficient—though by no means necessary—to imply the abovementioned coherence condition and thus are sufficient to ensure convergence of ALAVI. Under that assumption, we further show that ALAVI has in fact an o ( 1 / k ) global rate of convergence where k is the iteration count. By introducing a new gap function, this rate further improves to be O ( 1 / k ) if the mapping is monotone. Finally, we show that under a metric subregularity condition, even if the VI model may be nonmonotone, the local convergence rate of ALAVI improves to be linear. Numerical experiments on some randomly generated highly nonlinear and nonmonotone VI problems show the practical efficacy of the newly proposed method. Funding: L. Zhao and D. Zhu were partially supported by the Major Project of the National Natural Science Foundation of China [Grant 72293582], the National Key R&D Program of China [Grant 2023YFA0915202], and the Fundamental Research Funds for the Central Universities (the Interdisciplinary Program of Shanghai Jiao Tong University) [Grant YG2024QNA36]. L. Zhao was partially supported by the Startup Fund for Young Faculty at SJTU (SFYF at SJTU) [Grant 22X010503839].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0167},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1868-1900},
  shortjournal = {Math. Oper. Res.},
  title        = {An augmented lagrangian approach to conically constrained nonmonotone variational inequality problems},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonzero-sum optimal stopping game with continuous vs. periodic exercise opportunities. <em>MOOR</em>, <em>50</em>(3), 1832-1867. (<a href='https://doi.org/10.1287/moor.2023.0123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new nonzero-sum game of optimal stopping with asymmetric exercise opportunities. Given a stochastic process modeling the value of an asset, one player observes and can act on the process continuously, whereas the other player can act on it only periodically at independent Poisson arrival times. The first one to stop receives a reward, different for each player, whereas the other one gets nothing. We study how each player balances the maximization of gains against the maximization of the likelihood of stopping before the opponent. In such a setup driven by a Lévy process with positive jumps, we not only prove the existence but also explicitly construct a Nash equilibrium with values of the game written in terms of the scale function. Numerical illustrations with put-option payoffs are also provided to study the behavior of the players’ strategies as well as the quantification of the value of available exercise opportunities. Funding: K. Yamazaki was partly supported by The Japan Society for the Promotion of Science (JSPS) Grant-in-Aid for Scientific Research (KAKENHI) [Grants 19H01791, 20K03758, and 24K06844], Open Partnership Joint Research Projects [Grant JPJSBP120209921], and the University of Queensland [start-up grant].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0123},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1832-1867},
  shortjournal = {Math. Oper. Res.},
  title        = {Nonzero-sum optimal stopping game with continuous vs. periodic exercise opportunities},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploration noise for learning linear-quadratic mean field games. <em>MOOR</em>, <em>50</em>(3), 1762-1831. (<a href='https://doi.org/10.1287/moor.2021.0157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this paper is to demonstrate that common noise may serve as an exploration noise for learning the solution of a mean field game. This concept is here exemplified through a toy linear-quadratic model, for which a suitable form of common noise has already been proven to restore existence and uniqueness. We here go one step further and prove that the same form of common noise may force the convergence of the learning algorithm called fictitious play, and this without any further potential or monotone structure. Several numerical examples are provided to support our theoretical analysis. Funding: F. Delarue acknowledges the financial support of the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme [AdG ELISA project, Grant 101054746]. A. Vasileiadis acknowledge the financial support of French ANR project ANR-19-P3IA-0002-3IA Côte d'Azur-Nice-Interdisciplinary Institute for Artificial Intelligence.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0157},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1762-1831},
  shortjournal = {Math. Oper. Res.},
  title        = {Exploration noise for learning linear-quadratic mean field games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A moment-sum-of-squares hierarchy for robust polynomial matrix inequality optimization with sum-of-squares convexity. <em>MOOR</em>, <em>50</em>(3), 1734-1761. (<a href='https://doi.org/10.1287/moor.2023.0361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of polynomial optimization problems with a robust polynomial matrix inequality (PMI) constraint where the uncertainty set itself is also defined by a PMI. These can be viewed as matrix generalizations of semi-infinite polynomial programs because they involve actually infinitely many PMI constraints in general. Under certain sum-of-squares (SOS)-convexity assumptions, we construct a hierarchy of increasingly tight moment-SOS relaxations for solving such problems. Most of the nice features of the moment-SOS hierarchy for the usual polynomial optimization are extended to this more complicated setting. In particular, asymptotic convergence of the hierarchy is guaranteed, and finite convergence can be certified if some flat extension condition holds true. To extract global minimizers, we provide a linear algebra procedure for recovering a finitely atomic matrix-valued measure from truncated matrix-valued moments. As an application, we are able to solve the problem of minimizing the smallest eigenvalue of a polynomial matrix subject to a PMI constraint. If SOS convexity is replaced by convexity, we can still approximate the optimal value as closely as desired by solving a sequence of semidefinite programs and certify global optimality in case that certain flat extension conditions hold true. Finally, an extension to the nonconvexity setting is provided under a rank 1 condition. To obtain the above-mentioned results, techniques from real algebraic geometry, matrix-valued measure theory, and convex optimization are employed. Funding: This work was supported by the National Key Research and Development Program of China [Grant 2023YFA1009401] and the National Natural Science Foundation of China [Grants 11571350 and 12201618].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0361},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1734-1761},
  shortjournal = {Math. Oper. Res.},
  title        = {A moment-sum-of-squares hierarchy for robust polynomial matrix inequality optimization with sum-of-squares convexity},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-averse markov decision processes through a distributional lens. <em>MOOR</em>, <em>50</em>(3), 1707-1733. (<a href='https://doi.org/10.1287/moor.2023.0211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By adopting a distributional viewpoint on law-invariant convex risk measures, we construct dynamic risk measures (DRMs) at the distributional level. We then apply these DRMs to investigate Markov decision processes, incorporating latent costs, random actions, and weakly continuous transition kernels. Furthermore, the proposed DRMs allow risk aversion to change dynamically. Under mild assumptions, we derive a dynamic programming principle and show the existence of an optimal policy in both finite and infinite time horizons. Moreover, we provide a sufficient condition for the optimality of deterministic actions. For illustration, we conclude the paper with examples from optimal liquidation with limit order books and autonomous driving. Funding: This work was supported by Natural Sciences and Engineering Research Council of Canada [Grants RGPAS-2018-522715 and RGPIN-2018-05705].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0211},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1707-1733},
  shortjournal = {Math. Oper. Res.},
  title        = {Risk-averse markov decision processes through a distributional lens},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convex formulations of robust markov decision processes. <em>MOOR</em>, <em>50</em>(3), 1681-1706. (<a href='https://doi.org/10.1287/moor.2022.0284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust Markov decision processes (MDPs) are used for applications of dynamic optimization in uncertain environments and have been studied extensively. Many of the main properties and algorithms of MDPs, such as value iteration and policy iteration, extend directly to RMDPs. Surprisingly, there is no known analog of the MDP convex optimization formulation for solving RMDPs. This work describes the first convex optimization formulation of RMDPs under the classical sa-rectangularity and s-rectangularity assumptions. By using entropic regularization and exponential change of variables, we derive a convex formulation with a number of variables and constraints polynomial in the number of states and actions, but with large coefficients in the constraints. We further simplify the formulation for RMDPs with polyhedral, ellipsoidal, or entropy-based uncertainty sets, showing that, in these cases, RMDPs can be reformulated as conic programs based on exponential cones, quadratic cones, and nonnegative orthants. Our work opens a new research direction for RMDPs and can serve as a first step toward obtaining a tractable convex formulation of RMDPs. Funding: The work in the paper was supported, in part, by NSF [Grants 2144601 and 1815275]; and Agence Nationale de la Recherche [Grant 11-LABX-0047].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0284},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1681-1706},
  shortjournal = {Math. Oper. Res.},
  title        = {On the convex formulations of robust markov decision processes},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimality conditions in control problems with random state constraints in probabilistic or almost sure form. <em>MOOR</em>, <em>50</em>(3), 1654-1680. (<a href='https://doi.org/10.1287/moor.2023.0177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss optimality conditions for optimization problems involving random state constraints, which are modeled in probabilistic or almost sure form. Although the latter can be understood as the limiting case of the former, the derivation of optimality conditions requires substantially different approaches. We apply them to a linear elliptic partial differential equation with random inputs. In the probabilistic case, we rely on the spherical-radial decomposition of Gaussian random vectors in order to formulate fully explicit optimality conditions involving a spherical integral. In the almost sure case, we derive optimality conditions and compare them with a model based on robust constraints with respect to the (compact) support of the given distribution. Funding: The authors thank the Deutsche Forschungsgemeinschaft [Projects B02 and B04 in the “Sonderforschungsbereich/Transregio 154 Mathematical Modelling, Simulation and Optimization Using the Example of Gas Networks”] for support. C. Geiersbach acknowledges support from the Deutsche Forschungsgemeinschaft [Germany’s Excellence Strategy–the Berlin Mathematics Research Center MATH+ Grant EXC-2046/1, Project 390685689]. R. Henrion acknowledges support from the Fondation Mathématique Jacques Hadamard [Program Gaspard Monge in Optimization and Operations Research, including support to this program by Electricité de France].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0177},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1654-1680},
  shortjournal = {Math. Oper. Res.},
  title        = {Optimality conditions in control problems with random state constraints in probabilistic or almost sure form},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Order independence in sequential, issue-by-issue voting. <em>MOOR</em>, <em>50</em>(3), 1635-1653. (<a href='https://doi.org/10.1287/moor.2022.0342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study when the voting outcome is independent of the order of issues put up for vote in a spatial multidimensional voting model. Agents equipped with norm-based preferences that use a norm to measure the distance from their ideal policy vote sequentially and issue by issue via simple majority. If the underlying norm is generated by an inner product—such as the Euclidean norm—then the voting outcome is order independent if and only if the issues are orthogonal. If the underlying norm is a general one, then the outcome is order independent if the basis defining the issues to be voted upon satisfies the following property; for any vector in the basis, any linear combination of the other vectors is Birkhoff–James orthogonal to it. We prove a partial converse in the case of two dimensions; if the underlying basis fails this property, then the voting order matters. Finally, despite existence results for the two-dimensional case and for the general l p case, we show that nonexistence of bases with this property is generic. Funding: The research of A. Gershkov is supported by the Israel Science Foundation [Grant 1118/22]. The research of B. Moldovanu is supported by the German Science Foundation through the Hausdorff Center for Mathematics and The Collaborative Research Center Transregio 224. The research of X. Shi is supported by the Social Sciences and Humanities Research Council of Canada.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0342},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1635-1653},
  shortjournal = {Math. Oper. Res.},
  title        = {Order independence in sequential, issue-by-issue voting},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large independent sets in recursive markov random graphs. <em>MOOR</em>, <em>50</em>(3), 1611-1634. (<a href='https://doi.org/10.1287/moor.2022.0215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the maximum size of an independent set in a graph is a famously hard combinatorial problem that has been well studied for various classes of graphs. When it comes to random graphs, the classic Erdős–Rényi–Gilbert random graph G n , p has been analyzed and shown to have the largest independent sets of size Θ ( log n ) with high probability (w.h.p.) This classic model does not capture any dependency structure between edges that can appear in real-world networks. We define random graphs G n , p r whose existence of edges is determined by a Markov process that is also governed by a decay parameter r ∈ ( 0 , 1 ] . We prove that w.h.p. G n , p r has independent sets of size ( 1 − r 2 + ε ) n log n for arbitrary ε > 0 . This is derived using bounds on the terms of a harmonic series, a Turán bound on a stability number, and a concentration analysis for a certain sequence of dependent Bernoulli variables that may also be of independent interest. Because G n , p r collapses to G n , p when there is no decay, it follows that having even the slightest bit of dependency (any r < 1 ) in the random graph construction leads to the presence of large independent sets, and thus, our random model has a phase transition at its boundary value of r = 1. This implies that there are large matchings in the line graph of G n , p r , which is a Markov random field. For the maximal independent set output by a greedy algorithm, we deduce that it has a performance ratio of at most 1 + log n ( 1 − r ) w.h.p. when the lowest degree vertex is picked at each iteration and also show that, under any other permutation of vertices, the algorithm outputs a set of size Ω ( n 1 / 1 + τ ) , where τ = 1 / ( 1 − r ) and, hence, has a performance ratio of O ( n 1 2 − r ) . Funding: The initial phase of this research was supported by the National Science Foundation [Grant DMS-1913294].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0215},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1611-1634},
  shortjournal = {Math. Oper. Res.},
  title        = {Large independent sets in recursive markov random graphs},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rockafellian relaxation and stochastic optimization under perturbations. <em>MOOR</em>, <em>50</em>(3), 1585-1610. (<a href='https://doi.org/10.1287/moor.2022.0122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, optimization models are often prone to unavoidable inaccuracies because of dubious assumptions and corrupted data. Traditionally, this placed special emphasis on risk-based and robust formulations, and their focus on “conservative” decisions. We develop, in contrast, an “optimistic” framework based on Rockafellian relaxations in which optimization is conducted not only over the original decision space but also jointly with a choice of model perturbation. The framework enables us to address challenging problems with ambiguous probability distributions from the areas of two-stage stochastic optimization without relatively complete recourse, probability functions lacking continuity properties, expectation constraints, and outlier analysis. We are also able to circumvent the fundamental difficulty in stochastic optimization that convergence of distributions fails to guarantee convergence of expectations. The framework centers on the novel concepts of exact and limit-exact Rockafellians, with interpretations of “negative” regularization emerging in certain settings. We illustrate the role of Phi-divergence, examine rates of convergence under changing distributions, and explore extensions to first-order optimality conditions. The main development is free of assumptions about convexity, smoothness, and even continuity of objective functions. Numerical results in the setting of computer vision and text analytics with label noise illustrate the framework. Funding: This work was supported by the Air Force Office of Scientific Research (Mathematical Optimization Program) under the grant: “Optimal Decision Making under Tight Performance Requirements in Adversarial and Uncertain Environments: Insight from Rockafellian Functions.”},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0122},
  journal      = {Mathematics of Operations Research},
  month        = {8},
  number       = {3},
  pages        = {1585-1610},
  shortjournal = {Math. Oper. Res.},
  title        = {Rockafellian relaxation and stochastic optimization under perturbations},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ms">MS - 42</h2>
<ul>
<li><details>
<summary>
(2025). Reinforcing research transparency at management science. <em>MS</em>, <em>71</em>(9), vii-viii. (<a href='https://doi.org/10.1287/mnsc.2025.editorial.v71.n9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2025.editorial.v71.n9},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {vii-viii},
  shortjournal = {Manag. Sci.},
  title        = {Reinforcing research transparency at management science},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Call for Papers—Management science special issue on business and its interrelationship with democratic resilience, geopolitics, and society in a time of change. <em>MS</em>, <em>71</em>(9), ix-x. (<a href='https://doi.org/10.1287/mnsc.2025.call.v71.n9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2025.call.v71.n9},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {ix-x},
  shortjournal = {Manag. Sci.},
  title        = {Call for Papers—Management science special issue on business and its interrelationship with democratic resilience, geopolitics, and society in a time of change},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Managerial ability and labor investment. <em>MS</em>, <em>71</em>(9), 8072-8095. (<a href='https://doi.org/10.1287/mnsc.2020.01932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability of higher-ability managers to acquire and use resources more efficiently than lower-ability managers suggests a positive linear relation between labor investment efficiency (LIE) and managerial ability (MA). However, a puzzle emerges about how the best managers set themselves apart from their peers, calling into question the linearity of the relation between LIE and MA. We explore this puzzle by asking how the highest-ability managers achieve the highest performance levels. We then investigate this puzzle empirically by considering alternatives to a linear relation between LIE and MA. We begin with the distinction that managers achieve the highest performance when they combine efficient exploitation of existing products and services with successful exploration for innovations in products and services. This point is relevant to our puzzle because a firm’s labor needs for exploration are high and unpredictable. Thus, we expect the highest-ability managers to purposefully invest more than predicted by a model of optimal labor investment across firms. In contrast, we expect low-ability managers, who are less able to evaluate, forecast, and make efficient investments, to deviate more from predicted labor investment and vacillate between over- and underinvestment. We present evidence that supports our predictions of nonlinear relations between LIE and MA, with high-ability managers investing more than predicted and low-ability managers over- and underinvesting. We make and test related hypotheses about exploration (investment in research and development), and we probe further by relating future firm performance to over- and underinvestment in labor for different levels of MA. This paper was accepted by Suraj Srinivasan, accounting. Funding: This work was supported by the CPA Alberta Education Foundation. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2020.01932 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2020.01932},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {8072-8095},
  shortjournal = {Manag. Sci.},
  title        = {Managerial ability and labor investment},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sometimes, always, never: Regulatory clarity and the development of digital financing. <em>MS</em>, <em>71</em>(9), 8027-8071. (<a href='https://doi.org/10.1287/mnsc.2023.01538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines how the level of detail in legal regulations impacts crowdfunding. We find that clear, explicit regulations significantly boost crowdfunding volumes in a global sample of digital finance. Using proxies to measure regulatory detail in three types of countries—countries that sometimes, always, or never had regulation—in a series of difference-in-difference regressions, we show a significant positive relation between regulatory clarity and the level of debt crowdfunding. There is little effect on the level of equity crowdfunding. Clear regulations appear to encourage the creation of new crowdfunding platforms rather than concentrating activity in existing ones. This paper was accepted by Will Cong, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.01538 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01538},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {8027-8071},
  shortjournal = {Manag. Sci.},
  title        = {Sometimes, always, never: Regulatory clarity and the development of digital financing},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of functional diversity on team creativity: Behavioral and fNIRS evidence. <em>MS</em>, <em>71</em>(9), 8007-8026. (<a href='https://doi.org/10.1287/mnsc.2023.02157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variation in functional expertise is an important decision that managers face when designing teams tasked with being creative. We develop theory that predicts that functional diversity has countervailing effects on team creativity through its positive (negative) effect on the uniqueness (usefulness) of the proposals generated. We conduct an experiment where functionally homogeneous or heterogeneous two-person teams are tasked with proposing a creative use for an unused university space. We measure the uniqueness, usefulness, and overall creativity of team proposals. We also use functional near-infrared spectroscopy (fNIRS) neuroimaging technology to investigate the underlying team cognitive processes. Measured outcomes and fNIRS data support our predictions. Combining conventional experiment and advanced neuroimaging techniques, our study informs theory and practice by providing evidence of how functional diversity affects team creativity. This paper was accepted by Suraj Srinivasan, accounting. Funding: This work was supported by the National Natural Science Foundation of China (NA) [Grant 72172132] and CPA Ontario’s Centre for Sustainability Reporting and Performance Management (NA). Supplemental Material: The online supplemental appendix and data files are available at https://doi.org/10.1287/mnsc.2023.02157 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02157},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {8007-8026},
  shortjournal = {Manag. Sci.},
  title        = {The effect of functional diversity on team creativity: Behavioral and fNIRS evidence},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversification quotients: Quantifying diversification via risk measures. <em>MS</em>, <em>71</em>(9), 7990-8006. (<a href='https://doi.org/10.1287/mnsc.2023.00513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish the first axiomatic theory for diversification indices using six intuitive axioms: nonnegativity, location invariance, scale invariance, rationality, normalization, and continuity. The unique class of indices satisfying these axioms, called the diversification quotients (DQs), are defined based on a parametric family of risk measures. A further axiom of portfolio convexity pins down DQs based on coherent risk measures. The DQ has many attractive properties, and it can address several theoretical and practical limitations of existing indices. In particular, for the popular risk measures value at risk and expected shortfall, the corresponding DQ admits simple formulas, and it is efficient to optimize in portfolio selection. Moreover, it can properly capture tail heaviness and common shocks, which are neglected by traditional diversification indices. When illustrated with financial data, the DQ is intuitive to interpret, and its performance is competitive against other diversification indices. This paper was accepted by Manel Baucells, behavioral economics and decision analysis. Funding: X. Han is supported by the National Natural Science Foundation of China [Grants 12301604, 12371471, and 12471449). L. Lin is supported by the Hickman Scholarship from the Society of Actuaries. R. Wang is supported by the Natural Sciences and Engineering Research Council of Canada [Grants CRC-2022-00141 and RGPIN-2024-03728] and the Sun Life Research Fellowship. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00513 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00513},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7990-8006},
  shortjournal = {Manag. Sci.},
  title        = {Diversification quotients: Quantifying diversification via risk measures},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multicell experiments for marginal treatment effect estimation of digital ads. <em>MS</em>, <em>71</em>(9), 7970-7989. (<a href='https://doi.org/10.1287/mnsc.2023.01185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized experiments with treatment and control groups are an important tool to measure the impacts of interventions. However, in experimental settings with one-sided noncompliance extant empirical approaches may not produce the estimands a decision maker needs to solve the problem of interest. For example, these experimental designs are common in digital advertising settings but typical methods do not yield effects that inform the intensive margin: how many consumers should be reached or how much should be spent on a campaign. We propose a solution that combines a novel multicell experimental design with modern estimation techniques that enables decision makers to solve problems with an intensive margin. Our design is straightforward to implement and does not require additional budget. We illustrate our method through simulations calibrated using an advertising experiment at Facebook, demonstrating its superior performance in various scenarios and its advantage over direct optimization approaches. This paper was accepted by Jean-Pierre Dubé, marketing. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.01185 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01185},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7970-7989},
  shortjournal = {Manag. Sci.},
  title        = {Multicell experiments for marginal treatment effect estimation of digital ads},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subcoalition cluster analysis: A new method for modeling conflict in organizations. <em>MS</em>, <em>71</em>(9), 7948-7969. (<a href='https://doi.org/10.1287/mnsc.2020.00013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The renewed interest among managers and management researchers in stakeholder governance has also underscored the shortage of quantitative methods available for studying business firms as political systems. A critical problem for researchers applying coalition-based theories of organizational politics to observational data is that the units of analysis, coined “subcoalitions,” are often unobservable. This paper introduces subcoalition cluster analysis (SCA) as a new computational framework for analyzing intrafirm conflict that permits researchers to model groups of heterogeneous actors in terms of a smaller set of representative subcoalitions. The SCA approach to identifying latent fault-lines among groups of actors is based on widely held ideas in management research about the structure of intrafirm coalition politics, is computationally practicable in settings with many alternatives or actors, and can be straightforwardly applied to observational settings with incomplete data on actor preferences. We then apply SCA to two cases in which an organization characterized by multiple, partially inconsistent goals and stakeholders with heterogeneous preferences face a politically contested decision. In both cases, we first analyze preference data using SCA to identify the subcoalition structure that best characterizes the set of actors in the organization. We then use the SCA output as a dependent variable in an analysis of the predictors of subcoalition membership in the first case and as an independent variable in an analysis of the changing patterns of social influence in the second. This paper was accepted by Lamar Pierce, organizations. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2020.00013 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2020.00013},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7948-7969},
  shortjournal = {Manag. Sci.},
  title        = {Subcoalition cluster analysis: A new method for modeling conflict in organizations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of OM content in earnings calls on a firm’s stock performance. <em>MS</em>, <em>71</em>(9), 7929-7947. (<a href='https://doi.org/10.1287/mnsc.2023.02221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate whether and to what extent financial markets value and respond to operations management (OM)-related information in quarterly earnings calls directed toward financial market participants. We develop and use a novel construct called “stated OM focus” (SOMF) to mine the incidence of and emphasis on OM information in the quarterly earnings conference calls for a large cross-section of firms (S&P 1500) over a time frame of 15 years. We empirically establish the value-relevance of OM information as well as summarize and capture both explanatory and predictive aspects of the OM function’s contribution to firm value. We show that OM-related information disclosed by firms systematically, significantly, positively, and persistently affects abnormal stock returns after controlling for known covariates and controls (financial metrics, standard OM metrics, firm and time effects, and lexical-structure related) and after accounting for earnings surprise. Furthermore, we find evidence that firms’ stated OM focus bears significant predictive power in the short term. Long-short portfolios, created using a sorting signal derived solely from stated OM-focus information, predict future abnormal returns consistently over one, two and three months forward from the earnings call event. We show that the OM information metric is reliable and bears internal, external, and predictive validity. These findings offer useful implications for firms, financial market participants, and OM function stakeholders. This paper was accepted by Jayashankar Swaminathan, operations management. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.02221 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02221},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7929-7947},
  shortjournal = {Manag. Sci.},
  title        = {Impact of OM content in earnings calls on a firm’s stock performance},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monetary easing, leveraged payouts, and lack of investment. <em>MS</em>, <em>71</em>(9), 7907-7928. (<a href='https://doi.org/10.1287/mnsc.2022.01440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a model in which a low monetary policy rate lowers the cost of corporate debt, potentially spurring productive investment; low interest rates, however, also induce firms to lever up so as to increase payouts to equity. Whereas such leveraged payouts privately benefit shareholders, leverage comes at the social cost of distorting their incentives, thereby lowering productivity and discouraging investment. If leverage is unregulated (for example, because of the presence of a shadow banking system), then the optimal monetary policy seeks to contain such socially costly leveraged payouts by stimulating investment in response to adverse shocks only up to a level below the first best. The optimal monetary policy may even consist of leaning against the wind, that is, not stimulating the economy at all, in order to fully contain leveraged payouts and maintain productive efficiency. This paper was accepted by Tomasz Piskorski, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01440 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01440},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7907-7928},
  shortjournal = {Manag. Sci.},
  title        = {Monetary easing, leveraged payouts, and lack of investment},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChatGPT for textual analysis? how to use generative LLMs in accounting research. <em>MS</em>, <em>71</em>(9), 7888-7906. (<a href='https://doi.org/10.1287/mnsc.2023.03253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative large language models (GLLMs), such as ChatGPT and GPT-4 by OpenAI, are emerging as powerful tools for textual analysis tasks in accounting research. GLLMs can solve any textual analysis task solvable using nongenerative methods as well as tasks previously only solvable using human coding. Whereas GLLMs are new and powerful, they also come with limitations and present new challenges that require care and due diligence. This paper highlights the applications of GLLMs for accounting research and compares them with existing methods. It also provides a framework on how to effectively use GLLMs by addressing key considerations, such as model selection, prompt engineering, and ensuring construct validity. In a case study, I demonstrate the capabilities of GLLMs by detecting nonanswers in earnings conference calls, a traditionally challenging task to automate. The new GPT method achieves an accuracy of 96% and reduces the nonanswer error rate by 70% relative to the existing Gow et al. (2021) method. Finally, I discuss the importance of addressing bias, replicability, and data sharing concerns when using GLLMs. Taken together, this paper provides researchers, reviewers, and editors with the knowledge and tools to effectively use and evaluate GLLMs for academic research. This paper was accepted by Eric So, accounting. Funding: Supported by the Foster School of Business – University of Washington. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.03253 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.03253},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7888-7906},
  shortjournal = {Manag. Sci.},
  title        = {ChatGPT for textual analysis? how to use generative LLMs in accounting research},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using subsidies, fines, and restitution with budget balance to combat digital piracy. <em>MS</em>, <em>71</em>(9), 7863-7887. (<a href='https://doi.org/10.1287/mnsc.2021.03463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite utilizing technical prevention methods and enacting copyright protection legislation, digital piracy has remained a persistent problem. We examine policy remedies to digital piracy whereby the policymaker has to balance its budget between fines on detected pirates, subsidies for legal purchases, and restitution to the firm. In our model, users choose whether to subscribe, copy, or not use the good, a firm decides on subscription fee and quality, and a policymaker determines subsidies, fines, and restitution. We find that the firm’s subscription fee is always increasing in subsidies, fines, and restitution under a budget balance constraint. The impact of these policy instruments on the firm’s investment in quality depends on how user marginal utility is influenced by the quality of the good and how the policymaker redistributes fines back to society, if at all. Using two specific functional forms with additive and multiplicative utility, we explain how these factors come into play. With additive utility, fines increase the firm’s investment in quality. However, with multiplicative utility the firm’s investment in quality decreases with fines if fines are used alone or alongside subsidies. In contrast to prior research, findings of our general framework illustrate that imposing fines on detected pirates “can” be socially optimal when accounting for the policymaker’s budget balance. Our specific functional forms also serve as two instances where imposing fines is always welfare maximizing. Finally, we find that although the policymaker’s optimal intervention improves social welfare and mitigates digital piracy, it leads to a reduction in consumer surplus. This paper was accepted by Hemant Bhargava, information systems. Funding: This work was supported by the Social Sciences and Humanities Research Council of Canada [Grant 435-2016-0431]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2021.03463 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03463},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7863-7887},
  shortjournal = {Manag. Sci.},
  title        = {Using subsidies, fines, and restitution with budget balance to combat digital piracy},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unearthing zombies. <em>MS</em>, <em>71</em>(9), 7840-7862. (<a href='https://doi.org/10.1287/mnsc.2022.01356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bankruptcy reforms that improve lenders’ ability to recover claims from financially distressed borrowers can mitigate zombie lending. However, we show that after a 2016 bankruptcy reform in India, lenders are reluctant to recognize zombie credit as nonperforming, impeding reform efficacy. A subsequent complementary regulation targeting lender discretion in recognizing nonperforming assets improves zombie recognition fivefold. The lender disincentive to recognize zombies arises from undercapitalized banks’ reluctance to realize loan losses and political economy frictions at state-owned banks. Resolving zombie credit allows lenders to redirect credit to healthy borrowers, but effects are muted at banks more exposed to zombie borrowers. This paper was accepted by Victoria Ivashina, finance. Funding: The authors are grateful for financial support provided by the NSE-NYU Stern Initiative on the Study of Indian Capital Markets. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01356 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01356},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7840-7862},
  shortjournal = {Manag. Sci.},
  title        = {Unearthing zombies},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracking truth with liquid democracy. <em>MS</em>, <em>71</em>(9), 7821-7839. (<a href='https://doi.org/10.1287/mnsc.2023.02470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamics of random transitive delegations on a graph are of particular interest when viewed through the lens of an emerging voting paradigm: liquid democracy . This paradigm allows voters to choose between directly voting and transitively delegating their votes to other voters so that those selected cast a vote weighted by the number of delegations that they received. In the epistemic setting, where voters decide on a binary issue for which there is a ground truth, previous work showed that a few voters may amass such a large amount of influence that liquid democracy is less likely to identify the ground truth than direct voting. We quantify the amount of permissible concentration of power and examine more realistic delegation models, showing that they behave well by ensuring that (with high probability) there is a permissible limit on the maximum number of delegations received. Our theoretical results demonstrate that the delegation process is similar to well-known processes on random graphs that are sufficiently bounded for our purposes. Along the way, we prove new bounds on the size of the largest component in an infinite Pólya urn process, which may be of independent interest. In addition, we empirically validate the theoretical results, running six experiments (for a total of N = 168 participants, 62 delegation graphs, and over 11,000 votes collected). We find that empirical delegation behaviors meet the conditions for our positive theoretical guarantees. Overall, our work alleviates concerns raised about liquid democracy and bolsters the case for the applicability of this emerging paradigm. This paper was accepted by Martin Bichler, market design, platform, and demand analytics. Funding: This work was supported by the Michael Hammer Fellowship, the Office of Naval Research [2016 Vannevar Bush Faculty Fellowship, 2020 ONR Vannevar Bush Faculty Fellowship, and Grant N00014-20-1-2488], the Office of Secretary of Defense [Grants ARO MURI W911NF-19-0217 and ARO W911NF-17-1-0592], Simons [Investigator Award 622132], the Open Philanthropy Foundation, and the National Science Foundation [Grants IIS-178108, CCF-1733556, CCF-1918421, CCF-2007080, IIS-1703846, and IIS-2024287]. J. Y. Halpern was supported by a grant from the Cooperative AI Foundation [ARO Grant W911NF-22-1-0061]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.02470 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02470},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7821-7839},
  shortjournal = {Manag. Sci.},
  title        = {Tracking truth with liquid democracy},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investor perceptions of the corporate alternative minimum tax. <em>MS</em>, <em>71</em>(9), 7800-7820. (<a href='https://doi.org/10.1287/mnsc.2024.04750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Inflation Reduction Act establishes a new 15% corporate minimum tax on the adjusted financial accounting income for large U.S. corporations. Although the minimum tax is estimated to raise $222 billion over 10 years, some fear firms will manipulate their accounting earnings to reduce their tax liabilities, resulting in less revenue raised. Using an event study, we examine the extent to which investors believe this tax will reduce firm value. We examine stock market reactions around key legislative developments leading to the enactment of the book minimum tax. Our findings show targeted firms experience significantly lower stock returns than nontargeted firms during the enactment process (about 1.4%–1.8% of firm value). In aggregate, our findings are consistent with the Joint Committee on Taxation’s revenue estimates. In cross-sectional tests, we fail to find evidence that firms most likely to avoid the tax via earnings management experience more positive returns. We also fail to find less negative returns for firms most likely to pass on the tax to consumers. Overall, our results suggest investors do not expect firms to avoid this tax. Instead, they appear to expect a significant portion of the corporate minimum tax to be remitted by firms and borne by shareholders. This paper was accepted by Ranjani Krishnan, accounting. Funding: F. B. Gaertner acknowledges support from the Cynthia and Jay Ihlenfeld Professorship, M. Pflitsch acknowledges support of a postdoc fellowship of the German Academic Exchange Service (DAAD), S. O. Kelley gratefully acknowledges support from the James L. Henderson Chair, and J. L. Hoopes gratefully acknowledges support from the Harold Q. Langenderfer fellowship. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2024.04750 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2024.04750},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7800-7820},
  shortjournal = {Manag. Sci.},
  title        = {Investor perceptions of the corporate alternative minimum tax},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Timing complex news to target attention. <em>MS</em>, <em>71</em>(9), 7774-7799. (<a href='https://doi.org/10.1287/mnsc.2021.03722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investors have limited and time-varying attention. These constraints are heterogeneous across investors, which can create asymmetric information and adverse selection problems. We show how firms take these constraints into account: They release harder-to-process news in periods when investor attention is higher. We use an institutional discontinuity within the U.S. corporate filing system to measure these effects. Filings before 5:30 p.m. become available immediately, whereas filings after 5:30 p.m. only become visible the next morning and attract less attention. Firms release longer and more complex news just before the cutoff, giving investors the longest possible period to absorb the information before markets open. Firms experience faster price convergence and more liquidity after precutoff news despite their complexity, which is consistent with the additional attention that they attract. We outline a framework in which the need for investors to spread their attention across different ideas induces firms to file their more complex filings at times when investor attention is higher. Our results are consistent with an equilibrium in which investors pay more attention to complex news and in which firms with complex news time them to target investor attention. This paper was accepted by David Sraer, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.03722 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03722},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7774-7799},
  shortjournal = {Manag. Sci.},
  title        = {Timing complex news to target attention},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multitask learning and bandits via robust statistics. <em>MS</em>, <em>71</em>(9), 7752-7773. (<a href='https://doi.org/10.1287/mnsc.2022.00490'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision makers often simultaneously face many related but heterogeneous learning problems. For instance, a large retailer may wish to learn product demand at different stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to learn patient risk at different providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. Motivated by real data sets, we study a natural setting where the unknown parameter in each learning instance can be decomposed into a shared global parameter plus a sparse instance-specific term. We propose a novel two-stage multitask learning estimator that exploits this structure in a sample-efficient way, using a unique combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). Our estimator yields improved sample complexity bounds in the feature dimension d relative to commonly employed estimators; this improvement is exponential for “data-poor” instances, which benefit the most from multitask learning. We illustrate the utility of these results for online learning by embedding our multitask estimator within simultaneous contextual bandit algorithms. We specify a dynamic calibration of our estimator to appropriately balance the bias-variance trade-off over time, improving the resulting regret bounds in the context dimension d . Finally, we illustrate the value of our approach on synthetic and real data sets. This paper was accepted by J. George Shanthikumar, data science. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00490 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00490},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7752-7773},
  shortjournal = {Manag. Sci.},
  title        = {Multitask learning and bandits via robust statistics},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intraday market return predictability culled from the factor zoo. <em>MS</em>, <em>71</em>(9), 7731-7751. (<a href='https://doi.org/10.1287/mnsc.2023.01657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide strong empirical evidence for time-series predictability of the intraday return on the aggregate market portfolio by exploiting lagged high-frequency cross-sectional returns on the factor zoo. Our results rely on the use of modern machine-learning techniques to regularize the predictive regressions and help tame the signals stemming from the zoo together with techniques from financial econometrics to differentiate between continuous and theoretically nonpredictable discontinuous high-frequency price increments. Using the predictions from the model estimated for the aggregate market portfolio in the formulation of simple intraday trading strategies for a set of highly liquid ETFs results in sizeable out-of-sample Sharpe ratios and alphas after accounting for transaction costs. Further dissecting the abnormal intraday returns, we find that most of the superior performance may be traced to periods of high economic uncertainty and a few key factors related to tail risk and liquidity, pointing to slow-moving capital and the gradual incorporation of new information as the underlying mechanisms at work. This paper was accepted by Kay Giesecke, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.01657 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01657},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7731-7751},
  shortjournal = {Manag. Sci.},
  title        = {Intraday market return predictability culled from the factor zoo},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is information risk priced? new evidence from outer space. <em>MS</em>, <em>71</em>(9), 7707-7730. (<a href='https://doi.org/10.1287/mnsc.2023.00713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite images of the parking lots of U.S. retail firms provide information about the firms’ future earnings, and the limited access to these images produces information asymmetry between sophisticated and unsophisticated investors. We construct a cloud-based information risk (CIR) measure to capture this satellite information risk and investigate its asset pricing performance. We find that CIR positively predicts future stock returns of retail firms in the cross-section and that the predictability cannot be explained by weather reasons. We provide evidence that CIR indeed captures the information asymmetry among investors. The profitability of short selling is more pronounced on clear days. The return predictability of CIR is more pronounced in preannouncement periods. During cloudy days, parking lot traffic data from satellite images are harder to obtain, and the retail store sales estimated using parking lot data are noisier. We further show that high CIR is associated with low liquidity and that the decrease in liquidity purchases is greater than the decrease in liquidity sales. Our empirical analyses suggest that information asymmetry is priced and that CIR affects equity premiums through the liquidity channel, which are consistent with the theoretical predictions from a noisy rational expectations equilibrium model under imperfect competition. This paper was accepted by Kay Giesecke, finance. Funding: S. Wang acknowledges financial support from the National Natural Science Foundation of China [Grants 72373110 and 71902140] and Fundamental Research Funds for the Central Universities in China. Y. Wang acknowledges financial support from the National Natural Science Foundation of China [Grants 72342003 and 72071114]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00713 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00713},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7707-7730},
  shortjournal = {Manag. Sci.},
  title        = {Is information risk priced? new evidence from outer space},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Limited time offer and consumer search. <em>MS</em>, <em>71</em>(9), 7692-7706. (<a href='https://doi.org/10.1287/mnsc.2022.00445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines a widely observed yet theoretically underexplored sales tactic: the limited time offer. By focusing on price-directed search markets, we offer a novel perspective on the application of limited time offers by firms and their implications for social welfare. We build a search model where firms advertise dynamic pricing to influence a consumer’s search order. Across various model settings, we demonstrate that limited time offers serve as the most effective pricing tool for firms to achieve search prominence or implement search discrimination, with their impact on social welfare varying depending on the market context. In particular, although existing literature suggests that limited time offers are anticompetitive in markets where prices are unobservable presearch, our findings show that, in markets with observable prices, limited time offers can enhance social efficiency in certain cases by facilitating a socially optimal search order. This paper was accepted by Dmitri Kuksov, marketing. Funding: Z. Gong thanks the National Science Foundation of China [Grants 72332004 and 72373025] for the support of this work. Supplemental Material: The online appendices are available at https://doi.org/10.1287/mnsc.2022.00445 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00445},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7692-7706},
  shortjournal = {Manag. Sci.},
  title        = {Limited time offer and consumer search},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online algorithms for matching platforms with multichannel traffic. <em>MS</em>, <em>71</em>(9), 7674-7691. (<a href='https://doi.org/10.1287/mnsc.2022.00910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-sided platforms rely on their recommendation algorithms to help visitors successfully find a match. However, on platforms such as VolunteerMatch, which has facilitated millions of connections between volunteers and nonprofits, a sizable fraction of website traffic arrives directly to a nonprofit’s volunteering page via an external link, thus bypassing the platform’s recommendation algorithm. We study how such platforms should account for this external traffic in the design of their recommendation algorithms, given the goal of maximizing successful matches. We model the platform’s problem as a special case of online matching, where (using VolunteerMatch terminology) volunteers arrive sequentially and probabilistically match with one opportunity, each of which has a finite need for volunteers. In our framework, external traffic is interested only in their targeted opportunity; by contrast, internal traffic may be interested in many opportunities, and the platform’s online algorithm selects which opportunity to recommend. In evaluating the performance of different algorithms, we refine the notion of competitive ratio by parameterizing it based on the amount of external traffic. After demonstrating the shortcomings of a commonly used algorithm that is optimal in the absence of external traffic, we propose a new algorithm, adaptive capacity ( AC ), which accounts for matches differently based on whether they originate from internal or external traffic. We provide a lower bound on AC ’s competitive ratio that is increasing in the amount of external traffic and that is close to (and, in some regimes, exactly matches) the parameterized upper bound we establish on the competitive ratio of any online algorithm. We complement our theoretical results with a numerical study motivated by VolunteerMatch data where we demonstrate the strong performance of AC relative to current practice and further our understanding of the difference between AC and other commonly used algorithms. This paper was accepted by Omar Besbes, revenue management and market analytics. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00910 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00910},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7674-7691},
  shortjournal = {Manag. Sci.},
  title        = {Online algorithms for matching platforms with multichannel traffic},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference and impact of category captaincy. <em>MS</em>, <em>71</em>(9), 7655-7673. (<a href='https://doi.org/10.1287/mnsc.2023.02039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies category captaincy , a vertical relationship where retailers delegate shelf placement and pricing decisions of an entire product category to one of the leading manufacturers within that category. These contracts involve preferential treatment given to certain brands within the retailer. However, they are confidential, and empirical analysis has been limited. To study the prevalence and welfare impact of these contracts, I develop a novel approach to infer the presence of captaincy contracts based on preferential treatment in shelf placement and pricing. I first classify retailers into different captaincy types based on a brand × retailer specific quality of shelf placement inferred from a demand model. I then conduct pricing tests and find that captains, in line with theoretical predictions and industry anecdotes, eliminate double markups from their own products but not from competitor products. Comparative statics show that captaincy relationships increase the market share of the captain, but they can also increase consumer welfare by about 5% due to the elimination of double markups on the captain’s products. This paper was accepted by Raphael Thomadsen, marketing. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2023.02039 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02039},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7655-7673},
  shortjournal = {Manag. Sci.},
  title        = {Inference and impact of category captaincy},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Planning bike lanes with data: Ridership, congestion, and path selection. <em>MS</em>, <em>71</em>(9), 7631-7654. (<a href='https://doi.org/10.1287/mnsc.2022.00775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban infrastructure is vital for sustainable cities. In recent years, municipal governments have invested heavily in the expansion of bike lane networks to meet growing demand, promote ridership, and reduce emissions. However, reallocating road capacity to cycling is often contentious because of the risk of amplifying traffic congestion. In this paper, we develop a method for planning bike lanes that accounts for ridership and congestion effects. We first present a procedure for estimating parameters of a traffic equilibrium model, which combines an inverse optimization method for predicting driving times with an instrumental variables method for estimating a commuter mode choice model. We then formulate a prescriptive model that selects paths in a road network for bike lane installation while endogenizing cycling demand and driving travel times. We conduct an empirical study on the City of Chicago that brings together several data sets that describe the urban environment—including the road and bike lane networks, vehicle flows, commuter mode choices, bike share trips, driving and cycling routes, demographic features, and points of interest—with the goal of estimating the impact of expanding Chicago’s bike lane network. We estimate that adding 25 miles of bike lanes as prescribed by our model can lift cycling ridership from 3.6% to 6.1%, with at most a 9.4% increase in driving times. We also find that three intuitive heuristics for bike lane planning can lead to lower ridership and worse congestion outcomes, highlighting the value of a holistic and data-driven approach to urban infrastructure planning. This paper was accepted by Karan Girotra, operations management. Funding: Funding: The authors acknowledge funding from the UCLA Anderson Easton Technology Management Center (Siddiq & Zhang) and the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2022-04950] and TD-MDAL Research Grant from the Rotman School of Management (Liu). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00775 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00775},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7631-7654},
  shortjournal = {Manag. Sci.},
  title        = {Planning bike lanes with data: Ridership, congestion, and path selection},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intersectional peer effects at work: The effect of white coworkers on black women’s careers. <em>MS</em>, <em>71</em>(9), 7600-7630. (<a href='https://doi.org/10.1287/mnsc.2022.02010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates how having more White coworkers influences the subsequent retention and promotion of Black women relative to other race-gender groups. Studying 9,037 new hires at a professional services firm, we first document large racial turnover and promotion gaps: Even after controlling for observable characteristics, Black employees are 6.7 percentage points (32%) more likely to turn over within two years and 18.7 percentage points (26%) less likely to be promoted on time than their White counterparts. The largest turnover gap is between Black and White women, at 8.9 percentage points (51%). We argue that initial assignment to project teams is conditionally random based on placebo tests and qualitative evidence. Under this assumption, we show that a one-standard-deviation (20.8 percentage points) increase in the share of White coworkers is associated with a 15.8-percentage-point increase in turnover and an 11.5-percentage-point decrease in promotion for Black women. We refer to these effects as intersectional: Black women are the only race-gender group whose turnover and promotion are negatively impacted by White coworkers. We explore potential causal pathways through which these peer effects may emerge: Black women who were initially assigned to Whiter teams are subsequently more likely to be labeled as low performers and report fewer billable hours, both of which are predictors of higher turnover and lower promotion for all employees. Our findings contribute to the literature on peer effects, intersectionality, and the practice of managing race and gender inequality in organizations. This paper was accepted by Isabel Fernandez-Mateo, organizations. Funding: The authors gratefully acknowledge the financial support of the Center for Equity, Gender, and Leadership at University of California, Berkeley-Haas, Boston University’s Center for Antiracist Research, and the Hub for Equal Representation at the London School of Economics and Political Science. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02010 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02010},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7600-7630},
  shortjournal = {Manag. Sci.},
  title        = {Intersectional peer effects at work: The effect of white coworkers on black women’s careers},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pay, pat, and clawback: Incentivizing service providers’ participation in on-demand digital platforms. <em>MS</em>, <em>71</em>(9), 7579-7599. (<a href='https://doi.org/10.1287/mnsc.2023.02603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical governance challenge for on-demand digital platforms is to increase the participation of their service providers. In this research, we design novel incentive structures by taking the unique features of on-demand digital platforms into account. In 12 micro randomized trials conducted in partnership with a major on-demand digital platform, we examine how combining monetary with nonmonetary incentives and providing them within a loss-aversion framework could motivate service providers to increase their participation levels. We show that in on-demand platforms the nonmonetary incentives inhibit the impact of monetary incentives on service provider participation once they are offered together. Furthermore, in contrast to traditional work settings, offering incentives within a loss-aversion framework only increases the effectiveness of nonmonetary incentives. We provide theoretical explanations and empirical examinations for these counterintuitive results. The insights from this research could be used by on-demand digital platforms to effectively mobilize and sustain their service providers’ participation to meet real-time stochastic demand. This paper was accepted by Anindya Ghose, information systems. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.02603 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02603},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7579-7599},
  shortjournal = {Manag. Sci.},
  title        = {Pay, pat, and clawback: Incentivizing service providers’ participation in on-demand digital platforms},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategy-proofness made simpler. <em>MS</em>, <em>71</em>(9), 7560-7578. (<a href='https://doi.org/10.1287/mnsc.2023.02531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is evidence that many people do not report their types truthfully in strategy-proof mechanisms. One of the leading explanations is mechanism complexity. We propose a novel way of describing any strategy-proof mechanism with the aim to reduce the ensuing cognitive load on players and, hence, to increase truthful reporting. The description highlights that a player is guaranteed to obtain a most preferred feasible allocation based on her reported type and that feasibility depends only on the other players’ reports. Optimality of truthful reporting should therefore become more “apparent.” We experimentally test this prediction using the top trading cycles mechanism for matching applicants to positions. The proposed description increases truth-telling. This increase positively interacts with subject numeracy. This paper was accepted by Dorothea Kübler, behavioral economics and decision analysis. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.02531 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02531},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7560-7578},
  shortjournal = {Manag. Sci.},
  title        = {Strategy-proofness made simpler},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance attribution for portfolio constraints. <em>MS</em>, <em>71</em>(9), 7537-7559. (<a href='https://doi.org/10.1287/mnsc.2024.05365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new performance attribution framework that decomposes a constrained portfolio’s holdings, expected returns, variance, expected utility, and realized returns into components attributable to (1) the unconstrained mean-variance optimal portfolio; (2) individual static constraints; and (3) information, if any, arising from those constraints. A key contribution of our framework is the recognition that constraints may contain information that is correlated with returns, in which case imposing such constraints can affect performance. We extend our framework to accommodate estimation risk in portfolio construction using Bayesian portfolio analysis, which allows one to select constraints that improve—or are least detrimental to—future performance. We provide simulations and empirical examples involving constraints on environmental, social, and governance portfolios. Under certain scenarios, constraints may improve portfolio performance relative to a passive benchmark that does not account for the information contained in these constraints. This paper was accepted by Kay Giesecke, finance. Funding: Research funding from the National Key Research and Development Program of China [Grant 2022YFA1007900], the National Natural Science Foundation of China [Grants 12271013, 72342004], Peking University’s Fundamental Research Funds for the Central Universities, and the MIT Laboratory for Financial Engineering is gratefully acknowledged. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2024.05365 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2024.05365},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7537-7559},
  shortjournal = {Manag. Sci.},
  title        = {Performance attribution for portfolio constraints},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing initial screening for colorectal cancer detection with adherence behavior. <em>MS</em>, <em>71</em>(9), 7516-7536. (<a href='https://doi.org/10.1287/mnsc.2023.01319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-stage screening programs for colorectal cancer (CRC) detection typically involve a first-stage test that evaluates fecal-hemoglobin (f-Hb) concentration in stool samples, with positive results leading to a recommended second-stage diagnostic test (colonoscopy). We explore the design of the first-stage test—specifically the selection of f-Hb cutoffs to report test outcomes—to balance screening effectiveness (CRC and polyp detection) and efficiency (colonoscopy costs), considering that not all individuals follow up with a colonoscopy. We propose an information design model that integrates Bayesian persuasion with information avoidance to address this issue. The model is applied to the design of Singapore’s CRC screening program and calibrated using data from multiple sources, including a nationwide survey of 3,920 respondents in Singapore. Our findings indicate that under certain conditions, using a single cutoff maximizes follow-up adherence, whereas showing exact f-Hb readings optimizes detection effectiveness. Raising the cutoff to 39 μ g / g , as compared with the current practice, could detect 21% more CRC and polyp cases, reduce colonoscopies by 27%, and lower lifetime CRC risk by 11%. This adjustment would reduce public healthcare expenditure by S$20 million and individual spending by S$12 million on average in screening costs. Choosing appropriate cutoffs for the first-stage test can significantly enhance the screening effectiveness while efficiently managing colonoscopy demands. The prevalent practice of using lower cutoffs to achieve high sensitivity may lead to excessive unnecessary colonoscopies and reduced follow-up adherence. This paper was accepted by Scholtes Stefan, healthcare management. Funding: This work was supported by the Ministry of Education - Singapore [Grant 18-C207-SMU-011]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.01319 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01319},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7516-7536},
  shortjournal = {Manag. Sci.},
  title        = {Optimizing initial screening for colorectal cancer detection with adherence behavior},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How should time estimates be structured to increase customer satisfaction?. <em>MS</em>, <em>71</em>(9), 7497-7515. (<a href='https://doi.org/10.1287/mnsc.2023.00137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Businesses across industries, such as food delivery apps and GPS navigation systems, routinely provide customers with time estimates in inherently uncertain contexts. How does the format of these time estimates affect customers’ satisfaction? In particular, should companies provide customers with a point estimate representing the best estimate, or should they communicate the inherent uncertainty in outcomes by providing a range estimate? In eight preregistered experiments ( N = 5,323), participants observed time estimates provided by an app, and we manipulated whether the app presented the time estimates as a point estimate (e.g., “Your food will arrive in 45 minutes.”) or a range (e.g., “Your food will arrive in 40–50 minutes.”). After participants learned about the app’s prediction performance by sampling a set of past outcomes, we measured participants’ evaluation of the app. We find that participants judged the app more positively when it provided a range rather than a point estimate. These results held across different domains, different time durations, different underlying outcome distributions, and an incentive-compatible design. We also find that this preference is not simply due to people’s dislike of late outcomes, as participants also rated ranges more positively than conservative point estimates corresponding to the upper (i.e., later) bound of the range. These findings suggest that companies can increase customer satisfaction with realized time estimates by communicating the uncertainty inherent in these time estimates. This paper was accepted by Jack Soll, behavioral economics and decision analysis. Funding: This research was supported by the Bakar Faculty Fellowship at the Haas School of Business at UC Berkeley and the Beatrice Foods Co. Faculty Research Fund at the University of Chicago Booth School of Business. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00137 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00137},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7497-7515},
  shortjournal = {Manag. Sci.},
  title        = {How should time estimates be structured to increase customer satisfaction?},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segment Profit/Loss and the limitations of a “Management approach”. <em>MS</em>, <em>71</em>(9), 7474-7496. (<a href='https://doi.org/10.1287/mnsc.2023.01224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GAAP’s “Management Approach” to segment reporting (ASC 280) requires firms to report segment profit/loss as the measure managers use when allocating resources internally across segments under the assumption that investors benefit from viewing segment performance through the eyes of management. We examine how segment performance measurement aligns with the way that investors use segment profit/loss measures externally. We first survey professional investors and find that they primarily use segment profit/loss to assess firm value and therefore desire measures focused on persistent income items that help predict future performance. Next, for a sample of public multi-segment firms from 2003 to 2018, we hand-collect details on the construction of firms’ segment profit/loss measures and compare them to the same firms’ valuation-focused measures (i.e., non-GAAP earnings and non-GAAP segment profit/loss). Relative to non-GAAP measures, ASC 280 segment profit/loss is significantly more likely to include less persistent items (e.g., restructuring charges) and exclude more persistent items (e.g., interest expense). ASC 280 segment profit/loss is also less predictive of future performance and less value relevant than non-GAAP measures. These differences are attributable to ASC 280 yielding segment profit/loss measures that focus on the controllability of items by segment managers instead of items’ persistence. Overall, ASC 280’s segment profit/loss exhibits misalignment between its internal use by managers (i.e., internal resource allocation) and its primary external use by investors (i.e., valuation), which limits its usefulness for investors. Our study informs market participants and standard setters considering the usefulness of segment disclosures. This paper was accepted by Suraj Srinivasan, accounting. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.01224 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01224},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7474-7496},
  shortjournal = {Manag. Sci.},
  title        = {Segment Profit/Loss and the limitations of a “Management approach”},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Economies before scale: I.T. strategy and performance dynamics of young U.S. businesses. <em>MS</em>, <em>71</em>(9), 7449-7473. (<a href='https://doi.org/10.1287/mnsc.2019.03116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine how dimensions of information technology (IT) strategy affect the performance of young businesses, as well as dynamics as they age. Drawing from lifecycle theory and firm boundary research, we derive the relationship between age-based performance differences and IT sourcing decisions. We highlight the dynamic tension between outsourcing’s support for accessing frontier inputs in the short term and ownership’s advantages for developing organization-specific resources and capabilities over time. Leveraging a large panel of Census Bureau microdata from 2006 to 2014, we provide the first systematic evidence that young manufacturing establishments (both startups and new units of existing firms) disproportionately benefit from modern IT outsourcing (ITO). The young also enjoy productivity benefits from owned IT capital (ITK), despite high uncertainty, smaller operational scale, and less complementary organizational capital. Although these returns appear commensurate with those of older producers, they are conditional on survival, which is improved by ITO but harmed by ITK accumulation. Combining flexibility-related gains from ITK with vintage-related advantages in early ITK investment, the young are revealed to have significantly greater IT productivity than older incumbents. A large battery of tests supports a causal interpretation, as well as mechanisms rooted in mitigating the effects of uncertainty (as opposed to size- or cost-related reasons). These findings illuminate an often-overlooked pattern of young-business dynamism relevant to economic trends and management practices in an increasingly digital age. This paper was accepted by DJ Wu, information systems. Funding: This work was supported by the Social Sciences and Humanities Research Council (SSHRC) of Canada, the National Bureau of Economic Research, and the Stanford Digital Economy Lab. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2019.03116 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2019.03116},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7449-7473},
  shortjournal = {Manag. Sci.},
  title        = {Economies before scale: I.T. strategy and performance dynamics of young U.S. businesses},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal match recommendations in two-sided marketplaces with endogenous prices. <em>MS</em>, <em>71</em>(9), 7431-7448. (<a href='https://doi.org/10.1287/mnsc.2022.02691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many two-sided marketplaces rely on match recommendations to help customers find suitable service providers at suitable prices. This paper develops a tractable methodology that a platform can use to optimize its match recommendation policy to maximize the total value generated by the platform while accounting for the endogeneity of transaction prices, which are set by the providers based on supply and demand and can depend on the platform’s match recommendation policy. Despite the complications of price endogeneity, an optimal match recommendation policy has a simple structure and can be computed efficiently. In particular, an optimal policy always recommends the providers who deliver the highest conversion rates. Moreover, an optimal policy can be encoded simply in terms of the frequency of recommending each provider to each customer segment, without the need to encode which subsets of providers are to be recommended together. On the other hand, if the platform were to optimize its match recommendations without accounting for price endogeneity, then the resultant policy would be more complex, and the market is likely to get stuck at a strictly suboptimal outcome, even if the platform were to continually reoptimize its match recommendations after prices re-equilibrate. This paper was accepted by Omar Besbes, revenue management and market analytics. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2022.02691 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02691},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7431-7448},
  shortjournal = {Manag. Sci.},
  title        = {Optimal match recommendations in two-sided marketplaces with endogenous prices},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fearless woman: Financial literacy, confidence, and stock market participation. <em>MS</em>, <em>71</em>(9), 7414-7430. (<a href='https://doi.org/10.1287/mnsc.2023.00425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Women are less financially literate than men, and it has been difficult to determine whether this gap reflects a lack of knowledge or, rather, a lack of confidence. To address this important research question, we designed two survey modules that enable us to calculate the extent to which confidence matters for both financial literacy and behavior. We developed and estimated a model that provides a new measure of financial literacy and disentangles confidence from knowledge. We find that confidence accounts for about 30% of the gender difference in financial literacy. Moreover, both financial knowledge and confidence are linked to stock market participation. We also provide researchers with a method to account for confidence in regressions. This paper was accepted by Bo Becker, finance. Funding: This work was supported by the European Investment Bank Institute (n/a), which is gratefully acknowledged. The findings and views presented in this article are entirely those of the authors and should not be attributed in any manner to the European Investment Bank or its Institute or to the European Central Bank, the Eurosystem, or De Nederlandsche Bank. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00425 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00425},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7414-7430},
  shortjournal = {Manag. Sci.},
  title        = {Fearless woman: Financial literacy, confidence, and stock market participation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cyberattacks, operational disruption, and investment in resilience measures. <em>MS</em>, <em>71</em>(9), 7390-7413. (<a href='https://doi.org/10.1287/mnsc.2022.00430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increased frequency and magnitude of cyberattacks, policymakers and the private sector search for ways to counter this threat. One of the main initiatives suggested to achieve this goal is sharing cybersecurity-related information. Although the general belief is that information sharing can increase both industry profit and social welfare, it is unclear whether firms would voluntarily share such information. In this paper, we examine the incentives of firms to share cybersecurity-related information, how information sharing impacts investments in cyber resilience, and the aggregate impact on welfare. We find that firms only voluntarily share information in less competitive markets when the impact of the disruption is high. In all other cases, firms elect not to share information, despite potential welfare benefits. To facilitate information sharing, we investigate an exclusionary policy (i.e., sharing must be mutual) and demonstrate market conditions under which this policy incentivizes information sharing. However, when competition is intense, even the exclusionary policy is ineffective because it reduces industry profit. To inform stronger interventions, we examine firms being mandated to disclose their private cyberthreat information. We demonstrate that an opportunity does exist for such disclosure, particularly when the cost of investing in cyber resilience is high. However, policymakers must use caution with such a policy because applying this intervention when investment costs are not high leads to a steep reduction in welfare. This paper was accepted by D. J. Wu, information systems. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2022.00430 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00430},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7390-7413},
  shortjournal = {Manag. Sci.},
  title        = {Cyberattacks, operational disruption, and investment in resilience measures},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relational resonance and content creation. <em>MS</em>, <em>71</em>(9), 7366-7389. (<a href='https://doi.org/10.1287/mnsc.2023.00830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates how tie formation shapes the production of user-generated content on social media, offering insights into enhancing user engagement and alleviating social media fatigue. We conceptualize tie formation as a relationship codevelopment process involving a tie initiator and a respondent. Drawing on social psychology literature, we propose a framework that traces how initiators’ social cognition evolves throughout this process—from proactively extending an inviting social tie to receiving a responsive one in return. Central to our framework is a perceptual mechanism we term relational resonance , which fosters intrinsic motivation and prompts initiators to adjust their content creation. Using data from YouTube, we develop a Bayesian hierarchical model to identify individual creators’ content adjustments, measured by changes in video volume and viewer engagement. Our results reveal that while initiators tend to reduce content production after receiving a responsive tie, the videos they create generate higher engagement, with “like” and “subscribe” rates increasing by 1.1 and 2.9 percentage points, respectively. Remarkably, the receipt of a responsive tie leads to average weekly increases of 66 likes and 183 subscriptions even after accounting for the decrease in video volume, reinforcing the “quality over quantity” principle in content creation strategies. An online experiment further elucidates the mechanism underlying the observed adjustment behavior, providing robust validation for our proposed framework. Implications for research and practice are discussed. This paper was accepted by D.J. Wu, information systems. Funding: T. Song was supported by National Natural Science Foundation of China [Grants 71902114 and 72372101]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00830 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00830},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7366-7389},
  shortjournal = {Manag. Sci.},
  title        = {Relational resonance and content creation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auctions of auctions. <em>MS</em>, <em>71</em>(9), 7347-7365. (<a href='https://doi.org/10.1287/mnsc.2024.05233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online advertising impressions are traded through a multitiered network of intermediaries. We model the revenue optimization of a publisher that auctions off advertising impressions to intermediary exchanges, which, in turn, run their own internal auctions among the advertisers they represent. We show that the resulting auction-of-auctions market arrangement suffers from a double marginalization problem: the multitier bid shading prompts the publisher to raise the reserve price above the level under the nonintermediated benchmark. The reserve distortion decreases channel efficiency as well as the profits of all channel members. Our findings also demonstrate that, in the auction of auctions, reserve price optimization is both more intricate and more important than in the direct mechanism: optimal reserves depend on the number of both exchanges and advertisers. Moreover, optimizing the reserve price given a fixed number of bidders may be more profitable than keeping the auction absolute and adding more bidders. Interestingly, we find that a simple scheme in which exchanges coordinate to charge their advertisers a fixed percentage commission perfectly coordinates the channel. This paper was accepted by Dmitri Kuksov, marketing. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2024.05233 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2024.05233},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7347-7365},
  shortjournal = {Manag. Sci.},
  title        = {Auctions of auctions},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The crowdfunding effects on venture capital investment. <em>MS</em>, <em>71</em>(9), 7333-7346. (<a href='https://doi.org/10.1287/mnsc.2022.00994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the impact of crowdfunding on venture capital (VC) investments in the presence of competition among VC firms. Our economic model comprises a startup, a crowdfunding platform, and two VC firms, each with its own perception of the startup’s potential. The startup seeks equity funding from the VC firms and decides on the size of its equity offer. If the VC firms decline to invest, the startup pivots to crowdfunding. Following the crowdfunding campaign, all firms update their beliefs about the startup’s likelihood of success based on the crowdfunding outcome, prompting the VC firms to revisit their investment decisions, now with a reduced equity offer if the crowdfunding outcome was positive. This study provides theoretical underpinnings at the firm level for the observed aggregate-level empirical relationships, both positive and negative, between crowdfunding and VC investments. Specifically, based on our model, the positive relationship, where increased crowdfunding activity leads to a rise in subsequent VC investments, is attributed to startups that fail to secure VC funding in the absence of a crowdfunding platform but succeed in attracting VC attention after a successful crowdfunding campaign. In contrast, the negative relationship is attributed to highly valued startups that could have secured VC funding without crowdfunding, but with crowdfunding, VC firms choose to defer their investment decision until after the crowdfunding campaign’s result is known; on the one hand, a successful crowdfunding outcome lowers their post-crowdfunding VC investment demand, and on the other hand, an unsuccessful outcome deters potential VC investors. Besides these relationships, we also identify another dynamic, where the option of accessing crowdfunding raises VC investment, even if the startup does not actually launch a crowdfunding campaign. This paper was accepted by Sridhar Tayur, entrepreneurship and innovation. Funding: This work was supported by the Natural Sciences and Engineering Research Council of Canada [Grants RGPIN-2015-06757 and RGPIN-2021-04295] and the National Natural Science Foundation of China [Grant 71901135]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2022.00994 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00994},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7333-7346},
  shortjournal = {Manag. Sci.},
  title        = {The crowdfunding effects on venture capital investment},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graphical point process framework for understanding removal effects in multi-touch attribution. <em>MS</em>, <em>71</em>(9), 7312-7332. (<a href='https://doi.org/10.1287/mnsc.2023.00457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marketers rely on various online advertising channels to reach customers and are increasingly interested in multi-touch attribution , which evaluates the contribution of each touchpoint to a conversion. However, as the numbers of marketing channels and touchpoints increase, the attribution challenge becomes more intricate because of the complex interplay among different touchpoints within and across channels. Utilizing customer path-to-purchase data, this article addresses this challenge by developing a novel graphical point process framework to investigate the relational structure among various touchpoints. Based on this framework, we propose graphical attribution methods that allocate attribution scores to individual touchpoints or corresponding channels for each customer’s path to purchase. These scores are calculated using a probabilistic definition of removal effects. We evaluate the proposed methods and compare their performance with commonly used attribution models through extensive simulation studies and a real-world attribution application. This paper was accepted by J. George Shanthikumar, data science. Funding: This work was supported by the National Science Foundation [Grant DMS-2210775]. Additionally, the research of L. Xue was partly supported by the National Science Foundation [Grants DMS-1811552 and DMS-1953189]. Supplemental Material: The electronic companion and data files are available at https://doi.org/10.1287/mnsc.2023.00457 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00457},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7312-7332},
  shortjournal = {Manag. Sci.},
  title        = {A graphical point process framework for understanding removal effects in multi-touch attribution},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of lockups in stock mergers. <em>MS</em>, <em>71</em>(9), 7286-7311. (<a href='https://doi.org/10.1287/mnsc.2023.00993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We document the frequent use of stock lockup agreements in mergers and acquisitions (M&As) paid in stock and examine the corporate determinants and consequences of the use and duration of lockups. Lockup agreements prohibit target shareholders from selling shares issued by the acquirer as a means of payment for a prespecified period. We find support for the hypothesis that target shareholders agree to lockups to precommit to hold on to the acquirer’s stock if they believe the merger’s long-term fundamentals are strong. Consistent with our hypothesis, lockups come with larger acquirer announcement returns, particularly when acquirer valuations are high; ex ante, lockup adoption likelihood increases with acquirers’ valuation. Lockups also come with higher deal completion likelihood, shorter merger negotiations, and higher long-term operating performance. We conclude the market interprets lockups as a signal of strong fundamentals, particularly when acquirers’ valuations are high. This paper was accepted by Bo Becker, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00993 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00993},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7286-7311},
  shortjournal = {Manag. Sci.},
  title        = {The role of lockups in stock mergers},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intertemporal pricing with resellers: An empirical study of product drops. <em>MS</em>, <em>71</em>(9), 7263-7285. (<a href='https://doi.org/10.1287/mnsc.2022.04152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product drops occur when a retailer releases a limited-edition product line on a specific date for a short period of time. Due to limited inventory of the product and the short sales horizon, a resale market emerges where products may resell at higher prices once the firm stocks out. The firm faces a central tradeoff: pricing too high may lead to a longer stockout time and future markdowns; pricing too low may lead to transfer of profits to resellers. Any firm in this position may ask, “How do resellers impact my profit?” To answer this question, we build a dynamic structural model that captures customer decisions to consume or strategically resell, as well as the firm’s intertemporal pricing decision in the presence of resellers. We estimate our model using a unique data set from a retailer of baby clothing with weekly product drops, where customers engage in a resale marketplace through Facebook groups. We find resale data collection and analysis is valuable to incorporate into pricing as firm profit increases by 8.8%, and reseller presence reduces firm profit by 5.8% even if resale data are used in pricing. These impacts are magnified by resale market growth, demonstrating that managers need to take action as resale markets gain wider adoption. This paper was accepted by Victor Martínez-de-Albéniz, operations management. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.04152 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.04152},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7263-7285},
  shortjournal = {Manag. Sci.},
  title        = {Intertemporal pricing with resellers: An empirical study of product drops},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delegation with technology migration: An empirical analysis of mobile virtual network operators. <em>MS</em>, <em>71</em>(9), 7244-7262. (<a href='https://doi.org/10.1287/mnsc.2021.01832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the impact of mobile virtual network operators (MVNOs) on the performance of mobile network operators (MNOs) in the presence of overlapping generations of wireless mobile network technologies (2G and 3 G). MVNOs distribute MNOs’ mobile services to customers without owning any spectrum or network infrastructures, either as a subsidiary (branded MVNOs) or delegated under wholesale agreements with MNOs (third-party MVNOs). We investigate MVNO value chain governance, that is, delegation, in the context of overlapping generations of technologies. Leveraging a proprietary data set, we design a quasi-experiment to analyze the impact of branded and third-party MVNOs in 2G and 3 G market expansion. Using first a difference-in-differences analysis and then dynamic propensity score matching, we find that an MNO’s 2G market share increases if the MNO launches a third-party MVNO, whereas its 3 G market share increases if it launches a branded MVNO. More importantly, the positive effects of branded MVNOs diminish over time, but those of third-party MVNOs become stronger over time. For managers challenged by managing overlapping generations of technologies, it is important to customize value chain governance for market expansion: to create branded MVNOs to explore new mobile technologies, and to delegate to third-party MVNOs to exploit existing technology markets when the technology market is mature. This paper was accepted by Jayashankar Swaminathan, operations management. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.01832 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.01832},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7244-7262},
  shortjournal = {Manag. Sci.},
  title        = {Delegation with technology migration: An empirical analysis of mobile virtual network operators},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). You’ve got mail! the late 19th century U.S. postal service expansion, firm creation, and firm performance. <em>MS</em>, <em>71</em>(9), 7223-7243. (<a href='https://doi.org/10.1287/mnsc.2023.03369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the impact of the expansion of the U.S. Postal Service in the late 19th century on firm creation and performance. Utilizing newly digitized archival data on historic business establishments, post office locations, and road networks in California, our study identifies a positive relationship between the expansion of the Postal Service and the emergence of new firms. To address endogeneity concerns, we leverage an unexpected change in the Postal Service route network. Our findings suggest that the Postal Service played a significant role in facilitating firm entry by acting as a carrier of specialized knowledge rather than as a financial service or mass communication infrastructure. We further reveal that, although increased competition from new entrants generally exerted downward pressure on incumbent firms, those relying on specialized knowledge and public technology inputs significantly benefited from local Postal Service access. Taken together, our study underscores the critical role played by the Postal Service in knowledge diffusion and local economic development by enabling the sourcing of specialized knowledge and technologies from other geographies. Overall, our results contribute to a broader understanding of how communication and knowledge dissemination infrastructure can drive entrepreneurship and firm growth, carrying important implications for contemporary discussions on infrastructure development, its potential to stimulate entrepreneurial activity, innovation, and foster local economic communities. This paper was accepted by Olav Sorenson, organizations. Funding: The authors gratefully acknowledge funding from the Harvard Business School Division of Research and Faculty Development. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.03369 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.03369},
  journal      = {Management Science},
  month        = {9},
  number       = {9},
  pages        = {7223-7243},
  shortjournal = {Manag. Sci.},
  title        = {You’ve got mail! the late 19th century U.S. postal service expansion, firm creation, and firm performance},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="msom">MSOM - 20</h2>
<ul>
<li><details>
<summary>
(2025). Advance selling and upgrading in priority queues. <em>MSOM</em>, <em>27</em>(5), 1664-1682. (<a href='https://doi.org/10.1287/msom.2023.0410'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We study advance selling and upgrading in a priority queue setting that emerges in the amusement park industry. Customers choose to buy fast-track or regular tickets depending on their heterogeneous waiting costs. At the park entrance, customers can choose tickets based on the observed congestion levels. Customers who purchase tickets in advance suffer from congestion uncertainty. Upgrading options allow customers to purchase regular tickets in advance and upgrade on-site if the congestion turns out to be high. The seller’s objective is to find the optimal pricing scheme to maximize revenue. Methodology/results : We analyze a two-stage model in a queueing game framework. Under full coverage and fixed regular ticket prices, the seller sets the priority ticket prices to maximize the revenue. We investigate two pricing schemes, both with and without upgrading options, under different scenarios depending on the proportion of customers who have access to advance ticket purchasing. When all customers have access to advance ticket purchasing, our findings indicate that the advance-and-spot selling scheme yields the same maximum revenue as the scheme that includes the upgrading option if the spot regular ticket price is not too large; thus, the upgrading option becomes unnecessary. Otherwise, allowing upgrading generates more revenue (and social welfare). Interestingly, although upgrading options offer consumers greater flexibility, they reduce consumer surplus in scenarios where upgrading increases revenue. When some customers only purchase tickets on the spot, our numerical analysis shows that allowing upgrading results in higher revenue when the probability of high system congestion is large and highlights cases when the revenue gap is significant. Finally, we extend our analysis to account for customers’ balking behavior and the additional waiting time incurred when purchasing tickets on the spot, providing a robust check on scheme comparison. Managerial implications : Our paper is one of the first to investigate—in the context of queueing economics—the advance selling strategy together with upgrading option, taking into account arrival rate uncertainty. Our results offer guidance on selling scheme design for ticket sellers and provide one economic explanation for the various pricing policies seen in the amusement park industry. Funding: Financial support from the National Natural Science Foundation of China [Grants 72101248, 72471215, 72122019, 72471005, and 72031006] is gratefully acknowledged. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0410 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0410},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1664-1682},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Advance selling and upgrading in priority queues},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiproduct dynamic pricing with reference effects under logit demand. <em>MSOM</em>, <em>27</em>(5), 1645-1663. (<a href='https://doi.org/10.1287/msom.2024.0801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We consider the dynamic pricing problem of multiple products under (asymmetric) reference effects over an infinite horizon. Unlike existing literature, which is mostly focused on the single-product setting, our multiproduct setting takes into account the cross-product effects among substitutes and incorporates the memory-based reference prices into the multinomial logit (MNL) demand model. Even with the single-product logit demand, the structure of the optimal pricing policy is intractable. Therefore, we focus on the long-run patterns of the optimal pricing policy and also discuss the performance of the myopic pricing policy. Methodology/results : We first provide a comprehensive characterization of the myopic pricing policy, including its solution, long-run convergence behavior, and optimality gap. For the optimal pricing policy, we show an intricate connection between its long-run dynamics and types of reference effects. We demonstrate that the presence of any gain-seeking product renders a long-run constant pricing policy suboptimal. Conversely, the constant policy (or optimal steady state) can exist in both loss-neutral and loss-averse scenarios, where we provide a sufficient condition for such existence and give the analytical expression for the optimal steady state. We further show that when pricing perfect substitutes, the true optimal policy under the multiproduct framework is more likely to yield a long-run cyclic pattern than the policy derived from the single-product framework, a phenomenon that aligns well with the periodic discounts in real-world markets. Managerial implications : This discrepancy in the long-run behaviors between multi- and single-product-based policies highlights the importance of employing the multiproduct framework and addressing the cross-product effects, as sticking to the single-product framework while managing multiple substitutes can misrepresent long-run dynamics and result in suboptimality. In the multiproduct domain, our model suggests that retailers are more likely to benefit from appropriate price variations than maintaining a constant pricing policy. Funding: H. Jiang acknowledges support from the Natural Sciences and Engineering Research Council of Canada [NSERC Discovery Grant RGPIN 2024 05796]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2024.0801 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2024.0801},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1645-1663},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Multiproduct dynamic pricing with reference effects under logit demand},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coastal groundwater management: Seawater intrusion prevention, artificial recharge, and climate adaptation. <em>MSOM</em>, <em>27</em>(5), 1625-1644. (<a href='https://doi.org/10.1287/msom.2022.0441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Most of the world’s population relies on coastal aquifers for freshwater supplies. Groundwater is experiencing substantial overdrafts and facing ever-mounting freshwater demand. Existing groundwater management strategies are myopic and fail to coordinate production and the operation of protection approaches, including seawater intrusion barriers (SWIBs) and managed aquifer recharge (MAR). Motivated by the urgency of sustainable groundwater management, we investigate how to optimize the joint operations of groundwater production, protection (by injecting fresh water through SWIBs), and replenishment (via MAR). Methodology/results : We model a central planner’s decision on groundwater production, freshwater injection quantities, and artificial replenishment using stochastic dynamic models and identify that the optimal groundwater management policies follow a threshold-type structure. We find that SWIBs and MAR are strategic complements, except in cases with very high groundwater levels, when they turn into strategic substitutes. When the penalty for low groundwater levels decreases, the planner should use SWIBs more aggressively if groundwater levels are low and more conservatively if they are high. A similar pattern holds when natural recharge becomes more abundant, assuming that the natural recharge quantity has no impact on the purchasing cost of imported water. Moreover, we calibrate our model using real data sets in Orange County, California and find that the joint operations of SWIBs and MAR expand groundwater operational flexibility. In contrast, employing SWIBs alone comes at the expense of a lower groundwater level. Managerial implications : Our analysis offers strategic guidance on when to use SWIBs and MAR as complements or substitutes based on groundwater levels. It highlights the value of their joint operation in stabilizing groundwater, especially amid worsening droughts. Funding: This work was partially supported by the National Natural Science Foundation of China [Grants 72242106, 72242107, and 72188101]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0441 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0441},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1625-1644},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Coastal groundwater management: Seawater intrusion prevention, artificial recharge, and climate adaptation},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cheap thrills! an empirical analysis of the impact of supercenters on consumer waste. <em>MSOM</em>, <em>27</em>(5), 1604-1624. (<a href='https://doi.org/10.1287/msom.2023.0207'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : To generate more sales, retailers are incentivized to open large stores. However, large store formats can carry negative environmental externalities. Entry of large stores in a community may stimulate mass consumption through its impact on consumer behaviors and local competition. In this study, we examine the local impact on the amount of consumer waste generated following an expansion of retail Supercenters. Methodology/results : We leverage the staggered expansion of Walmart Supercenters and adopt a difference-in-differences approach to investigate the impact of Supercenter entry on consumer waste. Our difference-in-differences estimates suggest that Supercenter entry results in up to a 6.97% increase in consumer waste in the affected counties. The increase is larger for new Supercenter launches compared with Supercenter conversions. In addition, we examine the roles of convenience stores and circular economy channels in mitigating the impact of Supercenter expansion. Managerial implications : For policymakers, our results also highlight a silver lining: the negative environmental effect of Supercenter entry can be mitigated through a balanced retail strategy that includes both convenience stores and Supercenters. Furthermore, we show that developing and promoting local circular economy channels may also mitigate the Supercenter effect. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0207 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0207},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1604-1624},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Cheap thrills! an empirical analysis of the impact of supercenters on consumer waste},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing emission permits regulation for multiple compliance periods. <em>MSOM</em>, <em>27</em>(5), 1587-1603. (<a href='https://doi.org/10.1287/msom.2023.0341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This paper investigates the effectiveness of emission permits policies with varying levels of temporal flexibility, including different compliance periods and the allowance/prohibition of intertemporal banking and borrowing. We also explore advanced policy designs aimed at improving social welfare. Methodology/results : Our analysis reveals that extreme policies, such as fully flexible or fully nonflexible permits, are generally suboptimal from a welfare perspective. Restricting temporal flexibility can be beneficial, particularly when pollution damage is severe or production costs are highly volatile. This finding arises from the tradeoff between the positive industrial impacts and the negative environmental consequences of temporal flexibility. We further demonstrate that partially flexible permits—such as transfer cap/discount and permits-tax hybrid—can achieve a better balance between industrial efficiency and environmental protection, even surpassing the classic welfare bound associated with fully flexible permits. Additionally, we find that temporal flexibility may reduce overall industrial profits by intensifying market competition, and allocating additional permits can widen profit gaps among firms. Managerial implications : This study provides actionable insights for regulators aiming to enhance emission permits policies and offers a deeper understanding of their impacts on firms’ profitability and operations. Funding: X. Fu acknowledges financial support from the University of New South Wales [Start-Up Grant, UNSW Business School Dean’s Research Fellowship]. P. Gao’s research is supported by the National Natural Science Foundation of China [Grants 72201234 and 72192805], Collaborative Research Funds [Grant C6032-21G] of the Hong Kong Research Grants Council, and the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [Grant 2023B1212010001]. Y.-J. Chen acknowledges financial support from the Hong Kong Research Grants Council [Grants 16501722, 16212821, and C6020-21GF]. G. Gallego is supported by Collaborative Research Funds [Grant C6032-21G] of the Hong Kong Research Grants Council. M. Lu is supported by Collaborative Research Funds [Grants C6032-21G, C5004-23G, and C5002-22Y] and the General Research Fund [Grant 16300424] of the Hong Kong Research Grants Council. This work also contributes to the UNESCO-endorsed program “Seamless Prediction and Services for Sustainable Natural and Built Environments (SEPRESS) 2025–2032” under their International Decade of Science for Sustainable Development. Supplemental Material: The online appendices are available at https://doi.org/10.1287/msom.2023.0341 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0341},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1587-1603},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Designing emission permits regulation for multiple compliance periods},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of automation on workers when workers are strategic: The case of ride-hailing. <em>MSOM</em>, <em>27</em>(5), 1571-1586. (<a href='https://doi.org/10.1287/msom.2023.0416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Motivated by the behavior of drivers on ride-hailing platforms (individual drivers decide whether to work based on the offered wage and where to locate themselves in anticipation of future fares), we examine how the introduction of autonomous vehicles impacts the strategic behavior of human drivers and driver welfare. Specifically, we consider a setting in which a ride-hailing platform deploys a mixed fleet of conventional vehicles (CVs) and autonomous vehicles (AVs). The CVs are operated by human drivers who make independent decisions about whether to work for the platform and where to position themselves when they become idle. The AVs are under the control of the platform. The platform decides on the wage it pays the drivers, the size of the AV fleet, and how the AVs are positioned spatially when they are idle. The platform can also make decisions on whether to prioritize the AVs or the CVs in assigning vehicles to customer requests. Methodology/results : We use a fluid model to characterize the optimal decisions of the platform and contrast those with the optimal decisions in the absence of AVs. We examine the impact of automation on strategic drivers and the ride-hailing platform. We show that, although the introduction of AVs can displace drivers and depress effective wages, there are settings in which the introduction of AVs leads to higher effective wages and more drivers being hired. We discuss how these results can, in part, be explained by the interplay of two counteracting effects: (i) the introduction of AVs provides the platform with an additional source of supply and renders human driver substitutable (displacement effect), and (ii) having access to and control over AVs enables the platform to influence the strategic behavior of CVs, thereby reducing the inefficiency from self-interested behavior (incentive effect). The relative strength of these two effects depends on the cost of AVs and the vehicle dispatching policy. Managerial implications : Our results uncover a new effect through which the introduction of AVs affects the welfare of human drivers (the incentive effect) and another mechanism to mitigate inefficiencies because of human drivers acting strategically. Our results have potentially broader applications to other areas in which automation is introduced and workers are strategic. Funding: This work was supported by the National Science Foundation [Grant SCC-1831140]. The Guangdong (China) Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [2023B1212010001]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0416 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0416},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1571-1586},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {The impact of automation on workers when workers are strategic: The case of ride-hailing},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Market formation, pricing, and value generation in ride-hailing services. <em>MSOM</em>, <em>27</em>(5), 1551-1570. (<a href='https://doi.org/10.1287/msom.2022.0502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We empirically study the market for ride-hailing services. In particular, we explore the following questions: (i) How do the two-sided market and prices jointly form in ride-hailing marketplaces? (ii) Does surge pricing create value, and for whom? How can its efficiency be improved? (iii) Can platforms’ strategy on revenue sharing with drivers be improved? (iv) What is the value generated by ride-hailing services, including hosting rival taxi services on ride-hailing apps? Methodology/results : We develop a discrete choice model for the formation of mutually dependent demand (customer side) and supply (driver side) that jointly determine pricing. Using this model and a comprehensive data set obtained from the largest mobile ride platform in China, we estimate customer and driver price elasticities and other factors that affect market participation for the company’s two main markets, namely, basic ride-hailing and taxi services. Based on these estimation results and counterfactual analysis, we demonstrate that surge pricing improves customer and driver welfare as well as platform revenues while counterintuitively reducing taxi revenues on the platform. However, surge pricing should be avoided during nonpeak hours because it can hurt both customer and platform surplus. We show that platform revenues can be improved by increasing drivers’ revenue share from the current levels. Finally, we estimate that the platform’s basic ride-hailing services generated customer value equivalent to $13.25 billion in China in 2024, and hosting rival taxi services on the platform boosted customer surplus by $3.6 billion. Managerial implications : Our empirical framework provides ride-hailing companies a way to estimate demand and supply functions, which can help with optimization of multiple aspects of their operations. Our findings suggest that ride-hailing platforms can improve profits by containing surge-pricing to peak hours only and boosting supply by increasing driver compensation. Finally, our results demonstrate that restricting ride-hailing services create significant welfare losses, whereas including taxi services on ride-hail platforms generates substantial economic value. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0502 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0502},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1551-1570},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Market formation, pricing, and value generation in ride-hailing services},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Managing quality on two-sided platforms in the presence of provider competition. <em>MSOM</em>, <em>27</em>(5), 1532-1550. (<a href='https://doi.org/10.1287/msom.2023.0326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We study two-sided markets in which competing platforms enforce service standards to control access of providers with heterogeneous service quality and employ pricing strategies to balance supply and demand. We further investigate the effectiveness of launching regulations, aimed to maximize social welfare, in enhancing quality, and we examine multihoming to yield additional insights. Methodology/results : We build a game-theoretic model wherein two platforms enforce service standards and prices, based on which heterogeneous providers and consumers decide whether and which platform to enroll. The transaction revenue from service matches is shared between the platform and the providers according to a pricing scheme, which comprises a service fee and a commission rate. Our results reveal that platforms’ strategies for balancing supply and demand depend on the consumer-to-provider ratio (termed as consumer size) and the value of high-quality service relative to that of low-quality service (termed as service value). Managerial implications : The standards enforced by platforms are not always monotone with respect to consumer size or service value. A large influx of consumers prompts platforms to enforce a high standard when service value is sufficiently high. Platforms can enforce different service standards, albeit only when they compete to balance providers and consumers. Platform competition can be a substitute for regulation in upholding and enhancing quality, especially when the commission rate is high. Regulation is more effective in enhancing quality on a monopolistic platform than on competing platforms. Relative to single homing, multihoming has inconsequential effects on the pattern for the enforcement of service standards, whereas it may lead platforms to raise prices. We alert regulators to consumer size, service value, and pricing scheme in addressing quality concerns in two-sided markets. Fostering competition can be more effective than launching regulations to enhance quality on platforms. Funding: The research of L. Jiang is supported in part by the National Natural Science Foundation of China [Grant 72171204] and the Research Grants Council of the Hong Kong General Research Fund [Grant PolyU15500922]. The research of X. Zhao is supported in part by the Natural Sciences and Engineering Research Council of Canada [Discovery Grant 06690], the Einwechter Faculty Research Grant, and the Lazaridis Institute Seed Fund. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0326 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0326},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1532-1550},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Managing quality on two-sided platforms in the presence of provider competition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing supply with demand on ride-hailing platforms in markets with price regulations: An operational approach. <em>MSOM</em>, <em>27</em>(5), 1515-1531. (<a href='https://doi.org/10.1287/msom.2022.0216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Ride-hailing platforms face a supply-demand imbalance. During peak periods, the demand from passengers far exceeds the supply of drivers, whereas during off-peak periods, there is an abundance of supply but weak demand. The well-studied surge pricing can be challenging to implement in markets where prices are subject to regulation and are inflexible to adjust. We study an alternative operational approach that shifts the supply of drivers from off-peak to peak periods to address the supply-demand imbalance. Methodology/results : We propose two novel incentive schemes: the qualification scheme and the prioritization scheme. Specifically, the platform sets a work target for the peak period. Under the qualification scheme, the platform assigns off-peak service requests only to drivers who meet the peak period target. Under the prioritization scheme, the platform prioritizes off-peak requests for drivers who meet the peak period target. We analyze the effectiveness of these schemes, considering the openness of the platform’s supply system. For platforms with a closed system that only allows full-time drivers to provide service, the qualification scheme improves the total matching volume to a greater extent but hurts full-time drivers more than the prioritization scheme. For platforms with an open system that also allows the abundant part-time drivers to serve off-peak requests, the prioritization scheme outperforms the qualification scheme in improving total matching volume. Furthermore, the implementation of an incentive scheme in an open system may benefit both the platform and full-time drivers. Managerial implications : Our study suggests an alternative to surge pricing for on-demand platforms to address the supply-demand imbalance when prices are inflexible. It provides insights for platforms with varying levels of supply system openness in choosing appropriate incentive schemes. The welfare results offer guidance for platforms and policymakers regarding both matching volume and worker welfare. Funding: This work was supported by the National Natural Science Foundation of China [Grant 61771331]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0216 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0216},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1515-1531},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Balancing supply with demand on ride-hailing platforms in markets with price regulations: An operational approach},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mental accounting in allocating capacity. <em>MSOM</em>, <em>27</em>(5), 1497-1514. (<a href='https://doi.org/10.1287/msom.2024.0804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This study investigates a seller’s allocation of a limited resource to sequentially arriving customers when the seller is influenced by two types of mental accounting bias: prospective accounting (overestimating future revenue) and behavioral discounting (underestimating future revenue). Methodology/results : We establish structural properties on how mental accounting affects capacity allocation decisions and performance. Interestingly, whereas additional capacity consistently benefits the seller, the same does not hold true for additional demands. That is, an additional class of demand can hurt the seller, depending on the type of mental accounting. This is true even if the additional demand class has a higher reservation price than existing ones. Managerial implications : This result highlights the importance for companies to address and mitigate biases in decision makers before embarking on market expansion initiatives through promotions and advertising campaigns. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2024.0804 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2024.0804},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1497-1514},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Mental accounting in allocating capacity},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal policy for inventory management with periodic and controlled resets. <em>MSOM</em>, <em>27</em>(5), 1484-1496. (<a href='https://doi.org/10.1287/msom.2022.0318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Inventory management problems with periodic and controllable resets occur in the context of managing water storage in the developing world and dynamically optimizing endcap promotion duration in retail outlets. In this paper, we consider a set of sequential decision problems in which the decision maker must not only balance holding and shortage costs but discard all inventory before a fixed number of decision epochs with the option for an early inventory reset. Methodology/results : Finding optimal policies for these problems through dynamic programming presents unique challenges because of the nonconvex nature of the resulting value functions. Moreover, this structure cannot be readily analyzed even with extended convexity definitions, such as K -convexity. Managerial implications : Our key contribution is to present sufficient conditions that ensure the optimal policy has an easily interpretable structure, which generalizes the well-known ( s , S ) policy from the operations management literature. Furthermore, we demonstrate that, under these rather mild conditions, the optimal policy exhibits a four-threshold structure. We then conclude with computational experiments, thereby illustrating the policy structures that can be extracted in various inventory management scenarios. Funding: This work was supported by the National Science Foundation [Grant CMMI-1847666] and the Division of Graduate Education [Grant DGE-2125913]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0318 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0318},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1484-1496},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Optimal policy for inventory management with periodic and controlled resets},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The value of online interactions for store execution. <em>MSOM</em>, <em>27</em>(5), 1464-1483. (<a href='https://doi.org/10.1287/msom.2024.1290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Omnichannel retailers interact with customers both online and offline. So far, they have used the richer information available—gathered from customer interactions across digital and physical channels—to optimize the sales process by designing the right channel and supply chain structures and by personalizing offers, pricing, and promotions. We advance an additional dimension of omnichannel value: retailers can use online clickstreams to better understand customer needs and optimize store layouts to maximize webrooming conversion , which we define as the ratio of sales to webrooming activity. Methodology/results : We develop a model in which in-store purchases depend on the customer’s shopping list and the effort required to locate and reach the products within the store. Category location in the store thus drives the likelihood of a sale. We then apply our model to a large home improvement retailer and find that shoppers’ preferences are revealed by nearby online traffic and hard-to-reach locations lead to lower webrooming conversion. Finally, we optimize category location assignments using our demand model and find that putting higher-interest and higher-price items in the most effective locations can increase revenues by about 2%–5% in comparison with models that ignore online clicks. Managerial implications : We show how using online clickstream information for optimizing offline operations can create significant value. More fundamentally, our results provide a word of caution that in some retailing segments such as home improvement, longer in-store paths might not necessarily be better. Funding: V. Martínez-de-Albéniz acknowledges the financial support provided by the Agencia Estatal de Investigación (AEI) from the Spanish Ministry of Science and Innovation [Grant PID2020-116135GB-I00 MCIN/AEI/10.13039/501100011033]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2024.1290 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2024.1290},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1464-1483},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {The value of online interactions for store execution},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-control in the face of multiple projects. <em>MSOM</em>, <em>27</em>(5), 1449-1463. (<a href='https://doi.org/10.1287/msom.2025.0426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We study how people with present bias make choices when they face multiple projects. Each project consists of a starting and a finishing stage, both requiring costly effort to complete but yield rewards only after project completion. Methodology/results : We derive people’s perception-perfect strategies for project scheduling. Naifs may start a project, but never finish it. They may multitask; that is, start a new project before finishing an old project. They may not prioritize projects by NPV. Sophisticates do not start a project but not finish it. However, like naifs, they may multitask and not prioritize projects by NPV. What happens if people can choose the cost structure endogenously? If people multitask when the cost structure is exogenously given, then when the cost structure can be chosen by them, naifs delay the costs to the finishing stages as much as possible, resulting in either multitasking or procrastination. If, in addition, the project with a higher NPV has a lower total cost, then naifs prioritize projects by NPV. Sophisticates, however, may not always delay the costs to finishing stages when given a choice because delaying the costs may lead to multitasking, which decreases the utility and they, unlike naifs, are fully aware of. Therefore, under endogenous cost structure, sophisticates may or may not multitask or prioritize projects by NPV. Managerial implications : Enhancing individuals’ awareness of their present bias makes them less likely to behave sub-optimally. Increasing load can also, though not always, alleviate procrastination and reduce the possibility of multitasking and not prioritizing projects based on NPV. When project portfolios are predetermined, increasing awareness of present bias always improves people’s long-run utility. However, when project portfolios are selected endogenously, sophistication does not always pay because naifs may either complete more projects or complete a more profitable project than sophisticates. Funding: P. Yu was supported by the National Natural Science Foundation of China [Grants 72371038 and 72033003]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2025.0426 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2025.0426},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1449-1463},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Self-control in the face of multiple projects},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Waiting time prediction with invisible customers. <em>MSOM</em>, <em>27</em>(5), 1433-1448. (<a href='https://doi.org/10.1287/msom.2022.0098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Motivated by technological advances in real-time data collection about customers location in service systems, we study the effect of partial visibility of customers on waiting time prediction. We consider systems where the predictor observes only a subset of the customers interacting with the system while serving all customers indiscriminately. Methodology/results : We formulate a novel model of a partially visible queue and analyze the waiting time prediction problem, deriving a closed-form expression for the optimal prediction. This facilitates quantifying the performance loss of arbitrary prediction methods because of partial visibility and their inherent limitations (i.e., bias and variance). We compare the performance of a wide range of commonly used predictive methods and examine how partial visibility along with other system parameters affects their performance. We further extend these numerical analyses to queueing systems that exhibit characteristics that are common in practice and that were studied in the service operations literature. Managerial implications : Our analysis shows that the phenomenon of invisible customers profoundly impacts the ability to accurately predict waiting times and should, therefore, be considered an important factor in the development of prediction tools. Such tools cannot be effectively deployed if technological barriers or operational limitations prevent a sufficiently high level of data integrity. This work provides specific insights into the effectiveness of various commonly used prediction methods, some of which are shown to be highly sensitive to partial visibility and other queueing systems characteristics. Our findings suggest that machine learning methods that use carefully chosen features offer the most effective generic solution for waiting time prediction in the presence of invisible customers and explain the mechanisms through which partial visibility deteriorates the performance of prediction methods. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0098 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0098},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1433-1448},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Waiting time prediction with invisible customers},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does transparency hinder technological novelty? evidence from large pharmaceutical firms. <em>MSOM</em>, <em>27</em>(5), 1415-1432. (<a href='https://doi.org/10.1287/msom.2023.0527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The drug development process has historically lacked transparency, leading to selective reporting of clinical trial results. Although clinical trial transparency aims to address this issue, increased public scrutiny may make managers more risk averse, potentially stifling novel research and development (R&D) initiatives. This raises the following question. Does transparency hinder technologically novel R&D? Methodology/results : Using data from nearly 10,000 clinical trials from 2000 to 2014, we examine how a 2005 policy to increase transparency affects drug novelty. Using a continuous difference-in-differences approach, we find that transparency prompts firms to rely more on previously used technologies (i.e., exploitative R&D) without reducing their pursuit of never-before-tested technologies (i.e., exploratory R&D). We identify negative informational spillovers as a mechanism; technology failures that are now more visible because of transparency could erode confidence in related trials, prompting firms toward familiar technologies to avoid reputational damage. Consistent with this mechanism, the impact of transparency is strongest among firms with less-diversified portfolios that are the most exposed to negative informational spillovers. Managerial implications : Our primary theoretical contribution is in demonstrating the unintended negative impact of transparency on novelty. For managers, we highlight diversification as a mitigation strategy. We urge policymakers to create incentives to support novel R&D despite transparency pressures. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0527 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0527},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1415-1432},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Does transparency hinder technological novelty? evidence from large pharmaceutical firms},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic care unit placements under unknown demand with learning. <em>MSOM</em>, <em>27</em>(5), 1396-1414. (<a href='https://doi.org/10.1287/msom.2022.0260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Care units are the facilities where admitted hospital patients receive treatment and monitoring services. This paper studies the problem of deciding which patients to place into the various available care units at any time. To determine placements in practice, hospitals rely on clinicians to discern a patient’s care needs and appropriately trade-off between future demand and limited bed availability. Making the right decisions remains challenging because patients are heterogeneous, and demand is uncertain. Methodology/results : We develop a dynamic resource allocation algorithm to decide unit placements by learning the care needs of different patient types. We model hospital beds as reusable resources and assume decision feedback is not immediately available, but rather delayed for an unknown and random length of time. Lastly, we consider the demand to be unknown and allow patient arrivals to be arbitrarily sequenced for robustness. The applicability of our algorithm is demonstrated with real-patient data from a hospital collaboration, where we evaluate our proposed approach using unplanned readmission rates as the performance metric. From extensive simulations, our results suggest the proposed algorithm tends to outperform several greedy benchmarks as well as a hospital benchmark model. A theoretical performance guarantee for our algorithm is provided to complement the case study. Managerial implications : This paper contributes new insights into designing dynamic decision-making models for hospital admissions operations. Our work presents a simple but effective data-driven support tool to help clinicians trade-off between available bed capacity and a patient’s care needs when making care unit placements. We also demonstrate how our algorithm can support the reduction of unplanned readmissions through improved placement decisions. Funding: This work was supported by National Science Foundation Graduate Research Fellowship Program [Grant DGE 1256260]. Partial support for this research was provided to the first-author (A. Dean) by the National Science Foundation Graduate Research Fellowship under Grant DGE1841052. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0260 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0260},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1396-1414},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Dynamic care unit placements under unknown demand with learning},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Waiting online vs. in person: An empirical study on outpatient clinic visit incompletion. <em>MSOM</em>, <em>27</em>(5), 1377-1395. (<a href='https://doi.org/10.1287/msom.2023.0365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The adoption of online services, such as telemedicine, has increased rapidly over the last few years. To better manage online services and effectively integrate them with in-person services, we need to better understand customer behaviors under the two service modalities. Utilizing data from two large internal medicine outpatient clinics, we take an empirical approach to study service incompletion, which can be because of either patient no-show or leaving without being seen. Methodology/results : We focus on estimating the causal effect of whether the provider has cleared prior appointments—used as a proxy of intraday delay—on service incompletion for in-person and telemedicine appointments, respectively. When providers have not cleared prior appointments, patients may have to wait, making them more likely to leave without being seen, leading to a higher service incompletion rate. We introduce a multivariate probit model with instrumental variables to handle estimation challenges because of endogeneity, sample selection, and measurement error. We also conduct a numerical analysis of the intraday sequencing rule when having both telemedicine and in-person patients. Our estimation results show that intraday delay increases the telemedicine service incompletion rate by 7.40%, but it does not have a significant effect on the in-person service incompletion rate. Managerial implications : Our study suggests that telemedicine patients may leave without being seen, whereas in-person patients are not sensitive to intraday delay. More importantly, failing to properly distinguish between incompletions caused by intraday delays and those resulting from no-shows can lead to highly inferior patient sequencing decisions. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0365 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0365},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1377-1395},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Waiting online vs. in person: An empirical study on outpatient clinic visit incompletion},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test allocation and pool composition in heterogenous populations under strict capacity constraints. <em>MSOM</em>, <em>27</em>(5), 1362-1376. (<a href='https://doi.org/10.1287/msom.2021.0149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Motivated by the persistent lack of testing capacity in the first year of the COVID-19 pandemic, we study the question “who should be tested?” when there are general costs and rewards, testing capacity is strictly limited, tests have errors, and patients differ in their prior probability of being infected. We specifically study how the answer to that question changes when pooled testing, a method of grouping samples to conserve tests, is an option. Methodology/results : We use a two-stage stochastic optimization model with recourse, incorporating costs and rewards for different test outcomes, under a conservative capacity constraint that reflects severe shortages of tests or high uncertainty about future test availability. This setting reflects the situation decision makers faced at the beginning of the COVID-19 pandemic in March 2020. Although health officials might intuitively prioritize testing patients who are highly likely to be infected, we find that it may be better to focus on patients who are less likely to be infected, particularly when the test has low sensitivity (i.e., the false-negative rate is substantial). Moreover, it may be optimal to test two groups of individuals: those who are very unlikely to be infected (in pools) and those who are very likely to be infected (individually). Managerial implications : We develop a heuristic policy supported by the analysis, which indicates when pooling should be used and which type of samples should be tested. In some settings, the decision may be characterized simply by understanding the costs and rewards involved. In more complex testing settings, the characteristics of the test and the size of the pool affect the desirability of pooling: Lower specificity, higher sensitivity, and larger pool sizes all result in testing environments that are more favorable to pooling. Managers and policymakers should understand how characteristics of the test and the setting impact whether it is optimal to test patients who are deemed likely to test positive or those who are likely to test negative. Incorporating pooling as a test strategy may change which patients should be prioritized for a test. Our results can inform both public health policy and healthcare operations management in settings where testing capacity is strictly limited. Funding: This work was supported by the Division of Civil, Mechanical and Manufacturing Innovation [Grant 1634822]. S. Ziya was supported by the National Science Foundation [Award CMMI1635574]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2021.0149 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2021.0149},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1362-1376},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Test allocation and pool composition in heterogenous populations under strict capacity constraints},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of family engagement in influencing patient satisfaction and readmission. <em>MSOM</em>, <em>27</em>(5), 1344-1361. (<a href='https://doi.org/10.1287/msom.2023.0734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Family engagement (FE) is an ongoing partnership between health professionals and patients’ families to enhance healthcare quality, safety, and delivery. Although FE is increasingly considered an essential component of patient-centered care, its importance in influencing healthcare delivery outcomes is unclear. This study examines the impact of FE on patient care outcomes (PCOs)—specifically, patient satisfaction (PS) and readmission (Readm)—in the context of varying severity of illness (SOI). Methodology/results : Using secondary data from South Korea, this study develops an integrated framework to examine the relationships between FE, SOI, and PCOs. The methodology involves a two-stage treatment effects model to address endogeneity in FE, supplemented by instrumental variable estimations for SOI. The findings reveal that patients with FE exhibit improved PS and lower Readm. Additionally, FE lessens the negative impact of SOI on these outcomes. The study also explores how the benefits of FE vary depending on the nature of the patient’s relationship with their family, distinguishing between family members living together and separately. We also conduct structured interviews to validate the mechanisms supporting these relationships and identify implications for practice. Managerial implications : The findings have significant implications for healthcare operations management (HOM). Hospitals can develop family-centered policies and training programs, emphasizing the importance of FE in patient care, especially for severely ill patients. Healthcare professionals can benefit from understanding the dynamics between FE, SOI, and PCOs, guiding the development of curricula that focus on family-centered care. Overall, the study underscores the importance of integrating FE into HOM to enhance the quality of patient care. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0734 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0734},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1344-1361},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {The role of family engagement in influencing patient satisfaction and readmission},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OM Forum–Working with practitioners in operations management. <em>MSOM</em>, <em>27</em>(5), 1333-1343. (<a href='https://doi.org/10.1287/msom.2025.0147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Collaborations between researchers and practitioners in operations management are becoming more common, and they can bridge the gap between theory and real-world application. This paper explores the process of managing such collaborations, emphasizing the importance of mutual trust, stakeholder management, and the alignment of academic rigor with practical value. Methodology/results : Drawing on experience and previous operations management literature, the study describes the operational steps of working with practitioners. It outlines a five-phase framework that can guide successful practitioner collaboration research: relationship building, stakeholder management, study design, study implementation, and poststudy management. The framework illustrates how rigorous research methods can be embedded in partner relationships that are governed by trust and ethical sensitivity, the success of which may sustain long-term academic-practitioner collaborations. Managerial implications : This paper offers a five-step road map for academics to undertake collaborations with practitioners that can generate theoretically sound as well as impactful research outcomes.},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2025.0147},
  journal      = {Manufacturing & Service Operations Management},
  month        = {9-10},
  number       = {5},
  pages        = {1333-1343},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {OM Forum–Working with practitioners in operations management},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="or">OR - 32</h2>
<ul>
<li><details>
<summary>
(2025). Acknowledgment to referees (2024). <em>OR</em>, <em>73</em>(4), iii. (<a href='https://doi.org/10.1287/opre.2025.apprec.v73.n4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OR},
  doi          = {10.1287/opre.2025.apprec.v73.n4},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {iii},
  shortjournal = {Oper. Res.},
  title        = {Acknowledgment to referees (2024)},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Off-line estimation of controlled markov chains: Minimaxity and sample complexity. <em>OR</em>, <em>73</em>(4), 2281-2295. (<a href='https://doi.org/10.1287/opre.2023.0046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study a natural nonparametric estimator of the transition probability matrices of a finite controlled Markov chain. We consider an off-line setting with a fixed data set of size m , collected using a so-called logging policy. We develop sample complexity bounds for the estimator and establish conditions for minimaxity. Our statistical bounds depend on the logging policy through its mixing properties. We show that achieving a particular statistical risk bound involves a subtle and interesting trade-off between the strength of the mixing properties and the number of samples. We demonstrate the validity of our results under various examples, such as ergodic Markov chains; weakly ergodic inhomogeneous Markov chains; and controlled Markov chains with nonstationary Markov, episodic, and greedy controls. Lastly, we use these sample complexity bounds to establish concomitant ones for off-line evaluation of stationary Markov control policies. Funding: I. Banerjee was supported in part by the Ross-Lynn fellowship and McLean scholarship at Purdue University. H. Honnappa was partly supported by the National Science Foundation [Grants CAREER/2143752, DMS/1812197 and DMS/2153915]. V. Rao was supported by the National Science Foundation [Grants RI/1816499 and DMS/1812197]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0046 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0046},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2281-2295},
  shortjournal = {Oper. Res.},
  title        = {Off-line estimation of controlled markov chains: Minimaxity and sample complexity},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convex chance-constrained programs with wasserstein ambiguity. <em>OR</em>, <em>73</em>(4), 2264-2280. (<a href='https://doi.org/10.1287/opre.2021.0709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chance constraints yield nonconvex feasible regions in general. In particular, when the uncertain parameters are modeled by a Wasserstein ball, existing studies showed that the distributionally robust (pessimistic) chance constraint admits a mixed-integer conic representation. This paper identifies sufficient conditions that lead to convex feasible regions of chance constraints with Wasserstein ambiguity. First, when uncertainty arises from the right-hand side of a pessimistic joint chance constraint, we show that the ensuing feasible region is convex if the Wasserstein ball is centered around a log-concave distribution (or, more generally, an α -concave distribution with α ≥ − 1 ). In addition, we propose a block coordinate ascent algorithm and prove its convergence to global optimum, as well as the rate of convergence. Second, when uncertainty arises from the left-hand side of a pessimistic two-sided chance constraint, we show the convexity if the Wasserstein ball is centered around an elliptical and star unimodal distribution. In addition, we propose a family of second-order conic inner approximations, and we bound their approximation error and prove their asymptotic exactness. Furthermore, we extend the convexity results to optimistic chance constraints. Funding: This work was supported by the National Science Foundation [Grants ECCS-1845980, OIA-2119691, and OIA-1946391] and the Air Force Office of Scientific Research [Grant FA9550-23-1-0323]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0709 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0709},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2264-2280},
  shortjournal = {Oper. Res.},
  title        = {Convex chance-constrained programs with wasserstein ambiguity},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic control of service systems with returns: Application to design of postdischarge hospital readmission prevention programs. <em>OR</em>, <em>73</em>(4), 2242-2263. (<a href='https://doi.org/10.1287/opre.2022.0066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a control problem for queueing systems in which customers may return for additional episodes of service after their initial service completion. At each service completion epoch, the decision maker can choose to reduce the probability of return for the departing customer but at a cost that is convex increasing in the amount of reduction in the return probability. Other costs are incurred as customers wait in the queue and every time they return for service. Our primary motivation comes from postdischarge quality improvement interventions (e.g., follow-up phone calls, outpatient appointments) frequently used in a variety of healthcare settings to reduce unplanned hospital readmissions. Our objective is to understand how the cost of interventions should be balanced with the reductions in congestion and service costs. To this end, we consider a fluid approximation of the queueing system and characterize the structure of optimal long-run average and bias-optimal transient control policies for the fluid model. Our structural results motivate the design of intuitive surge protocols whereby different intensities of interventions (corresponding to different levels of reduction in the return probability) are provided based on the congestion in the system. Through extensive simulation experiments, we study the performance of the fluid policy for the stochastic system and identify parameter regimes in which it leads to significant cost savings compared with a fixed long-run average optimal policy that ignores holding costs and a simple policy that uses the highest level of intervention whenever the queue is nonempty. In particular, we find that, in a parameter regime relevant to our motivating application, dynamically adjusting the intensity of interventions could result in up to 25.4% reduction in long-run average cost and 33.7% in finite-horizon costs compared with the simple aggressive policy. Funding: V. Sarhangian was supported by the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2018-04518]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0066 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0066},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2242-2263},
  shortjournal = {Oper. Res.},
  title        = {Dynamic control of service systems with returns: Application to design of postdischarge hospital readmission prevention programs},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strong optimal classification trees. <em>OR</em>, <em>73</em>(4), 2223-2241. (<a href='https://doi.org/10.1287/opre.2021.0034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are among the most popular machine learning models and are used routinely in applications ranging from revenue management and medicine to bioinformatics. In this paper, we consider the problem of learning optimal binary classification trees with univariate splits. Literature on the topic has burgeoned in recent years, motivated both by the empirical suboptimality of heuristic approaches and the tremendous improvements in mixed-integer optimization (MIO) technology. Yet, existing MIO-based approaches from the literature do not leverage the power of MIO to its full extent: they rely on weak formulations, resulting in slow convergence and large optimality gaps. To fill this gap in the literature, we propose an intuitive flow-based MIO formulation for learning optimal binary classification trees. Our formulation can accommodate side constraints to enable the design of interpretable and fair decision trees. Moreover, we show that our formulation has a stronger linear optimization relaxation than existing methods in the case of binary data. We exploit the decomposable structure of our formulation and max-flow/min-cut duality to derive a Benders’ decomposition method to speed-up computation. We propose a tailored procedure for solving each decomposed subproblem that provably generates facets of the feasible set of the MIO as constraints to add to the main problem. We conduct extensive computational experiments on standard benchmark data sets on which we show that our proposed approaches are 29 times faster than state-of-the-art MIO-based techniques and improve out-of-sample performance by up to 8%. Funding: P. Vayanos and S. Aghaei gratefully acknowledge support from the Hilton C. Foundation, the Homeless Policy Research Institute, the Home for Good foundation under the “C.E.S. Triage Tool Research & Refinement” grant. P. Vayanos is funded in part by the National Science Foundation, under CAREER [Grant 2046230]. She is grateful for this support. A. Gómez is funded in part by the National Science Foundation under [Grants 1930582 and 2006762]. Supplemental Material: The online appendix and code files are available at https://doi.org/10.1287/opre.2021.0034 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0034},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2223-2241},
  shortjournal = {Oper. Res.},
  title        = {Strong optimal classification trees},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonadaptive stochastic score classification and explainable half-space evaluation. <em>OR</em>, <em>73</em>(4), 2204-2222. (<a href='https://doi.org/10.1287/opre.2023.0431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential testing problems involve a complex system with several components, each of which is “working” with some independent probability. The outcome of each component can be determined by performing a test, which incurs some cost. The overall system status is given by a function f of the outcomes of its components. The goal is to evaluate this function f by performing tests at the minimum expected cost. Although there has been extensive prior work on this topic, provable approximation bounds are mainly limited to simple functions, like “ k -out-of- n ” and half-spaces. We consider significantly more general “score classification” functions, and we provide the first constant-factor approximation algorithm (improving over a previous logarithmic approximation ratio). Moreover, our policy is nonadaptive; it just involves performing tests in an a priori fixed order. We also consider the related half-space evaluation problem, where we want to evaluate some function on d half-spaces (e.g., the intersection of half-spaces). We show that our approach provides an O ( d 2 log d ) -approximation algorithm for this problem. Our algorithms also extend to the setting of “batched” tests, where multiple tests can be performed simultaneously while incurring an extra setup cost. Finally, we perform computational experiments that demonstrate the practical performance of our algorithm for score classification. We observe that, for most instances, the cost of our algorithm is within a factor of 1.5 of an information-theoretic lower bound on the optimal value. Funding: This work was supported by the Division of Computing and Communication Foundations [Grants CCF-2006778 and CCF-2006953] and the Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-1940766].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0431},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2204-2222},
  shortjournal = {Oper. Res.},
  title        = {Nonadaptive stochastic score classification and explainable half-space evaluation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logarithmic regret in multisecretary and online linear programs with continuous valuations. <em>OR</em>, <em>73</em>(4), 2188-2203. (<a href='https://doi.org/10.1287/opre.2022.0036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I use empirical processes to study how the shadow prices of a linear program that allocates an endowment of n β ∈ R m resources to n customers behave as n → ∞ . I show the shadow prices (i) adhere to a concentration of measure, (ii) converge to a multivariate normal under central-limit-theorem scaling, and (iii) have a variance that decreases like Θ ( 1 / n ) . I use these results to prove that the expected regret in an online linear program is Θ ( log n ) , both when the customer variable distribution is known upfront and must be learned on the fly. This result tightens the sharpest known upper bound from O ( log n log log n ) to O ( log n ) , and it extends the Ω ( log n ) lower bound known for single-dimensional problems to the multidimensional setting. I illustrate my new techniques with a simple analysis of a multisecretary problem. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0036 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0036},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2188-2203},
  shortjournal = {Oper. Res.},
  title        = {Logarithmic regret in multisecretary and online linear programs with continuous valuations},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardness of pricing routes for two-stage stochastic vehicle routing problems with scenarios. <em>OR</em>, <em>73</em>(4), 2177-2187. (<a href='https://doi.org/10.1287/opre.2023.0569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem with stochastic demands (VRPSD) generalizes the classic vehicle routing problem by considering customer demands as random variables. Similar to other vehicle routing variants, state-of-the-art algorithms for the VRPSD are often based on set-partitioning formulations, which require efficient routines for the associated pricing problems. However, all of these set partitioning–based approaches have strong assumptions on the correlation between the demands of random variables (e.g., no correlation), a simplification that diverges from real-world settings where correlations frequently exist. In contrast, there is a significant effort in the stochastic programming community to solve problems where the uncertainty is modeled with a finite set of scenarios. This approach can approximate more diverse distributions via sampling and is particularly appealing in data-driven contexts where historical data are readily available. To fill this gap, we focus on the VRPSD with demands given by scenarios. We show that for any route relaxation (where repeated visits are allowed in a route) and any approximation of the recourse cost that satisfies some mild assumptions, the VRPSD pricing problem is still strongly N P -hard. This provides a very strong argument for the difficulty of developing efficient column generation–based algorithms for the VRPSD with demands following an empirical probability distribution of scenarios. Funding: This work was supported by Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2020-04030].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0569},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2177-2187},
  shortjournal = {Oper. Res.},
  title        = {Hardness of pricing routes for two-stage stochastic vehicle routing problems with scenarios},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving blockchain consistency bound by assigning weights to random blocks. <em>OR</em>, <em>73</em>(4), 2156-2176. (<a href='https://doi.org/10.1287/opre.2022.0463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchains based on the celebrated Nakamoto consensus protocol have shown promise in several applications, including cryptocurrencies. However, these blockchains have inherent scalability limits caused by the protocol’s consensus properties. In particular, the consistency property demonstrates a tight trade-off between block production speed and the system’s security in terms of resisting adversarial attacks. As such, this paper proposes a novel method called Ironclad, which improves the blockchain consistency bound by assigning a different weight to randomly selected blocks. We apply our method to the original Nakamoto protocol and rigorously prove that such a combination can significantly improve the consistency bound by analyzing the fundamental consensus properties. This kind of improvement enables a much faster block production rate than the original Nakamoto protocol but with the same security guarantee. Funding: This work was supported in part by the WeBank-Hong Kong University of Science and Technology Joint Lab [Project WEB19EG01-M]. The research of J. Zhang was supported in part by the Hong Kong Research Grants Council [Grants 16208120 and 16214121]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0463 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0463},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2156-2176},
  shortjournal = {Oper. Res.},
  title        = {Improving blockchain consistency bound by assigning weights to random blocks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A short and general duality proof for wasserstein distributionally robust optimization. <em>OR</em>, <em>73</em>(4), 2146-2155. (<a href='https://doi.org/10.1287/opre.2023.0135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general duality result for Wasserstein distributionally robust optimization that holds for any Kantorovich transport cost, measurable loss function, and nominal probability distribution. Assuming an interchangeability principle inherent in existing duality results, our proof only uses one-dimensional convex analysis. Furthermore, we demonstrate that the interchangeability principle holds if and only if certain measurable projection and weak measurable selection conditions are satisfied. To illustrate the broader applicability of our approach, we provide a rigorous treatment of duality results in distributionally robust Markov decision processes and distributionally robust multistage stochastic programming. Additionally, we extend our analysis to other problems such as infinity-Wasserstein distributionally robust optimization, risk-averse optimization, and globalized distributionally robust counterpart. Funding: L. Zhang acknowledges the support of Xunyu Zhou and the Nie Center for Intelligent Asset Management at Columbia University. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0135 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0135},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2146-2155},
  shortjournal = {Oper. Res.},
  title        = {A short and general duality proof for wasserstein distributionally robust optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-adaptive local decision rules. <em>OR</em>, <em>73</em>(4), 2125-2145. (<a href='https://doi.org/10.1287/opre.2023.0564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For parameterized mixed-binary optimization problems, we construct local decision rules that prescribe near-optimal courses of action across a set of parameter values. The decision rules stem from solving risk-adaptive training problems over classes of continuous, possibly nonlinear mappings. In asymptotic and nonasymptotic analysis, we establish that the decision rules prescribe near-optimal decisions locally for the actual problems without relying on linearity, convexity, or smoothness. The development also accounts for practically important aspects such as inexact function evaluations, solution tolerances in training problems, regularization, and reformulations to solver-friendly models. The decision rules also furnish a means to carry out sensitivity and stability analysis for broad classes of parameterized optimization problems. We develop a decomposition algorithm for solving the resulting training problems and demonstrate its ability to generate quality decision rules on a nonlinear binary optimization model from search theory. Funding: J. O. Royset is supported in part by the Office of Naval Research (ONR) [Grant N000142412277]. M. A. Lejeune was supported by the National Science Foundation [Grants DMS-2318519 and ECCS-2114100], and the Office of Naval Research [Grant N00014-22-1-2649]. Supplemental Material: The computer code and data that support the findings of this study and the online appendix are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0564 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0564},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2125-2145},
  shortjournal = {Oper. Res.},
  title        = {Risk-adaptive local decision rules},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic allocation of reusable resources: Logarithmic regret in overloaded networks. <em>OR</em>, <em>73</em>(4), 2097-2124. (<a href='https://doi.org/10.1287/opre.2022.0429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of dynamically allocating reusable resources to customers of n types. There are d pools of resources and a finite number of units from each resource. If a customer request is accepted, the decision maker collects a type-dependent reward, and the customer occupies, for a random service time, one unit from each resource in a set of these. Upon service completion, these resource units become available for future allocation. This is a loss network: requests that are not accepted leave immediately. The decision maker’s objective is to maximize the long-run average reward subject to the resource capacity constraint. A natural linear programming (LP) relaxation of the problem serves as an upper bound on the performance of any policy. We identify a condition that generalizes the notion of overload in single-resource networks (i.e., when d = 1 ). The LP guides our construction of a threshold policy. In this policy, the number of thresholds equals the number of resource types (hence, does not depend on the number of customer types). These thresholds are applied to a “corrected” headcount process. In the case of a single resource, the corrected head count is the number of resource units that are occupied. We prove that, in overloaded networks, the additive loss (or regret) of this policy benchmarked against the LP upper bound is logarithmic in the total arrival volume in the high-customer-volume, many-resource-units, asymptotic regime. No policy can achieve sublogarithmic regret. Simulations showcase the performance of the proposed policy. Funding: X. Xie and I. Gurvich were supported by an Amazon Research Award and DRAGONS – Dynamic Resource Allocation Gains for Operational Networked Sharing, Department of Defense (Army) [Grant STTR A18B-T007]. S. Küçükyavuz was supported by an Office of Naval Research [Grant N00014-22-1-2602]. Supplemental Material: Data and code files are available at https://doi.org/10.1287/opre.2022.0429 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0429},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2097-2124},
  shortjournal = {Oper. Res.},
  title        = {Dynamic allocation of reusable resources: Logarithmic regret in overloaded networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal regularized online allocation by adaptive re-solving. <em>OR</em>, <em>73</em>(4), 2079-2096. (<a href='https://doi.org/10.1287/opre.2022.0486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a dual-based algorithm framework for solving the regularized online resource allocation problems, which have potentially nonconcave cumulative rewards, hard resource constraints, and a nonseparable regularizer. Under a strategy of adaptively updating the resource constraints, the proposed framework only requests approximate solutions to the empirical dual problems up to a certain accuracy and yet delivers an optimal logarithmic regret under a locally second-order growth condition. Surprisingly, a delicate analysis of the dual objective function enables us to eliminate the notorious log-log factor in regret bound. The flexible framework renders renowned and computationally fast algorithms immediately applicable, for example, dual stochastic gradient descent. Additionally, an infrequent re-solving scheme is proposed, which significantly reduces computational demands without compromising the optimal regret performance. A worst-case square-root regret lower bound is established if the resource constraints are not adaptively updated during dual optimization, which underscores the critical role of adaptive dual variable update. Comprehensive numerical experiments demonstrate the merits of the proposed algorithm framework. Funding: This work was supported by the National Foreign Expert Project [G2022030026], the Research Grants Council, and the University Grants Committee [Grants GRF 16211220, GRF 16300121, and GRF 16301622]. Supplemental Material: The online appendices and code files are available at https://doi.org/10.1287/opre.2022.0486 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0486},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2079-2096},
  shortjournal = {Oper. Res.},
  title        = {Optimal regularized online allocation by adaptive re-solving},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically optimal clearing control of backlogs in multiclass processing systems. <em>OR</em>, <em>73</em>(4), 2061-2078. (<a href='https://doi.org/10.1287/opre.2022.0570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a dynamic scheduling problem for a processing system facing the problem of optimally clearing a large backlog of unsatisfied demand from several classes of customers (or jobs). We formulate the problem as a multiclass queueing model with a large initial queue and arrival rates that approximately equal the system’s processing capacity. The goal is to find a scheduling policy that minimizes a holding-and-abandonment cost during the transient period in which the system is considered congested. Because computing an exact solution to the optimal-control problem is infeasible, we develop a unified asymptotic approximation that covers, in particular, the conventional and the many-server heavy-traffic regimes. In addition to the generality and flexibility of our unified asymptotic framework, we also prove a strong form of asymptotic optimality, under which the costs converge in expectation and in probability. In particular, for the special two-class case, we prove that a static priority policy, which follows a discounted c μ / θ rule, is asymptotically optimal. When there are more than two classes of customers, we show that any admissible control that follows the best-effort rule, which gives the lowest priority to one of the classes according to the discounted c μ / θ ordering, becomes asymptotically optimal after some relatively short time period. Finally, using heuristic arguments and insights from our analyses, we propose scheduling policies that build on the best-effort rule. An extensive numerical study shows that those proposed policies are effective and provides guidance as to when to use either policy in practice. Funding: O. Perry and L. Yu were partially supported by NSF [Grants CMMI 1763100 and CMMI 2006350]. L. Yu is currently supported by NSFC [Grants 72201153, 72242106, and 72394361]. Supplemental Material: The online appendix and code are available at https://doi.org/10.1287/opre.2022.0570 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0570},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2061-2078},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically optimal clearing control of backlogs in multiclass processing systems},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal dynamic mechanism under customer search. <em>OR</em>, <em>73</em>(4), 2045-2060. (<a href='https://doi.org/10.1287/opre.2022.0136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the seller’s revenue-maximizing mechanism in the face of a customer who searches for outside alternatives over a finite horizon. The customer’s utility from searches is modeled as a general function—referred to as the recall function—of the past search outcomes. Without observing the customer’s valuation of the product or any realization of search outcomes, the seller can propose and commit to a contract with the customer before the search process begins. Under a general recall function, we show that the optimal strategy for the seller is to offer a menu of American options consisting of deposits and strike prices. In the case in which the customer can only recall a few recent outside alternatives, we further establish that, under the optimal mechanism, customers with low valuation search for outside alternatives without engaging with the seller, whereas high-valuation customers exercise the option immediately, effectively turning the option into an exploding offer. Customers with intermediate valuation only exercise the option, if ever, at the end of the search horizon. Whereas a longer search horizon or smaller search cost both increase the customer’s utility from searches, they have different impacts on the seller’s revenue. More search opportunities lead to an exponential decrease in the seller’s revenue, and in the limit, the optimal mechanism converges to a posted price mechanism. In contrast, as the search cost increases, the seller’s revenue may initially decrease and then increase. In the extreme case in which the search cost exceeds the average value of outside alternatives, the customer’s sequential search problem reduces to strategically timing purchases of the seller’s product. Our optimal mechanism, in this case, reduces to making a single exploding offer with a monopoly price. Funding: This work is funded in part by the Ministry of Education, Singapore, under its 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0136 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0136},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2045-2060},
  shortjournal = {Oper. Res.},
  title        = {Optimal dynamic mechanism under customer search},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boundary effects in the diffusion of new products on cartesian networks. <em>OR</em>, <em>73</em>(4), 2026-2044. (<a href='https://doi.org/10.1287/opre.2022.0004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the effect of boundaries in the discrete Bass model on D -dimensional Cartesian networks. In two dimensions, this model describes the diffusion of new products that spread primarily by spatial peer effects, such as residential photovoltaic solar systems. We show analytically that nodes (residential units) that are located near the boundary are less likely to adopt than centrally located ones. This boundary effect is local and decays exponentially with the distance from the boundary. At the aggregate level, boundary effects reduce the overall adoption level. The magnitude of this reduction scales as 1 M 1 / D , where M is the number of nodes. Our analysis is supported by empirical evidence on the effect of boundaries on the adoption of solar. Funding: This material is based upon work supported by the Department of Energy (DOE) [Grant DE-EE0007657]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0004 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0004},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2026-2044},
  shortjournal = {Oper. Res.},
  title        = {Boundary effects in the diffusion of new products on cartesian networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic relocations in car-sharing networks. <em>OR</em>, <em>73</em>(4), 2010-2025. (<a href='https://doi.org/10.1287/opre.2021.0062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel dynamic car relocation policy for a car-sharing network with centralized control and uncertain, unbalanced demand. The policy is derived from a reformulation of the linear programming fluid model approximation of the dynamic problem. We project the full-dimensional fluid approximation onto the lower-dimensional space of relocation decisions only. This projection results in a characterization of the problem as n + 1 linear programs, where n is the number of nodes in the network. The reformulation uncovers structural properties that are interpretable using absorbing Markov chain concepts and allows us to write the gradient with respect to the relocation decisions in closed form. Our policy exploits these gradients to make dynamic car relocation decisions. We provide extensive numerical results on hundreds of random networks where our dynamic car relocation policy consistently outperforms the standard static policy. Our policy reduces the optimality gap in steady state by more than 23% on average. Also, in a short-term, time-varying setting, the lookahead version of our dynamic policy outperforms the static lookahead policy slightly more than in the time-homogeneous tests. Funding: This work was supported by the Natural Sciences and Engineering Research Council of Canada [Grants RGPGP-2015-00050 and RGPIN-2018-04561]. Supplemental Material: The computer code, data, and e-companion that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0062 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0062},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2010-2025},
  shortjournal = {Oper. Res.},
  title        = {Dynamic relocations in car-sharing networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Establishing convergence of infinite-server queues with batch arrivals to shot-noise processes. <em>OR</em>, <em>73</em>(4), 2002-2009. (<a href='https://doi.org/10.1287/opre.2023.0353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Across domains as diverse as communication channels, computing systems, and public health management, a myriad of real-world queueing systems receive batch arrivals of jobs or customers. In this work, we show that under a natural scaling regime, both the queue-length process and the workload process associated with a properly scaled sequence of infinite-server queueing systems with batch arrivals converge almost surely, uniformly on compact sets, to shot-noise processes. Given the applicability of these models, our relatively direct and accessible methodology may also be of independent interest, where we invoke the Glivenko–Cantelli theorem when the Strong Law of Large Numbers fails to hold for the queue-length batch scaling yet then, exploit the continuity of stationary excess distributions and the classic strong law when the Glivenko–Cantelli theorem fails to hold in the workload batch scaling. These results strengthen a convergence result recently established in the work of de Graaf et al. [de Graaf WF, Scheinhardt WR, Boucherie RJ (2017) Shot-noise fluid queues and infinite-server systems with batch arrivals. Performance Evaluation 116:143–155] in multiple ways, and furthermore, they provide new insight into how the queue-length and workload limits differ from one another.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0353},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2002-2009},
  shortjournal = {Oper. Res.},
  title        = {Establishing convergence of infinite-server queues with batch arrivals to shot-noise processes},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partition and prosper: Design and pricing of single bundle. <em>OR</em>, <em>73</em>(4), 1983-2001. (<a href='https://doi.org/10.1287/opre.2022.0465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product bundling is a widely used selling strategy among multiproduct firms, yet designing and pricing bundles optimally remain a complex challenge. This paper addresses this fundamental issue by exploring the selection and pricing of a single bundle from a range of products. For instance, in the single bundle with the rest (SBR) framework, the bundle is optimally chosen and priced, whereas the remaining products are offered individually, collectively maximizing profit. We show that the SBR optimization problem under multivariate normal valuations is polynomial-time solvable, provided that the associated covariance matrix can be decomposed into a positive diagonal matrix minus a positive semidefinite matrix of (small) fixed rank. Interestingly, we also show that the subproblem of SBR optimization, where individual product prices are predetermined, is N P -hard, even if customer valuations are independent. Building on these results, we use a Bayesian optimization (BO) algorithm combined with a conic integer programming reformulation to solve the general SBR optimization problem under correlated valuations. We further show that SBR is a constant approximation to more complex mechanisms in terms of profit performance. Extensive numerical results demonstrate that our BO algorithm has superior performance over baseline heuristics, and SBR achieves significantly higher profit than separate selling and grand bundling. Interestingly, simulation studies reveal that allowing customers the additional option to purchase products either as part of a bundle or individually enhances social welfare (i.e., increases both profit and customer surplus) compared with SBR, separate selling, and grand bundling. These findings highlight the potential benefits of bundle pricing strategies in achieving improved outcomes for both firms and customers. Funding : H. Sun is supported by the National Natural Science Foundation of China [Grant 72301168] and the Shanghai Pujiang Programme [Grant 23PJC062]. X. Li is supported by the Singapore Ministry of Education [Tier 1 Grant 23-0619-P0001 and Tier 1 Grant 24-0500-A0001] and the National Natural Science Foundation of China [Grant 72171156]. C.-P. Teo is supported by the National Research Foundation Singapore [Grant I2001E0059] and the Singapore Ministry of Education [Grant MOE-2019-T3-1-010]. C.-P. Teo is also supported by the Natural Science Foundation of Chongqing, China [Grant CSTB2022NSCQ-MSX1667]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2022.0465 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0465},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1983-2001},
  shortjournal = {Oper. Res.},
  title        = {Partition and prosper: Design and pricing of single bundle},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian and randomized clock auctions. <em>OR</em>, <em>73</em>(4), 1965-1982. (<a href='https://doi.org/10.1287/opre.2022.0421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a single-parameter mechanism design problem, a provider is looking to sell some service to a group of potential buyers. Each buyer i has a private value v i for receiving this service, but a feasibility constraint restricts which buyers can be simultaneously served. Recent work in economics introduced (deferred-acceptance) clock auctions as a superior class of auctions for this problem due to their transparency, simplicity, and strong incentive guarantees. Subsequent work focused on evaluating these auctions in terms of worst-case social welfare approximation, leading to strong impossibility results: Without prior information regarding buyers’ values, deterministic clock auctions cannot achieve bounded approximations, even for feasibility constraints comprising two maximal feasible sets. We demonstrate how to circumvent these negative results by leveraging prior information or randomization . In particular, we provide clock auctions that give an O ( log log k ) -approximation for arbitrary downward-closed feasibility constraints with k maximal feasible sets for three different information regimes. The more prior information we have access to, the simpler the proposed auctions. In addition, we propose a parametrization of the complexity of clock auctions, paving the way for exciting future research. Funding: This work was supported by the Simons Foundation [Grant 820931], the Science and Technology Innovation 2030 [Grant 2018AAA0100903], the National Natural Science Foundation of China [Grant 62150610500], the Central University Basic Research Fund of China, the National Science Foundation [Grants CCF-1755955 and CCF-2008280], the H2020 European Research Council [Grant 866132], and the Israel Science Foundation [Grant 317/17]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0421 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0421},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1965-1982},
  shortjournal = {Oper. Res.},
  title        = {Bayesian and randomized clock auctions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian mechanism design for blockchain transaction fee allocation. <em>OR</em>, <em>73</em>(4), 1944-1964. (<a href='https://doi.org/10.1287/opre.2024.0865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In blockchain systems, the design of transaction fee mechanisms (TFMs) is essential for stability and satisfaction for both miners and users. A recent work has proven the impossibility of collusion-proof mechanisms that achieve both nonzero miner revenue and Dominant Strategy Incentive Compatibility (DSIC) for users. However, a positive miner revenue is important in practice to motivate miners. To address this challenge, we consider a Bayesian game setting and relax the DSIC requirement for users to Bayesian Nash Incentive Compatibility (BNIC). In particular, we propose an auxiliary mechanism method that makes connections between BNIC and DSIC mechanisms. With the auxiliary mechanism method, we design a TFM based on the multinomial logit (MNL) choice model, and prove that the TFM has both BNIC and collusion-proof properties with an asymptotic constant-factor approximation of optimal miner revenue for i.i.d. bounded valuations. Our result breaks the zero-revenue barrier while preserving truthfulness and collusion-proof properties. Funding: X. Chen thanks the NSF [Grant IIS-1845444] for support. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2024.0865 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.0865},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1944-1964},
  shortjournal = {Oper. Res.},
  title        = {Bayesian mechanism design for blockchain transaction fee allocation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—On dynamic pricing with covariates. <em>OR</em>, <em>73</em>(4), 1932-1943. (<a href='https://doi.org/10.1287/opre.2021.0802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider dynamic pricing with covariates under a generalized linear demand model: A seller can dynamically adjust the price of a product over a horizon of T time periods, and at each time period t , the demand of the product is jointly determined by the price and an observable covariate vector x t ∈ R d through a generalized linear model with unknown coefficients. Most of the existing literature assumes the covariate vectors x t s are independently and identically distributed (i.i.d.); the few papers that relax this assumption either sacrifice model generality or yield suboptimal regret bounds. In this paper, we show that Upper Confidence Bound and Thompson sampling-based pricing algorithms can achieve an O ( d T log T ) regret upper bound without assuming any statistical structure on the covariates x t . Our upper bound on the regret matches the lower bound up to logarithmic factors. We thus show that (i) the i.i.d. assumption is not necessary for obtaining low regret, and (ii) the regret bound can be independent of the (inverse) minimum eigenvalue of the covariance matrix of the x t s, a quantity present in previous bounds. Moreover, we consider a constrained setting of the dynamic pricing problem where there is a limited and unreplenishable inventory, and we develop theoretical results that relate the best achievable algorithm performance to a variation measure with respect to the temporal distribution shift of the covariates. We also demonstrate the proposed algorithms’ performance with numerical experiments. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0802 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0802},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1932-1943},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On dynamic pricing with covariates},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Community-engaged school district design: A stream-based approach. <em>OR</em>, <em>73</em>(4), 1916-1931. (<a href='https://doi.org/10.1287/opre.2022.0621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehensive community engagement in public school district design is essential to create equitable and effective enrollment policies reflective of community needs and values. We revisit the school district design problem with a focus on codesigning with community partners. We introduce a new compact formulation that incorporates multiple decisions simultaneously by assigning students in each geographic unit to a set of schools (e.g., elementary, middle, and high schools, and schools with specialized programming) with a single composite variable, referred to as a “stream.” This formulation is computationally efficient and easily reconfigurable for evolving problem specifications that are endemic to community codesign. These features were essential in the district redesign process described in this paper, allowing the community to iteratively develop proposals to address inequities in access to education and improve the student assignment process. Funding: This work was supported by the National Science Foundation [Grant CMMI-1727744] and Northwestern University’s McCormick Catalyst Fund.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0621},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1916-1931},
  shortjournal = {Oper. Res.},
  title        = {Community-engaged school district design: A stream-based approach},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically optimal competitive ratio for online allocation of reusable resources. <em>OR</em>, <em>73</em>(4), 1897-1915. (<a href='https://doi.org/10.1287/opre.2021.0695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of online allocation (matching, budgeted allocations, and assortments) of reusable resources for which an adversarial sequence of resource requests is revealed over time and any allocated resource is used/rented for a stochastic duration drawn independently from a resource-dependent usage distribution. Previously, it was known that a greedy algorithm is 0.5-competitive against the clairvoyant benchmark that knows the entire sequence of requests in advance. We give a novel algorithm that is ( 1 − 1 / e ) -competitive for arbitrary usage distributions when the starting capacity of each resource is large and the usage distributions are known. This is the best achievable competitive ratio guarantee for the problem; that is, no online algorithm can have a better competitive ratio. We also give a distribution-oblivious online algorithm and show that it is ( 1 − 1 / e ) -competitive in special cases. At the heart of our algorithms is a new quantity that factors in the potential of reusability for each resource by (computationally) creating an asymmetry between identical units of the resource. We establish the performance guarantee for our algorithms by constructing a feasible solution to a novel system of inequalities that allows direct comparison with the clairvoyant benchmark instead of a linear programming relaxation of the benchmark. Our technique generalizes the primal-dual analysis framework for online resource allocation and may be of broader interest. Funding: This work was supported by Google (Google Research Scholar Program) and the Division of Civil, Mechanical and Manufacturing Innovation [Grants 1351838, 1636046, and 2340306]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0695 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0695},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1897-1915},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically optimal competitive ratio for online allocation of reusable resources},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic fair division with partial information. <em>OR</em>, <em>73</em>(4), 1876-1896. (<a href='https://doi.org/10.1287/opre.2023.0608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the fundamental problem of fairly and efficiently allocating T indivisible items among n agents with additive preferences. Items become available over a sequence of rounds, and every item must be allocated immediately and irrevocably before the next one arrives. Previous work shows that when the agents’ valuations for the items are drawn from known distributions, it is possible (under mild assumptions) to find allocations that are envy-free with high probability and Pareto efficient ex post. However, this requires that agents accurately report their values to the algorithm, which rarely happens in practice. We study a partial-information setting, where true item values are hidden from the algorithm and it is only possible to elicit ordinal information in the form of a ranking or pairwise comparison relative to prior items. When values are drawn from i.i.d. distributions, or correlated distributions consisting of a shared common value for each item with i.i.d. noise, we give an algorithm that is envy-free and ( 1 − ϵ ) -welfare-maximizing with high probability. We provide similar guarantees (envy-freeness and a constant approximation to welfare with high probability) even with minimally expressive queries that ask for a comparison with a single previous item. For independent but nonidentical agents, we obtain envy-freeness and a constant approximation to Pareto efficiency with high probability. Our results are asymptotically tight. A computational study shows that envy-freeness and efficiency can be achieved on practical time-horizons. Funding: D. Halpern is supported by the National Science Foundation Graduate Research Fellowship Program [Grant DGE1745303]. A. Psomas is supported in part by an NSF CAREER award (Division of Computing and Communication Foundations) [Grant CCF-2144208], a Google AI for Social Good award, and research awards from Google and Supra. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0608 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0608},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1876-1896},
  shortjournal = {Oper. Res.},
  title        = {Dynamic fair division with partial information},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On simple mechanisms for dependent items. <em>OR</em>, <em>73</em>(4), 1849-1875. (<a href='https://doi.org/10.1287/opre.2022.0552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of selling n heterogeneous items to a single buyer, whose values for different items are dependent. Under arbitrary dependence, others show that no simple mechanism can achieve a nonnegligible fraction of the optimal revenue even with only two items. We consider the setting where the buyer’s type is drawn from a correlated distribution that can be captured by a Markov random field (MRF), one of the most prominent frameworks for modeling high-dimensional distributions with structure. We show how the performance of simple mechanisms depends on some natural parameters of the MRF for several fundamental classes of the buyer’s valuations. Our results are based on the duality framework by of others and a new concentration inequality for XOR-of-OR-of-Singletons functions over dependent random variables. Funding: This research was supported by a Sloan Foundation Research Fellowship and the National Science Foundation [Award CCF-1942583 (CAREER)].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0552},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1849-1875},
  shortjournal = {Oper. Res.},
  title        = {On simple mechanisms for dependent items},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simultaneous column-and-row generation solution method for liner shipping network design. <em>OR</em>, <em>73</em>(4), 1825-1848. (<a href='https://doi.org/10.1287/opre.2020.0458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The liner shipping network design (LSND) problem involves creating regular ship rotations to transport containerized cargo between seaports. The objective is to maximize carrier profit by balancing revenue from satisfied demand against operating and transshipment costs. Finding an optimal solution is challenging because of complex rotation structures and joint decisions on fleet deployment, cargo routing, and rotation design. This work introduces a set partitioning–like formulation for LSND with transshipment costs, featuring an exponential number of variables and constraints. The formulation captures key service components, such as ship type, sailing speed, and frequency. Addressing transshipment costs requires numerous rotation-dependent variables and constraints, making even linear programming relaxation difficult to solve. To tackle this, we propose a simultaneous column-and-row generation (SCRG) solution method with novel speedup techniques. Integrating SCRG into a branch-and-price algorithm, we develop an exact method for LSND and test it on two variants with different rotation configurations. Extensive computational experiments demonstrate the method’s effectiveness and efficiency. In addition to advancing solution methods for LSND, this work enhances the SCRG-based method and expands its practical applications. Funding: This research was supported by the National Natural Science Foundation of China [Grants 72171147, 72031006] and the Research Grants Council of Hong Kong SAR, China [Grant 15221619]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2020.0458 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0458},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1825-1848},
  shortjournal = {Oper. Res.},
  title        = {A simultaneous column-and-row generation solution method for liner shipping network design},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint order fulfillment and inventory management in assemble-to-order generalized w systems. <em>OR</em>, <em>73</em>(4), 1805-1824. (<a href='https://doi.org/10.1287/opre.2021.0281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper characterizes joint order fulfillment and inventory policies for assemble-to-order generalized W systems, in which k products are assembled from a common component and k product-specific (dedicated) components. We consider a periodic-review system and focus on nested fulfillment policies, in which orders are fulfilled in decreasing order of profit margins. We prove that the optimal fulfillment policy of a two-product W system is nested. For systems with more than two products, although nested policies may not be optimal in general, we identify a sufficient condition for the optimal policy to be nested, and furthermore, we show that a nested policy is asymptotically optimal in a high-demand regime. Based on these results, we develop an asymptotically optimal strategy for the joint fulfillment and inventory decision under which the policy for demand fulfillment is static nested and that for inventory procurement is of the newsvendor type. In addition, we derive insights regarding the interactions between the component inventories and find that although the inventories of the common component and a particular dedicated component are complementary, those of different dedicated components may not be substitutable. Funding: The work of M. Yu was partially supported by the Hong Kong Research Grant Council [Grants 16502018 and 647312]. The work of S. Zheng was partially supported by the Hong Kong Research Grant Council [Grant 16502621]. The work of J. Chen was partially supported by the National Natural Science Foundation of China [Grants 72171202 and 72232007]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0281 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0281},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1805-1824},
  shortjournal = {Oper. Res.},
  title        = {Joint order fulfillment and inventory management in assemble-to-order generalized w systems},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Split liver transplantation: An analytical decision support model. <em>OR</em>, <em>73</em>(4), 1785-1804. (<a href='https://doi.org/10.1287/opre.2022.0131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Split liver transplantation (SLT) is a procedure that potentially saves two lives using one liver, increasing the total benefit derived from the limited number of donated livers available. SLT may also improve equity by giving transplant candidates who are physically smaller (including children) increased access to liver transplants. However, SLT is rarely used in the United States. To help quantify the benefits of increased SLT utilization and provide decision support tools, we introduce a deceased-donor liver allocation model with both efficiency and fairness objectives. We formulate our model as a multiqueue fluid system, incorporating the specifics of donor-recipient size matching and patients’ dynamically changing health conditions. Leveraging a novel decomposition result, we find the exact optimal matching procedure, enabling us to benchmark the performance of different allocation policies against the theoretical optimal. Numerical results, utilizing data from the Organ Procurement and Transplantation Network, show that increased utilization of SLT can significantly reduce patient deaths, increase total quality-adjusted life years, and improve fairness among different patient groups. Funding: This work was supported by Carnegie Mellon University Tepper’s Health Care Initiative Funding from 2022 to 2023. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0131 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0131},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1785-1804},
  shortjournal = {Oper. Res.},
  title        = {Split liver transplantation: An analytical decision support model},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard and soft defense against a sequence of aerial threats. <em>OR</em>, <em>73</em>(4), 1767-1784. (<a href='https://doi.org/10.1287/opre.2024.1025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of missiles and drones (hereafter referred to as threats) in attacks by both state and nonstate actors highlights the critical need for a robust defense system to counter these threats. We develop a combat model for the engagement between a Blue defender who is subject to repeated attacks by Red threats. The defender employs two types of defenses: hard interceptors, such as antiballistic missiles, and soft measures, such as directed-energy weapons and jamming. Employing strategies for these two types of defensive options are evaluated by a two-dimensional measure of effectiveness: expected number of leaking Red threats and the expected expenditure of hard interceptors. We define efficient frontiers on this two-dimensional space and identify defense strategies that compose these frontiers. Funding: This research is supported by funding from the Naval Postgraduate School, Naval Research Program [PE 0605853N/2098]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2024.1025 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1025},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1767-1784},
  shortjournal = {Oper. Res.},
  title        = {Hard and soft defense against a sequence of aerial threats},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unit commitment without commitment: A dynamic programming approach for managing an integrated energy system under uncertainty. <em>OR</em>, <em>73</em>(4), 1744-1766. (<a href='https://doi.org/10.1287/opre.2023.0546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though variability and uncertainty have always posed challenges for power systems, the increasing use of renewable energy sources has exacerbated these issues. At a vertically integrated utility, the system operator manages many generation units—renewable and otherwise—and storage units to ensure that the total energy produced matches contemporaneous demand. Current industry practice at these utilities involves solving “unit commitment” and “economic dispatch” optimization problems to choose production plans. These models, though complex, do not explicitly incorporate uncertainty. In this paper, we develop a dynamic programming approach to help system operators manage production under uncertainty. We formulate the problem as a stochastic dynamic program and use Lagrangian methods to decompose the system across units. The Lagrangian model relaxes the demand-matching constraint and introduces stochastic Lagrange multipliers that can be interpreted as prices representing the varying marginal value of energy production; each unit is then operated to maximize its own expected “profit” given these uncertain prices. These unit-specific value functions are then used to incorporate longer-term effects in dispatch decisions. The unit-specific value functions also provide a way to value generation and storage units in an uncertain environment. We develop relevant theory and demonstrate this dynamic approach using data from the Duke Energy Carolinas and Progress systems. Our numerical experiments demonstrate that this dynamic approach is computationally feasible at an industrial scale and can improve current practice. Specifically, our results suggest that this dynamic approach can reduce operational costs by about 2% on average in the present Duke Energy system and, in a “future” system with increased solar and storage capacity, can reduce operational costs by 4%–5% on average. Perhaps more strikingly, this dynamic approach, on average, performs within 0.2%–0.3% of production plans based on perfect foresight about future net demands. Funding: This work was supported by the Department of Energy Advanced Research Projects Agency - Energy [Grant DE-AR0001283, “A Grid that’s Risk-Aware for Clean Electricity (GRACE)”] and Fuqua and Tuck. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2023.0546 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0546},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1744-1766},
  shortjournal = {Oper. Res.},
  title        = {Unit commitment without commitment: A dynamic programming approach for managing an integrated energy system under uncertainty},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tailored base-surge policies in dual-sourcing inventory systems with demand learning. <em>OR</em>, <em>73</em>(4), 1723-1743. (<a href='https://doi.org/10.1287/opre.2022.0624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a periodic-review dual-sourcing inventory system in which the expedited supplier is faster and more costly, whereas the regular supplier is slower and cheaper. Under full demand distributional information, it is well known that the optimal policy is extremely complex but the celebrated Tailored Base-Surge (TBS) policy performs near optimally. Under such a policy, a constant order is placed at the regular source in each period, whereas the order placed at the expedited source follows a simple order-up-to rule. In this paper, we assume that the firm does not know the demand distribution a priori and makes adaptive inventory ordering decisions in each period based only on the past sales (a.k.a. censored demand) data. The standard performance measure is regret, which is the cost difference between a feasible learning algorithm and the clairvoyant (full-information) benchmark. When the benchmark is chosen to be the (full-information) best Tailored Base-Surge policy, we develop the first nonparametric learning algorithm that admits a regret bound of O ( T ( log T ) 3 log log T ) , which is provably tight up to a logarithmic factor. Leveraging the structure of this problem, our approach combines the power of bisection search and stochastic gradient descent and also involves a delicate high-probability coupling argument between our and the clairvoyant optimal system dynamics. Funding: The research of C. Shi is partially supported by an Amazon research award. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0624 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0624},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1723-1743},
  shortjournal = {Oper. Res.},
  title        = {Tailored base-surge policies in dual-sourcing inventory systems with demand learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="orsc">ORSC - 18</h2>
<ul>
<li><details>
<summary>
(2025). Contract frames and supplier learning. <em>ORSC</em>, <em>36</em>(4), 1625-1642. (<a href='https://doi.org/10.1287/orsc.2022.16526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research examines the impact of contract framing on supplier learning activities within buyer-supplier relationships, an area previously overshadowed by a focus on internal organizational learning mechanisms. Grounded in regulatory focus theory and organizational learning theory, this study assesses how promotion and prevention contract framing influences suppliers’ exploration and exploitation activities. Survey data from 135 buyer-supplier relationship dyads reveals that promotion and prevention framing distinctly affect suppliers’ learning activities. Specifically, during the duration of the contract, (a) compared with prevention contract framing, promotion contract framing has a stronger positive impact on the supplier’s exploration learning activities; and (b) compared with promotion contract framing, prevention contract framing has a stronger negative impact on the supplier’s exploitation learning activities. This study also reveals how the portfolios and clusters of contract framing influence supplier learning activities. Finally, this study reveals that different learning activities result in varied outcomes for the buyer, with exploration boosting continuity and exploitation improving performance. This contribution broadens the understanding of contract framing’s psychological effects on supplier behavior, offering new insights into the dynamics of organizational learning activities and contract design. Funding: This work was supported by National Natural Science Foundation of China [Grant 72072152]; Research Grants Council, University Grants Committee [Grant CityU 11502218]; and Natural Science Foundation of Shandong Province [Grant ZR2023QG003]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16526 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16526},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1625-1642},
  shortjournal = {Organ. Sci.},
  title        = {Contract frames and supplier learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long jump learning: Absorbing distant knowledge via familiar components. <em>ORSC</em>, <em>36</em>(4), 1598-1624. (<a href='https://doi.org/10.1287/orsc.2019.13171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do firms overcome the challenge of absorbing distant knowledge? Scholars argue that organizations need knowledge that is distant from their existing knowledge base to create novel innovative output, whereas others argue, in contrast, that organizations need knowledge to be similar to the firm’s knowledge base in order to absorb it. We argue that organizations can improve their ability to identify and comprehend knowledge from a distant knowledge category through learning associations made by a category link—a familiar tangible component previously used by somebody else in the distant knowledge category. We test our arguments using a sample of patents in which the material graphene is used as a component across diverse patent classes (i.e., knowledge categories). We find that increasing experience with category-linking graphene enables an organization to patent in increasingly distant graphene-linked patent classes and also increases the number of patents in such classes. We also find that the extent to which category-linking graphene is prominent in a distant knowledge category enhances the effect of graphene experience on a firm’s ability to absorb knowledge from the distant knowledge category. We, thus, present a novel internal mechanism by which an organization can absorb distant knowledge.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2019.13171},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1598-1624},
  shortjournal = {Organ. Sci.},
  title        = {Long jump learning: Absorbing distant knowledge via familiar components},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Have your cake and eat it too? understanding leisure-work synergizing and its impact on employee thriving. <em>ORSC</em>, <em>36</em>(4), 1574-1597. (<a href='https://doi.org/10.1287/orsc.2021.15472'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the world of work continues to evolve, employees are increasingly seeking to do more than manage the bifurcated domains of work and home. Instead, amid greater demands at work alongside greater valuation of leisure, employees are striving to thrive at work without sacrificing their free time. To explore this tension, we adopt an agentic, blended, and future-oriented perspective of the work-nonwork interface and examine leisure-work synergizing as a unique practice for employees to find synergies between leisure and work. Drawing on foundational theorizing on workplace thriving, we theorize that strategically integrating work into leisure activities in an attempt to build work-relevant competencies can inspire (via self-assurance) and hinder (via fatigue) employee thriving at work. To test these predictions, we first conducted a series of validation studies to demonstrate the conceptual relationship among leisure-work synergizing and other work and nonwork constructs. Then, using an experience sampling design, we generally found support for the bolstering effects of leisure-work synergizing on thriving through self-assurance. Moreover, segmentation preference—a theoretical moderator related to this unique boundary blurring practice—significantly influenced the proposed relationship between leisure-work synergizing and fatigue. Taken together, our research expands our understanding of the work-nonwork interface and the utility of leisure activities for working adults, integrates work-nonwork research with positive psychology to reveal nonwork agents of thriving, and highlights a novel blended practice to help scholars and managers better understand how employees can “have their cake and eat it too.” Funding: This work was supported by Terry College of Business at the University of Georgia.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15472},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1574-1597},
  shortjournal = {Organ. Sci.},
  title        = {Have your cake and eat it too? understanding leisure-work synergizing and its impact on employee thriving},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beefing IT up for your investor? engagement with open source communities, innovation, and startup funding: Evidence from GitHub. <em>ORSC</em>, <em>36</em>(4), 1551-1573. (<a href='https://doi.org/10.1287/orsc.2023.18348'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the engagement of nascent firms with open source communities and its implications for innovation and attracting funding. To do so, we link data on 160,065 U.S. startups from Crunchbase to their activities on the open source software development platform GitHub. In a matched sample of firms with and without GitHub activities, difference-in-differences models reveal a substantial increase in the likelihood of being funded after early stage startups engage with open source communities on GitHub. This relationship is weaker for firms that employ GitHub for internal development only. Startups developing novel technologies tend to benefit more from engaging with open source communities, unlike those in highly competitive environments. This heterogeneity highlights a potential trade-off between engaging with open source communities and appropriability. To provide insight regarding mechanisms, we classify startups’ technology use-cases on GitHub using machine learning and exploit data on product launches. Our results from these additional analyses support the notion that one important channel potentially driving our findings is the access to external knowledge for technology development provided by open source communities. Engaging with these communities may thereby aid startups in innovating and creating a (minimum) viable product. Funding: A. Conti and C. Peukert received financial support from the Swiss National Science Foundation [Project IDs 100013_188998 and 100013_197807]. M. Roche received financial support from the Harvard Business School Division of Research and Faculty Development. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.18348 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.18348},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1551-1573},
  shortjournal = {Organ. Sci.},
  title        = {Beefing IT up for your investor? engagement with open source communities, innovation, and startup funding: Evidence from GitHub},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple law for the distribution of long-term profit: The empirical regularity behind the 1% of firms that capture 73% of value. <em>ORSC</em>, <em>36</em>(4), 1531-1550. (<a href='https://doi.org/10.1287/orsc.2023.18085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical laws and regularities, such as Klepper’s shakeout pattern for industry evolution or Gibrat’s law for firm size, have shaped our understanding of organizations. Despite decades of research into profit patterns, no such widely applicable empirical regularities have been found for the “dependent variable of strategy”: long-term value capture. This study reports the discovery of a simple law governing this variable’s distribution. A four-parameter normal log-normal (NLN) mixture distribution very well fits observed data of listed firms’ 20-year long-term profit (LTP). The distribution correctly describes, for instance, a remarkable asymmetry in value capture: fewer than 1% of all firms in the data set generated 73% of the total LTP. Though the NLN law applies across different industries, geographies, and time periods, its distributional parameters vary. These parameters provide a novel and precise description of differences across settings in economic outcomes, such as the rise of “superstars.” More broadly, the law’s discovery raises profound questions relating to competitive strategy, evolutionary path dependence, the structure of technological opportunity, and social inequality. Code and data for replication are made available. Supplemental Material: The online appendix and code and data files are available at https://doi.org/10.1287/orsc.2023.18085 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.18085},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1531-1550},
  shortjournal = {Organ. Sci.},
  title        = {A simple law for the distribution of long-term profit: The empirical regularity behind the 1% of firms that capture 73% of value},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from inconsistent performance feedback. <em>ORSC</em>, <em>36</em>(4), 1509-1530. (<a href='https://doi.org/10.1287/orsc.2022.16833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizations and the decision makers within them are increasingly subject to inconsistent performance feedback—feedback that contains elements that are incompatible with each other—which can lead to multiple interpretations of performance feedback. When this occurs, decision makers often recode inconsistent performance feedback as successful and continue with their current strategies, which allows them to avoid any self-threat from negative elements of performance feedback but implies that they do not learn from inconsistent performance feedback because they do not change. In contrast, we explore whether decision makers can learn from inconsistent performance feedback. Leveraging over 10 years of complete behavioral records in an online community and a laboratory experiment, we study how decision makers respond to inconsistent performance feedback stemming from multiple evaluators who do not agree on performance quality. Consistent with prior work, we find that decision makers change their strategies less after inconsistent performance feedback. Departing from prior work, we show a corresponding increase in clarification efforts aimed at better understanding which performance strategies work well. Importantly, clarification efforts mediate improved future performance. Our results suggest that inconsistent performance feedback can trigger deeper learning and enhanced performance, contributing to performance feedback theory and research on the microfoundations of organizational learning. Funding: The authors thank the University of São Paulo, Johns Hopkins University, and SKEMA Business School for institutional and financial support. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16833 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16833},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1509-1530},
  shortjournal = {Organ. Sci.},
  title        = {Learning from inconsistent performance feedback},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “Optimal” feedback use in crowdsourcing contests: Source effect and priming intervention. <em>ORSC</em>, <em>36</em>(4), 1489-1508. (<a href='https://doi.org/10.1287/orsc.2021.15885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing contests allow firms to seek ideas from external solvers to address their problems. This research examines solvers’ use of developmental feedback from different sources and of different constructiveness when generating ideas in contests. I theorize a source effect in solvers’ feedback use where they use seeker feedback more than peer feedback, even if both give identical suggestions for their ideas. I also show how the source effect affects solvers’ use of constructive and less constructive feedback from the respective sources. An insight is that compared with their use of peer feedback, solvers’ use of seeker feedback is more extensive at any level of, but less sensitive to, feedback constructiveness. An implication is that solvers may underuse constructive peer feedback and overuse less constructive seeker feedback. Such behaviors can be solver optimal (in terms of improving solvers’ winning prospects) but not seeker optimal (in terms of enhancing ideas for seekers’ problems), as constructive feedback is likely to improve idea quality, whereas less constructive feedback may hurt it. I propose a priming intervention of a feedback evaluation mechanism to mitigate the source effect in solvers’ feedback use—in a way, the intervention can cause solvers to behave more optimally for the seekers. A field survey and three online experiments test the theorizing and proposed intervention. I discuss the contributions and implications of this research for various stakeholders in crowdsourcing contests. Funding: This work was supported by the Hong Kong Research Grants Council [Grant 16502518]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.15885 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15885},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1489-1508},
  shortjournal = {Organ. Sci.},
  title        = {“Optimal” feedback use in crowdsourcing contests: Source effect and priming intervention},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The rise of scientific research in corporate america. <em>ORSC</em>, <em>36</em>(4), 1466-1488. (<a href='https://doi.org/10.1287/orsc.2023.18053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely believed that university and corporate research are complementary: companies invest in research in part to develop the capacity to absorb the knowledge emerging from universities. However, as we show in this paper, corporate research in the United States emerged when American universities were behind the world frontier in scientific research. Why, then, did for-profit businesses choose to invest in creating new knowledge, much of which could spill over to rivals, and whose conduct presented many managerial challenges? We argue that corporate research in America arose in the 1920s to compensate for weak university research, not to complement it. Using newly assembled firm-level data from the 1920s and 1930s, we find that companies invested in research because inventions increasingly relied on science but American universities were unable to meet their needs. Large firms close to the technological frontier and operating in concentrated industries were likely to invest in research, especially in scientific disciplines where American universities lagged behind the scientific frontier. Corporate science seems to have paid off, resulting in novel patents and high market valuations for firms engaged in research. Funding: A. Arora, S. Belenzon, and J. Suh acknowledge support from the Fuqua School of Business, Duke University. S. Belenzon and Y. Yafeh gratefully acknowledge financial support from the Israel Science Foundation [Grant No. 963-2020]. K. Kosenko is grateful for support from the Bank of Israel. Y. Yafeh acknowledges support from the Krueger Center at the Hebrew University of Jerusalem School of Business. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.18053 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.18053},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1466-1488},
  shortjournal = {Organ. Sci.},
  title        = {The rise of scientific research in corporate america},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Passion penalizes women and advantages (Unexceptional) men in high-potential designations. <em>ORSC</em>, <em>36</em>(4), 1438-1465. (<a href='https://doi.org/10.1287/orsc.2023.18018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-potential programs offer a swift path up the corporate ladder for those who secure a place on them. However, the evaluation of “potential” occurs under considerable uncertainty, creating fertile ground for gender bias. We document that men are more likely than women to be designated as high potential, and unpack how gendered responses to employees’ expressions of passion—one of the most commonly used criteria used in evaluating potential—both penalize women and advantage men in high-potential selection processes. First, and based on prior research on gender display rules, we suggest that expressions of passion are viewed as a less appropriate emotional display for women than men, giving rise to a female penalty. Second, and drawing on shifting standards theorizing, we posit that expressions of passion shift evaluators’ predictions of candidates’ diligence more meaningfully for men than women, creating a male advantage—particularly for men who are reasonably high but not exceptional performers. We provide supporting evidence across two studies examining placement into high-potential programs in a real talent review setting ( N = 796) and a preregistered experiment that uses videos featuring trained actors ( N = 1,366), supported by two supplementary studies ( N = 1,590). Taken together, this work sheds light on the ways the increasing emphasis on passion in contemporary workplaces may exacerbate gender inequalities. Progressing our understanding of gender bias beyond gendered reactions to criteria that penalize women (i.e., backlash), our work also unveils a novel and particularly pernicious form of gender bias driven by gendered inferences about passion that advantage men. Supplemental Material: The online supplement is available at https://doi.org/10.1287/orsc.2023.18018 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.18018},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1438-1465},
  shortjournal = {Organ. Sci.},
  title        = {Passion penalizes women and advantages (Unexceptional) men in high-potential designations},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Board gender diversity reforms around the world: The impact on corporate innovation. <em>ORSC</em>, <em>36</em>(4), 1416-1437. (<a href='https://doi.org/10.1287/orsc.22.16956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to persistent gender inequality in corporate leadership, many countries have implemented board gender diversity reforms, either through legislation or by revising governance codes for board appointments. Whereas these reforms aim to enhance women’s representation and influence in leadership roles, their effects on corporate outcomes, such as innovation, remain unclear. This study develops a two-mechanism institutional contingency model to investigate how board gender diversity reforms affect firm innovation through representation and empowerment mechanisms. Using a unique hand-collected data set on worldwide board gender diversity reforms and a quasi-experimental difference-in-differences design, we find that these reforms significantly improve innovation outcomes, with the empowerment mechanism having a stronger positive effect than the representation mechanism. Additionally, we show that rule-based reforms, although they are effective at increasing female board representation, often lead to symbolic compliance and tokenism, which limits their ability to enhance innovation. In contrast, comply-or-explain reforms, which emphasize empowerment and genuine engagement, yield more meaningful progress in firm innovation. Our findings also reveal that countries with higher prereform gender disparities and a larger pool of qualified female directors experience greater innovation gains following the implementation of these reforms. By distinguishing between the effects of representation and empowerment, this study provides a nuanced understanding of how gender diversity regulations can serve as catalysts for innovation and offers valuable insights for policymakers designing reforms to promote gender equality and economic outcomes. Funding: K. T. Wang and L. Cui gratefully acknowledge financial support from the College of Business and Economics at the Australian National University, and N. Z. Zhu acknowledges financial support from the School of Management at Zhejiang University. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.22.16956 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.22.16956},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1416-1437},
  shortjournal = {Organ. Sci.},
  title        = {Board gender diversity reforms around the world: The impact on corporate innovation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discursive struggles and contested stigma extensions: Explaining the gradual stigmatization of the U.S. tobacco industry. <em>ORSC</em>, <em>36</em>(4), 1384-1415. (<a href='https://doi.org/10.1287/orsc.2022.16145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite extensive research on stigma, we still lack a comprehensive understanding of how industry stigmatization progresses when constantly contested by resourceful incumbents. To shed light on this issue, we focus on the revealing case of the U.S. tobacco industry between 1980 and 2016. Combining structural topic modeling and discourse analysis to explore the extensive media discussions surrounding the industry, we find that stigmatization unfolds through three phases. These were each characterized by discursive struggles, which resulted in contested stigma extensions about establishing harm (1980–1992), assigning responsibility (1993–2010), and creating new norms (2011–2016). We develop a process model highlighting three key mechanisms in stigmatization processes: attention , which shifts focus to new issues and discussions; stigma construction work , where the stigmatizers use discursive strategies to establish stigma; and resistance work , where targets use discursive strategies to slow down stigmatization. The interplay of these mechanisms reveals that stigmatization is neither linear nor complete but characterized by partial and contested stigma extensions . While acknowledging the limitations of our case, our study advances research by showing how industry stigmatization persists even when challenged, opening new avenues for future research in related settings. Supplemental Material: The online appendices are available at https://doi.org/10.1287/orsc.2022.16145 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16145},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1384-1415},
  shortjournal = {Organ. Sci.},
  title        = {Discursive struggles and contested stigma extensions: Explaining the gradual stigmatization of the U.S. tobacco industry},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minding the gap: How perspective-taking and status reflexivity help black women executives to relate across difference at work. <em>ORSC</em>, <em>36</em>(4), 1357-1383. (<a href='https://doi.org/10.1287/orsc.2020.14375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workplace relationships are a necessary and critical component of being able to perform one’s job and advance in one’s career. The personal and professional resources required for navigating relationships with dissimilar work colleagues can be particularly costly for those in minority groups who are most often different from their relational partners. Drawing from interviews conducted with Black women executives, we examined how these women experience relational triggers that emphasize their differences from others because of their limited numbers at their level. Our findings indicate that Black women executives respond to these relational triggers by engaging in perspective-taking and status reflexivity to understand others’, and their own, perspectives on the identity and status differentials present in the interaction. Through an introspective process, these women assess and address gaps in how they believe their partners see them and how they see themselves, which prompts them to either reduce or maintain perceived gaps depending on the importance of the interaction partner. We also explore how reducing or maintaining the perceived gap ultimately influences how Black women think, feel, or behave toward their relational partner (i.e., relational valence) in ways that may shape how they interpret future interactions. This study advances workplace relationships research by integrating intersectionality literature and by considering how minority perspective-taking and status reflexivity can be useful in navigating relationships across difference.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2020.14375},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1357-1383},
  shortjournal = {Organ. Sci.},
  title        = {Minding the gap: How perspective-taking and status reflexivity help black women executives to relate across difference at work},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counter-activism against ideological opponents: Evidence based on the competitive dynamics of corporate engagement in advocacy giving. <em>ORSC</em>, <em>36</em>(4), 1333-1356. (<a href='https://doi.org/10.1287/orsc.2023.17382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing body of literature has sought to understand the strategic drivers of corporate activism, emphasizing the ideological composition of a firm’s workforce as its salient internal driver. Despite these advancements, however, the heterogeneity in its effect remains underexplored, leaving it unclear when employee ideology is more likely to translate into activism. I address this gap by considering competitors’ actions—another important driver uncovered in the scholarship—as a key source of heterogeneity. I examine how these actions can shape the external context in which a firm’s employees evaluate the value of its activism, thereby influencing the firm’s incentives to act on employees’ preferences. My central conjecture is that activism by so-called ideological opponents of a firm’s employee base will be a particularly strong driver of its own activism. This is because actions taken by competitors who hold stances opposite to those valued by a firm’s employees can create a temporal window during which the firm can gain enough employee appreciation for its activism—particularly counter-activism opposing the stance of the competitor. Leveraging the donation records of corporate foundations to advocacy nonprofits as a novel channel for studying corporate activism, I find support for this conjecture. Consistent with my theory, I further find that this effect is driven by firms whose employees hold homogeneous ideological preferences and arises primarily around issues with strong ideological contention. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17382 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17382},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1333-1356},
  shortjournal = {Organ. Sci.},
  title        = {Counter-activism against ideological opponents: Evidence based on the competitive dynamics of corporate engagement in advocacy giving},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How psychological barriers constrain men’s interest in gender-atypical jobs and facilitate occupational segregation. <em>ORSC</em>, <em>36</em>(4), 1314-1332. (<a href='https://doi.org/10.1287/orsc.2023.17550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scholarship regarding occupational gender segregation has almost exclusively focused on women’s experiences (e.g., as targets of discrimination in masculine domains), yet understanding factors that perpetuate men’s underrepresentation in traditionally feminine occupations is equally important. We examine a consequential dynamic early in the job search process in which individuals come to learn that an occupation that fits them is perceived as feminine versus masculine. Our research develops and tests the prediction that femininity or masculinity of occupations will exert a stronger impact on men’s (versus women’s) interest in them such that men will be less interested in gender-atypical occupations than women. Across five studies ( n = 4,477), we consistently observed robust evidence for this prediction among diverse samples, including high school students (Study 1), unemployed job seekers (Study 2), U.S. adults (Study 3), and undergraduates (Study 4) and using experimental and archival methods. We observed this asymmetry after controlling for alternative accounts related to economic factors (e.g., expected salary), suggesting that they alone cannot fully explain men’s lack of interest in feminine occupations as previously discussed in the literature. Further, we consistently observed that men, compared with women, show heightened sensitivity to gender-based occupational status, and this greater sensitivity explains men’s (versus women’s) reduced interest in gender-atypical occupations. Though past scholarship suggests that increasing pay is key to stoking men’s interest in feminine occupations, our research suggests that targeting men’s underlying psychological concern—sensitivity to gender-based occupational status—may be an underappreciated pathway to reducing gender segregation. Supplemental Material: The data, materials, preregistration, and ancillary analyses for all studies are available at https://osf.io/h4mgx/?view_only=9a4dbfc9d122417c880354d6b3462072 and at https://doi.org/10.1287/orsc.2023.17550 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17550},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1314-1332},
  shortjournal = {Organ. Sci.},
  title        = {How psychological barriers constrain men’s interest in gender-atypical jobs and facilitate occupational segregation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embrace the unexpected: How organizations foster participatory improvisation with customers. <em>ORSC</em>, <em>36</em>(4), 1288-1313. (<a href='https://doi.org/10.1287/orsc.2023.17551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores how organizations, together with their customers, solve problems in the face of disruptions, a process I call “participatory improvisation.” Drawing on interviews, ethnographic observations, and archival data collected from an underground restaurant, Secret Kitchen, and the theory of interaction order, I develop a process model of participatory improvisation with a two-part structure. First, I find that an organization must lay the foundation for participatory improvisation by establishing alternative conventions (e.g., expect and embrace the unexpected). Second, these conventions facilitate mutual face work by both the organization and customers in response to disruptions, thereby protecting interactions from breakdowns. When alternative conventions are not established, participatory improvisation may be ineffective, and interactions may be severely threatened. These findings contribute to the literature on organizational improvisation by uncovering how organizations can foster participatory improvisation and how it unfolds in situ. They also reveal an alternate way for customer-facing organizations to achieve their goals beyond routinization. Funding: This work was supported by the Ewing Marion Kauffman Foundation Dissertation Grant.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17551},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1288-1313},
  shortjournal = {Organ. Sci.},
  title        = {Embrace the unexpected: How organizations foster participatory improvisation with customers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design- and theory-based approaches to strategic decisions. <em>ORSC</em>, <em>36</em>(4), 1271-1287. (<a href='https://doi.org/10.1287/orsc.2023.18245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a unified framework to examine the implications of two primary approaches to strategic decision making under uncertainty: designing and shaping future scenarios vis-à-vis testing theories about future scenarios. We conducted a three-arm randomized controlled trial involving 308 early stage entrepreneurs, dividing them into three groups—design-based training, theory-based training, and a control group—and tracked them over approximately 1.5 years. Our findings reveal that both approaches reduce the need for information in decision making and lead to higher commitment rates. The design-based approach encourages action despite negative beliefs, resulting in less frequent and later project termination. In contrast, the theory-based approach promotes a more conservative termination rule, leading to earlier and more frequent project abandonment. Although the theory-based approach is associated with higher average performance upon survival, the design-based approach fosters breakthroughs for decision makers. In sum, the design-based approach is well-suited for innovative ventures that gather information to shape their environment, whereas the theory-based approach is optimal for pursuing high performance under lower degrees of uncertainty. Funding: The research leading to these results has received funding from the Italian Ministry for Education, “Entrepreneurs as Scientists: When and How Start-ups Benefit from a Scientific Approach to Decision Making” project [Grant PRIN 2017, Prot. 2017PM7R52, CUP J44I20000220001]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.18245 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.18245},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1271-1287},
  shortjournal = {Organ. Sci.},
  title        = {Design- and theory-based approaches to strategic decisions},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The six dimensions of strong theory. <em>ORSC</em>, <em>36</em>(4), 1242-1270. (<a href='https://doi.org/10.1287/orsc.2024.19018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important features on which to judge the merit of any academic paper is the strength of its theory. Although commentary about what constitutes strong theory is widespread, there is no holistic account of the full range of existing perspectives. To address this oversight, I construct a typology composed of six dimensions of strong theory: importance, interestingness, actionability, generality, simplicity, and accuracy. This typology provides a lens to examine a vexing problem: the effort to increase the strength of a theory on one dimension will usually compromise its strength on others. Despite this reality, authors experience pressure in the review process to optimize theories on all six dimensions. I explain how the expectation to build theories that cannot possibly be built (what I call the fruitless search for unicorn theories) is driven by historical forces. The study of organizations emerged not from one source but from a half-dozen distinct traditions, including applied scientists who study time-sensitive problems, disciplinary scholars who identify universal laws of human behavior, and empiricists who prefer a tight link between concepts and measures. Review teams are often composed of referees who hail from different traditions and possess divergent views on theory. Even when each reviewer has concerns that are reasonable in isolation, authors often confront unrealistic expectations when reviewers’ preferences are aggregated. To mitigate this problem, I recommend that authors, reviewers, and editors (1) prioritize fewer dimensions in any single theory and (2) emphasize distinct dimensions in different theories on the same topic.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2024.19018},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1242-1270},
  shortjournal = {Organ. Sci.},
  title        = {The six dimensions of strong theory},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How social movements catalyze firm innovation. <em>ORSC</em>, <em>36</em>(4), 1221-1241. (<a href='https://doi.org/10.1287/orsc.2023.17497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the impact that social movements have on firm innovation through private politics. We argue that firms strategically respond to private politics by investing in new technologies that address movement-advocated issues material to firms’ performance. Although both contentious private politics—when activists contentiously target firms—and cooperative private politics—when activists and firms collaborate—catalyze innovation, they do so in different ways. Contentious private politics increases the amount of innovation that firms undertake by drawing managerial attention to movement-advocated issues material to the firm, prompting search for solutions to those issues. Conversely, cooperative private politics provides firms access to new knowledge that encourages firms to search for solutions in areas more distant from their existing knowledge and in so doing, increase innovation involving distant recombination on material issues. We find support for our arguments in a matched sample of firms contentiously targeted and with activist collaborations on climate change issues and firms that were not targets of private politics on those issues but had otherwise similar histories of climate-related innovation and relationships with climate movements and other environmental movements. Supplementary analyses corroborate the mechanisms that undergird our theoretical predictions; contentious private politics is associated with more innovation closer to a firm’s expertise, whereas cooperative private politics is associated with innovations that draw on more distant knowledge. We also find that when collaboration follows contention, their respective impacts on innovation are reduced, which may result from firms seeking collaborations for their legitimacy-granting benefits after contention rather than the learning opportunities they offer. Funding: Funding for this research was provided by the Strategic Management Society Strategy Research Foundation [dissertation grant] and Wharton’s Mack Institute for Innovation Management. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17497 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17497},
  journal      = {Organization Science},
  month        = {7-8},
  number       = {4},
  pages        = {1221-1241},
  shortjournal = {Organ. Sci.},
  title        = {How social movements catalyze firm innovation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="serv">SERV - 4</h2>
<ul>
<li><details>
<summary>
(2025). Clothing consumers’ aesthetic preferences and sustainable marketing model system based on kansei engineering. <em>SERV</em>, <em>17</em>(2-3), 111-126. (<a href='https://doi.org/10.1287/serv.2024.0179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {China’s online retail sales growth rate of men’s plain color shirts had declined because they could not meet customers’ aesthetic value needs. This study aims to determine consumers’ aesthetic value preferences and corresponding design elements. Based on Kansei Engineering, a semantic differential scale questionnaire was conducted with seven crucial Kansei words and styling specimens. Then, researchers validated these Kansei words using principal component analysis. More significantly, this study used partial least squares to construct a relationship between crucial Kansei words and styling design elements. Furthermore, researchers obtained the matching colors corresponding to Kansei words in the color image scale. Thus, Kansei words, styling design elements, and corresponding colors made up the recommendation guidelines. Lastly, questionnaires and interviews were used to verify the effectiveness of these guidelines. These recommendation guidelines were the most important results of this study. Besides, this study also found that the recent popular oversized fit appeared in distinctive and casual styles, but the audience is relatively small. Purchasers preferred a one-button square cuff, and the separate left-side pocket is more popular than both-side pockets. Purchasers liked the wide-spread collar compared with the classical pointed collar because it appeared most frequently in recommendation guidelines. Classic fit was still the mainstream, but its popularity would decline. In addition, modern fit and slim fit would become increasingly popular. Funding: B. Ge has two funding projects (100%): the Soft Science Research Program of Henan Province, China [Project 252400410636] and the general project of Colleges and Universities Humanities and Social Sciences of the Henan Provincial Department of Education, China [Project 2025-ZDJH-776].},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2024.0179},
  journal      = {Service Science},
  month        = {6-9},
  number       = {2-3},
  pages        = {111-126},
  shortjournal = {Serv. Sci.},
  title        = {Clothing consumers’ aesthetic preferences and sustainable marketing model system based on kansei engineering},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaboration between an airline and railways to reduce the cancellation-risk of waitlisted passengers and improve seat utilization. <em>SERV</em>, <em>17</em>(2-3), 92-110. (<a href='https://doi.org/10.1287/serv.2024.0146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a setting in which two rivals, an airline and railways, create value through a new joint service that reduces the cancellation-risk of waitlisted passengers. This setting is motivated by a recent collaboration between a railway company and two low-cost airlines in India. Under this arrangement, the railways offers train ticket holders whose reservations are not confirmed the option to upgrade to a flight ticket on the same route for an additional fee. The airline sets the price for the upgrade while the railways charges the airline a referral commission. The new service creates value for waitlisted passengers and thereby stimulates additional demand for the railways and the airline. We analyze the strategic interactions that arise from this collaboration, derive the equilibrium upgrade prices and commission, and compare the outcomes to those under centralized decision-making. Our findings demonstrate that cooperation between two competing service providers can create a mutually beneficial outcome for both firms as well as for consumers. Notably, the airline and the railways can generate profit from the collaboration only if they engage in a sequential game; a simultaneous-move game fails to generate any value. Furthermore, we identify scenarios in which increasing railway capacity also benefits the airline by stimulating additional demand. Finally, we analyze the case of strategic consumers who reduce the value created from the collaboration, and we show that the price difference between early and late booking mitigates such strategic behavior. Supplemental Material: The online appendix is available at https://doi.org/10.1287/serv.2024.0146 .},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2024.0146},
  journal      = {Service Science},
  month        = {6-9},
  number       = {2-3},
  pages        = {92-110},
  shortjournal = {Serv. Sci.},
  title        = {Collaboration between an airline and railways to reduce the cancellation-risk of waitlisted passengers and improve seat utilization},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Psychological distance and algorithm aversion: Congruency and advisor confidence. <em>SERV</em>, <em>17</em>(2-3), 74-91. (<a href='https://doi.org/10.1287/serv.2023.0054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employees and consumers have varying preferences between human and algorithmic advisors. Drawing on construal level theory, I hypothesize that individual differences in algorithm aversion can be explained by the perception that algorithms are psychologically farther away than human advisors. The first set of studies ( n = 266) shows that algorithms are viewed as abstract and distant compared with humans, even when their outputs are perceived at a low-level construal, challenging prior research. Leveraging construal congruency, the second set of studies ( n = 1,148) shows that farther within-task psychological distance generally increases preference for algorithmic advisors due to differences in advisor confidence. Specifically, I contribute to the literature by showing that a far psychological distance within a task reduces confidence in human advisors. In contrast, confidence in algorithms remains stable, increasing algorithm appreciation at farther within-task distances. History: This paper has been accepted for the Service Science Special Section on Navigating the Use of Technology in Service Marketing. Supplemental Material: The online appendix is available at https://doi.org/10.1287/serv.2023.0054 .},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2023.0054},
  journal      = {Service Science},
  month        = {6-9},
  number       = {2-3},
  pages        = {74-91},
  shortjournal = {Serv. Sci.},
  title        = {Psychological distance and algorithm aversion: Congruency and advisor confidence},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to service science special section on navigating the use of technology in service marketing. <em>SERV</em>, <em>17</em>(2-3), 73. (<a href='https://doi.org/10.1287/serv.2025.intro.v17.n2-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SERV},
  doi          = {10.1287/serv.2025.intro.v17.n2-3},
  journal      = {Service Science},
  month        = {6-9},
  number       = {2-3},
  pages        = {73},
  shortjournal = {Serv. Sci.},
  title        = {Introduction to service science special section on navigating the use of technology in service marketing},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="stsc">STSC - 5</h2>
<ul>
<li><details>
<summary>
(2025). Signposts for problemistic search: Reference points and adaptation in rugged landscapes. <em>STSC</em>, <em>10</em>(3), 263-279. (<a href='https://doi.org/10.1287/stsc.2023.0072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reference points form an essential element of organizations’ problemistic search and adaptation behavior. Yet, if search is triggered by shortfalls compared with peers but alternatives are discovered on the fly, it is not clear whether and when peer comparison leads to better search outcomes. We contribute to the literature by studying how reference points guide search and which outcomes they allow organizations to achieve. Specifically, we develop a model of search in complex landscapes in which agents’ search behavior is guided by an upper (aspiration-level) social reference point and a lower (survival-point) social reference point. In our model, agents move across a subjective “terraced” landscape that is a simplified transformation of the “real” one. The vertical positions and shapes of these terraces are determined by the agents’ reference points and change over time as a result of their own and their peers’ performance evolution. In turn, these terraces define the search space that is navigated and the outcomes that can be reached. We show that the upper and lower bounds play fundamentally different roles in the search process, with the upper bound being more important in the short run and the lower bound more important in the long run. Studying heterogeneous populations, we find that reference points drive dynamic trade-offs between how easily decision makers can reach their aspiration level and how much they benefit from doing so. We highlight the importance of both internal fit between reference points and external fit with environmental factors. Supplemental Material: The online appendix is available at https://doi.org/10.1287/stsc.2023.0072 .},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2023.0072},
  journal      = {Strategy Science},
  month        = {9},
  number       = {3},
  pages        = {263-279},
  shortjournal = {Strat. Sci.},
  title        = {Signposts for problemistic search: Reference points and adaptation in rugged landscapes},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extending intellectual property research in copyright: A new data set from the U.S. copyright office. <em>STSC</em>, <em>10</em>(3), 245-262. (<a href='https://doi.org/10.1287/stsc.2023.0130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a newly available data set containing U.S. copyright records for 1978–2021. The data include nearly 19 million copyright registrations, as well as more than 12 million records of copyright renewals, terminations of granted rights, rights transfers, and other activities. The data include both raw and processed files, along with code books, documentation, and our data processing scripts; we provide tips and guidelines for using these data. We facilitate further research by linking copyright registration records with firm identifiers in Compustat as well as U.S. federal litigation data. We then use the data for three descriptive exercises. First, we characterize the relative usage of patenting and copyright protection across firms and industries. Second, we document the propensities for firms registering copyrights to be involved in copyright litigation. Third, we compare actual data on the incidence of copyright and patent registration with commonly used proxies: advertising and research and development expenditure. We hope that the availability of these data can facilitate progress on copyright research to parallel the broader intellectual property literature that has blossomed since patent data became widely available. Supplemental Material: The online appendix is available at https://doi.org/10.1287/stsc.2023.0130 .},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2023.0130},
  journal      = {Strategy Science},
  month        = {9},
  number       = {3},
  pages        = {245-262},
  shortjournal = {Strat. Sci.},
  title        = {Extending intellectual property research in copyright: A new data set from the U.S. copyright office},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Management by originals: Inventor CEOs and firms’ strategic change. <em>STSC</em>, <em>10</em>(3), 226-244. (<a href='https://doi.org/10.1287/stsc.2023.0058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build upon theories and studies regarding inventors and strategic leadership to explain how inventor CEOs are likely to drive a firm’s strategic change. We explain why inventor CEOs have the unique attributes of divergent thinking and intrinsic motivation and, accordingly, pursue strategies that deviate from the past. We further suggest that the positive relationship between inventor CEO status and strategic change is strengthened by CEO liberalism, CEO career variety, and industry dynamism. Using a comprehensive longitudinal database of S&P 1500 firms, we test our theoretical predictions and find empirical support for most of our hypotheses except for the moderating effect of industry dynamism. In highlighting the prevalence of inventor CEOs and showing how this achievement-based experience of CEOs is related to strategic change, our study identifies an important and novel antecedent of this important strategic outcome. Supplemental Material: The online appendix is available at https://doi.org/10.1287/stsc.2023.0058 .},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2023.0058},
  journal      = {Strategy Science},
  month        = {9},
  number       = {3},
  pages        = {226-244},
  shortjournal = {Strat. Sci.},
  title        = {Management by originals: Inventor CEOs and firms’ strategic change},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From church leadership to firm leadership: Religion of early state residents, state institutions, and present-day corporate female executives. <em>STSC</em>, <em>10</em>(3), 207-225. (<a href='https://doi.org/10.1287/stsc.2024.0244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I argue that early residents of a U.S. state have left imprints in the state constitution to reflect their religious preferences. Such imprints help explain the varying constitutions across U.S. states, which in turn affect the present-day female representation in corporate leadership. Analyzing a firm-level data set supplemented by a state-level sample and extensive robustness checks, I find that the prevalence of Protestantism at the time of a U.S. state’s admission to the Union is positively related to female representation in the leadership teams of S&P 1500 firms. One mechanism for the persistence of the Protestant imprinting is state constitutions’ emphasis on equality issues. In addition, the historical Catholic immigration to different states, serving as a counter-imprinting force, has weakened this effect. My study contributes to imprinting theory by considering dynamics and developing a regional-level institutional imprinting perspective. I also contribute to a more nuanced understanding of historical antecedents and contemporary firm consequences of subnational institutions, tracing the persistent heterogeneities among firms to some deep historical roots.},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2024.0244},
  journal      = {Strategy Science},
  month        = {9},
  number       = {3},
  pages        = {207-225},
  shortjournal = {Strat. Sci.},
  title        = {From church leadership to firm leadership: Religion of early state residents, state institutions, and present-day corporate female executives},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CEO succession and patenting in family firms. <em>STSC</em>, <em>10</em>(3), 185-206. (<a href='https://doi.org/10.1287/stsc.2023.0122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much research has examined whether family firms led by family or professional CEOs differ in terms of financial performance. Yet, whether family CEOs promote or hinder innovation remains an open question. Given the importance of innovation as well as the ubiquity of family enterprises, this is a major gap. In this study, we focus on a large sample of Danish firms and examine the patenting performance (in terms of patent counts and citations) of these firms. We exploit the gender of departing CEOs’ first-born children to yield exogenous variations in the decision to appoint a family or professional CEO. Our difference-in-differences results indicate that appointing a family CEO leads to an increase in patent counts and citations relative to appointing a professional CEO. These effects are driven by incoming family CEOs who hold a university degree in engineering and, to a lesser extent, business. Appointing a family CEO also leads to fewer job terminations, which suggests that the increase in patenting might stem from higher job stability and tolerance for failure among employees. Our study has implications for the growing literature on the management of family firms and, more broadly, for strategy research on CEO succession. Funding: The ICRIOS Research Center at Bocconi University provided financial support.},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2023.0122},
  journal      = {Strategy Science},
  month        = {9},
  number       = {3},
  pages        = {185-206},
  shortjournal = {Strat. Sci.},
  title        = {CEO succession and patenting in family firms},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="stsy">STSY - 3</h2>
<ul>
<li><details>
<summary>
(2025). Almost sure one-endedness of a random graph model of distributed ledgers. <em>STSY</em>, <em>15</em>(3), 252-272. (<a href='https://doi.org/10.1287/stsy.2023.0045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed ledgers (DLs) are modern decentralized databases where trusted network members can transparently update data while maintaining security. IOTA is an exemplar alternative to the well-known Bitcoin protocol, and such alternatives, which aim to overcome shortcomings, have gained increasing attention recently. These systems warrant deeper theoretical analysis using directed acyclic graph (DAG) models. One essential property of a properly functioning DL is that all network members holding a copy of the database agree on the sequence of information added, which is referred to as consensus and is known to be related to a structural property of DAGs called one-endedness. In this paper, we consider a model of a DL with sequential stochastic arrivals that mimic IOTA’s attachment rules. Although the resulting DAG model is more complex than Bitcoin, we demonstrate that as time goes to infinity, the IOTA DAG almost surely achieves one-endedness.},
  archive      = {J_STSY},
  doi          = {10.1287/stsy.2023.0045},
  journal      = {Stochastic Systems},
  month        = {9},
  number       = {3},
  pages        = {252-272},
  shortjournal = {Stoch. Syst.},
  title        = {Almost sure one-endedness of a random graph model of distributed ledgers},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal control of queueing systems with error-prone servers. <em>STSY</em>, <em>15</em>(3), 220-251. (<a href='https://doi.org/10.1287/stsy.2024.0061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a Markovian tandem line with finite intermediate buffers and an equal number of stations and servers. Servers are flexible but noncollaborative, so that a job can be processed by at most one server at any time. When a job is being processed, it can be damaged and wasted depending on the proficiency of the server. We identify the dynamic server assignment policy that maximizes the long-run average throughput of the system with two stations and two servers. We find that the optimal policy is either a single or a double threshold policy on the number of jobs in the buffer, where the thresholds depend on the service rates and defect probabilities of the two servers at the two stations. For larger systems, we show that the optimal policy may involve server idling and that improving the service rate at any station is always beneficial. Finally, we propose heuristic server assignment policies motivated by experimentation for small systems with finite buffers and analysis of larger systems with infinite buffers. Numerical results suggest that our heuristics yield near-optimal performance. Funding: This research was supported by the National Science Foundation [Grants CMMI-1536990 and CMMI-2127778]. S. Andradóttir was also supported by the National Science Foundation [Grant CMMI-2348409].},
  archive      = {J_STSY},
  doi          = {10.1287/stsy.2024.0061},
  journal      = {Stochastic Systems},
  month        = {9},
  number       = {3},
  pages        = {220-251},
  shortjournal = {Stoch. Syst.},
  title        = {Optimal control of queueing systems with error-prone servers},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Queueing, predictions, and large language models: Challenges and open problems. <em>STSY</em>, <em>15</em>(3), 195-219. (<a href='https://doi.org/10.1287/stsy.2025.0106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Queueing systems present many opportunities for applying machine learning predictions, such as estimated service times, to improve system performance. This integration raises numerous open questions about how predictions can be effectively leveraged to improve scheduling decisions. Recent studies explore queues with predicted service times, typically aiming to minimize job time in the system. We review these works, highlight the effectiveness of predictions, and present open questions on queue performance. We then move to consider an important practical example of using predictions in scheduling, namely large language model (LLM) systems, which presents novel scheduling challenges and highlights the potential for predictions to improve performance. In particular, we consider LLMs performing inference. Inference requests (jobs) in LLM systems are inherently complex; they have variable inference times, dynamic memory footprints that are constrained by key-value store memory limitations, and multiple possible preemption approaches that affect performance differently. We provide background on the important aspects of scheduling in LLM systems and introduce new models and open problems that arise from them. We argue that there are significant opportunities for applying insights and analysis from queueing theory to scheduling in LLM systems. Funding: M. Mitzenmacher was supported in part by the National Science Foundation [Grant CCF-2101140]. R. Shahout was supported in part by the Schmidt Futures Initiative and the Zuckerman Institute. M. Mitzenmacher and R. Shahout were supported in part the National Science Foundation [Grants CNS-2107078 and DMS-2023528].},
  archive      = {J_STSY},
  doi          = {10.1287/stsy.2025.0106},
  journal      = {Stochastic Systems},
  month        = {9},
  number       = {3},
  pages        = {195-219},
  shortjournal = {Stoch. Syst.},
  title        = {Queueing, predictions, and large language models: Challenges and open problems},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="trsc">TRSC - 10</h2>
<ul>
<li><details>
<summary>
(2025). Special issue on machine learning methods for urban passenger mobility. <em>TRSC</em>, <em>59</em>(4), iii-vi. (<a href='https://doi.org/10.1287/trsc.2025.intro.v59.n4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2025.intro.v59.n4},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {iii-vi},
  shortjournal = {Trans. Sci.},
  title        = {Special issue on machine learning methods for urban passenger mobility},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end learning of user equilibrium: Expressivity, generalization, and optimization. <em>TRSC</em>, <em>59</em>(4), 853-882. (<a href='https://doi.org/10.1287/trsc.2023.0489'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes an end-to-end learning framework for constructing transportation network equilibrium models. The proposed framework directly learns supply and demand components as well as equilibrium states from multiday traffic state observations. Specifically, it parameterizes unknown model components with neural networks and embeds them in an implicit layer to enforce user equilibrium conditions. By minimizing the differences between the predicted and observed traffic states, parameters for supply and demand components are simultaneously estimated. We demonstrate that the end-to-end framework is expressive: when parameterized with sufficiently large neural networks, it can replicate any unique, differentiable equilibrium state that solves a well-posed variational inequality. Moreover, it can generalize to new, unseen data when trained with sufficient observations. For efficient training, we design an autodifferentiation-based gradient descent algorithm that handles link- and path-based user equilibrium constraints and ensures local convergence. The proposed framework is demonstrated using three synthesized data sets. History: This paper has been accepted for the Transportation Science Special Issue on Machine Learning Methods for Urban Passenger Mobility. Funding: This work described in this paper was partly supported by research grants from the USDOT Center for Connected and Automated Transportation and the National Science Foundation, Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-2233057].},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0489},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {853-882},
  shortjournal = {Trans. Sci.},
  title        = {End-to-end learning of user equilibrium: Expressivity, generalization, and optimization},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward human-like trajectory prediction for autonomous driving: A behavior-centric approach. <em>TRSC</em>, <em>59</em>(4), 823-852. (<a href='https://doi.org/10.1287/trsc.2023.0366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the trajectories of vehicles is crucial for the development of autonomous driving systems, particularly in complex and dynamic traffic environments. In this study, we introduce human-like trajectory prediction (HiT), a novel model designed to enhance trajectory prediction by incorporating behavior-aware modules and dynamic centrality measures. Unlike traditional methods that primarily rely on static graph structures, HiT leverages a dynamic framework that accounts for both direct and indirect interactions among traffic participants. This allows the model to capture the subtle, yet significant, influences of surrounding vehicles, enabling more accurate and human-like predictions. To evaluate HiT’s performance, we conducted extensive experiments using diverse and challenging real-world data sets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results demonstrate that HiT consistently outperforms state-of-the-art models across multiple metrics, particularly excelling in scenarios involving aggressive driving behaviors. This research presents a significant step forward in trajectory prediction, offering a more reliable and interpretable approach for enhancing the safety and efficiency of autonomous driving systems. History: This paper has been accepted for the Transportation Science Special Issue on Machine Learning Methods for Urban Passenger Mobility. Funding: This work was supported by the Shenzhen-Hong Kong-Macau Science and Technology Program Category C [SGDX20230821095159012], University of Macau [SRG2023-00037-IOTSC, MYRG-GRG2024-00284-IOTSC], the Science and Technology Development Fund of Macau [0021/2022/ITP, 0122/2024/RIB2 and 001/2024/SKL], the State Key Lab of Intelligent Transportation System [2024-B001], and the Jiangsu Provincial Science and Technology Program [BZ2024055]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0366 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0366},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {823-852},
  shortjournal = {Trans. Sci.},
  title        = {Toward human-like trajectory prediction for autonomous driving: A behavior-centric approach},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling metro passenger routing choices with a fully differentiable end-to-end simulation-based optimization (SBO) approach. <em>TRSC</em>, <em>59</em>(4), 802-822. (<a href='https://doi.org/10.1287/trsc.2024.0557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metro systems in densely populated urban areas are often complicated, with some origin-destinations (OD) having multiple routes with similar travel times, leading to complex passenger routing behaviors. To improve modeling and calibration, this paper proposes a novel passenger route choice model with a metro simulator that accounts for passenger flows, queueing, congestion, and transfer delays. A novel, data-driven approach that utilizes a fully differentiable end-to-end simulation-based optimization (SBO) framework is proposed to calibrate the model, with the gradients calculated automatically and analytically using the iterative backpropagation (IB) algorithm. The SBO framework integrates data from multiple sources, including smart card data and train loadings, to calibrate the route choice parameters that best match the observed data. The full differentiability of the proposed framework enables it to calibrate for more than 20,000 passenger route choice ratios, covering every OD pair. To further improve the efficiency of the framework, a matrix-based optimization (MBO) mechanism is proposed, which provides better initial values for the SBO and ensures high efficiency with large datasets. A hybrid optimization algorithm combining MBO and SBO effectively calibrates the model, demonstrating high accuracy with synthetic data from actual passenger OD demands, where hypothesis tests are conducted for accuracies and significances. The accuracies and robustness are validated by experiments with synthetic passenger flow data, offering potential for optimizing passenger flow management in densely populated urban metro systems. Then, the SBO framework is extended for user equilibrium formulations with a crowding-aware route choice model and iterative metro simulations, calibrated by the hybrid optimization algorithm with additional matrix operations. Case studies with actual observed passenger flows are conducted to illustrate the proposed framework with multiple setups, exhibiting the heterogeneity of passenger route choice preferences and providing insights for operation management in the Hong Kong Mass Transit Railway system. History: This paper has been accepted for the Transportation Science Special Issue on Machine Learning Methods for Urban Passenger Mobility. Funding: This work was supported by the General Research Fund of the Research Grants Council of Hong Kong [Grant 16219224], the Key Research and Development Program of Hubei Province [Grant 2023BAB076], and the National Natural Science Foundation of China [Grant 72001162]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2024.0557 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0557},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {802-822},
  shortjournal = {Trans. Sci.},
  title        = {Modeling metro passenger routing choices with a fully differentiable end-to-end simulation-based optimization (SBO) approach},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffIRM: A diffusion-augmented invariant risk minimization framework for spatiotemporal prediction over graphs. <em>TRSC</em>, <em>59</em>(4), 782-801. (<a href='https://doi.org/10.1287/trsc.2024.0562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal prediction over graphs (STPG) is challenging because real-world data suffer from the out-of-distribution (OOD) generalization problem, where test data follow different distributions from training ones. To address this issue, invariant risk minimization (IRM) has emerged as a promising approach for learning invariant representations across different environments. However, IRM and its variants are originally designed for Euclidean data, such as images, and may not generalize well to graph-structure data, such as spatiotemporal graphs, because of spatial correlations in graphs. To overcome the challenge posed by graph-structure data, the existing graph OOD methods adhere to the principles of invariance existence (i.e., there exist invariant features that consistently relate to the label across various environments) or environment diversity (i.e., diversifying training environments increases the likelihood that test environments align with training ones). However, there is little research that combines both principles in the STPG problem. A combination of the two is crucial for efficiently distinguishing between invariant features and spurious ones. In this study, we fill in this research gap and propose a diffusion-augmented invariant risk minimization (diffIRM) framework that combines these two principles for the STPG problem. Our diffIRM contains two processes: (1) data augmentation, and (2) invariant learning. In the data augmentation process, a causal mask generator identifies causal features, and a graph-based diffusion model acts as an environment augmentor to generate augmented spatiotemporal graph data. In the invariant learning process, an invariance penalty is designed using the augmented data and then serves as a regularizer for training the spatiotemporal prediction model. We provide theoretical evidence supporting diffIRM’s ability to identify invariant features. The effectiveness of diffIRM is further demonstrated through experiments on both numerical and real-world data. The numerical data are generated from a known structural causal model (SCM), and our proposed diffIRM successfully identifies the true invariant features. The real-world experiment uses three human mobility data sets, that is, SafeGraph, PeMS04, and PeMS08. Our proposed diffIRM outperforms baselines. Furthermore, our model demonstrates interpretability by discerning invariant features while making predictions. History: This paper has been accepted for the Transportation Science Special Issue on Machine Learning Methods for Urban Passenger Mobility. Funding: This work was supported by the National Science Foundation [Grant 2218809].},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0562},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {782-801},
  shortjournal = {Trans. Sci.},
  title        = {DiffIRM: A diffusion-augmented invariant risk minimization framework for spatiotemporal prediction over graphs},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous multimodal demand imputation and forecasting via graph-guided generative and adversarial network. <em>TRSC</em>, <em>59</em>(4), 763-781. (<a href='https://doi.org/10.1287/trsc.2023.0326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of multimodal transport demand is essential for effective transport planning and management and enables service optimization based on historical and (predicted) future demand. However, dealing with missing data remains a common challenge in multimodal demand analytics. Furthermore, the potential benefits of knowledge sharing across different modes for simultaneous imputation and forecasting have not been thoroughly explored. This study introduces the Graph-guided Generative-Adversarial Imputation and Forecasting Network (GIF) to tackle these challenges. GIF utilizes a Generative Adversarial Network with a generator and a discriminator. The generator simultaneously fills in missing values and predicts future demand, whereas the discriminator differentiates between synthetic and real data. An Encoder-Decoder framework is employed to reconstruct the generated data to the original data to ensure that the important information is preserved. Spatiotemporal features of each mode’s demand are captured via Transformer-encoder layers, whereas the knowledge sharing among multiple modes is facilitated by graph-guided feature fusion of different modes. The proposed method is evaluated on three real-world transport data sets, demonstrating its potential to address forecasting tasks with missing data in multimodal transport systems. Overall, this study provides insights into the effectiveness of cross-modal knowledge sharing and joint imputation and prediction in enhancing the accuracy of multimodal demand prediction. History: This paper has been accepted for the Transportation Science Special Issue on Machine Learning Methods for Urban Passenger Mobility. Funding: Financial support from the National Natural Science Foundation of China [Grants 52325210, 52131204, 52402407, and 72301228], the Research Grants Council of Hong Kong, University Grants Committee [Grants T41-603/20R and N_PolyU521/22], and Hong Kong Polytechnic University [Grants P0039246, P0040900, and P0041316] is gratefully acknowledged.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0326},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {763-781},
  shortjournal = {Trans. Sci.},
  title        = {Simultaneous multimodal demand imputation and forecasting via graph-guided generative and adversarial network},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using multiple biased data sets to recover missing trips with a behaviorally informed model. <em>TRSC</em>, <em>59</em>(4), 743-762. (<a href='https://doi.org/10.1287/trsc.2024.0550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trip generation, a critical first step in travel demand forecasting, requires not only estimating trips from the observed sample data, but also calculating the total number of trips in the population, including both the observed trips and the trips missed from the sample (we call them missing trips in this paper). The latter, how to recover missing trips, is scarcely studied in the academic literature, and the state-of-the-art practice is through the application of sample weights to extrapolate from observed trips to the population total. In recent years, big location-based service (LBS) has become a promising alternative data source (in addition to household travel survey data) in trip generation. Because users self-select into using different mobile services that result in LBS data, selection bias exists in the LBS data, and the kinds of trips excluded or included differ systematically among data sources. This study addresses this issue and develops a behaviorally informed approach to quantify the selection biases and recover missing trips. The key idea is that because biases reflected in different data sources are likely different, the integration of multiple biased data sources will mitigate biases. This is achieved by formulating a capture probability that specifies the probability of capturing a trip in a data set as a function of various behavioral factors (e.g., socio-demographics and area-related factors) and estimating the associated parameters through maximum likelihood or Bayesian methods. This approach is evaluated through experimental studies that test the effects of data and model uncertainty on its ability of recovering missing trips. The model is also applied to two real-world case studies: one using the 2017 National Household Travel Survey data and the other using two LBS data sets. Our results demonstrate the robustness of the model in recovering missing trips, even when the analyst completely mis-specifies the underlying trip generation process and the capture probability functions (for quantifying selection biases). The developed methodology can be scalable to any number of data sets and is applicable to both big and small data sets. History: This paper has been accepted for the Transportation Science Special Issue on Machine Learning Methods for Urban Passenger Mobility. Funding: This work was supported by the Division of Civil, Mechanical and Manufacturing Innovation [Grant 2114260], the National Institute of General Medical Sciences [Grant 1R01GM108731-01A1], and the U.S. Department of Transportation [Grant 69A3551747116]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2024.0550 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0550},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {743-762},
  shortjournal = {Trans. Sci.},
  title        = {Using multiple biased data sets to recover missing trips with a behaviorally informed model},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating erratic measurement errors in network-wide traffic flow via virtual balance sensors. <em>TRSC</em>, <em>59</em>(4), 721-742. (<a href='https://doi.org/10.1287/trsc.2023.0493'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale traffic flow data are collected by numerous sensors for managing and operating transport systems. However, various measurement errors exist in the sensor data and their distributions or structures are usually not known in the real world, which diminishes the reliability of the collected data and impairs the performance of smart mobility applications. Such irregular error is referred to as the erratic measurement error and has not been well investigated in existing studies. In this research, we propose to estimate the erratic measurement errors in networked traffic flow data. Different from existing studies that mainly focus on measurement errors with known distributions or structures, we allow the distributions and structures of measurement errors to be unknown except that measurement errors occur based on a Poisson process. By exploiting the flow balance law, we first introduce the concept of virtual balance sensors and develop a mixed integer nonlinear programming model to simultaneously estimate sensor error probabilities and recover traffic flow. Under suitable assumptions, the complex integrated problem can be equivalently viewed as an estimate-then-optimize problem: first, estimation using machine learning (ML) methods, and then optimization with mathematical programming. When the assumptions fail in more realistic scenarios, we further develop a smart estimate-then-optimize (SEO) framework that embeds the optimization model into ML training loops to solve the problem. Compared with the two-stage method, the SEO framework ensures that the optimization process can recognize and compensate for inaccurate estimations caused by ML methods, which can produce more reliable results. Finally, we conduct numerical experiments using both synthetic and real-world examples under various scenarios. Results demonstrate the effectiveness of our decomposition approach and the superiority of the SEO framework. History: This paper has been accepted for the Transportation Science Special Issue on Machine Learning Methods for Urban Passenger Mobility. Funding: The work described in this paper was supported by the National Natural Science Foundation of China [Grant Project No. 72288101, 72101012, 72301023] and a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China [Grant Project No. PolyU/15206322]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0493 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0493},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {721-742},
  shortjournal = {Trans. Sci.},
  title        = {Estimating erratic measurement errors in network-wide traffic flow via virtual balance sensors},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable dynamic mixture model with full covariance for probabilistic traffic forecasting. <em>TRSC</em>, <em>59</em>(4), 708-720. (<a href='https://doi.org/10.1287/trsc.2024.0547'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning-based multivariate and multistep-ahead traffic forecasting models are typically trained with the mean squared error (MSE) or mean absolute error (MAE) as the loss function in a sequence-to-sequence setting, simply assuming that the errors follow an independent and isotropic Gaussian or Laplacian distributions. However, such assumptions are often unrealistic for real-world traffic forecasting tasks, where the probabilistic distribution of spatiotemporal forecasting is very complex with strong concurrent correlations across both sensors and forecasting horizons in a time-varying manner. In this paper, we model the time-varying distribution for the matrix-variate error process as a dynamic mixture of zero-mean Gaussian distributions. To achieve efficiency, flexibility, and scalability, we parameterize each mixture component using a matrix normal distribution and allow the mixture weight to change and be predictable over time. The proposed method can be seamlessly integrated into existing deep-learning frameworks with only a few additional parameters to be learned. We evaluate the performance of the proposed method on a traffic speed forecasting task and find that our method not only improves model performance but also provides interpretable spatiotemporal correlation structures. History: This paper has been accepted for the Transportation Science Special Issue on Machine Learning Methods for Urban Passenger Mobility. Funding: This work was supported by Institut de Valorisation des Données (IVADO) through its Fundamental Research Funding Program.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0547},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {708-720},
  shortjournal = {Trans. Sci.},
  title        = {Scalable dynamic mixture model with full covariance for probabilistic traffic forecasting},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic traffic forecasting with dynamic regression. <em>TRSC</em>, <em>59</em>(4), 689-707. (<a href='https://doi.org/10.1287/trsc.2024.0560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a dynamic regression (DR) framework that enhances existing deep spatiotemporal models by incorporating structured learning for the error process in traffic forecasting. The framework relaxes the assumption of time independence by modeling the error series of the base model (i.e., a well-established traffic forecasting model) using a matrix-variate autoregressive (AR) model. The AR model is integrated into training by redesigning the loss function. The newly designed loss function is based on the likelihood of a nonisotropic error term, enabling the model to generate probabilistic forecasts while preserving the original outputs of the base model. Importantly, the additional parameters introduced by the DR framework can be jointly optimized alongside the base model. Evaluation on state-of-the-art traffic forecasting models using speed and flow data sets demonstrates improved performance, with interpretable AR coefficients and spatiotemporal covariance matrices enhancing the understanding of the model. History: This paper has been accepted for the Transportation Science Special Issue on Machine Learning Methods for Urban Passenger Mobility. Funding: Financial support from the Natural Sciences and Engineering Research Council of Canada [Discovery Grant RGPIN 2019-05950] is gratefully acknowledged. In addition, V. Z. Zheng received financial support from the FRQNT B2X Doctoral Scholarship Program.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0560},
  journal      = {Transportation Science},
  month        = {7-8},
  number       = {4},
  pages        = {689-707},
  shortjournal = {Trans. Sci.},
  title        = {Probabilistic traffic forecasting with dynamic regression},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
