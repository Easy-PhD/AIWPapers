<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijds">IJDS - 5</h2>
<ul>
<li><details>
<summary>
(2025). Call for Papers—INFORMS journal on data science virtual special issue on the dual edge of AI: Catalyzing and challenging the future of energy systems. <em>IJDS</em>, <em>4</em>(3), iii-iv. (<a href='https://doi.org/10.1287/ijds.2026.cfp.v05.n1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2026.cfp.v05.n1},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {iii-iv},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Call for Papers—INFORMS journal on data science virtual special issue on the dual edge of AI: Catalyzing and challenging the future of energy systems},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based feature selection method under budget constraint for multiclass classification problems. <em>IJDS</em>, <em>4</em>(3), 265-282. (<a href='https://doi.org/10.1287/ijds.2024.0050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel graph-based method for budget-constrained feature selection (GB-BC-FS) in multiclass classification problems. The method identifies a subset of features that complement each other’s ability to distinguish between different classes, thereby utilizing the entire feature space while maintaining the model’s predictive performance and adhering to budget constraints on feature costs. This is achieved through an intuitive heuristic based on a scoring function, allowing users to calibrate the solution provided by GB-BC-FS. The calibration prioritizes selecting features with complementary qualities while minimizing the costs associated with feature collection, under constraint compliance. The approach is designed to handle practical limitations, making it suitable for applications where resources like cost and time are constrained. This not only improves computational efficiency but also aligns with broader implications related to optimizing resource utilization and ensuring practical applicability in data-driven industries. The effectiveness of GB-BC-FS was validated through extensive experimental analysis, including two comprehensive experiments with a real case study. These experiments demonstrated that GB-BC-FS significantly outperforms existing state-of-the-art approaches, achieving an average accuracy improvement of 10.4% and saving an average of 85.17% in run time compared with finding the optimal set of features, all while adhering to budget limits. Our code is fully documented and available online at https://github.com/davidlevinwork/gbfs/ . Funding: This work was supported by the Israeli Ministry of Innovation, Science and Technology [Grant 0004323]. Data Ethics & Reproducibility Note: The code capsule is available at https://github.com/davidlevinwork/gbfs/ and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2024.0050 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2024.0050},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {265-282},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Graph-based feature selection method under budget constraint for multiclass classification problems},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic neighbourhood components analysis. <em>IJDS</em>, <em>4</em>(3), 248-264. (<a href='https://doi.org/10.1287/ijds.2023.0018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance metric learning is a fundamental task in data mining and is known to enhance the performance of various distance-based algorithms. In this paper, we consider stochastic training data in which repeated feature vectors can belong to different classes, a scenario in which existing methods of metric learning are known to struggle. This type of data is common in stochastic simulations, where multidimensional, recurrent system states are subject to inherent randomness. Classification models on such high-resolution simulation-generated data play a critical role in real-time decision making across diverse applications. This paper presents and implements a stochastic version of the popular neighbourhood components analysis. We demonstrate its behaviour on stochastic data using simulation models and reveal its advantages when used for nearest neighbour classification. Meanwhile, the assumptions of stochastic labelling and repeated feature vectors extend to data from various domains, suggesting that the method can attain broad impact. For example, beyond its applications to system control and decision making with digital twin simulation, it may enhance the analysis of data from sensor networks, recommender systems, and crowdsourced platforms, where stochasticity and recurring feature patterns are typical. Funding: This work was supported by the Engineering and Physical Sciences Research Council–funded STOR-i Centre for Doctoral Training at Lancaster University [Grant EP/L015692/1]. In addition, Barry L. Nelson’s work was supported by the U.S. National Science Foundation [Grant DMS-1854562]. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.0189724.v5 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0018 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0018},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {248-264},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Stochastic neighbourhood components analysis},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating hidden epidemic: A bayesian spatiotemporal compartmental modeling approach. <em>IJDS</em>, <em>4</em>(3), 230-247. (<a href='https://doi.org/10.1287/ijds.2023.0020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efforts to mitigate public health crises have been complicated by unreported cases and the ever-changing trends of those monitored health events across geographic regions and socioeconomic cultures. To resolve both challenges, we propose a Bayesian spatiotemporal susceptible-exposed-infected-recovered-removed (BayST-SEIRD) framework that builds the hidden effects of neighboring communities, local features, and the reporting rates into its transmission mechanism. To alleviate the computational burdens embedded in a fully Bayesian algorithm, we propose an alternating approach that learns the compartmental structure and the spatial effects separately. With a simulation study, we show that this algorithm can accurately retrieve our designed system. Then, we apply BayST-SEIRD to model the coronavirus disease 2019 (COVID-19) dynamics in the metropolitan Atlanta area. We observe that most counties’ reporting rates were below 10% of the projected total infected population and that age and educational level are negatively correlated with the exposing rate, suggesting the needs for stronger incentives for COVID-19 testing and quarantine among the younger population. Importantly, BayST-SEIRD facilitates the reconstruction of actual case counts of the monitored subject among neighboring communities, which is critical to designing impactful public health policy interventions. Funding: This research was supported by the National Center for Advancing Translational Sciences of the National Institutes of Health under Award Number UL1TR002378. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The work of K. Paynabar was partially supported by the Fouts Family Chair. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6447675/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0020 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0020},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {230-247},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Estimating hidden epidemic: A bayesian spatiotemporal compartmental modeling approach},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Observational vs. experimental data when making automated decisions using machine learning. <em>IJDS</em>, <em>4</em>(3), 197-229. (<a href='https://doi.org/10.1287/ijds.2023.0012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decisions supported by machine learning often aim to improve outcomes through interventions, such as influencing purchasing behavior with ads or increasing customer retention with special offers. However, using observational data to estimate these effects can introduce confounding bias. Although experimental data can mitigate confounding, it is not always feasible to obtain and can be costly when it is. This paper presents theoretical results focusing on the impact of confounding on decision making, emphasizing that optimizing decisions often involves determining whether a causal effect exceeds a threshold rather than minimizing bias in the estimate. Consequently, models built with readily available but confounded data can sometimes yield decisions as good as or better than those based on costly, unconfounded data. This can occur when larger effects are more likely to be overestimated or when the benefits of larger, cheaper data sets outweigh the drawbacks of confounding. We validate the theoretical findings using benchmark data from the 2016 Atlantic Causal Inference Conference causal modeling competition, encompassing 77 scenarios and 7,700 data sets. We then introduce theoretical conditions, weaker than ignorability, that characterize when confounding preserves effect rankings. These conditions allow for empirical heuristic tests to assess whether observational data aligns with this structure. Finally, we apply our findings in a large-scale case study using advertising data, demonstrating how these insights can guide decision making in practice. Funding: This research, including Yanfang Hou’s contributions, was supported by the Research Grants Council [Grant 26500822]. The authors thank Ira Rennert and the New York University/Stern Fubon Center for support. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.6587526.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0012 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0012},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {197-229},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Observational vs. experimental data when making automated decisions using machine learning},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
