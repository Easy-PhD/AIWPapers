<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="or">OR - 32</h2>
<ul>
<li><details>
<summary>
(2025). A robust optimization approach to network control using local information exchange. <em>OR</em>, <em>73</em>(5), 2849-2866. (<a href='https://doi.org/10.1287/opre.2020.0217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing policies for a network of agents is typically done by formulating an optimization problem where each agent has access to state measurements of all the other agents in the network. Such policy designs with centralized information exchange result in optimization problems that are typically hard to solve, require establishing substantial communication links, and do not promote privacy since all information is shared among the agents. Designing policies based on arbitrary communication structures can lead to nonconvex optimization problems that are typically NP-hard. In this work, we propose an optimization framework for decentralized policy designs. In contrast to the centralized information exchange, our approach requires only local communication exchange among the neighboring agents matching the physical coupling of the network. Thus, each agent only requires information from its direct neighbors, minimizing the need for excessive communication and promoting privacy amongst the agents. Using robust optimization techniques, we formulate a convex optimization problem with a loosely coupled structure that can be solved efficiently. We numerically demonstrate the efficacy of the proposed approach in energy management and supply chain applications. We show that the proposed approach leads to solutions that closely approximate those obtained by the centralized formulation only at a fraction of the computational effort. Funding: This research was supported by the Swiss National Science Foundation [Grant 51NF40_180545 under the National Centres of Competence in Research (NCCR) Automation and Grant P2ELP2_195149, Early Postdoc Mobility Fellowship]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2020.0217 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0217},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2849-2866},
  shortjournal = {Oper. Res.},
  title        = {A robust optimization approach to network control using local information exchange},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regular variable returns to scale production frontier and efficiency measurement. <em>OR</em>, <em>73</em>(5), 2830-2848. (<a href='https://doi.org/10.1287/opre.2021.0470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most frequently used empirical production frontier in data envelopment analysis, the variable returns to scale frontier, has a convex technology set and displays a special structure in economics, called the regular variable returns to scale in this paper; the production technology exhibits increasing returns to scale at the beginning of the production process followed by constant returns to scale and decreasing returns to scale. When the assumption of convexity is relaxed, modeling regular variable returns to scale becomes difficult, and currently, no satisfactory solution is available in multioutput production. Overcoming these difficulties, this paper adopts a suggestion in literature to incorporate regular variable returns to scale into the free disposal hull frontier under multiple outputs. We establish a framework for analyzing regular variable returns to scale and recommend an empirical production frontier for measuring technical efficiency with such pattern and multiple outputs. In the presence of regular variable returns to scale without convexity, the value of the technical efficiency measure computed from this new frontier is closer to the “true” value than that from the free disposal hull frontier, and the conventional variable returns to scale frontier may cause misleading implications. Funding: This research was partially supported by the National Natural Science Foundation of China [Grants 72001061 and 72243002], Hong Kong Shue Yan University [University Research Grant URG/20/01], and the Research Grants Council of the Hong Kong Special Administrative Region [Faculty Development Scheme/UGC/FDS15/E02/21]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0470 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0470},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2830-2848},
  shortjournal = {Oper. Res.},
  title        = {Regular variable returns to scale production frontier and efficiency measurement},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustifying conditional portfolio decisions via optimal transport. <em>OR</em>, <em>73</em>(5), 2801-2829. (<a href='https://doi.org/10.1287/opre.2021.0243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a data-driven portfolio selection model that integrates side information, conditional estimation, and robustness using the framework of distributionally robust optimization. Conditioning on the observed side information, the portfolio manager solves an allocation problem that minimizes the worst-case conditional risk-return tradeoff, subject to all possible perturbations of the covariate-return probability distribution in an optimal transport ambiguity set. Despite the nonlinearity of the objective function in the probability measure, we show that the distributionally robust portfolio allocation with a side information problem can be reformulated as a finite-dimensional optimization problem. If portfolio decisions are made based on either the mean-variance or the mean-conditional value-at-risk criterion, the reformulation can be further simplified to second-order or semidefinite cone programs. Empirical studies in the U.S. equity market demonstrate the advantage of our integrative framework against other benchmarks. Funding: The material in this paper is based on work supported by the Air Force Office of Scientific Research [Award FA9550-20-1-0397]. Additional support is gratefully acknowledged from the National Science Foundation [Grants 1915967, 1820942, and 1838676], the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2016-05208], and the China Merchant Bank. V. A. Nguyen gratefully acknowledges the generous support from the Chinese University of Hong Kong [Improvement on Competitiveness in Hiring New Faculties Funding Scheme] and the Chinese University of Hong Kong [Direct Grant 4055191]. S. Wang is partially supported by the National Natural Science Foundation of China [Grant 72371022]. Finally, this research was enabled in part by support provided by Compute Canada. Supplemental Material: The computer code and data that support the findings of this study and the online appendix are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0243 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0243},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2801-2829},
  shortjournal = {Oper. Res.},
  title        = {Robustifying conditional portfolio decisions via optimal transport},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical approach to robust stability of multiclass queueing networks. <em>OR</em>, <em>73</em>(5), 2782-2800. (<a href='https://doi.org/10.1287/opre.2023.0147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the global—relative to control policies—stability of multiclass queueing networks. In these, as is known, it is generally insufficient that the nominal utilization at each server is below 100%. Certain policies, although work conserving, may destabilize a network that satisfies the nominal-load conditions; additional conditions on the primitives are needed for global stability (stability under any work-conserving policy). The global-stability region was fully characterized for two-station networks in Dai and Vande Vate [Dai JG, Vande Vate JH (1996) Global stability of two-station queueing networks. Stochastic Networks (Springer, New York), 1–26.] but a general framework for networks with more than two stations remains elusive. In this paper, we offer progress on this front by considering a subset of nonidling control policies, namely, queue-ratio (QR) policies. These include as special cases all static-priority policies. With this restriction, we are able to introduce a complete framework that applies to networks of any size. Our framework breaks the analysis of robust QR stability (stability under any QR policy) into (i) robust state-space collapse, and (ii) robust stability of the Skorohod problem (SP) representing the fluid workload. Sufficient conditions for both are specified in terms of simple optimization problems. We use these optimization problems to prove that the family of QR policies satisfies a weak form of convexity relative to policies. A direct implication of this convexity is that if the SP is stable for all static-priority policies (the “extreme” QR policies), then it is also stable under any QR policy. Whereas robust QR stability is weaker than global stability, our framework recovers necessary and sufficient conditions for global stability in specific networks. Funding: This work was supported by the National Science Foundation [Grant CMMI-1856511]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0147 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0147},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2782-2800},
  shortjournal = {Oper. Res.},
  title        = {A hierarchical approach to robust stability of multiclass queueing networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolution bounds on quantile aggregation. <em>OR</em>, <em>73</em>(5), 2761-2781. (<a href='https://doi.org/10.1287/opre.2021.0765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile aggregation with dependence uncertainty has a long history in probability theory, with wide applications in finance, risk management, statistics, and operations research. Using a recent result on inf-convolution of quantile-based risk measures, we establish new analytical bounds for quantile aggregation, which we call convolution bounds. Convolution bounds both unify every analytical result available in quantile aggregation and enlighten our understanding of these methods. These bounds are the best available in general. Moreover, convolution bounds are easy to compute, and we show that they are sharp in many relevant cases. They also allow for interpretability on the extremal dependence structure. The results directly lead to bounds on the distribution of the sum of random variables with arbitrary dependence. We discuss relevant applications in risk management and economics. Funding: This work was supported by the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [Grant 2023B1212010001]; The Chinese University of Hong Kong (Shenzhen) research startup fund [Grant UDF01003336]; Natural Sciences and Engineering Research Council of Canada [Grants CRC-2022-00141 and RGPIN-2024-03728]; Shenzhen Science and Technology Program [Grant RCBS20231211090814028]; National Science Foundation [Grants 1915967, 2118199, 2229011, CAREER CMMI-1834710, and IIS-1849280]; Air Force Office of Scientific Research [Grant FA9550-20-1-0397]; and National Natural Science Foundation of China [Grant 12401624]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2021.0765 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0765},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2761-2781},
  shortjournal = {Oper. Res.},
  title        = {Convolution bounds on quantile aggregation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Near-optimal adaptive policies for serving stochastically departing customers. <em>OR</em>, <em>73</em>(5), 2744-2760. (<a href='https://doi.org/10.1287/opre.2022.0548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a multistage stochastic optimization problem, studying how a single server should prioritize stochastically departing customers. In this setting, our objective is to determine an adaptive service policy that maximizes the expected total reward collected along a discrete planning horizon, in the presence of customers who are independently departing between one stage and the next with known stationary probabilities. Despite its deceiving structural simplicity, we are unaware of nontrivial results regarding the rigorous design of optimal or truly near-optimal policies at present time. Our main contribution resides in proposing a quasi-polynomial-time approximation scheme for serving impatient customers. Specifically, letting n be the number of underlying customers, our algorithm identifies in O ( n O ϵ ( log 2 n ) ) time a service policy whose expected reward is within factor 1 − ϵ of the optimal adaptive reward. Our method for deriving this approximation scheme synthesizes various stochastic analyses in order to investigate how the adaptive optimum is affected by alterations to several instance parameters, including the reward values, the departure probabilities, and the collection of customers itself. Funding: This work was supported by the Israel Science Foundation [1407/20]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0548 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0548},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2744-2760},
  shortjournal = {Oper. Res.},
  title        = {Near-optimal adaptive policies for serving stochastically departing customers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximating the set of nash equilibria for convex games. <em>OR</em>, <em>73</em>(5), 2729-2743. (<a href='https://doi.org/10.1287/opre.2023.0541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Feinstein and Rudloff (2024) , it was shown that the set of Nash equilibria for any noncooperative N player game coincides with the set of Pareto optimal points of a certain vector optimization problem with nonconvex ordering cone. To avoid dealing with a nonconvex ordering cone, an equivalent characterization of the set of Nash equilibria as the intersection of the Pareto optimal points of N multi-objective problems (i.e., with the natural ordering cone) is proven. So far, algorithms to compute the exact set of Pareto optimal points of a multi-objective problem exist only for the class of linear problems, which reduces the possibility of finding the true set of Nash equilibria by those algorithms to linear games only. In this paper, we will consider the larger class of convex games. Because typically only approximate solutions can be computed for convex vector optimization problems, we first show, in total analogy to the result above, that the set of ϵ -approximate Nash equilibria can be characterized by the intersection of ϵ -approximate Pareto optimal points for N convex multi-objective problems. Then, we propose an algorithm based on results from vector optimization and convex projections that allows for the computation of a set that, on one hand, contains the set of all true Nash equilibria and is, on the other hand, contained in the set of ϵ -approximate Nash equilibria. In addition to the joint convexity of the cost function for each player, this algorithm works provided the players are restricted by either shared polyhedral constraints or independent convex constraints. Funding: This work was supported by Austrian Science Funds (FWF) [W1260-N35]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0541 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0541},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2729-2743},
  shortjournal = {Oper. Res.},
  title        = {Approximating the set of nash equilibria for convex games},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The analytics of robust satisficing: Predict, optimize, satisfice, then fortify. <em>OR</em>, <em>73</em>(5), 2708-2728. (<a href='https://doi.org/10.1287/opre.2023.0199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel approach to prescriptive analytics that leverages robust satisficing techniques to determine optimal decisions in situations of distribution ambiguity and parameter estimation uncertainty. Our decision model relies on a reward function that incorporates uncertain parameters, which can be predicted using available side information. However, the accuracy of the linear prediction model depends on the quality of regression coefficient estimates derived from the available data. To achieve a desired level of fragility under distribution ambiguity, we begin by solving a residual-based robust satisficing model in which the residuals from the regression are used to construct an estimated empirical distribution and a target is established relative to the predict-then-optimize objective value. In the face of estimation uncertainty, we then solve an estimation-fortified robust satisficing model that minimizes the influence of estimation uncertainty while ensuring that the solution would maintain at most the same level of fragility in achieving a less ambitious guarding target. Our approach is supported by statistical justifications, and we propose tractable models for various scenarios, such as saddle functions, two-stage linear optimization problems, and decision-dependent predictions. We demonstrate the effectiveness of our approach through case studies involving a wine portfolio investment problem and a multiproduct pricing problem using real-world data. Our numerical studies show that our approach outperforms the predict-then-optimize approach in achieving higher expected rewards and at lower risks when evaluated on the actual distribution. Notably, we observe significant improvements over the benchmarks, particularly in cases of limited data availability. Funding: The research of M. Sim was supported by the Ministry of Education, Singapore under its 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010]. The research of Q. Tang was supported by Nanyang Technological University [Start-Up Grant 020022-00001] and the Ministry of Education, Singapore [Tier 1 Grant 25010057]. The research of M. Zhou was supported by the National Natural Science Foundation of China [Grants 72301075 and 72293564/72293560]. The research of T. Zhu was supported by the National Natural Science Foundation of China [Grant 72401058]. Any opinions, findings, conclusions, or recommendations expressed in the material are those of the authors and do not necessarily reflect the views of the Ministry of Education, Singapore. Supplementary Material: The online appendix and computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0199 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0199},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2708-2728},
  shortjournal = {Oper. Res.},
  title        = {The analytics of robust satisficing: Predict, optimize, satisfice, then fortify},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ranking and contextual selection. <em>OR</em>, <em>73</em>(5), 2695-2707. (<a href='https://doi.org/10.1287/opre.2023.0378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new ranking-and-selection procedure, called ranking and contextual selection, in which covariates provide context for data-driven decisions. Our procedure optimizes over a set of covariate design points off-line and then, given an actual observation of the covariate, makes an online decision based on classification—a distinctly new approach. We prove the existence of an experimental design that yields a pointwise probability of good selection guarantee and derive a postexperiment assessment of our procedure that provides an optimality gap upper bound with guaranteed coverage for decisions with respect to future covariates. We illustrate ranking and contextual selection with an application to assortment optimization using data available from Yahoo!. Funding: This work was supported by the National Science Foundation [Grant CMMI-2206973]. Supplemental Material: This article includes an online appendix and computer code and data supporting the study’s findings at https://doi.org/10.1287/opre.2023.0378 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0378},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2695-2707},
  shortjournal = {Oper. Res.},
  title        = {Ranking and contextual selection},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally constrained black-box stochastic gradient estimation and optimization. <em>OR</em>, <em>73</em>(5), 2680-2694. (<a href='https://doi.org/10.1287/opre.2021.0307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider stochastic gradient estimation using only black-box function evaluations, where the function argument lies within a probability simplex. This problem is motivated from gradient-descent optimization procedures in multiple applications in distributionally robust analysis and inverse model calibration involving decision variables that are probability distributions. We are especially interested in obtaining gradient estimators where one or few sample observations or simulation runs apply simultaneously to all directions. Conventional zeroth-order gradient schemes such as simultaneous perturbation face challenges as the required moment conditions that allow the “canceling” of higher-order biases cannot be satisfied without violating the simplex constraints. We investigate a new set of required conditions on the random perturbation generator, which leads us to a class of implementable gradient estimators using Dirichlet mixtures. We study the statistical properties of these estimators and their utility in constrained stochastic approximation. We demonstrate the effectiveness of our procedures and compare with benchmarks via several numerical examples. Funding: The authors gratefully acknowledge support from the National Science Foundation [Grants CAREER CMMI-1834710 and IIS-1849280]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0307 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0307},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2680-2694},
  shortjournal = {Oper. Res.},
  title        = {Distributionally constrained black-box stochastic gradient estimation and optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in inverse optimization: Incenter cost, augmented suboptimality loss, and algorithms. <em>OR</em>, <em>73</em>(5), 2661-2679. (<a href='https://doi.org/10.1287/opre.2023.0254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In inverse optimization (IO), an expert agent solves an optimization problem parametric in an exogenous signal. From a learning perspective, the goal is to learn the expert’s cost function given a data set of signals and corresponding optimal actions. Motivated by the geometry of the IO set of consistent cost vectors, we introduce the “incenter” concept, a new notion akin to the recently proposed circumcenter concept. Discussing the geometric and robustness interpretation of the incenter cost vector, we develop corresponding tractable convex reformulations that are in contrast with the circumcenter, which we show is equivalent to an intractable optimization program. We further propose a novel loss function called augmented suboptimality loss (ASL), a relaxation of the incenter concept for problems with inconsistent data. Exploiting the structure of the ASL, we propose a novel first-order algorithm, which we name stochastic approximate mirror descent . This algorithm combines stochastic and approximate subgradient evaluations, together with mirror descent update steps, which are provably efficient for the IO problems with discrete feasible sets with high cardinality. We implement the IO approaches developed in this paper as a Python package called InvOpt. Our numerical experiments are reproducible, and the underlying source code is available as examples in the InvOpt package. Funding: This work was partially supported by the European Research Council [Grant TRUST-949796]. Supplemental Review: The empirical results in this paper were replicated. The code, data, and files required to reproduce the results were reviewed and are available at https://doi.org/10.1287/opre.2023.0254.cd . Supplemental Material: This article includes an online appendix, computer code and data supporting the study’s findings, and replication files. All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2023.0254.cd .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0254},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2661-2679},
  shortjournal = {Oper. Res.},
  title        = {Learning in inverse optimization: Incenter cost, augmented suboptimality loss, and algorithms},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven clustering and feature-based retail electricity pricing with smart meters. <em>OR</em>, <em>73</em>(5), 2636-2660. (<a href='https://doi.org/10.1287/opre.2022.0112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an electric utility company that serves retail electricity customers over a discrete-time horizon. In each period, the company observes the customers’ consumption and high-dimensional features on customer characteristics and exogenous factors. A distinctive element of our work is that these features exhibit three types of heterogeneity—over time, customers, or both. Based on the consumption and feature observations, the company can dynamically adjust the retail electricity price at the customer level. The consumption depends on the features: there is an underlying structure of clusters in the feature space, and the relationship between consumption and features is different in each cluster. Initially, the company knows neither the underlying cluster structure nor the corresponding consumption models. We design a data-driven policy of joint spectral clustering and feature-based pricing and show that our policy achieves near-optimal performance; that is, its average regret converges to zero at the fastest achievable rate. This work is the first to theoretically analyze joint clustering and feature-based pricing with different types of feature heterogeneity. Our case study based on real-life smart meter data from Texas illustrates that our policy increases company profits by more than 100% over a three-month period relative to the company policy and is robust to various forms of model misspecification. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0112 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0112},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2636-2660},
  shortjournal = {Oper. Res.},
  title        = {Data-driven clustering and feature-based retail electricity pricing with smart meters},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note–Dynamic duopolistic competition with sticky prices. <em>OR</em>, <em>73</em>(5), 2627-2635. (<a href='https://doi.org/10.1287/opre.2023.0473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A paradoxical conclusion arises in a series of game-theoretic models: the limit equilibria retain frictional qualities even as frictions seemingly vanish. This originates in textbook models, such as the differential game by Fershtman and Kamien [Fershtman C, Kamien MI (1987) Dynamic duopolistic competition with sticky prices. Econometrica 55(5):1151–1164] on duopolistic competition with sticky prices. We show that this paradox is an artifact of the type of limit restricted by continuous-time modeling. Fershtman and Kamien find that the closed-loop equilibrium remains surprisingly distinct from the static Cournot equilibrium as price adjustment becomes infinitely fast. We formulate and solve a discrete-time analog that nests their continuous-time model. Contrary to their conclusion, we show that the frictionless closed-loop equilibrium converges to the static Cournot equilibrium. Price stickiness persists instantaneously in the continuous-time setting because this approach cannot control the extent of price adjustment per period. Because of this subtle limitation, limit results differ between continuous- and discrete-time formulations of the model.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0473},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2627-2635},
  shortjournal = {Oper. Res.},
  title        = {Technical Note–Dynamic duopolistic competition with sticky prices},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reciprocity between tree ensemble optimization and multilinear optimization. <em>OR</em>, <em>73</em>(5), 2610-2626. (<a href='https://doi.org/10.1287/opre.2022.0150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we establish a low-degree polynomially-sized reduction between tree ensemble optimization and optimization of multilinear functions over a Cartesian product of simplices. We use this insight to derive new formulations for tree ensemble optimization problems and to obtain new convex hull results for multilinear polytopes. A computational experiment on multicommodity transportation problems with costs modeled using tree ensembles shows the practical advantage of our formulation relative to existing formulations of tree ensembles and other piecewise-linear modeling techniques. Funding: This work was supported by Division of Civil, Mechanical and Manufacturing Innovation [Grants 1727989, 1917323]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0150 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0150},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2610-2626},
  shortjournal = {Oper. Res.},
  title        = {A reciprocity between tree ensemble optimization and multilinear optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deepest cuts for benders decomposition. <em>OR</em>, <em>73</em>(5), 2591-2609. (<a href='https://doi.org/10.1287/opre.2021.0503'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its inception, Benders decomposition (BD) has been successfully applied to a wide range of large-scale mixed-integer (linear) problems. The key element of BD is the derivation of Benders cuts, which are often not unique. In this paper, we introduce a novel unifying Benders cut selection technique based on a geometric interpretation of cut depth, produce deepest Benders cuts based on ℓ p -norms, and study their properties. Specifically, we show that deepest cuts resolve infeasibility through minimal deviation (in a distance sense) from the incumbent point, are relatively sparse, and may produce optimality cuts even when classic Benders would require a feasibility cut. Leveraging the duality between separation and projection, we develop a guided projections algorithm for producing deepest cuts, exploiting the combinatorial structure and decomposability of problem instances. We then propose a generalization of our Benders separation problem, which not only brings several well-known cut selection strategies under one umbrella, but also, when endowed with a homogeneous function, enjoys several properties of geometric separation problems. We show that, when the homogeneous function is linear, the separation problem takes the form of the minimal infeasible subsystems (MIS) problem. As such, we provide systematic ways of selecting the normalization coefficients of the MIS method and introduce a directed depth-maximizing algorithm for deriving these cuts. Inspired by the geometric interpretation of distance-based cuts and the repetitive nature of two-stage stochastic programs, we introduce a tailored algorithm to further facilitate deriving these cuts. Our computational experiments on various benchmark problems illustrate effectiveness of deepest cuts in reducing both computation time and number of Benders iterations and producing high-quality bounds at early iterations. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0503 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0503},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2591-2609},
  shortjournal = {Oper. Res.},
  title        = {Deepest cuts for benders decomposition},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Search in the dark: The case with recall and gaussian learning. <em>OR</em>, <em>73</em>(5), 2572-2590. (<a href='https://doi.org/10.1287/opre.2023.0150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classic sequential search problem rewards the decision maker with the highest sampled value minus a cost per sample. If the sampling distribution is unknown, then a Bayesian decision maker faces a complex balance between exploration and exploitation. We solve the stopping problem of sampling from a normal distribution with unknown mean and variance and a conjugate prior, a longstanding open problem. The optimal stopping region may be empty (it may be optimal to continue the search regardless of the offer one receives, especially at the early stages), or it may consist of one or two bounded intervals. Whereas a single reservation price cannot describe the optimal rule, we do find an optimal index policy taking the form of a standardized reservation rule: stop if and only if the standardized value of the current best exceeds a threshold that depends on the standardized search cost. We also provide an algorithm to compute the index function, producing a practical way to implement the optimal stopping rule for any given prior, sampling history, and sampling horizon. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0150 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0150},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2572-2590},
  shortjournal = {Oper. Res.},
  title        = {Search in the dark: The case with recall and gaussian learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note–Stability of a queue fed by scheduled traffic at critical loading. <em>OR</em>, <em>73</em>(5), 2567-2571. (<a href='https://doi.org/10.1287/opre.2023.0039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the workload process for a single server queue with deterministic service times in which customers arrive according to a scheduled traffic process. A scheduled arrival sequence is one in which customers are scheduled to arrive at constant interarrival times, but each customer’s actual arrival time is perturbed from her scheduled arrival time by a random perturbation. In this paper, we consider a critically loaded queue in which the service rate equals the arrival rate. Unlike a queue fed by renewal traffic, this queue can be stable even in the presence of critical loading. We show that for finite mean perturbations, a necessary and sufficient condition for stability is when the positive part of the perturbation has bounded support, with no requirement on the negative part of the perturbation. Perhaps surprisingly, this criterion is not reversible, in the sense that such a queue can be stable for a scheduled traffic process in forward time, but unstable for the time-reversal of the same traffic process.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0039},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2567-2571},
  shortjournal = {Oper. Res.},
  title        = {Technical Note–Stability of a queue fed by scheduled traffic at critical loading},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Markdown policies for demand learning with forward-looking customers. <em>OR</em>, <em>73</em>(5), 2550-2566. (<a href='https://doi.org/10.1287/opre.2019.0402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the markdown pricing problem of a firm that sells a product to a mixture of myopic and forward-looking customers. The firm faces uncertainty about the customers’ forward-looking behavior, arrival pattern, and valuations for the product, which we collectively refer to as the demand model. Over a multiperiod selling season, the firm sequentially marks down the product’s price and makes demand observations to learn about the underlying demand model. Because forward-looking customers create an intertemporal dependency, we identify that the keys to achieving good profit performance are (i) judiciously accumulating information on the demand model and (ii) preserving the market size in early sales periods. Based on these, we construct and analyze markdown policies that exhibit near-optimal performance under a wide variety of forward-looking customer behaviors. Funding: Financial support from Duke University Fuqua School of Business; the University of Chicago Booth School of Business; and CUHK Business School, the Chinese University of Hong Kong is gratefully acknowledged. H. (K.) Chen thanks the Hong Kong Research Grants Council [Grant GRF14506622]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2019.0402 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.0402},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2550-2566},
  shortjournal = {Oper. Res.},
  title        = {Markdown policies for demand learning with forward-looking customers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On consistency of signature using lasso. <em>OR</em>, <em>73</em>(5), 2530-2549. (<a href='https://doi.org/10.1287/opre.2024.1133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signatures are iterated path integrals of continuous and discrete-time processes, and their universal nonlinearity linearizes the problem of feature selection in time series data analysis. This paper studies the consistency of signature using Lasso regression, both theoretically and numerically. We establish conditions under which the Lasso regression is consistent both asymptotically and in finite sample. Furthermore, we show that the Lasso regression is more consistent with the Itô signature for time series and processes that are closer to the Brownian motion and with weaker interdimensional correlations, whereas it is more consistent with the Stratonovich signature for mean-reverting time series and processes. We demonstrate that signature can be applied to learn nonlinear functions and option prices with high accuracy, and the performance depends on properties of the underlying process and the choice of the signature. Funding: R. Zhang’s research was supported by the National Key Research and Development Program of China [Grant 2022YFA1007900], the National Natural Science Foundation of China [Grant 72342004 and Grant 12271013], the Fundamental Research Funds for the Central Universities (Peking University), and Yinhua Education Foundation. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2024.1133 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1133},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2530-2549},
  shortjournal = {Oper. Res.},
  title        = {On consistency of signature using lasso},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Policy learning with competing agents. <em>OR</em>, <em>73</em>(5), 2515-2529. (<a href='https://doi.org/10.1287/opre.2022.0687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision makers often aim to learn a treatment assignment policy under a capacity constraint on the number of agents that they can treat. When agents can respond strategically to such policies, competition arises, complicating estimation of the optimal policy. In this paper, we study capacity-constrained treatment assignments in the presence of such interference. We consider a dynamic model in which the decision maker allocates treatments at each time step and heterogeneous agents myopically best respond to the previous treatment assignment policy. When the number of agents is large but finite, we show that the threshold for receiving treatment under a given policy converges to the policy’s mean-field equilibrium threshold. Based on this result, we develop a consistent estimator for the policy gradient. In a semisynthetic experiment with data from the National Education Longitudinal Study of 1988, we demonstrate that this estimator can be used for learning capacity-constrained policies in the presence of strategic behavior. Funding: This work was supported by National Science Foundation (NSF) [Grant SES-2242876]. R. Sahoo is supported by NSF Graduate Research Fellowship Program [Grant DGE-1656518], a Stanford University Data Science Fellowship, and a Stanford University Ethics in Society Fellowship. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2022.0687 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0687},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2515-2529},
  shortjournal = {Oper. Res.},
  title        = {Policy learning with competing agents},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind network revenue management and bandits with knapsacks under limited switches. <em>OR</em>, <em>73</em>(5), 2496-2514. (<a href='https://doi.org/10.1287/opre.2020.0753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the impact of limited switches on resource-constrained dynamic pricing with demand learning. We focus on the classical price-based blind network revenue management problem and extend our results to the bandits with knapsacks problem. In both settings, a decision maker faces stochastic and distributionally unknown demand, and must allocate finite initial inventory across multiple resources over time. In addition to standard resource constraints, we impose a switching constraint that limits the number of allowable action changes over the time horizon. We establish matching upper and lower bounds on the optimal regret and develop computationally efficient limited-switch algorithms that achieve it. We show that the optimal regret rate is fully characterized by a piecewise-constant function of the switching budget, which further depends on the number of resource constraints. Our results highlight the fundamental role of resource constraints in shaping the statistical complexity of online learning under limited switches. Extensive simulations demonstrate that our algorithms maintain strong cumulative reward performance while significantly reducing the number of switches. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2020.0753 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0753},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2496-2514},
  shortjournal = {Oper. Res.},
  title        = {Blind network revenue management and bandits with knapsacks under limited switches},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instrumenting while experimenting: An empirical method for competitive pricing at scale. <em>OR</em>, <em>73</em>(5), 2477-2495. (<a href='https://doi.org/10.1287/opre.2022.0157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate operational decisions require precise knowledge of the causal effects of such decisions on outcomes, a task that becomes increasingly complex in dynamic business environments. We propose an idea of “instrumenting while experimenting,” whereby researchers can create their own instruments by “injecting” small, random variations directly into the decision-making process and then use such variations to obtain causal estimates of the impact of varying business decisions at scale without disrupting everyday operations. To illustrate the effectiveness of this idea, we partner with a leading U.S. e-commerce retailer and develop a competitive pricing method in the context of increasing competition in online retailing. Our method allows retailers to respond more accurately to competitors’ price changes at scale. Operationally, we first construct a parsimonious demand model to capture the key trade-offs in competitive pricing. This model accounts for potential shifts in customer behaviors based on whether the focal retailer holds a price advantage relative to its competitors. Next, we design and implement a large-scale randomized price experiment on over 10,000 products. Leveraging the experiment as well as the control function approach, we are able to obtain unbiased estimates of key pricing components in the demand model, in particular, price elasticities of customers in both price advantage and disadvantage regions as well as the sales lift when undercutting competitors in price. Lastly, we recommend price responses by solving a constrained optimization problem that uses the estimated demand model as an input. We test this pricing method through another large-scale controlled field experiment on over 10,000 products and demonstrate significant improvements—increasing revenue by over 15% and increasing profit by over 10%. Simulation analyses reveal that these improvements are attributable to the joint implementation of demand modeling (contributing 17% of the total improvement), price optimization (36%), and our proposed estimation method (48%). Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2022.0157 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0157},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2477-2495},
  shortjournal = {Oper. Res.},
  title        = {Instrumenting while experimenting: An empirical method for competitive pricing at scale},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online learning with sample selection bias. <em>OR</em>, <em>73</em>(5), 2458-2476. (<a href='https://doi.org/10.1287/opre.2023.0223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of personalized recommendations on online platforms, where user preferences are unknown, and users interact with the platform through a series of sequential decisions (such as clicking to watch on video platforms or clicking to donate on donation platforms). The platform aims to maximize the final outcome (e.g., viewing duration on video platforms or donations on donation platforms). However, the platform only observes the final outcome for users who complete the first stage (clicking on the recommendation). The final outcome for users who do not complete the first stage (not clicking on the recommendation) remains unobserved (also referred to as funneling ). This censoring of outcomes creates a selection bias issue, as the observed outcomes at different stages are often correlated. We demonstrate that failing to account for this selection bias results in biased estimates and suboptimal recommendations. In fact, well-performing personalized learning algorithms perform poorly and incur linear regret in this setting. Therefore, we propose the sample selection bandit (SSB) algorithm, which combines Heckman’s two-step estimator with the “optimism under uncertainty” principle to address the sample selection bias issue. We show that the SSB algorithm achieves a rate-optimal regret rate (up to logarithmic terms) of O ˜ ( T ) . Furthermore, we conduct extensive numerical experiments on both synthetic data and real donation data collected from GoFundMe (a crowdfunding platform), demonstrating significant improvements over benchmark state-of-the-art learning algorithms in this setting. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2023.0223 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0223},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2458-2476},
  shortjournal = {Oper. Res.},
  title        = {Online learning with sample selection bias},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Better regularization for sequential decision spaces: Fast convergence rates for nash, correlated, and team equilibria. <em>OR</em>, <em>73</em>(5), 2430-2457. (<a href='https://doi.org/10.1287/opre.2021.0633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the application of iterative first-order methods to the problem of computing equilibria of large-scale extensive-form games. First-order methods must typically be instantiated with a regularizer that serves as a distance-generating function (DGF) for the decision sets of the players. In this paper, we introduce a new weighted entropy-based distance-generating function. We show that this function is equivalent to a particular set of new weights for the dilated entropy distance–generating function on a treeplex while retaining the simpler structure of the regular entropy function for the unit cube. This function achieves significantly better strong-convexity properties than existing weight schemes for the dilated entropy while maintaining the same easily implemented closed-form proximal mapping as the prior state of the art. Extensive numerical simulations show that these superior theoretical properties translate into better numerical performance as well. We then generalize our new entropy distance function, as well as general dilated distance functions, to the scaled extension operator. The scaled extension operator is a way to recursively construct convex sets, which generalizes the decision polytope of extensive-form games as well as the convex polytopes corresponding to correlated and team equilibria. Correspondingly, we give the first efficiently computable distance-generating function for all those strategy polytopes. By instantiating first-order methods with our regularizers, we achieve several new results, such as the first method for computing ex ante correlated team equilibria with a guaranteed 1 / T rate of convergence and efficient proximal updates. Similarly, we show that our regularizers can be used to speed up the computation of correlated solution concepts. Funding: G. Farina was supported by the National Science Foundations [Grant CCF-2443068] and by T. Sandholm’s grants listed below and a Facebook fellowship. C. Kroer was supported by the Office of Naval Research [Grants N00014-22-1-2530 and N00014-23-1-2374] and the National Science Foundation [Grants IIS-2147361 and IIS-2238960]. T. Sandholm was supported by the Vannevar Bush Faculty Fellowship, Office of Naval Research [Grant ONR N00014-23-1-2876], the National Science Foundation Division of Information and Intelligent Systems [Grants RI-1718457, RI-2312342, RI-1901403, and CCF-1733556], the Army Research Office [Grants W911NF2010081 and W911NF2210266], and the National Institutes of Health [Grant A240108S001]. This work was further supported by the National Science Foundation Division of Information and Intelligent Systems [Grant 1617590] and the Army Research Office [Grant W911NF-17-1-0082]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0633 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0633},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2430-2457},
  shortjournal = {Oper. Res.},
  title        = {Better regularization for sequential decision spaces: Fast convergence rates for nash, correlated, and team equilibria},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Change-point detection in dynamic networks with missing links. <em>OR</em>, <em>73</em>(5), 2417-2429. (<a href='https://doi.org/10.1287/opre.2021.0413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural changes occur in dynamic networks quite frequently and their detection is an important question in many situations, such as fraud detection or cybersecurity. Real-life networks are often incompletely observed because of individual nonresponse or network size. In the present paper, we consider the problem of change-point detection at a temporal sequence of partially observed networks. The goal is to test whether there is a change in the network parameters. Our approach is based on the matrix cumulative sum test statistic and allows growing the size of networks. We show that the proposed test is minimax optimal and robust to missing links. We also demonstrate the good behavior of our approach in practice through simulation study and a real-data application. Funding: The work of O. Klopp was funded by the CY Initiative [Grant Investissements d’Avenir Agence Nationale de Recherche-16-Initiatives d’Excellence-0008] and Labex MME-DII [Grant ANR11-LBX-0023-01]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0413 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0413},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2417-2429},
  shortjournal = {Oper. Res.},
  title        = {Change-point detection in dynamic networks with missing links},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal abort policy for mission-critical systems under imperfect condition monitoring. <em>OR</em>, <em>73</em>(5), 2396-2416. (<a href='https://doi.org/10.1287/opre.2022.0643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although most on-demand mission-critical systems are engineered to be reliable to support critical tasks, occasional failures may still occur during missions. To increase system survivability, a common practice is to abort the mission before an imminent failure. We consider optimal mission abort for a system whose deterioration follows a general three-state (normal, defective, failed) semi-Markov chain. The failure is assumed self-revealed, whereas the healthy and defective states have to be inferred from imperfect condition-monitoring data. Because of the non-Markovian process dynamics, optimal mission abort for this partially observable system is an intractable stopping problem. For a tractable solution, we introduce a novel tool of Erlang mixtures to approximate nonexponential sojourn times in the semi-Markov chain. This allows us to approximate the original process by a surrogate continuous-time Markov chain whose optimal control policy can be solved through a partially observable Markov decision process (POMDP). We show that the POMDP optimal policies converge almost surely to the optimal abort decision rules when the Erlang rate parameter diverges. This implies that the expected cost by adopting the POMDP solution converges to the optimal expected cost. Next, we provide comprehensive structural results on the optimal policy of the surrogate POMDP. Based on the results, we develop a modified point-based value iteration algorithm to numerically solve the surrogate POMDP. We further consider mission abort in a multitask setting where a system executes several tasks consecutively before a thorough inspection. Through a case study on an unmanned aerial vehicle, we demonstrate the capability of real-time implementation of our model, even when the condition-monitoring signals are generated with high frequency. Funding: This work was supported in part by the National Science Foundation of China [Grants 72171037, 72471144, 72371161, and 72071071], Singapore MOE AcRF Tier 2 grants [Grants A-8001052-00-00 and A-8002472-00-00], and the Future Resilient Systems project supported by the National Research Foundation Singapore under its CREATE program. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2022.0643 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0643},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2396-2416},
  shortjournal = {Oper. Res.},
  title        = {Optimal abort policy for mission-critical systems under imperfect condition monitoring},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D-optimal orienteering for post-earthquake reconnaissance planning. <em>OR</em>, <em>73</em>(5), 2375-2395. (<a href='https://doi.org/10.1287/opre.2023.0470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immediately following a major earthquake, reconnaissance surveys seek to assess structural damage throughout the region with the help of a limited number of on-ground inspections. The goal is to collect informative and representative data that will guide subsequent relief efforts. We formulate a new type of vehicle routing problem, in which vehicles are tasked with data collection, and the objective function measures data quality using a nonlinear, nonseparable experimental design criterion. We create novel exact methods for this problem and demonstrate their practical potential in a realistic case study using a state-of-the-art earthquake simulator. Funding: J. Wang, I. O. Ryzhov, N. Marković, and G. Ou acknowledge the support of the National Science Foundation (NSF) Division of Civil, Mechanical and Manufacturing Innovation [Grant 2112828]. W. Xie acknowledges the support of the NSF Division of Computing and Communication Foundations [Grant 2246417]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2023.0470 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0470},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2375-2395},
  shortjournal = {Oper. Res.},
  title        = {D-optimal orienteering for post-earthquake reconnaissance planning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stationary mean-field equilibrium model of irreversible investment in a two-regime economy. <em>OR</em>, <em>73</em>(5), 2351-2374. (<a href='https://doi.org/10.1287/opre.2023.0250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a mean-field model of firms competing à la Cournot on a commodity market, where the commodity price is given in terms of a power inverse demand function of the industry-aggregate production. Investment is irreversible and production capacity depreciates at a constant rate. Production is subject to Gaussian productivity shocks, whereas large nonanticipated macroeconomic events driven by a two-state continuous-time Markov chain can change the volatility of the shocks, as well as the price function. Firms wish to maximize expected discounted revenues of production, net of investment, and operational costs. Investment decisions are based on the long-run stationary price of the commodity. We prove existence, uniqueness, and characterization of the stationary mean-field equilibrium of the model. The equilibrium investment strategy is of barrier type, and it is triggered by a couple of endogenously determined investment thresholds, one per state of the economy. We provide a quasi-closed form expression of the stationary density of the state, and we show that our model can produce Pareto distribution of firms’ size. This is a feature that is consistent both with observations at the aggregate level of industries and at the level of a particular industry. We provide evidence that persistent periods of economic downturn increase market concentration. We demonstrate that firms with slowly depreciating production capacities fare better in a stable, average economy, whereas firms with quickly depreciating assets can benefit from sequences of boom and bust. Funding: This work was supported by the Agence Nationale de la Recherche [Grants ANR-19-CE05-0042 and MIRTE ANR-23-EXMA-0011] and the Deutsche Forschungsgemeinschaft [Grant SFB 1283/22021-317210226].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0250},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2351-2374},
  shortjournal = {Oper. Res.},
  title        = {A stationary mean-field equilibrium model of irreversible investment in a two-regime economy},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Political districting to optimize the polsby-popper compactness score with application to voting rights. <em>OR</em>, <em>73</em>(5), 2330-2350. (<a href='https://doi.org/10.1287/opre.2024.1078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the academic literature and in expert testimony, the Polsby-Popper score is the most popular way to measure the compactness of a political district. Given a district with area A and perimeter P , its Polsby-Popper score is given by ( 4 π A ) / P 2 . This score takes values between zero and one, with circular districts achieving a perfect score of one. In this paper, we propose the first mathematical optimization models to draw districts (or districting plans) with optimum Polsby-Popper score. Specifically, we propose new mixed-integer second-order cone programs (MISOCPs), which can be solved with existing optimization software. Experiments show that they can identify the most compact single districts at the precinct level and the most compact plans at the county level. Then, we turn to the problem of drawing compact plans with a large number of majority-minority districts. This is the task faced by plaintiffs in Voting Rights Act cases who must show that an alternative plan exists in which the minority group could achieve better representation, a legal hurdle known as the first Gingles precondition. For this task, we propose new MISOCP-based heuristics that often outperform enacted maps on standard criteria, sometimes by substantial margins. They also perform well against state-of-the-art heuristics like short bursts and can be used to polish maps with hundreds of thousands of census blocks. Our techniques could assist plaintiffs when seeking to overturn maps that dilute the voting strength of minority groups. Our code is available on GitHub. Funding: This work was supported by the National Science Foundation [Grant 1942065].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1078},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2330-2350},
  shortjournal = {Oper. Res.},
  title        = {Political districting to optimize the polsby-popper compactness score with application to voting rights},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the almost threshold policy for multisourcing under uncertain supplies. <em>OR</em>, <em>73</em>(5), 2319-2329. (<a href='https://doi.org/10.1287/opre.2024.1193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With extended supply chains and increased global sourcing, the uncertainty in supply streams has become a major consideration in formulating procurement strategies. Many studies in the existing literature characterize the optimal procurement policies under specific assumptions of the supply and demand distributions. In several special cases, a threshold policy or an almost threshold policy is shown to be optimal. A recent study by Feng and Shathikumar [Feng Q, Shanthikumar JG (2018) Supply and demand functions in inventory models. Oper. Res. 66(1):77–91] generalizes the previous results by establishing the optimality of an almost threshold policy under any demand distribution and a set of conditions on the stochastic supply functions, including stochastic linearity in midpoint, stochastic increasing in the dispersive order, and vanishing of the no-delivery probability. In this note, we significantly enhance this result and for the first time, fully characterize the optimal multisourcing policy for general stochastic supply functions that are stochastically linear in midpoint, the weakest known condition to ensure the concavity of the associated dynamic program. Using an iterative constructive approach, we prove that an almost threshold policy is optimal. This result is extended to price-dependent demands and positively dependent supplies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1193},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2319-2329},
  shortjournal = {Oper. Res.},
  title        = {On the almost threshold policy for multisourcing under uncertain supplies},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Near-optimal mixed (s,S) policy for a multiwarehouse, multistore inventory system with lost sales and fixed cost. <em>OR</em>, <em>73</em>(5), 2306-2318. (<a href='https://doi.org/10.1287/opre.2024.0717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a firm managing a multiperiod, multiwarehouse, multistore (MWMS) inventory problem with fixed ordering cost at each store over a finite time horizon. The warehouses are endowed with initial inventories at the start of the horizon, and the stores are periodically replenished from the warehouses. The decisions are the order quantities from each store at each period. The optimal policy for this problem is complex and computationally intractable. We construct a mixed ( s , S ) policy based on the optimal solutions of a Lagrangian relaxation. Under this policy, each store makes use of at most two ( s , S ) policies; one is applied during the first phase of the selling horizon, and the second is applied in the remaining periods. We prove that this policy is near optimal as the length of the time horizon grows. In contrast to the existing works on the MWMS problem without fixed cost for which near-optimal policies can be developed using an optimal Lagrangian solution, with fixed cost, it is crucial to adopt a mixture of Lagrangian solutions, and simply applying a pure optimal Lagrangian solution can be highly suboptimal. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2024.0717 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.0717},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2306-2318},
  shortjournal = {Oper. Res.},
  title        = {Near-optimal mixed (s,S) policy for a multiwarehouse, multistore inventory system with lost sales and fixed cost},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-item online order fulfillment in a two-layer network. <em>OR</em>, <em>73</em>(5), 2297-2305. (<a href='https://doi.org/10.1287/opre.2022.0100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global e-commerce boom has driven rapid expansion of fulfillment infrastructure, with e-retailers building more warehouses to offer faster deliveries. However, fulfillment costs have surged over the past decade. This paper addresses the problem of minimizing these costs, where an e-retailer must decide in real time which warehouse(s) will fulfill each order, considering inventory constraints. Orders can be split among warehouses at an additional cost. We focus on a regional distribution center (RDC)–front distribution center (FDC) distribution network used by major e-retailers, which consists of larger RDCs and smaller FDCs. We analyze the performance of a simple myopic policy that selects the least expensive fulfillment option for each order without considering future impacts. We provide theoretical bounds on the performance ratio of the myopic policy compared with an optimal clairvoyant policy and demonstrate the strengths of the myopic policy within this two-layer network. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2022.0100 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0100},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2297-2305},
  shortjournal = {Oper. Res.},
  title        = {Multi-item online order fulfillment in a two-layer network},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
