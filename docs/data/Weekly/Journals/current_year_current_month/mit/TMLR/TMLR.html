<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMLR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmlr">TMLR - 40</h2>
<ul>
<li><details>
<summary>
(2025). Contextual combinatorial bandits with changing action sets via gaussian processes. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=2RgfAY3jnI'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a contextual bandit problem with a combinatorial action set and time-varying base arm availability. At the beginning of each round, the agent observes the set of available base arms and their contexts and then selects an action that is a feasible subset of the set of available base arms to maximize its cumulative reward in the long run. We assume that the mean outcomes of base arms are samples from a Gaussian Process \rev{(GP)} indexed by the context set ${\cal X}$, and the expected reward is Lipschitz continuous in expected base arm outcomes. For this setup, we propose an algorithm called Optimistic Combinatorial Learning and Optimization with Kernel Upper Confidence Bounds (O'CLOK-UCB) and prove that it incurs $\tilde{O}(\sqrt{\lambda^*(K)KT\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)} )$ regret with high probability, where $\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)$ is the maximum information gain associated with the sets of base arm contexts $\mathcal{X}_t$ that appeared in the first $T$ rounds, $K$ is the maximum cardinality of any feasible action over all rounds, and $\lambda^*(K)$ is the maximum eigenvalue of all covariance matrices of selected actions up to time $T$, which is a function of $K$. To dramatically speed up the algorithm, we also propose a variant of O'CLOK-UCB that uses sparse GPs. Finally, we experimentally show that both algorithms exploit inter-base arm outcome correlation and vastly outperform the previous state-of-the-art UCB-based algorithms in realistic setups.},
  archive      = {J_TMLR},
  author       = {Andi Nika and Sepehr Elahi and Cem Tekin},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Contextual combinatorial bandits with changing action sets via gaussian processes},
  url          = {https://openreview.net/forum?id=2RgfAY3jnI},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial robustness of graph transformers. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=4xK0vjxTWL'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies have shown that Message-Passing Graph Neural Networks (MPNNs) are highly susceptible to adversarial attacks. In contrast, despite the increasing importance of Graph Transformers (GTs), their robustness properties are unexplored. We close this gap and design the first adaptive attacks for GTs. In particular, we provide general design principles for strong gradient-based attacks on GTs w.r.t. structure perturbations and instantiate our attack framework for five representative and popular GT architectures. Specifically, we study GTs with specialized attention mechanisms and Positional Encodings (PEs) based on pairwise shortest paths, random walks, and the Laplacian spectrum. We evaluate our attacks on multiple tasks and perturbation models, including structure perturbations for node and graph classification, and node injection for graph classification. Our results reveal that GTs can be catastrophically fragile in many cases. Addressing this vulnerability, we show how our adaptive attacks can be effectively used for adversarial training, substantially improving robustness.},
  archive      = {J_TMLR},
  author       = {Philipp Foth and Lukas Gosch and Simon Geisler and Leo Schwinn and Stephan Günnemann},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Adversarial robustness of graph transformers},
  url          = {https://openreview.net/forum?id=4xK0vjxTWL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting B2T: Discovering and mitigating visual biases through keyword explanations. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=5GS1q65pv6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to reproduce and extend the findings of "Discovering and Mitigating Visual Biases through Keyword Explanation" by Kim et al.(2024). The paper proposes the B2T framework, which detects and mitigates visual biases by extracting keywords from generated captions. By identifying biases in datasets, B2T contributes to the prevention of discriminatory behavior in vision-language models. We aim to investigate the five key claims from the original paper, namely that B2T (i) is able to identify whether a word represents a bias, (ii) can extract these keywords from captions of mispredicted images, (iii) outperforms other bias discovery models, (iv) can improve CLIP zero-shot prompting with the discovered keywords, and (v) identifies labeling errors in a dataset. To reproduce their results, we use the publicly available codebase and our re-implementations. Our findings confirm the first three claims and partially validate the fourth. We reject the fifth claim, due to the failure to identify pertinent labeling errors. Finally, we enhance the original work by optimizing the efficiency of the implementation, and assessing the generalizability of B2T on a new dataset.},
  archive      = {J_TMLR},
  author       = {Faissal El Kayouhi and Aïda Asma and Joey Laarhoven and Fiona Nagelhout},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Revisiting B2T: Discovering and mitigating visual biases through keyword explanations},
  url          = {https://openreview.net/forum?id=5GS1q65pv6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Capsule network projectors are equivariant and invariant learners. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=7owCO3qskH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning invariant representations has been the longstanding approach to self-supervised learning. However, recently progress has been made in preserving equivariant properties in representations, yet do so with highly prescribed architectures. In this work, we propose an invariant-equivariant self-supervised architecture that employs Capsule Networks (CapsNets), which have been shown to capture equivariance with respect to novel viewpoints. We demonstrate that the use of CapsNets in equivariant self-supervised architectures achieves improved downstream performance on equivariant tasks with higher efficiency and fewer network parameters. To accommodate the architectural changes of CapsNets, we introduce a new objective function based on entropy minimisation. This approach, which we name CapsIE (Capsule Invariant Equivariant Network), achieves state-of-the-art performance on the equivariant rotation tasks on the 3DIEBench dataset compared to prior equivariant SSL methods, while performing competitively against supervised counterparts. Our results demonstrate the ability of CapsNets to learn complex and generalised representations for large-scale, multi-task datasets compared to previous CapsNet benchmarks.},
  archive      = {J_TMLR},
  author       = {Miles Everett and Aiden Durrant and Mingjun Zhong and Georgios Leontidis},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Capsule network projectors are equivariant and invariant learners},
  url          = {https://openreview.net/forum?id=7owCO3qskH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent trajectory: A new framework for deep actor-critic reinforcement learning with uncertainty quantification. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=8B74xdaRHa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification in deep learning is challenging due to the complexity of deep neural networks. This challenge is particularly pronounced in deep reinforcement learning (RL), where agents interact with stochastic environments. In deep actor-critic RL, this challenge is further exacerbated due to the interdependence between the actor and critic updates. Existing uncertainty quantification methods for RL are predominantly developed within the Bayesian framework. While these methods estimate the uncertainty of the value function, their confidence intervals are often misleading, with the coverage rate frequently falling well below the nominal level. To address this issue, we introduce a novel deep RL framework that treats transition trajectories as latent variables. Leveraging this framework, we propose an adaptive Stochastic Gradient Markov Chain Monte Carlo algorithm to train deep actor-critic models, which naturally accounts for the interdependence between the actor and critic updates. We provide theoretical guarantees for the convergence of the proposed method and offer empirical evidence for its effectiveness in uncertainty quantification of the value function. The proposed latent trajectory framework is highly flexible, allowing for the integration of advanced RL strategies to further enhance deep actor-critic learning.},
  archive      = {J_TMLR},
  author       = {Frank Shih and Faming Liang},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Latent trajectory: A new framework for deep actor-critic reinforcement learning with uncertainty quantification},
  url          = {https://openreview.net/forum?id=8B74xdaRHa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D2 actor critic: Diffusion actor meets distributional critic. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=8KbstCUXhH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce D2AC, a new model-free reinforcement learning (RL) algorithm designed to train expressive diffusion policies online effectively. At its core is a policy improvement objective that avoids the high variance of typical policy gradients and the complexity of backpropagation through time. This stable learning process is critically enabled by our second contribution: a robust distributional critic, which we design through a fusion of distributional RL and clipped double Q-learning. The resulting algorithm is highly effective, achieving state-of-the-art performance on a benchmark of eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains, spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard benchmarks, we also evaluate a biologically motivated predator-prey task to examine the behavioral robustness and generalization capacity of our approach.},
  archive      = {J_TMLR},
  author       = {Lunjun Zhang and Shuo Han and Hanrui Lyu and Bradly C. Stadie},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {D2 actor critic: Diffusion actor meets distributional critic},
  url          = {https://openreview.net/forum?id=8KbstCUXhH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Illusion or algorithm? investigating memorization, emergence, and symbolic processing in in-context learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=10QqO1tM1H'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere "memorization" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.},
  archive      = {J_TMLR},
  author       = {Jingcheng Niu and Subhabrata Dutta and Ahmed Elshabrawy and Harish Tayyar Madabushi and Iryna Gurevych},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Illusion or algorithm? investigating memorization, emergence, and symbolic processing in in-context learning},
  url          = {https://openreview.net/forum?id=10QqO1tM1H},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blending adversarial training and representation-conditional purification via aggregation improves adversarial robustness. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=40BXthYscW'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a novel adversarial defence mechanism for image classification - CARSO - blending the paradigms of adversarial training and adversarial purification in a synergistic robustness-enhancing way. The method builds upon an adversarially-trained classifier, and learns to map its internal representation associated with a potentially perturbed input onto a distribution of tentative clean reconstructions. Multiple samples from such distribution are classified by the same adversarially-trained model, and a carefully chosen aggregation of its outputs finally constitutes the robust prediction of interest. Experimental evaluation by a well-established benchmark of strong adaptive attacks, across different image datasets, shows that CARSO is able to defend itself against adaptive end-to-end white-box attacks devised for stochastic defences. With a modest clean accuracy penalty, our method improves by a significant margin the state-of-the-art for Cifar-10, Cifar-100, and TinyImageNet-200 $\ell_\infty$ robust classification accuracy against AutoAttack.},
  archive      = {J_TMLR},
  author       = {Emanuele Ballarin and Alessio ansuini and Luca Bortolussi},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Blending adversarial training and representation-conditional purification via aggregation improves adversarial robustness},
  url          = {https://openreview.net/forum?id=40BXthYscW},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven discovery of PDEs via the adjoint method. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Az3mJ4d1eT'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form and formulate a PDE-constrained optimization problem aimed at minimizing the error of the PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations), allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, we consider a family of temporal parameterized PDEs that encompass linear, nonlinear, and spatial derivative candidate terms, and elegantly derive the corresponding adjoint equations. We show the efficacy of the proposed approach in identifying the form of the PDE up to machine accuracy, enabling the accurate discovery of PDEs from data. We also compare its performance with the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND \cite{rudy2017data} among others, on both smooth and noisy data sets. Even though the proposed adjoint method relies on forward/backward solvers, it outperforms PDE-FIND in the limit of large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter.},
  archive      = {J_TMLR},
  author       = {Mohsen Sadr and Tony Tohme and KAMAL YOUCEF-TOUMI},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Data-driven discovery of PDEs via the adjoint method},
  url          = {https://openreview.net/forum?id=Az3mJ4d1eT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal horizons in forecasting: A performance-learnability trade-off. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=BeudQIxT1R'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training autoregressive models to forecast dynamical systems, a critical question arises: how far into the future should the model be trained to predict for optimal performance? In this work, we address this question by analyzing the relationship between the geometry of the loss landscape and the training time horizon. Using dynamical systems theory, we prove that loss minima for long horizons generalize well to short-term forecasts, whereas minima found on short horizons result in worse long-term predictions. However, we also prove that the loss landscape becomes rougher as the training horizon grows, making long-horizon training inherently challenging. We validate our theory through numerical experiments and discuss practical implications for selecting training horizons. Our results provide a principled foundation for hyperparameter optimization in autoregressive forecasting models.},
  archive      = {J_TMLR},
  author       = {Pau Vilimelis Aceituno and Jack William Miller and Noah Marti and Youssef Farag and Victor Boussange},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Temporal horizons in forecasting: A performance-learnability trade-off},
  url          = {https://openreview.net/forum?id=BeudQIxT1R},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VSCoDe: Visual-augmentation selection for contrastive decoding. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=CqSyPc9W7Y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive performance of recent Large Vision-Language Models (LVLMs), these models often produce inaccurate responses. To address this issue, previous studies have aimed to reduce hallucinations by using contrastive decoding (CD) with modified images, such as cropping objects related to query or adding noise, thereby contrasting with the original image. However, these methods have several limitations. First, employing fixed visual augmentation, such as adding noise, is a simple approach but too rigid to contrast on various queries. Conversely, using semantics in queries or images by leveraging external models can adaptively generate contrastive images, but it entails significant additional costs. To address these shortcomings, we explore using pre-defined visual augmentations to enable flexible adaptation to each query without relying on external models. We observe that each query achieves different contrasts through different visual augmentations. Based on this, we propose a novel method called VSCoDe, Visual-Augmentation Selection for Contrastive Decoding, which adaptively selects augmentations using a proposed distance metric to identify those with higher contrast. Our empirical evaluations demonstrate that VSCoDe outperforms previous methods and enhances the quality of various vision-language tasks without additional training or reliance on external models.},
  archive      = {J_TMLR},
  author       = {Sihyeon Kim and Boryeong Cho and Sangmin Bae and Sumyeong Ahn and Se-Young Yun},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {VSCoDe: Visual-augmentation selection for contrastive decoding},
  url          = {https://openreview.net/forum?id=CqSyPc9W7Y},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized smooth stochastic variational inequalities: Almost sure convergence and convergence rates. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=EjqSpbUBWU'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on solving a stochastic variational inequality (SVI) problem under relaxed smoothness assumption for a class of structured non-monotone operators. The SVI problem has attracted significant interest in the machine learning community due to its immediate application to adversarial training and multi-agent reinforcement learning. In many such applications, the resulting operators do not satisfy the smoothness assumption. To address this issue, we focus on a weaker generalized smoothness assumption called $\alpha$-symmetric. Under $p$-quasi sharpness and $\alpha$-symmetric assumptions on the operator, we study clipped projection (gradient descent-ascent) and clipped Korpelevich (extragradient) methods. For these clipped methods, we provide the first almost-sure convergence results without making any assumptions on the boundedness of either the stochastic operator or the stochastic samples. We also provide the first in-expectation unbiased convergence rate results for these methods under a relaxed smoothness assumption for $\alpha \leq \frac{1}{2}$.},
  archive      = {J_TMLR},
  author       = {Daniil Vankov and Angelia Nedich and Lalitha Sankar},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Generalized smooth stochastic variational inequalities: Almost sure convergence and convergence rates},
  url          = {https://openreview.net/forum?id=EjqSpbUBWU},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EL-clustering: Combining upper- and lower-bounded clusterings for equitable load constraints. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=EkjDfnJ1gU'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of an ordinary clustering algorithm may yield a clustering output where the number of points per cluster (cluster size) varies significantly. In settings where the centers correspond to facilities that provide a service, this can be highly undesirable as the cluster size is essentially the service load for a facility. While prior work has considered imposing either a lower bound on the cluster sizes or an upper bound, imposing both bounds simultaneously has seen limited work, especially for the $k$-median objective, despite its strong practical motivation. In this paper, we solve the \emph{equitable load} (\EL{}) clustering problem where we minimize the $k$-median objective subject to the cluster sizes not exceeding an upper bound or falling below a lower bound. We solve this problem using a modular approach. Specifically, given a clustering solution that satisfies the lower bound constraints and another that satisfies the upper bound constraints, we introduce a combination algorithm which essentially combines both solutions to produce one that satisfies both constraints simultaneously at the expense of a bounded degradation in the $k$-median objective and a slight violation of the upper bound. Our combination algorithm runs in $O(k^3+n)$ time, where $n$ is the number of points and is faster than standard $k$-median algorithms that satisfy either the lower or upper bound constraints. Interestingly, our results can be generalized to various other clustering objectives, including the $k$-means objective. We also do empirical evaluation for $k$-Median objective on benchmark datasets to show that both, the cost as well as the violation factor are significantly smaller in practice than the theoretical worst-case guarantees\footnote{https://github.com/0-rudra-0/el-clustering}.},
  archive      = {J_TMLR},
  author       = {Rajni Dabas and Neelima Gupta and Rudra Bhardwaj and Sapna Grover},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {EL-clustering: Combining upper- and lower-bounded clusterings for equitable load constraints},
  url          = {https://openreview.net/forum?id=EkjDfnJ1gU},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From novelty to imitation: Self-distilled rewards for offline reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=F5K94JI2Jb'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline Reinforcement Learning (RL) aims to learn effective policies from a static dataset without requiring further agent environment interactions. However, its practical adoption is often hindered by the need for explicit reward annotations, which can be costly to engineer or difficult to obtain retrospectively. To address this, we propose ReLOAD (Reinforcement Learning with Offline Reward Annotation via Distillation), a novel reward annotation framework for offline RL. Unlike existing methods that depend on complex alignment procedures, our approach adapts Random Network Distillation (RND) to generate intrinsic rewards from expert demonstrations using a simple yet effective embedding discrepancy measure. First, we train a predictor network to mimic a fixed target network’s embeddings based on expert state transitions. Later, the prediction error between these networks serves as a reward signal for each transition in the static dataset. This mechanism provides a structured reward signal without requiring handcrafted reward annotations. We provide a formal theoretical construct that provides insights into how RND prediction errors effectively serve as intrinsic rewards by distinguishing expert-like transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables robust offline policy learning and achieves performance competitive with traditional reward-annotated methods.},
  archive      = {J_TMLR},
  author       = {Gaurav Chaudhary and Laxmidhar Behera},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {From novelty to imitation: Self-distilled rewards for offline reinforcement learning},
  url          = {https://openreview.net/forum?id=F5K94JI2Jb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An architecture built for federated learning: Addressing data heterogeneity through adaptive normalization-free feature recalibration. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=GtdYFLsblb'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a decentralized collaborative training paradigm preserving stakeholders’ data ownership while improving performance and generalization. However, statistical heterogeneity among client datasets degrades system performance. To address this issue, we propose Adaptive Normalization-free Feature Recalibration (ANFR), a model architecture-level approach that combines weight standardization and channel attention to combat heterogeneous data in FL. ANFR leverages weight standardization to avoid mismatched client statistics and inconsistent averaging, ensuring robustness under heterogeneity, and channel attention to produce learnable scaling factors for feature maps, suppressing inconsistencies across clients due to heterogeneity. We demonstrate that combining these techniques boosts model performance beyond their individual contributions, by improving class selectivity and channel attention weight distribution. ANFR works with any aggregation method, supports both global and personalized FL, and adds minimal overhead. Furthermore, when training with differential privacy, ANFR achieves an appealing balance between privacy and utility, enabling strong privacy guarantees without sacrificing performance. By integrating weight standardization and channel attention in the backbone model, ANFR offers a novel and versatile approach to the challenge of statistical heterogeneity. Extensive experiments show ANFR consistently outperforms established baselines across various aggregation methods, datasets, and heterogeneity conditions. Code is provided at https://github.com/siomvas/ANFR.},
  archive      = {J_TMLR},
  author       = {Vasilis Siomos and Jonathan Passerat-Palmbach and Giacomo Tarroni},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {An architecture built for federated learning: Addressing data heterogeneity through adaptive normalization-free feature recalibration},
  url          = {https://openreview.net/forum?id=GtdYFLsblb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially private clustered federated learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=JSsko0a4yr'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL), which is a decentralized machine learning (ML) approach, often incorporates differential privacy (DP) to provide rigorous data privacy guarantees to clients. Previous works attempted to address high structured data heterogeneity in vanilla FL settings through clustering clients (a.k.a clustered FL), but these methods remain sensitive and prone to errors, further exacerbated by the DP noise. This vulnerability makes the previous methods inappropriate for differentially private FL (DPFL) under structured data heterogeneity. To address this gap, we propose an algorithm for differentially private clustered FL, which is robust to the DP noise in the system and identifies the underlying clients’ clusters correctly. To this end, we propose to cluster clients based on both their model updates and training loss values. Furthermore, for clustering clients’ model updates at the end of the first round, our proposed approach addresses the server’s uncertainties by employing large batch sizes as well as Gaussian Mixture Models (GMM) to reduce the impact of DP and stochastic noise and avoid potential clustering errors. We provide theoretical analysis to justify our approach and evaluate it across diverse data distributions and privacy budgets. Our experimental results show the approach’s effectiveness in addressing high structured data heterogeneity in DPFL.},
  archive      = {J_TMLR},
  author       = {Saber Malekmohammadi and Afaf Taik and Golnoosh Farnadi},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Differentially private clustered federated learning},
  url          = {https://openreview.net/forum?id=JSsko0a4yr},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On diffusion posterior sampling via sequential monte carlo for zero-shot scaffolding of protein motifs. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=KXRYY7iwqh'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of diffusion models, new proteins can be generated at an unprecedented rate. The motif scaffolding problem requires steering this generative process to yield proteins with a desirable functional substructure called a motif. While models have been trained to take the motif as conditional input, recent techniques in diffusion posterior sampling can be leveraged as zero-shot alternatives whose approximations can be corrected with sequential Monte Carlo (SMC) algorithms. In this work, we introduce a new set of guidance potentials for describing scaffolding tasks and solve them by adapting SMC-aided diffusion posterior samplers with an unconditional model, Genie, as a prior. In single motif problems, we find that (i) the proposed potentials perform comparably, if not better, than the conventional masking approach, (ii) samplers based on reconstruction guidance outperform their replacement method counterparts, and (iii) measurement tilted proposals and twisted targets improve performance substantially. Furthermore, as a demonstration, we provide solutions to two multi-motif problems by pairing reconstruction guidance with an SE(3)-invariant potential. We also produce designable internally symmetric monomers with a guidance potential for point symmetry constraints. Our code is available at: https://github.com/matsagad/mres-project.},
  archive      = {J_TMLR},
  author       = {James Matthew Young and O. Deniz Akyildiz},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On diffusion posterior sampling via sequential monte carlo for zero-shot scaffolding of protein motifs},
  url          = {https://openreview.net/forum?id=KXRYY7iwqh},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic primal-dual double block-coordinate for two- way partial AUC maximization. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=M3kibBFP4q'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-way partial AUC (TPAUC) is a critical performance metric for binary classification with imbalanced data, as it focuses on specific ranges of the true positive rate (TPR) and false positive rate (FPR). However, stochastic algorithms for TPAUC optimization remain under-explored, with existing methods either limited to approximated TPAUC loss functions or burdened by sub-optimal complexities. To overcome these limitations, we introduce two innovative stochastic primal-dual double block-coordinate algorithms for TPAUC maximization. These algorithms utilize stochastic block-coordinate updates for both the primal and dual variables, catering to both convex and non-convex settings. We provide theoretical convergence rate analyses, demonstrating significant improvements over prior approaches. Our experimental results, based on multiple benchmark datasets, validate the superior performance of our algorithms, showcasing faster convergence and better generalization. This work advances the state of the art in TPAUC optimization and offers practical tools for real-world machine learning applications.},
  archive      = {J_TMLR},
  author       = {Linli Zhou and Bokun Wang and My T. Thai and Tianbao Yang},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Stochastic primal-dual double block-coordinate for two- way partial AUC maximization},
  url          = {https://openreview.net/forum?id=M3kibBFP4q},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning time-series representations by hierarchical uniformity-tolerance latent balancing. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=NTmVEAiyB5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose TimeHUT, a novel method for learning time-series representations by hierarchical uniformity-tolerance balancing of contrastive representations. Our method uses two distinct losses to learn strong representations with the aim of striking an effective balance between uniformity and tolerance in the embedding space. First, TimeHUT uses a hierarchical setup to learn both instance-wise and temporal information from input time-series. Next, we integrate a temperature scheduler within the vanilla contrastive loss to balance the uniformity and tolerance characteristics of the embeddings. Additionally, a hierarchical angular margin loss enforces instance-wise and temporal contrast losses, creating geometric margins between positive and negative pairs of temporal sequences. This approach improves the coherence of positive pairs and their separation from the negatives, enhancing the capture of temporal dependencies within a time-series sample. We evaluate our approach on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and multivariate classification, as well as Yahoo and KPI datasets for anomaly detection. The results demonstrate that TimeHUT outperforms prior methods by considerable margins on classification, while obtaining competitive results for anomaly detection. Finally, detailed sensitivity and ablation studies are performed to evaluate different components and hyperparameters of our method.},
  archive      = {J_TMLR},
  author       = {Amin Jalali and Milad Soltany and Michael Greenspan and Ali Etemad},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Learning time-series representations by hierarchical uniformity-tolerance latent balancing},
  url          = {https://openreview.net/forum?id=NTmVEAiyB5},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale targeted cause discovery via learning from simulated data. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=NVgy29IQw8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel machine learning approach for inferring causal variables of a target variable from observations. Our focus is on directly inferring a set of causal factors without requiring full causal graph reconstruction, which is computationally challenging in large-scale systems. The identified causal set consists of all potential regulators of the target variable under experimental settings, enabling efficient regulation through intervention. To achieve this, we train a neural network using supervised learning on simulated data to infer causality. By employing a subsampled-ensemble inference strategy, our approach scales with linear complexity in the number of variables, efficiently scaling up to thousands of variables. Empirical results demonstrate superior performance in identifying causal relationships within large-scale gene regulatory networks, outperforming existing methods that emphasize full-graph discovery. We validate our model's generalization capability across out-of-distribution graph structures and generating mechanisms, including gene regulatory networks of E. coli and the human K562 cell line.},
  archive      = {J_TMLR},
  author       = {Jang-Hyun Kim and Claudia Skok Gibbs and Sangdoo Yun and Hyun Oh Song and Kyunghyun Cho},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Large-scale targeted cause discovery via learning from simulated data},
  url          = {https://openreview.net/forum?id=NVgy29IQw8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSC: Posterior sampling-based compression. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=OsqgU6Jz4t'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have transformed the landscape of image generation and now show remarkable potential for image compression. Most of the recent diffusion-based compression methods require training and are tailored for a specific bit-rate. In this work, we propose Posterior Sampling-based Compression (PSC) -- a zero-shot compression method that leverages a pre-trained diffusion model as its sole neural network component, thus enabling the use of diverse, publicly available models without additional training. Our approach is inspired by transform coding methods, which encode the image in some pre-chosen transform domain. However, PSC constructs a transform that is adaptive to the image. This is done by employing a zero-shot diffusion-based posterior sampler so as to progressively construct the rows of the transform matrix. Each new chunk of rows is chosen to reduce the uncertainty about the image given the quantized measurements collected thus far. Importantly, the same adaptive scheme can be replicated at the decoder, thus avoiding the need to encode the transform itself. We demonstrate that even with basic quantization and entropy coding, PSC's performance is comparable to established training-based methods in terms of rate, distortion, and perceptual quality. This is while providing greater flexibility, allowing to choose at inference time any desired rate or distortion.},
  archive      = {J_TMLR},
  author       = {Noam Elata and Tomer Michaeli and Michael Elad},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {PSC: Posterior sampling-based compression},
  url          = {https://openreview.net/forum?id=OsqgU6Jz4t},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coreset-driven re-labeling: Tackling noisy annotations with noise-free gradients. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Tk78vb2Qd7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale datasets invariably contain annotation noise. Re-labeling methods have been developed to handle annotation noise in large-scale datasets. Though various methodologies to alleviate annotation noise have been developed, these are particularly time-consuming and computationally intensive. The requirement of high computational power and longer time duration can be drastically reduced by selecting a representative coreset. In this work, we adapt a noise-free gradient-based coreset selection method towards re-labeling applications for noisy datasets with erroneous labels. We introduce ‘confidence score’ to the coreset selection method to cater for the presence of noisy labels. Through extensive evaluation over CIFAR-100N, Web Vision, and ImageNet-1K Datasets, we demonstrate that our method outperforms the SOTA coreset selection for re-labeling methods (DivideMix and SOP+). We have provided the codebase at URL.},
  archive      = {J_TMLR},
  author       = {Saumyaranjan Mohanty and Konda Reddy Mopuri},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Coreset-driven re-labeling: Tackling noisy annotations with noise-free gradients},
  url          = {https://openreview.net/forum?id=Tk78vb2Qd7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A case for library-level $k$-means binning in histogram gradient-boosted trees. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=UaTrLLspJa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern Gradient Boosted Decision Trees (GBDTs) accelerate split finding with histogram-based binning, which reduces complexity from $O(N\log N)$ to $O(N)$ by aggregating gradients into fixed-size bins. However, the predominant quantile binning strategy—designed to distribute data points evenly among bins—may overlook critical boundary values that could enhance predictive performance. In this work, we consider a novel approach that replaces quantile binning with a $k$-means discretizer initialized with quantile bins, and justify the swap with a proof showing how, for any $L$-Lipschitz function, k-means maximizes the worst-case explained variance of Y obtained when treating all values in a given bin as equivalent. We test this swap against quantile and uniform binning on 33 OpenML datasets plus synthetics that control for modality, skew, and bin budget. Across 18 regression datasets, k-means shows no statistically significant losses at the 5% level and wins in three cases—most strikingly a 55% MSE drop on one particularly skewed dataset—even though k-means' mean reciprocal rank (MRR) is slightly lower (0.65 vs 0.72). On the 15 classification datasets the two methods are statistically tied (MRR 0.70 vs 0.68) with gaps $\leq$0.2 pp. Synthetic experiments confirm consistently large MSE gains—typically $>$20% and rising to 90% as outlier magnitude increases or bin budget drops. We find that k-means keeps error on par with exhaustive (no-binning) splitting when extra cuts add little value, yet still recovers key split points that quantile overlooks. As such, we advocate for a built-in bin_method=$k$-means flag, especially in regression tasks and in tight-budget settings such as the 32–64-bin GPU regime—because it is a "safe default" with large upside, yet adds only a one-off, cacheable overhead ($\approx$ 3.5s per feature to bin 10M rows on one Apple M1 thread).},
  archive      = {J_TMLR},
  author       = {Asher Labovich},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {A case for library-level $k$-means binning in histogram gradient-boosted trees},
  url          = {https://openreview.net/forum?id=UaTrLLspJa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical comparison of membership inference attacks in deep transfer learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=UligTUCgdt'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of powerful large-scale foundation models, the training paradigm is increasingly shifting from from-scratch training to transfer learning. This enables high utility training with small, domain-specific datasets typical in sensitive applications. Membership inference attacks (MIAs) provide an empirical estimate of the privacy leakage by machine learning models. Yet, prior assessments of MIAs against models fine-tuned with transfer learning rely on a small subset of possible attacks. We address this by comparing performance of diverse MIAs in transfer learning settings to help practitioners identify the most efficient attacks for privacy risk evaluation. We find that attack efficacy decreases with the increase in training data for score-based MIAs. We find that there is no one MIA which captures all privacy risks in models trained with transfer learning. While the Likelihood Ratio Attack (LiRA) demonstrates superior performance across most experimental scenarios, the Inverse Hessian Attack (IHA) proves to be more effective against models fine-tuned on PatchCamelyon dataset in high data regime.},
  archive      = {J_TMLR},
  author       = {Yuxuan Bai and Gauri Pradhan and Marlon Tobaben and Antti Honkela},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Empirical comparison of membership inference attacks in deep transfer learning},
  url          = {https://openreview.net/forum?id=UligTUCgdt},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating spatial information into goal-conditioned hierarchical reinforcement learning via graph representations. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=a7Bx4s5gA8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.},
  archive      = {J_TMLR},
  author       = {Shuyuan Zhang and Zihan Wang and Xiao-Wen Chang and Doina Precup},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Incorporating spatial information into goal-conditioned hierarchical reinforcement learning via graph representations},
  url          = {https://openreview.net/forum?id=a7Bx4s5gA8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pref-GUIDE: Continual policy learning from real-time human feedback via preference-based learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=dWGUwidXDm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training reinforcement learning agents with human feedback is crucial when task objectives are difficult to specify through dense reward functions. While prior methods rely on offline trajectory comparisons to elicit human preferences, such data is unavailable in online learning scenarios where agents must adapt on the fly. Recent approaches address this by collecting real-time scalar feedback to guide agent behavior and train reward models for continued learning after human feedback becomes unavailable. However, scalar feedback is often noisy and inconsistent, limiting the accuracy and generalization of learned rewards. We propose Pref-GUIDE, a framework that transforms real-time scalar feedback into preference-based data to improve reward model learning for continual policy training. Pref-GUIDE Individual mitigates temporal inconsistency by comparing agent behaviors within short windows and filtering ambiguous feedback. Pref-GUIDE Voting further enhances robustness by aggregating reward models across a population of users to form consensus preferences. Across three challenging environments, Pref-GUIDE significantly outperforms scalar-feedback baselines, with the voting variant exceeding even expert-designed dense rewards. By reframing scalar feedback as structured preferences with population feedback, Pref-GUIDE, offers a scalable and principled approach for harnessing human input in online reinforcement learning.},
  archive      = {J_TMLR},
  author       = {Zhengran Ji and Boyuan Chen},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Pref-GUIDE: Continual policy learning from real-time human feedback via preference-based learning},
  url          = {https://openreview.net/forum?id=dWGUwidXDm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decomposed direct preference optimization for structure-based drug design. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=dwSpo5DRk8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have achieved promising results for Structure-Based Drug Design (SBDD). Nevertheless, high-quality protein subpocket and ligand data are relatively scarce, which hinders the models’ generation capabilities. Recently, Direct Preference Optimization (DPO) has emerged as a pivotal tool for aligning generative models with human preferences. In this paper, we propose DecompDpo, a structure-based optimization method aligns diffusion models with pharmaceutical needs using multi-granularity preference pairs. DecompDpo introduces decomposition into the optimization objectives and obtains preference pairs at the molecule or decomposed substructure level based on each objective’s decomposability. Additionally, DecompDpo introduces a physics-informed energy term to ensure reasonable molecular conformations in the optimization results. Notably, DecompDpo can be effectively used for two main purposes: (1) fine-tuning pretrained diffusion models for molecule generation across various protein families, and (2) molecular optimization given a specific protein subpocket after generation. Extensive experiments on the CrossDocked2020 benchmark show that DecompDpo significantly improves model performance, achieving up to 98.5% Med. High Affinity and a 43.9% success rate for molecule generation, and 100% Med. High Affinity and a 52.1% success rate for targeted molecule optimization.},
  archive      = {J_TMLR},
  author       = {Xiwei Cheng and Xiangxin Zhou and Yuwei Yang and Yu Bao and Quanquan Gu},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Decomposed direct preference optimization for structure-based drug design},
  url          = {https://openreview.net/forum?id=dwSpo5DRk8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does unsupervised domain adaptation improve the robustness of amortized bayesian inference? a systematic evaluation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ewgLuvnEw6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in silico and practice. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications -- a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA to increase the robustness of ABI.},
  archive      = {J_TMLR},
  author       = {Lasse Elsemüller and Valentin Pratz and Mischa von Krause and Andreas Voss and Paul-Christian Bürkner and Stefan T. Radev},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Does unsupervised domain adaptation improve the robustness of amortized bayesian inference? a systematic evaluation},
  url          = {https://openreview.net/forum?id=ewgLuvnEw6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-NN as a simple and effective estimator of transferability. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=hGlkjP1zHc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How well can one expect transfer learning to work in a new setting where the domain is shifted, the task is different, and the architecture changes? Many transfer learning metrics have been proposed to answer this question. But how accurate are their predictions in a realistic new setting? We conducted an extensive evaluation involving over 42,000 experiments comparing 23 transferability metrics across 16 different datasets to assess their ability to predict transfer performance. Our findings reveal that none of the existing metrics perform well across the board. However, we find that a simple k-nearest neighbor evaluation -- as is commonly used to evaluate feature quality for self-supervision -- not only surpasses existing metrics, but also offers better computational efficiency and ease of implementation.},
  archive      = {J_TMLR},
  author       = {Moein Sorkhei and Christos Matsoukas and Johan Fredin Haslum and Emir Konuk and Kevin Smith},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {K-NN as a simple and effective estimator of transferability},
  url          = {https://openreview.net/forum?id=hGlkjP1zHc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond marginals: Learning joint spatio-temporal patterns for multivariate anomaly detection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=iETTv1okjX'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to improve anomaly detection (AD) by incorporating the time-varying non-linear spatio-temporal correlations of the multi-variate time series data in the modeling process. In multivariate AD, the simultaneous deviation of multiple nodes from their expected behavior can indicate an anomaly, even if no individual node shows a clearly abnormal pattern. In many existing approaches, time series variables are assumed to be (conditionally) independent, which oversimplifies real-world interactions. Our approach addresses this by modeling joint dependencies using a copula-based framework, which decouples the modeling of marginal distributions, temporal dynamics, and inter-variable dependencies. We use a transformer encoder to capture temporal patterns, and to model spatial (inter-variable) dependencies, we integrate a copula. Both components are trained jointly in a latent space using a self-supervised contrastive learning objective to learn meaningful feature representations to separate normal and anomaly samples.},
  archive      = {J_TMLR},
  author       = {Padmaksha Roy and Almuatazbellah Boker and Lamine Mili},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Beyond marginals: Learning joint spatio-temporal patterns for multivariate anomaly detection},
  url          = {https://openreview.net/forum?id=iETTv1okjX},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse-to-sparse training of diffusion models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=iRupdoPLJa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models (DMs) are a powerful type of generative models that have achieved state-of-the-art results in various image synthesis tasks and have shown potential in other domains, such as natural language processing and temporal data modeling. Despite their stable training dynamics and ability to produce diverse high-quality samples, DMs are notorious for requiring significant computational resources, both in the training and inference stages. Previous work has focused mostly on increasing the efficiency of model inference. This paper introduces, for the first time, the paradigm of sparse-to-sparse training to DMs, with the aim of improving both training and inference efficiency. We focus on unconditional generation and train sparse DMs from scratch (Latent Diffusion and ChiroDiff) on six datasets using three different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of sparsity in model performance. Our experiments show that sparse DMs are able to match and often outperform their Dense counterparts, while substantially reducing the number of trainable parameters and FLOPs. We also identify safe and effective values to perform sparse-to-sparse training of DMs.},
  archive      = {J_TMLR},
  author       = {Inês Cardoso Oliveira and Decebal Constantin Mocanu and Luis A. Leiva},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Sparse-to-sparse training of diffusion models},
  url          = {https://openreview.net/forum?id=iRupdoPLJa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral clustering and labeling for crowdsourcing with inherently distinct task types. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=jVQjtzcvAc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dawid-Skene model is the most widely assumed model in the analysis of crowdsourcing algorithms that estimate ground-truth labels from noisy worker responses. In this work, we are motivated by crowdsourcing applications where workers have distinct skill sets and their accuracy additionally depends on a task's type. Focusing on the case where there are two types of tasks, we propose a spectral method to partition tasks into two groups such that a worker has the same reliability for all tasks within a group. Our analysis reveals a separability condition such that task types can be perfectly recovered if the number of workers $n$ scales logarithmically with the number of tasks $d$. Numerical experiments show how clustering tasks by type before estimating ground-truth labels enhances the performance of crowdsourcing algorithms in practical applications.},
  archive      = {J_TMLR},
  author       = {Saptarshi Mandal and Seo Taek Kong and Dimitrios Katselis and R. Srikant},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Spectral clustering and labeling for crowdsourcing with inherently distinct task types},
  url          = {https://openreview.net/forum?id=jVQjtzcvAc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Activation sharding for scalable training of large models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=kQCuMcEneq'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite fast progress, efficiently training large language models (LLMs) in extremely long contexts remains challenging. Existing methods fall back to training LLMs with short contexts (up to a few thousand tokens) and use inference time techniques when evaluating on very long contexts (above 1M tokens). Training on very long contexts is limited by GPU memory availability and the prohibitively long training times it requires on state-of-the-art hardware. Meanwhile, many real-life applications require training/fine-tuning with long context on specific tasks. Such applications include, for example, augmenting the context with various sources of raw reference information for extraction, summarization, or fact reconciliation tasks. We propose adjoint sharding, a novel technique that comprises sharding gradient calculation during training to reduce memory requirements by orders of magnitude, making training on very long contexts computationally tractable. At the core of our adjoint sharding algorithm lies the adjoint method, which efficiently computes gradients that are provably equivalent to the gradients computed using standard backpropagation. We also propose truncated adjoint sharding to accelerate the algorithm while maintaining performance. We provide a distributed and a parallel-computing version of adjoint sharding to speed up training and to show that adjoint sharding is compatible with these standard memory-reduction techniques. Empirical results show the proposed adjoint sharding algorithm reduces memory usage by up to 3$\times$ on a large language model with 1.27B parameters on 1M context length training. This reduction in memory usage allows increasing the maximum context length of training a 1.27B parameter model from 35K tokens to above 100K tokens on a training infrastructure composed of five AWS P4 instances.},
  archive      = {J_TMLR},
  author       = {Xingzi Xu and Amir Tavanaei and Kavosh Asadi and Karim Bouyarmane},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Activation sharding for scalable training of large models},
  url          = {https://openreview.net/forum?id=kQCuMcEneq},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable and responsible foundation models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=nLJZh4M6S5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.},
  archive      = {J_TMLR},
  author       = {Xinyu Yang and Junlin Han and Rishi Bommasani and Jinqi Luo and Wenjie Qu and Wangchunshu Zhou and Adel Bibi and Xiyao Wang and Jaehong Yoon and Elias Stengel-Eskin and Shengbang Tong and Lingfeng Shen and Rafael Rafailov and Runjia Li and Zhaoyang Wang and Yiyang Zhou and Chenhang Cui and Yu Wang and Wenhao Zheng and Huichi Zhou and Jindong Gu and Zhaorun Chen and Peng Xia and Tony Lee and Thomas P Zollo and Vikash Sehwag and Jixuan Leng and Jiuhai Chen and Yuxin Wen and Huan Zhang and Zhun Deng and Linjun Zhang and Pavel Izmailov and Pang Wei Koh and Yulia Tsvetkov and Andrew Gordon Wilson and Jiaheng Zhang and James Zou and Cihang Xie and Hao Wang and Philip Torr and Julian McAuley and David Alvarez-Melis and Florian Tramèr and Kaidi Xu and Suman Jana and Chris Callison-Burch and Rene Vidal and Filippos Kokkinos and Mohit Bansal and Beidi Chen and Huaxiu Yao},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Reliable and responsible foundation models},
  url          = {https://openreview.net/forum?id=nLJZh4M6S5},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlearning misalignment for personalized LLM adaptation via instance-response-dependent discrepancies. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=njE3swFBMc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Large Language Models (LLMs) have revolutionized chatbot interactions, they often fall short of aligning responses with the nuanced preferences of individual users, a challenge rooted in the inherently subjective and proprietary nature of those preferences. Consequently, prompt-based learning, though effective in enhancing factual accuracy due to its emphasis on universal correctness, remains insufficient for achieving accurate personalised response alignment. Because user preferences vary widely across individuals and contexts, aligning responses requires a more personalized and context-aware approach. To address this limitation, we propose Consistent Marginalization (CM), a novel framework that aims to unlearn misalignment by constructing a personalised memory bank of instance-response-dependent discrepancies, built from a small set of user preference samples. This personalised memory bank equips LLMs with the ability to understand, recall, and adapt to individual preferences, enabling more consistent and personalized responses. Evaluated across a diverse range of domain-specific datasets and model architectures, CM yields notable improvements in response alignment and robustness. We believe Consistent Marginalization represents a valuable step toward enabling LLMs to become genuinely personable and adaptive conversational agents by understanding user preferences and generating responses that are better aligned with individual user expectations.},
  archive      = {J_TMLR},
  author       = {Cheng Chen and Atsushi Nitanda and Ivor Tsang},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Unlearning misalignment for personalized LLM adaptation via instance-response-dependent discrepancies},
  url          = {https://openreview.net/forum?id=njE3swFBMc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dependency-aware maximum likelihood estimation for active learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=qDVDSXXGK1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning aims to efficiently build a labeled training set by strategically selecting samples to query labels from annotators. In this sequential process, each sample acquisition influences subsequent selections, causing dependencies among samples in the labeled set. However, these dependencies are overlooked during the model parameter estimation stage when updating the model using Maximum Likelihood Estimation (MLE), a conventional method that assumes independent and identically distributed (i.i.d.) data. We propose Dependency-aware MLE (DMLE), which corrects MLE within the active learning framework by addressing sample dependencies typically neglected due to the i.i.d. assumption, ensuring consistency with active learning principles in the model parameter estimation process. This improved method achieves superior performance across multiple benchmark datasets, reaching higher performance in earlier cycles compared to conventional MLE. Specifically, we observe average accuracy improvements of 6%, 8.6%, and 10.5% for k=1, k=5, and k=10 respectively, after collecting the first 100 samples, where entropy is the acquisition function and k is the query batch size acquired at every active learning cycle.},
  archive      = {J_TMLR},
  author       = {Beyza Kalkanli and Tales Imbiriba and Stratis Ioannidis and Deniz Erdogmus and Jennifer Dy},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Dependency-aware maximum likelihood estimation for active learning},
  url          = {https://openreview.net/forum?id=qDVDSXXGK1},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing utility and privacy: Dynamically private SGD with random projection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=u6OSRdkAwl'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic optimization is a pivotal enabler in modern machine learning, producing effective models for various tasks. However, several existing works have shown that model parameters and gradient information are susceptible to privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy concerns, its static noise mechanism impacts the error bounds for model performance. Additionally, with the exponential increase in model parameters, efficient learning of these models using stochastic optimizers has become more challenging. To address these concerns, we introduce the Dynamically Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we combine two important ideas: (i) dynamic differential privacy (DDP) with automatic gradient clipping and (ii) random projection with SGD, allowing dynamic adjustment of the tradeoff between utility and privacy of the model. It exhibits provably sub-linear convergence rates across different objective functions, matching the best available rate. The theoretical analysis further suggests that DDP leads to better utility at the cost of privacy, while random projection enables more efficient model learning. Extensive experiments across diverse datasets show that D2P2-SGD remarkably enhances accuracy while maintaining privacy. Our code is available here.},
  archive      = {J_TMLR},
  author       = {Zhanhong Jiang and Md Zahid Hasan and Nastaran Saadati and Aditya Balu and Chao Liu and Soumik Sarkar},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Balancing utility and privacy: Dynamically private SGD with random projection},
  url          = {https://openreview.net/forum?id=u6OSRdkAwl},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MGPATH: A vision-language model with multi-granular prompt learning for few-shot whole slide pathology classification. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=u7U81JLGjH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model’s ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this https://github.com/HauschildLab/MGPATH.},
  archive      = {J_TMLR},
  author       = {Anh-Tien Nguyen and Duy Minh Ho Nguyen and Nghiem Tuong Diep and Trung Quoc Nguyen and Nhat Ho and Jacqueline Michelle Metsch and Miriam Cindy Maurer and Daniel Sonntag and Hanibal Bohnenberger and Anne-Christin Hauschild},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {MGPATH: A vision-language model with multi-granular prompt learning for few-shot whole slide pathology classification},
  url          = {https://openreview.net/forum?id=u7U81JLGjH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HybridFlow: Quantification of aleatoric and epistemic uncertainty with a single hybrid model. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=xRiEdSyVjY'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification is critical for ensuring robustness in high-stakes machine learning applications. We introduce HybridFlow, a modular hybrid architecture that unifies the modeling of aleatoric and epistemic uncertainty by combining a Conditional Masked Autoregressive normalizing flow for estimating aleatoric uncertainty with a flexible probabilistic predictor for epistemic uncertainty. The framework supports integration with any probabilistic model class, allowing users to easily adapt HybridFlow to existing architectures without sacrificing predictive performance. HybridFlow improves upon previous uncertainty quantification frameworks across a range of regression tasks, such as depth estimation, a collection of regression benchmarks, and a scientific case study of ice sheet emulation. We also provide empirical results of the quantified uncertainty, showing that the uncertainty quantified by HybridFlow is calibrated and better aligns with model error than existing methods for quantifying aleatoric and epistemic uncertainty. HybridFlow addresses a key challenge in Bayesian deep learning, unifying aleatoric and epistemic uncertainty modeling in a single robust framework.},
  archive      = {J_TMLR},
  author       = {Peter Van Katwyk and Karianne Bergen},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {HybridFlow: Quantification of aleatoric and epistemic uncertainty with a single hybrid model},
  url          = {https://openreview.net/forum?id=xRiEdSyVjY},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chimera: State space models beyond sequences. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=yv0TUssepk'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based deep learning methods have emerged as the standard approach to model diverse data such as sequences, images, and graphs. These methods rely on self-attention, which treats data as an unordered set of elements. This ignores the neighborhood structure or graph topology of the data and requires the use of inductive biases, such as position embeddings in sequences and images, and random walks in graphs, to incorporate topology. However, developing bespoke inductive biases for each task requires significant effort and can also introduce side-effects hindering generalization. In this work, we introduce Chimera, a unified model that directly incorporates the data topology in a principled way, obviating the need for domain-specific biases. Central to Chimera is the observation that state-space models---which naturally do not require position embeddings---can be generalized to capture any general graph topology. Our experiments demonstrate the versatility of our approach---Chimera achieves strong performance across the domains of language, vision, and graphs, outperforming BERT on GLUE by 0.7 points, ViT on ImageNet-1k by 2.6%, and all the baselines on the Long Range Graph Benchmark. Our results validate Chimera's principled methodological contributions and affirm the long-held belief that data topology is a powerful inductive bias across modalities. We further propose algorithmic optimizations to improve Chimera's efficiency while maintaining performance: 1) For the subclass of Directed Acyclic Graphs we show that Chimera can be implemented as a linear time recurrence. 2) For general graphs, we relax the method with a simple mathematical approximation, achieving Transformer's quadratic complexity without relying on domain-specific biases.},
  archive      = {J_TMLR},
  author       = {Aakash Lahoti and Tanya Marwah and Ratish Puduppully and Albert Gu},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Chimera: State space models beyond sequences},
  url          = {https://openreview.net/forum?id=yv0TUssepk},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
