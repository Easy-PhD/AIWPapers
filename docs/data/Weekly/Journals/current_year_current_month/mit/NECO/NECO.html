<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco">NECO - 6</h2>
<ul>
<li><details>
<summary>
(2025). Fast multigroup gaussian process factor models. <em>NECO</em>, <em>37</em>(9), 1709-1782. (<a href='https://doi.org/10.1162/neco.a.22'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes are now commonly used in dimensionality reduction approaches tailored to neuroscience, especially to describe changes in high-dimensional neural activity over time. As recording capabilities expand to include neuronal populations across multiple brain areas, cortical layers, and cell types, interest in extending gaussian process factor models to characterize multipopulation interactions has grown. However, the cubic runtime scaling of current methods with the length of experimental trials and the number of recorded populations (groups) precludes their application to large-scale multipopulation recordings. Here, we improve this scaling from cubic to linear in both trial length and group number. We present two approximate approaches to fitting multigroup gaussian process factor models based on inducing variables and the frequency domain. Empirically, both methods achieved orders of magnitude speed-up with minimal impact on statistical performance, in simulation and on neural recordings of hundreds of neurons across three brain areas. The frequency domain approach, in particular, consistently provided the greatest runtime benefits with the fewest trade-offs in statistical performance. We further characterize the estimation biases introduced by the frequency domain approach and demonstrate effective strategies to mitigate them. This work enables a powerful class of analysis techniques to keep pace with the growing scale of multipopulation recordings, opening new avenues for exploring brain function.},
  archive      = {J_NECO},
  author       = {Gokcen, Evren and Jasper, Anna I. and Kohn, Adam and Machens, Christian K. and Yu, Byron M.},
  doi          = {10.1162/neco.a.22},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1709-1782},
  shortjournal = {Neural Comput.},
  title        = {Fast multigroup gaussian process factor models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From function to implementation: Exploring degeneracy in evolved artificial agents. <em>NECO</em>, <em>37</em>(9), 1677-1708. (<a href='https://doi.org/10.1162/neco.a.19'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Degeneracy—the ability of different structures to perform the same function—is a fundamental feature of biological systems, contributing to their robustness and evolvability. However, the ubiquity of degeneracy in systems generated through adaptive processes complicates our understanding of the behavioral and computational strategies they employ. In this study, we investigated degeneracy in simple computational agents, known as Markov brains, trained using an artificial evolution algorithm to solve a spatial navigation task with or without associative memory. We analyzed degeneracy at three levels: behavioral, structural, and computational, with a focus on the last. Using information-theoretical concepts, Tononi et al. (1999) proposed a functional measure of degeneracy within biological networks. Here, we extended this approach to compare degeneracy across multiple networks. Using information-theoretical tools and causal analysis, we explored the computational strategies of the evolved agents and quantified their computational degeneracy. Our findings reveal a hierarchy of degenerate solutions, from varied behaviors to diverse structures and computations. Even agents with identical evolved behaviors demonstrated different underlying structures and computations. These results underscore the pervasive nature of degeneracy in neural networks, blurring the lines between the algorithmic and implementation levels in adaptive systems, and highlight the importance of advanced analytical tools to understand their complex behaviors.},
  archive      = {J_NECO},
  author       = {Hu, Zhimin and Cingiler, Oğulcan and Bohm, Clifford and Albantakis, Larissa},
  doi          = {10.1162/neco.a.19},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1677-1708},
  shortjournal = {Neural Comput.},
  title        = {From function to implementation: Exploring degeneracy in evolved artificial agents},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward generalized entropic sparsification for convolutional neural networks. <em>NECO</em>, <em>37</em>(9), 1648-1676. (<a href='https://doi.org/10.1162/neco.a.21'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are reported to be overparametrized. The search for optimal (minimal) and sufficient architecture is an NP-hard problem: if the network has N neurons, then there are 2 N possibilities to connect them—and therefore 2 N possible architectures and 2 N Boolean hyperparameters to encode them. Selecting the best possible hyperparameter out of them becomes an N p -hard problem since 2 N grows in N faster then any polynomial N p ⁠ . Here, we introduce a layer-by-layer data-driven pruning method based on the mathematical idea aiming at a computationally scalable entropic relaxation of the pruning problem. The sparse subnetwork is found from the pretrained (full) CNN using the network entropy minimization as a sparsity constraint. This allows deploying a numerically scalable algorithm with a sublinear scaling cost. The method is validated on several benchmarks (architectures): on MNIST (LeNet), resulting in sparsity of 55% to 84% and loss in accuracy of just 0.1% to 0.5%, and on CIFAR-10 (VGG-16, ResNet18), resulting in sparsity of 73% to 89% and loss in accuracy of 0.1% to 0.5%.},
  archive      = {J_NECO},
  author       = {Barisin, Tin and Horenko, Illia},
  doi          = {10.1162/neco.a.21},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1648-1676},
  shortjournal = {Neural Comput.},
  title        = {Toward generalized entropic sparsification for convolutional neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring stimulus information transfer between neural populations through the communication subspace. <em>NECO</em>, <em>37</em>(9), 1600-1647. (<a href='https://doi.org/10.1162/neco.a.17'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensory processing arises from the communication between neural populations across multiple brain areas. While the widespread presence of neural response variability shared throughout a neural population limits the amount of stimulus-related information those populations can accurately represent, how this variability affects the interareal communication of sensory information is unknown. We propose a mathematical framework to understand the impact of neural population response variability on sensory information transmission. We combine linear Fisher information, a metric connecting stimulus representation and variability, with the framework of communication subspaces, which suggests that functional mappings between cortical populations are low-dimensional relative to the space of population activity patterns. From this, we partition Fisher information depending on the alignment between the population covariance and the mean tuning direction projected onto the communication subspace or its orthogonal complement. We provide mathematical and numerical analyses of our proposed decomposition of Fisher information and examine theoretical scenarios that demonstrate how to leverage communication subspaces for flexible routing and gating of stimulus information. This work will provide researchers investigating interareal communication with a theoretical lens through which to understand sensory information transmission and guide experimental design.},
  archive      = {J_NECO},
  author       = {Weiss, Oren and Coen-Cagli, Ruben},
  doi          = {10.1162/neco.a.17},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1600-1647},
  shortjournal = {Neural Comput.},
  title        = {Measuring stimulus information transfer between neural populations through the communication subspace},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the architectural biases of the cortical microcircuit. <em>NECO</em>, <em>37</em>(9), 1551-1599. (<a href='https://doi.org/10.1162/neco.a.23'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cortex plays a crucial role in various perceptual and cognitive functions, driven by its basic unit, the canonical cortical microcircuit. Yet, we remain short of a framework that definitively explains the structure-function relationships of this fundamental neuroanatomical motif. To better understand how physical substrates of cortical circuitry facilitate their neuronal dynamics, we employ a computational approach using recurrent neural networks and representational analyses. We examine the differences manifested by the inclusion and exclusion of biologically motivated interareal laminar connections on the computational roles of different neuronal populations in the microcircuit of hierarchically related areas throughout learning. Our findings show that the presence of feedback connections correlates with the functional modularization of cortical populations in different layers and provides the microcircuit with a natural inductive bias to differentiate expected and unexpected inputs at initialization, which we justify mathematically. Furthermore, when testing the effects of training the microcircuit and its variants with a predictive-coding-inspired strategy, we find that doing so helps better encode noisy stimuli in areas of the cortex that receive feedback, all of which combine to suggest evidence for a predictive-coding mechanism serving as an intrinsic operative logic in the cortex.},
  archive      = {J_NECO},
  author       = {Balwani, Aishwarya and Cho, Suhee and Choi, Hannah},
  doi          = {10.1162/neco.a.23},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1551-1599},
  shortjournal = {Neural Comput.},
  title        = {Exploring the architectural biases of the cortical microcircuit},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic pathways of modulation enable robust task packing within neural dynamics. <em>NECO</em>, <em>37</em>(9), 1529-1550. (<a href='https://doi.org/10.1162/neco.a.18'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how brain networks learn and manage multiple tasks simultaneously is of interest in both neuroscience and artificial intelligence. In this regard, a recent research thread in theoretical neuroscience has focused on how recurrent neural network models and their internal dynamics enact multitask learning. To manage different tasks requires a mechanism to convey information about task identity or context into the model, which from a biological perspective may involve mechanisms of neuromodulation. In this study, we use recurrent network models to probe the distinctions between two forms of contextual modulation of neural dynamics, at the level of neuronal excitability and at the level of synaptic strength. We characterize these mechanisms in terms of their functional outcomes, focusing on their robustness to context ambiguity and, relatedly, their efficiency with respect to packing multiple tasks into finite-size networks. We also demonstrate the distinction between these mechanisms at the level of the neuronal dynamics they induce. Together, these characterizations indicate complementarity and synergy in how these mechanisms act, potentially over many timescales, toward enhancing the robustness of multitask learning.},
  archive      = {J_NECO},
  author       = {Vedovati, Giacomo and Ching, ShiNung},
  doi          = {10.1162/neco.a.18},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1529-1550},
  shortjournal = {Neural Comput.},
  title        = {Synergistic pathways of modulation enable robust task packing within neural dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
