<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>mit</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alj">ALJ - 9</h2>
<ul>
<li><details>
<summary>
(2025). Automating the search for artificial life with foundation models. <em>ALJ</em>, <em>31</em>(3), 368-396. (<a href='https://doi.org/10.1162/ARTL.a.8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial and error to discover the configurations of lifelike simulations. This article presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called automated search for Artificial Life (ASAL), (a) finds simulations that produce target phenomena, (b) discovers simulations that generate temporally open-ended novelty, and (c) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates, including Boids, Particle Life, the Game of Life, Lenia, and neural cellular automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids life-forms, as well as cellular automata that are open-ended like Conway’s Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.},
  archive      = {J_ALJ},
  author       = {Kumar, Akarsh and Lu, Chris and Kirsch, Louis and Tang, Yujin and Stanley, Kenneth O. and Isola, Phillip and Ha, David},
  doi          = {10.1162/ARTL.a.8},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {368-396},
  shortjournal = {Artif. Life},
  title        = {Automating the search for artificial life with foundation models},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive distinctions as a language for cognitive science: Comparing methods of description in a model of referential communication. <em>ALJ</em>, <em>31</em>(3), 345-367. (<a href='https://doi.org/10.1162/artl_a_00475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An analysis of the language we use in scientific practice is critical to developing more rigorous and sound methodologies. This article argues that how certain methods of description are commonly employed in cognitive science risks obscuring important features of an agent’s cognition. We propose to make explicit a method of description whereby the concept of cognitive distinctions is the core principle. A model of referential communication is developed and analyzed as a platform to compare methods of description. We demonstrate that cognitive distinctions, realized in a graph theoretic formalism, better describe the behavior and perspective of a simple model agent than other, less systematic or natural language–dependent methods. We then consider how different descriptions relate to one another in the broader methodological framework of minimally cognitive behavior. Finally, we explore the consequences of, and challenges for, cognitive distinctions as a useful concept and method in the tool kit of cognitive scientists.},
  archive      = {J_ALJ},
  author       = {Gaul, Thomas M. and Izquierdo, Eduardo J.},
  doi          = {10.1162/artl_a_00475},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {345-367},
  shortjournal = {Artif. Life},
  title        = {Cognitive distinctions as a language for cognitive science: Comparing methods of description in a model of referential communication},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behaviour diversity in a walking and climbing centipede-like virtual creature. <em>ALJ</em>, <em>31</em>(3), 321-344. (<a href='https://doi.org/10.1162/artl_a_00476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot controllers are often optimized for a single robot in a single environment. This approach proves brittle, as such a controller will often fail to produce sensible behavior for a new morphology or environment. In comparison, animal gaits are robust and versatile. By observing animals, and attempting to extract general principles of locomotion from their movement, we aim to design a single, decentralized controller applicable to diverse morphologies and environments. The controller implements the three components of (a) undulation, (b) peristalsis, and (c) leg motion, which we believe are the essential elements in most animal gaits. This work is a first step toward a general controller. Accordingly, the controller has been evaluated on a limited range of simulated centipede-like robot morphologies. The centipede is chosen as inspiration because it moves using both body contractions and legged locomotion. For a controller to work in qualitatively different settings, it must also be able to exhibit qualitatively different behaviors. We find that six different modes of locomotion emerge from our controller in response to environmental and morphological changes. We also find that different parts of the centipede model can exhibit different modes of locomotion, simultaneously, based on local morphological features. This controller can potentially aid in the design or evolution of robots, by quickly testing the potential of a morphology, or be used to get insights about underlying locomotion principles in the centipede.},
  archive      = {J_ALJ},
  author       = {Norstein, Emma Stensby and Yasui, Kotaro and Kano, Takeshi and Ishiguro, Akio and Glette, Kyrre},
  doi          = {10.1162/artl_a_00476},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {321-344},
  shortjournal = {Artif. Life},
  title        = {Behaviour diversity in a walking and climbing centipede-like virtual creature},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benefit game 2.0: Alien seaweed Swarms—Exploring the interplay of human activity and environmental sustainability. <em>ALJ</em>, <em>31</em>(3), 304-320. (<a href='https://doi.org/10.1162/artl_a_00468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Benefit Game 2.0, a multiscreen Artificial Life gameplay installation. Saccharina latissima , a seaweed species economically beneficial to humans but threatened by overexploitation, motivates the creation of this artwork. Technically, the authors create an underwater virtual ecosystem consisting of a seaweed swarm and symbiotic fungi, created using procedural content generation via machine learning and rule-based methods. Moreover, the work features a unique cybernetic loop structure, incorporating audience observation and game token interactions. This virtual system is also symbolically influenced in real time by indoor carbon dioxide measurements, serving as an artistic metaphor for the broader impacts of climate change. This integration with the physical game machine underscores the fragile relationship between human activities and the environment under severe global climate change and immerses the audience in the challenging balance between sustainability and profit seeking in this context.},
  archive      = {J_ALJ},
  author       = {Fei, Dan-Lu and Wu, Zi-Wei and Zhang, Kang},
  doi          = {10.1162/artl_a_00468},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {304-320},
  shortjournal = {Artif. Life},
  title        = {Benefit game 2.0: Alien seaweed Swarms—Exploring the interplay of human activity and environmental sustainability},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complexity, artificial life, and artificial intelligence. <em>ALJ</em>, <em>31</em>(3), 289-303. (<a href='https://doi.org/10.1162/artl_a_00462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scientific fields of complexity, Artificial Life (ALife), and artificial intelligence (AI) share commonalities: historic, conceptual, methodological, and philosophical. Although their origins trace back to the 1940s birth of cybernetics, they were able to develop properly only as modern information technology became available. In this perspective, I offer a personal (and thus biased) account of the expectations and limitations of these fields, some of which have their roots in the limits of formal systems. I use interactions, self-organization, emergence, and balance to compare different aspects of complexity, ALife, and AI. Even when the trajectory of the article is influenced by my personal experience, the general questions posed (which outweigh the answers) will, I hope, be useful in aligning efforts in these fields toward overcoming—or accepting—their limits.},
  archive      = {J_ALJ},
  author       = {Gershenson, Carlos},
  doi          = {10.1162/artl_a_00462},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {289-303},
  shortjournal = {Artif. Life},
  title        = {Complexity, artificial life, and artificial intelligence},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolvability in artificial development of large, complex structures and the principle of terminal addition. <em>ALJ</em>, <em>31</em>(3), 276-288. (<a href='https://doi.org/10.1162/artl_a_00460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epigenetic tracking (ET) is a model of development that is capable of generating diverse, arbitrary, complex three-dimensional cellular structures starting from a single cell. The generated structures have a level of complexity (in terms of the number of cells) comparable to multicellular biological organisms. In this article, we investigate the evolvability of the development of a complex structure inspired by the “French flag” problem: an “Italian Anubis” (a three-dimensional, doglike figure patterned in three colors). Genes during development are triggered in ET at specific developmental stages, and the fitness of individuals during simulated evolution is calculated after a certain stage. When this evaluation stage was allowed to evolve, genes that were triggered at later stages of development tended to be incorporated into the genome later during evolutionary runs. This suggests the emergence of the property of terminal addition in this system. When the principle of terminal addition was explicitly incorporated into ET, and was the sole mechanism for introducing morphological innovation, evolvability improved markedly, leading to the development of structures much more closely approximating the target at a much lower computational cost.},
  archive      = {J_ALJ},
  author       = {Fontana, Alessandro and Wróbel, Borys},
  doi          = {10.1162/artl_a_00460},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {276-288},
  shortjournal = {Artif. Life},
  title        = {Evolvability in artificial development of large, complex structures and the principle of terminal addition},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous evolution in the NK treadmill model. <em>ALJ</em>, <em>31</em>(3), 256-275. (<a href='https://doi.org/10.1162/artl_a_00467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The NK fitness landscape is a well-known model with which to study evolutionary dynamics in landscapes of different ruggedness. However, the model is static, and genomes are typically small, allowing observations over only a short adaptive period. Here we introduce an extension to the model that allows the experimenter to set the velocity at which the landscape changes independently from other parameters, such as the ruggedness or the mutation rate. We find that, similar to the previously observed complexity catastrophe, where evolution comes to a halt when environments become too complex due to overly high degrees of epistasis, here the same phenomenon occurs when changes happen too rapidly. Our expanded model also preserves essential properties of the static NK landscape, allowing for proper comparisons between static and dynamic landscapes.},
  archive      = {J_ALJ},
  author       = {Mehra, Priyanka and Hintze, Arend},
  doi          = {10.1162/artl_a_00467},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {256-275},
  shortjournal = {Artif. Life},
  title        = {Continuous evolution in the NK treadmill model},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neurons as autoencoders. <em>ALJ</em>, <em>31</em>(3), 250-255. (<a href='https://doi.org/10.1162/artl_c_00461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter presents the idea that neural backpropagation is exploiting dendritic processing to enable individual neurons to perform autoencoding. Using a very simple connection weight search heuristic and artificial neural network model, the effects of interleaving autoencoding for each neuron in a hidden layer of a feedforward network are explored. This is contrasted with the equivalent standard layered approach to autoencoding. It is shown that such individualized processing is not detrimental and can improve network learning.},
  archive      = {J_ALJ},
  author       = {Bull, Larry},
  doi          = {10.1162/artl_c_00461},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {250-255},
  shortjournal = {Artif. Life},
  title        = {Neurons as autoencoders},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A word from the editors. <em>ALJ</em>, <em>31</em>(3), 249. (<a href='https://doi.org/10.1162/ARTL.e.11'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue of contributed articles, we are pleased to share with you a diversity of ALife research.In his letter “Neurons as Autoencoders,” Bull brings together ideas from backpropagation, dendritic computing, the NK model, and autoencoders to augment standard models of neuronal autoencoding. The NK model reappears in the article “Continuous Evolution in the NK Treadmill Model,” in which Mehra and Hintze extend the originally static model to allow it to change in a parameterized manner and investigate how evolution behaves at different rates of change. In “Evolvability in Artificial Development of Large Complex Structures and the Principle of Terminal Addition,” Fontana and Wróbel examine evo-devo processes and the emergence of modification to the later stages of a developmental process.In a historical and philosophical perspective, Gershenson offers a personal account that compares various aspects of, and raises questions about, the closely related topics of “Complexity, Artificial Life, and Artificial Intelligence.”Games are an interesting subtopic in ALife, useful both for understanding complex systems and for outreach. In “Benefit Game 2.0: Alien Seaweed Swarms—Exploring the Interplay of Human Activity and Environmental Sustainability,” Fei, Wu, and Zhang present their immersive seaweed ecosystem simulation game that responds not only to audience input but also to the real-world environmental state. Moving from plant to animal behavior, in “Behaviour Diversity in a Walking and Climbing Centipede-Like Virtual Creature,” Norstein et al. take inspiration from arthropod multilegged locomotion to help develop robotic controllers robust to morphological and environmental variation.In their methodological article “Cognitive Distinctions as a Language for Cognitive Science: Comparing Methods of Description in a Model of Referential Communication,” Gaul and Izquierdo examine the effect of choices of language and terminology on theoretical frameworks in cognitive science and use a model to expose the consequences. In another methodological work, “Automating the Search for Artificial Life With Foundation Models,” Kumar et al. exploit concepts of artificial intelligence’s foundation models to develop a new approach to searching for lifelike behaviors, applied to a range of ALife simulations.We thank all the authors for the time and care they took to perform their research and present their work. We hope you enjoy the results!},
  archive      = {J_ALJ},
  author       = {Dorin, Alan and Stepney, Susan},
  doi          = {10.1162/ARTL.e.11},
  journal      = {Artificial Life},
  month        = {9},
  number       = {3},
  pages        = {249},
  shortjournal = {Artif. Life},
  title        = {A word from the editors},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="coli">COLI - 9</h2>
<ul>
<li><details>
<summary>
(2025). Natural language processing RELIES on linguistics. <em>COLI</em>, <em>51</em>(3), 1009-1032. (<a href='https://doi.org/10.1162/coli_a_00560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES , which encapsulates six major facets where linguistics contributes to NLP: R esources, E valuation, L ow-resource settings, I nterpretability, E xplanation, and the S tudy of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-à-vis systems of human language.},
  archive      = {J_COLI},
  author       = {Opitz, Juri and Wein, Shira and Schneider, Nathan},
  doi          = {10.1162/coli_a_00560},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1009-1032},
  shortjournal = {Comput. Lingu.},
  title        = {Natural language processing RELIES on linguistics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated essay scoring. <em>COLI</em>, <em>51</em>(3), 1005-1008. (<a href='https://doi.org/10.1162/coli_r_00513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated essay scoring is concerned with the development of language technologies that make it possible for an essay—or any piece of writing for that matter—to be evaluated or scored by a computer. These technologies find their utility primarily in the context of educational measurement, where they serve a dual purpose. On the one hand, they provide crucial support to educators and institutions, facilitating the assessment of students’ writing skills and content knowledge. A good example is the TOEFL iBT®, the Internet-based Test of English as a Foreign Language, administered by the Educational Testing Service and widely adopted by institutions worldwide. On the other hand, these technologies benefit writers themselves, including students, by offering a platform to assess and enhance their writing skills. One illustrative tool for this purpose is the Write &amp; Improve software developed at the University of Cambridge.The field of automated essay scoring emerged in the pioneering era of artificial intelligence and computational linguistics, with the inception of Ellis Page’s Project Essay Grade system at the University of Connecticut in 1964, and the subsequent publication of his seminal article “The Imminence of…Grading Essays by Computer” in 1966. Page’s automated scoring system is seen by the authors of this book as one of the first concrete applications of natural language processing, after the Audrey system for speech recognition and the Georgetown–IBM demonstration of machine translation. But just like speech recognition and machine translation, the industrialization of automated essay scoring mostly gained momentum in the 1990s. This decade saw an increase in richly annotated language data and corpora, which enabled the use of statistical and supervised machine learning in developing essay-scoring systems. Pearson’s Intelligent Essay Assessor™, released in 1998, is a prominent example of this evolution. A year later, in 1999, the e-rater® scoring engine was launched by the Educational Testing Service, commonly abbreviated as ETS, a private organization overseeing several standardized tests and high-stakes examinations such as TOEFL® and GRE®.This book is written by Beata Beigman Klebanov and Nitin Madnani, two researchers at ETS with many years of research experience and numerous peer-reviewed publications in the field of automated essay scoring. Their book provides a concise yet indispensable introduction to the field. After this short introduction, the eager reader is invited to take a deep dive into the scientific literature, encompassing slightly over half of the book’s content, approximately 115 pages. The literature review primarily caters to computational linguists and NLP practitioners, as it delves comprehensively into diverse machine learning models—ranging from linear regression to artificial neural networks—and intricate linguistic features. These features include general indicators of writing quality, such as text organization, coherence, and grammaticality, as well as genre-specific features, such as argument structure in argumentative essays. Furthermore, the reader not only gains insight into the extensive research carried out at ETS throughout the years but also delves into their technical expertise through a series of guided experiments with RSMTool—an open-source tool developed by the second author and colleagues. In addition, the authors also provide insight into a scalable and production-ready computer architecture used to build ETS products such as c-rater™, e-rater®, Language Muse®, and Writing Mentor™.The book is divided into five parts counting 13 chapters in total. The first part contains an introductory chapter in which the authors introduce the reader to Page’s aforementioned seminal 1966 article. Chapter 1 enumerates several arguments made by Page in favor of automated essay scoring, including its educational need, computational feasibility, high quality, and low cost. The chapter then sets forth four challenges associated with automated essay scoring. Two among them are the evaluation of original and source-based writing. Original writing, which reflects an author’s unique voice and thus stands out from existing and conventional works, poses a challenge because its originality could be overlooked or even under-evaluated by a computer. Source-based writing, a form of writing that reviews main points from external sources, presents a different problem because the assessment focuses on the correctness of content rather than measuring writing skills and essay quality. Another challenge is avoiding potential gaming strategies that test takers may employ to inflate their scores. A final challenge is automated feedback, as the effectiveness of providing linguistic and stylistic commentary on written work is not always proven.The second part contains two chapters covering a series of guided experiments and a set of best practices when building an automated essay scoring system. Chapter 2 provides a step-by-step guide for building an automated scoring system with supervised machine learning. First, the reader learns the usual engineering setup, including the use of a standard dataset (viz., the Automated Student Assessment Prize competition), an interpretable machine learning model (viz., linear regression), and a set of basic features derived from a scoring rubric. The authors also introduce the reader to their RSMTool. Then, the reader is guided through a series of ten experiments illustrating the incremental development (experiments 1–5) and evaluation (experiments 6–10) of the machine learning model. In each experiment, the reader is taught an important lesson such as feature fairness. For each experiment, the authors refer the reader to a report (i.e., correction key) available online.Chapter 3 provides some best practices for building an automated essay scoring system. First, the authors identify potentially conflicting perspectives between NLP developers and other stakeholders, including end users, subject matter experts, and business units. The authors then describe three natural use cases for automatic essay scoring, where it is added to a pre-existing assessment, developed concurrently with a new assessment, or implemented in a classroom. The authors provide some practical considerations and concrete actions that are well thought out and will appeal to many different readers.The third part contains five chapters that provide an up-to-date overview of the scientific literature and a more detailed account of the concepts introduced in the previous chapters. Chapter 4 describes various statistical models, including linear regression, latent semantic analysis, support vector machines, random forests, ensemble methods, and neural networks. For each of these models, the authors describe their mathematical underpinnings and explain how they can be used to score an essay. The chapter pays special attention to recent deep-learning architectures for automated essay scoring.Chapters 5 and 6 describe computational features that capture various aspects of the writing construct and the scoring rubric. Chapter 5 deals with general features. The features are organized into three classes: discourse features aiming to capture essay organization, development, and coherence; content features related to vocabulary use and topicality; and conventional features based on grammatical error detection. Chapter 6 then gives an in-depth overview of computational features that pertain to specific writing genres. The chapter is focused on four genres: argumentative writing, narrative writing, source-based writing, and reflective writing. Argumentative writing involves defending a particular position on a given topic (e.g., why technology should be integrated into education), presenting several claims as to why this position is valid, and supporting these claims with premises and evidence. Narrative writing involves telling a story that describes, for example, the historical evolution of technology in education. Source-based writing involves summarizing and comparing key points from external sources to compose an informed essay, which, for example, reviews the effectiveness of integrating technology into education based on scholarly sources. Finally, reflective writing involves examining personal experiences, such as teachers describing their experiences integrating technology into the classroom and reflecting on important lessons learned from this experience. This detailed overview of different writing genres also interestingly introduces the reader to research from related fields, such as computational argumentation and text summarization.Chapters 7 and 8 address two concerns in setting up real-world applications of automated essay scoring. Chapter 7 deals with the issues of reliability, scalability, and flexibility when deploying a scoring system at a large scale. The chapter describes and illustrates an Apache Storm architecture implemented in several ETS systems. Chapter 8 is about evaluating construct validity and fairness. When deploying an essay scoring system for high-stakes testing, fundamental issues arise when the system fails to measure the construct, assigns scores influenced by factors irrelevant to the measured construct, or is biased towards specific personal characteristics in the population of test takers. If an essay scoring system overlooks important features or favors particular writing styles or cultural representations, it undermines the validity and fairness of high-stakes assessments.The fourth part contains four chapters examining some broader challenges introduced in the first chapter and which remain to be solved for automated scoring. Chapter 9 deals with the automated generation of useful feedback on writing. The authors review several existing feedback systems and discuss how to define and evaluate the usefulness of feedback. Chapter 10 focuses on evaluating essay content, which is separate from assessing essay quality. Content scoring emphasizes measuring test-takers’ content knowledge, prioritizing these elements over writing skills. This evaluation can adopt either a reference-based or a response-based approach. Reference-based scoring involves comparing responses to a set of predefined reference answers, while response-based scoring independently assesses the response content. The chapter primarily explores the latter, investigating computational features and models tailored for this approach. Chapter 11 deals with another task related to but different from essay scoring, namely the automated scoring of spontaneous speech. After a brief account of the challenges with automated speech recognition, the authors review three sets of features for speech scoring: the delivery and fluency of spontaneous speech, vocabulary and grammar use, and topic development. The authors also contrast features relevant for scoring speech with those relevant for scoring writing. Lastly, chapter 12 examines several gaming strategies test-takers could use to fool the automated scoring system into giving a higher score. The authors review four types of strategies: the unnecessary use of shell language, the artificial generation of essays, the submission of off-topic responses, and the use of canned responses or plagiarized essays.The fifth and final part of the book contains a concluding chapter. The authors revisit the desiderata put forth by Ellis Page in his 1966 publication and summarize the overall achievements and remaining challenges in this respect. In addition, the authors discuss other challenging aspects that Page did not envision, such as the present-day ubiquity of technology, dealing with multiple languages, and setting up high-stakes tests that are valid, defensible, and fair.In sum, the book offers an excellent introduction to and deepening of the field of automated essay scoring. The book is well-structured and easy to read. Throughout the book, the authors provide thoughtful insights and practical advice based on their many years of experience at ETS. Compared to other books on the subject, the book offers a valuable combination of practical lessons and scientific deepening. By the end of the book, the reader has acquired a broad knowledge of the possibilities, challenges, and practical concerns involved with the automated scoring of student writing.},
  archive      = {J_COLI},
  author       = {Tack, Anaïs},
  doi          = {10.1162/coli_r_00513},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1005-1008},
  shortjournal = {Comput. Lingu.},
  title        = {Automated essay scoring},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey of cultural awareness in language models: Text and beyond. <em>COLI</em>, <em>51</em>(3), 907-1004. (<a href='https://doi.org/10.1162/COLI.a.14'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive, going beyond multilinguality and building on findings from psychology and anthropology. In this article, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking definitions of culture from the anthropology and psychology literature as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of human–computer interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature. 1},
  archive      = {J_COLI},
  author       = {Pawar, Siddhesh and Park, Junyeong and Jin, Jiho and Arora, Arnav and Myung, Junho and Yadav, Srishti and Haznitrama, Faiz Ghifari and Song, Inhwa and Oh, Alice and Augenstein, Isabelle},
  doi          = {10.1162/COLI.a.14},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {907-1004},
  shortjournal = {Comput. Lingu.},
  title        = {Survey of cultural awareness in language models: Text and beyond},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models are biased because they are large language models. <em>COLI</em>, <em>51</em>(3), 885-906. (<a href='https://doi.org/10.1162/coli_a_00558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This position paper’s primary goal is to provoke thoughtful discussion about the relationship between bias and fundamental properties of large language models (LLMs). I do this by seeking to convince the reader that harmful biases are an inevitable consequence arising from the design of any large language model as LLMs are currently formulated. To the extent that this is true, it suggests that the problem of harmful bias cannot be properly addressed without a serious reconsideration of AI driven by LLMs, going back to the foundational assumptions underlying their design.},
  archive      = {J_COLI},
  author       = {Resnik, Philip},
  doi          = {10.1162/coli_a_00558},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {885-906},
  shortjournal = {Comput. Lingu.},
  title        = {Large language models are biased because they are large language models},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics. <em>COLI</em>, <em>51</em>(3), 843-883. (<a href='https://doi.org/10.1162/coli_a_00543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate two essential challenges in the context of hierarchical topic modeling (HTM)—(i) the impact of data representation and (ii) topic evaluation. The data representation directly influences the performance of the topic generation, and the impact of new representations such as contextual embeddings in this task has been under-investigated. Topic evaluation , responsible for driving the advances in the field, assesses the overall quality of the topic generation process. HTM studies exploit the exact topic modeling (TM) evaluation metrics as traditional TM to measure the quality of topics. One significant result of our work is demonstrating that the HTM’s hierarchical nature demands novel ways of evaluating the quality of topics. As our main contribution, we propose two new topic quality metrics to assess the topical quality of the hierarchical structures. Uniqueness considers topic topological consistency, while the Semantic Hierarchical Structure (SHS) captures the semantic relatedness of the hierarchies. We also present an additional advance to the state-of-the-art by proposing the c-CluHTM. To the best of our knowledge, c-CluHTM is the first method that exploits contextual embeddings into NMF in HTM tasks. c-CluHTM enhances the topics’ semantics while preserving the hierarchical structure. We perform an experimental evaluation, and our results demonstrate the superiority of our proposal with gains between 12% and 21%, regarding NPMI and Coherence over the best baselines. Regarding the newly proposed metrics, our results reveal that Uniqueness and SHS can capture relevant information about the structure of the hierarchical topics that traditional metrics cannot.},
  archive      = {J_COLI},
  author       = {Viegas, Felipe and Pereira, Antonio and Cunha, Washington and França, Celso and Andrade, Claudio and Tuler, Elisa and Rocha, Leonardo and Gonçalves, Marcos André},
  doi          = {10.1162/coli_a_00543},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {843-883},
  shortjournal = {Comput. Lingu.},
  title        = {Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The emergence of chunking structures with hierarchical RNN. <em>COLI</em>, <em>51</em>(3), 815-841. (<a href='https://doi.org/10.1162/coli_a_00545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Natural Language Processing (NLP), predicting linguistic structures, such as parsing and chunking, has mostly relied on manual annotations of syntactic structures. This article introduces an unsupervised approach to chunking, a syntactic task that involves grouping words in a non-hierarchical manner. We present a Hierarchical Recurrent Neural Network (HRNN) designed to model word-to-chunk and chunk-to-sentence compositions. Our approach involves a two-stage training process: pretraining with an unsupervised parser and finetuning on downstream NLP tasks. Experiments on multiple datasets reveal a notable improvement of unsupervised chunking performance in both pretraining and finetuning stages. Interestingly, we observe that the emergence of the chunking structure is transient during the neural model’s downstream-task training. This study contributes to the advancement of unsupervised syntactic structure discovery and opens avenues for further research in linguistic theory. 1},
  archive      = {J_COLI},
  author       = {Wu, Zijun and Deshmukh, Anup Anand and Wu, Yongkang and Lin, Jimmy and Mou, Lili},
  doi          = {10.1162/coli_a_00545},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {815-841},
  shortjournal = {Comput. Lingu.},
  title        = {The emergence of chunking structures with hierarchical RNN},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tokenization changes meaning in large language models: Evidence from chinese. <em>COLI</em>, <em>51</em>(3), 785-814. (<a href='https://doi.org/10.1162/coli_a_00557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models segment many words into multiple tokens, and there is mixed evidence as to whether tokenization affects how state-of-the-art models represent meanings. Chinese characters present an opportunity to investigate this issue: They contain semantic radicals, which often convey useful information; characters with the same semantic radical tend to begin with the same one or two bytes (when using UTF-8 encodings); and tokens are common strings of bytes, so characters with the same radical often begin with the same token. This study asked GPT-4, GPT-4o, and Llama 3 whether characters contain the same semantic radical, elicited semantic similarity ratings, and conducted odd-one-out tasks (i.e., which character is not like the others). In all cases, misalignment between tokens and radicals systematically corrupted representations of Chinese characters. In experiments comparing characters represented by single tokens to multi-token characters, the models were less accurate for single-token characters, which suggests that segmenting words into fewer, longer tokens obscures valuable information in word form and will not resolve the problems introduced by tokenization. In experiments with 12 European languages, misalignment between tokens and suffixes systematically corrupted categorization of words by all three models, which suggests that the tendency to treat malformed tokens like linguistic units is pervasive.},
  archive      = {J_COLI},
  author       = {Haslett, David A.},
  doi          = {10.1162/coli_a_00557},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {785-814},
  shortjournal = {Comput. Lingu.},
  title        = {Tokenization changes meaning in large language models: Evidence from chinese},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniASA: A unified generative framework for argument structure analysis. <em>COLI</em>, <em>51</em>(3), 739-784. (<a href='https://doi.org/10.1162/coli_a_00553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation is a fundamental human activity that involves reasoning and persuasion, which also serves as the basis for the development of AI systems capable of complex reasoning. In NLP, to better understand human argumentation, argument structure analysis aims to identify argument components, such as claims and premises, and their relations from free text. It encompasses a variety of divergent tasks, such as end-to-end argument mining, argument pair extraction, and argument quadruplet extraction. Existing methods are usually tailored to only one specific argument structure analysis task, overlooking the inherent connections among different tasks. We observe that the fundamental goal of these tasks is similar: identifying argument components and their interrelations. Motivated by this, we present a unified generative framework for argument structure analysis (UniASA). It can uniformly address multiple argument structure analysis tasks in a sequence-to-sequence manner. Further, we enhance UniASA with a multi-view learning strategy based on subtask decomposition. We conduct experiments on seven datasets across three tasks. The results indicate that UniASA can address these tasks uniformly and achieve performance that is either superior to or comparable with the previous state-of-the-art methods. Also, we show that UniASA can be effectively integrated with large language models, such as Llama, through fine-tuning or in-context learning.},
  archive      = {J_COLI},
  author       = {Bao, Jianzhu and Jing, Mohan and Dong, Kuicai and Sun, Aixin and Sun, Yang and Xu, Ruifeng},
  doi          = {10.1162/coli_a_00553},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {739-784},
  shortjournal = {Comput. Lingu.},
  title        = {UniASA: A unified generative framework for argument structure analysis},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graded suspiciousness of adversarial texts to humans. <em>COLI</em>, <em>51</em>(3), 705-738. (<a href='https://doi.org/10.1162/coli_a_00555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples pose a significant challenge to deep neural networks across both image and text domains, with the intent to degrade model performance through carefully altered inputs. Adversarial texts, however, are distinct from adversarial images due to their requirement for semantic similarity and the discrete nature of the textual contents. This study delves into the concept of human suspiciousness, a quality distinct from the traditional focus on imperceptibility found in image-based adversarial examples, where adversarial changes are often desired to be indistinguishable to the human eye even when placed side by side with originals. Although this is generally not possible with text, textual adversarial content must still often remain undetected or non-suspicious to human readers. Even when the text’s purpose is to deceive NLP systems or bypass filters, the text is often expected to be natural to read. In this research, we expand the study of human suspiciousness by analyzing how individuals perceive adversarial texts. We gather and publish a novel dataset of Likert-scale human evaluations on the suspiciousness of adversarial sentences, crafted by four widely used adversarial attack methods and assess their correlation with the human ability to detect machine-generated alterations. Additionally, we develop a regression-based model to predict levels of suspiciousness and establish a baseline for future research in reducing the suspiciousness in adversarial text generation. We also demonstrate how the regressor-generated suspicious scores can be incorporated into adversarial generation methods to produce texts that are less likely to be perceived as computer-generated.},
  archive      = {J_COLI},
  author       = {Tonni, Shakila Mahjabin and Faustini, Pedro and Dras, Mark},
  doi          = {10.1162/coli_a_00555},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {705-738},
  shortjournal = {Comput. Lingu.},
  title        = {Graded suspiciousness of adversarial texts to humans},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="ecj">ECJ - 5</h2>
<ul>
<li><details>
<summary>
(2025). On the use of the doubly stochastic matrix models for the quadratic assignment problem. <em>ECJ</em>, <em>33</em>(3), 425-457. (<a href='https://doi.org/10.1162/evco_a_00369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Permutation problems have captured the attention of the combinatorial optimization community for decades due to the challenge they pose. Although their solutions are naturally encoded as permutations, in each problem, the information to be used to optimize them can vary substantially. In this paper, we consider the Quadratic Assignment Problem (QAP) as a case study, and propose using Doubly Stochastic Matrices (DSMs) under the framework of Estimation of Distribution Algorithms. To that end, we design efficient learning and sampling schemes that enable an effective iterative update of the probability model. Conducted experiments on commonly adopted benchmarks for the QAP prove doubly stochastic matrices to be preferred to the other four models for permutations, both in terms of effectiveness and computational efficiency. Moreover, additional analyses performed on the structure of the QAP and the Linear Ordering Problem (LOP) show that DSMs are good to deal with assignment problems, but they have interesting capabilities to deal also with ordering problems such as the LOP. The paper concludes with a description of the potential uses of DSMs for other optimization paradigms, such as genetic algorithms or model-based gradient search.},
  archive      = {J_ECJ},
  author       = {Santucci, Valentino and Ceberio, Josu},
  doi          = {10.1162/evco_a_00369},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {425-457},
  shortjournal = {Evol. Comput.},
  title        = {On the use of the doubly stochastic matrix models for the quadratic assignment problem},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P-NP instance decomposition based on the fourier transform for solving the linear ordering problem. <em>ECJ</em>, <em>33</em>(3), 395-423. (<a href='https://doi.org/10.1162/evco_a_00368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fourier transform over finite groups has proved to be a useful tool for analyzing combinatorial optimization problems. However, few heuristic and metaheuristic algorithms have been proposed in the literature that utilize the information provided by this technique to guide the search process. In this work, we attempt to address this research gap by considering the case study of the Linear Ordering Problem (LOP). Based on the Fourier transform, we propose an instance decomposition strategy that divides any LOP instance into the sum of two LOP instances associated with a P and an NP-Hard optimization problem. By linearly aggregating the instances obtained from the decomposition, it is possible to create artificial instances with modified proportions of the P and NP-Hard components. Conducted experiments show that increasing the weight of the P component leads to a less rugged fitness landscape suitable for local search-based optimization. We take advantage of this phenomenon by presenting a new metaheuristic algorithm called P-Descent Search (PDS). The proposed method, first, optimizes a surrogate instance with a high proportion of the P component, and then, gradually increases the weight of the NP-Hard component until the original instance is reached. The multi-start version of PDS shows a promising and predictable performance that appears to be correlated to specific characteristics of the problem, which could open the door to an automatic tuning of its hyperparameters.},
  archive      = {J_ECJ},
  author       = {Benavides, Xabier and Hernando, Leticia and Ceberio, Josu and Lozano, Jose A.},
  doi          = {10.1162/evco_a_00368},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {395-423},
  shortjournal = {Evol. Comput.},
  title        = {P-NP instance decomposition based on the fourier transform for solving the linear ordering problem},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing monotone chance-constrained submodular functions using evolutionary multiobjective algorithms. <em>ECJ</em>, <em>33</em>(3), 363-393. (<a href='https://doi.org/10.1162/evco_a_00360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world optimization problems can be stated in terms of submodular functions. Furthermore, these real-world problems often involve uncertainties which may lead to the violation of given constraints. A lot of evolutionary multiobjective algorithms following the Pareto optimization approach have recently been analyzed and applied to submodular problems with different types of constraints. We present a first runtime analysis of evolutionary multiobjective algorithms based on Pareto optimization for chance-constrained submodular functions. Here the constraint involves stochastic components and the constraint can only be violated with a small probability of α ⁠ . We investigate the classical GSEMO algorithm for two different bi-objective formulations using tail bounds to determine the feasibility of solutions. We show that the algorithm GSEMO obtains the same worst case performance guarantees for monotone submodular functions as recently analyzed greedy algorithms for the case of uniform IID weights and uniformly distributed weights with the same dispersion when using the appropriate bi-objective formulation. As part of our investigations, we also point out situations where the use of tail bounds in the first bi-objective formulation can prevent GSEMO from obtaining good solutions in the case of uniformly distributed weights with the same dispersion if the objective function is submodular but non-monotone due to a single element impacting monotonicity. Furthermore, we investigate the behavior of the evolutionary multiobjective algorithms GSEMO, NSGA-II, and SPEA2 on different submodular chance-constrained network problems. Our experimental results show that the use of evolutionary multiobjective algorithms leads to significant performance improvements compared to state-of-the-art greedy algorithms for submodular optimization.},
  archive      = {J_ECJ},
  author       = {Neumann, Aneta and Neumann, Frank},
  doi          = {10.1162/evco_a_00360},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {363-393},
  shortjournal = {Evol. Comput.},
  title        = {Optimizing monotone chance-constrained submodular functions using evolutionary multiobjective algorithms},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Genetic programming for automatically evolving multiple features to classification. <em>ECJ</em>, <em>33</em>(3), 335-362. (<a href='https://doi.org/10.1162/evco_a_00359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing classification on high-dimensional data poses a significant challenge due to the huge search space. Moreover, complex feature interactions introduce an additional obstacle. The problems can be addressed by using feature selection to select relevant features or feature construction to construct a small set of high-level features. However, performing feature selection or feature construction might only make the feature set suboptimal. To remedy this problem, this study investigates the use of genetic programming for simultaneous feature selection and feature construction in addressing different classification tasks. The proposed approach is tested on 16 datasets and compared with seven methods including both feature selection and feature construction techniques. The results show that the obtained feature sets with the constructed and/or selected features can significantly increase the classification accuracy and reduce the dimensionality of the datasets. Further analysis reveals the complementarity of the obtained features leading to the promising classification performance of the proposed method.},
  archive      = {J_ECJ},
  author       = {Wang, Peng and Xue, Bing and Liang, Jing and Zhang, Mengjie},
  doi          = {10.1162/evco_a_00359},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {335-362},
  shortjournal = {Evol. Comput.},
  title        = {Genetic programming for automatically evolving multiple features to classification},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogates of scalarization functions. <em>ECJ</em>, <em>33</em>(3), 309-334. (<a href='https://doi.org/10.1162/evco_a_00354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, computationally intensive multiobjective optimization problems have been efficiently solved by surrogate-assisted multiobjective evolutionary algorithms. However, most of those algorithms can handle no more than 200 decision variables. As the number of decision variables increases further, unreliable surrogate models will result in a dramatic deterioration of their performance, which makes large-scale expensive multiobjective optimization challenging. To address this challenge, we develop a large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogate models of scalarization functions. The proposed algorithm (termed LDS-AF) reduces the dimension of the original decision space based on principal component analysis, and then directly approximates the scalarization functions in a decomposition-based multiobjective evolutionary algorithm. With the help of a two-stage modeling strategy and convergence control strategy, LDS-AF can keep a good balance between convergence and diversity, and achieve a promising performance without being trapped in a local optimum prematurely. The experimental results on a set of test instances have demonstrated its superiority over eight state-of-the-art algorithms on multiobjective optimization problems with up to 1,000 decision variables using only 500 real function evaluations.},
  archive      = {J_ECJ},
  author       = {Gu, Haoran and Wang, Handing and He, Cheng and Yuan, Bo and Jin, Yaochu},
  doi          = {10.1162/evco_a_00354},
  journal      = {Evolutionary Computation},
  month        = {9},
  number       = {3},
  pages        = {309-334},
  shortjournal = {Evol. Comput.},
  title        = {Large-scale multiobjective evolutionary algorithm guided by low-dimensional surrogates of scalarization functions},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jmlr">JMLR - 194</h2>
<ul>
<li><details>
<summary>
(2025). Linear separation capacity of self-supervised representation learning. <em>JMLR</em>, <em>26</em>(194), 1-48. (<a href='https://jmlr.org/papers/v26/24-2032.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. Our theoretical analysis further underscores that the performance of downstream linear classifiers primarily hinges on the linear separability of data representations rather than the size of the labeled data set, reaffirming the viability of constructing efficient classifiers with limited labeled data amid an expansive unlabeled data set.},
  archive      = {J_JMLR},
  author       = {Shulei Wang},
  journal      = {Journal of Machine Learning Research},
  number       = {194},
  pages        = {1-48},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Linear separation capacity of self-supervised representation learning},
  url          = {https://jmlr.org/papers/v26/24-2032.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convergence of projected policy gradient for any constant step sizes. <em>JMLR</em>, <em>26</em>(193), 1-35. (<a href='https://jmlr.org/papers/v26/24-1530.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projected policy gradient (PPG) is a basic policy optimization method in reinforcement learning. Given access to exact policy evaluations, previous studies have established the sublinear convergence of PPG for sufficiently small step sizes based on the smoothness and the gradient domination properties of the value function. However, as the step size goes to infinity, PPG reduces to the classic policy iteration method, which suggests the convergence of PPG even for large step sizes. In this paper, we fill this gap and show that PPG admits a sublinear convergence for any constant step sizes. Due to the existence of the state-wise visitation measure in the expression of policy gradient, the existing optimization-based analysis framework for a preconditioned version of PPG (i.e., projected Q-ascent) is not applicable, to the best of our knowledge. Instead, we proceed the proof by computing the state-wise improvement lower bound of PPG based on its inherent structure. In addition, the finite iteration convergence of PPG for any constant step size is further established, which is also new.},
  archive      = {J_JMLR},
  author       = {Jiacai Liu and Wenye Li and Dachao Lin and Ke Wei and Zhihua Zhang},
  journal      = {Journal of Machine Learning Research},
  number       = {193},
  pages        = {1-35},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On the convergence of projected policy gradient for any constant step sizes},
  url          = {https://jmlr.org/papers/v26/24-1530.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with linear function approximations in mean-field control. <em>JMLR</em>, <em>26</em>(192), 1-53. (<a href='https://jmlr.org/papers/v26/24-1221.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper focuses on mean-field type multi-agent control problems with finite state and action spaces where the dynamics and cost structures are symmetric and homogeneous, and are affected by the distribution of the agents. A standard solution method for these problems is to consider the infinite population limit as an approximation and use symmetric solutions of the limit problem to achieve near optimality. The control policies, and in particular the dynamics, depend on the population distribution in the finite population setting, or the marginal distribution of the state variable of a representative agent for the infinite population setting. Hence, learning and planning for these control problems generally require estimating the reaction of the system to all possible state distributions of the agents. To overcome this issue, we consider linear function approximation for the control problem and provide coordinated and independent learning methods. We rigorously establish error upper bounds for the performance of learned solutions. The performance gap stems from (i) the mismatch due to estimating the true model with a linear one, and (ii) using the infinite population solution in the finite population problem as an approximate control. The provided upper bounds quantify the impact of these error sources on the overall performance.},
  archive      = {J_JMLR},
  author       = {Erhan Bayraktar and Ali Devran Kara},
  journal      = {Journal of Machine Learning Research},
  number       = {192},
  pages        = {1-53},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Learning with linear function approximations in mean-field control},
  url          = {https://jmlr.org/papers/v26/24-1221.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new random reshuffling method for nonsmooth nonconvex finite-sum optimization. <em>JMLR</em>, <em>26</em>(191), 1-46. (<a href='https://jmlr.org/papers/v26/24-0891.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random reshuffling techniques are prevalent in large-scale applications, such as training neural networks. While the convergence and acceleration effects of random reshuffling-type methods are fairly well understood in the smooth setting, much less studies seem available in the nonsmooth case. In this work, we design a new normal map-based proximal random reshuffling (norm-PRR) method for nonsmooth nonconvex finite-sum problems. We show that norm-PRR achieves the iteration complexity ${\cal O}(n^{-1/3}T^{-2/3})$ where $n$ denotes the number of component functions $f(\cdot,i)$ and $T$ counts the total number of iterations. This improves the currently known complexity bounds for this class of problems by a factor of $n^{-1/3}$ in terms of the number of gradient evaluations. Additionally, we prove that norm-PRR converges linearly under the (global) Polyak-Łojasiewicz condition and in the interpolation setting. We further complement these non-asymptotic results and provide an in-depth analysis of the asymptotic properties of norm-PRR. Specifically, under the (local) Kurdyka-Łojasiewicz inequality, the whole sequence of iterates generated by norm-PRR is shown to converge to a single stationary point. Moreover, we derive last-iterate convergence rates that can match those in the smooth, strongly convex setting. Finally, numerical experiments are performed on nonconvex classification tasks to illustrate the efficiency of the proposed approach.},
  archive      = {J_JMLR},
  author       = {Junwen Qiu and Xiao Li and Andre Milzarek},
  journal      = {Journal of Machine Learning Research},
  number       = {191},
  pages        = {1-46},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {A new random reshuffling method for nonsmooth nonconvex finite-sum optimization},
  url          = {https://jmlr.org/papers/v26/24-0891.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-free change-point detection using AUC of a classifier. <em>JMLR</em>, <em>26</em>(190), 1-50. (<a href='https://jmlr.org/papers/v26/24-0365.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary data analysis, it is increasingly common to work with non-stationary complex data sets. These data sets typically extend beyond the classical low-dimensional Euclidean space, making it challenging to detect shifts in their distribution without relying on strong structural assumptions. This paper proposes a novel offline change-point detection method that leverages classifiers developed in the statistics and machine learning community. With suitable data splitting, the test statistic is constructed through sequential computation of the Area Under the Curve (AUC) of a classifier, which is trained on data segments on both ends of the sequence. It is shown that the resulting AUC process attains its maxima at the true change-point location, which facilitates the change-point estimation. The proposed method is characterized by its complete nonparametric nature, high versatility, considerable flexibility, and absence of stringent assumptions on the underlying data or any distributional shifts. Theoretically, we derive the limiting pivotal distribution of the proposed test statistic under null, as well as the asymptotic behaviors under both local and fixed alternatives. The localization rate of the change-point estimator is also provided. Extensive simulation studies and the analysis of two real-world data sets illustrate the superior performance of our approach compared to existing model-free change-point detection methods.},
  archive      = {J_JMLR},
  author       = {Rohit Kanrar and Feiyu Jiang and Zhanrui Cai},
  journal      = {Journal of Machine Learning Research},
  number       = {190},
  pages        = {1-50},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Model-free change-point detection using AUC of a classifier},
  url          = {https://jmlr.org/papers/v26/24-0365.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EF21 with bells & whistles: Six algorithmic extensions of modern error feedback. <em>JMLR</em>, <em>26</em>(189), 1-50. (<a href='https://jmlr.org/papers/v26/24-0059.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {First proposed by Seide (2014) as a heuristic, error feedback (EF) is a very popular mechanism for enforcing convergence of distributed gradient-based optimization methods enhanced with communication compression strategies based on the application of contractive compression operators. However, existing theory of EF relies on very strong assumptions (e.g., bounded gradients), and provides pessimistic convergence rates (e.g., while the best known rate for EF in the smooth nonconvex regime, and when full gradients are compressed, is $O(1/T^{2/3})$, the rate of gradient descent in the same regime is $O(1/T)$). Recently, Richtàrik et al. (2021) proposed a new error feedback mechanism, EF21, based on the construction of a Markov compressor induced by a contractive compressor. EF21 removes the aforementioned theoretical deficiencies of EF and at the same time works better in practice. In this work we propose six practical extensions of EF21, all supported by strong convergence theory: partial participation, stochastic approximation, variance reduction, proximal setting, momentum, and bidirectional compression. To the best of our knowledge, several of these techniques have not been previously analyzed in combination with EF, and in cases where prior analysis exists---such as for bidirectional compression---our theoretical convergence guarantees significantly improve upon existing results.},
  archive      = {J_JMLR},
  author       = {Ilyas Fatkhullin and Igor Sokolov and Eduard Gorbunov and Zhize Li and Peter Richtárik},
  journal      = {Journal of Machine Learning Research},
  number       = {189},
  pages        = {1-50},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {EF21 with bells & whistles: Six algorithmic extensions of modern error feedback},
  url          = {https://jmlr.org/papers/v26/24-0059.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple instance verification. <em>JMLR</em>, <em>26</em>(188), 1-46. (<a href='https://jmlr.org/papers/v26/23-1590.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore multiple instance verification, a problem setting in which a query instance is verified against a bag of target instances with heterogeneous, unknown relevancy. We show that naive adaptations of attention-based multiple instance learning (MIL) methods and standard verification methods like Siamese neural networks are unsuitable for this setting: directly combining state-of-the-art (SOTA) MIL methods and Siamese networks is shown to be no better, and sometimes significantly worse, than a simple baseline model. Postulating that this may be caused by the failure of the representation of the target bag to incorporate the query instance, we introduce a new pooling approach named “cross-attention pooling” (CAP). Under the CAP framework, we propose two novel attention functions to address the challenge of distinguishing between highly similar instances in a target bag. Through empirical studies on three different verification tasks, we demonstrate that CAP outperforms adaptations of SOTA MIL methods and the baseline by substantial margins, in terms of both classification accuracy and the ability to detect key instances. The superior ability to identify key instances is attributed to the new attention functions by ablation studies.},
  archive      = {J_JMLR},
  author       = {Xin Xu and Eibe Frank and Geoffrey Holmes},
  journal      = {Journal of Machine Learning Research},
  number       = {188},
  pages        = {1-46},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Multiple instance verification},
  url          = {https://jmlr.org/papers/v26/23-1590.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from similar linear representations: Adaptivity, minimaxity, and robustness. <em>JMLR</em>, <em>26</em>(187), 1-125. (<a href='https://jmlr.org/papers/v26/23-0902.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation multi-task learning (MTL) has achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL almost always improves performance. Nevertheless, as the number of tasks grows, assuming all tasks share the same representation is unrealistic. Furthermore, empirical findings often indicate that a shared representation does not necessarily improve single-task learning performance. In this paper, we aim to understand how to learn from tasks with similar but not exactly the same linear representations, while dealing with outlier tasks. Assuming a known intrinsic dimension, we propose a penalized empirical risk minimization method and a spectral method that are adaptive to the similarity structure and robust to outlier tasks. Both algorithms outperform single-task learning when representations across tasks are sufficiently similar and the proportion of outlier tasks is small. Moreover, they always perform at least as well as single-task learning, even when the representations are dissimilar. We provide information-theoretic lower bounds to demonstrate that both methods are nearly minimax optimal in a large regime, with the spectral method being optimal in the absence of outlier tasks. Additionally, we introduce a thresholding algorithm to adapt to an unknown intrinsic dimension. We conduct extensive numerical experiments to validate our theoretical findings.},
  archive      = {J_JMLR},
  author       = {Ye Tian and Yuqi Gu and Yang Feng},
  journal      = {Journal of Machine Learning Research},
  number       = {187},
  pages        = {1-125},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Learning from similar linear representations: Adaptivity, minimaxity, and robustness},
  url          = {https://jmlr.org/papers/v26/23-0902.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exponential family graphical models: Correlated replicates and unmeasured confounders, with applications to fMRI data. <em>JMLR</em>, <em>26</em>(186), 1-66. (<a href='https://jmlr.org/papers/v26/22-1421.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical models have been used extensively for modeling brain connectivity networks. However, unmeasured confounders and correlations among measurements are often overlooked during model fitting, which may lead to spurious scientific discoveries. Motivated by functional magnetic resonance imaging (fMRI) studies, we propose a novel method for constructing brain connectivity networks with correlated replicates and latent effects. In a typical fMRI study, each participant is scanned and fMRI measurements are collected across a period of time. In many cases, subjects may have different states of mind that cannot be measured during the brain scan: for instance, some subjects may be awake during the first half of the brain scan, and may fall asleep during the second half of the brain scan. To model the correlation among replicates and latent effects induced by the different states of mind, we assume that the correlated replicates within each independent subject follow a one-lag vector autoregressive model, and that the latent effects induced by the unmeasured confounders are piecewise constant. Theoretical guarantees are established for parameter estimation. We demonstrate via extensive numerical studies that our method is able to estimate latent variable graphical models with correlated replicates more accurately than existing methods.},
  archive      = {J_JMLR},
  author       = {Yanxin Jin and Yang Ning and Kean Ming Tan},
  journal      = {Journal of Machine Learning Research},
  number       = {186},
  pages        = {1-66},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Exponential family graphical models: Correlated replicates and unmeasured confounders, with applications to fMRI data},
  url          = {https://jmlr.org/papers/v26/22-1421.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing return distributions with distributional dynamic programming. <em>JMLR</em>, <em>26</em>(185), 1-90. (<a href='https://jmlr.org/papers/v26/25-0210.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce distributional dynamic programming (DP) methods for optimizing statistical functionals of the return distribution, with standard reinforcement learning as a special case. Previous distributional DP methods could optimize the same class of expected utilities as classic DP. To go beyond, we combine distributional DP with stock augmentation, a technique previously introduced for classic DP in the context of risk-sensitive RL, where the MDP state is augmented with a statistic of the rewards obtained since the first time step. We find that a number of recently studied problems can be formulated as stock-augmented return distribution optimization, and we show that we can use distributional DP to solve them. We analyze distributional value and policy iteration, with bounds and a study of what objectives these distributional DP methods can or cannot optimize. We describe a number of applications outlining how to use distributional DP to solve different stock-augmented return distribution optimization problems, for example maximizing conditional value-at-risk, and homeostatic regulation. To highlight the practical potential of stock-augmented return distribution optimization and distributional DP, we introduce an agent that combines DQN and the core ideas of distributional DP, and empirically evaluate it for solving instances of the applications discussed.},
  archive      = {J_JMLR},
  author       = {Bernardo Ávila Pires and Mark Rowland and Diana Borsa and Zhaohan Daniel Guo and Khimya Khetarpal and André Barreto and David Abel and Rémi Munos and Will Dabney},
  journal      = {Journal of Machine Learning Research},
  number       = {185},
  pages        = {1-90},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Optimizing return distributions with distributional dynamic programming},
  url          = {https://jmlr.org/papers/v26/25-0210.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imprecise multi-armed bandits: Representing irreducible uncertainty as a zero-sum game. <em>JMLR</em>, <em>26</em>(184), 1-75. (<a href='https://jmlr.org/papers/v26/24-2001.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel multi-armed bandit framework, where each arm is associated with a fixed unknown credal set over the space of outcomes (which can be richer than just the reward). The arm-to-credal-set correspondence comes from a known class of hypotheses. We then define a notion of regret corresponding to the lower prevision defined by these credal sets. Equivalently, the setting can be regarded as a two-player zero-sum game, where, on each round, the agent chooses an arm and the adversary chooses the distribution over outcomes from a set of options associated with this arm. The regret is defined with respect to the value of game. For certain natural hypothesis classes, loosely analogous to stochastic linear bandits (which are a special case of the resulting setting), we propose an algorithm and prove a corresponding upper bound on regret.},
  archive      = {J_JMLR},
  author       = {Vanessa Kosoy},
  journal      = {Journal of Machine Learning Research},
  number       = {184},
  pages        = {1-75},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Imprecise multi-armed bandits: Representing irreducible uncertainty as a zero-sum game},
  url          = {https://jmlr.org/papers/v26/24-2001.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early alignment in two-layer networks training is a two-edged sword. <em>JMLR</em>, <em>26</em>(183), 1-75. (<a href='https://jmlr.org/papers/v26/24-1523.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018). For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and only converge to a spurious stationary point instead.},
  archive      = {J_JMLR},
  author       = {Etienne Boursier and Nicolas Flammarion},
  journal      = {Journal of Machine Learning Research},
  number       = {183},
  pages        = {1-75},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Early alignment in two-layer networks training is a two-edged sword},
  url          = {https://jmlr.org/papers/v26/24-1523.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical decision making based on structural information principles. <em>JMLR</em>, <em>26</em>(182), 1-55. (<a href='https://jmlr.org/papers/v26/24-1184.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical Reinforcement Learning (HRL) is a promising approach for managing task complexity across multiple levels of abstraction and accelerating long-horizon agent exploration. However, the effectiveness of hierarchical policies heavily depends on prior knowledge and manual assumptions about skill definitions and task decomposition. In this paper, we propose a novel Structural Information principles-based framework, namely SIDM, for hierarchical Decision Making in both single-agent and multi-agent scenarios. Central to our work is the utilization of structural information embedded in the decision-making process to adaptively and dynamically discover and learn hierarchical policies through environmental abstractions. Specifically, we present an abstraction mechanism that processes historical state-action trajectories to construct abstract representations of states and actions. We define and optimize directed structural entropy—a metric quantifying the uncertainty in transition dynamics between abstract states—to discover skills that capture key transition patterns in RL environments. Building on these findings, we develop a skill-based learning method for single-agent scenarios and a role-based collaboration method for multi-agent scenarios, both of which can flexibly integrate various underlying algorithms for enhanced performance. Extensive evaluations on challenging benchmarks demonstrate that our framework significantly and consistently outperforms state-of-the-art baselines, improving the effectiveness, efficiency, and stability of policy learning by up to 32.70%, 64.86%, and 88.26%, respectively, as measured by average rewards, convergence timesteps, and standard deviations.},
  archive      = {J_JMLR},
  author       = {Xianghua Zeng and Hao Peng and Dingli Su and Angsheng Li},
  journal      = {Journal of Machine Learning Research},
  number       = {182},
  pages        = {1-55},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Hierarchical decision making based on structural information principles},
  url          = {https://jmlr.org/papers/v26/24-1184.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative adversarial networks: Dynamics. <em>JMLR</em>, <em>26</em>(181), 1-30. (<a href='https://jmlr.org/papers/v26/24-0848.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study quantitatively the overparametrization limit of the original Wasserstein-GAN algorithm. Effectively, we show that the algorithm is a stochastic discretization of a system of continuity equations for the parameter distributions of the generator and discriminator. We show that parameter clipping to satisfy the Lipschitz condition in the algorithm induces a discontinuous vector field in the mean field dynamics, which gives rise to blow-up in finite time of the mean field dynamics. We look into a specific toy example that shows that all solutions to the mean field equations converge in the long time limit to time periodic solutions, this helps explain the failure to converge of the algorithm.},
  archive      = {J_JMLR},
  author       = {Matias G. Delgadino and Bruno B. Suassuna and Rene Cabrera},
  journal      = {Journal of Machine Learning Research},
  number       = {181},
  pages        = {1-30},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Generative adversarial networks: Dynamics},
  url          = {https://jmlr.org/papers/v26/24-0848.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “What is different between these datasets?” a framework for explaining data distribution shifts. <em>JMLR</em>, <em>26</em>(180), 1-64. (<a href='https://jmlr.org/papers/v26/24-0352.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of machine learning models relies heavily on the quality of input data, yet real-world applications often face significant data-related challenges. A common issue arises when curating training data or deploying models: two datasets from the same domain may exhibit differing distributions. While many techniques exist for detecting such distribution shifts, there is a lack of comprehensive methods to explain these differences in a human-understandable way beyond opaque quantitative metrics. To bridge this gap, we propose a versatile framework of interpretable methods for comparing datasets. Using a variety of case studies, we demonstrate the effectiveness of our approach across diverse data modalities—including tabular data, text data, images, time-series signals – in both low and high-dimensional settings. These methods complement existing techniques by providing actionable and interpretable insights to better understand and address distribution shifts.},
  archive      = {J_JMLR},
  author       = {Varun Babbar* and Zhicheng Guo* and Cynthia Rudin},
  journal      = {Journal of Machine Learning Research},
  number       = {180},
  pages        = {1-64},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {“What is different between these datasets?” a framework for explaining data distribution shifts},
  url          = {https://jmlr.org/papers/v26/24-0352.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assumption-lean and data-adaptive post-prediction inference. <em>JMLR</em>, <em>26</em>(179), 1-31. (<a href='https://jmlr.org/papers/v26/24-0056.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primary challenge facing modern scientific research is the limited availability of gold-standard data, which can be costly, labor-intensive, or invasive to obtain. With the rapid development of machine learning (ML), scientists can now employ ML algorithms to predict gold-standard outcomes using variables that are easier to obtain. However, these predicted outcomes are often used directly in subsequent statistical analyses, ignoring imprecision and heterogeneity introduced by the prediction procedure. This will likely result in false positive findings and invalid scientific conclusions. In this work, we introduce PoSt-Prediction Adaptive inference (PSPA) that allows valid and powerful inference based on ML-predicted data. Its “assumption-lean” property guarantees reliable statistical inference without assumptions on the ML prediction. Its “data-adaptive” feature guarantees an efficiency gain over existing methods, regardless of the accuracy of ML prediction. We demonstrate the statistical superiority and broad applicability of our method through simulations and real-data applications.},
  archive      = {J_JMLR},
  author       = {Jiacheng Miao and Xinran Miao and Yixuan Wu and Jiwei Zhao and Qiongshi Lu},
  journal      = {Journal of Machine Learning Research},
  number       = {179},
  pages        = {1-31},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Assumption-lean and data-adaptive post-prediction inference},
  url          = {https://jmlr.org/papers/v26/24-0056.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bagged regularized k-distances for anomaly detection. <em>JMLR</em>, <em>26</em>(178), 1-59. (<a href='https://jmlr.org/papers/v26/23-1519.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the paradigm of unsupervised anomaly detection, which involves the identification of anomalies within a dataset in the absence of labeled examples. Though distance-based methods are top-performing for unsupervised anomaly detection, they suffer heavily from the sensitivity to the choice of the number of the nearest neighbors. In this paper, we propose a new distance-based algorithm called bagged regularized $k$-distances for anomaly detection (BRDAD), converting the unsupervised anomaly detection problem into a convex optimization problem. Our BRDAD algorithm selects the weights by minimizing the surrogate risk, i.e., the finite sample bound of the empirical risk of the bagged weighted $k$-distances for density estimation (BWDDE). This approach enables us to successfully address the sensitivity challenge of the hyperparameter choice in distance-based algorithms. Moreover, when dealing with large-scale datasets, the efficiency issues can be addressed by the incorporated bagging technique in our BRDAD algorithm. On the theoretical side, we establish fast convergence rates of the AUC regret of our algorithm and demonstrate that the bagging technique significantly reduces the computational complexity. On the practical side, we conduct numerical experiments to illustrate the insensitivity of the parameter selection of our algorithm compared with other state-of-the-art distance-based methods. Furthermore, our method achieves superior performance on real-world datasets with the introduced bagging technique compared to other approaches.},
  archive      = {J_JMLR},
  author       = {Yuchao Cai and Hanfang Yang and Yuheng Ma and Hanyuan Hang},
  journal      = {Journal of Machine Learning Research},
  number       = {178},
  pages        = {1-59},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Bagged regularized k-distances for anomaly detection},
  url          = {https://jmlr.org/papers/v26/23-1519.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Four axiomatic characterizations of the integrated gradients attribution method. <em>JMLR</em>, <em>26</em>(177), 1-31. (<a href='https://jmlr.org/papers/v26/23-0671.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have produced significant progress among machine learning models in terms of accuracy and functionality, but their inner workings are still largely unknown. Attribution methods seek to shine a light on these "black box" models by indicating how much each input contributed to a model's outputs. The Integrated Gradients (IG) method is a state of the art baseline attribution method in the axiomatic vein, meaning it is designed to conform to particular principles of attributions. We present four axiomatic characterizations of IG, establishing IG as the unique method satisfying four different sets of axioms.},
  archive      = {J_JMLR},
  author       = {Daniel Lundstrom and Meisam Razaviyayn},
  journal      = {Journal of Machine Learning Research},
  number       = {177},
  pages        = {1-31},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Four axiomatic characterizations of the integrated gradients attribution method},
  url          = {https://jmlr.org/papers/v26/23-0671.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast algorithm for constrained linear inverse problems. <em>JMLR</em>, <em>26</em>(176), 1-41. (<a href='https://jmlr.org/papers/v26/22-1380.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the constrained Linear Inverse Problem (LIP), where a certain atomic norm (like the $\ell_1 $ norm) is minimized subject to a quadratic constraint. Typically, such cost functions are non-differentiable, which makes them not amenable to the fast optimization methods existing in practice. We propose two equivalent reformulations of the constrained LIP with improved convex regularity: (i) a smooth convex minimization problem, and (ii) a strongly convex min-max problem. These problems could be solved by applying existing acceleration-based convex optimization methods which provide better $ O \left( \frac{1}{k^2} \right)$ theoretical convergence guarantee, improving upon the current best rate of $O \left( \frac{1}{k} \right)$. We also provide a novel algorithm named the Fast Linear Inverse Problem Solver (FLIPS), which is tailored to maximally exploit the structure of the reformulations. We demonstrate the performance of FLIPS on the classical problems of Binary Selection, Compressed Sensing, and Image Denoising. We also provide open source \texttt{MATLAB} and \texttt{PYTHON} packages for these three examples, which can be easily adapted to other LIPs.},
  archive      = {J_JMLR},
  author       = {Mohammed Rayyan Sheriff and Floor Fenne Redel and Peyman Mohajerin Esfahani},
  journal      = {Journal of Machine Learning Research},
  number       = {176},
  pages        = {1-41},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Fast algorithm for constrained linear inverse problems},
  url          = {https://jmlr.org/papers/v26/22-1380.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-rank irreducible cartesian tensor decomposition and bases of equivariant spaces. <em>JMLR</em>, <em>26</em>(175), 1-53. (<a href='https://jmlr.org/papers/v26/25-0134.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Irreducible Cartesian tensors (ICTs) play a crucial role in the design of equivariant graph neural networks, as well as in theoretical chemistry and chemical physics. Meanwhile, the design space of available linear operations on tensors that preserve symmetry presents a significant challenge. The ICT decomposition and a basis of this equivariant space are difficult to obtain for high-rank tensors. After decades of research, Bonvicini (2024) has recently achieved an explicit ICT decomposition for $n=5$ with factorial time/space complexity. In this work we, for the first time, obtain decomposition matrices for ICTs up to rank $n=9$ with reduced and affordable complexity, by constructing what we call path matrices. The path matrices are obtained via performing chain-like contractions with Clebsch-Gordan matrices following the parentage scheme. We prove and leverage that the concatenation of path matrices is an orthonormal change-of-basis matrix between the Cartesian tensor product space and the spherical direct sum spaces. Furthermore, we identify a complete orthogonal basis for the equivariant space, rather than a spanning set (Pearce-Crump, 2023b), through this path matrices technique. Our method avoids the RREF algorithm and maintains a fully analytical derivation of each ICT decomposition matrix, thereby significantly improving the algorithm’s speed to obtain arbitrary rank orthogonal ICT decomposition matrices and orthogonal equivariant bases. We further extend our result to the arbitrary tensor product and direct sum spaces, enabling free design between different spaces while keeping symmetry. The Python code is available at https://github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases, where the $n=6,\dots,9$ ICT decomposition matrices are obtained in 1s, 3s, 11s, and 4m32s on 28-core Intel Xeon Gold 6330 CPU @ 2.00GHz, respectively.},
  archive      = {J_JMLR},
  author       = {Shihao Shao and Yikang Li and Zhouchen Lin and Qinghua Cui},
  journal      = {Journal of Machine Learning Research},
  number       = {175},
  pages        = {1-53},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {High-rank irreducible cartesian tensor decomposition and bases of equivariant spaces},
  url          = {https://jmlr.org/papers/v26/25-0134.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Best linear unbiased estimate from privatized contingency tables. <em>JMLR</em>, <em>26</em>(174), 1-41. (<a href='https://jmlr.org/papers/v26/24-1962.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In differential privacy (DP) mechanisms, it can be beneficial to release "redundant" outputs, where some quantities can be estimated in multiple ways by combining different privatized values. Indeed, the DP 2020 Decennial Census products published by the U.S. Census Bureau consist of such redundant noisy counts. When redundancy is present, the DP output can be improved by enforcing self-consistency (i.e., estimators obtained using different noisy counts result in the same value), and we show that the minimum variance processing is a linear projection. However, standard projection algorithms require excessive computation and memory, making them impractical for large-scale applications such as the Decennial Census. We propose the Scalable Efficient Algorithm for Best Linear Unbiased Estimate (SEA BLUE), based on a two-step process of aggregation and differencing that 1) enforces self-consistency through a linear and unbiased procedure, 2) is computationally and memory efficient, 3) achieves the minimum variance solution under certain structural assumptions, and 4) is empirically shown to be robust to violations of these structural assumptions. We propose three methods of calculating confidence intervals from our estimates, under various assumptions. Finally, we apply SEA BLUE to two 2010 Census demonstration products, illustrating its scalability and validity.},
  archive      = {J_JMLR},
  author       = {Jordan Awan and Adam Edwards and Paul Bartholomew and Andrew Sillers},
  journal      = {Journal of Machine Learning Research},
  number       = {174},
  pages        = {1-41},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Best linear unbiased estimate from privatized contingency tables},
  url          = {https://jmlr.org/papers/v26/24-1962.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable global minima of deep ReLU neural networks on sequentially separable data. <em>JMLR</em>, <em>26</em>(173), 1-31. (<a href='https://jmlr.org/papers/v26/24-1516.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explicitly construct zero loss neural network classifiers. We write the weight matrices and bias vectors in terms of cumulative parameters, which determine truncation maps acting recursively on input space. The configurations for the training data considered are $(i)$ sufficiently small, well separated clusters corresponding to each class, and $(ii)$ equivalence classes which are sequentially linearly separable. In the best case, for $Q$ classes of data in $\mathbb{R}^{M}$, global minimizers can be described with $Q(M+2)$ parameters.},
  archive      = {J_JMLR},
  author       = {Thomas Chen and Patrícia Muñoz Ewald},
  journal      = {Journal of Machine Learning Research},
  number       = {173},
  pages        = {1-31},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Interpretable global minima of deep ReLU neural networks on sequentially separable data},
  url          = {https://jmlr.org/papers/v26/24-1516.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced feature learning via regularisation: Integrating neural networks and kernel methods. <em>JMLR</em>, <em>26</em>(172), 1-56. (<a href='https://jmlr.org/papers/v26/24-1178.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for feature learning and function estimation in supervised learning via regularised empirical risk minimisation. Our approach considers functions as expectations of Sobolev functions over all possible one-dimensional projections of the data. This framework is similar to kernel ridge regression, where the kernel is E_w(k(B)(wx, wx')), with k(B)(a, b) := min(|a|, |b|)1_{ab>0} the Brownian kernel, and the distribution of the projections w is learnt. This can also be viewed as an infinite-width one-hidden layer neural network, optimising the first layer’s weights through gradient descent and explicitly adjusting the non-linearity and weights of the second layer. We introduce a gradient-based computational method for the estimator, called Brownian Kernel Neural Network (BKerNN), using particles to approximate the expectation, where the positive homogeneity of the Brownian kernel leads to improved robustness to local minima. Using Rademacher complexity, we show that BKerNN’s expected risk converges to the minimal risk with explicit high-probability rates of O(min((d/n)^1/2, n^−1/6)) (up to logarithmic factors). Numerical experiments confirm our optimisation intuitions, and BKerNN outperforms kernel ridge regression, and favourably compares to a one-hidden layer neural network with ReLU activations in various settings and real datasets.},
  archive      = {J_JMLR},
  author       = {Bertille FOLLAIN and Francis BACH},
  journal      = {Journal of Machine Learning Research},
  number       = {172},
  pages        = {1-56},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Enhanced feature learning via regularisation: Integrating neural networks and kernel methods},
  url          = {https://jmlr.org/papers/v26/24-1178.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven performance guarantees for classical and learned optimizers. <em>JMLR</em>, <em>26</em>(171), 1-49. (<a href='https://jmlr.org/papers/v26/24-0755.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a data-driven approach to analyze the performance of continuous optimization algorithms using generalization guarantees from statistical learning theory. We study classical and learned optimizers to solve families of parametric optimization problems. We build generalization guarantees for classical optimizers, using a sample convergence bound, and for learned optimizers, using the Probably Approximately Correct (PAC)-Bayes framework. To train learned optimizers, we use a gradient-based algorithm to directly minimize the PAC-Bayes upper bound. Numerical experiments in signal processing, control, and meta-learning showcase the ability of our framework to provide strong generalization guarantees for both classical and learned optimizers given a fixed budget of iterations. For classical optimizers, our bounds which hold with high probability are much tighter than those that worst-case guarantees provide. For learned optimizers, our bounds outperform the empirical outcomes observed in their non-learned counterparts.},
  archive      = {J_JMLR},
  author       = {Rajiv Sambharya and Bartolomeo Stellato},
  journal      = {Journal of Machine Learning Research},
  number       = {171},
  pages        = {1-49},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Data-driven performance guarantees for classical and learned optimizers},
  url          = {https://jmlr.org/papers/v26/24-0755.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual bandits with stage-wise constraints. <em>JMLR</em>, <em>26</em>(170), 1-57. (<a href='https://jmlr.org/papers/v26/24-0267.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study contextual bandits in the presence of a stage-wise constraint when the constraint must be satisfied both with high probability and in expectation. We start with the linear case where both the reward function and the stage-wise constraint (cost function) are linear. In each of the high probability and in expectation settings, we propose an upper-confidence bound algorithm for the problem and prove a $T$-round regret bound for it. We also prove a lower-bound for this constrained problem, show how our algorithms and analyses can be extended to multiple constraints, and provide simulations to validate our theoretical results. In the high probability setting, we describe the minimum requirements for the action set for our algorithm to be tractable. In the setting that the constraint is in expectation, we specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting with regret analysis. Finally, we extend our results to the case where the reward and cost functions are both non-linear. We propose an algorithm for this case and prove a regret bound for it that characterize the function class complexity by the eluder dimension.},
  archive      = {J_JMLR},
  author       = {Aldo Pacchiano and Mohammad Ghavamzadeh and Peter Bartlett},
  journal      = {Journal of Machine Learning Research},
  number       = {170},
  pages        = {1-57},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Contextual bandits with stage-wise constraints},
  url          = {https://jmlr.org/papers/v26/24-0267.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting causal additive models. <em>JMLR</em>, <em>26</em>(169), 1-49. (<a href='https://jmlr.org/papers/v26/24-0052.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a boosting-based method to learn additive Structural Equation Models (SEMs) from observational data, with a focus on the theoretical aspects of determining the causal order among variables. We introduce a family of score functions based on arbitrary regression techniques, for which we establish sufficient conditions that guarantee consistent identification of the true causal ordering. Our analysis reveals that boosting with early stopping meets these criteria and thus offers a consistent score function for causal orderings. To address the challenges posed by high-dimensional data sets, we adapt our approach through a component-wise gradient descent in the space of additive SEMs. Our simulation study supports the theoretical findings in low-dimensional settings and demonstrates that our high-dimensional adaptation is competitive with state-of-the-art methods. In addition, it exhibits robustness with respect to the choice of hyperparameters, thereby simplifying the tuning process.},
  archive      = {J_JMLR},
  author       = {Maximilian Kertel and Nadja Klein},
  journal      = {Journal of Machine Learning Research},
  number       = {169},
  pages        = {1-49},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Boosting causal additive models},
  url          = {https://jmlr.org/papers/v26/24-0052.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequentist guarantees of distributed (Non)-bayesian inference. <em>JMLR</em>, <em>26</em>(168), 1-65. (<a href='https://jmlr.org/papers/v26/23-1504.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish frequentist properties, i.e., posterior consistency, asymptotic normality, and posterior contraction rates, for the distributed (non-)Bayesian inference problem for a set of agents connected over a network. These results are motivated by the need to analyze large, decentralized datasets, where distributed (non)-Bayesian inference has become a critical research area across multiple fields, including statistics, machine learning, and economics. Our results show that, under appropriate assumptions on the communication graph, distributed (non)-Bayesian inference retains parametric efficiency while enhancing robustness in uncertainty quantification. We also explore the trade-off between statistical efficiency and communication efficiency by examining how the design and size of the communication graph impact the posterior contraction rate. Furthermore, we extend our analysis to time-varying graphs and apply our results to exponential family models, distributed logistic regression, and decentralized detection models.},
  archive      = {J_JMLR},
  author       = {Bohan Wu and César A. Uribe},
  journal      = {Journal of Machine Learning Research},
  number       = {168},
  pages        = {1-65},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Frequentist guarantees of distributed (Non)-bayesian inference},
  url          = {https://jmlr.org/papers/v26/23-1504.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic inference for multi-stage stationary treatment policy with variable selection. <em>JMLR</em>, <em>26</em>(167), 1-50. (<a href='https://jmlr.org/papers/v26/23-0660.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes or policies are a sequence of decision functions over multiple stages that are tailored to individual features. One important class of treatment policies in practice, namely multi-stage stationary treatment policies, prescribes treatment assignment probabilities using the same decision function across stages, where the decision is based on the same set of features consisting of time-evolving variables (e.g., routinely collected disease biomarkers). Although there has been extensive literature on constructing valid inference for the value function associated with dynamic treatment policies, little work has focused on the policies themselves, especially in the presence of high-dimensional features. We aim to fill the gap in this work. Specifically, we first obtain the multi-stage stationary treatment policy by minimizing the negative augmented inverse probability weighted estimator of the value function to increase asymptotic efficiency. An $L_1$ penalty is applied on the policy parameters to select important features. We then construct one-step improvements of the policy parameter estimators for valid inference. Theoretically, we show that the improved estimators are asymptotically normal, even if nuisance parameters are estimated at a slow convergence rate and the dimension of the features increases with the sample size. Our numerical studies demonstrate that the proposed method estimates a sparse policy with a near-optimal value function and conducts valid inference for the policy parameters.},
  archive      = {J_JMLR},
  author       = {Daiqi Gao and Yufeng Liu and Donglin Zeng},
  journal      = {Journal of Machine Learning Research},
  number       = {167},
  pages        = {1-50},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Asymptotic inference for multi-stage stationary treatment policy with variable selection},
  url          = {https://jmlr.org/papers/v26/23-0660.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMaP: Explainable AI with manifold-based perturbations. <em>JMLR</em>, <em>26</em>(166), 1-35. (<a href='https://jmlr.org/papers/v26/22-1157.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, many explanation methods based on the perturbations of input data have been introduced to shed light on the predictions generated by black-box models. The goal of this work is to introduce a novel perturbation scheme so that more faithful and robust explanations can be obtained. Our study focuses on the impact of perturbing directions on the data topology. We show that perturbing along the orthogonal directions of the input manifold better preserves the data topology, both in the worst-case analysis of the discrete Gromov-Hausdorff distance and in the average-case analysis via persistent homology. From those results, we introduce EMaP algorithm, realizing the orthogonal perturbation scheme. Our experiments show that EMaP not only improves the explainers' performance but also helps them overcome a recently developed attack against perturbation-based explanation methods.},
  archive      = {J_JMLR},
  author       = {Minh Nhat Vu and Huy Quang Mai and My T. Thai},
  journal      = {Journal of Machine Learning Research},
  number       = {166},
  pages        = {1-35},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {EMaP: Explainable AI with manifold-based perturbations},
  url          = {https://jmlr.org/papers/v26/22-1157.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autoencoders in function space. <em>JMLR</em>, <em>26</em>(165), 1-54. (<a href='https://jmlr.org/papers/v26/25-0035.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoders have found widespread application in both their original deterministic form and in their variational formulation (VAEs). In scientific applications and in image processing it is often of interest to consider data that are viewed as functions; while discretisation (of differential equations arising in the sciences) or pixellation (of images) renders problems finite dimensional in practice, conceiving first of algorithms that operate on functions, and only then discretising or pixellating, leads to better algorithms that smoothly operate between resolutions. In this paper function-space versions of the autoencoder (FAE) and variational autoencoder (FVAE) are introduced, analysed, and deployed. Well-definedness of the objective governing VAEs is a subtle issue, particularly in function space, limiting applicability. For the FVAE objective to be well defined requires compatibility of the data distribution with the chosen generative model; this can be achieved, for example, when the data arise from a stochastic differential equation, but is generally restrictive. The FAE objective, on the other hand, is well defined in many situations where FVAE fails to be. Pairing the FVAE and FAE objectives with neural operator architectures that can be evaluated on any mesh enables new applications of autoencoders to inpainting, superresolution, and generative modelling of scientific data.},
  archive      = {J_JMLR},
  author       = {Justin Bunker and Mark Girolami and Hefin Lambley and Andrew M. Stuart and T. J. Sullivan},
  journal      = {Journal of Machine Learning Research},
  number       = {165},
  pages        = {1-54},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Autoencoders in function space},
  url          = {https://jmlr.org/papers/v26/25-0035.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric regression on random geometric graphs sampled from submanifolds. <em>JMLR</em>, <em>26</em>(164), 1-65. (<a href='https://jmlr.org/papers/v26/24-1960.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the nonparametric regression problem when the covariates are located on an unknown compact submanifold of a Euclidean space. Under defining a random geometric graph structure over the covariates we analyse the asymptotic frequentist behaviour of the posterior distribution arising from Bayesian priors designed through random basis expansion in the graph Laplacian eigenbasis. Under Hölder smoothness assumption on the regression function and the density of the covariates over the submanifold, we prove that the posterior contraction rates of such methods are minimax optimal (up to logarithmic factors) for any positive smoothness index.},
  archive      = {J_JMLR},
  author       = {Paul Rosa and Judith Rousseau},
  journal      = {Journal of Machine Learning Research},
  number       = {164},
  pages        = {1-65},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Nonparametric regression on random geometric graphs sampled from submanifolds},
  url          = {https://jmlr.org/papers/v26/24-1960.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). System neural diversity: Measuring behavioral heterogeneity in multi-agent learning. <em>JMLR</em>, <em>26</em>(163), 1-27. (<a href='https://jmlr.org/papers/v26/24-1477.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary science provides evidence that diversity confers resilience in natural systems. Yet, traditional multi-agent reinforcement learning techniques commonly enforce homogeneity to increase training sample efficiency. When a system of learning agents is not constrained to homogeneous policies, individuals may develop diverse behaviors, resulting in emergent complementarity that benefits the system. Despite this, there is a surprising lack of tools that quantify behavioral diversity. Such techniques would pave the way towards understanding the impact of diversity in collective artificial intelligence and enabling its control. In this paper, we introduce System Neural Diversity (SND): a measure of behavioral heterogeneity in multi-agent systems. We discuss and prove its theoretical properties, and compare it with alternate, state-of-the-art behavioral diversity metrics used in the robotics domain. Through simulations of a variety of cooperative multi-robot tasks, we show how our metric constitutes an important tool that enables measurement and control of behavioral heterogeneity. In dynamic tasks, where the problem is affected by repeated disturbances during training, we show that SND allows us to measure latent resilience skills acquired by the agents, while other proxies, such as task performance (reward), fail to. Finally, we show how the metric can be employed to control diversity, allowing us to enforce a desired heterogeneity set-point or range. We demonstrate how this paradigm can be used to bootstrap the exploration phase, finding optimal policies faster, thus enabling novel and more efficient MARL paradigms.},
  archive      = {J_JMLR},
  author       = {Matteo Bettini and Ajay Shankar and Amanda Prorok},
  journal      = {Journal of Machine Learning Research},
  number       = {163},
  pages        = {1-27},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {System neural diversity: Measuring behavioral heterogeneity in multi-agent learning},
  url          = {https://jmlr.org/papers/v26/24-1477.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution estimation under the infinity norm. <em>JMLR</em>, <em>26</em>(162), 1-30. (<a href='https://jmlr.org/papers/v26/24-1166.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present novel bounds for estimating discrete probability distributions under the $\ell_\infty$ norm. These are nearly optimal in various precise senses, including a kind of instance-optimality. Our data-dependent convergence guarantees for the maximum likelihood estimator significantly improve upon the currently known results. A variety of techniques are utilized and innovated upon, including Chernoff-type inequalities and empirical Bernstein bounds. We illustrate our results in synthetic and real-world experiments. Finally, we apply our proposed framework to a basic selective inference problem, where we estimate the most frequent probabilities in a sample.},
  archive      = {J_JMLR},
  author       = {Aryeh Kontorovich and Amichai Painsky},
  journal      = {Journal of Machine Learning Research},
  number       = {162},
  pages        = {1-30},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Distribution estimation under the infinity norm},
  url          = {https://jmlr.org/papers/v26/24-1166.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extending temperature scaling with homogenizing maps. <em>JMLR</em>, <em>26</em>(161), 1-46. (<a href='https://jmlr.org/papers/v26/24-0700.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As machine learning models continue to grow more complex, poor calibration significantly limits the reliability of their predictions. Temperature scaling learns a single temperature parameter to scale the output logits, and despite its simplicity, remains one of the most effective post-hoc recalibration methods. We identify one of temperature scaling's defining attributes, that it increases the uncertainty of the predictions in a manner that we term homogenization, and propose to learn the optimal recalibration mapping from a larger class of functions that satisfies this property. We demonstrate the advantage of our method over temperature scaling in both calibration and out-of-distribution detection. Additionally, we extend our methodology and experimental evaluation to recalibration in the Bayesian setting.},
  archive      = {J_JMLR},
  author       = {Christopher Qian and Feng Liang and Jason Adams},
  journal      = {Journal of Machine Learning Research},
  number       = {161},
  pages        = {1-46},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Extending temperature scaling with homogenizing maps},
  url          = {https://jmlr.org/papers/v26/24-0700.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density estimation using the perceptron. <em>JMLR</em>, <em>26</em>(160), 1-51. (<a href='https://jmlr.org/papers/v26/24-0261.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new density estimation algorithm. Given $n$ i.i.d. observations from a distribution belonging to a class of densities on $\mathbb{R}^d$, our estimator outputs any density in the class whose “perceptron discrepancy” with the empirical distribution is at most $O(\sqrt{d/n})$. The perceptron discrepancy is defined as the largest difference in mass two distribution place on any halfspace. It is shown that this estimator achieves the expected total variation distance to the truth that is almost minimax optimal over the class of densities with bounded Sobolev norm and Gaussian mixtures. This suggests that the regularity of the prior distribution could be an explanation for the efficiency of the ubiquitous step in machine learning that replaces optimization over large function spaces with simpler parametric classes (such as discriminators of GANs). We also show that replacing the perceptron discrepancy with the generalized energy distance of Székely and Rizzo (2013) further improves total variation loss. The generalized energy distance between empirical distributions is easily computable and differentiable, which makes it especially useful for fitting generative models. To the best of our knowledge, it is the first “simple” distance with such properties that yields minimax optimal statistical guarantees. In addition, we shed light on the ubiquitous method of representing discrete data in domain $[k]$ via embedding vectors on a unit ball in $\mathbb{R}^d$. We show that taking $d \asymp \log(k)$ allows one to use simple linear probing to evaluate and estimate total variation distance, as well as recovering minimax optimal sample complexity for the class of discrete distributions on $[k]$.},
  archive      = {J_JMLR},
  author       = {Patrik Róbert Gerber and Tianze Jiang and Yury Polyanskiy and Rui Sun},
  journal      = {Journal of Machine Learning Research},
  number       = {160},
  pages        = {1-51},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Density estimation using the perceptron},
  url          = {https://jmlr.org/papers/v26/24-0261.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simplex constrained sparse optimization via tail screening. <em>JMLR</em>, <em>26</em>(159), 1-38. (<a href='https://jmlr.org/papers/v26/24-0010.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the probabilistic simplex-constrained sparse recovery problem. The commonly used Lasso-type penalty for promoting sparsity is ineffective in this context since it is a constant within the simplex. Despite this challenge, fortunately, simplex constraint itself brings a self-regularization property, i.e., the empirical risk minimizer without any sparsity-promoting procedure obtains the usual Lasso-type estimation error. Moreover, we analyze the iterates of a projected gradient descent method and show its convergence to the ground truth sparse solution in the geometric rate until a satisfied statistical precision is attained. Although the estimation error is statistically optimal, the resulting solution is usually more dense than the sparse ground truth. To further sparsify the iterates, we propose a method called PERMITS via embedding a tail screening procedure, i.e., identifying negligible components and discarding them during iterations, into the projected gradient descent method. Furthermore, we combine tail screening and the special information criterion to balance the trade-off between fitness and complexity. Theoretically, the proposed PERMITS method can exactly recover the ground truth support set under mild conditions and thus obtain the oracle property. We demonstrate the statistical and computational efficiency of PERMITS with both synthetic and real data. The implementation of the proposed method can be found in https://github.com/abess-team/PERMITS.},
  archive      = {J_JMLR},
  author       = {Peng Chen and Jin Zhu and Junxian Zhu and Xueqin Wang},
  journal      = {Journal of Machine Learning Research},
  number       = {159},
  pages        = {1-38},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Simplex constrained sparse optimization via tail screening},
  url          = {https://jmlr.org/papers/v26/24-0010.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Score-based diffusion models in function space. <em>JMLR</em>, <em>26</em>(158), 1-62. (<a href='https://jmlr.org/papers/v26/23-1472.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have recently emerged as a powerful framework for generative modeling. They consist of a forward process that perturbs input data with Gaussian white noise and a reverse process that learns a score function to generate samples by denoising. Despite their tremendous success, they are mostly formulated on finite-dimensional spaces, e.g., Euclidean, limiting their applications to many domains where the data has a functional form, such as in scientific computing and 3D geometric data analysis. This work introduces a mathematically rigorous framework called Denoising Diffusion Operators (DDOs) for training diffusion models in function space. In DDOs, the forward process perturbs input functions gradually using a Gaussian process. The generative process is formulated by a function-valued annealed Langevin dynamic. Our approach requires an appropriate notion of the score for the perturbed data distribution, which we obtain by generalizing denoising score matching to function spaces that can be infinite-dimensional. We show that the corresponding discretized algorithm generates accurate samples at a fixed cost independent of the data resolution. We theoretically and numerically verify the applicability of our approach on a set of function-valued problems, including generating solutions to the Navier-Stokes equation viewed as the push-forward distribution of forcings from a Gaussian Random Field (GRF), as well as volcano InSAR and MNIST-SDF.},
  archive      = {J_JMLR},
  author       = {Jae Hyun Lim and Nikola B. Kovachki and Ricardo Baptista and Christopher Beckham and Kamyar Azizzadenesheli and Jean Kossaifi and Vikram Voleti and Jiaming Song and Karsten Kreis and Jan Kautz and Christopher Pal and Arash Vahdat and Anima Anandkumar},
  journal      = {Journal of Machine Learning Research},
  number       = {158},
  pages        = {1-62},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Score-based diffusion models in function space},
  url          = {https://jmlr.org/papers/v26/23-1472.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularized rényi divergence minimization through bregman proximal gradient algorithms. <em>JMLR</em>, <em>26</em>(157), 1-56. (<a href='https://jmlr.org/papers/v26/23-0573.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the variational inference problem of minimizing a regularized Rényi divergence over an exponential family. We propose to solve this problem with a Bregman proximal gradient algorithm. We propose a sampling-based algorithm to cover the black-box setting, corresponding to a stochastic Bregman proximal gradient algorithm with biased gradient estimator. We show that the resulting algorithms can be seen as relaxed moment-matching algorithms with an additional proximal step. Using Bregman updates instead of Euclidean ones allows us to exploit the geometry of our approximate model. We prove strong convergence guarantees for both our deterministic and stochastic algorithms using this viewpoint, including monotonic decrease of the objective, convergence to a stationary point or to the minimizer, and geometric convergence rates. These new theoretical insights lead to a versatile, robust, and competitive method, as illustrated by numerical experiments},
  archive      = {J_JMLR},
  author       = {Thomas Guilmeau and Emilie Chouzenoux and Víctor Elvira},
  journal      = {Journal of Machine Learning Research},
  number       = {157},
  pages        = {1-56},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Regularized rényi divergence minimization through bregman proximal gradient algorithms},
  url          = {https://jmlr.org/papers/v26/23-0573.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WEFE: A python library for measuring and mitigating bias in word embeddings. <em>JMLR</em>, <em>26</em>(156), 1-6. (<a href='https://jmlr.org/papers/v26/22-1133.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word embeddings, which are a mapping of words into continuous vectors, are widely used in modern Natural Language Processing (NLP) systems. However, they are prone to inherit stereotypical social biases from the corpus on which they are built. The research community has focused on two main tasks to address this problem: 1) how to measure these biases, and 2) how to mitigate them. Word Embedding Fairness Evaluation (WEFE) is an open source library that implements many fairness metrics and mitigation methods in a unified framework. It also provides a standard interface for designing new ones. The software follows the object-oriented paradigm with a strong focus on extensibility. Each of its methods is appropriately documented, verified and tested. WEFE is not limited to just a library: it also contains several replications of previous studies as well as tutorials that serve as educational material for newcomers to the field. It is licensed under BSD-3 and can be easily installed through pip and conda package managers.},
  archive      = {J_JMLR},
  author       = {Pablo Badilla and Felipe Bravo-Marquez and María José Zambrano and Jorge Pérez},
  journal      = {Journal of Machine Learning Research},
  number       = {156},
  pages        = {1-6},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {WEFE: A python library for measuring and mitigating bias in word embeddings},
  url          = {https://jmlr.org/papers/v26/22-1133.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frontiers to the learning of nonparametric hidden markov models. <em>JMLR</em>, <em>26</em>(155), 1-75. (<a href='https://jmlr.org/papers/v26/24-2230.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models (HMMs) are flexible tools for clustering dependent data coming from unknown populations, allowing nonparametric modelling of the population densities. Identifiability fails when the data is in fact independent and identically distributed (i.i.d.), and we study the frontier between learnable and unlearnable two-state nonparametric HMMs. Learning the parameters of the HMM requires solving a nonlinear inverse problem whose difficulty depends not only on the smoothnesses of the populations but also on the distance to the i.i.d. boundary of the parameter set. The latter difficulty is mostly ignored in the literature in favour of assumptions precluding nearly independent data. This is the first work conducting a precise nonasymptotic, nonparametric analysis of the minimax risk taking into account all aspects of the hardness of the problem, in the case of two populations. Our analysis reveals an unexpected interplay between the distance to the i.i.d. boundary and the relative smoothnesses of the two populations: a surprising and intriguing transition occurs in the rate when the two densities have differing smoothnesses. We obtain upper and lower bounds revealing that, close to the i.i.d. boundary, it is possible to "borrow strength" from the estimator of the smoother density to improve the risk of the other.},
  archive      = {J_JMLR},
  author       = {Kweku Abraham and Elisabeth Gassiat and Zacharie Naulet},
  journal      = {Journal of Machine Learning Research},
  number       = {155},
  pages        = {1-75},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Frontiers to the learning of nonparametric hidden markov models},
  url          = {https://jmlr.org/papers/v26/24-2230.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On non-asymptotic theory of recurrent neural networks in temporal point processes. <em>JMLR</em>, <em>26</em>(154), 1-67. (<a href='https://jmlr.org/papers/v26/24-1953.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal point process (TPP) is an important tool for modeling and predicting irregularly timed events across various domains. Recently, the recurrent neural network (RNN)-based TPPs have shown practical advantages over traditional parametric TPP models. However, in the current literature, it remains nascent in understanding neural TPPs from theoretical viewpoints. In this paper, we establish the excess risk bounds of RNN-TPPs under many well-known TPP settings. We especially show that an RNN-TPP with no more than four layers can achieve vanishing generalization errors. Our technical contributions include the characterization of the complexity of the multi-layer RNN class, the construction of $\tanh$ neural networks for approximating dynamic event intensity functions, and the truncation technique for alleviating the issue of unbounded event sequences. Our results bridge the gap between TPP's application and neural network theory.},
  archive      = {J_JMLR},
  author       = {Zhiheng Chen and Guanhua Fang and Wen Yu},
  journal      = {Journal of Machine Learning Research},
  number       = {154},
  pages        = {1-67},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On non-asymptotic theory of recurrent neural networks in temporal point processes},
  url          = {https://jmlr.org/papers/v26/24-1953.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification in the high dimensional anisotropic mixture framework: A new take on robust interpolation. <em>JMLR</em>, <em>26</em>(153), 1-39. (<a href='https://jmlr.org/papers/v26/24-1366.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the classification problem under the two-component anisotropic sub-Gaussian mixture model in high dimensions and in the non-asymptotic setting. First, we derive lower bounds and matching upper bounds for the minimax risk of classification in this framework. We also show that in the high-dimensional regime, the linear discriminant analysis classifier turns out to be sub-optimal in the minimax sense. Next, we give precise characterization of the risk of classifiers based on solutions of $\ell_2$-regularized least squares problem. We deduce that the interpolating solutions may outperform the regularized classifiers under mild assumptions on the covariance structure of the noise, and present concrete examples of this phenomenon. Our analysis also demonstrates robustness of interpolation to certain models of corruption. To the best of our knowledge, this peculiar fact has not yet been investigated in the rapidly growing literature related to interpolation. We conclude that interpolation is not only benign but can also be optimal, and in some cases robust.},
  archive      = {J_JMLR},
  author       = {Stanislav Minsker and Mohamed Ndaoud and Yiqiu Shen},
  journal      = {Journal of Machine Learning Research},
  number       = {153},
  pages        = {1-39},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Classification in the high dimensional anisotropic mixture framework: A new take on robust interpolation},
  url          = {https://jmlr.org/papers/v26/24-1366.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Universal online convex optimization meets second-order bounds. <em>JMLR</em>, <em>26</em>(152), 1-53. (<a href='https://jmlr.org/papers/v26/24-1131.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, several universal methods have been proposed for online convex optimization, and attain minimax rates for multiple types of convex functions simultaneously. However, they need to design and optimize one surrogate loss for each type of functions, making it difficult to exploit the structure of the problem and utilize existing algorithms. In this paper, we propose a simple strategy for universal online convex optimization, which avoids these limitations. The key idea is to construct a set of experts to process the original online functions, and deploy a meta-algorithm over the linearized losses to aggregate predictions from experts. Specifically, the meta-algorithm is required to yield a second-order bound with excess losses, so that it can leverage strong convexity and exponential concavity to control the meta-regret. In this way, our strategy inherits the theoretical guarantee of any expert designed for strongly convex functions and exponentially concave functions, up to a double logarithmic factor. As a result, we can plug in off-the-shelf online solvers as black-box experts to deliver problem-dependent regret bounds. For general convex functions, it maintains the minimax optimality and also achieves a small-loss bound. Furthermore, we extend our universal strategy to online composite optimization, where the loss function comprises a time-varying function and a fixed regularizer. To deal with the composite loss functions, we employ a meta-algorithm based on the optimistic online learning framework, which not only enjoys a second-order bound, but also can utilize estimations for upcoming loss functions. With suitable configurations, we show that the additional regularizer does not contribute to the meta-regret, thus ensuring the universality in the composite setting.},
  archive      = {J_JMLR},
  author       = {Lijun Zhang and Yibo Wang and Guanghui Wang and Jinfeng Yi and Tianbao Yang},
  journal      = {Journal of Machine Learning Research},
  number       = {152},
  pages        = {1-53},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Universal online convex optimization meets second-order bounds},
  url          = {https://jmlr.org/papers/v26/24-1131.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample complexity of the linear quadratic regulator: A reinforcement learning lens. <em>JMLR</em>, <em>26</em>(151), 1-50. (<a href='https://jmlr.org/papers/v26/24-0636.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide the first known algorithm that provably achieves $\varepsilon$-optimality within $\widetilde{O}(1/\varepsilon)$ function evaluations for the discounted discrete-time linear quadratic regulator problem with unknown parameters, without relying on two-point gradient estimates. These estimates are known to be unrealistic in many settings, as they depend on using the exact same initialization, which is to be selected randomly, for two different policies. Our results substantially improve upon the existing literature outside the realm of two-point gradient estimates, which either leads to $\widetilde{O}(1/\varepsilon^2)$ rates or heavily relies on stability assumptions.},
  archive      = {J_JMLR},
  author       = {Amirreza Neshaei Moghaddam and Alex Olshevsky and Bahman Gharesifard},
  journal      = {Journal of Machine Learning Research},
  number       = {151},
  pages        = {1-50},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Sample complexity of the linear quadratic regulator: A reinforcement learning lens},
  url          = {https://jmlr.org/papers/v26/24-0636.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomization can reduce both bias and variance: A case study in random forests. <em>JMLR</em>, <em>26</em>(150), 1-49. (<a href='https://jmlr.org/papers/v26/24-0255.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the often overlooked phenomenon, first noted in Breiman (2001), that random forests appear to reduce bias compared to bagging. Motivated by an interesting paper by Mentch and Zhou (2020), where the authors explain the success of random forests in low signal-to-noise ratio (SNR) settings through regularization, we explore how random forests can capture patterns in the data that bagging ensembles fail to capture. We empirically demonstrate that in the presence of such patterns, random forests reduce bias along with variance and can increasingly outperform bagging ensembles when SNR is high. Our observations offer insights into the real-world success of random forests across a range of SNRs and enhance our understanding of the difference between random forests and bagging ensembles. Our investigations also yield practical insights into the importance of tuning $mtry$ in random forests.},
  archive      = {J_JMLR},
  author       = {Brian Liu and Rahul Mazumder},
  journal      = {Journal of Machine Learning Research},
  number       = {150},
  pages        = {1-49},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Randomization can reduce both bias and variance: A case study in random forests},
  url          = {https://jmlr.org/papers/v26/24-0255.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skglm: Improving scikit-learn for regularized generalized linear models. <em>JMLR</em>, <em>26</em>(149), 1-6. (<a href='https://jmlr.org/papers/v26/24-0008.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce skglm, an open-source Python package for regularized Generalized Linear Models. Thanks to its composable nature, it supports combining datafits, penalties, and solvers to fit a wide range of models, many of them not included in scikit-learn (e.g. Group Lasso and variants). It uses state-of-the-art algorithms to solve problems involving high-dimensional datasets, providing large speed-ups compared to existing implementations. It is fully compliant with the scikit-learn API and acts as a drop-in replacement for its estimators. Finally, it abides by the standards of open source development and is integrated in the scikit-learn-contrib GitHub organization.},
  archive      = {J_JMLR},
  author       = {Badr Moufad and Pierre-Antoine Bannier and Quentin Bertrand and Quentin Klopfenstein and Mathurin Massias},
  journal      = {Journal of Machine Learning Research},
  number       = {149},
  pages        = {1-6},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Skglm: Improving scikit-learn for regularized generalized linear models},
  url          = {https://jmlr.org/papers/v26/24-0008.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Losing momentum in continuous-time stochastic optimisation. <em>JMLR</em>, <em>26</em>(148), 1-55. (<a href='https://jmlr.org/papers/v26/23-1396.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training of modern machine learning models often consists in solving high-dimensional non-convex optimisation problems that are subject to large-scale data. In this context, momentum-based stochastic optimisation algorithms have become particularly widespread. The stochasticity arises from data subsampling which reduces computational cost. Both, momentum and stochasticity help the algorithm to converge globally. In this work, we propose and analyse a continuous-time model for stochastic gradient descent with momentum. This model is a piecewise-deterministic Markov process that represents the optimiser by an underdamped dynamical system and the data subsampling through a stochastic switching. We investigate longtime limits, the subsampling-to-no-subsampling limit, and the momentum-to-no-momentum limit. We are particularly interested in the case of reducing the momentum over time. Under convexity assumptions, we show convergence of our dynamical system to the global minimiser when reducing momentum over time and letting the subsampling rate go to infinity. We then propose a stable, symplectic discretisation scheme to construct an algorithm from our continuous-time dynamical system. In experiments, we study our scheme in convex and non-convex test problems. Additionally, we train a convolutional neural network in an image classification problem. Our algorithm attains competitive results compared to stochastic gradient descent with momentum.},
  archive      = {J_JMLR},
  author       = {Kexin Jin and Jonas Latz and Chenguang Liu and Alessandro Scagliotti},
  journal      = {Journal of Machine Learning Research},
  number       = {148},
  pages        = {1-55},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Losing momentum in continuous-time stochastic optimisation},
  url          = {https://jmlr.org/papers/v26/23-1396.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent process models for functional network data. <em>JMLR</em>, <em>26</em>(147), 1-69. (<a href='https://jmlr.org/papers/v26/23-0444.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network data are often sampled with auxiliary information or collected through the observation of a complex system over time, leading to multiple network snapshots indexed by a continuous variable. Many methods in statistical network analysis are traditionally designed for a single network, and can be applied to an aggregated network in this setting, but that approach can miss important functional structure. Here we develop an approach to estimating the expected network explicitly as a function of a continuous index, be it time or another indexing variable. We parameterize the network expectation through low dimensional latent processes, whose components we represent with a fixed, finite-dimensional functional basis. We derive a gradient descent estimation algorithm, establish theoretical guarantees for recovery of the low dimensional structure, compare our method to competitors, and apply it to a data set of international political interactions over time, showing our proposed method to adapt well to data, outperform competitors, and provide interpretable and meaningful results.},
  archive      = {J_JMLR},
  author       = {Peter W. MacDonald and Elizaveta Levina and Ji Zhu},
  journal      = {Journal of Machine Learning Research},
  number       = {147},
  pages        = {1-69},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Latent process models for functional network data},
  url          = {https://jmlr.org/papers/v26/23-0444.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic bayesian learning for spatiotemporal mechanistic models. <em>JMLR</em>, <em>26</em>(146), 1-43. (<a href='https://jmlr.org/papers/v26/22-0896.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an approach for Bayesian learning of spatiotemporal dynamical mechanistic models. Such learning consists of statistical emulation of the mechanistic system that can efficiently interpolate the output of the system from arbitrary inputs. The emulated learner can then be used to train the system from noisy data achieved by melding information from observed data with the emulated mechanistic system. This joint melding of mechanistic systems employ hierarchical state-space models with Gaussian process regression. Assuming the dynamical system is controlled by a finite collection of inputs, Gaussian process regression learns the effect of these parameters through a number of training runs, driving the stochastic innovations of the spatiotemporal state-space component. This enables efficient modeling of the dynamics over space and time. This article details exact inference with analytically accessible posterior distributions in hierarchical matrix-variate Normal and Wishart models in designing the emulator. This step obviates expensive iterative algorithms such as Markov chain Monte Carlo or variational approximations. We also show how emulation is applicable to large-scale emulation by designing a dynamic Bayesian transfer learning framework. Inference on mechanistic model parameters proceeds using Markov chain Monte Carlo as a post-emulation step using the emulator as a regression component. We demonstrate this framework through solving inverse problems arising in the analysis of ordinary and partial nonlinear differential equations and, in addition, to a black-box computer model generating spatiotemporal dynamics across a graphical model.},
  archive      = {J_JMLR},
  author       = {Sudipto Banerjee and Xiang Chen and Ian Frankenburg and Daniel Zhou},
  journal      = {Journal of Machine Learning Research},
  number       = {146},
  pages        = {1-43},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Dynamic bayesian learning for spatiotemporal mechanistic models},
  url          = {https://jmlr.org/papers/v26/22-0896.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the ability of deep networks to learn symmetries from data: A neural kernel theory. <em>JMLR</em>, <em>26</em>(145), 1-70. (<a href='https://jmlr.org/papers/v26/24-2175.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetries (transformations by group actions) are present in many datasets, and leveraging them holds considerable promise for improving predictions in machine learning. In this work, we aim to understand when and how deep networks---with standard architectures trained in a standard, supervised way---learn symmetries from data. Inspired by real-world scenarios, we study a classification paradigm where data symmetries are only partially observed during training: some classes include all transformations of a cyclic group, while others---only a subset. We ask: under which conditions will deep networks correctly classify the partially sampled classes? In the infinite-width limit, where neural networks behave like kernel machines, we derive a neural kernel theory of symmetry learning. The group-cyclic nature of the dataset allows us to analyze the Gram matrix of neural kernels in the Fourier domain; here we find a simple characterization of the generalization error as a function of class separation (signal) and class-orbit density (noise). This characterization reveals that generalization can only be successful when the local structure of the data prevails over its non-local, symmetry-induced structure, in the kernel space defined by the architecture. This occurs when (1) classes are sufficiently distinct and (2) class orbits are sufficiently dense. We extend our theoretical treatment to any finite group, including non-abelian groups. Our framework also applies to equivariant architectures (e.g., CNNs), and recovers their success in the special case where the architecture matches the inherent symmetry of the data. Empirically, our theory reproduces the generalization failure of finite-width networks (MLP, CNN, ViT) trained on partially observed versions of rotated-MNIST. We conclude that conventional deep networks lack a mechanism to learn symmetries that have not been explicitly embedded in their architecture a priori. In the future, our framework could be extended to guide the design of architectures and training procedures able to learn symmetries from data.},
  archive      = {J_JMLR},
  author       = {Andrea Perin and Stephane Deny},
  journal      = {Journal of Machine Learning Research},
  number       = {145},
  pages        = {1-70},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On the ability of deep networks to learn symmetries from data: A neural kernel theory},
  url          = {https://jmlr.org/papers/v26/24-2175.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained analysis and faster algorithms for iteratively solving linear systems. <em>JMLR</em>, <em>26</em>(144), 1-49. (<a href='https://jmlr.org/papers/v26/24-1906.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite being a key bottleneck in many machine learning tasks, the cost of solving large linear systems has proven challenging to quantify due to problem-dependent quantities such as condition numbers. To tackle this, we consider a fine-grained notion of complexity for solving linear systems, which is motivated by applications where the data exhibits low-dimensional structure, including spiked covariance models and kernel machines, and when the linear system is explicitly regularized, such as ridge regression. Concretely, let $\kappa_\ell$ be the ratio between the $\ell$th largest and the smallest singular value of $n\times n$ matrix $A$. We give a stochastic algorithm based on the Sketch-and-Project paradigm, that solves the linear system $Ax=b$ in time $\tilde O(\kappa_\ell\cdot n^2\log1/\epsilon)$ for any $\ell = O(n^{0.729})$. This is a direct improvement over preconditioned conjugate gradient, and it provides a stronger separation between stochastic linear solvers and algorithms accessing $A$ only through matrix-vector products. Our main technical contribution is the new analysis of the first and second moments of the random projection matrix that arises in Sketch-and-Project.},
  archive      = {J_JMLR},
  author       = {Michal Dereziński and Daniel LeJeune and Deanna Needell and Elizaveta Rebrova},
  journal      = {Journal of Machine Learning Research},
  number       = {144},
  pages        = {1-49},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Fine-grained analysis and faster algorithms for iteratively solving linear systems},
  url          = {https://jmlr.org/papers/v26/24-1906.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep generative models: Complexity, dimensionality, and approximation. <em>JMLR</em>, <em>26</em>(143), 1-37. (<a href='https://jmlr.org/papers/v26/24-1335.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative networks have shown remarkable success in learning complex data distributions, particularly in generating high-dimensional data from lower-dimensional inputs. While this capability is well-documented empirically, its theoretical underpinning remains unclear. One common theoretical explanation appeals to the widely accepted manifold hypothesis, which suggests that many real-world datasets, such as images and signals, often possess intrinsic low-dimensional geometric structures. Under this manifold hypothesis, it is widely believed that to approximate a distribution on a $d$-dimensional Riemannian manifold, the latent dimension needs to be at least $d$ or $d+1$. In this work, we show that this requirement on the latent dimension is not necessary by demonstrating that generative networks can approximate distributions on $d$-dimensional Riemannian manifolds from inputs of any arbitrary dimension, even lower than $d$, taking inspiration from the concept of space-filling curves. This approach, in turn, leads to a super-exponential complexity bound of the deep neural networks through expanded neurons. Our findings thus challenge the conventional belief on the relationship between input dimensionality and the ability of generative networks to model data distributions. This novel insight not only corroborates the practical effectiveness of generative networks in handling complex data structures, but also underscores a critical trade-off between approximation error, dimensionality, and model complexity.},
  archive      = {J_JMLR},
  author       = {Kevin Wang and Hongqian Niu and Yixin Wang and Didong Li},
  journal      = {Journal of Machine Learning Research},
  number       = {143},
  pages        = {1-37},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Deep generative models: Complexity, dimensionality, and approximation},
  url          = {https://jmlr.org/papers/v26/24-1335.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ClimSim-online: A large multi-scale dataset and framework for hybrid physics-ML climate emulation. <em>JMLR</em>, <em>26</em>(142), 1-85. (<a href='https://jmlr.org/papers/v26/24-1014.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern climate projections lack adequate spatial and temporal resolution due to computational constraints, leading to inaccuracies in representing critical processes like thunderstorms that occur on the sub-resolution scale. Hybrid methods combining physics with machine learning (ML) offer faster, higher fidelity climate simulations by outsourcing compute-hungry, high-resolution simulations to ML emulators. However, these hybrid physics-ML simulations require domain-specific data and workflows that have been inaccessible to many ML experts. This paper is an extended version of our NeurIPS award-winning ClimSim dataset paper. The ClimSim dataset includes 5.7 billion pairs of multivariate input/output vectors spanning ten years at high temporal resolution, capturing the influence of high-resolution, high-fidelity physics on a host climate simulator's macro-scale state. In this extended version, we introduce a significant new contribution in Section 5, which provides a cross-platform, containerized pipeline to integrate ML models into operational climate simulators for hybrid testing. We also implement various baselines of ML models and hybrid simulators to highlight the ML challenges of building stable, skillful emulators. The data (https://huggingface.co/datasets/LEAP/ClimSim_high-res, also in a low-resolution version at https://huggingface.co/datasets/LEAP/ClimSim_low-res and an aquaplanet version at https://huggingface.co/datasets/LEAP/ClimSim_low-res_aqua-planet) and code (https://leap-stc.github.io/ClimSim and https://github.com/leap-stc/climsim-online) are publicly released to support the development of hybrid physics-ML and high-fidelity climate simulations.},
  archive      = {J_JMLR},
  author       = {Sungduk Yu and Zeyuan Hu and Akshay Subramaniam and Walter Hannah and Liran Peng and Jerry Lin and Mohamed Aziz Bhouri and Ritwik Gupta and Björn Lütjens and Justus C. Will and Gunnar Behrens and Julius J. M. Busecke and Nora Loose and Charles I Stern and Tom Beucler and Bryce Harrop and Helge Heuer and Benjamin R Hillman and Andrea Jenney and Nana Liu and Alistair White and Tian Zheng and Zhiming Kuang and Fiaz Ahmed and Elizabeth Barnes and Noah D. Brenowitz and Christopher Bretherton and Veronika Eyring and Savannah Ferretti and Nicholas Lutsko and Pierre Gentine and Stephan Mandt and J. David Neelin and Rose Yu and Laure Zanna and Nathan M. Urban and Janni Yuval and Ryan Abernathey and Pierre Baldi and Wayne Chuang and Yu Huang and Fernando Iglesias-Suarez and Sanket Jantre and Po-Lun Ma and Sara Shamekh and Guang Zhang and Michael Pritchard},
  journal      = {Journal of Machine Learning Research},
  number       = {142},
  pages        = {1-85},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {ClimSim-online: A large multi-scale dataset and framework for hybrid physics-ML climate emulation},
  url          = {https://jmlr.org/papers/v26/24-1014.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional wasserstein distances with applications in bayesian OT flow matching. <em>JMLR</em>, <em>26</em>(141), 1-47. (<a href='https://jmlr.org/papers/v26/24-0586.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback–Leibler divergence, the same in general does not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 distance resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance. Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation.},
  archive      = {J_JMLR},
  author       = {Jannis Chemseddine and Paul Hagemann and Gabriele Steidl and Christian Wald},
  journal      = {Journal of Machine Learning Research},
  number       = {141},
  pages        = {1-47},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Conditional wasserstein distances with applications in bayesian OT flow matching},
  url          = {https://jmlr.org/papers/v26/24-0586.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep variational multivariate information bottleneck - A framework for variational losses. <em>JMLR</em>, <em>26</em>(140), 1-50. (<a href='https://jmlr.org/papers/v26/24-0204.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational dimensionality reduction methods are widely used for their accuracy, generative capabilities, and robustness. We introduce a unifying framework that generalizes both such as traditional and state-of-the-art methods. The framework is based on an interpretation of the multivariate information bottleneck, trading off the information preserved in an encoder graph (defining what to compress) against that in a decoder graph (defining a generative model for data). Using this approach, we rederive existing methods, including the deep variational information bottleneck, variational autoencoders, and deep multiview information bottleneck. We naturally extend the deep variational CCA (DVCCA) family to beta-DVCCA and introduce a new method, the deep variational symmetric information bottleneck (DVSIB). DSIB, the deterministic limit of DVSIB, connects to modern contrastive learning approaches such as Barlow Twins, among others. We evaluate these methods on Noisy MNIST and Noisy CIFAR-100, showing that algorithms better matched to the structure of the problem like DVSIB and beta-DVCCA produce better latent spaces as measured by classification accuracy, dimensionality of the latent variables, sample efficiency, and consistently outperform other approaches under comparable conditions. Additionally, we benchmark against state-of-the-art models, achieving superior or competitive accuracy. Our results demonstrate that this framework can seamlessly incorporate diverse multi-view representation learning algorithms, providing a foundation for designing novel, problem-specific loss functions.},
  archive      = {J_JMLR},
  author       = {Eslam Abdelaleem and Ilya Nemenman and K. Michael Martini},
  journal      = {Journal of Machine Learning Research},
  number       = {140},
  pages        = {1-50},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Deep variational multivariate information bottleneck - A framework for variational losses},
  url          = {https://jmlr.org/papers/v26/24-0204.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffeomorphism-based feature learning using poincaré inequalities on augmented input space. <em>JMLR</em>, <em>26</em>(139), 1-31. (<a href='https://jmlr.org/papers/v26/23-1707.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a gradient-enhanced algorithm for high-dimensional function approximation. The algorithm proceeds in two steps: firstly, we reduce the input dimension by learning the relevant input features from gradient evaluations, and secondly, we regress the function output against the pre-learned features. To ensure theoretical guarantees, we construct the feature map as the first components of a diffeomorphism, which we learn by minimizing an error bound obtained using Poincaré Inequality applied either in the input space or in the feature space. This leads to two different strategies, which we compare both theoretically and numerically and relate to existing methods in the literature. In addition, we propose a dimension augmentation trick to increase the approximation power of feature detection. A generalization to vector-valued functions demonstrate that our methodology directly applies to learning autoencoders. Here, we approximate the identity function over a given dataset by a composition of feature map (encoder) with the regression function (decoder). In practice, we construct the diffeomorphism using coupling flows, a particular class of invertible neural networks. Numerical experiments on various high-dimensional functions show that the proposed algorithm outperforms state-of-the-art competitors, especially with small datasets.},
  archive      = {J_JMLR},
  author       = {Romain Verdière and Clémentine Prieur and Olivier Zahm},
  journal      = {Journal of Machine Learning Research},
  number       = {139},
  pages        = {1-31},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Diffeomorphism-based feature learning using poincaré inequalities on augmented input space},
  url          = {https://jmlr.org/papers/v26/23-1707.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite expression method for solving high-dimensional partial differential equations. <em>JMLR</em>, <em>26</em>(138), 1-31. (<a href='https://jmlr.org/papers/v26/23-1290.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing efficient and accurate numerical solvers for high-dimensional partial differential equations (PDEs) remains a challenging and important topic in computational science and engineering, mainly due to the "curse of dimensionality" in designing numerical schemes that scale in dimension. This paper introduces a new methodology that seeks an approximate PDE solution in the space of functions with finitely many analytic expressions and, hence, this methodology is named the finite expression method (FEX). It is proved in approximation theory that FEX can avoid the curse of dimensionality. As a proof of concept, a deep reinforcement learning method is proposed to implement FEX for various high-dimensional PDEs in different dimensions, achieving high and even machine accuracy with a memory complexity polynomial in dimension and an amenable time complexity. An approximate solution with finite analytic expressions also provides interpretable insights into the ground truth PDE solution, which can further help to advance the understanding of physical systems and design postprocessing techniques for a refined solution.},
  archive      = {J_JMLR},
  author       = {Senwei Liang and Haizhao Yang},
  journal      = {Journal of Machine Learning Research},
  number       = {138},
  pages        = {1-31},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Finite expression method for solving high-dimensional partial differential equations},
  url          = {https://jmlr.org/papers/v26/23-1290.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomly projected convex clustering model: Motivation, realization, and cluster recovery guarantees. <em>JMLR</em>, <em>26</em>(137), 1-57. (<a href='https://jmlr.org/papers/v26/23-0384.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a randomly projected convex clustering model for clustering a collection of $n$ high dimensional data points in $\mathbb{R}^d$ with $K$ hidden clusters. Compared to the convex clustering model for clustering original data with dimension $d$, we prove that, under some mild conditions, the perfect recovery of the cluster membership assignments of the convex clustering model, if exists, can be preserved by the randomly projected convex clustering model with embedding dimension $m = O(\epsilon^{-2}\log(n))$, where $\epsilon > 0$ is some given parameter. We further prove that the embedding dimension can be improved to be $O(\epsilon^{-2}\log(K))$, which is independent of the number of data points. We also establish the recovery guarantees of our proposed model with uniform weights for clustering a mixture of spherical Gaussians. Extensive numerical results demonstrate the robustness and superior performance of the randomly projected convex clustering model. The numerical results will also demonstrate that the randomly projected convex clustering model can outperform other popular clustering models on the dimension-reduced data, including the randomly projected K-means model.},
  archive      = {J_JMLR},
  author       = {Ziwen Wang and Yancheng Yuan and Jiaming Ma and Tieyong Zeng and Defeng Sun},
  journal      = {Journal of Machine Learning Research},
  number       = {137},
  pages        = {1-57},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Randomly projected convex clustering model: Motivation, realization, and cluster recovery guarantees},
  url          = {https://jmlr.org/papers/v26/23-0384.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimax optimal deep neural network classifiers under smooth decision boundary. <em>JMLR</em>, <em>26</em>(136), 1-38. (<a href='https://jmlr.org/papers/v26/22-0758.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has gained huge empirical successes in large-scale classification problems. In contrast, there is a lack of statistical understanding about deep learning methods, particularly in the minimax optimality perspective. For instance, in the classical smooth decision boundary setting, existing deep neural network (DNN) approaches are rate-suboptimal, and it remains elusive how to construct minimax optimal DNN classifiers. Moreover, it is interesting to explore whether DNN classifiers can circumvent the "curse of dimensionality" in handling high-dimensional data. The contributions of this paper are two-fold. First, based on a localized margin framework, we discover the source of suboptimality of existing DNN approaches. Motivated by this, we propose a new deep learning classifier using a divide-and-conquer technique: DNN classifiers are constructed on each local region and then aggregated to a global one. We further propose a localized version of the classical Tsybakov’s noise condition, under which statistical optimality of our new classifier is established. Second, we show that DNN classifiers can adapt to low-dimensional data structures and circumvent the “curse of dimensionality” in the sense that the minimax rate only depends on the effective dimension, potentially much smaller than the actual data dimension. Numerical experiments are conducted on simulated data to corroborate our theoretical results.},
  archive      = {J_JMLR},
  author       = {Tianyang Hu and Ruiqi Liu and Zuofeng Shang and Guang Cheng},
  journal      = {Journal of Machine Learning Research},
  number       = {136},
  pages        = {1-38},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Minimax optimal deep neural network classifiers under smooth decision boundary},
  url          = {https://jmlr.org/papers/v26/22-0758.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal and efficient algorithms for decentralized online convex optimization. <em>JMLR</em>, <em>26</em>(135), 1-43. (<a href='https://jmlr.org/papers/v26/24-2137.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications. Previous studies have established $O(n^{5/4}\rho^{-1/2}\sqrt{T})$ and ${O}(n^{3/2}\rho^{-1}\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\rho<1$ is the spectral gap of the communication matrix, and $T$ is the time horizon. However, there exist large gaps from the existing lower bounds, i.e., $\Omega(n\sqrt{T})$ for convex functions and $\Omega(n)$ for strongly convex functions. To fill these gaps, in this paper, we first develop a novel D-OCO algorithm that can respectively reduce the regret bounds for convex and strongly convex functions to $\tilde{O}(n\rho^{-1/4}\sqrt{T})$ and $\tilde{O}(n\rho^{-1/2}\log T)$. The primary technique is to design an online accelerated gossip strategy that enjoys a faster average consensus among local learners. Furthermore, by carefully exploiting spectral properties of a specific network topology, we enhance the lower bounds for convex and strongly convex functions to $\Omega(n\rho^{-1/4}\sqrt{T})$ and $\Omega(n\rho^{-1/2}\log T)$, respectively. These results suggest that the regret of our algorithm is nearly optimal in terms of $T$, $n$, and $\rho$ for both convex and strongly convex functions. Finally, we propose a projection-free variant of our algorithm to efficiently handle practical applications with complex constraints. Our analysis reveals that the projection-free variant can achieve ${O}(nT^{3/4})$ and ${O}(nT^{2/3}(\log T)^{1/3})$ regret bounds for convex and strongly convex functions with nearly optimal $\tilde{O}(\rho^{-1/2}\sqrt{T})$ and $\tilde{O}(\rho^{-1/2}T^{1/3}(\log T)^{2/3})$ communication rounds, respectively.},
  archive      = {J_JMLR},
  author       = {Yuanyu Wan and Tong Wei and Bo Xue and Mingli Song and Lijun Zhang},
  journal      = {Journal of Machine Learning Research},
  number       = {135},
  pages        = {1-43},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Optimal and efficient algorithms for decentralized online convex optimization},
  url          = {https://jmlr.org/papers/v26/24-2137.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing dynamical stability of stochastic gradient descent in overparameterized learning. <em>JMLR</em>, <em>26</em>(134), 1-46. (<a href='https://jmlr.org/papers/v26/24-1547.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For overparameterized optimization tasks, such as those found in modern machine learning, global minima are generally not unique. In order to understand generalization in these settings, it is vital to study to which minimum an optimization algorithm converges. The possibility of having minima that are unstable under the dynamics imposed by the optimization algorithm limits the potential minima that the algorithm can find. In this paper, we characterize the global minima that are dynamically stable/unstable for both deterministic and stochastic gradient descent (SGD). In particular, we introduce a characteristic Lyapunov exponent that depends on the local dynamics around a global minimum and rigorously prove that the sign of this Lyapunov exponent determines whether SGD can accumulate at the respective global minimum.},
  archive      = {J_JMLR},
  author       = {Dennis Chemnitz and Maximilian Engel},
  journal      = {Journal of Machine Learning Research},
  number       = {134},
  pages        = {1-46},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Characterizing dynamical stability of stochastic gradient descent in overparameterized learning},
  url          = {https://jmlr.org/papers/v26/24-1547.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PREMAP: A unifying PREiMage APproximation framework for neural networks. <em>JMLR</em>, <em>26</em>(133), 1-44. (<a href='https://jmlr.org/papers/v26/24-1297.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most methods for neural network verification focus on bounding the image, i.e., set of outputs for a given input set. This can be used to, for example, check the robustness of neural network predictions to bounded perturbations of an input. However, verifying properties concerning the preimage, i.e., the set of inputs satisfying an output property, requires abstractions in the input space. We present a general framework for preimage abstraction that produces under- and over-approximations of any polyhedral output set. Our framework employs cheap parameterised linear relaxations of the neural network, together with an anytime refinement procedure that iteratively partitions the input region by splitting on input features and neurons. The effectiveness of our approach relies on carefully designed heuristics and optimisation objectives to achieve rapid improvements in the approximation volume. We evaluate our method on a range of tasks, demonstrating significant improvement in efficiency and scalability to high-input-dimensional image classification tasks compared to state-of-the-art techniques. Further, we showcase the application to quantitative verification and robustness analysis, presenting a sound and complete algorithm for the former and providing sound quantitative results for the latter.},
  archive      = {J_JMLR},
  author       = {Xiyue Zhang and Benjie Wang and Marta Kwiatkowska and Huan Zhang},
  journal      = {Journal of Machine Learning Research},
  number       = {133},
  pages        = {1-44},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {PREMAP: A unifying PREiMage APproximation framework for neural networks},
  url          = {https://jmlr.org/papers/v26/24-1297.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Score-aware policy-gradient and performance guarantees using local lyapunov stability. <em>JMLR</em>, <em>26</em>(132), 1-74. (<a href='https://jmlr.org/papers/v26/24-1009.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a policy-gradient method for model-based reinforcement learning (RL) that exploits a type of stationary distributions commonly obtained from Markov decision processes (MDPs) in stochastic networks, queueing systems, and statistical mechanics. Specifically, when the stationary distribution of the MDP belongs to an exponential family that is parametrized by policy parameters, we can improve existing policy gradient methods for average-reward RL. Our key identification is a family of gradient estimators, called score-aware gradient estimators (SAGEs), that enable policy gradient estimation without relying on value-function estimation in the aforementioned setting. We show that SAGE-based policy-gradient locally converges, and we obtain its regret. This includes cases when the state space of the MDP is countable and unstable policies can exist. Under appropriate assumptions such as starting sufficiently close to a maximizer and the existence of a local Lyapunov function, the policy under SAGE-based stochastic gradient ascent has an overwhelming probability of converging to the associated optimal policy. Furthermore, we conduct a numerical comparison between a SAGE-based policy-gradient method and an actor-critic method on several examples inspired from stochastic networks, queueing systems, and models derived from statistical physics. Our results demonstrate that a SAGE-based method finds close-to-optimal policies faster than an actor-critic method.},
  archive      = {J_JMLR},
  author       = {Céline Comte and Matthieu Jonckheere and Jaron Sanders and Albert Senen-Cerda},
  journal      = {Journal of Machine Learning Research},
  number       = {132},
  pages        = {1-74},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Score-aware policy-gradient and performance guarantees using local lyapunov stability},
  url          = {https://jmlr.org/papers/v26/24-1009.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the O(sqrt(d)/T^(1/4)) convergence rate of RMSProp and its momentum extension measured by l_1 norm. <em>JMLR</em>, <em>26</em>(131), 1-25. (<a href='https://jmlr.org/papers/v26/24-0523.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although adaptive gradient methods have been extensively used in deep learning, their convergence rates proved in the literature are all slower than that of SGD, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\frac{1}{T}\sum_{k=1}^TE\left[||\nabla f(\mathbf{x}^k)||_1\right]\leq O(\frac{\sqrt{d}C}{T^{1/4}})$ measured by $\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable, $T$ is the iteration number, and $C$ is a constant identical to that appeared in the optimal convergence rate of SGD. Our convergence rate matches the lower bound with respect to all the coefficients except the dimension $d$. Since $||\mathbf{x}||_2\ll ||\mathbf{x}||_1\leq\sqrt{d}||\mathbf{x}||_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\frac{1}{T}\sum_{k=1}^TE\left[||\nabla f(\mathbf{x}^k)||_2\right]\leq O(\frac{C}{T^{1/4}})$ rate of SGD in the ideal case of $||\nabla f(\mathbf{x})||_1=\varTheta(\sqrt{d})||\nabla f(\mathbf{x})||_2$.},
  archive      = {J_JMLR},
  author       = {Huan Li and Yiming Dong and Zhouchen Lin},
  journal      = {Journal of Machine Learning Research},
  number       = {131},
  pages        = {1-25},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On the O(sqrt(d)/T^(1/4)) convergence rate of RMSProp and its momentum extension measured by l_1 norm},
  url          = {https://jmlr.org/papers/v26/24-0523.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Categorical semantics of compositional reinforcement learning. <em>JMLR</em>, <em>26</em>(130), 1-37. (<a href='https://jmlr.org/papers/v26/24-0197.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional knowledge representations in reinforcement learning (RL) facilitate modular, interpretable, and safe task specifications. However, generating compositional models requires the characterization of minimal assumptions for the robustness of the compositionality feature, especially in the case of functional decompositions. Using a categorical point of view, we develop a knowledge representation framework for a compositional theory of RL. Our approach relies on the theoretical study of the category $\mathsf{MDP}$, whose objects are Markov decision processes (MDPs) acting as models of tasks. The categorical semantics models the compositionality of tasks through the application of pushout operations akin to combining puzzle pieces. As a practical application of these pushout operations, we introduce zig-zag diagrams that rely on the compositional guarantees engendered by the category $\mathsf{MDP}$. We further prove that properties of the category $\mathsf{MDP}$ unify concepts, such as enforcing safety requirements and exploiting symmetries, generalizing previous abstraction theories for RL.},
  archive      = {J_JMLR},
  author       = {Georgios Bakirtzis and Michail Savvas and Ufuk Topcu},
  journal      = {Journal of Machine Learning Research},
  number       = {130},
  pages        = {1-37},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Categorical semantics of compositional reinforcement learning},
  url          = {https://jmlr.org/papers/v26/24-0197.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformers from diffusion: A unified framework for neural message passing. <em>JMLR</em>, <em>26</em>(129), 1-47. (<a href='https://jmlr.org/papers/v26/23-1672.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning representations for structured data with certain geometries (e.g., observed or unobserved) is a fundamental challenge, wherein message passing neural networks (MPNNs) have become a de facto class of model solutions. In this paper, inspired by physical systems, we propose an energy-constrained diffusion model, which integrates the inductive bias of diffusion on manifolds with layer-wise constraints of energy minimization. We identify that the diffusion operators have a one-to-one correspondence with the energy functions implicitly descended by the diffusion process, and the finite-difference iteration for solving the energy-constrained diffusion system induces the propagation layers of various types of MPNNs operating on observed or latent structures. This leads to a unified mathematical framework for common neural architectures whose computational flows can be cast as message passing (or its special case), including MLPs, GNNs, and Transformers. Building on these insights, we devise a new class of neural message passing models, dubbed diffusion-inspired Transformers (DIFFormer), whose global attention layers are derived from the principled energy-constrained diffusion framework. Across diverse datasets ranging from real-world networks to images, texts, and physical particles, we demonstrate that the new model achieves promising performance in scenarios where the data structures are observed (as a graph), partially observed, or entirely unobserved.},
  archive      = {J_JMLR},
  author       = {Qitian Wu and David Wipf and Junchi Yan},
  journal      = {Journal of Machine Learning Research},
  number       = {129},
  pages        = {1-47},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Transformers from diffusion: A unified framework for neural message passing},
  url          = {https://jmlr.org/papers/v26/23-1672.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal sample selection through uncertainty estimation and its application in deep learning. <em>JMLR</em>, <em>26</em>(128), 1-47. (<a href='https://jmlr.org/papers/v26/23-1160.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep learning heavily relies on large labeled datasets, which often comse with high costs in terms of both manual labeling and computational resources. To mitigate these challenges, researchers have explored the use of informative subset selection techniques. In this study, we present a theoretically optimal solution for addressing both sampling with and without labels within the context of linear softmax regression. Our proposed method, COPS (unCertainty based OPtimal Sub-sampling), is designed to minimize the expected loss of a model trained on subsampled data. Unlike existing approaches that rely on explicit calculations of the inverse covariance matrix, which are not easily applicable to deep learning scenarios, COPS leverages the model's logits to estimate the sampling ratio. This sampling ratio is closely associated with model uncertainty and can be effectively applied to deep learning tasks. Furthermore, we address the challenge of model sensitivity to misspecification by incorporating a down-weighting approach for low-density samples, drawing inspiration from previous works. To assess the effectiveness of our proposed method, we conducted extensive empirical experiments using deep neural networks on benchmark datasets. The results consistently showcase the superior performance of COPS compared to baseline methods, reaffirming its efficacy.},
  archive      = {J_JMLR},
  author       = {Yong Lin and Chen Liu and Chenlu Ye and Qing Lian and Yuan Yao and Tong Zhang},
  journal      = {Journal of Machine Learning Research},
  number       = {128},
  pages        = {1-47},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Optimal sample selection through uncertainty estimation and its application in deep learning},
  url          = {https://jmlr.org/papers/v26/23-1160.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Actor-critic learning for mean-field control in continuous time. <em>JMLR</em>, <em>26</em>(127), 1-42. (<a href='https://jmlr.org/papers/v26/23-0345.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study policy gradient for mean-field control in continuous time in a reinforcement learning setting. By considering randomised policies with entropy regularisation, we derive a gradient expectation representation of the value function, which is amenable to actor-critic type algorithms, where the value functions and the policies are learnt alternately based on observation samples of the state and model-free estimation of the population state distribution, either by offline or online learning. In the linear-quadratic mean-field framework, we obtain an exact parametrisation of the actor and critic functions defined on the Wasserstein space. Finally, we illustrate the results of our algorithms with some numerical experiments on concrete examples.},
  archive      = {J_JMLR},
  author       = {Noufel FRIKHA and Maximilien GERMAIN and Mathieu LAURIERE and Huyen PHAM and Xuanye SONG},
  journal      = {Journal of Machine Learning Research},
  number       = {127},
  pages        = {1-42},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Actor-critic learning for mean-field control in continuous time},
  url          = {https://jmlr.org/papers/v26/23-0345.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling populations of interaction networks via distance metrics. <em>JMLR</em>, <em>26</em>(126), 1-112. (<a href='https://jmlr.org/papers/v26/22-0706.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network data arises through the observation of relational information between a collection of entities, for example, friendships (relations) amongst a sample of people (entities). Traditionally, statistical models of such data have been developed to analyse a single network, that is, a single collection of entities and relations. More recently, attention has shifted to analysing samples of networks. A driving force has been the analysis of connectome data, arising in neuroscience applications, where a single network is observed for each patient in a study. These models typically assume, within each network, the entities are the units of observation, that is, more data equates to including more entities. However, an alternative paradigm considers relations—such as edges or paths—as the observational units, exemplified by email exchanges or user navigations across a website. This interaction network framework has generally been applied to single networks, without extending to the case where multiple such networks are observed, for instance, analysing navigation patterns from many users. Motivated by this gap, we propose a new Bayesian modelling framework to analyse such data. Our approach is based on practitioner-specified distance metrics between networks, allowing us to parameterise models analogous to Gaussian distributions in network space, using location and scale parameters. We address the key challenge of defining meaningful distances between interaction networks, proposing two new metrics with theoretical guarantees and practical computation strategies. To enable efficient Bayesian inference, we develop specialised Markov chain Monte Carlo (MCMC) algorithms within the involutive MCMC (iMCMC) framework, tailored to the doubly-intractable and discrete nature of the induced posteriors. Through simulation studies, we demonstrate the robustness and efficiency of our approach, and we showcase its applicability with a case study on a location-based social network (LSBN) dataset.},
  archive      = {J_JMLR},
  author       = {George Bolt and Simón Lunagómez and Christopher Nemeth},
  journal      = {Journal of Machine Learning Research},
  number       = {126},
  pages        = {1-112},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Modelling populations of interaction networks via distance metrics},
  url          = {https://jmlr.org/papers/v26/22-0706.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BitNet: 1-bit pre-training for large language models. <em>JMLR</em>, <em>26</em>(125), 1-29. (<a href='https://jmlr.org/papers/v26/24-2050.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing size of large language models (LLMs) has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. Previous research typically applies quantization after pre-training. While these methods avoid the need for model retraining, they often cause notable accuracy loss at extremely low bit-widths. In this work, we explore the feasibility and scalability of 1-bit pre-training. We introduce BitNet b1 and BitNet b1.58, the scalable and stable 1-bit Transformer architecture designed for LLMs. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results show that BitNet b1 achieves competitive performance, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. With the ternary weight, BitNet b1.58 matches the half-precision Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, BitNet defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. It enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
  archive      = {J_JMLR},
  author       = {Hongyu Wang and Shuming Ma and Lingxiao Ma and Lei Wang and Wenhui Wang and Li Dong and Shaohan Huang and Huaijie Wang and Jilong Xue and Ruiping Wang and Yi Wu and Furu Wei},
  journal      = {Journal of Machine Learning Research},
  number       = {125},
  pages        = {1-29},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {BitNet: 1-bit pre-training for large language models},
  url          = {https://jmlr.org/papers/v26/24-2050.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-informed kernel learning. <em>JMLR</em>, <em>26</em>(124), 1-39. (<a href='https://jmlr.org/papers/v26/24-1536.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-informed machine learning typically integrates physical priors into the learning process by minimizing a loss function that includes both a data-driven term and a partial differential equation (PDE) regularization. Building on the formulation of the problem as a kernel regression task, we use Fourier methods to approximate the associated kernel, and propose a tractable estimator that minimizes the physics-informed risk function. We refer to this approach as physics-informed kernel learning (PIKL). This framework provides theoretical guarantees, enabling the quantification of the physical prior’s impact on convergence speed. We demonstrate the numerical performance of the PIKL estimator through simulations, both in the context of hybrid modeling and in solving PDEs. In particular, we show that PIKL can outperform physics-informed neural networks in terms of both accuracy and computation time. Additionally, we identify cases where PIKL surpasses traditional PDE solvers, particularly in scenarios with noisy boundary conditions.},
  archive      = {J_JMLR},
  author       = {Nathan Doumèche and Francis Bach and Gérard Biau and Claire Boyer},
  journal      = {Journal of Machine Learning Research},
  number       = {124},
  pages        = {1-39},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Physics-informed kernel learning},
  url          = {https://jmlr.org/papers/v26/24-1536.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Last-iterate convergence of shuffling momentum gradient method under the kurdyka-lojasiewicz inequality. <em>JMLR</em>, <em>26</em>(123), 1-51. (<a href='https://jmlr.org/papers/v26/24-1243.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shuffling gradient algorithms are extensively used to solve finite-sum optimization problems in machine learning. However, their theoretical properties still need to be further explored, especially the last-iterate convergence in the non-convex setting. In this paper, we study the last-iterate convergence behavior of shuffling momentum gradient (SMG) method, a shuffling gradient algorithm with momentum. Specifically, we focus on the non-convex scenario and provide theoretical guarantees under arbitrary shuffling strategies. For non-convex objectives, we achieve the convergence of gradient norms at the last-iterate, showing that every accumulation point of the iterative sequence is a stationary point of the non-convex problem. Our analysis also reveals that the function values of the last-iterate converge to a finite value. Additionally, we obtain the asymptotic convergence rates of gradient norms at the minimum-iterate. By employing a uniform without-replacement sampling strategy, we further achieve an improved convergence rate for the minimum-iterate output. Under the Kurdyka-Lojasiewicz (KL) inequality, we establish the challenging strong limit-point convergence results. In particular, we prove that the whole sequence of iterates exhibits convergence to a stationary point of the finite-sum problem. By choosing an appropriate stepsize, we also obtain the corresponding rate of last-iterate convergence, matching available results in the strongly convex setting. Given that the last iteration is typically preferred as the output of the algorithm in applied scenarios, this paper contributes to narrowing the gap between theory and practice.},
  archive      = {J_JMLR},
  author       = {Yuqing Liang and Dongpo Xu},
  journal      = {Journal of Machine Learning Research},
  number       = {123},
  pages        = {1-51},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Last-iterate convergence of shuffling momentum gradient method under the kurdyka-lojasiewicz inequality},
  url          = {https://jmlr.org/papers/v26/24-1243.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Posterior and variational inference for deep neural networks with heavy-tailed weights. <em>JMLR</em>, <em>26</em>(122), 1-58. (<a href='https://jmlr.org/papers/v26/24-0894.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider deep neural networks in a Bayesian framework with a prior distribution sampling the network weights at random. Following a recent idea of Agapiou and Castillo (2024), who show that heavy-tailed prior distributions achieve automatic adaptation to smoothness, we introduce a simple Bayesian deep learning prior based on heavy-tailed weights and ReLU activation. We show that the corresponding posterior distribution achieves near-optimal minimax contraction rates, simultaneously adaptive to both intrinsic dimension and smoothness of the underlying function, in a variety of contexts including nonparametric regression, geometric data and Besov spaces. While most works so far need a form of model selection built-in within the prior distribution, a key aspect of our approach is that it does not require to sample hyperparameters to learn the architecture of the network. We also provide variational Bayes counterparts of the results, that show that mean-field variational approximations still benefit from near-optimal theoretical support.},
  archive      = {J_JMLR},
  author       = {Paul Egels and Ismaël Castillo},
  journal      = {Journal of Machine Learning Research},
  number       = {122},
  pages        = {1-58},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Posterior and variational inference for deep neural networks with heavy-tailed weights},
  url          = {https://jmlr.org/papers/v26/24-0894.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum causal entropy IRL in mean-field games and GNEP framework for forward RL. <em>JMLR</em>, <em>26</em>(121), 1-40. (<a href='https://jmlr.org/papers/v26/24-0458.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the use of Maximum Causal Entropy Inverse Reinforcement Learning (IRL) within the context of discrete-time stationary Mean-Field Games (MFGs) characterized by finite state spaces and an infinite-horizon, discounted-reward setting. Although the resulting optimization problem is non-convex with respect to policies, we reformulate it as a convex optimization problem in terms of state-action occupation measures by leveraging the linear programming framework of Markov Decision Processes. Based on this convex reformulation, we introduce a gradient descent algorithm with a guaranteed convergence rate to efficiently compute the optimal solution. Moreover, we develop a new method that conceptualizes the MFG problem as a Generalized Nash Equilibrium Problem (GNEP), enabling effective computation of the mean-field equilibrium for forward reinforcement learning (RL) problems and marking an advancement in MFG solution techniques. We further illustrate the practical applicability of our GNEP approach by employing this algorithm to generate data for numerical MFG examples.},
  archive      = {J_JMLR},
  author       = {Berkay Anahtarci and Can Deha Kariksiz and Naci Saldi},
  journal      = {Journal of Machine Learning Research},
  number       = {121},
  pages        = {1-40},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Maximum causal entropy IRL in mean-field games and GNEP framework for forward RL},
  url          = {https://jmlr.org/papers/v26/24-0458.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Degree of interference: A general framework for causal inference under interference. <em>JMLR</em>, <em>26</em>(120), 1-37. (<a href='https://jmlr.org/papers/v26/24-0119.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One core assumption typically adopted for valid causal inference is that of no interference between experimental units, i.e., the outcome of an experimental unit is unaffected by the treatments assigned to other experimental units. This assumption can be violated in real-life experiments, which significantly complicates the task of causal inference. As the number of potential outcomes increases, it becomes challenging to disentangle direct treatment effects from “spillover” effects. Current methodologies are lacking, as they cannot handle arbitrary, unknown interference structures to permit inference on causal estimands. We present a general framework to address the limitations of existing approaches. Our framework is based on the new concept of the “degree of interference” (DoI). The DoI is a unit-level latent variable that captures the latent structure of interference. We also develop a data augmentation algorithm that adopts a blocked Gibbs sampler and Bayesian nonparametric methodology to perform inferences on the estimands under our framework. We illustrate the DoI concept and properties of our Bayesian methodology via extensive simulation studies and an analysis of a randomized experiment investigating the impact of a cash transfer program for which interference is a critical concern. Ultimately, our framework enables us to infer causal effects without strong structural assumptions on interference.},
  archive      = {J_JMLR},
  author       = {Yuki Ohnishi and Bikram Karmakar and Arman Sabbaghi},
  journal      = {Journal of Machine Learning Research},
  number       = {120},
  pages        = {1-37},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Degree of interference: A general framework for causal inference under interference},
  url          = {https://jmlr.org/papers/v26/24-0119.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantifying the effectiveness of linear preconditioning in markov chain monte carlo. <em>JMLR</em>, <em>26</em>(119), 1-51. (<a href='https://jmlr.org/papers/v26/23-1633.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study linear preconditioning in Markov chain Monte Carlo. We consider the class of well-conditioned distributions, for which several mixing time bounds depend on the condition number $\kappa$. First we show that well-conditioned distributions exist for which $\kappa$ can be arbitrarily large and yet no linear preconditioner can reduce it. We then impose two sets of extra assumptions under which a linear preconditioner can significantly reduce $\kappa$. For the random walk Metropolis we further provide upper and lower bounds on the spectral gap with tight $1/\kappa$ dependence. This allows us to give conditions under which linear preconditioning can provably increase the gap. We then study popular preconditioners such as the covariance, its diagonal approximation, the Hessian at the mode, and the QR decomposition. We show conditions under which each of these reduce $\kappa$ to near its minimum. We also show that the diagonal approach can in fact increase the condition number. This is of interest as diagonal preconditioning is the default choice in well-known software packages. We conclude with a numerical study comparing preconditioners in different models, and we show how proper preconditioning can greatly reduce compute time in Hamiltonian Monte Carlo.},
  archive      = {J_JMLR},
  author       = {Max Hird and Samuel Livingstone},
  journal      = {Journal of Machine Learning Research},
  number       = {119},
  pages        = {1-51},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Quantifying the effectiveness of linear preconditioning in markov chain monte carlo},
  url          = {https://jmlr.org/papers/v26/23-1633.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse SVM with hard-margin loss: A newton-augmented lagrangian method in reduced dimensions. <em>JMLR</em>, <em>26</em>(118), 1-55. (<a href='https://jmlr.org/papers/v26/23-0953.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hard-margin loss function has been at the core of the support vector machine research from the very beginning due to its generalization capability. On the other hand, the cardinality constraint has been widely used for feature selection, leading to sparse solutions. This paper studies the sparse SVM with the hard-margin loss that integrates the virtues of both worlds, resulting in one of the most challenging models to solve. We cast the problem as a composite optimization with the cardinality constraint. We characterize its local minimizers in terms of pseudo KKT point that well captures the combinatorial structure of the problem, and investigate a sharper P-stationary point with a concise representation for algorithm design. We further develop an inexact proximal augmented Lagrangian method (iPAL). The different parts of the inexactness measurements from the {\rm P}-stationarity are controlled at different scales in a way that the generated sequence converges both globally and at a linear rate. To make iPAL practically efficient, we propose a gradient-Newton method in a subspace for the iPAL subproblem. This is accomplished by detecting active samples and features with the help of the proximal operator of the hard margin loss and the projection of the cardinality constraint. Extensive numerical results on both simulated and real data sets demonstrate that the proposed method is fast, produces sparse solution of high accuracy, and can lead to effective reduction on active samples and features when compared with several leading solvers.},
  archive      = {J_JMLR},
  author       = {Penghe Zhang and Naihua Xiu and Hou-Duo Qi},
  journal      = {Journal of Machine Learning Research},
  number       = {118},
  pages        = {1-55},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Sparse SVM with hard-margin loss: A newton-augmented lagrangian method in reduced dimensions},
  url          = {https://jmlr.org/papers/v26/23-0953.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On model identification and out-of-sample prediction of PCR with applications to synthetic controls. <em>JMLR</em>, <em>26</em>(117), 1-58. (<a href='https://jmlr.org/papers/v26/23-0102.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze principal component regression (PCR) in a high-dimensional error-in-variables setting with fixed design. Under suitable conditions, we show that PCR consistently identifies the unique model with minimum $\ell_2$-norm. These results enable us to establish non-asymptotic out-of-sample prediction guarantees that improve upon the best known rates. In the course of our analysis, we introduce a natural linear algebraic condition between the in- and out-of-sample covariates, which allows us to avoid distributional assumptions for out-of-sample predictions. Our simulations illustrate the importance of this condition for generalization, even under covariate shifts. Accordingly, we construct a hypothesis test to check when this condition holds in practice. As a byproduct, our results also lead to novel results for the synthetic controls literature, a leading approach for policy evaluation. To the best of our knowledge, our prediction guarantees for the fixed design setting have been elusive in both the high-dimensional error-in-variables and synthetic controls literatures.},
  archive      = {J_JMLR},
  author       = {Anish Agarwal and Devavrat Shah and Dennis Shen},
  journal      = {Journal of Machine Learning Research},
  number       = {117},
  pages        = {1-58},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On model identification and out-of-sample prediction of PCR with applications to synthetic controls},
  url          = {https://jmlr.org/papers/v26/23-0102.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian scalar-on-image regression with a spatially varying single-layer neural network prior. <em>JMLR</em>, <em>26</em>(116), 1-38. (<a href='https://jmlr.org/papers/v26/22-0246.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNN) have been widely used in scalar-on-image regression to predict an outcome variable from imaging predictors. However, training DNN typically requires large sample sizes for accurate prediction, and the resulting models often lack interpretability. In this work, we propose a novel Bayesian nonlinear scalar-on-image regression framework with a spatially varying single-layer neural network (SV-NN) prior. The SV-NN is constructed using a single hidden layer neural network with its weights generated by the soft-thresholded Gaussian process. Our framework enables the selection of interpretable image regions while achieving high prediction accuracy with limited training samples. The SV-NN offers large prior support for the imaging effect function, facilitating efficient posterior inference on image region selection and automatic network structures determination. We establish the posterior consistency for model parameters and selection consistency for image regions when the number of voxels/pixels grows much faster than the sample size. To ensure computational efficiency, we develop a stochastic gradient Langevin dynamics (SGLD) algorithm for posterior inference. We evaluate our method through extensive comparisons with state-of-the-art deep learning approaches, analyzing multiple real datasets, including task fMRI data from the Adolescent Brain Cognitive Development (ABCD) study.},
  archive      = {J_JMLR},
  author       = {Ben Wu and Keru Wu and Jian Kang},
  journal      = {Journal of Machine Learning Research},
  number       = {116},
  pages        = {1-38},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Bayesian scalar-on-image regression with a spatially varying single-layer neural network prior},
  url          = {https://jmlr.org/papers/v26/22-0246.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRM revisited: A complete error analysis. <em>JMLR</em>, <em>26</em>(115), 1-76. (<a href='https://jmlr.org/papers/v26/24-1258.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely known that the error analysis for deep learning involves approximation, statistical, and optimization errors. However, it is challenging to combine them together due to overparameterization. In this paper, we address this gap by providing a comprehensive error analysis of the Deep Ritz Method (DRM). Specifically, we investigate a foundational question in the theoretical analysis of DRM under the overparameterized regime: given a target precision level, how can one determine the appropriate number of training samples, the key architectural parameters of the neural networks, the step size for the projected gradient descent optimization procedure, and the requisite number of iterations, such that the output of the gradient descent process closely approximates the true solution of the underlying partial differential equation to the specified precision?},
  archive      = {J_JMLR},
  author       = {Yuling Jiao and Ruoxuan Li and Peiying Wu and Jerry Zhijian Yang and Pingwen Zhang},
  journal      = {Journal of Machine Learning Research},
  number       = {115},
  pages        = {1-76},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {DRM revisited: A complete error analysis},
  url          = {https://jmlr.org/papers/v26/24-1258.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Principled penalty-based methods for bilevel reinforcement learning and RLHF. <em>JMLR</em>, <em>26</em>(114), 1-49. (<a href='https://jmlr.org/papers/v26/24-0720.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bilevel optimization has been recently applied to many machine learning tasks. However, their applications have been restricted to the supervised learning setting, where static objective functions with benign structures are considered. But bilevel problems such as incentive design, inverse reinforcement learning (RL), and RL from human feedback (RLHF) are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. To tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel RL problems through the lens of penalty formulation. We provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. We demonstrate the effectiveness of our algorithms via simulations in the Stackelberg Markov game, RL from human feedback and incentive design.},
  archive      = {J_JMLR},
  author       = {Han Shen and Zhuoran Yang and Tianyi Chen},
  journal      = {Journal of Machine Learning Research},
  number       = {114},
  pages        = {1-49},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Principled penalty-based methods for bilevel reinforcement learning and RLHF},
  url          = {https://jmlr.org/papers/v26/24-0720.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precise high-dimensional asymptotics for quantifying heterogeneous transfers. <em>JMLR</em>, <em>26</em>(113), 1-88. (<a href='https://jmlr.org/papers/v26/24-0454.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of learning one task using samples from another task is central to transfer learning. In this paper, we focus on answering the following question: when does combining the samples from two related tasks perform better than learning with one target task alone? This question is motivated by an empirical phenomenon known as negative transfer often observed in transfer learning practice. While the transfer effect from one task to another depends on factors such as their sample sizes and the spectrum of their covariance matrices, precisely quantifying this dependence has remained a challenging problem. In order to compare a transfer learning estimator to single-task learning, one needs to compare the risks between the two estimators precisely. Further, the comparison depends on the distribution shifts between the two tasks. This paper applies recent developments of random matrix theory to tackle this challenge in a high-dimensional linear regression setting with two tasks. We provide precise high-dimensional asymptotics for the bias and variance of a classical hard parameter sharing (HPS) estimator in the proportional limit, when the sample sizes of both tasks increase proportionally with dimension at fixed ratios. The precise asymptotics apply to various types of distribution shifts, including covariate shifts, model shifts, and combinations of both. We illustrate these results in a random-effects model to mathematically prove a phase transition from positive to negative transfer as the number of source task samples increases. One insight from the analysis is that a rebalanced HPS estimator, which downsizes the source task when the model shift is high, achieves the minimax optimal rate. The finding regarding phase transition also applies to multiple tasks when feature covariates are shared across all tasks. Simulations validate the accuracy of the high-dimensional asymptotics for finite dimensions.},
  archive      = {J_JMLR},
  author       = {Fan Yang and Hongyang R. Zhang and Sen Wu and Christopher Re and Weijie J. Su},
  journal      = {Journal of Machine Learning Research},
  number       = {113},
  pages        = {1-88},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Precise high-dimensional asymptotics for quantifying heterogeneous transfers},
  url          = {https://jmlr.org/papers/v26/24-0454.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Score-based causal representation learning: Linear and general transformations. <em>JMLR</em>, <em>26</em>(112), 1-90. (<a href='https://jmlr.org/papers/v26/24-0194.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the identifiability and achievability aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure the recovery of the true latent causal variables and the underlying latent causal graph. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between score functions (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a score-based class of algorithms that ensures both identifiability and achievability. First, the paper focuses on linear transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability. It also provides partial identifiability guarantees for soft interventions, including identifiability up to mixing with parents for general causal models and perfect recovery of the latent graph for sufficiently nonlinear causal models. Secondly, it focuses on general transformations and demonstrates that two stochastic hard interventions per node are sufficient for identifiability. This is achieved by defining a differentiable loss function whose global optima ensure identifiability for general CRL. Notably, one does not need to know which pair of interventional environments has the same node intervened. Finally, the theoretical results are empirically validated via experiments on structured synthetic data and image data.},
  archive      = {J_JMLR},
  author       = {Burak Var{{\i}}c{{\i}} and Emre Acartürk and Karthikeyan Shanmugam and Abhishek Kumar and Ali Tajer},
  journal      = {Journal of Machine Learning Research},
  number       = {112},
  pages        = {1-90},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Score-based causal representation learning: Linear and general transformations},
  url          = {https://jmlr.org/papers/v26/24-0194.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the statistical properties of generative adversarial models for low intrinsic data dimension. <em>JMLR</em>, <em>26</em>(111), 1-57. (<a href='https://jmlr.org/papers/v26/24-0054.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable empirical successes of Generative Adversarial Networks (GANs), the theoretical guarantees for their statistical accuracy remain rather pessimistic. In particular, the data distributions on which GANs are applied, such as natural images, are often hypothesized to have an intrinsic low-dimensional structure in a typically high-dimensional feature space, but this is often not reflected in the derived rates in the state-of-the-art analyses. In this paper, we attempt to bridge the gap between the theory and practice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs), by deriving statistical guarantees on the estimated densities in terms of the intrinsic dimension of the data and the latent space. We analytically show that if one has access to $n$ samples from the unknown target distribution and the network architectures are properly chosen, the expected Wasserstein-1 distance of the estimates from the target scales as $O\left( n^{-1/d_\mu } \right)$ for GANs and $\tilde{O}\left( n^{-1/(d_\mu+\ell)} \right)$ for BiGANs, where $d_\mu$ and $\ell$ are the upper Wasserstein-1 dimension of the data-distribution and latent-space dimension, respectively. The theoretical analyses not only suggest that these methods successfully avoid the curse of dimensionality, in the sense that the exponent of $n$ in the error rates does not depend on the data dimension but also serve to bridge the gap between the theoretical analyses of GANs and the known sharp rates from optimal transport literature. Additionally, we demonstrate that GANs can effectively achieve the minimax optimal rate even for non-smooth underlying distributions, with the use of interpolating generator networks.},
  archive      = {J_JMLR},
  author       = {Saptarshi Chakraborty and Peter L. Bartlett},
  journal      = {Journal of Machine Learning Research},
  number       = {111},
  pages        = {1-57},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On the statistical properties of generative adversarial models for low intrinsic data dimension},
  url          = {https://jmlr.org/papers/v26/24-0054.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prominent roles of conditionally invariant components in domain adaptation: Theory and algorithms. <em>JMLR</em>, <em>26</em>(110), 1-92. (<a href='https://jmlr.org/papers/v26/23-1234.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) is a statistical learning problem that arises when the distribution of the source data used to train a model differs from that of the target data used to evaluate the model. While many DA algorithms have demonstrated considerable empirical success, blindly applying these algorithms can often lead to worse performance on new datasets. To address this, it is crucial to clarify the assumptions under which a DA algorithm has good target performance. In this work, we focus on the assumption of the presence of conditionally invariant components (CICs), which are relevant for prediction and remain conditionally invariant across the source and target data. We demonstrate that CICs, which can be estimated through conditional invariant penalty (CIP), play three prominent roles in providing target risk guarantees in DA. First, we propose a new algorithm based on CICs, importance-weighted conditional invariant penalty (IW-CIP), which has target risk guarantees beyond simple settings such as covariate shift and label shift. Second, we show that CICs help identify large discrepancies between source and target risks of other DA algorithms. Finally, we demonstrate that incorporating CICs into the domain invariant projection (DIP) algorithm can address its failure scenario caused by label-flipping features. We support our new algorithms and theoretical findings via numerical experiments on synthetic data, MNIST, CelebA, Camelyon17, and DomainNet datasets.},
  archive      = {J_JMLR},
  author       = {Keru Wu and Yuansi Chen and Wooseok Ha and Bin Yu},
  journal      = {Journal of Machine Learning Research},
  number       = {110},
  pages        = {1-92},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Prominent roles of conditionally invariant components in domain adaptation: Theory and algorithms},
  url          = {https://jmlr.org/papers/v26/23-1234.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Near-optimal nonconvex-strongly-convex bilevel optimization with fully first-order oracles. <em>JMLR</em>, <em>26</em>(109), 1-56. (<a href='https://jmlr.org/papers/v26/23-1104.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider bilevel optimization when the lower-level problem is strongly convex. Recent works show that with a Hessian-vector product (HVP) oracle, one can provably find an $\epsilon$-stationary point within ${O}(\epsilon^{-2})$ oracle calls. However, the HVP oracle may be inaccessible or expensive in practice. Kwon et al. (ICML 2023) addressed this issue by proposing a first-order method that can achieve the same goal at a slower rate of $\tilde{O}(\epsilon^{-3})$. In this paper, we incorporate a two-time-scale update to improve their method to achieve the near-optimal $\tilde{O}(\epsilon^{-2})$ first-order oracle complexity. Our analysis is highly extensible. In the stochastic setting, our algorithm can achieve the stochastic first-order oracle complexity of $\tilde {O}(\epsilon^{-4})$ and $\tilde {O}(\epsilon^{-6})$ when the stochastic noises are only in the upper-level objective and in both level objectives, respectively. When the objectives have higher-order smoothness conditions, our deterministic method can escape saddle points by injecting noise, and can be accelerated to achieve a faster rate of $\tilde {O}(\epsilon^{-1.75})$ using Nesterov's momentum.},
  archive      = {J_JMLR},
  author       = {Lesi Chen and Yaohua Ma and Jingzhao Zhang},
  journal      = {Journal of Machine Learning Research},
  number       = {109},
  pages        = {1-56},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Near-optimal nonconvex-strongly-convex bilevel optimization with fully first-order oracles},
  url          = {https://jmlr.org/papers/v26/23-1104.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive distributed kernel ridge regression: A feasible distributed learning scheme for data silos. <em>JMLR</em>, <em>26</em>(108), 1-54. (<a href='https://jmlr.org/papers/v26/23-0806.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data silos, mainly caused by privacy and interoperability, significantly constrain collaborations among different organizations with similar data for the same purpose. Distributed learning based on divide-and-conquer provides a promising way to settle the data silos, but it suffers from several challenges, including autonomy, privacy guarantees, and the necessity of collaborations. This paper focuses on developing an adaptive distributed kernel ridge regression (AdaDKRR) by taking autonomy in parameter selection, privacy in communicating non-sensitive information, and the necessity of collaborations for performance improvement into account. We provide both solid theoretical verifications and comprehensive experiments for AdaDKRR to demonstrate its feasibility and effectiveness. Theoretically, we prove that under some mild conditions, AdaDKRR performs similarly to running the optimal learning algorithms on the whole data, verifying the necessity of collaborations and showing that no other distributed learning scheme can essentially beat AdaDKRR under the same conditions. Numerically, we test AdaDKRR on both toy simulations and two real-world applications to show that AdaDKRR is superior to other existing distributed learning schemes. All these results show that AdaDKRR is a feasible scheme to overcome data silos, which are highly desired in numerous application regions such as intelligent decision-making, pricing forecasting, and performance prediction for products.},
  archive      = {J_JMLR},
  author       = {Shao-Bo Lin and Xiaotong Liu and Di Wang and Hai Zhang and Ding-Xuan Zhou},
  journal      = {Journal of Machine Learning Research},
  number       = {108},
  pages        = {1-54},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Adaptive distributed kernel ridge regression: A feasible distributed learning scheme for data silos},
  url          = {https://jmlr.org/papers/v26/23-0806.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On global and local convergence of iterative linear quadratic optimization algorithms for discrete time nonlinear control. <em>JMLR</em>, <em>26</em>(107), 1-85. (<a href='https://jmlr.org/papers/v26/22-1271.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A classical approach for solving discrete time nonlinear control on a finite horizon consists in repeatedly minimizing linear quadratic approximations of the original problem around current candidate solutions. While widely popular in many domains, such an approach has mainly been analyzed locally. We provide detailed convergence guarantees to stationary points as well as local linear convergence rates for the Iterative Linear Quadratic Regulator (ILQR) algorithm and its Differential Dynamic Programming (DDP) variant. For problems without costs on control variables, we observe that global convergence to minima can be ensured provided that the linearized discrete time dynamics are surjective, costs on the state variables are gradient dominated. We further detail quadratic local convergence when the costs are self-concordant. We show that surjectivity of the linearized dynamics hold for appropriate discretization schemes given the existence of a feedback linearization scheme. We present complexity bounds of algorithms based on linear quadratic approximations through the lens of generalized Gauss-Newton methods. Our analysis uncovers several convergence phases for regularized generalized Gauss-Newton algorithms.},
  archive      = {J_JMLR},
  author       = {Vincent Roulet and Siddhartha Srinivasa and Maryam Fazel and Zaid Harchaoui},
  journal      = {Journal of Machine Learning Research},
  number       = {107},
  pages        = {1-85},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On global and local convergence of iterative linear quadratic optimization algorithms for discrete time nonlinear control},
  url          = {https://jmlr.org/papers/v26/22-1271.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A decentralized proximal gradient tracking algorithm for composite optimization on riemannian manifolds. <em>JMLR</em>, <em>26</em>(106), 1-37. (<a href='https://jmlr.org/papers/v26/24-1989.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on minimizing a smooth function combined with a nonsmooth regularization term on a compact Riemannian submanifold embedded in the Euclidean space under a decentralized setting. Typically, there are two types of approaches at present for tackling such composite optimization problems. The first, subgradient-based approaches, rely on subgradient information of the objective function to update variables, achieving an iteration complexity of $O(\epsilon^{-4}\log^2(\epsilon^{-2}))$. The second, smoothing approaches, involve constructing a smooth approximation of the nonsmooth regularization term, resulting in an iteration complexity of $O(\epsilon^{-4})$. This paper proposes a proximal gradient type algorithm that fully exploits the composite structure. The global convergence to a stationary point is established with a significantly improved iteration complexity of $O(\epsilon^{-2})$. To validate the effectiveness and efficiency of our proposed method, we present numerical results from real-world applications, showcasing its superior performance compared to existing approaches.},
  archive      = {J_JMLR},
  author       = {Lei Wang and Le Bao and Xin Liu},
  journal      = {Journal of Machine Learning Research},
  number       = {106},
  pages        = {1-37},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {A decentralized proximal gradient tracking algorithm for composite optimization on riemannian manifolds},
  url          = {https://jmlr.org/papers/v26/24-1989.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning conditional distributions on continuous spaces. <em>JMLR</em>, <em>26</em>(105), 1-64. (<a href='https://jmlr.org/papers/v26/24-0924.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate sample-based learning of conditional distributions on multi-dimensional unit boxes, allowing for different dimensions of the feature and target spaces. Our approach involves clustering data near varying query points in the feature space to create empirical measures in the target space. We employ two distinct clustering schemes: one based on a fixed-radius ball and the other on nearest neighbors. We establish upper bounds for the convergence rates of both methods and, from these bounds, deduce optimal configurations for the radius and the number of neighbors. We propose to incorporate the nearest neighbors method into neural network training, as our empirical analysis indicates it has better performance in practice. For efficiency, our training process utilizes approximate nearest neighbors search with random binary space partitioning. Additionally, we employ the Sinkhorn algorithm and a sparsity-enforced transport plan. Our empirical findings demonstrate that, with a suitably designed structure, the neural network has the ability to adapt to a suitable level of Lipschitz continuity locally.},
  archive      = {J_JMLR},
  author       = {Cyril Benezet and Ziteng Cheng and Sebastian Jaimungal},
  journal      = {Journal of Machine Learning Research},
  number       = {105},
  pages        = {1-64},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Learning conditional distributions on continuous spaces},
  url          = {https://jmlr.org/papers/v26/24-0924.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified analysis of nonstochastic delayed feedback for combinatorial semi-bandits, linear bandits, and MDPs. <em>JMLR</em>, <em>26</em>(104), 1-60. (<a href='https://jmlr.org/papers/v26/24-0496.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a new analysis of Follow The Regularized Leader (FTRL) for online learning with delayed bandit feedback. By separating the cost of delayed feedback from that of bandit feedback, our analysis allows us to obtain new results in four important settings. We derive the first optimal (up to logarithmic factors) regret bounds for combinatorial semi-bandits with delay and adversarial Markov Decision Processes with delay (both known and unknown transition functions). Furthermore, we use our analysis to develop an efficient algorithm for linear bandits with delay achieving near-optimal regret bounds. In order to derive these results we show that FTRL remains stable across multiple rounds under mild assumptions on the regularizer.},
  archive      = {J_JMLR},
  author       = {Lukas Zierahn and Dirk van der Hoeven and Tal Lancewicki and Aviv Rosenberg and Nicolò Cesa-Bianchi},
  journal      = {Journal of Machine Learning Research},
  number       = {104},
  pages        = {1-60},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {A unified analysis of nonstochastic delayed feedback for combinatorial semi-bandits, linear bandits, and MDPs},
  url          = {https://jmlr.org/papers/v26/24-0496.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error bounds for particle gradient descent, and extensions of the log-sobolev and talagrand inequalities. <em>JMLR</em>, <em>26</em>(103), 1-38. (<a href='https://jmlr.org/papers/v26/24-0437.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive non-asymptotic error bounds for particle gradient descent (PGD, Kuntz et al. (2023)), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that the flow converges exponentially fast to the free energy's minimizers for models satisfying a condition that generalizes both the log-Sobolev and the Polyak--Łojasiewicz inequalities (LSI and PŁI, respectively). We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the PŁI implies the so-called quadratic growth condition), and applying the extension to our new setting. We also generalize the Bakry--Émery Theorem and show that the LSI/PŁI extension holds for models with strongly concave log-likelihoods. For such models, we further control PGD's discretization error and obtain the non-asymptotic error bounds. While we are motivated by the study of PGD, we believe that the inequalities and results we extend may be of independent interest.},
  archive      = {J_JMLR},
  author       = {Rocco Caprio and Juan Kuntz and Samuel Power and Adam M. Johansen},
  journal      = {Journal of Machine Learning Research},
  number       = {103},
  pages        = {1-38},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Error bounds for particle gradient descent, and extensions of the log-sobolev and talagrand inequalities},
  url          = {https://jmlr.org/papers/v26/24-0437.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear hypothesis testing in high-dimensional expected shortfall regression with heavy-tailed errors. <em>JMLR</em>, <em>26</em>(102), 1-54. (<a href='https://jmlr.org/papers/v26/24-0061.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expected shortfall (ES) is widely used for characterizing the tail of a distribution across various fields, particularly in financial risk management. In this paper, we explore a two-step procedure that leverages an orthogonality property to reduce sensitivity to nuisance parameters when estimating within a joint quantile and expected shortfall regression framework. For high-dimensional sparse models, we propose a robust $\ell_1$-penalized two-step approach capable of handling heavy-tailed data distributions. We establish non-asymptotic estimation error bounds and propose an appropriate growth rate for the diverging robustification parameter. To facilitate statistical inference for certain linear combinations of the ES regression coefficients, we construct debiased estimators and develop their asymptotic distributions, which form the basis for constructing valid confidence intervals. We validate the proposed method through simulation studies, demonstrating its effectiveness in high-dimensional linear models with heavy-tailed errors.},
  archive      = {J_JMLR},
  author       = {Gaoyu Wu and Jelena Bradic and Kean Ming Tan and Wen-Xin Zhou},
  journal      = {Journal of Machine Learning Research},
  number       = {102},
  pages        = {1-54},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Linear hypothesis testing in high-dimensional expected shortfall regression with heavy-tailed errors},
  url          = {https://jmlr.org/papers/v26/24-0061.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient numerical integration in reproducing kernel hilbert spaces via leverage scores sampling. <em>JMLR</em>, <em>26</em>(101), 1-55. (<a href='https://jmlr.org/papers/v26/23-1551.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we consider the problem of numerical integration, i.e., approximating integrals with respect to a target probability measure using only pointwise evaluations of the integrand. We focus on the setting in which the target distribution is only accessible through a set of $n$ i.i.d. observations, and the integrand belongs to a reproducing kernel Hilbert space. We propose an efficient procedure which exploits a small i.i.d. random subset of $m [abs] [ pdf ][ bib ] [ code ] &copy JMLR 2025. ( edit , beta )},
  archive      = {J_JMLR},
  author       = {Antoine Chatalic and Nicolas Schreuder and Ernesto De Vito and Lorenzo Rosasco},
  journal      = {Journal of Machine Learning Research},
  number       = {101},
  pages        = {1-55},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Efficient numerical integration in reproducing kernel hilbert spaces via leverage scores sampling},
  url          = {https://jmlr.org/papers/v26/23-1551.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution free tests for model selection based on maximum mean discrepancy with estimated parameters. <em>JMLR</em>, <em>26</em>(100), 1-52. (<a href='https://jmlr.org/papers/v26/23-1199.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There exist several testing procedures based on the maximum mean discrepancy (MMD) to address the challenge of model specification. However, these testing procedures ignore the presence of estimated parameters in the case of composite null hypotheses. In this paper, we first illustrate the effect of parameter estimation in model specification tests based on the MMD. Second, we propose simple model specification and model selection tests in the case of models with estimated parameters. All our tests are asymptotically standard normal under the null, even when the true underlying distribution belongs to the competing parametric families. A simulation study and a real data analysis illustrate the performance of our tests in terms of power and level.},
  archive      = {J_JMLR},
  author       = {Florian Brück and Jean-David Fermanian and Aleksey Min},
  journal      = {Journal of Machine Learning Research},
  number       = {100},
  pages        = {1-52},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Distribution free tests for model selection based on maximum mean discrepancy with estimated parameters},
  url          = {https://jmlr.org/papers/v26/23-1199.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical field theory for markov decision processes under uncertainty. <em>JMLR</em>, <em>26</em>(99), 1-24. (<a href='https://jmlr.org/papers/v26/23-0905.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A statistical field theory is introduced for finite state and action Markov decision processes with unknown parameters, in a Bayesian setting. The Bellman equation, for policy evaluation and the optimal value function in finite and discounted infinite horizon problems, is studied as a disordered interacting dynamical system. The Markov decision process transition probabilities and mean-rewards are interpreted as quenched random variables and the value functions, or the iterates of the Bellman equation, are deterministic variables that evolve dynamically. The posterior over value functions is then equivalent to the quenched average of Fourier inverse of the Martin-Siggia-Rose-De Dominicis-Janssen generating function. The formalism enables the use of methods from field theory to compute posterior moments of value functions. The paper presents two such methods, corresponding to two distinct asymptotic limits. First, the classical approximation is applied, corresponding to the asymptotic data limit. This approximation recovers so-called plug-in estimators for the mean of the value functions. Second, a dynamic mean field theory is derived, showing that under certain assumptions the state-action values are statistically independent across state-action pairs in the asymptotic state space limit. The state-action value statistics can be computed from a set of self-consistent mean field equations, which we call dynamic mean field programming (DMFP). Collectively, the results provide analytic insight into the structure of model uncertainty in Markov decision processes, and pave the way toward more advanced field theoretic techniques and applications to planning and reinforcement learning problems.},
  archive      = {J_JMLR},
  author       = {George Stamatescu},
  journal      = {Journal of Machine Learning Research},
  number       = {99},
  pages        = {1-24},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Statistical field theory for markov decision processes under uncertainty},
  url          = {https://jmlr.org/papers/v26/23-0905.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian data sketching for varying coefficient regression models. <em>JMLR</em>, <em>26</em>(98), 1-29. (<a href='https://jmlr.org/papers/v26/23-0505.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Varying coefficient models are popular for estimating nonlinear regression functions in functional data models. Their Bayesian variants have received limited attention in large data applications, primarily due to prohibitively slow posterior computations using Markov chain Monte Carlo (MCMC) algorithms. We introduce Bayesian data sketching for varying coefficient models to obviate computational challenges presented by large sample sizes. To address the challenges of analyzing large data, we compress the functional response vector and predictor matrix by a random linear transformation to achieve dimension reduction and conduct inference on the compressed data. Our approach distinguishes itself from several existing methods for analyzing large functional data in that it requires neither the development of new models or algorithms nor any specialized computational hardware while delivering fully model-based Bayesian inference. Well-established methods and algorithms for varying-coefficient regression models can be applied to the compressed data. We establish posterior contraction rates for estimating the varying coefficients and predicting the outcome at new locations with the randomly compressed data model. We use simulation experiments and analyze remote sensed vegetation data to empirically illustrate the inferential and computational efficiency of our approach.},
  archive      = {J_JMLR},
  author       = {Rajarshi Guhaniyogi and Laura Baracaldo and Sudipto Banerjee},
  journal      = {Journal of Machine Learning Research},
  number       = {98},
  pages        = {1-29},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Bayesian data sketching for varying coefficient regression models},
  url          = {https://jmlr.org/papers/v26/23-0505.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bagged k-distance for mode-based clustering using the probability of localized level sets. <em>JMLR</em>, <em>26</em>(97), 1-62. (<a href='https://jmlr.org/papers/v26/22-1179.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an ensemble learning algorithm named bagged $k$-distance for mode-based clustering (BDMBC) by putting forward a new measure called the probability of localized level sets (PLLS), which enables us to find all clusters for varying densities with a global threshold. On the theoretical side, we show that with a properly chosen number of nearest neighbors $k_D$ in the bagged $k$-distance, the sub-sample size $s$, the bagging rounds $B$, and the number of nearest neighbors $k_L$ for the localized level sets, BDMBC can achieve optimal convergence rates for mode estimation. It turns out that with a relatively small $B$, the sub-sample size $s$ can be much smaller than the number of training data $n$ at each bagging round, and the number of nearest neighbors $k_D$ can be reduced simultaneously. Moreover, we establish fast convergence rates for the level set estimation of the PLLS in terms of Hausdorff distance, which reveals that BDMBC can find localized level sets for varying densities and thus enjoys local adaptivity. On the practical side, we conduct numerical experiments to empirically verify the effectiveness of BDMBC for mode estimation and level set estimation, which demonstrates the promising accuracy and efficiency of our proposed algorithm.},
  archive      = {J_JMLR},
  author       = {Hanyuan Hang},
  journal      = {Journal of Machine Learning Research},
  number       = {97},
  pages        = {1-62},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Bagged k-distance for mode-based clustering using the probability of localized level sets},
  url          = {https://jmlr.org/papers/v26/22-1179.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear cost and exponentially convergent approximation of gaussian matérn processes on intervals. <em>JMLR</em>, <em>26</em>(96), 1-34. (<a href='https://jmlr.org/papers/v26/24-1779.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational cost for inference and prediction of statistical models based on Gaussian processes with Matérn covariance functions scales cubically with the number of observations, limiting their applicability to large data sets. The cost can be reduced in certain special cases, but there are no generally applicable exact methods with linear cost. Several approximate methods have been introduced to reduce the cost, but most lack theoretical guarantees for accuracy. We consider Gaussian processes on bounded intervals with Matérn covariance functions and, for the first time, develop a generally applicable method with linear cost and a covariance error that decreases exponentially fast in the order $m$ of the proposed approximation. The method is based on an optimal rational approximation of the spectral density and results in an approximation that can be represented as a sum of $m$ independent Gaussian Markov processes, facilitating usage in general software for statistical inference. Besides theoretical justifications, we demonstrate accuracy empirically through carefully designed simulation studies, which show that the method outperforms state-of-the-art alternatives in accuracy for fixed computational cost in tasks like Gaussian process regression.},
  archive      = {J_JMLR},
  author       = {David Bolin and Vaibhav Mehandiratta and Alexandre B. Simas},
  journal      = {Journal of Machine Learning Research},
  number       = {96},
  pages        = {1-34},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Linear cost and exponentially convergent approximation of gaussian matérn processes on intervals},
  url          = {https://jmlr.org/papers/v26/24-1779.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invariant subspace decomposition. <em>JMLR</em>, <em>26</em>(95), 1-56. (<a href='https://jmlr.org/papers/v26/24-0699.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the task of predicting a response $Y$ from a set of covariates $X$ in settings where the conditional distribution of $Y$ given $X$ changes over time. For this to be feasible, assumptions on how the conditional distribution changes over time are required. Existing approaches assume, for example, that changes occur smoothly over time so that short-term prediction using only the recent past becomes feasible. To additionally exploit observations further in the past, we propose a novel invariance-based framework for linear conditionals, called Invariant Subspace Decomposition (ISD), that splits the conditional distribution into a time-invariant and a residual time-dependent component. As we show, this decomposition can be employed both for zero-shot and time-adaptation prediction tasks, that is, settings where either no or a small amount of training data is available at the time points we want to predict $Y$ at, respectively. We propose a practical estimation procedure, which automatically infers the decomposition using tools from approximate joint matrix diagonalization. Furthermore, we provide finite sample guarantees for the proposed estimator and demonstrate empirically that it indeed improves on approaches that do not use the additional invariant structure.},
  archive      = {J_JMLR},
  author       = {Margherita Lazzaretto and Jonas Peters and Niklas Pfister},
  journal      = {Journal of Machine Learning Research},
  number       = {95},
  pages        = {1-56},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Invariant subspace decomposition},
  url          = {https://jmlr.org/papers/v26/24-0699.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Posterior concentrations of fully-connected bayesian neural networks with general priors on the weights. <em>JMLR</em>, <em>26</em>(94), 1-60. (<a href='https://jmlr.org/papers/v26/24-0425.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian approaches for training deep neural networks (BNNs) have received significant interest and have been effectively utilized in a wide range of applications. Several studies have examined the properties of posterior concentrations in BNNs. However, most of these studies focus solely on BNN models with sparse or heavy-tailed priors. Surprisingly, there are currently no theoretical results for BNNs using Gaussian priors, which are the most commonly used in practice. The lack of theory arises from the absence of approximation results of Deep Neural Networks (DNNs) that are non-sparse and have bounded parameters. In this paper, we present a new approximation theory for non-sparse DNNs with bounded parameters. Additionally, based on the approximation theory, we show that BNNs with non-sparse general priors can achieve near-minimax optimal posterior concentration rates around the true model.},
  archive      = {J_JMLR},
  author       = {Insung Kong and Yongdai Kim},
  journal      = {Journal of Machine Learning Research},
  number       = {94},
  pages        = {1-60},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Posterior concentrations of fully-connected bayesian neural networks with general priors on the weights},
  url          = {https://jmlr.org/papers/v26/24-0425.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outlier robust and sparse estimation of linear regression coefficients. <em>JMLR</em>, <em>26</em>(93), 1-79. (<a href='https://jmlr.org/papers/v26/23-1583.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider outlier-robust and sparse estimation of linear regression coefficients, when the covariates and the noises are contaminated by adversarial outliers and noises are sampled from a heavy-tailed distribution. Our results present sharper error bounds under weaker assumptions than prior studies that share similar interests with this study. Our analysis relies on some sharp concentration inequalities resulting from generic chaining.},
  archive      = {J_JMLR},
  author       = {Takeyuki Sasai and Hironori Fujisawa},
  journal      = {Journal of Machine Learning Research},
  number       = {93},
  pages        = {1-79},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Outlier robust and sparse estimation of linear regression coefficients},
  url          = {https://jmlr.org/papers/v26/23-1583.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affine rank minimization via asymptotic log-det iteratively reweighted least squares. <em>JMLR</em>, <em>26</em>(92), 1-44. (<a href='https://jmlr.org/papers/v26/23-0943.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The affine rank minimization problem is a well-known approach to matrix recovery. While there are various surrogates to this NP-hard problem, we prove that the asymptotic minimization of log-det objective functions indeed always reveals the desired, lowest-rank matrices---whereas such may or may not recover a sought-after ground truth. Concerning commonly applied methods such as iteratively reweighted least squares, one thus remains with two difficult to distinguish concerns: how problematic are local minima inherent to the approach truly; and opposingly, how influential instead is the numerical realization. We first show that comparable solution statements do not hold true for Schatten-$p$ functions, including the nuclear norm, and discuss the role of divergent minimizers. Subsequently, we outline corresponding implications for general optimization approaches as well as the more specific IRLS-$0$ algorithm, emphasizing through examples that the transition of the involved smoothing parameter to zero is frequently a more substantial issue than non-convexity. Lastly, we analyze several presented aspects empirically in a series of numerical experiments. In particular, allowing for instance sufficiently many iterations, one may even observe a phase transition for generic recoverability at the absolute theoretical minimum.},
  archive      = {J_JMLR},
  author       = {Sebastian Krämer},
  journal      = {Journal of Machine Learning Research},
  number       = {92},
  pages        = {1-44},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Affine rank minimization via asymptotic log-det iteratively reweighted least squares},
  url          = {https://jmlr.org/papers/v26/23-0943.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal effect of functional treatment. <em>JMLR</em>, <em>26</em>(91), 1-39. (<a href='https://jmlr.org/papers/v26/23-0381.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the causal effect with a functional treatment variable, where practical applications often arise in neuroscience, biomedical sciences, etc. Previous research concerning the effect of a functional variable on an outcome is typically restricted to exploring correlation rather than causality. The generalized propensity score, which is often used to calibrate the selection bias, is not directly applicable to a functional treatment variable due to a lack of definition of probability density function for functional data. We propose three estimators for the average dose-response functional based on the functional linear model, namely, the functional stabilized weight estimator, the outcome regression estimator and the doubly robust estimator, each of which has its own merits. We study their theoretical properties, which are corroborated through extensive numerical experiments. A real data application on electroencephalography data and disease severity demonstrates the practical value of our methods.},
  archive      = {J_JMLR},
  author       = {Ruoxu Tan and Wei Huang and Zheng Zhang and Guosheng Yin},
  journal      = {Journal of Machine Learning Research},
  number       = {91},
  pages        = {1-39},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Causal effect of functional treatment},
  url          = {https://jmlr.org/papers/v26/23-0381.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uplift model evaluation with ordinal dominance graphs. <em>JMLR</em>, <em>26</em>(90), 1-56. (<a href='https://jmlr.org/papers/v26/22-1455.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uplift modelling is a subfield of causal learning that focuses on ranking entities by individual treatment effects. Uplift models are typically evaluated using Qini curves or Qini scores. While intuitive, the theoretical grounding for Qini in the literature is limited, and the mathematical connection to the well-understood Receiver Operating Characteristic (ROC) curve is unclear. In this paper, we introduce pROCini, a novel uplift evaluation metric that improves upon Qini in two important ways. First, it explicitly incorporates more information by taking into account negative outcomes. Second, it leverages this additional information within the Ordinal Dominance Graph framework, which is the basis behind the well known ROC curve, resulting in a mathematically well-behaved metric that facilitates theoretical analysis. We derive confidence bounds for pROCini, exploiting its theoretical properties. Finally, we empirically validate the improved discriminative power of ROCini and pROCini in a simulation study as well as via experiments on real data.},
  archive      = {J_JMLR},
  author       = {Brecht Verbeken and Marie-Anne Guerry and Wouter Verbeke and Sam Verboven},
  journal      = {Journal of Machine Learning Research},
  number       = {90},
  pages        = {1-56},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Uplift model evaluation with ordinal dominance graphs},
  url          = {https://jmlr.org/papers/v26/22-1455.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional l2-boosting: Rate of convergence. <em>JMLR</em>, <em>26</em>(89), 1-54. (<a href='https://jmlr.org/papers/v26/21-0725.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boosting is one of the most significant developments in machine learning. This paper studies the rate of convergence of L2-Boosting in a high-dimensional setting under early stopping. We close a gap in the literature and provide the rate of convergence of L2-Boosting in a high-dimensional setting under approximate sparsity and without beta-min condition. We also show that the rate of convergence of the classical L2-Boosting depends on the design matrix described by a sparse eigenvalue condition. To show the latter results, we derive new, improved approximation results for the pure greedy algorithm, based on analyzing the revisiting behavior of L2-Boosting. These results might be of independent interest. Moreover, we introduce so-called "restricted" L2-Boosting. The restricted L2-Boosting algorithm sticks to the set of the previously chosen variables, exploits the information contained in these variables first and then only occasionally allows to add new variables to this set. We derive the rate of convergence for restricted L2-Boosting under early stopping which is close to the convergence rate of Lasso in an approximate sparse, high-dimensional setting without beta-min condition. We also introduce feasible rules for early stopping, which can be easily implemented and used in applied work. Finally, we present simulation studies to illustrate the relevance of our theoretical results and to provide insights into the practical aspects of boosting. In these simulation studies, L2-Boosting clearly outperforms Lasso. An empirical illustration and the proofs are contained in the Appendix.},
  archive      = {J_JMLR},
  author       = {Ye Luo and Martin Spindler and Jannis Kueck},
  journal      = {Journal of Machine Learning Research},
  number       = {89},
  pages        = {1-54},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {High-dimensional l2-boosting: Rate of convergence},
  url          = {https://jmlr.org/papers/v26/21-0725.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature learning in finite-width bayesian deep linear networks with multiple outputs and convolutional layers. <em>JMLR</em>, <em>26</em>(88), 1-35. (<a href='https://jmlr.org/papers/v26/24-1158.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep linear networks have been extensively studied, as they provide simplified models of deep learning. However, little is known in the case of finite-width architectures with multiple outputs and convolutional layers. In this manuscript, we provide rigorous results for the statistics of functions implemented by the aforementioned class of networks, thus moving closer to a complete characterization of feature learning in the Bayesian setting. Our results include: (i) an exact and elementary non-asymptotic integral representation for the joint prior distribution over the outputs, given in terms of a mixture of Gaussians; (ii) an analytical formula for the posterior distribution in the case of squared error loss function (Gaussian likelihood); (iii) a quantitative description of the feature learning infinite-width regime, using large deviation theory. From a physical perspective, deep architectures with multiple outputs or convolutional layers represent different manifestations of kernel shape renormalization, and our work provides a dictionary that translates this physics intuition and terminology into rigorous Bayesian statistics.},
  archive      = {J_JMLR},
  author       = {Federico Bassetti and Marco Gherardi and Alessandro Ingrosso and Mauro Pastore and Pietro Rotondo},
  journal      = {Journal of Machine Learning Research},
  number       = {88},
  pages        = {1-35},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Feature learning in finite-width bayesian deep linear networks with multiple outputs and convolutional layers},
  url          = {https://jmlr.org/papers/v26/24-1158.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How good is your laplace approximation of the bayesian posterior? finite-sample computable error bounds for a variety of useful divergences. <em>JMLR</em>, <em>26</em>(87), 1-81. (<a href='https://jmlr.org/papers/v26/24-0619.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Laplace approximation is a popular method for constructing a Gaussian approximation to the Bayesian posterior and thereby approximating the posterior mean and variance. But approximation quality is a concern. One might consider using rate-of-convergence bounds from certain versions of the Bayesian Central Limit Theorem (BCLT) to provide quality guarantees. But existing bounds require assumptions that are unrealistic even for relatively simple real-life Bayesian analyses; more specifically, existing bounds either (1) require knowing the true data-generating parameter, (2) are asymptotic in the number of samples, (3) do not control the Bayesian posterior mean, or (4) require strongly log concave models to compute. In this work, we provide the first computable bounds on quality that simultaneously (1) do not require knowing the true parameter, (2) apply to finite samples, (3) control posterior means and variances, and (4) apply generally to models that satisfy the conditions of the asymptotic BCLT. Moreover, we substantially improve the dimension dependence of existing bounds; in fact, we achieve the lowest-order dimension dependence possible in the general case. We compute exact constants in our bounds for a variety of standard models, including logistic regression, and numerically demonstrate their utility. We provide a framework for analysis of more complex models.},
  archive      = {J_JMLR},
  author       = {Mikołaj J. Kasprzak and Ryan Giordano and Tamara Broderick},
  journal      = {Journal of Machine Learning Research},
  number       = {87},
  pages        = {1-81},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {How good is your laplace approximation of the bayesian posterior? finite-sample computable error bounds for a variety of useful divergences},
  url          = {https://jmlr.org/papers/v26/24-0619.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integral probability metrics meet neural networks: The radon-kolmogorov-smirnov test. <em>JMLR</em>, <em>26</em>(86), 1-57. (<a href='https://jmlr.org/papers/v26/24-0245.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integral probability metrics (IPMs) constitute a general class of nonparametric two-sample tests that are based on maximizing the mean difference between samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of Radon bounded variation (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the IPM defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness degree $k \geq 0$. This test, which we refer to as the Radon-Kolmogorov-Smirnov (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test—the function $f$ achieving the maximum mean difference—is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. We can thus leverage the power of modern neural network optimization toolkits to (approximately) maximize the criterion that underlies the RKS test. We prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out experiments to elucidate the strengths and weaknesses of the RKS test versus the more traditional kernel MMD test.},
  archive      = {J_JMLR},
  author       = {Seunghoon Paik and Michael Celentano and Alden Green and Ryan J. Tibshirani},
  journal      = {Journal of Machine Learning Research},
  number       = {86},
  pages        = {1-57},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Integral probability metrics meet neural networks: The radon-kolmogorov-smirnov test},
  url          = {https://jmlr.org/papers/v26/24-0245.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On inference for the support vector machine. <em>JMLR</em>, <em>26</em>(85), 1-54. (<a href='https://jmlr.org/papers/v26/23-1581.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The linear support vector machine has a parametrised decision boundary. The paper considers inference for the corresponding parameters, which indicate the effects of individual variables on the decision boundary. The proposed inference is via a convolution-smoothed version of the SVM loss function, this having several inferential advantages over the original SVM, whose associated loss function is not everywhere differentiable. Notably, convolution-smoothing comes with non-asymptotic theoretical guarantees, including a distributional approximation to the parameter estimator that scales more favourably with the dimension of the feature vector. The differentiability of the loss function produces other advantages in some settings; for instance, by facilitating the inclusion of penalties or the synthesis of information from a large number of small samples. The paper closes by relating the linear SVM parameters to those of some probability models for binary outcomes.},
  archive      = {J_JMLR},
  author       = {Jakub Rybak and Heather Battey and Wen-Xin Zhou},
  journal      = {Journal of Machine Learning Research},
  number       = {85},
  pages        = {1-54},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On inference for the support vector machine},
  url          = {https://jmlr.org/papers/v26/23-1581.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random pruning over-parameterized neural networks can improve generalization: A training dynamics analysis. <em>JMLR</em>, <em>26</em>(84), 1-51. (<a href='https://jmlr.org/papers/v26/23-0832.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been observed that applying pruning-at-initialization methods and training the sparse networks can sometimes yield slightly better test performance than training the original dense network. Such experimental observations are yet to be understood theoretically. This work makes the first attempt to study this phenomenon. Specifically, we identify a theoretical minimal setting and study a classification task with a one-hidden-layer neural network, which is randomly pruned according to different rates at the initialization. We show that as long as the pruning rate is below a certain threshold, the network provably exhibits good generalization performance after training.More surprisingly, the generalization bound gets better as the pruning rate mildly gets larger. To complement this positive result, we also show a negative result: there exists a large pruning rate such that while gradient descent is still able to drive the training loss toward zero, the generalization performance is no better than random guessing. This further suggests that pruning can change the feature learning process, which leads to the performance drop of the pruned neural network. To our knowledge, this is the first theory work studying how different pruning rates affect neural networks' performance, suggesting that an appropriate pruning rate might improve the neural network's generalization.},
  archive      = {J_JMLR},
  author       = {Hongru Yang and Yingbin Liang and Xiaojie Guo and Lingfei Wu and Zhangyang Wang},
  journal      = {Journal of Machine Learning Research},
  number       = {84},
  pages        = {1-51},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Random pruning over-parameterized neural networks can improve generalization: A training dynamics analysis},
  url          = {https://jmlr.org/papers/v26/23-0832.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal abstraction: A theoretical foundation for mechanistic interpretability. <em>JMLR</em>, <em>26</em>(83), 1-64. (<a href='https://jmlr.org/papers/v26/23-0058.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal abstraction provides a theoretical foundation for mechanistic interpretability, the field concerned with providing intelligible algorithms that are faithful simplifications of the known, but opaque low-level details of black box AI models. Our contributions are (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old mechanisms to new mechanisms), (2) providing a flexible, yet precise formalization for the core concepts of polysemantic neurons, the linear representation hypothesis, modular features, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability methods in the common language of causal abstraction, namely, activation and path patching, causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure, sparse autoencoders, differential binary masking, distributed alignment search, and steering.},
  archive      = {J_JMLR},
  author       = {Atticus Geiger and Duligur Ibeling and Amir Zur and Maheep Chaudhary and Sonakshi Chauhan and Jing Huang and Aryaman Arora and Zhengxuan Wu and Noah Goodman and Christopher Potts and Thomas Icard},
  journal      = {Journal of Machine Learning Research},
  number       = {83},
  pages        = {1-64},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Causal abstraction: A theoretical foundation for mechanistic interpretability},
  url          = {https://jmlr.org/papers/v26/23-0058.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit vs unfolded graph neural networks. <em>JMLR</em>, <em>26</em>(82), 1-46. (<a href='https://jmlr.org/papers/v26/22-0459.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been observed that message-passing graph neural networks (GNN) sometimes struggle to maintain a healthy balance between the efficient / scalable modeling of long-range dependencies across nodes while avoiding unintended consequences such oversmoothed node representations, sensitivity to spurious edges, or inadequate model interpretability. To address these and other issues, two separate strategies have recently been proposed, namely implicit and unfolded GNNs (that we abbreviate to IGNN and UGNN respectively). The former treats node representations as the fixed points of a deep equilibrium model that can efficiently facilitate arbitrary implicit propagation across the graph with a fixed memory footprint. In contrast, the latter involves treating graph propagation as unfolded descent iterations as applied to some graph-regularized energy function. While motivated differently, in this paper we carefully quantify explicit situations where the solutions they produce are equivalent and others where their properties sharply diverge. This includes the analysis of convergence, representational capacity, and interpretability. In support of this analysis, we also provide empirical head-to-head comparisons across multiple synthetic and public real-world node classification benchmarks. These results indicate that while IGNN is substantially more memory-efficient, UGNN models support unique, integrated graph attention mechanisms and propagation rules that can achieve strong node classification accuracy across disparate regimes such as adversarially-perturbed graphs, graphs with heterophily, and graphs involving long-range dependencies.},
  archive      = {J_JMLR},
  author       = {Yongyi Yang and Tang Liu and Yangkun Wang and Zengfeng Huang and David Wipf},
  journal      = {Journal of Machine Learning Research},
  number       = {82},
  pages        = {1-46},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Implicit vs unfolded graph neural networks},
  url          = {https://jmlr.org/papers/v26/22-0459.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards optimal branching of linear and semidefinite relaxations for neural network robustness certification. <em>JMLR</em>, <em>26</em>(81), 1-59. (<a href='https://jmlr.org/papers/v26/21-0068.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study certifying the robustness of ReLU neural networks against adversarial input perturbations. To diminish the relaxation error suffered by the popular linear programming (LP) and semidefinite programming (SDP) certification methods, we take a branch-and-bound approach to propose partitioning the input uncertainty set and solving the relaxations on each part separately. We show that this approach reduces relaxation error, and that the error is eliminated entirely upon performing an LP relaxation with a partition intelligently designed to exploit the nature of the ReLU activations. To scale this approach to large networks, we consider using a coarser partition whereby the number of parts in the partition is reduced. We prove that computing such a coarse partition that directly minimizes the LP relaxation error is NP-hard. By instead minimizing the worst-case LP relaxation error, we develop a closed-form branching scheme in the single-hidden layer case. We extend the analysis to the SDP, where the feasible set geometry is exploited to design a branching scheme that minimizes the worst-case SDP relaxation error. Experiments on MNIST, CIFAR-10, and Wisconsin breast cancer diagnosis classifiers demonstrate significant increases in the percentages of test samples certified. By independently increasing the input size and the number of layers, we empirically illustrate under which regimes the branched LP and branched SDP are best applied. Finally, we extend our LP branching method into a multi-layer branching heuristic, which attains comparable performance to prior state-of-the-art heuristics on large-scale, deep neural network certification benchmarks.},
  archive      = {J_JMLR},
  author       = {Brendon G. Anderson and Ziye Ma and Jingqi Li and Somayeh Sojoudi},
  journal      = {Journal of Machine Learning Research},
  number       = {81},
  pages        = {1-59},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Towards optimal branching of linear and semidefinite relaxations for neural network robustness certification},
  url          = {https://jmlr.org/papers/v26/21-0068.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GraphNeuralNetworks.jl: Deep learning on graphs with julia. <em>JMLR</em>, <em>26</em>(80), 1-6. (<a href='https://jmlr.org/papers/v26/24-2130.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GraphNeuralNetworks.jl is an open-source framework for deep learning on graphs, written in the Julia programming language. It supports multiple GPU backends, generic sparse or dense graph representations, and offers convenient interfaces for manipulating standard, heterogeneous, and temporal graphs with attributes at the node, edge, and graph levels. The framework allows users to define custom graph convolutional layers using gather/scatter message-passing primitives or optimized fused operations. It also includes several popular layers, enabling efficient experimentation with complex deep architectures. The package is available on GitHub: https://github.com/JuliaGraphs/GraphNeuralNetworks.jl.},
  archive      = {J_JMLR},
  author       = {Carlo Lucibello and Aurora Rossi},
  journal      = {Journal of Machine Learning Research},
  number       = {80},
  pages        = {1-6},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {GraphNeuralNetworks.jl: Deep learning on graphs with julia},
  url          = {https://jmlr.org/papers/v26/24-2130.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic angular synchronization under smoothness constraints. <em>JMLR</em>, <em>26</em>(79), 1-45. (<a href='https://jmlr.org/papers/v26/24-0925.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an undirected measurement graph $\mathcal{H} = ([n], \mathcal{E})$, the classical angular synchronization problem consists of recovering unknown angles $\theta_1^*,\dots,\theta_n^*$ from a collection of noisy pairwise measurements of the form $(\theta_i^* - \theta_j^*) \mod 2\pi$, for all $\{i,j\} \in \mathcal{E}$. This problem arises in a variety of applications, including computer vision, time synchronization of distributed networks, and ranking from pairwise comparisons. In this paper, we consider a dynamic version of this problem where the angles, and also the measurement graphs evolve over $T$ time points. Assuming a smoothness condition on the evolution of the latent angles, we derive three algorithms for joint estimation of the angles over all time points. Moreover, for one of the algorithms, we establish non-asymptotic recovery guarantees for the mean-squared error (MSE) under different statistical models. In particular, we show that the MSE converges to zero as $T$ increases under milder conditions than in the static setting. This includes the setting where the measurement graphs are highly sparse and disconnected, and also when the measurement noise is large and can potentially increase with $T$. We complement our theoretical results with experiments on synthetic data.},
  archive      = {J_JMLR},
  author       = {Ernesto Araya and Mihai Cucuringu and Hemant Tyagi},
  journal      = {Journal of Machine Learning Research},
  number       = {79},
  pages        = {1-45},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Dynamic angular synchronization under smoothness constraints},
  url          = {https://jmlr.org/papers/v26/24-0925.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Derivative-informed neural operator acceleration of geometric MCMC for infinite-dimensional bayesian inverse problems. <em>JMLR</em>, <em>26</em>(78), 1-68. (<a href='https://jmlr.org/papers/v26/24-0745.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional Bayesian inverse problems (BIPs). While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires repeated computations of gradients and Hessians of the log-likelihood, which becomes prohibitive when the parameter-to-observable (PtO) map is defined through expensive-to-solve parametric partial differential equations (PDEs). We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal exploits fast surrogate predictions of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate must accurately approximate the PtO map and its Jacobian, which often demands a prohibitively large number of PtO map samples via conventional operator learning methods. In this work, we present an extension of derivative-informed operator learning [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] that uses joint samples of the PtO map and its Jacobian. This leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observables and posterior local geometry at a significantly lower training cost than conventional methods. Cost and error analysis for reduced basis DINO surrogates are provided. Numerical studies demonstrate that DINO-driven MCMC generates effective posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster than prior geometry-based MCMC. Furthermore, the training cost of DINO surrogates breaks even compared to geometric MCMC after just 10--25 effective posterior samples.},
  archive      = {J_JMLR},
  author       = {Lianghao Cao and Thomas O'Leary-Roseberry and Omar Ghattas},
  journal      = {Journal of Machine Learning Research},
  number       = {78},
  pages        = {1-68},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Derivative-informed neural operator acceleration of geometric MCMC for infinite-dimensional bayesian inverse problems},
  url          = {https://jmlr.org/papers/v26/24-0745.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wasserstein F-tests for frechet regression on bures-wasserstein manifolds. <em>JMLR</em>, <em>26</em>(77), 1-123. (<a href='https://jmlr.org/papers/v26/24-0493.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses regression analysis for covariance matrix-valued outcomes with Euclidean covariates, motivated by applications in single-cell genomics and neuroscience where covariance matrices are observed across many samples. Our analysis leverages Fr\'echet regression on the Bures-Wasserstein manifold to estimate the conditional Fr\'echet mean given covariates $x$. We establish a non-asymptotic uniform $\sqrt{n}$-rate of convergence (up to logarithmic factors) over covariates with $\|x\| \lesssim \sqrt{\log n}$ and derive a pointwise central limit theorem to enable statistical inference. For testing covariate effects, we devise a novel test whose null distribution converges to a weighted sum of independent chi-square distributions, with power guarantees against a sequence of contiguous alternatives. Simulations validate the accuracy of the asymptotic theory. Finally, we apply our methods to a single-cell gene expression dataset, revealing age-related changes in gene co-expression networks.},
  archive      = {J_JMLR},
  author       = {Haoshu Xu and Hongzhe Li},
  journal      = {Journal of Machine Learning Research},
  number       = {77},
  pages        = {1-123},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Wasserstein F-tests for frechet regression on bures-wasserstein manifolds},
  url          = {https://jmlr.org/papers/v26/24-0493.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed stochastic bilevel optimization: Improved complexity and heterogeneity analysis. <em>JMLR</em>, <em>26</em>(76), 1-58. (<a href='https://jmlr.org/papers/v26/24-0187.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers solving a class of nonconvex-strongly-convex distributed stochastic bilevel optimization (DSBO) problems with personalized inner-level objectives. Most existing algorithms require computational loops for hypergradient estimation, leading to computational inefficiency. Moreover, the impact of data heterogeneity on convergence in bilevel problems is not explicitly characterized yet. To address these issues, we propose LoPA, a loopless personalized distributed algorithm that leverages a tracking mechanism for iterative approximation of inner-level solutions and Hessian-inverse matrices without relying on extra computation loops. Our theoretical analysis explicitly characterizes the heterogeneity across nodes (denoted by $b$), and establishes a sublinear rate of $\mathcal{O}( {\frac{1}{{{{\left( {1 - \rho } \right)}}K}}\!+ \!\frac{{(\frac{b}{\sqrt{m}})^{\frac{2}{3}} }}{{\left( {1 - \rho } \right)^{\frac{2}{3}} K^{\frac{2}{3}} }} \!+ \!\frac{1}{\sqrt{ K }}( {\sigma _{\operatorname{p} }} + \frac{1}{\sqrt{m}}{\sigma _{\operatorname{c} }} ) } )$ without the boundedness of local hypergradients, where ${\sigma _{\operatorname{p} }}$ and ${\sigma _{\operatorname{c} }}$ represent the gradient sampling variances associated with the inner- and outer-level variables, respectively. We also integrate LoPA with a gradient tracking scheme to eliminate the impact of data heterogeneity, yielding an improved rate of ${{\mathcal{O}}}(\frac{{1}}{{ (1-\rho)^2K }} \!+\! \frac{1}{{\sqrt{K}}}( \sigma_{\rm{p}} \!+\! \frac{1}{\sqrt{m}}\sigma_{\rm{c}} ) )$. The computational complexity of LoPA is of ${{\mathcal{O}}}({\epsilon^{-2}})$ to an $\epsilon$-stationary point, matching the communication complexity due to the loopless structure, which outperforms existing counterparts for DSBO. Numerical experiments validate the effectiveness of the proposed algorithm.},
  archive      = {J_JMLR},
  author       = {Youcheng Niu and Jinming Xu and Ying Sun and Yan Huang and Li Chai},
  journal      = {Journal of Machine Learning Research},
  number       = {76},
  pages        = {1-58},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Distributed stochastic bilevel optimization: Improved complexity and heterogeneity analysis},
  url          = {https://jmlr.org/papers/v26/24-0187.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning causal graphs via nonlinear sufficient dimension reduction. <em>JMLR</em>, <em>26</em>(75), 1-46. (<a href='https://jmlr.org/papers/v26/24-0048.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new nonparametric methodology for estimating a directed acyclic graph (DAG) from observational data. Our method is nonparametric in nature: it does not impose any specific form on the joint distribution of the underlying DAG. Instead, it relies on a linear operator on reproducing kernel Hilbert spaces to evaluate conditional independence. However, a fully nonparametric approach would involve conditioning on a large number of random variables, subjecting it to the curse of dimensionality. To solve this problem, we apply nonlinear sufficient dimension reduction to reduce the number of variables before evaluating the conditional independence. We develop an estimator for the DAG, based on a linear operator that characterizes conditional independence, and establish the consistency and convergence rates of this estimator, as well as the uniform consistency of the estimated Markov equivalence class. We introduce a modified PC-algorithm to implement the estimating procedure efficiently such that the complexity depends on the sparseness of the underlying true DAG. We demonstrate the effectiveness of our methodology through simulations and a real data analysis.},
  archive      = {J_JMLR},
  author       = {Eftychia Solea and Bing Li and Kyongwon Kim},
  journal      = {Journal of Machine Learning Research},
  number       = {75},
  pages        = {1-46},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Learning causal graphs via nonlinear sufficient dimension reduction},
  url          = {https://jmlr.org/papers/v26/24-0048.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On consistent bayesian inference from synthetic data. <em>JMLR</em>, <em>26</em>(74), 1-65. (<a href='https://jmlr.org/papers/v26/23-1428.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating synthetic data, with or without differential privacy, has attracted significant attention as a potential solution to the dilemma between making data easily available, and the privacy of data subjects. Several works have shown that consistency of downstream analyses from synthetic data, including accurate uncertainty estimation, requires accounting for the synthetic data generation. There are very few methods of doing so, most of them for frequentist analysis. In this paper, we study how to perform consistent Bayesian inference from synthetic data. We prove that mixing posterior samples obtained separately from multiple large synthetic data sets, that are sampled from a posterior predictive, converges to the posterior of the downstream analysis under standard regularity conditions when the analyst's model is compatible with the data provider's model. We also present several examples showing how the theory works in practice, and showing how Bayesian inference can fail when the compatibility assumption is not met, or the synthetic data set is not significantly larger than the original.},
  archive      = {J_JMLR},
  author       = {Ossi Räisä and Joonas Jälkö and Antti Honkela},
  journal      = {Journal of Machine Learning Research},
  number       = {74},
  pages        = {1-65},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On consistent bayesian inference from synthetic data},
  url          = {https://jmlr.org/papers/v26/23-1428.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization over a probability simplex. <em>JMLR</em>, <em>26</em>(73), 1-35. (<a href='https://jmlr.org/papers/v26/23-1166.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new iteration scheme, the Cauchy-Simplex, to optimize convex problems over the probability simplex $\{w\in\mathbb{R}^n\ |\ \sum_i w_i=1\ \textrm{and}\ w_i\geq0\}$. Specifically, we map the simplex to the positive quadrant of a unit sphere, envisage gradient descent in latent variables, and map the result back in a way that only depends on the simplex variable. Moreover, proving rigorous convergence results in this formulation leads inherently to tools from information theory (e.g., cross-entropy and KL divergence). Each iteration of the Cauchy-Simplex consists of simple operations, making it well-suited for high-dimensional problems. In continuous time, we prove that $f(x_T)-f(x^*) = O(1/T)$ for differentiable real-valued convex functions, where $T$ is the number of time steps and $w^*$ is the optimal solution. Numerical experiments of projection onto convex hulls show faster convergence than similar algorithms. Finally, we apply our algorithm to online learning problems and prove the convergence of the average regret for (1) Prediction with expert advice and (2) Universal Portfolios.},
  archive      = {J_JMLR},
  author       = {James Chok and Geoffrey M. Vasil},
  journal      = {Journal of Machine Learning Research},
  number       = {73},
  pages        = {1-35},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Optimization over a probability simplex},
  url          = {https://jmlr.org/papers/v26/23-1166.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Laplace meets moreau: Smooth approximation to infimal convolutions using laplace's method. <em>JMLR</em>, <em>26</em>(72), 1-36. (<a href='https://jmlr.org/papers/v26/24-0944.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study approximations to the Moreau envelope---and infimal convolutions more broadly---based on Laplace's method, a classical tool in analysis which ties certain integrals to suprema of their integrands. We believe the connection between Laplace's method and infimal convolutions is generally deserving of more attention in the study of optimization and partial differential equations, since it bears numerous potentially important applications, from proximal-type algorithms to Hamilton-Jacobi equations.},
  archive      = {J_JMLR},
  author       = {Ryan J. Tibshirani and Samy Wu Fung and Howard Heaton and Stanley Osher},
  journal      = {Journal of Machine Learning Research},
  number       = {72},
  pages        = {1-36},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Laplace meets moreau: Smooth approximation to infimal convolutions using laplace's method},
  url          = {https://jmlr.org/papers/v26/24-0944.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sampling and estimation on manifolds using the langevin diffusion. <em>JMLR</em>, <em>26</em>(71), 1-50. (<a href='https://jmlr.org/papers/v26/24-0829.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Error bounds are derived for sampling and estimation using a discretization of an intrinsically defined Langevin diffusion with invariant measure $\text{d}\mu_\phi \propto e^{-\phi} \mathrm{dvol}_g $ on a compact Riemannian manifold. Two estimators of linear functionals of $\mu_\phi $ based on the discretized Markov process are considered: a time-averaging estimator based on a single trajectory and an ensemble-averaging estimator based on multiple independent trajectories. Imposing no restrictions beyond a nominal level of smoothness on $\phi$, first-order error bounds, in discretization step size, on the bias and variance/mean-square error of both estimators are derived. The order of error matches the optimal rate in Euclidean and flat spaces, and leads to a first-order bound on distance between the invariant measure $\mu_\phi$ and a stationary measure of the discretized Markov process. This order is preserved even upon using retractions when exponential maps are unavailable in closed form, thus enhancing practicality of the proposed algorithms. Generality of the proof techniques, which exploit links between two partial differential equations and the semigroup of operators corresponding to the Langevin diffusion, renders them amenable for the study of a more general class of sampling algorithms related to the Langevin diffusion. Conditions for extending analysis to the case of non-compact manifolds are discussed. Numerical illustrations with distributions, log-concave and otherwise, on the manifolds of positive and negative curvature elucidate on the derived bounds and demonstrate practical utility of the sampling algorithm.},
  archive      = {J_JMLR},
  author       = {Karthik Bharath and Alexander Lewis and Akash Sharma and Michael V. Tretyakov},
  journal      = {Journal of Machine Learning Research},
  number       = {71},
  pages        = {1-50},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Sampling and estimation on manifolds using the langevin diffusion},
  url          = {https://jmlr.org/papers/v26/24-0829.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sharp bounds for sequential federated learning on heterogeneous data. <em>JMLR</em>, <em>26</em>(70), 1-55. (<a href='https://jmlr.org/papers/v26/24-0668.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two paradigms in Federated Learning (FL): parallel FL (PFL), where models are trained in a parallel manner across clients, and sequential FL (SFL), where models are trained in a sequential manner across clients. Specifically, in PFL, clients perform local updates independently and send the updated model parameters to a global server for aggregation; in SFL, one client starts its local updates only after receiving the model parameters from the previous client in the sequence. In contrast to that of PFL, the convergence theory of SFL on heterogeneous data is still lacking. To resolve the theoretical dilemma of SFL, we establish sharp convergence guarantees for SFL on heterogeneous data with both upper and lower bounds. Specifically, we derive the upper bounds for the strongly convex, general convex and non-convex objective functions, and construct the matching lower bounds for the strongly convex and general convex objective functions. Then, we compare the upper bounds of SFL with those of PFL, showing that SFL outperforms PFL on heterogeneous data (at least, when the level of heterogeneity is relatively high). Experimental results validate the counterintuitive theoretical finding.},
  archive      = {J_JMLR},
  author       = {Yipeng Li and Xinchen Lyu},
  journal      = {Journal of Machine Learning Research},
  number       = {70},
  pages        = {1-55},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Sharp bounds for sequential federated learning on heterogeneous data},
  url          = {https://jmlr.org/papers/v26/24-0668.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local linear recovery guarantee of deep neural networks at overparameterization. <em>JMLR</em>, <em>26</em>(69), 1-30. (<a href='https://jmlr.org/papers/v26/24-0192.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining whether deep neural network (DNN) models can reliably recover target functions at overparameterization is a critical yet complex issue in the theory of deep learning. To advance understanding in this area, we introduce a concept we term “local linear recovery” (LLR), a weaker form of target function recovery that renders the problem more amenable to theoretical analysis. In the sense of LLR, we prove that functions expressible by narrower DNNs are guaranteed to be recoverable from fewer samples than model parameters. Specifically, we establish upper limits on the optimistic sample sizes, defined as the smallest sample size necessary to guarantee LLR, for functions in the space of a given DNN. Furthermore, we prove that these upper bounds are achieved in the case of two-layer tanh neural networks. Our research lays a solid groundwork for future investigations into the recovery capabilities of DNNs in overparameterized scenarios.},
  archive      = {J_JMLR},
  author       = {Yaoyu Zhang and Leyang Zhang and Zhongwang Zhang and Zhiwei Bai},
  journal      = {Journal of Machine Learning Research},
  number       = {69},
  pages        = {1-30},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Local linear recovery guarantee of deep neural networks at overparameterization},
  url          = {https://jmlr.org/papers/v26/24-0192.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stabilizing sharpness-aware minimization through a simple renormalization strategy. <em>JMLR</em>, <em>26</em>(68), 1-35. (<a href='https://jmlr.org/papers/v26/24-0065.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, sharpness-aware minimization (SAM) has attracted much attention because of its surprising effectiveness in improving generalization performance. However, compared to stochastic gradient descent (SGD), it is more prone to getting stuck at the saddle points, which as a result may lead to performance degradation. To address this issue, we propose a simple renormalization strategy, dubbed Stable SAM (SSAM), so that the gradient norm of the descent step maintains the same as that of the ascent step. Our strategy is easy to implement and flexible enough to integrate with SAM and its variants, almost at no computational cost. With elementary tools from convex optimization and learning theory, we also conduct a theoretical analysis of sharpness-aware training, revealing that compared to SGD, the effectiveness of SAM is only assured in a limited regime of learning rate. In contrast, we show how SSAM extends this regime of learning rate and then it can consistently perform better than SAM with the minor modification. Finally, we demonstrate the improved performance of SSAM on several representative data sets and tasks.},
  archive      = {J_JMLR},
  author       = {Chengli Tan and Jiangshe Zhang and Junmin Liu and Yicheng Wang and Yunda Hao},
  journal      = {Journal of Machine Learning Research},
  number       = {68},
  pages        = {1-35},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Stabilizing sharpness-aware minimization through a simple renormalization strategy},
  url          = {https://jmlr.org/papers/v26/24-0065.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained change point detection for topic modeling with pitman-yor process. <em>JMLR</em>, <em>26</em>(67), 1-53. (<a href='https://jmlr.org/papers/v26/23-1576.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying change points in dynamic text data is crucial for understanding the evolving nature of topics across various sources, such as news articles, scientific papers, and social media posts. While topic modeling has become a widely used technique for this purpose, capturing fine-grained shifts in individual topics over time remains a significant challenge. Traditional approaches typically use a two-stage process, separating topic modeling and change point detection. However, this separation can lead to information loss and inconsistency in capturing subtle changes in topic evolution. To address this issue, we propose TOPIC-PYP, a change point detection model specifically designed for fine-grained topic-level analysis, i.e., detecting change points for each individual topic. By leveraging the Pitman-Yor process, TOPIC-PYP effectively captures the dynamic evolution of topic meanings over time. Unlike traditional methods, TOPIC-PYP integrates topic modeling and change point detection into a unified framework, facilitating a more comprehensive understanding of the relationship between topic evolution and change points. Experimental evaluations on both synthetic and real-world datasets demonstrate the effectiveness of TOPIC-PYP in accurately detecting change points and generating high-quality topics.},
  archive      = {J_JMLR},
  author       = {Feifei Wang and Zimeng Zhao and Ruimin Ye and Xiaoge Gu and Xiaoling Lu},
  journal      = {Journal of Machine Learning Research},
  number       = {67},
  pages        = {1-53},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Fine-grained change point detection for topic modeling with pitman-yor process},
  url          = {https://jmlr.org/papers/v26/23-1576.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deletion robust non-monotone submodular maximization over matroids. <em>JMLR</em>, <em>26</em>(66), 1-28. (<a href='https://jmlr.org/papers/v26/23-1219.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the deletion robust version of submodular maximization under matroid constraints. The goal is to extract a small-size summary of the data set that contains a high-value independent set even after an adversary deletes some elements. We present constant-factor approximation algorithms, whose space complexity depends on the rank $k$ of the matroid, the number $d$ of deleted elements, and the input precision $\varepsilon$. In the centralized setting we present a $(4.494+O(\varepsilon))$-approximation algorithm with summary size $O( \frac{k+d}{\varepsilon^2}\log \frac{k}{\varepsilon})$ that improves to a $(3.582+O(\varepsilon))$-approximation with $O(k + \frac{d}{\varepsilon^2}\log \frac{k}{\varepsilon})$ summary size when the objective is monotone. In the streaming setting we provide a $(9.294 + O(\varepsilon))$-approximation algorithm with summary size and memory $O(k + \frac{d}{\varepsilon^2}\log \frac{k}{\varepsilon})$; the approximation factor is then improved to $(5.582+O(\varepsilon))$ in the monotone case.},
  archive      = {J_JMLR},
  author       = {Paul Dütting and Federico Fusco and Silvio Lattanzi and Ashkan Norouzi-Fard and Morteza Zadimoghaddam},
  journal      = {Journal of Machine Learning Research},
  number       = {66},
  pages        = {1-28},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Deletion robust non-monotone submodular maximization over matroids},
  url          = {https://jmlr.org/papers/v26/23-1219.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instability, computational efficiency and statistical accuracy. <em>JMLR</em>, <em>26</em>(65), 1-68. (<a href='https://jmlr.org/papers/v26/22-0300.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many statistical estimators are defined as the fixed point of a data-dependent operator, with estimators based on minimizing a cost function being an important special case. The limiting performance of such estimators depends on the properties of the population-level operator in the idealized limit of infinitely many samples. We develop a general framework that yields bounds on statistical accuracy based on the interplay between the deterministic convergence rate of the algorithm at the population level, and its degree of (in)stability when applied to an empirical object based on $n$ samples. Using this framework, we analyze both stable forms of gradient descent and some higher-order and unstable algorithms, including Newton's method and its cubic-regularized variant, as well as the EM algorithm. We provide applications of our general results to several concrete classes of models, including Gaussian mixture estimation, non-linear regression models, and informative non-response models. We exhibit cases in which an unstable algorithm can achieve the same statistical accuracy as a stable algorithm in exponentially fewer steps---namely, with the number of iterations being reduced from polynomial to logarithmic in sample size $n$.},
  archive      = {J_JMLR},
  author       = {Nhat Ho and Koulik Khamaru and Raaz Dwivedi and Martin J. Wainwright and Michael I. Jordan and Bin Yu},
  journal      = {Journal of Machine Learning Research},
  number       = {65},
  pages        = {1-68},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Instability, computational efficiency and statistical accuracy},
  url          = {https://jmlr.org/papers/v26/22-0300.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of local geometric structure on manifolds from noisy data. <em>JMLR</em>, <em>26</em>(64), 1-89. (<a href='https://jmlr.org/papers/v26/25-0183.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common observation in data-driven applications is that high-dimensional data have a low intrinsic dimension, at least locally. In this work, we consider the problem of point estimation for manifold-valued data. Namely, given a finite set of noisy samples of $\mathcal{M}$, a $d$ dimensional submanifold of $\mathbb{R}^D$, and a point $r$ near the manifold we aim to project $r$ onto the manifold. Assuming that the data was sampled uniformly from a tubular neighborhood of a $k$-times smooth boundaryless and compact manifold, we present an algorithm that takes $r$ from this neighborhood and outputs $\hat p_n\in \mathbb{R}^D$, and $\widehat{T_{\hat p_n}\mathcal{M}}$ an element in the Grassmannian $Gr(d, D)$. We prove that as the number of samples $n\to\infty$, the point $\hat p_n$ converges to $\mathbf{p}\in \mathcal{M}$, the projection of $r$ onto $\mathcal{M}$, and $\widehat{T_{\hat p_n}\mathcal{M}}$ converges to $T_{\mathbf{p}}\mathcal{M}$ (the tangent space at that point) with high probability. Furthermore, we show that $\hat p_n$ approaches the manifold with an asymptotic rate of $n^{-\frac{k}{2k + d}}$, and that $\hat p_n, \widehat{T_{\hat p_n}\mathcal{M}}$ approach $\mathbf{p}$ and $T_{\mathbf{p}}\mathcal{M}$ correspondingly with asymptotic rates of $n^{-\frac{k-1}{2k + d}}$. %While we These rates coincide with the optimal rates for the estimation of function derivatives.},
  archive      = {J_JMLR},
  author       = {Yariv Aizenbud and Barak Sober},
  journal      = {Journal of Machine Learning Research},
  number       = {64},
  pages        = {1-89},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Estimation of local geometric structure on manifolds from noisy data},
  url          = {https://jmlr.org/papers/v26/25-0183.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ontolearn---A framework for large-scale OWL class expression learning in python. <em>JMLR</em>, <em>26</em>(63), 1-6. (<a href='https://jmlr.org/papers/v26/24-1113.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present Ontolearn---a framework for learning OWL class expressions over large knowledge graphs. Ontolearn contains efficient implementations of recent state-of-the-art symbolic and neuro-symbolic class expression learners including EvoLearner and DRILL. A learned OWL class expression can be used to classify instances in the knowledge graph. Furthermore, Ontolearn integrates a verbalization module based on an LLM to translate complex OWL class expressions into natural language sentences. By mapping OWL class expressions into respective SPARQL queries, Ontolearn can be easily used to operate over a remote triplestore. The source code of Ontolearn is available at https://github.com/dice-group/Ontolearn.},
  archive      = {J_JMLR},
  author       = {Caglar Demir and Alkid Baci and N'Dah Jean Kouagou and Leonie Nora Sieger and Stefan Heindorf and Simon Bin and Lukas Blübaum and Alexander Bigerl and Axel-Cyrille Ngonga Ngomo},
  journal      = {Journal of Machine Learning Research},
  number       = {63},
  pages        = {1-6},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Ontolearn---A framework for large-scale OWL class expression learning in python},
  url          = {https://jmlr.org/papers/v26/24-1113.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuously evolving rewards in an open-ended environment. <em>JMLR</em>, <em>26</em>(62), 1-51. (<a href='https://jmlr.org/papers/v26/24-0847.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unambiguous identification of the rewards driving behaviours of entities operating in complex open-ended real-world environments is difficult, in part because goals and associated behaviours emerge endogenously and are dynamically updated as environments change. Reproducing such dynamics in models would be useful in many domains, particularly where fixed reward functions limit the adaptive capabilities of agents. Simulation experiments described here assess a candidate algorithm for the dynamic updating of the reward function, RULE: Reward Updating through Learning and Expectation. The approach is tested in a simplified ecosystem-like setting where experiments challenge entities' survival, calling for significant behavioural change. The population of entities successfully demonstrate the abandonment of an initially rewarded but ultimately detrimental behaviour, amplification of beneficial behaviour, and appropriate responses to novel items added to their environment. These adjustments happen through endogenous modification of the entities' reward function, during continuous learning, without external intervention.},
  archive      = {J_JMLR},
  author       = {Richard M. Bailey},
  journal      = {Journal of Machine Learning Research},
  number       = {62},
  pages        = {1-51},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Continuously evolving rewards in an open-ended environment},
  url          = {https://jmlr.org/papers/v26/24-0847.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recursive causal discovery. <em>JMLR</em>, <em>26</em>(61), 1-65. (<a href='https://jmlr.org/papers/v26/24-0384.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery from observational data, i.e., learning the causal graph from a finite set of samples from the joint distribution of the variables, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains. Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting. This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a). These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery. Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively. This reduction not only minimizes conditioning sets in each conditional independence (CI) test, leading to fewer errors but also significantly decreases the number of required CI tests. The worst-case performances of these methods nearly match the lower bound. In this paper, we present a unified framework for the proposed algorithms, refined with additional details and enhancements for a coherent presentation. A comprehensive literature review is also included, comparing the computational complexity of our methods with existing approaches, showcasing their state-of-the-art efficiency. Another contribution of this paper is the release of RCD, a Python package that efficiently implements these algorithms. This package is designed for practitioners and researchers interested in applying these methods in practical scenarios. The package is available at github.com/ban-epfl/rcd, with comprehensive documentation provided at rcdpackage.com.},
  archive      = {J_JMLR},
  author       = {Ehsan Mokhtarian and Sepehr Elahi and Sina Akbari and Negar Kiyavash},
  journal      = {Journal of Machine Learning Research},
  number       = {61},
  pages        = {1-65},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Recursive causal discovery},
  url          = {https://jmlr.org/papers/v26/24-0384.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of active feature acquisition methods for time-varying feature settings. <em>JMLR</em>, <em>26</em>(60), 1-84. (<a href='https://jmlr.org/papers/v26/23-1635.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning methods often assume that input features are available at no cost. However, in domains like healthcare, where acquiring features could be expensive or harmful, it is necessary to balance a feature's acquisition cost against its predictive value. The task of training an AI agent to decide which features to acquire is called active feature acquisition (AFA). By deploying an AFA agent, we effectively alter the acquisition strategy and trigger a distribution shift. To safely deploy AFA agents under this distribution shift, we present the problem of active feature acquisition performance evaluation (AFAPE). We examine AFAPE under i) a no direct effect (NDE) assumption, stating that acquisitions do not affect the underlying feature values; and ii) a no unobserved confounding (NUC) assumption, stating that retrospective feature acquisition decisions were only based on observed features. We show that one can apply missing data methods under the NDE assumption and offline reinforcement learning under the NUC assumption. When NUC and NDE hold, we propose a novel semi-offline reinforcement learning framework. This framework requires a weaker positivity assumption and introduces three new estimators: A direct method (DM), an inverse probability weighting (IPW), and a double reinforcement learning (DRL) estimator.},
  archive      = {J_JMLR},
  author       = {Henrik von Kleist and Alireza Zamanian and Ilya Shpitser and Narges Ahmidi},
  journal      = {Journal of Machine Learning Research},
  number       = {60},
  pages        = {1-84},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Evaluation of active feature acquisition methods for time-varying feature settings},
  url          = {https://jmlr.org/papers/v26/23-1635.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On adaptive stochastic optimization for streaming data: A newton's method with O(dN) operations. <em>JMLR</em>, <em>26</em>(59), 1-49. (<a href='https://jmlr.org/papers/v26/23-1565.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic optimization methods face new challenges in the realm of streaming data, characterized by a continuous flow of large, high-dimensional data. While first-order methods, like stochastic gradient descent, are the natural choice for such data, they often struggle with ill-conditioned problems. In contrast, second-order methods, such as Newton's method, offer a potential solution but are computationally impractical for large-scale streaming applications. This paper introduces adaptive stochastic optimization methods that effectively address ill-conditioned problems while functioning in a streaming context. Specifically, we present adaptive inversion-free stochastic quasi-Newton methods with computational complexity matching that of first-order methods, $\mathcal{O}(dN)$, where $d$ represents the number of dimensions/features and $N$ the number of data points. Theoretical analysis establishes their asymptotic efficiency, and empirical studies demonstrate their effectiveness in scenarios with complex covariance structures and poor initializations. In particular, we demonstrate that our adaptive quasi-Newton methods can outperform or match existing first- and second-order methods.},
  archive      = {J_JMLR},
  author       = {Antoine Godichon-Baggioni and Nicklas Werge},
  journal      = {Journal of Machine Learning Research},
  number       = {59},
  pages        = {1-49},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On adaptive stochastic optimization for streaming data: A newton's method with O(dN) operations},
  url          = {https://jmlr.org/papers/v26/23-1565.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Determine the number of states in hidden markov models via marginal likelihood. <em>JMLR</em>, <em>26</em>(58), 1-59. (<a href='https://jmlr.org/papers/v26/23-0343.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models (HMM) have been widely used by scientists to model stochastic systems: the underlying process is a discrete Markov chain, and the observations are noisy realizations of the underlying process. Determining the number of hidden states for an HMM is a model selection problem which is yet to be satisfactorily solved, especially for the popular Gaussian HMM with heterogeneous covariance. In this paper, we propose a consistent method for determining the number of hidden states of HMM based on the marginal likelihood, which is obtained by integrating out both the parameters and hidden states. Moreover, we show that the model selection problem of HMM includes the order selection problem of finite mixture models as a special case. We give rigorous proof of the consistency of the proposed marginal likelihood method and provide an efficient computation method for practical implementation. We numerically compare the proposed method with the Bayesian information criterion (BIC), demonstrating the effectiveness of the proposed marginal likelihood method.},
  archive      = {J_JMLR},
  author       = {Yang Chen and Cheng-Der Fuh and Chu-Lan Michael Kao},
  journal      = {Journal of Machine Learning Research},
  number       = {58},
  pages        = {1-59},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Determine the number of states in hidden markov models via marginal likelihood},
  url          = {https://jmlr.org/papers/v26/23-0343.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variance-aware estimation of kernel mean embedding. <em>JMLR</em>, <em>26</em>(57), 1-48. (<a href='https://jmlr.org/papers/v26/23-0161.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important feature of kernel mean embeddings (KME) is that the rate of convergence of the empirical KME to the true distribution KME can be bounded independently of the dimension of the space, properties of the distribution and smoothness features of the kernel. We show how to speed-up convergence by leveraging variance information in the reproducing kernel Hilbert space. Furthermore, we show that even when such information is a priori unknown, we can efficiently estimate it from the data, recovering the desiderata of a distribution agnostic bound that enjoys acceleration in fortuitous settings. We further extend our results from independent data to stationary mixing sequences and illustrate our methods in the context of hypothesis testing and robust parametric estimation.},
  archive      = {J_JMLR},
  author       = {Geoffrey Wolfer and Pierre Alquier},
  journal      = {Journal of Machine Learning Research},
  number       = {57},
  pages        = {1-48},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Variance-aware estimation of kernel mean embedding},
  url          = {https://jmlr.org/papers/v26/23-0161.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaling ResNets in the large-depth regime. <em>JMLR</em>, <em>26</em>(56), 1-48. (<a href='https://jmlr.org/papers/v26/22-0664.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep ResNets are recognized for achieving state-of-the-art results in complex machine learning tasks. However, the remarkable performance of these architectures relies on a training procedure that needs to be carefully crafted to avoid vanishing or exploding gradients, particularly as the depth $L$ increases. No consensus has been reached on how to mitigate this issue, although a widely discussed strategy consists in scaling the output of each layer by a factor $\alpha_L$. We show in a probabilistic setting that with standard i.i.d. initializations, the only non-trivial dynamics is for $\alpha_L = \frac{1}{\sqrt{L}}$---other choices lead either to explosion or to identity mapping. This scaling factor corresponds in the continuous-time limit to a neural stochastic differential equation, contrarily to a widespread interpretation that deep ResNets are discretizations of neural ordinary differential equations. By contrast, in the latter regime, stability is obtained with specific correlated initializations and $\alpha_L = \frac{1}{L}$. Our analysis suggests a strong interplay between scaling and regularity of the weights as a function of the layer index. Finally, in a series of experiments, we exhibit a continuous range of regimes driven by these two parameters, which jointly impact performance before and after training.},
  archive      = {J_JMLR},
  author       = {Pierre Marion and Adeline Fermanian and Gérard Biau and Jean-Philippe Vert},
  journal      = {Journal of Machine Learning Research},
  number       = {56},
  pages        = {1-48},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Scaling ResNets in the large-depth regime},
  url          = {https://jmlr.org/papers/v26/22-0664.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative evaluation of quantification methods. <em>JMLR</em>, <em>26</em>(55), 1-54. (<a href='https://jmlr.org/papers/v26/21-0241.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification represents the problem of estimating the distribution of class labels on unseen data. It also represents a growing research field in supervised machine learning, for which a large variety of different algorithms has been proposed in recent years. However, a comprehensive empirical comparison of quantification methods that supports algorithm selection is not available yet. In this work, we close this research gap by conducting a thorough empirical performance comparison of 24 different quantification methods on in total more than 40 datasets, considering binary as well as multiclass quantification settings. We observe that no single algorithm generally outperforms all competitors, but identify a group of methods that perform best in the binary setting, including the threshold selection-based median sweep and TSMax methods, the DyS framework including the HDy method, Forman's mixture model, and Friedman's method. For the multiclass setting, we observe that a different, broad group of algorithms yields good performance, including the HDx method, the generalized probabilistic adjusted count, the readme method, the energy distance minimization method, the EM algorithm for quantification, and Friedman's method. We also find that tuning the underlying classifiers has in most cases only a limited impact on the quantification performance. More generally, we find that the performance on multiclass quantification is inferior to the results obtained in the binary setting. Our results can guide practitioners who intend to apply quantification algorithms and help researchers identify opportunities for future research.},
  archive      = {J_JMLR},
  author       = {Tobias Schumacher and Markus Strohmaier and Florian Lemmerich},
  journal      = {Journal of Machine Learning Research},
  number       = {55},
  pages        = {1-54},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {A comparative evaluation of quantification methods},
  url          = {https://jmlr.org/papers/v26/21-0241.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightning UQ box: Uncertainty quantification for neural networks. <em>JMLR</em>, <em>26</em>(54), 1-7. (<a href='https://jmlr.org/papers/v26/24-2110.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although neural networks have shown impressive results in a multitude of application domains, the "black box" nature of deep learning and lack of confidence estimates have led to scepticism, especially in domains like medicine and physics where such estimates are critical. Research on uncertainty quantification (UQ) has helped elucidate the reliability of these models, but existing implementations of these UQ methods are sparse and difficult to reuse. To this end, we introduce Lightning UQ Box, a PyTorch-based Python library for deep learning-based UQ methods powered by PyTorch Lightning. Lightning UQ Box supports classification, regression, semantic segmentation, and pixelwise regression applications, and UQ methods from a variety of theoretical motivations. With this library, we provide an entry point for practitioners new to UQ, as well as easy-to-use components and tools for scalable deep learning applications.},
  archive      = {J_JMLR},
  author       = {Nils Lehmann and Nina Maria Gottschling and Jakob Gawlikowski and Adam J. Stewart and Stefan Depeweg and Eric Nalisnick},
  journal      = {Journal of Machine Learning Research},
  number       = {54},
  pages        = {1-7},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Lightning UQ box: Uncertainty quantification for neural networks},
  url          = {https://jmlr.org/papers/v26/24-2110.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaling data-constrained language models. <em>JMLR</em>, <em>26</em>(53), 1-66. (<a href='https://jmlr.org/papers/v26/24-1000.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current trend of scaling language models involves increasing both parameter count and training data set size. Extrapolating this trend suggests that training data set size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training data set with code data or removing commonly used filters. Models and data sets from our 400 training runs are freely available at https://github.com/huggingface/datablations.},
  archive      = {J_JMLR},
  author       = {Niklas Muennighoff and Alexander M. Rush and Boaz Barak and Teven Le Scao and Aleksandra Piktus and Nouamane Tazi and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
  journal      = {Journal of Machine Learning Research},
  number       = {53},
  pages        = {1-66},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Scaling data-constrained language models},
  url          = {https://jmlr.org/papers/v26/24-1000.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curvature-based clustering on graphs. <em>JMLR</em>, <em>26</em>(52), 1-67. (<a href='https://jmlr.org/papers/v26/24-0781.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised node clustering (or community detection) is a classical graph learning task. In this paper, we study algorithms that exploit the geometry of the graph to identify densely connected substructures, which form clusters or communities. Our method implements discrete Ricci curvatures and their associated geometric flows, under which the edge weights of the graph evolve to reveal its community structure. We consider several discrete curvature notions and analyze the utility of the resulting algorithms. In contrast to prior literature, we study not only single-membership community detection, where each node belongs to exactly one community, but also mixed-membership community detection, where communities may overlap. For the latter, we argue that it is beneficial to perform community detection on the line graph, i.e., the graph's dual. We provide both theoretical and empirical evidence for the utility of our curvature-based clustering algorithms. In addition, we give several results on the relationship between the curvature of a graph and that of its dual, which enable the efficient implementation of our proposed mixed-membership community detection approach and which may be of independent interest for curvature-based network analysis.},
  archive      = {J_JMLR},
  author       = {Yu Tian and Zachary Lubberts and Melanie Weber},
  journal      = {Journal of Machine Learning Research},
  number       = {52},
  pages        = {1-67},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Curvature-based clustering on graphs},
  url          = {https://jmlr.org/papers/v26/24-0781.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composite goodness-of-fit tests with kernels. <em>JMLR</em>, <em>26</em>(51), 1-60. (<a href='https://jmlr.org/papers/v26/24-0276.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. Our tests make use of minimum distance estimators based on kernel-based distances such as the maximum mean discrepancy. As our main result, we show that we are able to estimate the parameter and conduct our test on the same data (without data splitting), while maintaining a correct test level. We also prove that the popular wild bootstrap will lead to an overly conservative test, and show that the parametric bootstrap is consistent and can lead to significantly improved performance in practice. Our approach is illustrated on a range of problems, including testing for goodness-of-fit of a non-parametric density model, and an intractable generative model of a biological cellular network.},
  archive      = {J_JMLR},
  author       = {Oscar Key and Arthur Gretton and François-Xavier Briol and Tamara Fernandez},
  journal      = {Journal of Machine Learning Research},
  number       = {51},
  pages        = {1-60},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Composite goodness-of-fit tests with kernels},
  url          = {https://jmlr.org/papers/v26/24-0276.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PFLlib: A beginner-friendly and comprehensive personalized federated learning library and benchmark. <em>JMLR</em>, <em>26</em>(50), 1-10. (<a href='https://jmlr.org/papers/v26/23-1634.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amid the ongoing advancements in Federated Learning (FL), a machine learning paradigm that allows collaborative learning with data privacy protection, personalized FL (pFL) has gained significant prominence as a research direction within the FL domain. Whereas traditional FL (tFL) focuses on jointly learning a global model, pFL aims to balance each client's global and personalized goals in FL settings. To foster the pFL research community, we started and built PFLlib, a comprehensive pFL library with an integrated benchmark platform. In PFLlib, we implemented 37 state-of-the-art FL algorithms (8 tFL algorithms and 29 pFL algorithms) and provided various evaluation environments with three statistically heterogeneous scenarios and 24 datasets. At present, PFLlib has gained more than 1600 stars and 300 forks on GitHub.},
  archive      = {J_JMLR},
  author       = {Jianqing Zhang and Yang Liu and Yang Hua and Hao Wang and Tao Song and Zhengui Xue and Ruhui Ma and Jian Cao},
  journal      = {Journal of Machine Learning Research},
  number       = {50},
  pages        = {1-10},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {PFLlib: A beginner-friendly and comprehensive personalized federated learning library and benchmark},
  url          = {https://jmlr.org/papers/v26/23-1634.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of SGD batch size on autoencoder learning: Sparsity, sharpness, and feature learning. <em>JMLR</em>, <em>26</em>(49), 1-61. (<a href='https://jmlr.org/papers/v26/23-1022.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate the dynamics of stochastic gradient descent (SGD) when training a single-neuron autoencoder with linear or ReLU activation on orthogonal data. We show that for this non-convex problem, randomly initialized SGD with a constant step size successfully finds a global minimum for any batch size choice. However, the particular global minimum found depends upon the batch size. In the full-batch setting, we show that the solution is dense (i.e., not sparse) and is highly aligned with its initialized direction, showing that relatively little feature learning occurs. On the other hand, for any batch size strictly smaller than the number of samples, SGD finds a global minimum that is sparse and nearly orthogonal to its initialization, showing that the randomness of stochastic gradients induces a qualitatively different type of "feature selection" in this setting. Moreover, if we measure the sharpness of the minimum by the trace of the Hessian, the minima found with full-batch gradient descent are flatter than those found with strictly smaller batch sizes, in contrast to previous works which suggest that large batches lead to sharper minima. To prove convergence of SGD with a constant step size, we introduce a powerful tool from the theory of non-homogeneous random walks which may be of independent interest.},
  archive      = {J_JMLR},
  author       = {Nikhil Ghosh and Spencer Frei and Wooseok Ha and Bin Yu},
  journal      = {Journal of Machine Learning Research},
  number       = {49},
  pages        = {1-61},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {The effect of SGD batch size on autoencoder learning: Sparsity, sharpness, and feature learning},
  url          = {https://jmlr.org/papers/v26/23-1022.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and robust transfer learning of optimal individualized treatment regimes with right-censored survival data. <em>JMLR</em>, <em>26</em>(48), 1-54. (<a href='https://jmlr.org/papers/v26/23-0335.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An individualized treatment regime (ITR) is a decision rule that assigns treatments based on patients' characteristics. The value function of an ITR is the expected outcome in a counterfactual world had this ITR been implemented. Recently, there has been increasing interest in combining heterogeneous data sources, such as leveraging the complementary features of randomized controlled trial (RCT) data and a large observational study (OS). Usually, a covariate shift exists between the source and target population, rendering the source-optimal ITR not optimal for the target population. We present an efficient and robust transfer learning framework for estimating the optimal ITR with right-censored survival data that generalizes well to the target population. The value function accommodates a broad class of functionals of survival distributions, including survival probabilities and restrictive mean survival times (RMSTs). We propose a doubly robust estimator of the value function, and the optimal ITR is learned by maximizing the value function within a pre-specified class of ITRs. We establish the cubic rate of convergence for the estimated parameter indexing the optimal ITR, and show that the proposed optimal value estimator is consistent and asymptotically normal even with flexible machine learning methods for nuisance parameter estimation. We evaluate the empirical performance of the proposed method by simulation studies and a real data application of sodium bicarbonate therapy for patients with severe metabolic acidaemia in the intensive care unit (ICU), combining a RCT and an observational study with heterogeneity.},
  archive      = {J_JMLR},
  author       = {Pan Zhao and Julie Josse and Shu Yang},
  journal      = {Journal of Machine Learning Research},
  number       = {48},
  pages        = {1-54},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Efficient and robust transfer learning of optimal individualized treatment regimes with right-censored survival data},
  url          = {https://jmlr.org/papers/v26/23-0335.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAGs as minimal I-maps for the induced models of causal bayesian networks under conditioning. <em>JMLR</em>, <em>26</em>(47), 1-62. (<a href='https://jmlr.org/papers/v26/23-0002.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks (BNs) are a powerful tool for knowledge representation and reasoning, especially for complex systems. A critical task in the applications of BNs is conditional inference or inference in the presence of selection bias. However, post-conditioning, the conditional distribution family of a BN can become complex for analysis, and the corresponding induced subgraph may not accurately encode the conditional independencies for the remaining variables. In this work, we first investigate the conditions under which a BN remains closed under conditioning, meaning that the induced subgraph is consistent with the structural information of conditional distributions. Conversely, when a BN is not closed, we aim to construct a new directed acyclic graph (DAG) as a minimal $\mathcal{I}$-map for the conditional model by incorporating directed edges into the original induced graph. We present an equivalent characterization of this minimal $\mathcal{I}$-map and develop an efficient algorithm for its identification. The proposed framework improves the efficiency of conditional inference of a BN. Additionally, the DAG minimal $\mathcal{I}$-map offers graphical criteria for the safe integration of knowledge from diverse sources (subpopulations/conditional distributions), facilitating correct parameter estimation. Both theoretical analysis and simulation studies demonstrate that using a DAG minimal $\mathcal{I}$-map for conditional inference is more effective than traditional methods based on the joint distribution of the original BN.},
  archive      = {J_JMLR},
  author       = {Xiangdong Xie and Jiahua Guo and Yi Sun},
  journal      = {Journal of Machine Learning Research},
  number       = {47},
  pages        = {1-62},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {DAGs as minimal I-maps for the induced models of causal bayesian networks under conditioning},
  url          = {https://jmlr.org/papers/v26/23-0002.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjusted expected improvement for cumulative regret minimization in noisy bayesian optimization. <em>JMLR</em>, <em>26</em>(46), 1-33. (<a href='https://jmlr.org/papers/v26/22-0523.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expected improvement (EI) is one of the most popular acquisition functions for Bayesian optimization (BO) and has demonstrated good empirical performances in many applications for the minimization of simple regret. However, under the evaluation metric of cumulative regret, the performance of EI may not be competitive, and its existing theoretical regret upper bound still has room for improvement. To adapt the EI for better performance under cumulative regret, we introduce a novel quantity called the evaluation cost which is compared against the acquisition function, and with this, develop the expected improvement-cost (EIC) algorithm. In each iteration of EIC, a new point with the largest acquisition function value is sampled, only if that value exceeds its evaluation cost. If none meets this criteria, the current best point is resampled. This evaluation cost quantifies the potential downside of sampling a point, which is important under the cumulative regret metric as the objective function value in every iteration affects the performance measure. We establish in theory a high-probability regret upper bound of EIC based on the maximum information gain, which is tighter than the bound of existing EI-based algorithms. It is also comparable to the regret bound of other popular BO algorithms such as Thompson sampling (GP-TS) and upper confidence bound (GP-UCB). We further perform experiments to illustrate the improvement of EIC over several popular BO algorithms.},
  archive      = {J_JMLR},
  author       = {Shouri Hu and Haowei Wang and Zhongxiang Dai and Bryan Kian Hsiang Low and Szu Hui Ng},
  journal      = {Journal of Machine Learning Research},
  number       = {46},
  pages        = {1-33},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Adjusted expected improvement for cumulative regret minimization in noisy bayesian optimization},
  url          = {https://jmlr.org/papers/v26/22-0523.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Manifold fitting under unbounded noise. <em>JMLR</em>, <em>26</em>(45), 1-55. (<a href='https://jmlr.org/papers/v26/21-0039.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of non-Euclidean statistical analysis, a trend has emerged in recent times, of attempts to recover a low dimensional structure, namely a manifold, underlying the high dimensional data. Recovering the manifold requires the noise to be of a certain concentration and prevailing methods address this requirement by constructing an approximated manifold that is based on the tangent space estimation at each sample point. Although theoretical convergence for these methods is guaranteed, the samples are either noiseless or the noise is bounded. However, if the noise is unbounded, as is commonplace, the tangent space estimation at the noisy samples will be blurred – an undesirable outcome since fitting a manifold from the blurred tangent space might be more greatly compromised in terms of its accuracy. In this paper, we introduce a new manifold-fitting method, whereby the output manifold is constructed by directly estimating the tangent spaces at the projected points on the latent manifold, rather than at the sample points, thus reducing the error caused by the noise. Assuming the noise is unbounded, our new method has a high probability of achieving theoretical convergence, in terms of the upper bound of the distance between the estimated and latent manifold. The smoothness of the estimated manifold is also evaluated by bounding the supremum of twice difference above. Numerical simulations are conducted as part of this new method to help validate our theoretical findings and demonstrate the advantages of our method over other relevant manifold fitting methods. Finally, our method is applied to real data examples.},
  archive      = {J_JMLR},
  author       = {Zhigang Yao and Yuqing Xia},
  journal      = {Journal of Machine Learning Research},
  number       = {45},
  pages        = {1-55},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Manifold fitting under unbounded noise},
  url          = {https://jmlr.org/papers/v26/21-0039.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning global nash equilibrium in team competitive games with generalized fictitious cross-play. <em>JMLR</em>, <em>26</em>(44), 1-30. (<a href='https://jmlr.org/papers/v26/24-1503.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-play (SP) is a popular multi-agent reinforcement learning framework for competitive games. Despite the empirical success, the theoretical properties of SP are limited to two-player settings. For team competitive games where two teams of cooperative agents compete with each other, we show a counter-example where SP cannot converge to a global Nash equilibrium (NE) with high probability. Policy-Space Response Oracles (PSRO) is an alternative framework that finds NEs by iteratively learning the best response (BR) to previous policies. PSRO can be directly extended to team competitive games with unchanged convergence properties by learning team BRs, but its repeated training from scratch makes it hard to scale to complex games. In this work, we propose Generalized Fictitious Cross-Play (GFXP), a novel algorithm that inherits benefits from both frameworks. GFXP simultaneously trains an SP-based main policy and a counter population. The main policy is trained by fictitious self-play and cross-play against the counter population, while the counter policies are trained as the BRs to the main policy's checkpoints. We evaluate GFXP in matrix games and gridworld domains where GFXP achieves the lowest exploitabilities. We further conduct experiments in a challenging football game where GFXP defeats SOTA models with over 94% win rate.},
  archive      = {J_JMLR},
  author       = {Zelai Xu and Chao Yu and Yancheng Liang and Yi Wu and Yu Wang},
  journal      = {Journal of Machine Learning Research},
  number       = {44},
  pages        = {1-30},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Learning global nash equilibrium in team competitive games with generalized fictitious cross-play},
  url          = {https://jmlr.org/papers/v26/24-1503.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wasserstein convergence guarantees for a general class of score-based generative models. <em>JMLR</em>, <em>26</em>(43), 1-54. (<a href='https://jmlr.org/papers/v26/24-0902.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Score-based generative models are a recent class of deep generative models with state-of-the-art performance in many applications. In this paper, we establish convergence guarantees for a general class of score-based generative models in the 2-Wasserstein distance, assuming accurate score estimates and smooth log-concave data distribution. We specialize our results to several concrete score-based generative models with specific choices of forward processes modeled by stochastic differential equations, and obtain an upper bound on the iteration complexity for each model, which demonstrates the impacts of different choices of the forward processes. We also provide a lower bound when the data distribution is Gaussian. Numerically, we experiment with score-based generative models with different forward processes for unconditional image generation on CIFAR-10. We find that the experimental results are in good agreement with our theoretical predictions on the iteration complexity.},
  archive      = {J_JMLR},
  author       = {Xuefeng Gao and Hoang M. Nguyen and Lingjiong Zhu},
  journal      = {Journal of Machine Learning Research},
  number       = {43},
  pages        = {1-54},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Wasserstein convergence guarantees for a general class of score-based generative models},
  url          = {https://jmlr.org/papers/v26/24-0902.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extremal graphical modeling with latent variables via convex optimization. <em>JMLR</em>, <em>26</em>(42), 1-68. (<a href='https://jmlr.org/papers/v26/24-0472.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extremal graphical models encode the conditional independence structure of multivariate extremes and provide a powerful tool for quantifying the risk of rare events. Prior work on learning these graphs from data has focused on the setting where all relevant variables are observed. For the popular class of Husler-Reiss models, we propose the eglatent method, a tractable convex program for learning extremal graphical models in the presence of latent variables. Our approach decomposes the Husler-Reiss precision matrix into a sparse component encoding the graphical structure among the observed variables after conditioning on the latent variables, and a low-rank component encoding the effect of a few latent variables on the observed variables. We provide finite-sample guarantees of eglatent and show that it consistently recovers the conditional graph as well as the number of latent variables. We highlight the improved performances of our approach on synthetic and real data.},
  archive      = {J_JMLR},
  author       = {Sebastian Engelke and Armeen Taeb},
  journal      = {Journal of Machine Learning Research},
  number       = {42},
  pages        = {1-68},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Extremal graphical modeling with latent variables via convex optimization},
  url          = {https://jmlr.org/papers/v26/24-0472.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the approximation of kernel functions. <em>JMLR</em>, <em>26</em>(41), 1-30. (<a href='https://jmlr.org/papers/v26/24-0270.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various methods in statistical learning build on kernels considered in reproducing kernel Hilbert spaces. In applications, the kernel is often selected based on characteristics of the problem and the data. This kernel is then employed to infer response variables at points, where no explanatory data were observed. The data considered here are located in compact sets in higher dimensions and the paper addresses approximations of the kernel itself. The new approach considers Taylor series approximations of radial kernel functions. For the Gauss kernel on the unit cube, the paper establishes an upper bound of the associated eigenfunctions, which grows only polynomially with respect to the index. The novel approach substantiates smaller regularization parameters than considered in the literature, overall leading to better approximations. This improvement confirms low rank approximation methods such as the Nyström method.},
  archive      = {J_JMLR},
  author       = {Paul Dommel and Alois Pichler},
  journal      = {Journal of Machine Learning Research},
  number       = {41},
  pages        = {1-30},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {On the approximation of kernel functions},
  url          = {https://jmlr.org/papers/v26/24-0270.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and robust semi-supervised estimation of average treatment effect with partially annotated treatment and response. <em>JMLR</em>, <em>26</em>(40), 1-77. (<a href='https://jmlr.org/papers/v26/23-1587.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A notable challenge of leveraging Electronic Health Records (EHR) for treatment effect assessment is the lack of precise information on important clinical variables, including the treatment received and the response. Both treatment information and response cannot be accurately captured by readily available EHR features in many studies and require labor-intensive manual chart review to precisely annotate, which limits the number of available gold standard labels on these key variables. We considered average treatment effect (ATE) estimation when 1) exact treatment and outcome variables are only observed together in a small labeled subset and 2) noisy surrogates of treatment and outcome, such as relevant prescription and diagnosis codes, along with potential confounders are observed for all subjects. We derived the efficient influence function for ATE and used it to construct a semi-supervised multiple machine learning (SMMAL) estimator. We justified that our SMMAL ATE estimator is semi-parametric efficient with B-spline regression under low-dimensional smooth models. We developed the adaptive sparsity/model doubly robust estimation under high-dimensional logistic propensity score and outcome regression models. Results from simulation studies demonstrated the validity of our SMMAL method and its superiority over supervised and unsupervised benchmarks. We applied SMMAL to the assessment of targeted therapies for metastatic colorectal cancer in comparison to chemotherapy.},
  archive      = {J_JMLR},
  author       = {Jue Hou and Rajarshi Mukherjee and Tianxi Cai},
  journal      = {Journal of Machine Learning Research},
  number       = {40},
  pages        = {1-77},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Efficient and robust semi-supervised estimation of average treatment effect with partially annotated treatment and response},
  url          = {https://jmlr.org/papers/v26/23-1587.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonconvex stochastic bregman proximal gradient method with application to deep learning. <em>JMLR</em>, <em>26</em>(39), 1-44. (<a href='https://jmlr.org/papers/v26/23-0657.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient methods for minimizing nonconvex composite objective functions typically rely on the Lipschitz smoothness of the differentiable part, but this assumption fails in many important problem classes like quadratic inverse problems and neural network training, leading to instability of the algorithms in both theory and practice. To address this, we propose a family of stochastic Bregman proximal gradient (SBPG) methods that only require smooth adaptivity. SBPG replaces the quadratic approximation in SGD with a Bregman proximity measure, offering a better approximation model that handles non-Lipschitz gradients in nonconvex objectives. We establish the convergence properties of vanilla SBPG and show it achieves optimal sample complexity in the nonconvex setting. Experimental results on quadratic inverse problems demonstrate SBPG's robustness in terms of stepsize selection and sensitivity to the initial point. Furthermore, we introduce a momentum-based variant, MSBPG, which enhances convergence by relaxing the mini-batch size requirement while preserving the optimal oracle complexity. We apply MSBPG to the training of deep neural networks, utilizing a polynomial kernel function to ensure smooth adaptivity of the loss function. Experimental results on benchmark datasets confirm the effectiveness and robustness of MSBPG in training neural networks. Given its negligible additional computational cost compared to SGD in large-scale optimization, MSBPG shows promise as a universal open-source optimizer for future applications.},
  archive      = {J_JMLR},
  author       = {Kuangyu Ding and Jingyang Li and Kim-Chuan Toh},
  journal      = {Journal of Machine Learning Research},
  number       = {39},
  pages        = {1-44},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Nonconvex stochastic bregman proximal gradient method with application to deep learning},
  url          = {https://jmlr.org/papers/v26/23-0657.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing data collection for machine learning. <em>JMLR</em>, <em>26</em>(38), 1-52. (<a href='https://jmlr.org/papers/v26/23-0292.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep learning systems require huge data sets to achieve impressive performance, but there is little guidance on how much or what kind of data to collect. Over-collecting data incurs unnecessary present costs, while under-collecting may incur future costs and delay workflows. We propose a new paradigm to model the data collection workflow as a formal optimal data collection problem that allows designers to specify performance targets, collection costs, a time horizon, and penalties for failing to meet the targets. This formulation generalizes to tasks with multiple data sources, such as labeled and unlabeled data used in semi-supervised learning, and can be easily modified to customized analyses such as how to introduce data from new classes to an existing model. To solve our problem, we develop Learn-Optimize-Collect (LOC), which minimizes expected future collection costs. Finally, we numerically compare our framework to the conventional baseline of estimating data requirements by extrapolating from neural scaling laws. We significantly reduce the risks of failing to meet desired performance targets on several classification, segmentation, and detection tasks, while maintaining low total collection costs.},
  archive      = {J_JMLR},
  author       = {Rafid Mahmood and James Lucas and Jose M. Alvarez and Sanja Fidler and Marc T. Law},
  journal      = {Journal of Machine Learning Research},
  number       = {38},
  pages        = {1-52},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Optimizing data collection for machine learning},
  url          = {https://jmlr.org/papers/v26/23-0292.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbalanced kantorovich-rubinstein distance, plan, and barycenter on nite spaces: A statistical perspective. <em>JMLR</em>, <em>26</em>(37), 1-70. (<a href='https://jmlr.org/papers/v26/22-1262.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze statistical properties of plug-in estimators for unbalanced optimal transport quantities between finitely supported measures in different prototypical sampling models. Specifically, our main results provide non-asymptotic bounds on the expected error of empirical Kantorovich-Rubinstein (KR) distance, plans, and barycenters for mass penalty parameter $C>0$. The impact of the mass penalty parameter $C$ is studied in detail. Based on this analysis, we mathematically justify randomized computational schemes for KR quantities which can be used for fast approximate computations in combination with any exact solver. Using synthetic and real datasets, we empirically analyze the behavior of the expected errors in simulation studies and illustrate the validity of our theoretical bounds.},
  archive      = {J_JMLR},
  author       = {Shayan Hundrieser and Florian Heinemann and Marcel Klatt and Marina Struleva and Axel Munk},
  journal      = {Journal of Machine Learning Research},
  number       = {37},
  pages        = {1-70},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Unbalanced kantorovich-rubinstein distance, plan, and barycenter on nite spaces: A statistical perspective},
  url          = {https://jmlr.org/papers/v26/22-1262.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Copula-based sensitivity analysis for multi-treatment causal inference with unobserved confounding. <em>JMLR</em>, <em>26</em>(36), 1-60. (<a href='https://jmlr.org/papers/v26/22-0372.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has focused on the potential and pitfalls of causal identification in observational studies with multiple simultaneous treatments. Building on previous work, we show that even if the conditional distribution of unmeasured confounders given treatments were known exactly, the causal effects would not in general be identifiable, although they may be partially identified. Given these results, we propose a sensitivity analysis method for characterizing the effects of potential unmeasured confounding, tailored to the multiple treatment setting, that can be used to characterize a range of causal effects that are compatible with the observed data. Our method is based on a copula factorization of the joint distribution of outcomes, treatments, and confounders, and can be layered on top of arbitrary observed data models. We propose a practical implementation of this approach making use of the Gaussian copula, and establish conditions under which causal effects can be bounded. We also describe approaches for reasoning about effects, including calibrating sensitivity parameters, quantifying robustness of effect estimates, and selecting models that are most consistent with prior hypotheses.},
  archive      = {J_JMLR},
  author       = {Jiajing Zheng and Alexander D'Amour and Alexander Franks},
  journal      = {Journal of Machine Learning Research},
  number       = {36},
  pages        = {1-60},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Copula-based sensitivity analysis for multi-treatment causal inference with unobserved confounding},
  url          = {https://jmlr.org/papers/v26/22-0372.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rank-one convexification for sparse regression. <em>JMLR</em>, <em>26</em>(35), 1-50. (<a href='https://jmlr.org/papers/v26/19-159.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse regression models are increasingly prevalent due to their ease of interpretability and superior out-of-sample performance. However, the exact model of sparse regression with an $\ell_0$-constraint restricting the support of the estimators is a challenging (\NP-hard) non-convex optimization problem. In this paper, we derive new strong convex relaxations for sparse regression. These relaxations are based on the convex-hull formulations for rank-one quadratic terms with indicator variables. The new relaxations can be formulated as semidefinite optimization problems in an extended space and are stronger and more general than the state-of-the-art formulations, including the perspective reformulation and formulations with the reverse Huber penalty and the minimax concave penalty functions. Furthermore, the proposed rank-one strengthening can be interpreted as a non-separable, non-convex, unbiased sparsity-inducing regularizer, which dynamically adjusts its penalty according to the shape of the error function without inducing bias for the sparse solutions. In our computational experiments with benchmark datasets, the proposed conic formulations are solved within seconds and result in near-optimal solutions (with 0.4\% optimality gap on average) for non-convex $\ell_0$-problems. Moreover, the resulting estimators also outperform alternative convex approaches, such as lasso and elastic net regression, from a statistical perspective, achieving high prediction accuracy and good interpretability.},
  archive      = {J_JMLR},
  author       = {Alper Atamturk and Andres Gomez},
  journal      = {Journal of Machine Learning Research},
  number       = {35},
  pages        = {1-50},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Rank-one convexification for sparse regression},
  url          = {https://jmlr.org/papers/v26/19-159.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gsplat: An open-source library for gaussian splatting. <em>JMLR</em>, <em>26</em>(34), 1-17. (<a href='https://jmlr.org/papers/v26/24-1476.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. Source code is available at https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We welcome contributions from the open-source community.},
  archive      = {J_JMLR},
  author       = {Vickie Ye and Ruilong Li and Justin Kerr and Matias Turkulainen and Brent Yi and Zhuoyang Pan and Otto Seiskari and Jianbo Ye and Jeffrey Hu and Matthew Tancik and Angjoo Kanazawa},
  journal      = {Journal of Machine Learning Research},
  number       = {34},
  pages        = {1-17},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Gsplat: An open-source library for gaussian splatting},
  url          = {https://jmlr.org/papers/v26/24-1476.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference of constrained stochastic optimization via sketched sequential quadratic programming. <em>JMLR</em>, <em>26</em>(33), 1-75. (<a href='https://jmlr.org/papers/v26/24-0530.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider online statistical inference of constrained stochastic nonlinear optimization problems. We apply the Stochastic Sequential Quadratic Programming (StoSQP) method to solve these problems, which can be regarded as applying second-order Newton's method to the Karush-Kuhn-Tucker (KKT) conditions. In each iteration, the StoSQP method computes the Newton direction by solving a quadratic program, and then selects a proper adaptive stepsize $\bar{\alpha}_t$ to update the primal-dual iterate. To reduce dominant computational cost of the method, we inexactly solve the quadratic program in each iteration by employing an iterative sketching solver. Notably, the approximation error of the sketching solver need not vanish as iterations proceed, meaning that the per-iteration computational cost does not blow up. For the above StoSQP method, we show that under mild assumptions, the rescaled primal-dual sequence $1/\sqrt{\bar{\alpha}_t}\cdot (x_t -x^\star, \lambda_t - \lambda^\star)$ converges to a mean-zero Gaussian distribution with a nontrivial covariance matrix depending on the underlying sketching distribution. To perform inference in practice, we also analyze a plug-in covariance matrix estimator. We illustrate the asymptotic normality result of the method both on benchmark nonlinear problems in CUTEst test set and on linearly/nonlinearly constrained regression problems.},
  archive      = {J_JMLR},
  author       = {Sen Na and Michael Mahoney},
  journal      = {Journal of Machine Learning Research},
  number       = {33},
  pages        = {1-75},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Statistical inference of constrained stochastic optimization via sketched sequential quadratic programming},
  url          = {https://jmlr.org/papers/v26/24-0530.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sliced-wasserstein distances and flows on cartan-hadamard manifolds. <em>JMLR</em>, <em>26</em>(32), 1-76. (<a href='https://jmlr.org/papers/v26/24-0359.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While many Machine Learning methods have been developed or transposed on Riemannian manifolds to tackle data with known non-Euclidean geometry, Optimal Transport (OT) methods on such spaces have not received much attention. The main OT tool on these spaces is the Wasserstein distance, which suffers from a heavy computational burden. On Euclidean spaces, a popular alternative is the Sliced-Wasserstein distance, which leverages a closed-form solution of the Wasserstein distance in one dimension, but which is not readily available on manifolds. In this work, we derive general constructions of Sliced-Wasserstein distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive curvature, which include among others Hyperbolic spaces or the space of Symmetric Positive Definite matrices. Then, we propose different applications such as classification of documents with a suitably learned ground cost on a manifold, and data set comparison on a product manifold. Additionally, we derive non-parametric schemes to minimize these new distances by approximating their Wasserstein gradient flows.},
  archive      = {J_JMLR},
  author       = {Clément Bonet and Lucas Drumetz and Nicolas Courty},
  journal      = {Journal of Machine Learning Research},
  number       = {32},
  pages        = {1-76},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Sliced-wasserstein distances and flows on cartan-hadamard manifolds},
  url          = {https://jmlr.org/papers/v26/24-0359.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating optimization over the space of probability measures. <em>JMLR</em>, <em>26</em>(31), 1-40. (<a href='https://jmlr.org/papers/v26/23-1288.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The acceleration of gradient-based optimization methods is a subject of significant practical and theoretical importance, particularly within machine learning applications. While much attention has been directed towards optimizing within Euclidean space, the need to optimize over spaces of probability measures in machine learning motivates the exploration of accelerated gradient methods in this context, too. To this end, we introduce a Hamiltonian-flow approach analogous to momentum-based approaches in Euclidean space. We demonstrate that, in the continuous-time setting, algorithms based on this approach can achieve convergence rates of arbitrarily high order. We complement our findings with numerical examples.},
  archive      = {J_JMLR},
  author       = {Shi Chen and Qin Li and Oliver Tse and Stephen J. Wright},
  journal      = {Journal of Machine Learning Research},
  number       = {31},
  pages        = {1-40},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Accelerating optimization over the space of probability measures},
  url          = {https://jmlr.org/papers/v26/23-1288.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian multi-group gaussian process models for heterogeneous group-structured data. <em>JMLR</em>, <em>26</em>(30), 1-34. (<a href='https://jmlr.org/papers/v26/23-0291.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes are pervasive in functional data analysis, machine learning, and spatial statistics for modeling complex dependencies. Scientific data are often heterogeneous in their inputs and contain multiple known discrete groups of samples; thus, it is desirable to leverage the similarity among groups while accounting for heterogeneity across groups. We propose multi-group Gaussian processes (MGGPs) defined over $\mathbb{R}^p\times \mathscr{C}$, where $\mathscr{C}$ is a finite set representing the group label, by developing general classes of valid (positive definite) covariance functions on such domains. MGGPs are able to accurately recover relationships between the groups and efficiently share strength across samples from all groups during inference, while capturing distinct group-specific behaviors in the conditional posterior distributions. We demonstrate inference in MGGPs through simulation experiments, and we apply our proposed MGGP regression framework to gene expression data to illustrate the behavior and enhanced inferential capabilities of multi-group Gaussian processes by jointly modeling continuous and categorical variables.},
  archive      = {J_JMLR},
  author       = {Didong Li and Andrew Jones and Sudipto Banerjee and Barbara E. Engelhardt},
  journal      = {Journal of Machine Learning Research},
  number       = {30},
  pages        = {1-34},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Bayesian multi-group gaussian process models for heterogeneous group-structured data},
  url          = {https://jmlr.org/papers/v26/23-0291.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Orthogonal bases for equivariant graph learning with provable k-WL expressive power. <em>JMLR</em>, <em>26</em>(29), 1-35. (<a href='https://jmlr.org/papers/v26/23-0178.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network (GNN) models have been widely used for learning graph-structured data. Due to the permutation-invariant requirement of graph learning tasks, a basic element in graph neural networks is the invariant and equivariant linear layers. Previous work (Maron et al., 2019b) provided a maximal collection of invariant and equivariant linear layers and a simple deep neural network model, called k-IGN, for graph data defined on k-tuples of nodes. It is shown that the expressive power of k-IGN is at least as good as the k-Weisfeiler-Leman (WL) algorithm in graph isomorphism tests. However, the dimension of the invariant layer and equivariant layer is the k-th and 2k-th bell numbers, respectively. Such high complexity makes it computationally infeasible for k-IGNs with k >= 3. In this paper, we show that a much smaller dimension for the linear layers is sufficient to achieve the same expressive power. We provide two sets of orthogonal bases for the linear layers, each with only 3(2^k-1)-k basis elements. Based on these linear layers, we develop neural network models GNN-a and GNN-b and show that for the graph data defined on k-tuples of data, GNN-a and GNN-b achieve the expressive power of the k-WL algorithm and the (k+1)-WL algorithm in graph isomorphism tests, respectively. In molecular prediction tasks on benchmark datasets, we demonstrate that low-order neural network models consisting of the proposed linear layers achieve better performance than other neural network models. In particular, order-2 GNN-b and order-3 GNN-a both have 3-WL expressive power, but use a much smaller basis and hence much less computation time than known neural network models.},
  archive      = {J_JMLR},
  author       = {Jia He and Maggie Cheng},
  journal      = {Journal of Machine Learning Research},
  number       = {29},
  pages        = {1-35},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Orthogonal bases for equivariant graph learning with provable k-WL expressive power},
  url          = {https://jmlr.org/papers/v26/23-0178.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal experiment design for causal effect identification. <em>JMLR</em>, <em>26</em>(28), 1-56. (<a href='https://jmlr.org/papers/v26/22-1516.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pearl’s do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing a collection of interventions with the minimum cost to identify the desired effect. First, we prove that this problem is NP-complete and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial time heuristic algorithms to tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.},
  archive      = {J_JMLR},
  author       = {Sina Akbari and Jalal Etesami and Negar Kiyavash},
  journal      = {Journal of Machine Learning Research},
  number       = {28},
  pages        = {1-56},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Optimal experiment design for causal effect identification},
  url          = {https://jmlr.org/papers/v26/22-1516.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean aggregator is more robust than robust aggregators under label poisoning attacks on distributed heterogeneous data. <em>JMLR</em>, <em>26</em>(27), 1-51. (<a href='https://jmlr.org/papers/v26/24-1307.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robustness to malicious attacks is of paramount importance for distributed learning. Existing works usually consider the classical Byzantine attacks model, which assumes that some workers can send arbitrarily malicious messages to the server and disturb the aggregation steps of the distributed learning process. To defend against such worst-case Byzantine attacks, various robust aggregators have been proposed. They are proven to be effective and much superior to the often-used mean aggregator. In this paper, however, we demonstrate that the robust aggregators are too conservative for a class of weak but practical malicious attacks, known as label poisoning attacks, where the sample labels of some workers are poisoned. Surprisingly, we are able to show that the mean aggregator is more robust than the state-of-the-art robust aggregators in theory, given that the distributed data are sufficiently heterogeneous. In fact, the learning error of the mean aggregator is proven to be order-optimal in this case. Experimental results corroborate our theoretical findings, showing the superiority of the mean aggregator under label poisoning attacks.},
  archive      = {J_JMLR},
  author       = {Jie Peng and Weiyu Li and Stefan Vlaski and Qing Ling},
  journal      = {Journal of Machine Learning Research},
  number       = {27},
  pages        = {1-51},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Mean aggregator is more robust than robust aggregators under label poisoning attacks on distributed heterogeneous data},
  url          = {https://jmlr.org/papers/v26/24-1307.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The blessing of heterogeneity in federated Q-learning: Linear speedup and beyond. <em>JMLR</em>, <em>26</em>(26), 1-85. (<a href='https://jmlr.org/papers/v26/24-0579.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider federated Q-learning, which aims to learn an optimal Q-function by periodically aggregating local Q-estimates trained on local data alone. Focusing on infinite-horizon tabular Markov decision processes, we provide sample complexity guarantees for both the synchronous and asynchronous variants of federated Q-learning, which exhibit a linear speedup with respect to the number of agents and near-optimal dependencies on other salient problem parameters. In the asynchronous setting, existing analyses of federated Q-learning, which adopt an equally weighted averaging of local Q-estimates, require that every agent covers the entire state-action space. In contrast, our improved sample complexity scales inverse proportionally to the minimum entry of the average stationary state-action occupancy distribution of all agents, thus only requiring the agents to collectively cover the entire state-action space, unveiling the blessing of heterogeneity. However, its sample complexity still suffers when the local trajectories are highly heterogeneous. In response, we propose a novel federated Q-learning algorithm with importance averaging, giving larger weights to more frequently visited state-action pairs, which achieves a robust linear speedup as if all trajectories are centrally processed, regardless of the heterogeneity of local behavior policies.},
  archive      = {J_JMLR},
  author       = {Jiin Woo and Gauri Joshi and Yuejie Chi},
  journal      = {Journal of Machine Learning Research},
  number       = {26},
  pages        = {1-85},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {The blessing of heterogeneity in federated Q-learning: Linear speedup and beyond},
  url          = {https://jmlr.org/papers/v26/24-0579.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Depyf: Open the opaque box of PyTorch compiler for machine learning researchers. <em>JMLR</em>, <em>26</em>(25), 1-18. (<a href='https://jmlr.org/papers/v26/24-0383.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PyTorch 2.x introduces a compiler designed to accelerate deep learning programs. However, for machine learning researchers, fully leveraging the PyTorch compiler can be challenging due to its operation at the Python bytecode level, making it appear as an opaque box. To address this, we introduce depyf, a tool designed to demystify the inner workings of the PyTorch compiler. depyf decompiles the bytecode generated by PyTorch back into equivalent source code and establishes connections between the code objects in the memory and their counterparts in source code format on the disk. This feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. Notably, depyf is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. The project is openly available at https://github.com/thuml/depyf and is recognized as a PyTorch ecosystem project at https://pytorch.org/blog/introducing-depyf.},
  archive      = {J_JMLR},
  author       = {Kaichao You and Runsheng Bai and Meng Cao and Jianmin Wang and Ion Stoica and Mingsheng Long},
  journal      = {Journal of Machine Learning Research},
  number       = {25},
  pages        = {1-18},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Depyf: Open the opaque box of PyTorch compiler for machine learning researchers},
  url          = {https://jmlr.org/papers/v26/24-0383.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The ODE method for stochastic approximation and reinforcement learning with markovian noise. <em>JMLR</em>, <em>26</em>(24), 1-76. (<a href='https://jmlr.org/papers/v26/24-0100.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of the strong law of large numbers and a form of the law of the iterated logarithm.},
  archive      = {J_JMLR},
  author       = {Shuze Daniel Liu and Shuhang Chen and Shangtong Zhang},
  journal      = {Journal of Machine Learning Research},
  number       = {24},
  pages        = {1-76},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {The ODE method for stochastic approximation and reinforcement learning with markovian noise},
  url          = {https://jmlr.org/papers/v26/24-0100.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving graph neural networks on multi-node tasks with the labeling trick. <em>JMLR</em>, <em>26</em>(23), 1-44. (<a href='https://jmlr.org/papers/v26/23-0560.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study using graph neural networks (GNNs) for multi-node representation learning, where a representation for a set of more than one node (such as a link) is to be learned. Existing GNNs are mainly designed to learn single-node representations. When used for multi-node representation learning, a common practice is to directly aggregate the single-node representations obtained by a GNN. In this paper, we show a fundamental limitation of such an approach, namely the inability to capture the dependence among multiple nodes in the node set. A straightforward solution is to distinguish target nodes from others. Formalizing this idea, we propose \text{labeling trick}, which first labels nodes in the graph according to their relationships with the target node set before applying a GNN and then aggregates node representations obtained in the labeled graph for multi-node representations. Besides node sets in graphs, we also extend labeling tricks to posets, subsets and hypergraphs. Experiments verify that the labeling trick technique can boost GNNs on various tasks, including undirected link prediction, directed link prediction, hyperedge prediction, and subgraph prediction. Our work explains the superior performance of previous node-labeling-based methods and establishes a theoretical foundation for using GNNs for multi-node representation learning.},
  archive      = {J_JMLR},
  author       = {Xiyuan Wang and Pan Li and Muhan Zhang},
  journal      = {Journal of Machine Learning Research},
  number       = {23},
  pages        = {1-44},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Improving graph neural networks on multi-node tasks with the labeling trick},
  url          = {https://jmlr.org/papers/v26/23-0560.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Directed cyclic graphs for simultaneous discovery of time-lagged and instantaneous causality from longitudinal data using instrumental variables. <em>JMLR</em>, <em>26</em>(22), 1-62. (<a href='https://jmlr.org/papers/v26/23-0272.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of causal discovery from longitudinal observational data. We develop a novel framework that simultaneously discovers the time-lagged causality and the possibly cyclic instantaneous causality. Under common causal discovery assumptions, combined with additional instrumental information typically available in longitudinal data, we prove the proposed model is generally identifiable. To the best of our knowledge, this is the first causal identification theory for directed graphs with general cyclic patterns that achieves unique causal identifiability. Structural learning is carried out in a fully Bayesian fashion. Through extensive simulations and an application to the Women's Interagency HIV Study, we demonstrate the identifiability, utility, and superiority of the proposed model against state-of-the-art alternative methods.},
  archive      = {J_JMLR},
  author       = {Wei Jin and Yang Ni and Amanda B. Spence and Leah H. Rubin and Yanxun Xu},
  journal      = {Journal of Machine Learning Research},
  number       = {22},
  pages        = {1-62},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Directed cyclic graphs for simultaneous discovery of time-lagged and instantaneous causality from longitudinal data using instrumental variables},
  url          = {https://jmlr.org/papers/v26/23-0272.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian sparse gaussian mixture model for clustering in high dimensions. <em>JMLR</em>, <em>26</em>(21), 1-50. (<a href='https://jmlr.org/papers/v26/23-0142.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the sparse high-dimensional Gaussian mixture model when the number of clusters is allowed to grow with the sample size. A minimax lower bound for parameter estimation is established, and we show that a constrained maximum likelihood estimator achieves the minimax lower bound. However, this optimization-based estimator is computationally intractable because the objective function is highly nonconvex and the feasible set involves discrete structures. To address the computational challenge, we propose a computationally tractable Bayesian approach to estimate high-dimensional Gaussian mixtures whose cluster centers exhibit sparsity using a continuous spike-and-slab prior. We further prove that the posterior contraction rate of the proposed Bayesian method is minimax optimal. The mis- clustering rate is obtained as a by-product using tools from matrix perturbation theory. The proposed Bayesian sparse Gaussian mixture model does not require pre-specifying the number of clusters, which can be adaptively estimated. The validity and usefulness of the proposed method is demonstrated through simulation studies and the analysis of a real-world single-cell RNA sequencing data set.},
  archive      = {J_JMLR},
  author       = {Dapeng Yao and Fangzheng Xie and Yanxun Xu},
  journal      = {Journal of Machine Learning Research},
  number       = {21},
  pages        = {1-50},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Bayesian sparse gaussian mixture model for clustering in high dimensions},
  url          = {https://jmlr.org/papers/v26/23-0142.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularizing hard examples improves adversarial robustness. <em>JMLR</em>, <em>26</em>(20), 1-48. (<a href='https://jmlr.org/papers/v26/22-1428.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have validated that pruning hard-to-learn examples from training improves the generalization performance of neural networks (NNs). In this study, we investigate this intriguing phenomenon---the negative effect of hard examples on generalization---in adversarial training. Particularly, we theoretically demonstrate that the increase in the difficulty of hard examples in adversarial training is significantly greater than the increase in the difficulty of easy examples. Furthermore, we verify that hard examples are only fitted through memorization of the label in adversarial training. We conduct both theoretical and empirical analyses of this memorization phenomenon, showing that pruning hard examples in adversarial training can enhance the model's robustness. However, the challenge remains in finding the optimal threshold for removing hard examples that degrade robustness performance. Based upon these observations, we propose a new approach, difficulty proportional label smoothing (DPLS), to adaptively mitigate the negative effect of hard examples, thereby improving the adversarial robustness of NNs. Notably, our experimental result indicates that our method can successfully leverage hard examples while circumventing the negative effect.},
  archive      = {J_JMLR},
  author       = {Hyungyu Lee and Saehyung Lee and Ho Bae and Sungroh Yoon},
  journal      = {Journal of Machine Learning Research},
  number       = {20},
  pages        = {1-48},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Regularizing hard examples improves adversarial robustness},
  url          = {https://jmlr.org/papers/v26/22-1428.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random ReLU neural networks as non-gaussian processes. <em>JMLR</em>, <em>26</em>(19), 1-31. (<a href='https://jmlr.org/papers/v26/24-0737.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a large class of shallow neural networks with randomly initialized parameters and rectified linear unit activation functions. We prove that these random neural networks are well-defined non-Gaussian processes. As a by-product, we demonstrate that these networks are solutions to stochastic differential equations driven by impulsive white noise (combinations of random Dirac measures). These processes are parameterized by the law of the weights and biases as well as the density of activation thresholds in each bounded region of the input domain. We prove that these processes are isotropic and wide-sense self-similar with Hurst exponent 3/2. We also derive a remarkably simple closed-form expression for their autocovariance function. Our results are fundamentally different from prior work in that we consider a non-asymptotic viewpoint: The number of neurons in each bounded region of the input domain (i.e., the width) is itself a random variable with a Poisson law with mean proportional to the density parameter. Finally, we show that, under suitable hypotheses, as the expected width tends to infinity, these processes can converge in law not only to Gaussian processes, but also to non-Gaussian processes depending on the law of the weights. Our asymptotic results provide a new take on several classical results (wide networks converge to Gaussian processes) as well as some new ones (wide networks can converge to non-Gaussian processes).},
  archive      = {J_JMLR},
  author       = {Rahul Parhi and Pakshal Bohra and Ayoub El Biari and Mehrsa Pourya and Michael Unser},
  journal      = {Journal of Machine Learning Research},
  number       = {19},
  pages        = {1-31},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Random ReLU neural networks as non-gaussian processes},
  url          = {https://jmlr.org/papers/v26/24-0737.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Riemannian bilevel optimization. <em>JMLR</em>, <em>26</em>(18), 1-44. (<a href='https://jmlr.org/papers/v26/24-0397.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider the bilevel optimization problem on Riemannian manifolds. We inspect the calculation of the hypergradient of such problems on general manifolds and thus enable the utilization of gradient-based algorithms to solve such problems. The calculation of the hypergradient requires utilizing the notion of Riemannian cross-derivative and we inspect the properties and the numerical calculations of Riemannian cross-derivatives. Algorithms in both deterministic and stochastic settings, named respectively RieBO and RieSBO, are proposed that include the existing Euclidean bilevel optimization algorithms as special cases. Numerical experiments on robust optimization on Riemannian manifolds are presented to show the applicability and efficiency of the proposed methods.},
  archive      = {J_JMLR},
  author       = {Jiaxiang Li and Shiqian Ma},
  journal      = {Journal of Machine Learning Research},
  number       = {18},
  pages        = {1-44},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Riemannian bilevel optimization},
  url          = {https://jmlr.org/papers/v26/24-0397.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised learning with evolving tasks and performance guarantees. <em>JMLR</em>, <em>26</em>(17), 1-59. (<a href='https://jmlr.org/papers/v26/24-0343.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple supervised learning scenarios are composed by a sequence of classification tasks. For instance, multi-task learning and continual learning aim to learn a sequence of tasks that is either fixed or grows over time. Existing techniques for learning tasks that are in a sequence are tailored to specific scenarios, lacking adaptability to others. In addition, most of existing techniques consider situations in which the order of the tasks in the sequence is not relevant. However, it is common that tasks in a sequence are evolving in the sense that consecutive tasks often have a higher similarity. This paper presents a learning methodology that is applicable to multiple supervised learning scenarios and adapts to evolving tasks. Differently from existing techniques, we provide computable tight performance guarantees and analytically characterize the increase in the effective sample size. Experiments on benchmark datasets show the performance improvement of the proposed methodology in multiple scenarios and the reliability of the presented performance guarantees.},
  archive      = {J_JMLR},
  author       = {Verónica Álvarez and Santiago Mazuelas and Jose A. Lozano},
  journal      = {Journal of Machine Learning Research},
  number       = {17},
  pages        = {1-59},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Supervised learning with evolving tasks and performance guarantees},
  url          = {https://jmlr.org/papers/v26/24-0343.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error estimation and adaptive tuning for unregularized robust M-estimator. <em>JMLR</em>, <em>26</em>(16), 1-40. (<a href='https://jmlr.org/papers/v26/24-0060.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider unregularized robust M-estimators for linear models under Gaussian design and heavy-tailed noise, in the proportional asymptotics regime where the sample size n and the number of features p are both increasing such that $p/n \to \gamma\in (0,1)$. An estimator of the out-of-sample error of a robust M-estimator is analyzed and proved to be consistent for a large family of loss functions that includes the Huber loss. As an application of this result, we propose an adaptive tuning procedure of the scale parameter $\lambda>0$ of a given loss function $\rho$: choosing $\hat \lambda$ in a given interval $I$ that minimizes the out-of-sample error estimate of the M-estimator constructed with loss $\rho_\lambda(\cdot) = \lambda^2 \rho(\cdot/\lambda)$ leads to the optimal out-of-sample error over $I$. The proof relies on a smoothing argument: the unregularized M-estimation objective function is perturbed, or smoothed, with a Ridge penalty that vanishes as $n\to+\infty$, and shows that the unregularized M-estimator of interest inherits properties of its smoothed version.},
  archive      = {J_JMLR},
  author       = {Pierre C. Bellec and Takuya Koriyama},
  journal      = {Journal of Machine Learning Research},
  number       = {16},
  pages        = {1-40},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Error estimation and adaptive tuning for unregularized robust M-estimator},
  url          = {https://jmlr.org/papers/v26/24-0060.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From sparse to dense functional data in high dimensions: Revisiting phase transitions from a non-asymptotic perspective. <em>JMLR</em>, <em>26</em>(15), 1-40. (<a href='https://jmlr.org/papers/v26/23-1578.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric estimation of the mean and covariance functions is ubiquitous in functional data analysis and local linear smoothing techniques are most frequently used. Zhang and Wang (2016) explored different types of asymptotic properties of the estimation, which reveal interesting phase transition phenomena based on the relative order of the average sampling frequency per subject $T$ to the number of subjects $n$, partitioning the data into three categories: “sparse”, “semi-dense”, and “ultra-dense”. In an increasingly available high-dimensional scenario, where the number of functional variables $p$ is large in relation to $n$, we revisit this open problem from a non-asymptotic perspective by deriving comprehensive concentration inequalities for the local linear smoothers. Besides being of interest by themselves, our non-asymptotic results lead to elementwise maximum rates of $L_2$ convergence and uniform convergence serving as a fundamentally important tool for further convergence analysis when $p$ grows exponentially with $n$ and possibly $T$. With the presence of extra $\log p$ terms to account for the high-dimensional effect, we then investigate the scaled phase transitions and the corresponding elementwise maximum rates from sparse to semi-dense to ultra-dense functional data in high dimensions. We also discuss a couple of applications of our theoretical results. Finally, numerical studies are carried out to confirm the established theoretical properties.},
  archive      = {J_JMLR},
  author       = {Shaojun Guo and Dong Li and Xinghao Qiao and Yizhu Wang},
  journal      = {Journal of Machine Learning Research},
  number       = {15},
  pages        = {1-40},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {From sparse to dense functional data in high dimensions: Revisiting phase transitions from a non-asymptotic perspective},
  url          = {https://jmlr.org/papers/v26/23-1578.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Locally private causal inference for randomized experiments. <em>JMLR</em>, <em>26</em>(14), 1-40. (<a href='https://jmlr.org/papers/v26/23-1401.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local differential privacy is a differential privacy paradigm in which individuals first apply a privacy mechanism to their data (often by adding noise) before transmitting the result to a curator. The noise for privacy results in additional bias and variance in their analyses. Thus it is of great importance for analysts to incorporate the privacy noise into valid inference. In this article, we develop methodologies to infer causal effects from locally privatized data under randomized experiments. First, we present frequentist estimators under various privacy scenarios with their variance estimators and plug-in confidence intervals. We show a na\"ive debiased estimator results in inferior mean-squared error (MSE) compared to minimax lower bounds. In contrast, we show that using a customized privacy mechanism, we can match the lower bound, giving minimax optimal inference. We also develop a Bayesian nonparametric methodology along with a blocked Gibbs sampling algorithm, which can be applied to any of our proposed privacy mechanisms, and which performs especially well in terms of MSE for tight privacy budgets. Finally, we present simulation studies to evaluate the performance of our proposed frequentist and Bayesian methodologies for various privacy budgets, resulting in useful suggestions for performing causal inference for privatized data.},
  archive      = {J_JMLR},
  author       = {Yuki Ohnishi and Jordan Awan},
  journal      = {Journal of Machine Learning Research},
  number       = {14},
  pages        = {1-40},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Locally private causal inference for randomized experiments},
  url          = {https://jmlr.org/papers/v26/23-1401.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating network-mediated causal effects via principal components network regression. <em>JMLR</em>, <em>26</em>(13), 1-99. (<a href='https://jmlr.org/papers/v26/23-1317.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a method to decompose causal effects on a social network into an indirect effect mediated by the network, and a direct effect independent of the social network. To handle the complexity of network structures, we assume that latent social groups act as causal mediators. We develop principal components network regression models to differentiate the social effect from the non-social effect. Fitting the regression models is as simple as principal components analysis followed by ordinary least squares estimation. We prove asymptotic theory for regression coefficients from this procedure and show that it is widely applicable, allowing for a variety of distributions on the regression errors and network edges. We carefully characterize the counterfactual assumptions necessary to use the regression models for causal inference, and show that current approaches to causal network regression may result in over-control bias. The method is very general, so that it is applicable to many types of structured data beyond social networks, such as text, areal data, psychometrics, images and omics.},
  archive      = {J_JMLR},
  author       = {Alex Hayes and Mark M. Fredrickson and Keith Levin},
  journal      = {Journal of Machine Learning Research},
  number       = {13},
  pages        = {1-99},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Estimating network-mediated causal effects via principal components network regression},
  url          = {https://jmlr.org/papers/v26/23-1317.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selective inference with distributed data. <em>JMLR</em>, <em>26</em>(12), 1-44. (<a href='https://jmlr.org/papers/v26/23-0309.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When data are distributed across multiple sites or machines rather than centralized in one location, researchers face the challenge of extracting meaningful information without directly sharing individual data points. While there are many distributed methods for point estimation using sparse regression, few options are available for estimating uncertainties or conducting hypothesis tests based on the estimated sparsity. In this paper, we introduce a procedure for performing selective inference with distributed data. We consider a scenario where each local machine solves a lasso problem and communicates the selected predictors to a central machine. The central machine then aggregates these selected predictors to form a generalized linear model (GLM). Our goal is to provide valid inference for the selected GLM while reusing data that have been used in the model selection process. Our proposed procedure only requires low-dimensional summary statistics from local machines, thus keeping communication costs low and preserving the privacy of individual data sets. Furthermore, this procedure can be applied in scenarios where model selection is repeatedly conducted on randomly subsampled data sets, addressing the p-value lottery problem linked with model selection. We demonstrate the effectiveness of our approach through simulations and an analysis of a medical data set on ICU admissions.},
  archive      = {J_JMLR},
  author       = {Sifan Liu and Snigdha Panigrahi},
  journal      = {Journal of Machine Learning Research},
  number       = {12},
  pages        = {1-44},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Selective inference with distributed data},
  url          = {https://jmlr.org/papers/v26/23-0309.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-timescale gradient descent ascent algorithms for nonconvex minimax optimization. <em>JMLR</em>, <em>26</em>(11), 1-45. (<a href='https://jmlr.org/papers/v26/22-0863.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a unified analysis of two-timescale gradient descent ascent (TTGDA) for solving structured nonconvex minimax optimization problems in the form of $\min_x \max_{y \in Y} f(x, y)$, where the objective function $f(x, y)$ is nonconvex in $x$ and concave in $y$, and the constraint set $Y \subseteq \mathbb{R}^n$ is convex and bounded. In the convex-concave setting, the single-timescale gradient descent ascent (GDA) algorithm is widely used in applications and has been shown to have strong convergence guarantees. In more general settings, however, it can fail to converge. Our contribution is to design TTGDA algorithms that are effective beyond the convex-concave setting, efficiently finding a stationary point of the function $\Phi(\cdot) := \max_{y \in Y} f(\cdot, y)$. We also establish theoretical bounds on the complexity of solving both smooth and nonsmooth nonconvex-concave minimax optimization problems. To the best of our knowledge, this is the first systematic analysis of TTGDA for nonconvex minimax optimization, shedding light on its superior performance in training generative adversarial networks (GANs) and in other real-world application problems.},
  archive      = {J_JMLR},
  author       = {Tianyi Lin and Chi Jin and Michael I. Jordan},
  journal      = {Journal of Machine Learning Research},
  number       = {11},
  pages        = {1-45},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Two-timescale gradient descent ascent algorithms for nonconvex minimax optimization},
  url          = {https://jmlr.org/papers/v26/22-0863.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An axiomatic definition of hierarchical clustering. <em>JMLR</em>, <em>26</em>(10), 1-26. (<a href='https://jmlr.org/papers/v26/24-1052.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we take an axiomatic approach to defining a population hierarchical clustering for piecewise constant densities, and in a similar manner to Lebesgue integration, extend this definition to more general densities. When the density satisfies some mild conditions, e.g., when it has connected support, is continuous, and vanishes only at infinity, or when the connected components of the density satisfy these conditions, our axiomatic definition results in Hartigan's definition of cluster tree.},
  archive      = {J_JMLR},
  author       = {Ery Arias-Castro and Elizabeth Coda},
  journal      = {Journal of Machine Learning Research},
  number       = {10},
  pages        = {1-26},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {An axiomatic definition of hierarchical clustering},
  url          = {https://jmlr.org/papers/v26/24-1052.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test-time training on video streams. <em>JMLR</em>, <em>26</em>(9), 1-29. (<a href='https://jmlr.org/papers/v26/24-0439.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior work has established Test-Time Training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is first trained on the same instance using a self-supervised task such as reconstruction. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The improvements are more than 2.2x and 1.5x for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses strictly more information, training on all frames from the entire test video regardless of temporal order. This finding challenges those in prior work using synthetic videos. We formalize a notion of locality as the advantage of online over offline TTT, and analyze its role with ablations and a theory based on bias-variance trade-off.},
  archive      = {J_JMLR},
  author       = {Renhao Wang and Yu Sun and Arnuv Tandon and Yossi Gandelsman and Xinlei Chen and Alexei A. Efros and Xiaolong Wang},
  journal      = {Journal of Machine Learning Research},
  number       = {9},
  pages        = {1-29},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Test-time training on video streams},
  url          = {https://jmlr.org/papers/v26/24-0439.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive client sampling in federated learning via online learning with bandit feedback. <em>JMLR</em>, <em>26</em>(8), 1-67. (<a href='https://jmlr.org/papers/v26/24-0385.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high cost of communication, federated learning (FL) systems need to sample a subset of clients that are involved in each round of training. As a result, client sampling plays an important role in FL systems as it affects the convergence rate of optimization algorithms used to train machine learning models. Despite its importance, there is limited work on how to sample clients effectively. In this paper, we cast client sampling as an online learning task with bandit feedback, which we solve with an online stochastic mirror descent (OSMD) algorithm designed to minimize the sampling variance. We then theoretically show how our sampling method can improve the convergence speed of federated optimization algorithms over the widely used uniform sampling. Through both simulated and real data experiments, we empirically illustrate the advantages of the proposed client sampling algorithm over uniform sampling and existing online learning-based sampling strategies. The proposed adaptive sampling procedure is applicable beyond the FL problem studied here and can be used to improve the performance of stochastic optimization procedures such as stochastic gradient descent and stochastic coordinate descent.},
  archive      = {J_JMLR},
  author       = {Boxin Zhao and Lingxiao Wang and Ziqi Liu and Zhiqiang Zhang and Jun Zhou and Chaochao Chen and Mladen Kolar},
  journal      = {Journal of Machine Learning Research},
  number       = {8},
  pages        = {1-67},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Adaptive client sampling in federated learning via online learning with bandit feedback},
  url          = {https://jmlr.org/papers/v26/24-0385.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A random matrix approach to low-multilinear-rank tensor approximation. <em>JMLR</em>, <em>26</em>(7), 1-64. (<a href='https://jmlr.org/papers/v26/24-0193.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a comprehensive understanding of the estimation of a planted low-rank signal from a general spiked tensor model near the computational threshold. Relying on standard tools from the theory of large random matrices, we characterize the large-dimensional spectral behavior of the unfoldings of the data tensor and exhibit relevant signal-to-noise ratios governing the detectability of the principal directions of the signal. These results allow to accurately predict the reconstruction performance of truncated multilinear SVD (MLSVD) in the non-trivial regime. This is particularly important since it serves as an initialization of the higher-order orthogonal iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank approximation depends entirely on its initialization. We give a sufficient condition for the convergence of HOOI and show that the number of iterations before convergence tends to $1$ in the large-dimensional limit.},
  archive      = {J_JMLR},
  author       = {Hugo Lebeau and Florent Chatelain and Romain Couillet},
  journal      = {Journal of Machine Learning Research},
  number       = {7},
  pages        = {1-64},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {A random matrix approach to low-multilinear-rank tensor approximation},
  url          = {https://jmlr.org/papers/v26/24-0193.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory gym: Towards endless tasks to benchmark memory capabilities of agents. <em>JMLR</em>, <em>26</em>(6), 1-40. (<a href='https://jmlr.org/papers/v26/24-0043.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory Gym presents a suite of 2D partially observable environments, namely Mortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark memory capabilities in decision-making agents. These environments, originally with finite tasks, are expanded into innovative, endless formats, mirroring the escalating challenges of cumulative memory games such as “I packed my bag”. This progression in task design shifts the focus from merely assessing sample efficiency to also probing the levels of memory effectiveness in dynamic, prolonged scenarios. To address the gap in available memory-based Deep Reinforcement Learning baselines, we introduce an implementation within the open-source CleanRL library that integrates Transformer-XL (TrXL) with Proximal Policy Optimization. This approach utilizes TrXL as a form of episodic memory, employing a sliding window technique. Our comparative study between the Gated Recurrent Unit (GRU) and TrXL reveals varied performances across our finite and endless tasks. TrXL, on the finite environments, demonstrates superior effectiveness over GRU, but only when utilizing an auxiliary loss to reconstruct observations. Notably, GRU makes a remarkable resurgence in all endless tasks, consistently outperforming TrXL by significant margins. Website and Source Code: https://marcometer.github.io/jmlr_2024.github.io/},
  archive      = {J_JMLR},
  author       = {Marco Pleines and Matthias Pallasch and Frank Zimmer and Mike Preuss},
  journal      = {Journal of Machine Learning Research},
  number       = {6},
  pages        = {1-40},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Memory gym: Towards endless tasks to benchmark memory capabilities of agents},
  url          = {https://jmlr.org/papers/v26/24-0043.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing graph representation learning with localized topological features. <em>JMLR</em>, <em>26</em>(5), 1-36. (<a href='https://jmlr.org/papers/v26/23-1424.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning on graphs is a fundamental problem that can be crucial in various tasks. Graph neural networks, the dominant approach for graph representation learning, are limited in their representation power. Therefore, it can be beneficial to explicitly extract and incorporate high-order topological and geometric information into these models. In this paper, we propose a principled approach to extract the rich connectivity information of graphs based on the theory of persistent homology. Our method utilizes the topological features to enhance the representation learning of graph neural networks and achieve state-of-the-art performance on various node classification and link prediction benchmarks. We also explore the option of end-to-end learning of the topological features, i.e., treating topological computation as a differentiable operator during learning. Our theoretical analysis and empirical study provide insights and potential guidelines for employing topological features in graph learning tasks.},
  archive      = {J_JMLR},
  author       = {Zuoyu Yan and Qi Zhao and Ze Ye and Tengfei Ma and Liangcai Gao and Zhi Tang and Yusu Wang and Chao Chen},
  journal      = {Journal of Machine Learning Research},
  number       = {5},
  pages        = {1-36},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Enhancing graph representation learning with localized topological features},
  url          = {https://jmlr.org/papers/v26/23-1424.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep out-of-distribution uncertainty quantification via weight entropy maximization. <em>JMLR</em>, <em>26</em>(4), 1-68. (<a href='https://jmlr.org/papers/v26/23-1359.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with uncertainty quantification and out-of-distribution detection in deep learning using Bayesian and ensemble methods. It proposes a practical solution to the lack of prediction diversity observed recently for standard approaches when used out-of-distribution (Ovadia et al., 2019; Liu et al., 2021). Considering that this issue is mainly related to a lack of weight diversity, we claim that standard methods sample in "over-restricted" regions of the weight space due to the use of "over-regularization" processes, such as weight decay and zero-mean centered Gaussian priors. We propose to solve the problem by adopting the maximum entropy principle for the weight distribution, with the underlying idea to maximize the weight diversity. Under this paradigm, the epistemic uncertainty is described by the weight distribution of maximal entropy that produces neural networks "consistent" with the training observations. Considering stochastic neural networks, a practical optimization is derived to build such a distribution, defined as a trade-off between the average empirical risk and the weight distribution entropy. We provide both theoretical and numerical results to assess the efficiency of the approach. In particular, the proposed algorithm appears in the top three best methods in all configurations of an extensive out-of-distribution detection benchmark including more than thirty competitors.},
  archive      = {J_JMLR},
  author       = {Antoine de Mathelin and François Deheeger and Mathilde Mougeot and Nicolas Vayatis},
  journal      = {Journal of Machine Learning Research},
  number       = {4},
  pages        = {1-68},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Deep out-of-distribution uncertainty quantification via weight entropy maximization},
  url          = {https://jmlr.org/papers/v26/23-1359.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DisC2o-HD: Distributed causal inference with covariates shift for analyzing real-world high-dimensional data. <em>JMLR</em>, <em>26</em>(3), 1-50. (<a href='https://jmlr.org/papers/v26/23-1254.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional healthcare data, such as electronic health records (EHR) data and claims data, present two primary challenges due to the large number of variables and the need to consolidate data from multiple clinical sites. The third key challenge is the potential existence of heterogeneity in terms of covariate shift. In this paper, we propose a distributed learning algorithm accounting for covariate shift to estimate the average treatment effect (ATE) for high-dimensional data, named DisC2o-HD. Leveraging the surrogate likelihood method, our method calibrates the estimates of the propensity score and outcome models to approximately attain the desired covariate balancing property, while accounting for the covariate shift across multiple clinical sites. We show that our distributed covariate balancing propensity score estimator can approximate the pooled estimator, which is obtained by pooling the data from multiple sites together. The proposed estimator remains consistent if either the propensity score model or the outcome regression model is correctly specified. The semiparametric efficiency bound is achieved when both the propensity score and the outcome models are correctly specified. We conduct simulation studies to demonstrate the performance of the proposed algorithm; additionally, we conduct an empirical study to present the readiness of implementation and validity.},
  archive      = {J_JMLR},
  author       = {Jiayi Tong and Jie Hu and George Hripcsak and Yang Ning and Yong Chen},
  journal      = {Journal of Machine Learning Research},
  number       = {3},
  pages        = {1-50},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {DisC2o-HD: Distributed causal inference with covariates shift for analyzing real-world high-dimensional data},
  url          = {https://jmlr.org/papers/v26/23-1254.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayes meets bernstein at the meta level: An analysis of fast rates in meta-learning with PAC-bayes. <em>JMLR</em>, <em>26</em>(2), 1-60. (<a href='https://jmlr.org/papers/v26/23-025.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bernstein's condition is a key assumption that guarantees fast rates in machine learning. For example, under this condition, the Gibbs posterior with prior $\pi$ has an excess risk in $O(d_{\pi}/n)$, as opposed to $O(\sqrt{d_{\pi}/n})$ in the general case, where $n$ denotes the number of observations and $d_{\pi}$ is a complexity parameter which depends on the prior $\pi$. In this paper, we examine the Gibbs posterior in the context of meta-learning, i.e., when learning the prior $\pi$ from $T$ previous tasks. Our main result is that Bernstein's condition always holds at the meta level, regardless of its validity at the observation level. This implies that the additional cost to learn the Gibbs prior $\pi$, which will reduce the term $d_\pi$ across tasks, is in $O(1/T)$, instead of the expected $O(1/\sqrt{T})$. We further illustrate how this result improves on the standard rates in three different settings: discrete priors, Gaussian priors and mixture of Gaussian priors.},
  archive      = {J_JMLR},
  author       = {Charles Riou and Pierre Alquier and Badr-Eddine Chérief-Abdellatif},
  journal      = {Journal of Machine Learning Research},
  number       = {2},
  pages        = {1-60},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Bayes meets bernstein at the meta level: An analysis of fast rates in meta-learning with PAC-bayes},
  url          = {https://jmlr.org/papers/v26/23-025.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficiently escaping saddle points in bilevel optimization. <em>JMLR</em>, <em>26</em>(1), 1-61. (<a href='https://jmlr.org/papers/v26/22-0136.html'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bilevel optimization is one of the fundamental problems in machine learning and optimization. Recent theoretical developments in bilevel optimization focus on finding the first-order stationary points for nonconvex-strongly-convex cases. In this paper, we analyze algorithms that can escape saddle points in nonconvex-strongly-convex bilevel optimization. Specifically, we show that the perturbed approximate implicit differentiation (AID) with a warm start strategy finds an $\epsilon$-approximate local minimum of bilevel optimization in $\tilde{O}(\epsilon^{-2})$ iterations with high probability. Moreover, we propose an inexact NEgative-curvature-Originated-from-Noise Algorithm (iNEON), an algorithm that can escape saddle point and find local minimum of stochastic bilevel optimization. As a by-product, we provide the first nonasymptotic analysis of perturbed multi-step gradient descent ascent (GDmax) algorithm that converges to local minimax point for minimax problems.},
  archive      = {J_JMLR},
  author       = {Minhui Huang and Xuxing Chen and Kaiyi Ji and Shiqian Ma and Lifeng Lai},
  journal      = {Journal of Machine Learning Research},
  number       = {1},
  pages        = {1-61},
  shortjournal = {J. Mach. Learn. Res.},
  title        = {Efficiently escaping saddle points in bilevel optimization},
  url          = {https://jmlr.org/papers/v26/22-0136.html},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="neco">NECO - 4</h2>
<ul>
<li><details>
<summary>
(2025). Feature normalization prevents collapse of noncontrastive learning dynamics. <em>NECO</em>, <em>37</em>(11), 2079-2124. (<a href='https://doi.org/10.1162/neco.a.27'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning is a self-supervised representation learning framework where two positive views generated through data augmentation are made similar by an attraction force in a data representation space, while a repulsive force makes them far from negative examples. Noncontrastive learning, represented by BYOL and SimSiam, gets rid of negative examples and improves computational efficiency. While learned representations may collapse into a single point due to the lack of the repulsive force at first sight, Tian et al. ( 2021 ) revealed through learning dynamics analysis that the representations can avoid collapse if data augmentation is sufficiently stronger than regularization. However, their analysis does not take into account commonly used feature normalization, a normalizer before measuring the similarity of representations, and hence excessively strong regularization may still collapse the dynamics, an unnatural behavior under the presence of feature normalization. Therefore, we extend the previous theory based on the L2 loss by considering the cosine loss instead, which involves feature normalization. We show that the cosine loss induces sixth-order dynamics (while the L2 loss induces a third-order one), in which a stable equilibrium dynamically emerges even if there are only collapsed solutions with given initial parameters. Thus, we offer a new understanding that feature normalization plays an important role in robustly preventing the dynamics collapse.},
  archive      = {J_NECO},
  author       = {Bao, Han},
  doi          = {10.1162/neco.a.27},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2079-2124},
  shortjournal = {Neural Comput.},
  title        = {Feature normalization prevents collapse of noncontrastive learning dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling higher-order interactions in sparse and heavy-tailed neural population activity. <em>NECO</em>, <em>37</em>(11), 2011-2078. (<a href='https://doi.org/10.1162/neco.a.35'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons process sensory stimuli efficiently, showing sparse yet highly variable ensemble spiking activity involving structured higher-order interactions. Notably, while neural populations are mostly silent, they occasionally exhibit highly synchronous activity, resulting in sparse and heavy-tailed spike-count distributions. However, its mechanistic origin—specifically, what types of nonlinear properties in individual neurons induce such population-level patterns—remains unclear. In this study, we derive sufficient conditions under which the joint activity of homogeneous binary neurons generates sparse and widespread population firing rate distributions in infinitely large networks. We then propose a subclass of exponential family distributions that satisfy this condition. This class incorporates structured higher-order interactions with alternating signs and shrinking magnitudes, along with a base-measure function that offsets distributional concentration, giving rise to parameter-dependent sparsity and heavy-tailed population firing rate distributions. Analysis of recurrent neural networks that recapitulate these distributions reveals that individual neurons possess threshold-like nonlinearity, followed by supralinear activation that jointly facilitates sparse and synchronous population activity. These nonlinear features resemble those in modern Hopfield networks, suggesting a connection between widespread population activity and the network’s memory capacity. The theory establishes sparse and heavy-tailed distributions for binary patterns, forming a foundation for developing energy-efficient spike-based learning machines.},
  archive      = {J_NECO},
  author       = {Rodríguez-Domínguez, Ulises and Shimazaki, Hideaki},
  doi          = {10.1162/neco.a.35},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2011-2078},
  shortjournal = {Neural Comput.},
  title        = {Modeling higher-order interactions in sparse and heavy-tailed neural population activity},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Encoding of numerosity with robustness to object and scene identity in biologically inspired object recognition networks. <em>NECO</em>, <em>37</em>(11), 1975-2010. (<a href='https://doi.org/10.1162/neco.a.30'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Number sense, the ability to rapidly estimate object quantities in a visual scene without precise counting, is a crucial cognitive capacity found in humans and many other animals. Recent studies have identified artificial neurons tuned to numbers of items in biologically inspired vision models, even before training, and proposed these artificial neural networks as candidate models for the emergence of number sense in the brain. But real-world numerosity perception requires abstraction from the properties of individual objects and their contexts, unlike the simplified dot patterns used in previous studies. Using novel synthetically generated photorealistic stimuli, we show that deep convolutional neural networks optimized for object recognition encode information on approximate numerosity across diverse objects and scene types, which could be linearly read out from distributed activity patterns of later convolutional layers of different network architectures tested. In contrast, untrained networks with random weights failed to represent numerosity with abstractness to other visual properties and instead captured mainly low-level visual features. Our findings emphasize the importance of using complex, naturalistic stimuli to investigate mechanisms of number sense in both biological and artificial systems, and they suggest that the capacity of untrained networks to account for early-life numerical abilities should be reassessed. They further point to a possible, so far underappreciated, contribution of the brain's ventral visual pathway to representing numerosity with abstractness to other high-level visual properties.},
  archive      = {J_NECO},
  author       = {Chapalain, Thomas and Thirion, Bertrand and Eger, Evelyn},
  doi          = {10.1162/neco.a.30},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {1975-2010},
  shortjournal = {Neural Comput.},
  title        = {Encoding of numerosity with robustness to object and scene identity in biologically inspired object recognition networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A chimera model for motion anticipation in the retina and the primary visual cortex. <em>NECO</em>, <em>37</em>(11), 1925-1974. (<a href='https://doi.org/10.1162/neco.a.34'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a mean field model of the primary visual cortex (V1), connected to a realistic retina model, to study the impact of the retina on motion anticipation. We first consider the case where the retina does not itself provide anticipation—which is then only triggered by a cortical mechanism, the “anticipation by latency”—and unravel the effects of the retinal input amplitude, of stimulus features such as speed and contrast and of the size of cortical extensions and fiber conduction speed. Then we explore the changes in the cortical wave of anticipation when V1 is triggered by retina-driven anticipatory mechanisms: gain control and lateral inhibition by amacrine cells. Here, we show how retinal and cortical anticipation combine to provide an efficient processing where the simulated cortical response is in advance over the moving object that triggers this response, compensating the delays in visual processing.},
  archive      = {J_NECO},
  author       = {Emonet, Jérôme and Souihel, Selma and Chavane, Frédéric and Destexhe, Alain and Volo, Matteo di and Cessac, Bruno},
  doi          = {10.1162/neco.a.34},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {1925-1974},
  shortjournal = {Neural Comput.},
  title        = {A chimera model for motion anticipation in the retina and the primary visual cortex},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="netn">NETN - 6</h2>
<ul>
<li><details>
<summary>
(2025). Reproducibility of resting-state functional connectivity in healthy aging and brain injury: A mini-multiverse analysis. <em>NETN</em>, <em>9</em>(3), 1154-1175. (<a href='https://doi.org/10.1162/netn_a_00459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resting-state functional connectivity (RSFC) methods are the most widely applied tools in the network neurosciences, but their reliability remains an active area of study. We use back-to-back 10-min resting-state scans in a healthy aging ( n = 41) and traumatic brain injury (TBI) sample ( n = 45) composed of older adults to assess the replicability of RSFC using a “mini” multiverse approach. The goal was to evaluate the reproducibility of commonly used graph metrics and determine if aging and moderate-severe TBI influences RSFC reliability using intraclass correlation coefficients (ICCs). There is clear evidence for reliable results in aging and TBI. Global network metrics such as within-network connectivity and segregation were most reliable whereas other whole-brain connectivity estimates (e.g., clustering coefficient, eigenvector centrality) were least reliable. Analysis of canonical networks revealed the default mode and salience networks as most reliable. There was a notable influence of motion scrubbing on ICCs, with diminished reliability proportional to the number of volumes removed. Choice of brain atlas had a modest effect on findings. Overall, RSFC reproducibility is preserved in older adults and after significant neurological compromise. We also identify a subset of graph metrics and canonical networks with promising reliability. In this paper, we examine the reproducibility of resting-state functional connectivity in healthy aging and traumatic brain injury (TBI). We use a mini-verse approach to determine if workflows have a significant effect on resting-state functional connectivity (RSFC) reliability. While the study of RSFC reliability has been previously examined (e.g., in healthy, young adults), it is lacking in the aging and TBI literatures. To our knowledge, this is the first study of back-to-back (consecutive) scans that examine RSFC reliability in healthy aging and TBI. In brief, these data and these analyses do not currently exist in the literature. RSFC reliability remains a vital area of investigation in healthy aging and clinical samples. We believe there are critical contributions that can be made by this paper.},
  archive      = {J_NETN},
  author       = {Mullin, Hollie A. and Carpenter, Catherine M. and Cwiek, Andrew P. and Lan, Gloria and Chase, Spencer O. and Carter, Emily E. and Vervoordt, Samantha M. and Rabinowitz, Amanda and Venkatesan, Umesh and Hillary, Frank G.},
  doi          = {10.1162/netn_a_00459},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1154-1175},
  shortjournal = {Netw. Neuroscience},
  title        = {Reproducibility of resting-state functional connectivity in healthy aging and brain injury: A mini-multiverse analysis},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing dynamic functional connectivity subnetwork contributions in narrative classification with shapley values. <em>NETN</em>, <em>9</em>(3), 1138-1153. (<a href='https://doi.org/10.1162/NETN.a.25'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional connectivity derived from functional magnetic resonance imaging (fMRI) data has been increasingly used to study brain activity. In this study, we model brain dynamic functional connectivity during narrative tasks as a temporal brain network and employ a machine learning model to classify in a supervised setting the modality (audio, movie), the content (airport, restaurant situations) of narratives, and both combined. Leveraging Shapley values, we analyze subnetwork contributions within Yeo parcellations (7- and 17-subnetworks) to explore their involvement in narrative modality and comprehension. This work represents the first application of this approach to functional aspects of the brain, validated by existing literature, and provides novel insights at the whole-brain level. Our findings suggest that schematic representations in narratives may not depend solely on preexisting knowledge of the top-down process to guide perception and understanding, but may also emerge from a bottom-up process driven by the temporal parietal subnetwork. This study investigates how different brain subnetworks contribute to processing narratives. We used a machine learning model to analyze fMRI data from participants listening to or watching narratives that varied in modality (audio or movie) and thematic content (airport or restaurant). Our model accurately classified these different narrative aspects, and by using Shapley values, we identified the subnetworks most crucial for each classification. Consistent with existing neuroscience knowledge, our findings highlight the distinct roles of these subnetworks in narrative comprehension. This study provides a powerful approach for investigating brain function across various domains.},
  archive      = {J_NETN},
  author       = {Rossi, Aurora and Aeschlimann, Yanis and Natale, Emanuele and Deslauriers-Gauthier, Samuel and Dominey, Peter Ford},
  doi          = {10.1162/NETN.a.25},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1138-1153},
  shortjournal = {Netw. Neuroscience},
  title        = {Characterizing dynamic functional connectivity subnetwork contributions in narrative classification with shapley values},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-guided generative adversarial networks for synthesizing and augmenting structural connectivity matrices for connectivity-based prediction. <em>NETN</em>, <em>9</em>(3), 1110-1137. (<a href='https://doi.org/10.1162/NETN.a.24'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent machine learning techniques have improved connectome-based predictions by modeling complex dependencies between brain connectivity and cognitive traits. However, they typically require large datasets that are costly and time-consuming to collect. To address this, we propose Task-guided generative adversarial network (GAN) II, a novel data augmentation method that uses GANs to expand sample sizes in connectome-based prediction tasks. Our method incorporates a task-guided branch within the Wasserstein GAN framework, specifically designed to synthesize structural connectivity matrices and improve prediction accuracy by capturing task-relevant features. We evaluated Task-guided GAN II on the prediction of fluid intelligence using the NIMH Health Research Volunteer Dataset. Results showed that data augmentation improved prediction accuracy. To further assess whether augmentation can substitute for increasing actual collected sample sizes, we conducted additional validation using the Human Connectome Project WU-Minn S1200 dataset. Task-guided GAN II improved prediction performance with limited real data, with gains of up to twofold augmentation observed. However, excessive augmentation did not result in further improvements, suggesting that augmentation complements, but does not fully replace, real data augmentation. These results suggest that Task-guided GAN II is a promising tool for harnessing small datasets in human connectomics research, improving predictive modeling where large-scale data collection is impractical.},
  archive      = {J_NETN},
  author       = {Yamamoto, Tatsuya and Sugiura, Tomoki and Hiroyasu, Tomoyuki and Hiwa, Satoru},
  doi          = {10.1162/NETN.a.24},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1110-1137},
  shortjournal = {Netw. Neuroscience},
  title        = {Task-guided generative adversarial networks for synthesizing and augmenting structural connectivity matrices for connectivity-based prediction},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stable brain PET metabolic networks using a multiple sampling scheme. <em>NETN</em>, <em>9</em>(3), 1087-1109. (<a href='https://doi.org/10.1162/NETN.a.23'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interregional communication within the human brain is essential for maintaining functional integrity. A promising approach for investigating how brain regions communicate relies on the assumption that the brain operates as a complex network. In this context, positron emission tomography (PET) images have been suggested as a valuable source for understanding brain networks. However, such networks are typically assembled through direct computation without accounting for outliers, impacting the reliability of group representative networks. In this study, we used brain [ 18 F]fluoro-2-deoxyglucose PET data from 1,227 individuals in the Alzheimer’s disease (AD) continuum from the Alzheimer’s Disease Neuroimaging Initiative cohort to develop a novel method for constructing stable metabolic brain networks that are resilient to spurious data points. Our multiple sampling scheme generates brain networks with greater stability compared with conventional approaches. The proposed method is robust to imbalanced datasets and requires 50% fewer subjects to achieve stability than the conventional method. We further validated the approach in an independent AD cohort ( n = 114) from São Paulo, Brazil (Faculdade de Medicina da Universidade de São Paulo). This innovative method is flexible and improves the robustness of metabolic brain network analyses, supporting better insights into brain connectivity and resilience to data variability across multiple radiotracers for both health and disease.},
  archive      = {J_NETN},
  author       = {Schu, Guilherme and Limberger, Christian and Brum, Wagner S. and De Bastiani, Marco Antônio and Rodrigues, Yuri Elias and de Azeredo, Julio Cesar and Pascoal, Tharick A. and Benedet, Andrea L. and Mathotaarachchi, Sulantha and Rosa-Neto, Pedro and Almeida, Jorge and de Paula Faria, Daniele and de Souza Duran, Fábio Luiz and Buchpiguel, Carlos Alberto and Coutinho, Artur Martins and Busatto, Geraldo F. and Zimmer, Eduardo R. and for the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1162/NETN.a.23},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1087-1109},
  shortjournal = {Netw. Neuroscience},
  title        = {Stable brain PET metabolic networks using a multiple sampling scheme},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial correlation as a tool for mapping functional-structural correspondence in human brain connectivity. <em>NETN</em>, <em>9</em>(3), 1065-1086. (<a href='https://doi.org/10.1162/NETN.a.22'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain structure-function coupling has been studied in health and disease by many different researchers in recent years. Most of the studies have estimated functional connectivity matrices as correlation coefficients between different brain areas, despite well-known disadvantages compared with partial correlation connectivity matrices. Indeed, partial correlation represents a more sensible model for structural connectivity since, under a Gaussian approximation, it accounts only for direct dependencies between brain areas. Motivated by this and following previous results by different authors, we investigate structure-function coupling using partial correlation matrices of functional magnetic resonance imaging brain activity time series under various regularization (also known as noise-cleaning) algorithms. We find that, across different algorithms and conditions, partial correlation provides a higher match with structural connectivity retrieved from density-weighted imaging data than standard correlation, and this occurs at both subject and population levels. Importantly, we also show that regularization and thresholding are crucial for this match to emerge. Finally, we assess neurogenetic associations in relation to structure-function coupling, which presents promising opportunities to further advance research in the field of network neuroscience, particularly concerning brain disorders. A precise understanding of how brain structure and function interact is fundamentally relevant to understanding disease. For the functional representation, most of the previous research has used correlation methods, which have limitations. Our study explores a different approach called partial correlation methods, which more accurately reflect the brain’s direct connections. We found that partial correlation aligns better with the brain’s structural connectivity than standard methods, both in individuals and groups. Additionally, we identified promising links between brain connectivity and genetics, offering new insights into brain disorders. Our work highlights the importance of using advanced connectivity methods to improve our understanding of the brain’s structure-function relationship, paving the way for future research in brain health and disease.},
  archive      = {J_NETN},
  author       = {Santucci, Francesca and Jimenez-Marin, Antonio and Gabrielli, Andrea and Bonifazi, Paolo and Ibáñez-Berganza, Miguel and Gili, Tommaso and Cortes, Jesus M.},
  doi          = {10.1162/NETN.a.22},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1065-1086},
  shortjournal = {Netw. Neuroscience},
  title        = {Partial correlation as a tool for mapping functional-structural correspondence in human brain connectivity},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic fluctuations of intrinsic brain activity are associated with consistent topological patterns in puberty and are biomarkers of neural maturation. <em>NETN</em>, <em>9</em>(3), 1039-1064. (<a href='https://doi.org/10.1162/netn_a_00452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic brain dynamics play a fundamental role in cognitive function, but their development is incompletely understood. We investigated pubertal changes in temporal fluctuations of intrinsic network topologies (focusing on the strongest connections and coordination patterns) and signals, in an early longitudinal sample from the Adolescent Brain Cognitive Development (ABCD) study, with resting-state fMRI ( n = 4,099 at baseline; n = 3,376 at follow-up [median age = 10.0 (1.1) and 12.0 (1.1) years; n = 2,116 with both assessments]). Reproducible, inverse associations between low-frequency signal and topological fluctuations were estimated ( p < 0.05, β = −0.20 to −0.02, 95% confidence interval (CI) = [−0.23, −0.001]). Signal (but not topological) fluctuations increased in somatomotor and prefrontal areas with pubertal stage ( p < 0.03, β = 0.06–0.07, 95% CI = [0.03, 0.11]), but decreased in orbitofrontal, insular, and cingulate cortices, as well as cerebellum, hippocampus, amygdala, and thalamus ( p < 0.05, β = −0.09 to −0.03, 95% CI = [−0.15, −0.001]). Higher temporal signal and topological variability in spatially distributed regions were estimated in girls. In racial/ethnic minorities, several associations between signal and topological fluctuations were in the opposite direction of those in the entire sample, suggesting potential racial differences. Our findings indicate that during puberty, intrinsic signal dynamics change significantly in developed and developing brain regions, but their strongest coordination patterns may already be sufficiently developed and remain temporally consistent. We have investigated pubertal changes in intrinsic signal and network dynamics, estimated from resting-state fMRI in a sample of youth from the ABCD study. We have identified reproducible, inverse associations between low-frequency signal and topological fluctuations, as well as pubertal changes in intrinsic signal dynamics but not topological patterns of strongly connected networks. We have also identified sex differences in these dynamics and negative associations with body mass index (BMI). Several associations between signal and topological fluctuations were in the opposite direction in racial/ethnic minorities compared with those in the entire sample. Our findings indicate that intrinsic signal dynamics change significantly in developed and developing brain regions during puberty, but their strongest synchronization patterns may already be sufficiently developed prior to puberty and are dynamically reproducible.},
  archive      = {J_NETN},
  author       = {Lim, Jethro and Cooper, Kaitlynn and Stamoulis, Catherine},
  doi          = {10.1162/netn_a_00452},
  journal      = {Network Neuroscience},
  month        = {9},
  number       = {3},
  pages        = {1039-1064},
  shortjournal = {Netw. Neuroscience},
  title        = {Dynamic fluctuations of intrinsic brain activity are associated with consistent topological patterns in puberty and are biomarkers of neural maturation},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="tacl">TACL - 1</h2>
<ul>
<li><details>
<summary>
(2025). A systematic review of NLP for dementia: Tasks, datasets, and opportunities. <em>TACL</em>, <em>13</em>, 1204-1244. (<a href='https://doi.org/10.1162/TACL.a.35'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The close link between cognitive decline and language has fostered long-standing collaboration between the NLP and medical communities in dementia research. To examine this, we reviewed over 240 papers applying NLP to dementia-related efforts, drawing from medical, technological, and NLP-focused literature. We identify key research areas, including dementia detection, linguistic biomarker extraction, caregiver support, and patient assistance, showing that half of all papers focus solely on dementia detection using clinical data. Yet, many directions remain unexplored, such as artificially degraded language models, synthetic data, digital twins, and more. We highlight gaps and opportunities around trust, scientific rigor, applicability, and cross-community collaboration. We raise ethical dilemmas in the field, and highlight the diverse datasets encountered throughout our review including recorded, written, structured, spontaneous, synthetic, clinical, social media–based, and more. This review aims to inspire more creative, impactful, and rigorous research on NLP for dementia.},
  archive      = {J_TACL},
  author       = {Peled-Cohen, Lotem and Reichart, Roi},
  doi          = {10.1162/TACL.a.35},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {10},
  pages        = {1204-1244},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A systematic review of NLP for dementia: Tasks, datasets, and opportunities},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="tmlr">TMLR - 40</h2>
<ul>
<li><details>
<summary>
(2025). Contextual combinatorial bandits with changing action sets via gaussian processes. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=2RgfAY3jnI'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a contextual bandit problem with a combinatorial action set and time-varying base arm availability. At the beginning of each round, the agent observes the set of available base arms and their contexts and then selects an action that is a feasible subset of the set of available base arms to maximize its cumulative reward in the long run. We assume that the mean outcomes of base arms are samples from a Gaussian Process \rev{(GP)} indexed by the context set ${\cal X}$, and the expected reward is Lipschitz continuous in expected base arm outcomes. For this setup, we propose an algorithm called Optimistic Combinatorial Learning and Optimization with Kernel Upper Confidence Bounds (O'CLOK-UCB) and prove that it incurs $\tilde{O}(\sqrt{\lambda^*(K)KT\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)} )$ regret with high probability, where $\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)$ is the maximum information gain associated with the sets of base arm contexts $\mathcal{X}_t$ that appeared in the first $T$ rounds, $K$ is the maximum cardinality of any feasible action over all rounds, and $\lambda^*(K)$ is the maximum eigenvalue of all covariance matrices of selected actions up to time $T$, which is a function of $K$. To dramatically speed up the algorithm, we also propose a variant of O'CLOK-UCB that uses sparse GPs. Finally, we experimentally show that both algorithms exploit inter-base arm outcome correlation and vastly outperform the previous state-of-the-art UCB-based algorithms in realistic setups.},
  archive      = {J_TMLR},
  author       = {Andi Nika and Sepehr Elahi and Cem Tekin},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Contextual combinatorial bandits with changing action sets via gaussian processes},
  url          = {https://openreview.net/forum?id=2RgfAY3jnI},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial robustness of graph transformers. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=4xK0vjxTWL'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies have shown that Message-Passing Graph Neural Networks (MPNNs) are highly susceptible to adversarial attacks. In contrast, despite the increasing importance of Graph Transformers (GTs), their robustness properties are unexplored. We close this gap and design the first adaptive attacks for GTs. In particular, we provide general design principles for strong gradient-based attacks on GTs w.r.t. structure perturbations and instantiate our attack framework for five representative and popular GT architectures. Specifically, we study GTs with specialized attention mechanisms and Positional Encodings (PEs) based on pairwise shortest paths, random walks, and the Laplacian spectrum. We evaluate our attacks on multiple tasks and perturbation models, including structure perturbations for node and graph classification, and node injection for graph classification. Our results reveal that GTs can be catastrophically fragile in many cases. Addressing this vulnerability, we show how our adaptive attacks can be effectively used for adversarial training, substantially improving robustness.},
  archive      = {J_TMLR},
  author       = {Philipp Foth and Lukas Gosch and Simon Geisler and Leo Schwinn and Stephan Günnemann},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Adversarial robustness of graph transformers},
  url          = {https://openreview.net/forum?id=4xK0vjxTWL},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting B2T: Discovering and mitigating visual biases through keyword explanations. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=5GS1q65pv6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to reproduce and extend the findings of "Discovering and Mitigating Visual Biases through Keyword Explanation" by Kim et al.(2024). The paper proposes the B2T framework, which detects and mitigates visual biases by extracting keywords from generated captions. By identifying biases in datasets, B2T contributes to the prevention of discriminatory behavior in vision-language models. We aim to investigate the five key claims from the original paper, namely that B2T (i) is able to identify whether a word represents a bias, (ii) can extract these keywords from captions of mispredicted images, (iii) outperforms other bias discovery models, (iv) can improve CLIP zero-shot prompting with the discovered keywords, and (v) identifies labeling errors in a dataset. To reproduce their results, we use the publicly available codebase and our re-implementations. Our findings confirm the first three claims and partially validate the fourth. We reject the fifth claim, due to the failure to identify pertinent labeling errors. Finally, we enhance the original work by optimizing the efficiency of the implementation, and assessing the generalizability of B2T on a new dataset.},
  archive      = {J_TMLR},
  author       = {Faissal El Kayouhi and Aïda Asma and Joey Laarhoven and Fiona Nagelhout},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Revisiting B2T: Discovering and mitigating visual biases through keyword explanations},
  url          = {https://openreview.net/forum?id=5GS1q65pv6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Capsule network projectors are equivariant and invariant learners. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=7owCO3qskH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning invariant representations has been the longstanding approach to self-supervised learning. However, recently progress has been made in preserving equivariant properties in representations, yet do so with highly prescribed architectures. In this work, we propose an invariant-equivariant self-supervised architecture that employs Capsule Networks (CapsNets), which have been shown to capture equivariance with respect to novel viewpoints. We demonstrate that the use of CapsNets in equivariant self-supervised architectures achieves improved downstream performance on equivariant tasks with higher efficiency and fewer network parameters. To accommodate the architectural changes of CapsNets, we introduce a new objective function based on entropy minimisation. This approach, which we name CapsIE (Capsule Invariant Equivariant Network), achieves state-of-the-art performance on the equivariant rotation tasks on the 3DIEBench dataset compared to prior equivariant SSL methods, while performing competitively against supervised counterparts. Our results demonstrate the ability of CapsNets to learn complex and generalised representations for large-scale, multi-task datasets compared to previous CapsNet benchmarks.},
  archive      = {J_TMLR},
  author       = {Miles Everett and Aiden Durrant and Mingjun Zhong and Georgios Leontidis},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Capsule network projectors are equivariant and invariant learners},
  url          = {https://openreview.net/forum?id=7owCO3qskH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent trajectory: A new framework for deep actor-critic reinforcement learning with uncertainty quantification. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=8B74xdaRHa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification in deep learning is challenging due to the complexity of deep neural networks. This challenge is particularly pronounced in deep reinforcement learning (RL), where agents interact with stochastic environments. In deep actor-critic RL, this challenge is further exacerbated due to the interdependence between the actor and critic updates. Existing uncertainty quantification methods for RL are predominantly developed within the Bayesian framework. While these methods estimate the uncertainty of the value function, their confidence intervals are often misleading, with the coverage rate frequently falling well below the nominal level. To address this issue, we introduce a novel deep RL framework that treats transition trajectories as latent variables. Leveraging this framework, we propose an adaptive Stochastic Gradient Markov Chain Monte Carlo algorithm to train deep actor-critic models, which naturally accounts for the interdependence between the actor and critic updates. We provide theoretical guarantees for the convergence of the proposed method and offer empirical evidence for its effectiveness in uncertainty quantification of the value function. The proposed latent trajectory framework is highly flexible, allowing for the integration of advanced RL strategies to further enhance deep actor-critic learning.},
  archive      = {J_TMLR},
  author       = {Frank Shih and Faming Liang},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Latent trajectory: A new framework for deep actor-critic reinforcement learning with uncertainty quantification},
  url          = {https://openreview.net/forum?id=8B74xdaRHa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D2 actor critic: Diffusion actor meets distributional critic. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=8KbstCUXhH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce D2AC, a new model-free reinforcement learning (RL) algorithm designed to train expressive diffusion policies online effectively. At its core is a policy improvement objective that avoids the high variance of typical policy gradients and the complexity of backpropagation through time. This stable learning process is critically enabled by our second contribution: a robust distributional critic, which we design through a fusion of distributional RL and clipped double Q-learning. The resulting algorithm is highly effective, achieving state-of-the-art performance on a benchmark of eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains, spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard benchmarks, we also evaluate a biologically motivated predator-prey task to examine the behavioral robustness and generalization capacity of our approach.},
  archive      = {J_TMLR},
  author       = {Lunjun Zhang and Shuo Han and Hanrui Lyu and Bradly C. Stadie},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {D2 actor critic: Diffusion actor meets distributional critic},
  url          = {https://openreview.net/forum?id=8KbstCUXhH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Illusion or algorithm? investigating memorization, emergence, and symbolic processing in in-context learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=10QqO1tM1H'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere "memorization" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.},
  archive      = {J_TMLR},
  author       = {Jingcheng Niu and Subhabrata Dutta and Ahmed Elshabrawy and Harish Tayyar Madabushi and Iryna Gurevych},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Illusion or algorithm? investigating memorization, emergence, and symbolic processing in in-context learning},
  url          = {https://openreview.net/forum?id=10QqO1tM1H},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blending adversarial training and representation-conditional purification via aggregation improves adversarial robustness. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=40BXthYscW'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a novel adversarial defence mechanism for image classification - CARSO - blending the paradigms of adversarial training and adversarial purification in a synergistic robustness-enhancing way. The method builds upon an adversarially-trained classifier, and learns to map its internal representation associated with a potentially perturbed input onto a distribution of tentative clean reconstructions. Multiple samples from such distribution are classified by the same adversarially-trained model, and a carefully chosen aggregation of its outputs finally constitutes the robust prediction of interest. Experimental evaluation by a well-established benchmark of strong adaptive attacks, across different image datasets, shows that CARSO is able to defend itself against adaptive end-to-end white-box attacks devised for stochastic defences. With a modest clean accuracy penalty, our method improves by a significant margin the state-of-the-art for Cifar-10, Cifar-100, and TinyImageNet-200 $\ell_\infty$ robust classification accuracy against AutoAttack.},
  archive      = {J_TMLR},
  author       = {Emanuele Ballarin and Alessio ansuini and Luca Bortolussi},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Blending adversarial training and representation-conditional purification via aggregation improves adversarial robustness},
  url          = {https://openreview.net/forum?id=40BXthYscW},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven discovery of PDEs via the adjoint method. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Az3mJ4d1eT'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form and formulate a PDE-constrained optimization problem aimed at minimizing the error of the PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations), allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, we consider a family of temporal parameterized PDEs that encompass linear, nonlinear, and spatial derivative candidate terms, and elegantly derive the corresponding adjoint equations. We show the efficacy of the proposed approach in identifying the form of the PDE up to machine accuracy, enabling the accurate discovery of PDEs from data. We also compare its performance with the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND \cite{rudy2017data} among others, on both smooth and noisy data sets. Even though the proposed adjoint method relies on forward/backward solvers, it outperforms PDE-FIND in the limit of large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter.},
  archive      = {J_TMLR},
  author       = {Mohsen Sadr and Tony Tohme and KAMAL YOUCEF-TOUMI},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Data-driven discovery of PDEs via the adjoint method},
  url          = {https://openreview.net/forum?id=Az3mJ4d1eT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal horizons in forecasting: A performance-learnability trade-off. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=BeudQIxT1R'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training autoregressive models to forecast dynamical systems, a critical question arises: how far into the future should the model be trained to predict for optimal performance? In this work, we address this question by analyzing the relationship between the geometry of the loss landscape and the training time horizon. Using dynamical systems theory, we prove that loss minima for long horizons generalize well to short-term forecasts, whereas minima found on short horizons result in worse long-term predictions. However, we also prove that the loss landscape becomes rougher as the training horizon grows, making long-horizon training inherently challenging. We validate our theory through numerical experiments and discuss practical implications for selecting training horizons. Our results provide a principled foundation for hyperparameter optimization in autoregressive forecasting models.},
  archive      = {J_TMLR},
  author       = {Pau Vilimelis Aceituno and Jack William Miller and Noah Marti and Youssef Farag and Victor Boussange},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Temporal horizons in forecasting: A performance-learnability trade-off},
  url          = {https://openreview.net/forum?id=BeudQIxT1R},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VSCoDe: Visual-augmentation selection for contrastive decoding. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=CqSyPc9W7Y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive performance of recent Large Vision-Language Models (LVLMs), these models often produce inaccurate responses. To address this issue, previous studies have aimed to reduce hallucinations by using contrastive decoding (CD) with modified images, such as cropping objects related to query or adding noise, thereby contrasting with the original image. However, these methods have several limitations. First, employing fixed visual augmentation, such as adding noise, is a simple approach but too rigid to contrast on various queries. Conversely, using semantics in queries or images by leveraging external models can adaptively generate contrastive images, but it entails significant additional costs. To address these shortcomings, we explore using pre-defined visual augmentations to enable flexible adaptation to each query without relying on external models. We observe that each query achieves different contrasts through different visual augmentations. Based on this, we propose a novel method called VSCoDe, Visual-Augmentation Selection for Contrastive Decoding, which adaptively selects augmentations using a proposed distance metric to identify those with higher contrast. Our empirical evaluations demonstrate that VSCoDe outperforms previous methods and enhances the quality of various vision-language tasks without additional training or reliance on external models.},
  archive      = {J_TMLR},
  author       = {Sihyeon Kim and Boryeong Cho and Sangmin Bae and Sumyeong Ahn and Se-Young Yun},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {VSCoDe: Visual-augmentation selection for contrastive decoding},
  url          = {https://openreview.net/forum?id=CqSyPc9W7Y},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized smooth stochastic variational inequalities: Almost sure convergence and convergence rates. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=EjqSpbUBWU'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on solving a stochastic variational inequality (SVI) problem under relaxed smoothness assumption for a class of structured non-monotone operators. The SVI problem has attracted significant interest in the machine learning community due to its immediate application to adversarial training and multi-agent reinforcement learning. In many such applications, the resulting operators do not satisfy the smoothness assumption. To address this issue, we focus on a weaker generalized smoothness assumption called $\alpha$-symmetric. Under $p$-quasi sharpness and $\alpha$-symmetric assumptions on the operator, we study clipped projection (gradient descent-ascent) and clipped Korpelevich (extragradient) methods. For these clipped methods, we provide the first almost-sure convergence results without making any assumptions on the boundedness of either the stochastic operator or the stochastic samples. We also provide the first in-expectation unbiased convergence rate results for these methods under a relaxed smoothness assumption for $\alpha \leq \frac{1}{2}$.},
  archive      = {J_TMLR},
  author       = {Daniil Vankov and Angelia Nedich and Lalitha Sankar},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Generalized smooth stochastic variational inequalities: Almost sure convergence and convergence rates},
  url          = {https://openreview.net/forum?id=EjqSpbUBWU},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EL-clustering: Combining upper- and lower-bounded clusterings for equitable load constraints. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=EkjDfnJ1gU'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of an ordinary clustering algorithm may yield a clustering output where the number of points per cluster (cluster size) varies significantly. In settings where the centers correspond to facilities that provide a service, this can be highly undesirable as the cluster size is essentially the service load for a facility. While prior work has considered imposing either a lower bound on the cluster sizes or an upper bound, imposing both bounds simultaneously has seen limited work, especially for the $k$-median objective, despite its strong practical motivation. In this paper, we solve the \emph{equitable load} (\EL{}) clustering problem where we minimize the $k$-median objective subject to the cluster sizes not exceeding an upper bound or falling below a lower bound. We solve this problem using a modular approach. Specifically, given a clustering solution that satisfies the lower bound constraints and another that satisfies the upper bound constraints, we introduce a combination algorithm which essentially combines both solutions to produce one that satisfies both constraints simultaneously at the expense of a bounded degradation in the $k$-median objective and a slight violation of the upper bound. Our combination algorithm runs in $O(k^3+n)$ time, where $n$ is the number of points and is faster than standard $k$-median algorithms that satisfy either the lower or upper bound constraints. Interestingly, our results can be generalized to various other clustering objectives, including the $k$-means objective. We also do empirical evaluation for $k$-Median objective on benchmark datasets to show that both, the cost as well as the violation factor are significantly smaller in practice than the theoretical worst-case guarantees\footnote{https://github.com/0-rudra-0/el-clustering}.},
  archive      = {J_TMLR},
  author       = {Rajni Dabas and Neelima Gupta and Rudra Bhardwaj and Sapna Grover},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {EL-clustering: Combining upper- and lower-bounded clusterings for equitable load constraints},
  url          = {https://openreview.net/forum?id=EkjDfnJ1gU},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From novelty to imitation: Self-distilled rewards for offline reinforcement learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=F5K94JI2Jb'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline Reinforcement Learning (RL) aims to learn effective policies from a static dataset without requiring further agent environment interactions. However, its practical adoption is often hindered by the need for explicit reward annotations, which can be costly to engineer or difficult to obtain retrospectively. To address this, we propose ReLOAD (Reinforcement Learning with Offline Reward Annotation via Distillation), a novel reward annotation framework for offline RL. Unlike existing methods that depend on complex alignment procedures, our approach adapts Random Network Distillation (RND) to generate intrinsic rewards from expert demonstrations using a simple yet effective embedding discrepancy measure. First, we train a predictor network to mimic a fixed target network’s embeddings based on expert state transitions. Later, the prediction error between these networks serves as a reward signal for each transition in the static dataset. This mechanism provides a structured reward signal without requiring handcrafted reward annotations. We provide a formal theoretical construct that provides insights into how RND prediction errors effectively serve as intrinsic rewards by distinguishing expert-like transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables robust offline policy learning and achieves performance competitive with traditional reward-annotated methods.},
  archive      = {J_TMLR},
  author       = {Gaurav Chaudhary and Laxmidhar Behera},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {From novelty to imitation: Self-distilled rewards for offline reinforcement learning},
  url          = {https://openreview.net/forum?id=F5K94JI2Jb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An architecture built for federated learning: Addressing data heterogeneity through adaptive normalization-free feature recalibration. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=GtdYFLsblb'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a decentralized collaborative training paradigm preserving stakeholders’ data ownership while improving performance and generalization. However, statistical heterogeneity among client datasets degrades system performance. To address this issue, we propose Adaptive Normalization-free Feature Recalibration (ANFR), a model architecture-level approach that combines weight standardization and channel attention to combat heterogeneous data in FL. ANFR leverages weight standardization to avoid mismatched client statistics and inconsistent averaging, ensuring robustness under heterogeneity, and channel attention to produce learnable scaling factors for feature maps, suppressing inconsistencies across clients due to heterogeneity. We demonstrate that combining these techniques boosts model performance beyond their individual contributions, by improving class selectivity and channel attention weight distribution. ANFR works with any aggregation method, supports both global and personalized FL, and adds minimal overhead. Furthermore, when training with differential privacy, ANFR achieves an appealing balance between privacy and utility, enabling strong privacy guarantees without sacrificing performance. By integrating weight standardization and channel attention in the backbone model, ANFR offers a novel and versatile approach to the challenge of statistical heterogeneity. Extensive experiments show ANFR consistently outperforms established baselines across various aggregation methods, datasets, and heterogeneity conditions. Code is provided at https://github.com/siomvas/ANFR.},
  archive      = {J_TMLR},
  author       = {Vasilis Siomos and Jonathan Passerat-Palmbach and Giacomo Tarroni},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {An architecture built for federated learning: Addressing data heterogeneity through adaptive normalization-free feature recalibration},
  url          = {https://openreview.net/forum?id=GtdYFLsblb},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially private clustered federated learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=JSsko0a4yr'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL), which is a decentralized machine learning (ML) approach, often incorporates differential privacy (DP) to provide rigorous data privacy guarantees to clients. Previous works attempted to address high structured data heterogeneity in vanilla FL settings through clustering clients (a.k.a clustered FL), but these methods remain sensitive and prone to errors, further exacerbated by the DP noise. This vulnerability makes the previous methods inappropriate for differentially private FL (DPFL) under structured data heterogeneity. To address this gap, we propose an algorithm for differentially private clustered FL, which is robust to the DP noise in the system and identifies the underlying clients’ clusters correctly. To this end, we propose to cluster clients based on both their model updates and training loss values. Furthermore, for clustering clients’ model updates at the end of the first round, our proposed approach addresses the server’s uncertainties by employing large batch sizes as well as Gaussian Mixture Models (GMM) to reduce the impact of DP and stochastic noise and avoid potential clustering errors. We provide theoretical analysis to justify our approach and evaluate it across diverse data distributions and privacy budgets. Our experimental results show the approach’s effectiveness in addressing high structured data heterogeneity in DPFL.},
  archive      = {J_TMLR},
  author       = {Saber Malekmohammadi and Afaf Taik and Golnoosh Farnadi},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Differentially private clustered federated learning},
  url          = {https://openreview.net/forum?id=JSsko0a4yr},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On diffusion posterior sampling via sequential monte carlo for zero-shot scaffolding of protein motifs. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=KXRYY7iwqh'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of diffusion models, new proteins can be generated at an unprecedented rate. The motif scaffolding problem requires steering this generative process to yield proteins with a desirable functional substructure called a motif. While models have been trained to take the motif as conditional input, recent techniques in diffusion posterior sampling can be leveraged as zero-shot alternatives whose approximations can be corrected with sequential Monte Carlo (SMC) algorithms. In this work, we introduce a new set of guidance potentials for describing scaffolding tasks and solve them by adapting SMC-aided diffusion posterior samplers with an unconditional model, Genie, as a prior. In single motif problems, we find that (i) the proposed potentials perform comparably, if not better, than the conventional masking approach, (ii) samplers based on reconstruction guidance outperform their replacement method counterparts, and (iii) measurement tilted proposals and twisted targets improve performance substantially. Furthermore, as a demonstration, we provide solutions to two multi-motif problems by pairing reconstruction guidance with an SE(3)-invariant potential. We also produce designable internally symmetric monomers with a guidance potential for point symmetry constraints. Our code is available at: https://github.com/matsagad/mres-project.},
  archive      = {J_TMLR},
  author       = {James Matthew Young and O. Deniz Akyildiz},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {On diffusion posterior sampling via sequential monte carlo for zero-shot scaffolding of protein motifs},
  url          = {https://openreview.net/forum?id=KXRYY7iwqh},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic primal-dual double block-coordinate for two- way partial AUC maximization. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=M3kibBFP4q'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-way partial AUC (TPAUC) is a critical performance metric for binary classification with imbalanced data, as it focuses on specific ranges of the true positive rate (TPR) and false positive rate (FPR). However, stochastic algorithms for TPAUC optimization remain under-explored, with existing methods either limited to approximated TPAUC loss functions or burdened by sub-optimal complexities. To overcome these limitations, we introduce two innovative stochastic primal-dual double block-coordinate algorithms for TPAUC maximization. These algorithms utilize stochastic block-coordinate updates for both the primal and dual variables, catering to both convex and non-convex settings. We provide theoretical convergence rate analyses, demonstrating significant improvements over prior approaches. Our experimental results, based on multiple benchmark datasets, validate the superior performance of our algorithms, showcasing faster convergence and better generalization. This work advances the state of the art in TPAUC optimization and offers practical tools for real-world machine learning applications.},
  archive      = {J_TMLR},
  author       = {Linli Zhou and Bokun Wang and My T. Thai and Tianbao Yang},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Stochastic primal-dual double block-coordinate for two- way partial AUC maximization},
  url          = {https://openreview.net/forum?id=M3kibBFP4q},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning time-series representations by hierarchical uniformity-tolerance latent balancing. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=NTmVEAiyB5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose TimeHUT, a novel method for learning time-series representations by hierarchical uniformity-tolerance balancing of contrastive representations. Our method uses two distinct losses to learn strong representations with the aim of striking an effective balance between uniformity and tolerance in the embedding space. First, TimeHUT uses a hierarchical setup to learn both instance-wise and temporal information from input time-series. Next, we integrate a temperature scheduler within the vanilla contrastive loss to balance the uniformity and tolerance characteristics of the embeddings. Additionally, a hierarchical angular margin loss enforces instance-wise and temporal contrast losses, creating geometric margins between positive and negative pairs of temporal sequences. This approach improves the coherence of positive pairs and their separation from the negatives, enhancing the capture of temporal dependencies within a time-series sample. We evaluate our approach on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and multivariate classification, as well as Yahoo and KPI datasets for anomaly detection. The results demonstrate that TimeHUT outperforms prior methods by considerable margins on classification, while obtaining competitive results for anomaly detection. Finally, detailed sensitivity and ablation studies are performed to evaluate different components and hyperparameters of our method.},
  archive      = {J_TMLR},
  author       = {Amin Jalali and Milad Soltany and Michael Greenspan and Ali Etemad},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Learning time-series representations by hierarchical uniformity-tolerance latent balancing},
  url          = {https://openreview.net/forum?id=NTmVEAiyB5},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale targeted cause discovery via learning from simulated data. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=NVgy29IQw8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel machine learning approach for inferring causal variables of a target variable from observations. Our focus is on directly inferring a set of causal factors without requiring full causal graph reconstruction, which is computationally challenging in large-scale systems. The identified causal set consists of all potential regulators of the target variable under experimental settings, enabling efficient regulation through intervention. To achieve this, we train a neural network using supervised learning on simulated data to infer causality. By employing a subsampled-ensemble inference strategy, our approach scales with linear complexity in the number of variables, efficiently scaling up to thousands of variables. Empirical results demonstrate superior performance in identifying causal relationships within large-scale gene regulatory networks, outperforming existing methods that emphasize full-graph discovery. We validate our model's generalization capability across out-of-distribution graph structures and generating mechanisms, including gene regulatory networks of E. coli and the human K562 cell line.},
  archive      = {J_TMLR},
  author       = {Jang-Hyun Kim and Claudia Skok Gibbs and Sangdoo Yun and Hyun Oh Song and Kyunghyun Cho},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Large-scale targeted cause discovery via learning from simulated data},
  url          = {https://openreview.net/forum?id=NVgy29IQw8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSC: Posterior sampling-based compression. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=OsqgU6Jz4t'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have transformed the landscape of image generation and now show remarkable potential for image compression. Most of the recent diffusion-based compression methods require training and are tailored for a specific bit-rate. In this work, we propose Posterior Sampling-based Compression (PSC) -- a zero-shot compression method that leverages a pre-trained diffusion model as its sole neural network component, thus enabling the use of diverse, publicly available models without additional training. Our approach is inspired by transform coding methods, which encode the image in some pre-chosen transform domain. However, PSC constructs a transform that is adaptive to the image. This is done by employing a zero-shot diffusion-based posterior sampler so as to progressively construct the rows of the transform matrix. Each new chunk of rows is chosen to reduce the uncertainty about the image given the quantized measurements collected thus far. Importantly, the same adaptive scheme can be replicated at the decoder, thus avoiding the need to encode the transform itself. We demonstrate that even with basic quantization and entropy coding, PSC's performance is comparable to established training-based methods in terms of rate, distortion, and perceptual quality. This is while providing greater flexibility, allowing to choose at inference time any desired rate or distortion.},
  archive      = {J_TMLR},
  author       = {Noam Elata and Tomer Michaeli and Michael Elad},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {PSC: Posterior sampling-based compression},
  url          = {https://openreview.net/forum?id=OsqgU6Jz4t},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coreset-driven re-labeling: Tackling noisy annotations with noise-free gradients. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=Tk78vb2Qd7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale datasets invariably contain annotation noise. Re-labeling methods have been developed to handle annotation noise in large-scale datasets. Though various methodologies to alleviate annotation noise have been developed, these are particularly time-consuming and computationally intensive. The requirement of high computational power and longer time duration can be drastically reduced by selecting a representative coreset. In this work, we adapt a noise-free gradient-based coreset selection method towards re-labeling applications for noisy datasets with erroneous labels. We introduce ‘confidence score’ to the coreset selection method to cater for the presence of noisy labels. Through extensive evaluation over CIFAR-100N, Web Vision, and ImageNet-1K Datasets, we demonstrate that our method outperforms the SOTA coreset selection for re-labeling methods (DivideMix and SOP+). We have provided the codebase at URL.},
  archive      = {J_TMLR},
  author       = {Saumyaranjan Mohanty and Konda Reddy Mopuri},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Coreset-driven re-labeling: Tackling noisy annotations with noise-free gradients},
  url          = {https://openreview.net/forum?id=Tk78vb2Qd7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A case for library-level $k$-means binning in histogram gradient-boosted trees. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=UaTrLLspJa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern Gradient Boosted Decision Trees (GBDTs) accelerate split finding with histogram-based binning, which reduces complexity from $O(N\log N)$ to $O(N)$ by aggregating gradients into fixed-size bins. However, the predominant quantile binning strategy—designed to distribute data points evenly among bins—may overlook critical boundary values that could enhance predictive performance. In this work, we consider a novel approach that replaces quantile binning with a $k$-means discretizer initialized with quantile bins, and justify the swap with a proof showing how, for any $L$-Lipschitz function, k-means maximizes the worst-case explained variance of Y obtained when treating all values in a given bin as equivalent. We test this swap against quantile and uniform binning on 33 OpenML datasets plus synthetics that control for modality, skew, and bin budget. Across 18 regression datasets, k-means shows no statistically significant losses at the 5% level and wins in three cases—most strikingly a 55% MSE drop on one particularly skewed dataset—even though k-means' mean reciprocal rank (MRR) is slightly lower (0.65 vs 0.72). On the 15 classification datasets the two methods are statistically tied (MRR 0.70 vs 0.68) with gaps $\leq$0.2 pp. Synthetic experiments confirm consistently large MSE gains—typically $>$20% and rising to 90% as outlier magnitude increases or bin budget drops. We find that k-means keeps error on par with exhaustive (no-binning) splitting when extra cuts add little value, yet still recovers key split points that quantile overlooks. As such, we advocate for a built-in bin_method=$k$-means flag, especially in regression tasks and in tight-budget settings such as the 32–64-bin GPU regime—because it is a "safe default" with large upside, yet adds only a one-off, cacheable overhead ($\approx$ 3.5s per feature to bin 10M rows on one Apple M1 thread).},
  archive      = {J_TMLR},
  author       = {Asher Labovich},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {A case for library-level $k$-means binning in histogram gradient-boosted trees},
  url          = {https://openreview.net/forum?id=UaTrLLspJa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical comparison of membership inference attacks in deep transfer learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=UligTUCgdt'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of powerful large-scale foundation models, the training paradigm is increasingly shifting from from-scratch training to transfer learning. This enables high utility training with small, domain-specific datasets typical in sensitive applications. Membership inference attacks (MIAs) provide an empirical estimate of the privacy leakage by machine learning models. Yet, prior assessments of MIAs against models fine-tuned with transfer learning rely on a small subset of possible attacks. We address this by comparing performance of diverse MIAs in transfer learning settings to help practitioners identify the most efficient attacks for privacy risk evaluation. We find that attack efficacy decreases with the increase in training data for score-based MIAs. We find that there is no one MIA which captures all privacy risks in models trained with transfer learning. While the Likelihood Ratio Attack (LiRA) demonstrates superior performance across most experimental scenarios, the Inverse Hessian Attack (IHA) proves to be more effective against models fine-tuned on PatchCamelyon dataset in high data regime.},
  archive      = {J_TMLR},
  author       = {Yuxuan Bai and Gauri Pradhan and Marlon Tobaben and Antti Honkela},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Empirical comparison of membership inference attacks in deep transfer learning},
  url          = {https://openreview.net/forum?id=UligTUCgdt},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating spatial information into goal-conditioned hierarchical reinforcement learning via graph representations. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=a7Bx4s5gA8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.},
  archive      = {J_TMLR},
  author       = {Shuyuan Zhang and Zihan Wang and Xiao-Wen Chang and Doina Precup},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Incorporating spatial information into goal-conditioned hierarchical reinforcement learning via graph representations},
  url          = {https://openreview.net/forum?id=a7Bx4s5gA8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pref-GUIDE: Continual policy learning from real-time human feedback via preference-based learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=dWGUwidXDm'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training reinforcement learning agents with human feedback is crucial when task objectives are difficult to specify through dense reward functions. While prior methods rely on offline trajectory comparisons to elicit human preferences, such data is unavailable in online learning scenarios where agents must adapt on the fly. Recent approaches address this by collecting real-time scalar feedback to guide agent behavior and train reward models for continued learning after human feedback becomes unavailable. However, scalar feedback is often noisy and inconsistent, limiting the accuracy and generalization of learned rewards. We propose Pref-GUIDE, a framework that transforms real-time scalar feedback into preference-based data to improve reward model learning for continual policy training. Pref-GUIDE Individual mitigates temporal inconsistency by comparing agent behaviors within short windows and filtering ambiguous feedback. Pref-GUIDE Voting further enhances robustness by aggregating reward models across a population of users to form consensus preferences. Across three challenging environments, Pref-GUIDE significantly outperforms scalar-feedback baselines, with the voting variant exceeding even expert-designed dense rewards. By reframing scalar feedback as structured preferences with population feedback, Pref-GUIDE, offers a scalable and principled approach for harnessing human input in online reinforcement learning.},
  archive      = {J_TMLR},
  author       = {Zhengran Ji and Boyuan Chen},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Pref-GUIDE: Continual policy learning from real-time human feedback via preference-based learning},
  url          = {https://openreview.net/forum?id=dWGUwidXDm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decomposed direct preference optimization for structure-based drug design. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=dwSpo5DRk8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have achieved promising results for Structure-Based Drug Design (SBDD). Nevertheless, high-quality protein subpocket and ligand data are relatively scarce, which hinders the models’ generation capabilities. Recently, Direct Preference Optimization (DPO) has emerged as a pivotal tool for aligning generative models with human preferences. In this paper, we propose DecompDpo, a structure-based optimization method aligns diffusion models with pharmaceutical needs using multi-granularity preference pairs. DecompDpo introduces decomposition into the optimization objectives and obtains preference pairs at the molecule or decomposed substructure level based on each objective’s decomposability. Additionally, DecompDpo introduces a physics-informed energy term to ensure reasonable molecular conformations in the optimization results. Notably, DecompDpo can be effectively used for two main purposes: (1) fine-tuning pretrained diffusion models for molecule generation across various protein families, and (2) molecular optimization given a specific protein subpocket after generation. Extensive experiments on the CrossDocked2020 benchmark show that DecompDpo significantly improves model performance, achieving up to 98.5% Med. High Affinity and a 43.9% success rate for molecule generation, and 100% Med. High Affinity and a 52.1% success rate for targeted molecule optimization.},
  archive      = {J_TMLR},
  author       = {Xiwei Cheng and Xiangxin Zhou and Yuwei Yang and Yu Bao and Quanquan Gu},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Decomposed direct preference optimization for structure-based drug design},
  url          = {https://openreview.net/forum?id=dwSpo5DRk8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does unsupervised domain adaptation improve the robustness of amortized bayesian inference? a systematic evaluation. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=ewgLuvnEw6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in silico and practice. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications -- a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA to increase the robustness of ABI.},
  archive      = {J_TMLR},
  author       = {Lasse Elsemüller and Valentin Pratz and Mischa von Krause and Andreas Voss and Paul-Christian Bürkner and Stefan T. Radev},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Does unsupervised domain adaptation improve the robustness of amortized bayesian inference? a systematic evaluation},
  url          = {https://openreview.net/forum?id=ewgLuvnEw6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-NN as a simple and effective estimator of transferability. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=hGlkjP1zHc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How well can one expect transfer learning to work in a new setting where the domain is shifted, the task is different, and the architecture changes? Many transfer learning metrics have been proposed to answer this question. But how accurate are their predictions in a realistic new setting? We conducted an extensive evaluation involving over 42,000 experiments comparing 23 transferability metrics across 16 different datasets to assess their ability to predict transfer performance. Our findings reveal that none of the existing metrics perform well across the board. However, we find that a simple k-nearest neighbor evaluation -- as is commonly used to evaluate feature quality for self-supervision -- not only surpasses existing metrics, but also offers better computational efficiency and ease of implementation.},
  archive      = {J_TMLR},
  author       = {Moein Sorkhei and Christos Matsoukas and Johan Fredin Haslum and Emir Konuk and Kevin Smith},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {K-NN as a simple and effective estimator of transferability},
  url          = {https://openreview.net/forum?id=hGlkjP1zHc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond marginals: Learning joint spatio-temporal patterns for multivariate anomaly detection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=iETTv1okjX'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to improve anomaly detection (AD) by incorporating the time-varying non-linear spatio-temporal correlations of the multi-variate time series data in the modeling process. In multivariate AD, the simultaneous deviation of multiple nodes from their expected behavior can indicate an anomaly, even if no individual node shows a clearly abnormal pattern. In many existing approaches, time series variables are assumed to be (conditionally) independent, which oversimplifies real-world interactions. Our approach addresses this by modeling joint dependencies using a copula-based framework, which decouples the modeling of marginal distributions, temporal dynamics, and inter-variable dependencies. We use a transformer encoder to capture temporal patterns, and to model spatial (inter-variable) dependencies, we integrate a copula. Both components are trained jointly in a latent space using a self-supervised contrastive learning objective to learn meaningful feature representations to separate normal and anomaly samples.},
  archive      = {J_TMLR},
  author       = {Padmaksha Roy and Almuatazbellah Boker and Lamine Mili},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Beyond marginals: Learning joint spatio-temporal patterns for multivariate anomaly detection},
  url          = {https://openreview.net/forum?id=iETTv1okjX},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse-to-sparse training of diffusion models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=iRupdoPLJa'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models (DMs) are a powerful type of generative models that have achieved state-of-the-art results in various image synthesis tasks and have shown potential in other domains, such as natural language processing and temporal data modeling. Despite their stable training dynamics and ability to produce diverse high-quality samples, DMs are notorious for requiring significant computational resources, both in the training and inference stages. Previous work has focused mostly on increasing the efficiency of model inference. This paper introduces, for the first time, the paradigm of sparse-to-sparse training to DMs, with the aim of improving both training and inference efficiency. We focus on unconditional generation and train sparse DMs from scratch (Latent Diffusion and ChiroDiff) on six datasets using three different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of sparsity in model performance. Our experiments show that sparse DMs are able to match and often outperform their Dense counterparts, while substantially reducing the number of trainable parameters and FLOPs. We also identify safe and effective values to perform sparse-to-sparse training of DMs.},
  archive      = {J_TMLR},
  author       = {Inês Cardoso Oliveira and Decebal Constantin Mocanu and Luis A. Leiva},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Sparse-to-sparse training of diffusion models},
  url          = {https://openreview.net/forum?id=iRupdoPLJa},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral clustering and labeling for crowdsourcing with inherently distinct task types. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=jVQjtzcvAc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dawid-Skene model is the most widely assumed model in the analysis of crowdsourcing algorithms that estimate ground-truth labels from noisy worker responses. In this work, we are motivated by crowdsourcing applications where workers have distinct skill sets and their accuracy additionally depends on a task's type. Focusing on the case where there are two types of tasks, we propose a spectral method to partition tasks into two groups such that a worker has the same reliability for all tasks within a group. Our analysis reveals a separability condition such that task types can be perfectly recovered if the number of workers $n$ scales logarithmically with the number of tasks $d$. Numerical experiments show how clustering tasks by type before estimating ground-truth labels enhances the performance of crowdsourcing algorithms in practical applications.},
  archive      = {J_TMLR},
  author       = {Saptarshi Mandal and Seo Taek Kong and Dimitrios Katselis and R. Srikant},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Spectral clustering and labeling for crowdsourcing with inherently distinct task types},
  url          = {https://openreview.net/forum?id=jVQjtzcvAc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Activation sharding for scalable training of large models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=kQCuMcEneq'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite fast progress, efficiently training large language models (LLMs) in extremely long contexts remains challenging. Existing methods fall back to training LLMs with short contexts (up to a few thousand tokens) and use inference time techniques when evaluating on very long contexts (above 1M tokens). Training on very long contexts is limited by GPU memory availability and the prohibitively long training times it requires on state-of-the-art hardware. Meanwhile, many real-life applications require training/fine-tuning with long context on specific tasks. Such applications include, for example, augmenting the context with various sources of raw reference information for extraction, summarization, or fact reconciliation tasks. We propose adjoint sharding, a novel technique that comprises sharding gradient calculation during training to reduce memory requirements by orders of magnitude, making training on very long contexts computationally tractable. At the core of our adjoint sharding algorithm lies the adjoint method, which efficiently computes gradients that are provably equivalent to the gradients computed using standard backpropagation. We also propose truncated adjoint sharding to accelerate the algorithm while maintaining performance. We provide a distributed and a parallel-computing version of adjoint sharding to speed up training and to show that adjoint sharding is compatible with these standard memory-reduction techniques. Empirical results show the proposed adjoint sharding algorithm reduces memory usage by up to 3$\times$ on a large language model with 1.27B parameters on 1M context length training. This reduction in memory usage allows increasing the maximum context length of training a 1.27B parameter model from 35K tokens to above 100K tokens on a training infrastructure composed of five AWS P4 instances.},
  archive      = {J_TMLR},
  author       = {Xingzi Xu and Amir Tavanaei and Kavosh Asadi and Karim Bouyarmane},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Activation sharding for scalable training of large models},
  url          = {https://openreview.net/forum?id=kQCuMcEneq},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable and responsible foundation models. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=nLJZh4M6S5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.},
  archive      = {J_TMLR},
  author       = {Xinyu Yang and Junlin Han and Rishi Bommasani and Jinqi Luo and Wenjie Qu and Wangchunshu Zhou and Adel Bibi and Xiyao Wang and Jaehong Yoon and Elias Stengel-Eskin and Shengbang Tong and Lingfeng Shen and Rafael Rafailov and Runjia Li and Zhaoyang Wang and Yiyang Zhou and Chenhang Cui and Yu Wang and Wenhao Zheng and Huichi Zhou and Jindong Gu and Zhaorun Chen and Peng Xia and Tony Lee and Thomas P Zollo and Vikash Sehwag and Jixuan Leng and Jiuhai Chen and Yuxin Wen and Huan Zhang and Zhun Deng and Linjun Zhang and Pavel Izmailov and Pang Wei Koh and Yulia Tsvetkov and Andrew Gordon Wilson and Jiaheng Zhang and James Zou and Cihang Xie and Hao Wang and Philip Torr and Julian McAuley and David Alvarez-Melis and Florian Tramèr and Kaidi Xu and Suman Jana and Chris Callison-Burch and Rene Vidal and Filippos Kokkinos and Mohit Bansal and Beidi Chen and Huaxiu Yao},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Reliable and responsible foundation models},
  url          = {https://openreview.net/forum?id=nLJZh4M6S5},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlearning misalignment for personalized LLM adaptation via instance-response-dependent discrepancies. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=njE3swFBMc'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Large Language Models (LLMs) have revolutionized chatbot interactions, they often fall short of aligning responses with the nuanced preferences of individual users, a challenge rooted in the inherently subjective and proprietary nature of those preferences. Consequently, prompt-based learning, though effective in enhancing factual accuracy due to its emphasis on universal correctness, remains insufficient for achieving accurate personalised response alignment. Because user preferences vary widely across individuals and contexts, aligning responses requires a more personalized and context-aware approach. To address this limitation, we propose Consistent Marginalization (CM), a novel framework that aims to unlearn misalignment by constructing a personalised memory bank of instance-response-dependent discrepancies, built from a small set of user preference samples. This personalised memory bank equips LLMs with the ability to understand, recall, and adapt to individual preferences, enabling more consistent and personalized responses. Evaluated across a diverse range of domain-specific datasets and model architectures, CM yields notable improvements in response alignment and robustness. We believe Consistent Marginalization represents a valuable step toward enabling LLMs to become genuinely personable and adaptive conversational agents by understanding user preferences and generating responses that are better aligned with individual user expectations.},
  archive      = {J_TMLR},
  author       = {Cheng Chen and Atsushi Nitanda and Ivor Tsang},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Unlearning misalignment for personalized LLM adaptation via instance-response-dependent discrepancies},
  url          = {https://openreview.net/forum?id=njE3swFBMc},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dependency-aware maximum likelihood estimation for active learning. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=qDVDSXXGK1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning aims to efficiently build a labeled training set by strategically selecting samples to query labels from annotators. In this sequential process, each sample acquisition influences subsequent selections, causing dependencies among samples in the labeled set. However, these dependencies are overlooked during the model parameter estimation stage when updating the model using Maximum Likelihood Estimation (MLE), a conventional method that assumes independent and identically distributed (i.i.d.) data. We propose Dependency-aware MLE (DMLE), which corrects MLE within the active learning framework by addressing sample dependencies typically neglected due to the i.i.d. assumption, ensuring consistency with active learning principles in the model parameter estimation process. This improved method achieves superior performance across multiple benchmark datasets, reaching higher performance in earlier cycles compared to conventional MLE. Specifically, we observe average accuracy improvements of 6%, 8.6%, and 10.5% for k=1, k=5, and k=10 respectively, after collecting the first 100 samples, where entropy is the acquisition function and k is the query batch size acquired at every active learning cycle.},
  archive      = {J_TMLR},
  author       = {Beyza Kalkanli and Tales Imbiriba and Stratis Ioannidis and Deniz Erdogmus and Jennifer Dy},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Dependency-aware maximum likelihood estimation for active learning},
  url          = {https://openreview.net/forum?id=qDVDSXXGK1},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing utility and privacy: Dynamically private SGD with random projection. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=u6OSRdkAwl'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic optimization is a pivotal enabler in modern machine learning, producing effective models for various tasks. However, several existing works have shown that model parameters and gradient information are susceptible to privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy concerns, its static noise mechanism impacts the error bounds for model performance. Additionally, with the exponential increase in model parameters, efficient learning of these models using stochastic optimizers has become more challenging. To address these concerns, we introduce the Dynamically Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we combine two important ideas: (i) dynamic differential privacy (DDP) with automatic gradient clipping and (ii) random projection with SGD, allowing dynamic adjustment of the tradeoff between utility and privacy of the model. It exhibits provably sub-linear convergence rates across different objective functions, matching the best available rate. The theoretical analysis further suggests that DDP leads to better utility at the cost of privacy, while random projection enables more efficient model learning. Extensive experiments across diverse datasets show that D2P2-SGD remarkably enhances accuracy while maintaining privacy. Our code is available here.},
  archive      = {J_TMLR},
  author       = {Zhanhong Jiang and Md Zahid Hasan and Nastaran Saadati and Aditya Balu and Chao Liu and Soumik Sarkar},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Balancing utility and privacy: Dynamically private SGD with random projection},
  url          = {https://openreview.net/forum?id=u6OSRdkAwl},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MGPATH: A vision-language model with multi-granular prompt learning for few-shot whole slide pathology classification. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=u7U81JLGjH'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model’s ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this https://github.com/HauschildLab/MGPATH.},
  archive      = {J_TMLR},
  author       = {Anh-Tien Nguyen and Duy Minh Ho Nguyen and Nghiem Tuong Diep and Trung Quoc Nguyen and Nhat Ho and Jacqueline Michelle Metsch and Miriam Cindy Maurer and Daniel Sonntag and Hanibal Bohnenberger and Anne-Christin Hauschild},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {MGPATH: A vision-language model with multi-granular prompt learning for few-shot whole slide pathology classification},
  url          = {https://openreview.net/forum?id=u7U81JLGjH},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HybridFlow: Quantification of aleatoric and epistemic uncertainty with a single hybrid model. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=xRiEdSyVjY'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification is critical for ensuring robustness in high-stakes machine learning applications. We introduce HybridFlow, a modular hybrid architecture that unifies the modeling of aleatoric and epistemic uncertainty by combining a Conditional Masked Autoregressive normalizing flow for estimating aleatoric uncertainty with a flexible probabilistic predictor for epistemic uncertainty. The framework supports integration with any probabilistic model class, allowing users to easily adapt HybridFlow to existing architectures without sacrificing predictive performance. HybridFlow improves upon previous uncertainty quantification frameworks across a range of regression tasks, such as depth estimation, a collection of regression benchmarks, and a scientific case study of ice sheet emulation. We also provide empirical results of the quantified uncertainty, showing that the uncertainty quantified by HybridFlow is calibrated and better aligns with model error than existing methods for quantifying aleatoric and epistemic uncertainty. HybridFlow addresses a key challenge in Bayesian deep learning, unifying aleatoric and epistemic uncertainty modeling in a single robust framework.},
  archive      = {J_TMLR},
  author       = {Peter Van Katwyk and Karianne Bergen},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {HybridFlow: Quantification of aleatoric and epistemic uncertainty with a single hybrid model},
  url          = {https://openreview.net/forum?id=xRiEdSyVjY},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chimera: State space models beyond sequences. <em>TMLR</em>. (<a href='https://openreview.net/forum?id=yv0TUssepk'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based deep learning methods have emerged as the standard approach to model diverse data such as sequences, images, and graphs. These methods rely on self-attention, which treats data as an unordered set of elements. This ignores the neighborhood structure or graph topology of the data and requires the use of inductive biases, such as position embeddings in sequences and images, and random walks in graphs, to incorporate topology. However, developing bespoke inductive biases for each task requires significant effort and can also introduce side-effects hindering generalization. In this work, we introduce Chimera, a unified model that directly incorporates the data topology in a principled way, obviating the need for domain-specific biases. Central to Chimera is the observation that state-space models---which naturally do not require position embeddings---can be generalized to capture any general graph topology. Our experiments demonstrate the versatility of our approach---Chimera achieves strong performance across the domains of language, vision, and graphs, outperforming BERT on GLUE by 0.7 points, ViT on ImageNet-1k by 2.6%, and all the baselines on the Long Range Graph Benchmark. Our results validate Chimera's principled methodological contributions and affirm the long-held belief that data topology is a powerful inductive bias across modalities. We further propose algorithmic optimizations to improve Chimera's efficiency while maintaining performance: 1) For the subclass of Directed Acyclic Graphs we show that Chimera can be implemented as a linear time recurrence. 2) For general graphs, we relax the method with a simple mathematical approximation, achieving Transformer's quadratic complexity without relying on domain-specific biases.},
  archive      = {J_TMLR},
  author       = {Aakash Lahoti and Tanya Marwah and Ratish Puduppully and Albert Gu},
  journal      = {Transactions on Machine Learning Research},
  month        = {10},
  shortjournal = {Trans. Mach. Learn. Res.},
  title        = {Chimera: State space models beyond sequences},
  url          = {https://openreview.net/forum?id=yv0TUssepk},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
