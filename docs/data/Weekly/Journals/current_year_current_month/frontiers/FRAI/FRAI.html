<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FRAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="frai">FRAI - 51</h2>
<ul>
<li><details>
<summary>
(2025). Predicting BRICS NIFTY50 returns using XAI and S.A.F.E AI lens. <em>FRAI</em>, <em>8</em>, 1668700. (<a href='https://doi.org/10.3389/frai.2025.1668700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PurposeGlobal fund managers, in their effort toward risk diversification and generating higher returns, design portfolios that consist of financial assets of various countries. In the process, they expose their investors not only to the fundamentals of the assets but also to transnational volatility, macroeconomic shocks of different countries, and exchange rate fluctuations. These factors make forecasting returns from such global funds quite difficult and, at the same time, challenging. To aid global fund managers and investors, this study presents a forecasting framework for predicting returns from Goldman Sachs BRICs Nifty 50 Developed Markets Index (BRICS NIFTY 50), which is a traded and listed financial asset. It is a global portfolio, which not only exposes investors to the fundamentals of different companies but also to country risk.Design, methodology, and approachGradient boosting regression (GBR) and SHAP-based XAI are used to identify the top significant country-specific explanatory variables. Subsequently, with the selected variables, GBR, CatBoost, Light Gradient Boosting Machine (LGBM), Extreme Gradient Boosting (XGBoost), Random Forest (RF), and Extra Tree Regressor (ETR) are applied for forecasting returns from BRICS NIFTY 50. Along with standard evaluation tools, the S.A.F.E AI framework is used for measuring predictive accuracy, sustainability, and contribution of each predictor. To evaluate the relative efficacy of the six predictive models, the underlying research resorts to a multi-criteria decision-making (MCDM) framework.FindingsWe find that country-specific market volatility, industrial performance, financial sector development, and exchange rate fluctuations explain global returns significantly. Furthermore, the exercise also reveals that explanatory factors specific to India, China, and Brazil emerge to be relatively important.Research limitations and implicationsThe study focuses on a single index. Future work will extend it to other indices and global funds.Practical implicationsThe proposed methodology will be of practical use to global fund managers and investors. Policymakers may find it useful for identifying factors that make foreign direct investment and portfolio investment attractive.Originality and valueDevelopment of a two-step forecasting framework, identifying effects of country-specific explanatory variables, and applying different evaluation criteria to measure predictive efficiency underscore the novelty of the work.},
  archive      = {J_FRAI},
  author       = {Ghosh, Indranil and Datta Chaudhuri, Tamal and Babaei, Golnoosh and Giudici, Paolo and Raffinetti, Emanuela},
  doi          = {10.3389/frai.2025.1668700},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1668700},
  shortjournal = {Front. Artif. Intell.},
  title        = {Predicting BRICS NIFTY50 returns using XAI and S.A.F.E AI lens},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive noise-augmented attention for enhancing transformer fine-tuning on longitudinal medical data. <em>FRAI</em>, <em>8</em>, 1663484. (<a href='https://doi.org/10.3389/frai.2025.1663484'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer models pre-trained on self-supervised tasks and fine-tuned on downstream objectives have achieved remarkable results across a variety of domains. However, fine-tuning these models for clinical predictions from longitudinal medical data, such as electronic health records (EHR), remains challenging due to limited labeled data and the complex, event-driven nature of medical sequences. While self-attention mechanisms are powerful for capturing relationships within sequences, they may underperform when modeling subtle dependencies between sparse clinical events under limited supervision. We introduce a simple yet effective fine-tuning technique, Adaptive Noise-Augmented Attention (ANAA), which injects adaptive noise directly into the self-attention weights and applies a 2D Gaussian kernel to smooth the resulting attention maps. This mechanism broadens the attention distribution across tokens while refining it to emphasize more informative events. Unlike prior approaches that require expensive modifications to the architecture and pre-training phase, ANAA operates entirely during fine-tuning. Empirical results across multiple clinical prediction tasks demonstrate consistent performance improvements. Furthermore, we analyze how ANAA shapes the learned attention behavior, offering interpretable insights into the model's handling of temporal dependencies in EHR data.},
  archive      = {J_FRAI},
  author       = {Amirahmadi, Ali and Etminani, Farzaneh and Ohlsson, Mattias},
  doi          = {10.3389/frai.2025.1663484},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1663484},
  shortjournal = {Front. Artif. Intell.},
  title        = {Adaptive noise-augmented attention for enhancing transformer fine-tuning on longitudinal medical data},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting offer burden to optimize batch sizes in simultaneously expiring kidney offers. <em>FRAI</em>, <em>8</em>, 1662960. (<a href='https://doi.org/10.3389/frai.2025.1662960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundTimely and efficient allocation of deceased donor kidneys is a persistent challenge in transplantation. Traditional sequential offer systems often lead to extended delays and high nonuse rates, as many kidneys undergo multiple refusals before being accepted. Simultaneously expiring offers, where a kidney is offered to a batch of centers with synchronized response deadlines, offer a more efficient alternative. However, fixed batch sizes fail to account for variability in offer requirements, potentially introducing new inefficiencies or overwhelming transplant professionals with excessive notifications.MethodsWe investigated the use of machine learning-based survival models to dynamically predict the number of offers a kidney will require before acceptance. Utilizing data on over 16,000 deceased donor kidneys from the national organ offer dataset, we engineered predictive features from both donor profiles and recipient pool characteristics. We trained and evaluated multiple survival models using time-dependent concordance indices along with other survival and regression performance metrics.ResultsThe Random Survival Forest model achieved the best performance, with a time-dependent C-index of 0.882, effectively estimating the required offer volume for kidney placement. Feature importance analysis revealed that waitlist characteristics, such as mean Estimated Post-Transplant Survival (EPTS), mean Calculated Panel Reactive Antibody (CPRA), time on dialysis, and waitlist duration, were among the most influential predictors. When integrated into a dynamic simultaneous offer system, these predictions have the potential to reduce average placement delays from 17.37 h to 1.59 h while maintaining a manageable level of extraneous offers.DiscussionOur results demonstrate that survival-based predictive modeling can meaningfully improve the efficiency of simultaneously expiring offers in kidney allocation. By personalizing batch sizes based on expected offer burden, such models can reduce delays without overwhelming transplant professionals. These findings underscore the value of integrating real-time, data-driven tools into organ allocation systems to improve operational efficiency and facilitate practical implementation.},
  archive      = {J_FRAI},
  author       = {Berry, Sean and Görgülü, Berk and Tunç, Sait and Cevik, Mucahit},
  doi          = {10.3389/frai.2025.1662960},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1662960},
  shortjournal = {Front. Artif. Intell.},
  title        = {Predicting offer burden to optimize batch sizes in simultaneously expiring kidney offers},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using ChatGPT as an assessment tool for medical residents in mexico: A descriptive experience. <em>FRAI</em>, <em>8</em>, 1662203. (<a href='https://doi.org/10.3389/frai.2025.1662203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionArtificial intelligence (AI) in medical education has progressed gradually, with numerous authors debating whether to prohibit, restrict, or adopt its use in academic contexts. Growing evidence exists regarding the capabilities and applications of AI in this field, particularly in supporting educational tasks such as student assessment. In this article we described our experience using ChatGPT to evaluate medical residents.Materials and methodsA descriptive cross-sectional study was conducted involving 35 medical residents from different specialty’s at a secondary-level hospital. Two different exams were generated using ChatGPT in topics of Rocky Mountain Spotted Fever (RMSF) and Pertussis. Additionally, an opinion survey—previously validated was administered to assess participants’ perceptions of ChatGPT ability to generate multiple-choice questions.ResultsOverall average score for the Pertussis examination was 8.46, while the average for the RMSF examination was 8.29. All participants reported that the examination was well written and that the language used was coherent; 34 residents (97.14%) stated that the language was clear, concise, and easy to understand; 9 residents (25.71%) agreed that the language used was confusing; 33 residents (94.28%) rated the exams questions as difficult; 32 residents (91.42%) felt that they had adequately prepared for both examinations.DiscussionChatGPT exhibits a promising faculty as a tool to support teaching activities in the training of medical specialists, mainly in reducing the human workload of healthcare personnel, and becoming integral to the next phase of medical education through AI-assisted content creation supervised by educators.},
  archive      = {J_FRAI},
  author       = {Rivera-Rosas, Cristian N. and Calleja-López, J. R. Tadeo and Larios-Camacho, Sandra J. and Trujillo-López, Sergio},
  doi          = {10.3389/frai.2025.1662203},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1662203},
  shortjournal = {Front. Artif. Intell.},
  title        = {Using ChatGPT as an assessment tool for medical residents in mexico: A descriptive experience},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-dialectal arabic translation: Comparative analysis on large language models. <em>FRAI</em>, <em>8</em>, 1661789. (<a href='https://doi.org/10.3389/frai.2025.1661789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionExploring Arabic dialects in Natural Language Processing (NLP) is essential to understand linguistic variation and meet regional communication demands. Recent advances in Large Language Models (LLMs) have opened up new vistas for multilingual communication and text generation.MethodsThis paper investigates the performance of GPT-3.5, GPT-4, and Bard (Gemini) on the QADI and MADAR datasets, while GPT-5 was evaluated exclusively on MADAR encompassing over 15 different countries. Several metrics have been used in the evaluation, such as cosine similarity, universal similarity encoder, sentence BERT, TER, ROUGE, and BLEU. In this study, different prompting techniques were used: zero-shot and few-shot. Zero-shot was employed for all dialects, and few-shot was employed only for the least translation performance dialect, Tunisian.ResultsAnalysis revealed that in the QADI dataset, GPT-4 significantly outperformed others in translating MSA to DA, with ANOVA tests showing strong significance (p < 0.05) in most metrics, except for BLEU and TER where it does not show significance, indicating comparable translation performance among models. Furthermore, GPT-4 was highest in semantic similarity compared to GPT-3.5 and Bard (Gemini), 0.66, 0.61, and 0.63, respectively. GPT-4 was the best in identifying overlapping sentences (i.e., those where the source and target are identical) with a combined average of 0.41 in BLEU and ROUGE-L. All LLMs scored TER values between 6% and 25%, indicating generally good translation quality. However, GPT models, especially GPT-5, responded better to prompting and translation to Levant countries compared to Bard (Gemini). For the MADAR dataset, no significant translation differences were observed in sentence-BERT, ROUGE-L, and TER, while differences are identified in cosine similarity, BLEU, and universal similarity encoder metrics. Therefore, GPT-5 is the top performer in identifying sentence overlaps measured by BLEU and ROUGE-L (combined average 0.37).DiscussionThe few-shot approach did not show a significant improvement in translation performance, especially for GPT-4 and Bard (Gemini), while GPT-3.5 performed consistently. Zero-shot prompts were effective across dialects, while few-shot prompting, applied to the weakest-performing dialect (Tunisian), did not yield improvement. GPT-4 and Bard performed worse under this set-up, while GPT-3.5 remained consistent.},
  archive      = {J_FRAI},
  author       = {Beidas, Ayah and Mohi, Kousar and Ghaddar, Fatme and Ahmad, Imtiaz and Abed, Sa'Ed},
  doi          = {10.3389/frai.2025.1661789},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1661789},
  shortjournal = {Front. Artif. Intell.},
  title        = {Cross-dialectal arabic translation: Comparative analysis on large language models},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in traditional medicine: Evidence, barriers, and a research roadmap for personalized care. <em>FRAI</em>, <em>8</em>, 1659338. (<a href='https://doi.org/10.3389/frai.2025.1659338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundTraditional medicine (TM) systems such as Ayurveda, Traditional Chinese Medicine (TCM), and Thai Traditional Medicine (TTM) are increasingly intersecting with artificial intelligence (AI).ObjectiveTo synthesize how AI is currently applied to TM and to outline barriers and research needs for safe, equitable, and scalable adoption.MethodsWe conducted a targeted narrative mini review of peer reviewed studies (2017–Aug 2025) retrieved from PubMed, Scopus, and Google Scholar using terms spanning TM (Ayurveda/TCM/TTM) and AI (machine learning (ML), natural language processing (NLP), computer vision, telemedicine. Inclusion favored studies with reported methods and, when available, performance metrics; commentary and preprints without data were excluded.FindingsCurrent evidence supports AI assisted diagnostic pattern recognition, personalization frameworks integrating multi source data, digital preservation of TM knowledge, telemedicine enablement, and AI supported herbal pharmacology and safety assessment. Reported performance varies and is context dependent, with limited prospective external validation.LimitationsEvidence heterogeneity, small datasets, inconsistent ontologies across TM systems, and nascent regulatory pathways constrain real world deployment.ConclusionAI can augment TM education, research, and clinical services, but progress requires standards, culturally informed datasets, prospective trials, and clear governance. We propose a research roadmap to guide rigorous and ethical integration.},
  archive      = {J_FRAI},
  author       = {Jongjiamdee, Ketmanee and Pornwonglert, Pimnipa and Na Bangchang, Nutnichar and Akarasereenont, Pravit},
  doi          = {10.3389/frai.2025.1659338},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1659338},
  shortjournal = {Front. Artif. Intell.},
  title        = {Artificial intelligence in traditional medicine: Evidence, barriers, and a research roadmap for personalized care},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accuracy of AI chatbots in answering frequently asked questions on cervical cancer. <em>FRAI</em>, <em>8</em>, 1655303. (<a href='https://doi.org/10.3389/frai.2025.1655303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ObjectiveTo compare the accuracy of Deepseek and ChatGPT in answering frequently asked questions (FAQs) about cervical cancer.MethodsTo compile a list of FAQs concerning cervical cancer, a comprehensive search was conducted on social media and community platforms. The answer keys for all the selected questions were created on the basis of the guidelines of the National Comprehensive Cancer Network (NCCN), the International Federation of Gynecology and Obstetrics (FIGO), and the World Health Organization (WHO) for cervical cancer. The answers given by Deepseek-R1 and ChatGPT O1 were scored according to the Global Quality Score (GQS).ResultsA total of 74 FAQs covered a diverse range of topics related to cervical cancer, including diagnosis (n = 16), risk factors and epidemiology (n = 19), treatment (n = 20), and prevention (n = 19). When all the answers provided by DeepSeek to the FAQs about cervical cancer according to the GQS were evaluated, 68 answers were rated as score five, 4 answers were rated as score four, and 2 answers were rated as score three. For ChatGPT’s responses to the same set of FAQs, 67 answers were classified as score five, 6 answers were classified as score four, and 1 answer was classified as score three. There was no statistically significant difference between the two groups (p > 0.05).ConclusionBoth DeepSeek and ChatGPT demonstrated accurate and satisfactory responses to FAQs about cervical cancer when evaluated according to the GQS. However, in regard to treatment issues, a cautious attitude should be maintained. Compared to ChatGPT, DeepSeek stands out for its free availability, which makes it more accessible in resource-limited scenarios to the public.},
  archive      = {J_FRAI},
  author       = {Fan, Jielin and Xiao, Wenhong and Yan, Zhipeng and Ouyang, Qiang},
  doi          = {10.3389/frai.2025.1655303},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1655303},
  shortjournal = {Front. Artif. Intell.},
  title        = {Accuracy of AI chatbots in answering frequently asked questions on cervical cancer},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging pre-trained embeddings in an ensemble machine learning approach for arabic sentiment analysis. <em>FRAI</em>, <em>8</em>, 1653728. (<a href='https://doi.org/10.3389/frai.2025.1653728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionArabic sentiment analysis presents unique challenges due to the linguistic complexity of the language, including its wide range of dialects, orthographic ambiguity, and limited language resources. Addressing these issues is essential to develop robust sentiment classification systems.MethodsThis study investigates the application of ensemble machine learning methods for Arabic sentiment analysis. Several homogeneous ensemble techniques are implemented and evaluated on two datasets: the balanced ArTwitter dataset and the highly imbalanced Syria_Tweets dataset. To mitigate class imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) is employed. The models incorporate pre-trained word embeddings and unigram features.ResultsExperimental results indicate that individual classifiers using pre-trained embeddings achieve strong performance; however, ensemble models consistently yield superior outcomes. On the ArTwitter dataset, the ensemble of Naive Bayes, Support Vector Machine, and Decision Tree classifiers achieved an accuracy of 90.22% and an F1-score of 92.0%. On the Syria_Tweets dataset, an ensemble combining Stochastic Gradient Descent, k-Nearest Neighbors, and Random Forest attained 83.82% accuracy and an 83.86% F1-score.DiscussionThe findings highlight the effectiveness of ensemble learning in enhancing the robustness and generalizability of Arabic sentiment analysis systems. Incorporating pre-trained embeddings further strengthens performance, demonstrating that ensemble-based approaches can overcome challenges posed by linguistic complexity and dataset imbalance in Arabic natural language processing tasks.},
  archive      = {J_FRAI},
  author       = {Jaber, Areej and Bahati, Israa and Martínez, Paloma},
  doi          = {10.3389/frai.2025.1653728},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1653728},
  shortjournal = {Front. Artif. Intell.},
  title        = {Leveraging pre-trained embeddings in an ensemble machine learning approach for arabic sentiment analysis},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entropy-adaptive differential privacy federated learning for student performance prediction and privacy protection: A case study in python programming. <em>FRAI</em>, <em>8</em>, 1653437. (<a href='https://doi.org/10.3389/frai.2025.1653437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the digital transformation of engineering education, protecting student data privacy has become a key challenge for enabling data-driven instruction. This study proposes an Entropy-Adaptive Differential Privacy Federated Learning method (EADP-FedAvg) to enhance the accuracy of student performance prediction while ensuring data privacy. Based on online test records from Python programming courses for Electronic Engineering students (grade 2021–2023) at the School of Physics and Optoelectronic Technology, Baoji University of Arts and Sciences, China, the study uses a Multilayer Perceptron (MLP) model and 10 distributed clients for training. Under different privacy budgets (ε = 0.1, 1e-6, and 1.0), EADP-FedAvg achieves a test accuracy of 92.7%, macro-average score of 92.1%, and entropy of 0.207, outperforming standard federated learning and approaching centralized learning performance. The results demonstrate that by adaptively adjusting the noise level based on output entropy, EADP-FedAvg effectively balances privacy preservation and model accuracy. This method offers a novel solution for analyzing privacy-sensitive educational data in engineering education.},
  archive      = {J_FRAI},
  author       = {Chen, Shanwei and Qi, Xiuzhi},
  doi          = {10.3389/frai.2025.1653437},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1653437},
  shortjournal = {Front. Artif. Intell.},
  title        = {Entropy-adaptive differential privacy federated learning for student performance prediction and privacy protection: A case study in python programming},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting pediatric diagnostic imaging patient no-show and extended wait-times using LLMs, regression, and tree based models. <em>FRAI</em>, <em>8</em>, 1652397. (<a href='https://doi.org/10.3389/frai.2025.1652397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionPatients missing their appointments (no-shows) are a persistent issue that results in idle resources while delaying critical patient prognosis. Likewise, long waiting times increase frustration for patients, leaving a negative impression on the appointment. In this paper, we explore 3 modalities of diagnostic and interventional radiology appointments for pediatric patients at the Hospital for Sick Children (SickKids), Toronto, ON, Canada. Our goal was to survey machine learning methods that best predict the risk of patient no-shows and long wait-times exceeding 1 hour for scheduling teams to propose targeted downstream accommodations.MethodsWe experimented with 6 predictive model types separately trained on both tasks which included extreme gradient boosting (XGBoost), Random Forest (RF), Support Vector Machine, Logistic Regression, Artificial Neural Network, and a pre-trained large language model (LLM). Utilizing 20 features containing a mixture of patient demographics and appointment related data, we experimented with different data balancing methods including instance hardness threshold (IHT) and class weighting to reduce bias in prediction. We then conducted a comparative study of the improvements made by utilizing continuous contextual data in our LLM which boasted a 51% improvement in F1 score for the wait-time model.ResultsOur XGBoost model had the best combination of AUC and F1 scores (0.96 and 0.62, respectively) for predicting no-show while RF had the best AUC and F1 scores (0.83 and 0.61, respectively) for wait-time prediction. The LLMs also performed well for 90% probability thresholds (high risk patients) while being robustly calibrated on unseen test data.DiscussionOur results surveyed multiple algorithms and data balancing methods to propose the greatest performing models on our tasks, implemented a unique methodology to use LLMs on heterogeneous data within this domain, and demonstrated the greater importance of contextual appointment data over patient demographic features for a more equitable prediction algorithm. Going forward, the predictive output (calibrated probabilities of events) can be used as stochastic input for risk-based optimized scheduling to provide accommodation for patients less likely to receive quality access to healthcare.},
  archive      = {J_FRAI},
  author       = {Rafique, Daniel and Liu, Xuan and Gong, Bo and Belsito, Laura and McCradden, Melissa D. and Mazwi, Mjaye L. and Lee, Wayne and Ohanlon, Graham and Tsang, Kyle and Shroff, Manohar and Ertl-Wagner, Birgit and Khalvati, Farzad},
  doi          = {10.3389/frai.2025.1652397},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1652397},
  shortjournal = {Front. Artif. Intell.},
  title        = {Predicting pediatric diagnostic imaging patient no-show and extended wait-times using LLMs, regression, and tree based models},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on intelligent matching of students’ learning ability and healthcare job market demand based on industrial engineering expertise graph. <em>FRAI</em>, <em>8</em>, 1650095. (<a href='https://doi.org/10.3389/frai.2025.1650095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In China, there is a structural mismatch between the job market and student employment, characterized by “unfilled jobs” and “unqualified candidates,” particularly between the industrial engineering (IE) profession and the healthcare services sector. Expertise graphs are designed to identify the logical connections between academic disciplines and job market needs, linking students’ knowledge and skills with job requirements. This approach provides a systematic and visual alignment between students’ learning outcomes and job market demands, addressing the mismatch. However, current expertise graphs have not effectively captured the intrinsic connection between students’ learning abilities and healthcare job market demands. Additionally, research on intelligent matching and the construction of knowledge graphs for IE remains limited. This study aims to bridge this gap and alleviate the structural mismatch between the healthcare job market and student employment in China. First, an expertise graph for IE is developed, covering both expertise and healthcare job requirements. A multi-layer fusion information extraction model, combining BERT, BiLSTM, and GCN, is then proposed for knowledge extraction. An employment matching algorithm is introduced to extract healthcare job titles and requirements from the knowledge graph, calculate similarity with students’ overall ability scores, and recommend suitable positions. Finally, a case study demonstrates that the algorithm accurately analyzes students’ ability scores and successfully matches IE majors with relevant healthcare job positions, validating its effectiveness. This study aims to mitigate the structural mismatch between the healthcare job market and student employment, providing high-quality IE talent to medical services, which has significant scientific and practical value.},
  archive      = {J_FRAI},
  author       = {Xiao, Yan and Zeng, Lingtao and Yang, Jie and Wang, Mini Han and Lin, Zhiyuan and Li, Wei},
  doi          = {10.3389/frai.2025.1650095},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1650095},
  shortjournal = {Front. Artif. Intell.},
  title        = {Research on intelligent matching of students’ learning ability and healthcare job market demand based on industrial engineering expertise graph},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ethical prompting: Toward strategies for rapid and inclusive assistance in dual-use AI systems. <em>FRAI</em>, <em>8</em>, 1646444. (<a href='https://doi.org/10.3389/frai.2025.1646444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring technologies initially developed for individuals with disabilities carry inherent dual-use risks, especially evident in conflict or emergency scenarios. This article examines the dual-use dilemma posed by technologies whose civilian design objectives can unintentionally facilitate harmful applications in defense contexts. Specifically, we analyze the ethical risks associated with using civilian-generated data and systems, originally intended to enhance care and assistance, for military purposes without adequate safeguards. We argue that effective and ethically sound technological infrastructures require optimized and ethically-informed prompting strategies. These strategies must clearly define how data and system prompts are structured, reducing deployment biases, particularly against vulnerable populations.},
  archive      = {J_FRAI},
  author       = {Farnós, Joan and Sans Pinillos, Alger and Costa, Vicent},
  doi          = {10.3389/frai.2025.1646444},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1646444},
  shortjournal = {Front. Artif. Intell.},
  title        = {Ethical prompting: Toward strategies for rapid and inclusive assistance in dual-use AI systems},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weight-aware semi-supervised self-ensembling framework for interior decoration style classification. <em>FRAI</em>, <em>8</em>, 1645877. (<a href='https://doi.org/10.3389/frai.2025.1645877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic classification of interior decoration styles has great potential to guide and streamline the design process. Despite recent advancements, it remains challenging to construct an accurate interior decoration style recognition model due to the scarcity of expert annotations. In this article, we develop a new weight-aware semi-supervised self-ensembling framework for interior decoration style recognition, which selectively leverages the abundant unlabeled data to address the aforementioned challenge. Specifically, we devise a weight module that utilizes a truncated Gaussian function to automatically assess the reliability of unlabeled data. This enables more reliable unlabeled samples to be adaptively assigned higher weights during the training process. By incorporating adaptive weights, we devise a weighted consistency regularization to enforce consistent predictions for reliable unlabeled data under different perturbations. Furthermore, we devise a weighted relation consistency regularization to preserve the semantic relationships of reliable unlabeled data across various perturbations. Additionally, we introduce a weighted class-aware contrastive learning regularization to improve the model's discriminative feature learning capability using reliable unlabeled data. The synergistic learning of weighted consistency regularization, weighted relation consistency, and weighted class-aware contrastive learning significantly enhances the model's generalizability. Extensive experiments conducted on interior decoration style image datasets demonstrate the superior performance of our framework compared to existing semi-supervised learning methods.},
  archive      = {J_FRAI},
  author       = {Guo, Lichun and Zeng, Hao and Wang, Junliang and Liang, Shuang and Hang, Wenlong},
  doi          = {10.3389/frai.2025.1645877},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1645877},
  shortjournal = {Front. Artif. Intell.},
  title        = {Weight-aware semi-supervised self-ensembling framework for interior decoration style classification},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI for scientific integrity: Detecting ethical breaches, errors, and misconduct in manuscripts. <em>FRAI</em>, <em>8</em>, 1644098. (<a href='https://doi.org/10.3389/frai.2025.1644098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Generative AI (GenAI) in scientific writing has grown rapidly, offering tools for manuscript drafting, literature summarization, and data analysis. However, these benefits are accompanied by risks, including undisclosed AI authorship, manipulated content, and the emergence of papermills. This perspective examines two key strategies for maintaining research integrity in the GenAI era: (1) detecting unethical or inappropriate use of GenAI in scientific manuscripts and (2) using AI tools to identify mistakes in scientific literature, such as statistical errors, image manipulation, and incorrect citations. We reviewed the capabilities and limitations of existing AI detectors designed to differentiate human-written (HWT) from machine-generated text (MGT), highlighting performance gaps, genre sensitivity, and vulnerability to adversarial attacks. We also investigate emerging AI-powered systems aimed at identifying errors in published research, including tools for statistical verification, citation validation, and image manipulation detection. Additionally, we discuss recent publishing industry initiatives to AI-driven papermills. Our investigation shows that these developments are not yet sufficiently accurate or reliable yet for use in academic assessment, they mark an early but promising steps toward scalable, AI-assisted quality control in scholarly publishing.},
  archive      = {J_FRAI},
  author       = {Pellegrina, Diogo and Helmy, Mohamed},
  doi          = {10.3389/frai.2025.1644098},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1644098},
  shortjournal = {Front. Artif. Intell.},
  title        = {AI for scientific integrity: Detecting ethical breaches, errors, and misconduct in manuscripts},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-, linguistic-, and information-preserving synthesis of clinical documentation through generative agents. <em>FRAI</em>, <em>8</em>, 1644084. (<a href='https://doi.org/10.3389/frai.2025.1644084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of generative agents (GAs) is reshaping the healthcare landscape. Nonetheless, broad utilization is impeded by restricted access to high-quality, interoperable clinical documentation from electronic health records (EHRs) due to persistent legal, ethical, and technical barriers. Synthetic health data generation (SHDG), leveraging pre-trained large language models (LLMs) instantiated as GAs, could offer a practical solution by creating synthetic patient information that mimics genuine EHRs. The use of LLMs, however, is not without issues; significant concerns remain regarding privacy, potential bias propagation, the risk of generating inaccurate or misleading content, and the lack of transparency in how these models make decisions. We therefore propose a privacy-, linguistic-, and information-preserving SHDG protocol that employs multiple context-aware, role-specific GAs. Guided by targeted prompting and authentic EHRs—serving as structural and linguistic templates—role-specific GAs can, in principle, operate collaboratively through multi-turn interactions. We theorized that utilizing GAs in this fashion permits LLMs not only to produce synthetic EHRs that are accurate, consistent, and contextually appropriate, but also to expose the underlying decision-making process. To test this hypothesis, we developed a no-code GA-driven SHDG workflow as a proof of concept, which was implemented within a predefined, multi-layered data science infrastructure (DSI) stack—an integrated ensemble of software and hardware designed to support rapid prototyping and deployment. The DSI stack streamlines implementation for healthcare professionals, improving accessibility, usability, and cybersecurity. To deploy and validate GA-assisted workflows, we implemented a fully automated SHDG evaluation framework—co-developed with GenAI technology—which holistically compares the informational and linguistic features of synthetic, anonymized, and real EHRs at both the document and corpus levels. Our findings highlight that SHDG implemented through GAs offers a scalable, transparent, and reproducible methodology for unlocking the potential of clinical documentation to drive innovation, accelerate research, and advance the development of learning health systems. The source code, synthetic datasets, toolchains and prompts created for this study can be accessed at the GitHub repository: https://github.com/HR-DataLab-Healthcare/RESEARCH_SUPPORT/tree/main/PROJECTS/Generative_Agent_based_Data-Synthesis.},
  archive      = {J_FRAI},
  author       = {van Velzen, Mark and van der Willigen, Robert F. and de Beer, Vincent J. and de Graaf-Waar, Helen I. and Janssen, Esther R. C. and van Leeuwen, Sjemaine and van der Willigen, Micha F. and van der Willigen, Martijn J. and Renardus, Gavin and El Maaroufi, Rayan and Satimin, Sven J. and Hartog, Larissa M. and Hulsen, Tim and van Meeteren, Nico L. U. and Scheper, Mark C.},
  doi          = {10.3389/frai.2025.1644084},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1644084},
  shortjournal = {Front. Artif. Intell.},
  title        = {Privacy-, linguistic-, and information-preserving synthesis of clinical documentation through generative agents},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating LLMs on kazakhstan's mathematics exam for university admission. <em>FRAI</em>, <em>8</em>, 1642570. (<a href='https://doi.org/10.3389/frai.2025.1642570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe rapid advancement of large language models (LLMs) has prompted their exploration in educational contexts, particularly in high-stakes standardized tests such as Kazakhstan's Unified National Testing (UNT) mathematics component, which is critical for university admission. While most existing benchmarks for mathematical reasoning focus on English, concerns remain that LLMs may underperform in under-resourced or non-English languages. This study addresses this gap by evaluating LLM performance on a math test administered entirely in Russian.MethodsWe assessed six LLMs-Claude, DeepSeek, Gemini, Llama, Qwen, and o-on UNT multiple-choice mathematics questions covering algebra, functions, geometry, inequalities, and trigonometry. Three evaluation conditions were employed: (1) zero-shot performance, (2) hybrid integration with SymPy for symbolic computation, and (3) a role-specific simulated multi-agent refinement framework that builds on existing self-correction techniques with targeted feedback.ResultsIn zero-shot settings, DeepSeek, Gemini, Qwen, and o achieved near-perfect or perfect accuracy (X-Y%) across all difficulty levels and topics, while Claude and Llama lagged (A-B%). The hybrid approach significantly improved Claude and Llama's accuracy by C% and D%, respectively. Under the multi-agent refinement condition, Claude showed substantial gains, reaching E% accuracy, which represented a F% improvement over zero-shot performance.DiscussionThese findings provide important empirical evidence that LLMs can perform competitively on mathematics tasks in non-English languages. The results challenge prior assumptions about limited performance in under-resourced linguistic settings and highlight the potential of LLMs to support bilingual education and promote equitable access to higher education.},
  archive      = {J_FRAI},
  author       = {Kadyrov, Shirali and Abdrasilov, Bolatbek and Sabyrov, Aslan and Baizhanov, Nurseit and Makhmutova, Alfira and Kyllonen, Patrick C.},
  doi          = {10.3389/frai.2025.1642570},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1642570},
  shortjournal = {Front. Artif. Intell.},
  title        = {Evaluating LLMs on kazakhstan's mathematics exam for university admission},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing training of time series diffusion models via similarity score functions: Application to cyclic and acyclic motion with IMU data. <em>FRAI</em>, <em>8</em>, 1640948. (<a href='https://doi.org/10.3389/frai.2025.1640948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionDenoising diffusion probabilistic models have shown the capability to generate synthetic sensor signals. These models rely on a loss function that measures the difference between the noise added during the forward process and the noise predicted by the diffusion model, thereby enabling realistic data generation. However, the stochastic nature of the process and the loss function complicate the estimation of data quality.MethodsTo address this issue, we evaluated multiple similarity metrics and adapted an existing metric to monitor both the training and data synthesis processes. The adapted metric was further fine-tuned on the input data to align with the requirements of a downstream classification task.ResultsBy incorporating the adapted metric, we significantly reduced the number of training epochs required without observing performance degradation in the classification task.DiscussionOur findings demonstrate that optimizing the training process using similarity metrics not only conserves computational resources but also shortens the training time for generative models, making them more efficient and practical for real-world applications.},
  archive      = {J_FRAI},
  author       = {Oppel, Heiko and Spilz, Andreas and Munz, Michael},
  doi          = {10.3389/frai.2025.1640948},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1640948},
  shortjournal = {Front. Artif. Intell.},
  title        = {Optimizing training of time series diffusion models via similarity score functions: Application to cyclic and acyclic motion with IMU data},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Profiling investor behavior in the malaysian derivatives market using K-means clustering. <em>FRAI</em>, <em>8</em>, 1640776. (<a href='https://doi.org/10.3389/frai.2025.1640776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the trading behaviors of Malaysian derivatives traders using a comprehensive dataset from Bursa Malaysia with K-means clustering, representing one of the first AI applications to derivatives market segmentation. The analysis encompassed over 11 million trade records for FCPO and FKLI derivatives from January to December 2022. Six key features were engineered to segment derivative traders: Total Number of Trades, Total Traded Amount, Overall Realized Profit, Average ROI, Maximum Account Vintage (trader experience in years), and Median Holding Days (typical position duration). Inverse Hyperbolic Sine transformation was applied to address extreme outliers, ensuring robust feature scaling. K-means clustering identified five distinct profiles: “High-Frequency, High-Risk Derivative Traders with Consistent Losses,” “Conservative, Steady-Growth Derivative Trader,” “High-Frequency, High-Yield Derivative Traders,” “Conservative, Low-Yield Derivative Traders,” and “Cautious, Low-Activity Novice Derivative Traders.” Decision tree classifiers validated these clusters through interpretable splitting conditions. These profiles enable targeted risk management strategies, personalized trading services, and evidence-based regulatory policies for derivatives markets and future research.},
  archive      = {J_FRAI},
  author       = {Tan, Eng Hao Louis and Hamed, Yaman and Daud, Hanita and Abdul Wahab, Mohd Amirul Faiz and Azhar, Ahmad Amirul Adlan and Tan, Sieow Yeek},
  doi          = {10.3389/frai.2025.1640776},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1640776},
  shortjournal = {Front. Artif. Intell.},
  title        = {Profiling investor behavior in the malaysian derivatives market using K-means clustering},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming cataract care through artificial intelligence: An evaluation of large language models’ performance in addressing cataract-related queries. <em>FRAI</em>, <em>8</em>, 1639221. (<a href='https://doi.org/10.3389/frai.2025.1639221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PurposeTo evaluate the performance of five popular large language models (LLMs) in addressing cataract-related queries.MethodsThis comparative evaluation study was conducted at the Eye and ENT Hospital of Fudan University. We performed both qualitative and quantitative assessments of responses from five LLMs: ChatGPT-4, ChatGPT-4o, Gemini, Copilot, and the open-source Llama 3.5. Model outputs were benchmarked against human-generated responses using seven key metrics: accuracy, completeness, conciseness, harmlessness, readability, stability, and self-correction capability. Additional inter-model comparisons were performed across question subgroups categorized by clinical topic type.ResultsIn the information quality assessment, ChatGPT-4o demonstrated the best performance across most metrics, including accuracy score (6.70 ± 0.63), completeness score (4.63 ± 0.63), and harmlessness score (3.97 ± 0.17). Gemini achieved the highest conciseness score (4.00 ± 0.14). Further subgroup analysis showed that all LLMs performed comparably to or better than humans, regardless of the type of question posed. The readability assessment revealed that ChatGPT-4o had the lowest readability score (26.02 ± 10.78), indicating the highest level of reading difficulty. While Copilot recorded a higher readability score (40.26 ± 14.58) than the other LLMs, it still remained lower than that of humans (51.54 ± 13.71). Copilot also exhibited the best stability in reproducibility and stability assessment. All LLMs demonstrated strong self-correction capability when prompted.ConclusionOur study suggested that LLMs exhibited considerable potential in providing accurate and comprehensive responses to common cataract-related clinical issues. Notably, ChatGPT-4o achieved the best scores in accuracy, completeness, and harmlessness. Despite these promising results, clinicians and patients should be aware of the limitations of artificial intelligence (AI) to ensure critical evaluation in clinical practice.},
  archive      = {J_FRAI},
  author       = {Wang, Xinyue and Liu, Yan and Song, Linghao and Wen, Yinuo and Peng, Shenjie and Ren, Ruoxi and Zhang, Yi and Chen, Tianhui and Jiang, Yongxiang},
  doi          = {10.3389/frai.2025.1639221},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1639221},
  shortjournal = {Front. Artif. Intell.},
  title        = {Transforming cataract care through artificial intelligence: An evaluation of large language models’ performance in addressing cataract-related queries},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arabic speech recognition model using baidu's deep and cluster learning. <em>FRAI</em>, <em>8</em>, 1639147. (<a href='https://doi.org/10.3389/frai.2025.1639147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study involves extracting the spectrum from the Arabic raw, unlabeled audio signal and producing Mel-frequency cepstral coefficients (MFCCs). The clustering algorithm groups the retrieved MFCCs with analogous features. The K-means clustering technique played a crucial role in our research, enabling the unsupervised categorization of unlabeled Arabic audio data. Employing K-means on the extracted MFCC features allowed us to classify acoustically similar segments into distinct groups without prior knowledge of their characteristics. This initial phase was crucial for understanding the inherent diversity in our diverse sampled dataset. Dynamic Time Warping (DTW) and Euclidean Distance are utilized for illustration. Classification algorithms such as Decision Tree, eXtreme Gradient Boosting (XGBoost), K-Nearest Neighbors (KNN), and Random Forest are used to classify the various classes obtained based on clustering. This study also demonstrates the efficacy of Mozilla's Deep Speech framework for Arabic speech recognition. The core component of deep speech is its neural network architecture, which consists of multiple layers of Recurrent Neural Networks (RNNs). It strives to comprehend the intricate patterns and interactions between spoken sounds and their corresponding textual representations. The clustered labeled Arabic audio dataset, along with transcripts and Arabic Alphabets, is used as input to Baidu's Deep Speech model for training and testing purposes. PyCharm, in conjunction with Python 3.6, is used to build a Dockerfile. Creating, editing, and managing Dockerfiles within PyCharm's IDE is simplified by its functionality and integrated environment. Deep speech provides an eminent Arabic speech recognition quality with reduced loss, word error rate (WER), and character error rate (CER). Baidu's Deep Speech intends to achieve high performance in both end-to-end and isolated speech recognition with good precision and a low word rate and character error rate in a reasonable amount of time. The suggested strategy yielded a loss of 276.147, a word error rate of 0.3720, and a character error rate of 0.0568. This technique increases the accuracy of Arabic automatic speech recognition (ASR).},
  archive      = {J_FRAI},
  author       = {Al-Anzi, Fawaz S. and Sundaram Thankaleela, Bibin Shalini},
  doi          = {10.3389/frai.2025.1639147},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1639147},
  shortjournal = {Front. Artif. Intell.},
  title        = {Arabic speech recognition model using baidu's deep and cluster learning},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced YOLOv8 for industrial polymer films: A semi-supervised framework for micron-scale defect detection. <em>FRAI</em>, <em>8</em>, 1638772. (<a href='https://doi.org/10.3389/frai.2025.1638772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionPolymer material films are produced through extrusion machines, and their surfaces can develop micro-defects due to process and operational influences. The quantity and size of these defects significantly impact product quality.MethodsAs traditional machine learning defect detection methods suffer from low accuracy and poor adaptability to complex scenarios, requiring extensive effort for parameter tuning and exhibiting weak generalization capability, this paper proposes an improved YOLOv8 method to identify micro-defects on films. The approach embeds the CBAM attention mechanism into high-level networks to address feature sparsity in small target detection samples. Simultaneously, given the difficulty in obtaining large annotated datasets, we employ the Mean Teacher method for semi-supervised learning using limited labeled data. During training, the method optimizes neural network gradients through an improved loss function based on normalized Wasserstein distance (NWD), mitigating gradient instability caused by scale variations and enhancing detection accuracy for small targets. Additionally, a proposed multi-threshold mask segmentation algorithm extracts defect contours for further feature analysis.ResultsExperimental results demonstrate that the improved YOLOv8 algorithm achieves an 8.26% increase in mAP@0.5 compared to the baseline. It exhibits higher precision for small targets, and maintains defect detection rates exceeding 95.0% across validation data of varying image sizes, thereby meeting industrial production requirements. In generalization validation, the model demonstrates superior performance compared to traditional methods under test environments with lighting variations and environmental contamination.DiscussionThe improved YOLOv8 algorithm meeting the stringent requirements for high-precision small-target defect detection on polymer material film in industrial production. Future work will explore more advanced techniques to enhance model accuracy and robustness.},
  archive      = {J_FRAI},
  author       = {Yu, Xiaoxia and Hu, Bingyu and Jiang, Weifeng and Wan, Jinru and Yang, Xinduoji and Liu, Nianbo and Dong, Xiaoyan},
  doi          = {10.3389/frai.2025.1638772},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1638772},
  shortjournal = {Front. Artif. Intell.},
  title        = {Enhanced YOLOv8 for industrial polymer films: A semi-supervised framework for micron-scale defect detection},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auto-scaling LLM-based multi-agent systems through dynamic integration of agents. <em>FRAI</em>, <em>8</em>, 1638227. (<a href='https://doi.org/10.3389/frai.2025.1638227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionLarge Language Model-based Multi-Agent Systems (LLM-based MASs) represent a groundbreaking paradigm where diverse LLM-based agents collaborate, leveraging their unique capabilities to achieve shared objectives. Although LLM-based MASs outperform individual agents, their current architectures are limited by predefined, fixed, and static agent designs, restricting adaptability and scalability in dynamic environments.MethodTo address these limitations, this study proposes two novel approaches: Initial Automatic Agent Generation (IAAG) and Dynamic Real-Time Agent Generation (DRTAG). These approaches enable the automatic creation and seamless integration of new agents into MASs, driven by evolving conversational and task-specific contexts, thereby reducing the need for human intervention. Our method leverages advanced prompt engineering techniques such as persona pattern prompting, chain prompting, and few-shot prompting to generate new agents through existing LLM agents. Additionally, several evaluation metrics were adapted to score and rank LLM-generated texts.ResultsExperimental results demonstrate that the DRTAG approach significantly improves system adaptability and task performance compared to static MAS architectures. The IAAG framework also enhances initial system flexibility, supporting the creation of contextually relevant agents.DiscussionThese findings highlight the potential of dynamic LLM-based MASs to overcome the limitations of static architectures to address complex real-world challenges, paving the way for innovative applications across diverse domains.},
  archive      = {J_FRAI},
  author       = {Perera, Ravindu and Basnayake, Anuradha and Wickramasinghe, Manjusri},
  doi          = {10.3389/frai.2025.1638227},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1638227},
  shortjournal = {Front. Artif. Intell.},
  title        = {Auto-scaling LLM-based multi-agent systems through dynamic integration of agents},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv10-based detection of melanocytic nevi: Reverse exclusion optimization for melanoma screening. <em>FRAI</em>, <em>8</em>, 1637842. (<a href='https://doi.org/10.3389/frai.2025.1637842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malignant melanoma is the deadliest skin cancer, yet its early dermoscopic presentation closely mimics benign melanocytic nevi. Conventional visual or dermoscopic screening therefore suffers from high miss rates and generates excessive biopsies. In this study we focus on Chinese East-Asian patients and introduce a reversed-exclusion strategy—classifying “benign first, exclude malignancy”: lesions that fully meet benign nevus criteria are deemed low-risk; all others are flagged as high-risk. Building on the real-time detector YOLOv10, we incorporate three medical-oriented upgrades: (i) a PP-LCNet backbone to preserve sub-3 mm textures; (ii) a Multiscale Contextual Attention (MCA) neck to enhance cross-scale aggregation; and (iii) a Shape-IoU loss that jointly optimises position, scale, and curvature. The model was trained on a multi-centre dermoscopic dataset from three tertiary hospitals in mainland China (2,040 benign nevi) and independently tested on 365 biopsy-proven melanomas collected at the same medical institution but drawn from a demographically distinct patient cohort, achieving a detection mAP@0.5 of 97.69% for benign lesions and a melanoma false-negative rate (FNR) of only 0.27%. By delivering high-confidence benign identification followed by malignant exclusion, the proposed model offers a high-precision, low-risk pathway for early melanoma screening in Chinese clinical settings. It can markedly reduce unnecessary biopsies while keeping the miss rate below the clinical safety ceiling of 0.5%, thus preserving the life-saving window afforded by early detection.},
  archive      = {J_FRAI},
  author       = {Wang, ShengJie and Wang, Jian and Yin, Rui},
  doi          = {10.3389/frai.2025.1637842},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1637842},
  shortjournal = {Front. Artif. Intell.},
  title        = {YOLOv10-based detection of melanocytic nevi: Reverse exclusion optimization for melanoma screening},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lipschitz-based robustness estimation for hyperdimensional learning. <em>FRAI</em>, <em>8</em>, 1637105. (<a href='https://doi.org/10.3389/frai.2025.1637105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the adoption of machine learning models in various practical domains, there is a growing need for evaluating and increasing model robustness. Hyperdimensional computing (HDC) is a neurosymbolic computational paradigm that represents symbols as high dimensional vectors and symbolic operations as vector operations, seamlessly interfacing between neuro- and symbolic components of a model. However, there is a notable gap in HDC research regarding the robustness of HDC models to input perturbations. This study presents a novel theoretical framework tailored to evaluate the robustness of hyperdimensional classifiers against perturbations in the input space. In particular, our proposed measure of robustness gives a theoretical upper bound for the magnitude of noise a model can tolerate without changing its prediction for any given data point. We also propose a method to enhance the robustness of the model based on our proposed measure of robustness. Our approach introduces several methods to calculate model robustness as a function of the specific dataset and type of hyperdimensional encoding used. The results show that the average robustness of HDC models increases under the proposed optimization scheme while maintaining accuracy by varying the variance of the Gaussian distribution used to encode hypervectors. The practical effectiveness of our proposed measure of robustness is also demonstrated.},
  archive      = {J_FRAI},
  author       = {Yeung, Calvin and Errahmouni Barkam, Hamza and Zou, Zhuowen and Yun, Sanggeon and Bastian, Nathaniel D. and Imani, Mohsen},
  doi          = {10.3389/frai.2025.1637105},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1637105},
  shortjournal = {Front. Artif. Intell.},
  title        = {Lipschitz-based robustness estimation for hyperdimensional learning},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bibliometric review of deep learning in crop monitoring: Trends, challenges, and future perspectives. <em>FRAI</em>, <em>8</em>, 1636898. (<a href='https://doi.org/10.3389/frai.2025.1636898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global agricultural systems face unprecedented challenges from climate change, resource scarcity, and rising food demand, requiring transformative solutions. Artificial intelligence (AI), particularly deep learning (DL), has emerged as a critical tool for agricultural monitoring, yet a systematic synthesis of its applications remains understudied. This paper presents a comprehensive bibliometric and knowledge graph analysis of 650 + publications (2000–2024) to map AI’s role in agricultural information identification, with emphasis on DL and remote sensing integration (e.g., UAVs, satellites). Results highlight Convolutional Neural Networks (CNNs) as the dominant technology for real-time crop monitoring but reveal three persistent barriers: (1) scarcity of annotated datasets, (2) poor model generalization across environments, and (3) challenges in fusing multi-source data. Crucially, interdisciplinary collaboration—though vital for scalability—is identified as an underdeveloped research frontier. It is concluded that while AI can revolutionize agriculture, its potential hinges on improving data quality, developing environment-adaptive models, and fostering cross-domain partnerships. This study provides a strategic framework to accelerate AI’s integration into global agricultural systems, addressing both technical gaps and policy needs for future food security.},
  archive      = {J_FRAI},
  author       = {Zhang, Rui and Wu, Xue and Li, Jing and Zhao, Pengyu and Zhang, Qing and Wuri, Lige and Zhang, Donghui and Zhang, Zhijie and Yang, Linnan},
  doi          = {10.3389/frai.2025.1636898},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1636898},
  shortjournal = {Front. Artif. Intell.},
  title        = {A bibliometric review of deep learning in crop monitoring: Trends, challenges, and future perspectives},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Named entity recognition for chinese electronic medical records by integrating knowledge graph and ClinicalBERT. <em>FRAI</em>, <em>8</em>, 1634774. (<a href='https://doi.org/10.3389/frai.2025.1634774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionGeneral purpose language models often struggle with accurately identifying domain specific terminology in the medical field, resulting in suboptimal performance in named entity recognition (NER) tasks. This challenge is particularly pronounced in Chinese electronic medical records (EMRs), which lack clear word boundaries and contain complex medical expressions.MethodsThis study proposes a novel NER method for Chinese EMRs that integrates ClinicalBERT, a language model pre trained on clinical corpora, with structured knowledge from a medical knowledge graph. Entity representations derived via Translating Embeddings (TransE) are incorporated to inject external semantic knowledge. Furthermore, the model fuses multiple character level features, including positional labels, contextual category clues, and semantic embeddings, to enhance boundary detection. The input text is annotated using the BIOES (Begin, Inside, Outside, End, Single) tagging scheme and subsequently encoded by ClinicalBERT. The encoded features are then passed through a bidirectional long short term memory (BiLSTM) network and a conditional random field (CRF) layer for final label prediction.ResultsExperiments conducted on publicly available datasets demonstrate that the proposed approach achieves an F1 score of 89.44 percent, surpassing multiple existing baseline models in performance.DiscussionThese findings confirm that the integration of domain specific language modeling, structured medical knowledge, and enriched character level features significantly enhances NER accuracy in Chinese EMRs. The proposed method shows strong potential for practical deployment in clinical information extraction systems.},
  archive      = {J_FRAI},
  author       = {Xu, Xiang and Li, Zhengxiong and Zhang, Hongwei and Ma, Kai},
  doi          = {10.3389/frai.2025.1634774},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1634774},
  shortjournal = {Front. Artif. Intell.},
  title        = {Named entity recognition for chinese electronic medical records by integrating knowledge graph and ClinicalBERT},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI: The apollo guidance computer of the exposome moonshot. <em>FRAI</em>, <em>8</em>, 1632520. (<a href='https://doi.org/10.3389/frai.2025.1632520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Exposome—the totality of environmental exposures across a lifetime—remains one of the most significant challenges in understanding and preventing human disease. Translating its vast, heterogeneous data streams into actionable knowledge requires artificial intelligence (AI) integrated with human-relevant experimental systems. We propose a unifying vision in which Microphysiological Systems (MPS) and multi-omics platforms generate high-quality, context-specific data that iteratively calibrate AI models, enabling the creation of digital twins of organs, individuals, and ultimately populations. This “Exposome Moonshot” parallels the Apollo program in ambition, with MPS as the rocket, multi-omics as the lunar module, and AI as the guidance computer. Early applications demonstrate that deep learning can already outperform canonical animal tests for several toxicological endpoints, while reducing cost and time to decision. Realizing the full potential of Exposome intelligence will require expanding the applicability domain of models, implementing robust data security, and prioritizing transparent, interpretable algorithms. By linking predictive AI with experimental feedback, we can move toward a prevention-driven, personalized paradigm for human health and regulatory science.},
  archive      = {J_FRAI},
  author       = {Sillé, Fenna C. M. and Hartung, Thomas},
  doi          = {10.3389/frai.2025.1632520},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1632520},
  shortjournal = {Front. Artif. Intell.},
  title        = {AI: The apollo guidance computer of the exposome moonshot},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing XGBoosts to correct arcjet contamination in magnetic field measurements from GOES missions. <em>FRAI</em>, <em>8</em>, 1628029. (<a href='https://doi.org/10.3389/frai.2025.1628029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The magnetometers onboard the Geostationary Operational Environmental Satellites (GOES) provide crucial measurements for space weather monitoring and scientific research. However, periodic arcjet thruster firings introduce contamination in the measured magnetic field, affecting data accuracy. The currently used correction matrix approach mitigates these effects but struggles with transient variations and residual errors. In this study, we present an alternative correction method using XGBoost, a machine learning algorithm, to correct arcjet-induced contamination in the GOES-17 magnetometer data using GOES-18 as ground truth. Using cross-satellite comparisons and supervised learning techniques, our model is effective in reducing artificial disturbances, especially non-linear variations. We found that the XGBoost method works better than the existing correction matrix approach for E and P components, while the correction matrix performs better for the N component. Although some limitations remain due to training data constraints, our results highlight the importance of machine learning to improve magnetometer data quality by recognizing and correcting complex satellite-driven artifacts. The collocation of GOES-17 and GOES-18 provided a unique opportunity for cross-satellite calibration and validation, and with a longer collocation period, the XGBoost method shows significant promise for better correction of operational data, emphasizing the need for such configurations in future satellite missions.},
  archive      = {J_FRAI},
  author       = {Inceoglu, Fadil and Loto'aniu, Paul T. M.},
  doi          = {10.3389/frai.2025.1628029},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1628029},
  shortjournal = {Front. Artif. Intell.},
  title        = {Utilizing XGBoosts to correct arcjet contamination in magnetic field measurements from GOES missions},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An artificial intelligence model for early-stage breast cancer classification from histopathological biopsy images. <em>FRAI</em>, <em>8</em>, 1627876. (<a href='https://doi.org/10.3389/frai.2025.1627876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of breast cancer subtypes is essential for guiding treatment decisions and improving patient outcomes. In current clinical practice, determining histological subtypes often requires additional invasive procedures, delaying treatment initiation. This study proposes a deep learning-based model built on a DenseNet121 backbone with a multi-scale feature fusion strategy, designed to classify breast cancer from histopathological biopsy images. Trained and evaluated on the publicly available BreaKHis dataset using 5-fold cross-validation, the model achieved a binary classification accuracy of 97.1%, and subtype classification accuracies of 93.8% for benign tumors and 92.0% for malignant tumors. These results demonstrate the model’s ability to capture morphological cues at multiple levels of abstraction and highlight its potential as a diagnostic support tool in digital pathology workflows.},
  archive      = {J_FRAI},
  author       = {Chaudhary, Neil and Dhunny, A. Z.},
  doi          = {10.3389/frai.2025.1627876},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1627876},
  shortjournal = {Front. Artif. Intell.},
  title        = {An artificial intelligence model for early-stage breast cancer classification from histopathological biopsy images},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linguistic patterns in pandemic-related content: A comparative analysis of COVID-19, constraint, and monkeypox datasets. <em>FRAI</em>, <em>8</em>, 1627522. (<a href='https://doi.org/10.3389/frai.2025.1627522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis study investigates how linguistic features distinguish health misinformation from factual communication in pandemic-related online discourse. Understanding these differences is essential for improving detection of misinformation and informing effective public health messaging during crises.MethodsWe conducted a computational linguistic analysis across three corpora: COVID-19 false narratives (n = 7,588), general COVID-19 content (n = 10,700), and Monkeypox-related posts (n = 5,787). We examined readability, rhetorical markers, and persuasive language, focusing on differences between misinformation and factual communication.ResultsCOVID-19 misinformation exhibited markedly lower readability scores and contained more than twice the frequency of fear-related and persuasive terms compared to the other datasets. It showed minimal use of exclamation marks, contrasting with the more emotive style of Monkeypox content. These findings suggest that misinformation employs a deliberately complex rhetorical style combined with emotional cues, which may enhance perceived credibility.DiscussionOur findings contribute to the growing body of research on digital health misinformation by identifying linguistic indicators that can aid in detection. They also inform theoretical models of crisis communication and public health messaging strategies in networked media environments. However, the study has limitations, including reliance on traditional readability indices, a narrow persuasive lexicon, and static aggregate analysis. Future work should adopt longitudinal designs, incorporate broader emotion lexicons, and employ platform-sensitive approaches to improve robustness. The data and code supporting this study are openly available at: https://doi.org/10.5281/zenodo.17024569.},
  archive      = {J_FRAI},
  author       = {Sikosana, Mkululi and Maudsley-Barton, Sean and Ajao, Oluwaseun},
  doi          = {10.3389/frai.2025.1627522},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1627522},
  shortjournal = {Front. Artif. Intell.},
  title        = {Linguistic patterns in pandemic-related content: A comparative analysis of COVID-19, constraint, and monkeypox datasets},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable AI-driven depression detection from social media using natural language processing and black box machine learning models. <em>FRAI</em>, <em>8</em>, 1627078. (<a href='https://doi.org/10.3389/frai.2025.1627078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionMental disorders are highly prevalent in modern society, leading to substantial personal and societal burdens. Among these, depression is one of the most common, often exacerbated by socioeconomic, clinical, and individual risk factors. With the rise of social media, user-generated content offers valuable opportunities for the early detection of mental disorders through computational approaches.MethodsThis study explores the early detection of depression using black-box machine learning (ML) models, including Support Vector Machines (SVM), Random Forests (RF), Extreme Gradient Boosting (XGB), and Artificial Neural Networks (ANN). Advanced Natural Language Processing (NLP) techniques TF-IDF, Latent Dirichlet Allocation (LDA), N-grams, Bag of Words (BoW), and GloVe embeddings were employed to extract linguistic and semantic features. To address the interpretability limitations of black-box models, Explainable AI (XAI) methods were integrated, specifically the Local Interpretable Model-Agnostic Explanations (LIME).ResultsExperimental findings demonstrate that SVM achieved the highest accuracy in detecting depression from social media data, outperforming RF and other models. The application of LIME enabled granular insights into model predictions, highlighting linguistic markers strongly aligned with established psychological research.DiscussionUnlike most prior studies that focus primarily on classification accuracy, this work emphasizes both predictive performance and interpretability. The integration of LIME not only enhanced transparency and interpretability but also improved the potential clinical trustworthiness of ML-based depression detection models.},
  archive      = {J_FRAI},
  author       = {Hameed, Sidra and Nauman, Muhammad and Akhtar, Nadeem and Fayyaz, Muhammad A. B. and Nawaz, Raheel},
  doi          = {10.3389/frai.2025.1627078},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1627078},
  shortjournal = {Front. Artif. Intell.},
  title        = {Explainable AI-driven depression detection from social media using natural language processing and black box machine learning models},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning/machine learning approach for anomaly based network intrusion detection. <em>FRAI</em>, <em>8</em>, 1625891. (<a href='https://doi.org/10.3389/frai.2025.1625891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe increasing complexity and frequency of cybersecurity threats necessitate the development of advanced detection systems capable of identifying both known and emerging attacks. In this study, we present a hybrid anomaly-based Network Intrusion Detection System (NIDS) that integrates multiple machine learning and deep learning algorithms, including XGBoost, Random Forest, Graph Neural Networks (GNN), Long Short-Term Memory (LSTM) networks, and Autoencoders.MethodsThe proposed system was trained on a large-scale dataset comprising over 5.6 million network traffic records. Comprehensive data preprocessing and feature engineering were applied, and the Synthetic Minority Over-sampling Technique (SMOTE) was employed to address class imbalance. To enhance robustness and generalization, a weighted soft-voting ensemble strategy was used to combine predictions from the individual models.ResultsThe experimental evaluation demonstrated near-perfect performance, with accuracy, precision, recall, and F1-score values approaching 100% on the primary dataset. These results were validated through rigorous 5-fold cross-validation.DiscussionEvaluation on an independent benchmark dataset confirmed the strong generalizability and robustness of the proposed model across diverse intrusion scenarios. These findings highlight the effectiveness of the hybrid ensemble framework in significantly improving intrusion detection capabilities within complex and dynamic network environments.},
  archive      = {J_FRAI},
  author       = {Almuhanna, Reem and Dardouri, Samia},
  doi          = {10.3389/frai.2025.1625891},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1625891},
  shortjournal = {Front. Artif. Intell.},
  title        = {A deep learning/machine learning approach for anomaly based network intrusion detection},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in ADHD assessment: A comprehensive review of research progress from early screening to precise differential diagnosis. <em>FRAI</em>, <em>8</em>, 1624485. (<a href='https://doi.org/10.3389/frai.2025.1624485'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention deficit hyperactivity disorder (ADHD) diagnosis traditionally relies on subjective assessments, which lead to challenges like symptom overlap, heterogeneity, and misdiagnosis risk. Artificial intelligence (AI), especially machine learning (ML) and deep learning (DL), offers objective assessment opportunities by processing complex multimodal data (behavioral, neurophysiological, neuroimaging, genetic). This paper reviews AI’s current applications in objective ADHD assessment, covering early screening, risk prediction, diagnostic assistance, classification, assistance in precise differential diagnosis, symptom quantification, and heterogeneous subtype identification. While AI models show significant potential in extracting objective biomarkers and improving assessment efficiency, the field faces challenges: insufficient standardized data, limited generalization, interpretability issues, potential biases, and lack of rigorous clinical validation. Future research must establish large-scale, standardized multimodal databases, develop robust, interpretable, and fair AI models, and conduct rigorous clinical translation validation to achieve responsible, precise, objective, and personalized ADHD assessment and management.},
  archive      = {J_FRAI},
  author       = {Zhao, Cuijie and Xu, Yan and Li, Ruixing and Li, Huawei and Zhang, Meng},
  doi          = {10.3389/frai.2025.1624485},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1624485},
  shortjournal = {Front. Artif. Intell.},
  title        = {Artificial intelligence in ADHD assessment: A comprehensive review of research progress from early screening to precise differential diagnosis},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence attitudes and resistance to use robo-advisors: Exploring investor reluctance toward cognitive financial systems. <em>FRAI</em>, <em>8</em>, 1623534. (<a href='https://doi.org/10.3389/frai.2025.1623534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe study investigates resistance towards Financial Robo-Advisors (FRAs) among retail investors in India, grounded in innovation resistance theory. The study examines the impact of functional barriers and psychological barriers on resistance to FRAs, while considering user’s attitudes towards Artificial Intelligence (AI) as a moderator. It further evaluate the influence of such resistance on users’ intentions to use and recommend FRAs.MethodsUtilizing purposive sampling data was collected from 409 investors and further analyzed using structural equation modelling.ResultsThe findings revealed that all barriers under study, expect value barrier, substantially derive resistance towards robo-advisors, with inertia being the strongest determinant. Further, this resistance impedes both the intention to use FRAs and to recommend them. Moderation analysis results finds that users’ attitude towards AI significantly weakens the influence of inertia, overconfidence bias and data privacy risk on resistance, with no such impact on other relationships.DiscussionOverall, the study enriches IRT in Fintech context and provides theoretical and practical insights to enhance FRAs adoption in emerging markets.},
  archive      = {J_FRAI},
  author       = {Verma, Balraj and Schulze, Mike and Goswami, Divya and Upreti, Kamal},
  doi          = {10.3389/frai.2025.1623534},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1623534},
  shortjournal = {Front. Artif. Intell.},
  title        = {Artificial intelligence attitudes and resistance to use robo-advisors: Exploring investor reluctance toward cognitive financial systems},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing prediction of primary site recurrence in head and neck cancer using radiomics and uncertainty estimation. <em>FRAI</em>, <em>8</em>, 1623393. (<a href='https://doi.org/10.3389/frai.2025.1623393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionHead and neck squamous cell carcinomas (HNSCC) present a significant clinical challenge due to high recurrence rates despite advances in radiation and chemotherapy. Early detection of recurrence is critical for optimizing treatment outcomes and improving patient survival.MethodsWe developed two artificial intelligence (AI) pipelines—(1) machine learning models trained on radiomic and clinical data and (2) a Vision Transformer-based model directly applied to imaging data—to predict HNSCC recurrence using pre- and post-treatment PET/CT scans from a cohort of 249 patients. We incorporated Test-Time Augmentation (TTA) and Conformal Prediction to quantify prediction uncertainty and enhance model reliability.ResultsThe machine learning models achieved an average AUC of 0.820. The vision transformer model showed moderate performance (AUC = 0.658). Uncertainty quantification enabled the exclusion of ambiguous predictions, improving accuracy among more confident cases.DiscussionOur machine learning models achieved strong performance in predicting HNSCC recurrence from radiomic and clinical features. Incorporating uncertainty quantification further improved predictive performance and reliability.},
  archive      = {J_FRAI},
  author       = {Hu, Yu and Taing, Kimberly and Wang, Jing and Sher, David and Dohopolski, Michael},
  doi          = {10.3389/frai.2025.1623393},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1623393},
  shortjournal = {Front. Artif. Intell.},
  title        = {Enhancing prediction of primary site recurrence in head and neck cancer using radiomics and uncertainty estimation},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EmoShiftNet: A shift-aware multi-task learning framework with fusion strategies for emotion recognition in multi-party conversations. <em>FRAI</em>, <em>8</em>, 1618698. (<a href='https://doi.org/10.3389/frai.2025.1618698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionEmotion Recognition in Conversations (ERC) is vital for applications such as mental health monitoring, virtual assistants, and human–computer interaction. However, existing ERC models often neglect emotion shifts—transitions between emotional states across dialogue turns in multi-party conversations (MPCs). These shifts are subtle, context-dependent, and complicated by class imbalance in datasets such as the Multimodal EmotionLines Dataset (MELD).MethodsTo address this, we propose EmoShiftNet, a shift-aware multi-task learning (MTL) framework that jointly performs emotion classification and emotion shift detection. The model integrates multimodal features, including contextualized text embeddings from BERT, acoustic features (Mel-Frequency Cepstral Coefficients, pitch, loudness), and temporal cues (pause duration, speaker overlap, utterance length). Emotion shift detection is incorporated as an auxiliary task via a composite loss function combining focal loss, binary cross-entropy, and triplet margin loss.ResultsEvaluations on the MELD dataset demonstrate that EmoShiftNet achieves higher overall F1-scores than both traditional and graph-based ERC models. In addition, the framework improves the recognition of minority emotions under imbalanced conditions, confirming the effectiveness of incorporating shift supervision and multimodal fusion.DiscussionThese findings highlight the importance of modeling emotional transitions in ERC. By leveraging multi-task learning with explicit shift detection, EmoShiftNet enhances contextual awareness and offers more robust performance for multi-party conversational emotion recognition.},
  archive      = {J_FRAI},
  author       = {Nirujan, Hinduja and Priyadarshana, Y. H. P. P.},
  doi          = {10.3389/frai.2025.1618698},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1618698},
  shortjournal = {Front. Artif. Intell.},
  title        = {EmoShiftNet: A shift-aware multi-task learning framework with fusion strategies for emotion recognition in multi-party conversations},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image restoration and key field alignment for misaligned overlapping text in secondary printing document images. <em>FRAI</em>, <em>8</em>, 1616007. (<a href='https://doi.org/10.3389/frai.2025.1616007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of information technology, the demand for efficient recognition and information extraction from paper documents in industrial scenarios has grown rapidly. In practice, business information is often secondarily printed onto pre-designed templates, which frequently leads to text misalignment or overlap with backgrounds and tables, thereby significantly impairing the accuracy of subsequent Optical Character Recognition (OCR). To address this issue, this paper proposes a preprocessing method for OCR recognition of secondary printed documents, specifically targeting the problems of text misalignment and overlap. In particular, we design a Text Overlap Restoration Network (TORNet) to restore document images affected by text overlap. Experimental results demonstrate that, compared to the latest image restoration models, TORNet achieves PSNR improvements of 0.17 dB and 0.12 dB in foreground and background text restoration, respectively. Furthermore, to resolve residual misalignment issues after image restoration, a key-field alignment method is introduced. This method accurately locates the positional deviations of critical fields in the reconstructed image, enabling precise field-level alignment and structural correction. Based on the proposed preprocessing framework, the recognition accuracy and field-matching accuracy are improved by 23% and 31%, respectively, compared to existing commercial OCR models, significantly enhancing the recognition performance on misaligned and overlapping documents. This study provides an effective solution for recognizing secondary printed documents with text overlap in industrial environments.},
  archive      = {J_FRAI},
  author       = {Wang, Senlong and Ge, Junchao and Zhang, Jiantao and He, Hong and Zhang, Yunwei},
  doi          = {10.3389/frai.2025.1616007},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1616007},
  shortjournal = {Front. Artif. Intell.},
  title        = {Image restoration and key field alignment for misaligned overlapping text in secondary printing document images},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ethical implications of ChatGPT and other large language models in academia. <em>FRAI</em>, <em>8</em>, 1615761. (<a href='https://doi.org/10.3389/frai.2025.1615761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of technology in the digital age has significantly transformed human communication and knowledge exchange. At the forefront of this transformation are Large Language Models (LLMs), powerful neural networks trained on vast text corpora to perform a wide range of Natural Language Processing (NLP) tasks. While LLMs offer promising benefits such as enhanced productivity and human-like text generation, their integration into academic settings raises pressing ethical concerns. This study investigates the ethical dimensions surrounding the use of LLMs in academia, driven by their increasing prevalence and the need for responsible adoption. A mixed-methods approach was employed, combining surveys, semi-structured interviews, and focus groups with key stakeholders, including students, faculty, administrators, and AI developers. The findings reveal a high level of LLM adoption accompanied by concerns related to plagiarism, bias, authenticity, and academic integrity. In response, the study proposes concrete strategies for ethical integration, including: (1) the establishment of transparent usage policies, (2) the incorporation of LLM literacy training into academic curricula, (3) the development of institutional review frameworks for AI-generated content, and (4) ongoing stakeholder dialogue to adapt policies as the technology evolves. These recommendations aim to support the responsible and informed use of LLMs in scholarly environments. The widespread influence of technological advancement has notably transformed communication and knowledge sharing, with LLMs playing a central role. These advanced neural networks, trained on extensive text datasets, have become valuable tools for generating human-like text and improving efficiency. However, their growing use in academic contexts raises significant ethical concerns. This study investigates these issues, focusing on the implications of LLM integration in scholarly environments. Using mixed methods, including surveys, semi-structured interviews, and focus groups, the research gathered insights from students, faculty, administrators, and AI developers. The findings highlight substantial adoption of LLMs alongside concerns about plagiarism, bias, and academic integrity. Based on this input, the study proposes guidelines for their responsible and ethical use in academia.},
  archive      = {J_FRAI},
  author       = {Almufarreh, Ahmad and Ahmad, Ashfaq and Arshad, Muhammad and Onn, Choo Wou and Elechi, Robinson},
  doi          = {10.3389/frai.2025.1615761},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1615761},
  shortjournal = {Front. Artif. Intell.},
  title        = {Ethical implications of ChatGPT and other large language models in academia},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of the hybrid machine learning models for brain tumour segmentation and detection in medical images. <em>FRAI</em>, <em>8</em>, 1615550. (<a href='https://doi.org/10.3389/frai.2025.1615550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early and accurate detection of brain tumours using Magnetic Resonance Imaging (MRI) is critical for effective treatment and improved patient outcomes. This systematic review investigates the application of hybrid machine learning (ML) and deep learning (DL) models in enhancing the computational efficiency and diagnostic accuracy of brain tumour analysis from MRI images. The study synthesizes recent advances in combining traditional ML models such as Support Vector Machines (SVM) with deep neural networks like VGG-19 and YOLOv10n. A PRISMA-based literature search strategy was employed across major databases, including PubMed, Scopus, and IEEE Xplore, selecting 25 relevant studies published between 2019 and 2024. The review evaluates the performance of standalone and hybrid models using metrics such as Dice Similarity Coefficient (DSC), Intersection over Union (IoU), accuracy, precision, recall, and F1-score. Findings indicate that hybrid models, particularly those combining SVM with CNN-based architectures like VGG-19, demonstrate improved classification accuracy and reduced false positives, outperforming single-model approaches. Lightweight versions such as YOLOv10n offer faster inference times suitable for real-time applications while maintaining competitive accuracy. Despite these advances, challenges remain in model generalizability, lack of large, annotated datasets, and limited adoption of Explainable AI (XAI) for interpretability. This review highlights the potential of hybrid models for brain tumour detection and offers recommendations for future research to focus on scalable, interpretable, and clinically deployable solutions.},
  archive      = {J_FRAI},
  author       = {Netshamutshedzi, Ndivhuwo and Netshikweta, Rendani and Ndogmo, Jean-Claude and Obagbuwa, Ibidun Christiana},
  doi          = {10.3389/frai.2025.1615550},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1615550},
  shortjournal = {Front. Artif. Intell.},
  title        = {A systematic review of the hybrid machine learning models for brain tumour segmentation and detection in medical images},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI adoption among adolescents in education: Extending the UTAUT2 with psychological and contextual factors. <em>FRAI</em>, <em>8</em>, 1614993. (<a href='https://doi.org/10.3389/frai.2025.1614993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis correlational study investigates the psychological and contextual factors associated with the adoption of artificial intelligence (AI) technologies among Italian high school students. Building on the Unified Theory of Acceptance and Use of Technology 2 (UTAUT2), the study extends the model by incorporating Problematic Internet Use (PIU) and Attitudes Toward AI (ATAI) to better account for habitual AI use and behavioural intentions.MethodA sample of 933 students (Mage = 16.20, SDage = 1.29, 54.98% female) completed a survey assessing key UTAUT2 dimensions, psychological traits, and usage patterns of AI tools in educational contexts. Confirmatory factor analysis (CFA) was used to evaluate the functioning of the adapted UTAUT2. Multiple regression was used to investigate factors predicting habit formation and behavioural intention related to AI use.ResultsConfirmatory factor analysis supported the structural validity of the adapted UTAUT2 model. Multiple regression analyses revealed that Performance Expectancy, Social Influence, Hedonic Motivation, and Schoolwork-related AI use were significant predictors of both habit and behavioural intention. PIU showed a robust association with habitual use, suggesting a spillover effect from compulsive Internet behavior to AI engagement. ATAI was associated only with behavioural intention, indicating its role in initial adoption rather than sustained use. Demographic and contextual factors (e.g., school type, citizenship) showed additional effects.DiscussionThese findings contribute to a more comprehensive understanding of adolescent AI engagement by highlighting the role of compulsive tendencies and motivational beliefs. The study underscores the importance of designing inclusive, age-appropriate interventions to promote balanced and informed AI use in educational settings.},
  archive      = {J_FRAI},
  author       = {Caffaratti, Luca Ballestra and Longobardi, Claudio and Badenes-Ribera, Laura and Marengo, Davide},
  doi          = {10.3389/frai.2025.1614993},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1614993},
  shortjournal = {Front. Artif. Intell.},
  title        = {AI adoption among adolescents in education: Extending the UTAUT2 with psychological and contextual factors},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LMS-ViT: A multi-scale vision transformer approach for real-time smartphone-based skin cancer detection. <em>FRAI</em>, <em>8</em>, 1612502. (<a href='https://doi.org/10.3389/frai.2025.1612502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is the abnormal growth of skin cells. It occurs mostly in skin exposed to sunlight. To prevent the occurrence of skin cancer, avoid exposing skin to ultraviolet radiation. Skin cancer can be very harmful if found very late. Traditional convolutional neural networks (CNNs) face challenges in fine-grained lesion classification due to their limited ability to extract detailed features. To overcome such limitations, we introduced a novel approach in the form of a lightweight multi-scale vision transformer (LMS-ViT) application for the automated detection of skin cancer using dermoscopic images and the HAM10000 dataset. Unlike CNNs, LMS-ViT employs a multi-scale attention mechanism to capture both global lesion structures and fine-grained textural details, improving classification accuracy. This study combines skin images from the HAM10000 dataset with pictures taken using a smartphone. It uses a compact method to mix important features, which makes the system faster and suitable for real-time use in medical apps. The proposed system enables real-time skin cancer classification via a smartphone camera, making it portable and platform-independent. Experimental results show that LMS-ViT surpasses CNN-based models across all skin lesion categories, achieving 90% accuracy, an 18% improvement over CNN, while reducing computational cost by 30%. LMS-ViT also improves precision, recall, and F1-score, particularly in complex categories such as Vasc (0.96 to 1.01) and Nv (0.94 to 1.01), demonstrating superior classification power. With real-time android implementation, LMS-ViT offers accessible, mobile-friendly diagnostics for early skin cancer detection.},
  archive      = {J_FRAI},
  author       = {Leema, A. Anny and Balakrishnan, P. and Gopichand, G. and Rajarajan, G.},
  doi          = {10.3389/frai.2025.1612502},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1612502},
  shortjournal = {Front. Artif. Intell.},
  title        = {LMS-ViT: A multi-scale vision transformer approach for real-time smartphone-based skin cancer detection},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of vision transformers for the detection of fullness of garbage bins for efficient waste management. <em>FRAI</em>, <em>8</em>, 1612080. (<a href='https://doi.org/10.3389/frai.2025.1612080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient waste management is crucial for urban environments to maintain cleanliness, reduce environmental impact, and optimize resource allocation. Traditional waste collection systems often rely on scheduled pickups or manual inspections, leading to inefficient resource utilization and potential overflow issues. This paper presents a novel approach to automate the detection of garbage container fullness from images using machine learning techniques. More specifically, we explore three transformer-based architectures, namely, vision transformer, Swin transformer, and pyramid vision transformer to classify input images of garbage bins as clean or dirty. Our experimental results on the publicly available Clean dirty containers in Montevideo dataset suggest that transformer-based architectures are effective in garbage fullness detection. Moreover, a comparison with existing methods reveals that the proposed approach using the vision transformer surpasses the state-of-the-art, achieving a 96.74% accuracy in detecting garbage container fullness. In addition, the generalizability of the proposed approach is evaluated by testing the transformer-based classification frameworks on a synthetic image dataset generated using various generative AI models. The proposed approach achieved a highest test accuracy of 80% on this synthetic dataset, thereby highlighting its ability to generalize across different datasets. Synthetic dataset used in this work can be found at: https://www.kaggle.com/datasets/6df0652d2c4eb3b9f00043c40fba0afa0778b46d7c0685e212807c2f6967fe6f.},
  archive      = {J_FRAI},
  author       = {Tanwer, Parakram Singh and Maheshwari, Shishir and Behera, Sushree and Chauhan, Amit and Sunil Kumar, T.},
  doi          = {10.3389/frai.2025.1612080},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1612080},
  shortjournal = {Front. Artif. Intell.},
  title        = {Evaluation of vision transformers for the detection of fullness of garbage bins for efficient waste management},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lung cancer risk prediction using augmented machine learning pipelines with explainable AI. <em>FRAI</em>, <em>8</em>, 1602775. (<a href='https://doi.org/10.3389/frai.2025.1602775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer remains the leading cause of cancer-related deaths worldwide, making early and precise diagnosis is critical for improving the patient survival rates. Machine learning has shown promising results in predictive analysis for lung cancer prediction. However, class imbalance in clinical datasets negatively impacts the performance of Machine Learning classifiers, leading to biased predictions and reduced accuracy. In an attempt to address this issue, various data augmentation techniques were applied alongside classification models to enhance predictive performance. This study evaluates data augmentation techniques paired with machine learning classifiers to address class imbalance in a small lung cancer dataset. A comparative analysis was conducted to assess the impact of different augmentation techniques with classification models. Experimental findings demonstrate that K-Means SMOTE, combined with a Multi-Layer Perceptron classifier, achieves the highest accuracy of 93.55% and an AUC-ROC score of 96.76%, surpassing other augmentation-classifier combinations. These results underscore the importance of selecting optimal augmentation methods to improve classification performance. Furthermore, to ensure model interpretability and transparency in medical decision-making, LIME is utilized to provide insights into model predictions. The study highlights the significance of advanced augmentation techniques in addressing data imbalance, ultimately enhancing lung cancer risk prediction through machine learning. The findings contribute to the growing field of AI-driven healthcare by emphasizing the necessity of selecting effective augmentation-classifier pairs to develop more accurate and reliable diagnostic models. Due to the dataset’s high cancer prevalence (87.45%) and limited size, this work is a preliminary methodological comparison, not a clinical tool. Findings emphasize the importance of augmentation for imbalanced data and lay the groundwork for future validation with larger, representative datasets.},
  archive      = {J_FRAI},
  author       = {M S, Pavithran and D, Saranyaraj and Chakrabortty, Anirban},
  doi          = {10.3389/frai.2025.1602775},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1602775},
  shortjournal = {Front. Artif. Intell.},
  title        = {Lung cancer risk prediction using augmented machine learning pipelines with explainable AI},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for cardiovascular management: Optimizing pathways and cost control under diagnosis-related group models. <em>FRAI</em>, <em>8</em>, 1580445. (<a href='https://doi.org/10.3389/frai.2025.1580445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (CVDs) remain the leading causes of morbidity, mortality, and healthcare expenditures, presenting substantial challenges for hospitals operating under Diagnosis-Related Group (DRG) payment models. Recent advances in deep learning offer new strategies for optimizing CVD management to meet cost control objectives. This review synthesizes the roles of deep learning in CVD diagnosis, treatment planning, and prognostic modeling, emphasizing applications that reduce unnecessary diagnostic imaging, predict high-cost complications, and optimize the utilization of critical resources like ICU beds. By analyzing medical images, forecasting adverse events from patient data, and dynamically optimizing treatment plans, deep learning offers a data-driven strategy to manage high-cost procedures and prolonged hospital stays within DRG budgets. Deep learning offers the potential for earlier risk stratification and tailored interventions, helping mitigate the financial pressures associated with DRG reimbursements. Effective integration requires multidisciplinary collaboration, robust data governance, and transparent model design. Real-world evidence, drawn from retrospective studies and large clinical registries, highlights measurable improvements in cost control and patient outcomes; for instance, AI-optimized treatment strategies have been shown to reduce estimated mortality by 3.13%. However, challenges—such as data quality, regulatory compliance, ethical issues, and limited scalability—must be addressed to fully realize these benefits. Future research should focus on continuous model adaptation, multimodal data integration, equitable deployment, and standardized outcome monitoring to validate both clinical quality and financial return on investment under DRG metrics. By leveraging deep learning’s predictive power within DRG frameworks, healthcare systems can advance toward a more sustainable model of high-quality, cost-effective CVD care.},
  archive      = {J_FRAI},
  author       = {Chen, Haohao and Zeng, Ying and Cai, De},
  doi          = {10.3389/frai.2025.1580445},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1580445},
  shortjournal = {Front. Artif. Intell.},
  title        = {Deep learning for cardiovascular management: Optimizing pathways and cost control under diagnosis-related group models},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding manipulative narratives in cognitive warfare: A case study of the russia-ukraine conflict. <em>FRAI</em>, <em>8</em>, 1566022. (<a href='https://doi.org/10.3389/frai.2025.1566022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis study investigates the construction and dissemination of manipulative narratives in the context of cognitive warfare during the Russia-Ukraine conflict. Leveraging a mixed-methods approach that integrates AI-assisted semantic analysis with expert validation, we examine how adversarial messaging exploits cognitive biases-such as fear and confirmation bias-to influence perceptions and disrupt institutional trust.MethodsUsing the proprietary Attack-Index tool and large language models (LLMs), we detect linguistic markers of manipulation, including euphemisms, sarcasm, and strategic framing.ResultsOur findings demonstrate that emotionally charged narratives, particularly those invoking nuclear threat scenarios, are synchronized with key geopolitical events to influence decision-makers and public opinion. The study identifies five thematic clusters and traces shifts in rhetorical strategies over time, showing how manipulative discourse adapts to geopolitical contexts. Special attention is given to the differentiated targeting of international political elites, Western publics, and Russian domestic audiences, each exhibiting varied cognitive vulnerabilities.DiscussionWe acknowledge methodological and ethical limitations, including the dual-use potential of AI tools and challenges in establishing causal inferences. Nonetheless, this study offers the following key contributions:Empirically establishing nuclear rhetoric as a strategic element of narrative manipulation, particularly around NATO summits and military aid announcements.Advancing an integrated analytical framework that combines semantic clustering and AI-based discourse detection to monitor information threats in real time.Providing actionable insights for policy and digital security, including the development of countermeasures and international collaboration in addressing cognitive warfare.},
  archive      = {J_FRAI},
  author       = {Paziuk, Andrii and Lande, Dmytro and Shnurko-Tabakova, Elina and Kingston, Phillip},
  doi          = {10.3389/frai.2025.1566022},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1566022},
  shortjournal = {Front. Artif. Intell.},
  title        = {Decoding manipulative narratives in cognitive warfare: A case study of the russia-ukraine conflict},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A drop-out mechanism for active learning based on one-attribute heuristics. <em>FRAI</em>, <em>8</em>, 1562916. (<a href='https://doi.org/10.3389/frai.2025.1562916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active Learning (AL) leverages the principle that machine learning models can achieve high accuracy with fewer labeled samples by strategically selecting the most informative data points for training. However, when human annotators provide these labels, their decisions might exhibit a systematic bias. For example, humans frequently rely on a limited subset of the available attributes, or even on a single attribute, when making decisions, as when employing fast and frugal heuristics. This paper introduces a mathematically grounded approach to quantify the probability of mislabeling based on one attribute. We present a novel dropout mechanism designed to influence the attribute selection process used in annotation, effectively reducing the impact of bias. The proposed mechanism is evaluated using multiple AL algorithms and heuristic strategies across diverse prediction tasks. Experimental results demonstrate that the dropout mechanism significantly enhances active learning (AL) performance, achieving a minimum 70% improvement in effectiveness. These findings highlight the mechanism's potential to improve the reliability and accuracy of AL systems, providing valuable insights for designing and implementing robust intelligent systems.},
  archive      = {J_FRAI},
  author       = {Ravichandran, Sriram and Sudarsanam, Nandan and Ravindran, Balaraman and Katsikopoulos, Konstantinos V.},
  doi          = {10.3389/frai.2025.1562916},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1562916},
  shortjournal = {Front. Artif. Intell.},
  title        = {A drop-out mechanism for active learning based on one-attribute heuristics},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lexicon obtained and validated by a data-driven approach for organic residues valorization in emerging and developing countries. <em>FRAI</em>, <em>8</em>, 1557137. (<a href='https://doi.org/10.3389/frai.2025.1557137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open dump remains the main management process of organic residue in middle-and low-income countries [1]. Indeed, according to this study, municipal solid waste is composed of 44% organic fraction. However, waste recycling or valorization is about 7, 4.7, and 21% in Sub-Saharan Africa, Caribbean/Latin America, and South Asia respectively. It is thus interesting to determine organic residue valorization status in those regions. Answer to that question could be prospected through textual analysis. The method herein represents the first step to that end. Indeed, when missing, text mining could be used to extract thematic lexicon from a bibliographic corpus to drive a state-of-art in the valorization of organic residues in agriculture in developing countries. In this work, text mining and Natural Language Processing (NLP) methods enable to generate a specialized lexicon on this specific area. The definition of relevance of terms is challenging and discussed in this data paper. Actually, terminology extraction methods are generally based on benchmarks (i.e. gold-standard) or terms manually validated [2] but an experimental protocol that takes into account different kinds of relevance to consolidate the process is understudied. This needs to integrate expertise knowledge, agreement of experts regarding definitions and evaluation associated with, and the task to do. This paper highlights how this construction is conducted by considering different point-of-view of relevance in a multidisciplinary context. It is important to notice that this kind of lexicon specifically focused on organic residues valorization does not exist in agriculture semantic resources like AgroPortal which include more than 200 ontologies/thesaurii/lexicons [3].The present work consisted of using text mining approach to construct thematical lexicon from a corpus related to valorization of organic waste in developing countries. Method used to collect data was detailed first, followed by a section about the technic adopted to select, annotate, and validate the lexicon. Finally, future perspective work was explained in a concluding section. This exploratory methodology could be used to guide a more in-depth and oriented text analysis of scientific publications (i.e. scientometric analysis). Moreover, this methodology can be reused and/or adapted in other domain depending on purpose. In our ongoing work, we use this lexicon to conduct a semantic analysis of scientific publications dealing with organic residues valorization in emerging and developing countries.Proposed Method to Collect the DataSeveral online databases were consulted in 2021, to extract articles relating to biotransformation and valorization in agriculture of organic residues in emerging and developing countries (WoS, Ovid, Scopus, Google scholar, HAL, Cairn.info, AGRIS, and Agritropfoot_0 ) published until 2021. Terms used for bibliographic search in all databases through specific queries are detailed in the Appendix section of this paper.The equation used in the Web of Science collection was thereafter adapted for the other databases specificities. Advanced search was not available for most of the free online database, a global thematical search was then adopted (Appendix 1). The search gave 24 186 references on which a selective sorting was conducted to avoid duplicates and to select references in English only. A total of 7 692 references were used to generate the dataset available in the excel file (Initial_Corpus_References.xlsx) available on depository [4]. The corpus of the dataset combines articles, reports, book sections, and student thesis with bibliographic references (authors, year of publication, title, doi, and url).BioTex [5] was used to perform an Automatic Term Extraction (ATE) on the corpus. The terms extracted (e.g. rumen, humic acid, nutrient recovery, …) give a semantic point of view of the theme of the text. This tool was developed for Biomedical term extraction [6] and was adapted to extract terms associated with food security [7]. First, BioTex performed a linguistic screening through syntactic patterns (noun-noun, adjective-noun, …). In order to rank terms extracted on the "titles" corpus, the F-TF-IDF-C score integrated to BioTex was applied. This measure combines (i) C-value (4) to favor multi-word terms extracted, and (ii) TF-IDF (Term Frequency-Inverse Document Frequency) to highlight discriminative terms [6].Text mining was thereafter performed on titles of the corpus using the BioTex tools [5] and the result can be found in the associated excel file (Extracted_Terms.xlsx) on depository (1). The first column contains the 19 580 terms obtained from the extraction. The second column ("term") presents the terms constituted of words or compound nouns (e.g. mulch, effluents, soil amendments, bagasse cocomposting). The rank, in the last column, is obtained by maximizing a discriminative score associated with terms (i.e. F-TF-IDF-C).Five specialist raters conducted a first annotation on 200 sampled candidate terms among the 19 580 to exclude irrelevant terms to the topic of interest following the guideline file (Annotation_guidelines.pdf) available on the depository [4]. Specialists were researchers in "Recyclage et risque" unit of Cirad in France, working on recycling organic residue in agriculture and associated risks. The group was specialized in biochemistry, agronomy, microbiology, ecologist, soil science, and environmental assessment using both monitoring and modelling approaches. Each rater was asked to categorize each candidate term belonging to i) organic residues (OWT) and/or ii) biotransformation process (TM) and/or iii) valorization in agriculture (AV) or iv) none of them (None)following the first annotation guide. Definition of each category is described in the annotation guidelines. Table 1 shows example of the first step of annotation conducted by specialist.The Fleiss Kappa [8] which measures agreement between several raters equals to 0.52 for this first annotation corresponding to a bad agreement between the 5 raters. The 4 categories chosen to annotate the candidate terms appeared to be too restrictive. Terms indirectly associated to one or more of the 4 categories have been excluded by several raters.In a second annotation guideline, the manual labelling process focuses on the overall degree of pertinence related to the topic of valorization of organic residues. In this context, candidate term was annotated according 3 classes: (i) very pertinent when it was directly connected to one or more category(ries) (i.e. OWT+, TM+, AV+), (ii) pertinent when it was indirectly connected to one or more category(ries) (i.e. OWT, TM, AV), and (iii) irrelevant (i.e. None).A second annotation on the same 200 sampled terms was conducted. All results of the two series of annotation can be viewed with the file "Raters_Annotation_Results.xlsx" in our dataset [4]. Fleiss Kappa was calculated for 3 and 5 raters. It revealed a decreasing trend of the value (0.84 to 0.60) with increasing number of raters. Closer comparison highlighted more terms indirectly related to one or more category(ies) selected by 3 raters with high value of Kappa. In order to include as many terms indirectly related to the subject as possible, it was decided to apply the logic of these 3 annotators to pursue the categorization of the remaining terms.In Table 2, the results are evaluated in terms of precision (percentage of pertinent terms) obtained over the top k extracted terms (P@k). The results confirm that the ranking function of BioTex is adapted by highlighting relevant terms at the top of the list. For instance, precision value with k=100 and k=200 is high (more than 80%) but recall will be low because a lot of relevant terms are not proposed. Actually, a precise recall value is difficult to calculate because we do not have gold standard.The above detailed dataset can be found in the CIRAD Dataverse repository [4].One of the 5 specialists then pursued the annotation, with a degree of relevance, on the remaining extracted terms. It was decided to continue the categorization with the degree of pertinence and to apply the logic of the 3 annotators with the high kappa value explained above. It took about one-week work for the rater to conduct the categorization. The same five raters were then asked to verify and finalize the terms selection related to the biotransformation and valorization in agriculture of organic residues in low-income countries. All verified relevant terms were combined in the last file on the depository (Pertinent_Terms.xlsx), containing terms which can be indirectly (first sheet) or directly (second to fourth sheet) related to the topic.From the 19 580 initial candidate terms, about 75% were not associated to the topic of interest (Table 2). Irrelevant terms included words which are not related to organic residues nor biotransformation nor valorization in agriculture, such as: absence, certification, design, effect, fecundity, fitness, gray, immune response, integration analysis, low cost, marker genes, …. Among the 25% relevant terms, 2 079 were closely associated with the organic residues valorization in emerging and developing countries such as sludge, sewage, livestock, manure, slurry, anaerobic digestion, composting, vermicomposting. Several terms can be found in the glossary of terms related to livestock and manure management [9] and figure among terms with high pertinence in this dataset. Moreover, some of relevant terms are cited in literatures as the biotransformation (e.g.: anaerobic digestion, composting, bioethanol, biohydrogen) and valorization in agriculture (e.g.: biofertilization, organic fertilizers, amendments) of organic residues (e.g. rice straw, sugarcane bagasse, animal manure) [10], [11].The produced lexicon is currently used in a semantic-driven analysis of our corpus based on the CorTexT software [12]. In the context of this multidisciplinary work on-going, we obtain deeper knowledge regarding bioconversion and valorization in agriculture of organic residues in low-income countries as highlighted in Figure 1-A and B.The text-mining tool used in this work is based on statistical criteria that highlight discriminative terms. This method identifies significant terms that are present in the texts. As future work, the proposed framework could be extended by extracting variation of terms [13] that enables to recognize rare and/or unsystematic terms but also synonyms. Moreover, embedding approaches [14], language models [15], generative methods based on LLM (Large Language Models) techniques [16] could be applied to recognize new terms. Language model techniques are based on generic models like BERT -Bidirectional Encoder Representations from Transformers [15] or specific ones like AgriBERT -Knowledge-Infused Agricultural Language Models for Matching Food and Nutrition [17] dedicated to the agriculture domain. These models can be fine-tuned for specific tasks like terminology extraction.There can be used to improve terminology extraction. Note that the use of language models could be relevant for specific NLP tasks and domains like the agriculture area [18]. As future work, we plan to compare the applied methods described in this paper with other approaches based on language models but also Large Language Models (LLM) for terminology extraction [19]. LLM could also be used to expand our initial lexicon. This enables to extract variations of exiting terms and synonyms but also new terms. In the context of our work, the objective is to conduct a semantic analysis of terms present in the corpus, so the use of words or phrases in our lexicon but not used in our dataset (i.e. corpus) is not really useful. Web of science Core collection query: WOS, FSTA and Biosis TS = ("sewage sludge" OR "crop residue*" OR "agricultural waste" OR "industrial waste" OR "food waste" OR "household waste" OR "organic waste" OR "urban waste" OR "co-product*" OR "byproduct*" OR "biomass" OR "organic waste product*" OR mulch OR digestate* OR compost*) AND TS = (decomposition OR fermentation OR anaerobic OR aerobic OR methanisation OR composting OR vermicomposting OR fertilization OR bokashi OR biodegradation OR mineralization OR recycling OR "agricultural valuation" OR biotransformation OR mulching) AND TS = (africa OR "acp countries" OR "central america" OR "south america" OR "latin america" OR "south east asia" OR "south asia" OR afghanistan OR angola OR albania OR argentina OR armenia OR antigua OR azerbaijan OR burundi OR benin OR "burkina faso" OR bangladesh OR bosnia OR belarus OR belize OR bolivia OR brazil OR bhutan OR botswana OR "central african republic" OR china OR "ivory coast" OR cameroon OR congo OR colombia OR comoros OR "cape verde" OR "costa rica" OR cuba OR djibouti OR dominica OR "dominican republic" OR algeria OR ecuador OR egypt OR eritrea OR ethiopia OR fiji OR micronesia OR gabon OR georgia OR ghana OR guinea OR gambia OR grenada OR guatemala OR guyana OR honduras OR haiti OR indonesia OR india OR iran OR iraq OR jamaica OR jordan OR kazakhstan OR kenya OR kyrgyzstan OR cambodia OR kiribati OR "lao people's democratic republic" OR lebanon OR liberia OR libya OR "saint lucia" OR "sri lanka" OR lesotho OR morocco OR moldova OR madagascar OR maldives OR mexico OR "marshall islands" OR "north macedonia" OR mali OR myanmar OR montenegro OR mongolia OR mozambique OR mauritania OR montserrat OR mauritius OR malawi OR malaysia OR namibia OR niger OR nigeria OR nicaragua OR niue OR nepal OR nauru OR pakistan OR panama OR peru OR philippines OR palau OR "papua new guinea" OR " Democratic People's Republic of Korea" OR "north korea" OR paraguay OR "palestinian territory" OR rwanda OR sudan OR senegal OR "saint helena, ascension and tristan da cunha" OR "solomon islands" OR "sierra leone" OR "el Salvador" OR somalia OR serbia OR "south sudan" OR "sao tome and principe" OR suriname OR eswatini OR "syrian arab republic" OR chad OR togo OR thailand OR tajikistan OR tokelau OR turkmenistan OR "timor-leste" OR tonga OR tunisia OR turkey OR tuvalu OR tanzania OR uganda OR ukraine OR uzbekistan OR "saint vincent and the grenadines" OR venezuela OR "vietnam" OR vanuatu OR "wallis and futuna" OR samoa OR yemen OR "south africa" OR zambia OR zimbabwe). Due to a very high number obtained with the equivalent of WoS query, the following query was used for scopus with subject area=environmental sciences or agricultural. Then, only articles, reviews and conference paper were selected.TITLE-ABS-KEY ( "sewage sludge" OR "crop residue*" OR "agricultural waste" OR "industrial waste" OR "food waste" OR "household waste" OR "organic waste" OR "urban waste" OR "coproduct*" OR "by-product*" OR "biomass" OR "organic waste product*" OR mulch OR digestate* OR compost* ) AND TITLE-ABS-KEY ( decomposition OR fermentation OR anaerobic OR aerobic OR methanisation OR composting OR vermicomposting OR fertilisation OR bokashi OR biodegradation OR mineralisation OR recycling OR "agricultural valuation" OR biotransformation OR mulching ) AND ( LIMIT-TO ( SUBJAREA , "ENVI" ) OR LIMIT-TO ( SUBJAREA , "AGRI" ) ) AND ( LIMIT-TO ( DOCTYPE , "ar" ) OR LIMIT-TO ( DOCTYPE , "re" ) OR LIMIT-TO ( DOCTYPE , "cp" ) )Google scholar, HAL, Cairn.info, AGRIS:Advanced research was not available on free databases; the research was thus conducted with a general query on the topic which was:-In French : « biotransformation et valorisation en agriculture dans les contextes des pays du Sud » -In English « biotransformation et valorization in agriculture in low-income countries» in English. The query was then tested by adding Africa, Latin America, then South-East Asia ».},
  archive      = {J_FRAI},
  author       = {Rakotomalala, Christiane and Paillat, Jean-Marie and Feder, Frédéric and Avadí, Angel and Thuriès, Laurent and Vermeire, Marie-Liesse and Médoc, Jean-Michel and Wassenaar, Tom and Hottelart, Caroline and Kieffer, Lilou and Ndjie, Elisa and Picart, Mathieu and Tchamgoue, Jorel and Tulle, Alvin and Valade, Laurine and Boyer, Annie and Duchamp, Marie-Christine and Roche, Mathieu},
  doi          = {10.3389/frai.2025.1557137},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1557137},
  shortjournal = {Front. Artif. Intell.},
  title        = {A lexicon obtained and validated by a data-driven approach for organic residues valorization in emerging and developing countries},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An AI-powered framework for assessing teacher performance in classroom interactions: A deep learning approach. <em>FRAI</em>, <em>8</em>, 1553051. (<a href='https://doi.org/10.3389/frai.2025.1553051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionTeacher performance evaluation is essential for improving instructional quality and guiding professional development, yet traditional observation-based methods can be subjective, labor-intensive, and inconsistently reliable. This study proposes an AI-powered framework to objectively assess classroom interactions.MethodsWe developed and evaluated a computer-vision framework using three state-of-the-art object detectors—YOLOv8, Faster R-CNN, and RetinaNet—to identify eleven classroom interaction categories. A labeled dataset of 7,259 images collected from real classroom settings was annotated and used for training and evaluation. Performance was assessed using mean Average Precision (mAP).ResultsYOLOv8 achieved the best performance among the evaluated models, with an mAP of 85.8%, indicating strong accuracy in detecting diverse classroom interactions. Faster R-CNN and RetinaNet performed competitively but were outperformed by YOLOv8.Discussion/ConclusionThe results demonstrate that modern deep learning–based detection can provide more objective and reliable insights into teacher–student interactions than traditional approaches. The proposed framework supports evidence-based evaluation and has the potential to enhance feedback and outcomes in educational practice.].},
  archive      = {J_FRAI},
  author       = {Almubarak, Arwa and Alhalabi, Wadee and Albidewi, Ibrahim and Alharbi, Eaman},
  doi          = {10.3389/frai.2025.1553051},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1553051},
  shortjournal = {Front. Artif. Intell.},
  title        = {An AI-powered framework for assessing teacher performance in classroom interactions: A deep learning approach},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced deep convolutional neural network for SARS-CoV-2 variants classification. <em>FRAI</em>, <em>8</em>, 1512003. (<a href='https://doi.org/10.3389/frai.2025.1512003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionRapid and scalable classification of SARS-CoV-2 genomes from spike-gene sequences can support real-time genomic surveillance in contexts where whole-genome data or high-end computing resources are limited.MethodsWe curated approximately 35,800 quality-filtered spike sequences spanning multiple clades and lineages and trained a hybrid CNN–BiLSTM model with standard regularization and class-imbalance handling. Model performance was benchmarked against Nextclade assignments and compared with classical machine-learning baselines.ResultsAcross 10 experimental runs, the model achieved a mean training accuracy of 99.74% ± 0.11, a validation accuracy of 99.00% ± 0.00, and a test accuracy of 99.91% ± 0.03. In benchmarking against the molecular epidemiology tool Nextclade, our model demonstrated superior performance, correctly identifying 100% of Omicron sequences, compared to 34.95% achieved by Nextclade. Saliency and feature attribution analyses highlighted recurrent spike substitutions consistent with known variant-defining mutations, as well as additional uncharacterized motifs with potential biological relevance.DiscussionThese findings demonstrate that spike-only deep models can provide rapid and accurate clade or variant classification, while also yielding interpretable feature importance. Such models complement phylogenetic approaches in settings with constrained resources and enable efficient triage of samples for confirmatory whole-genome analysis, supporting more timely genomic surveillance.},
  archive      = {J_FRAI},
  author       = {Awe, Olaitan I. and Obura, Hesborn and Ssemuyiga, Charles and Mudibo, Evans and Mwanga, Mike J.},
  doi          = {10.3389/frai.2025.1512003},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1512003},
  shortjournal = {Front. Artif. Intell.},
  title        = {Enhanced deep convolutional neural network for SARS-CoV-2 variants classification},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global reform population health management as stewarded by higher expert medical science safety (HEMSS). <em>FRAI</em>, <em>8</em>, 1496948. (<a href='https://doi.org/10.3389/frai.2025.1496948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As described in a Memorandum of Understanding (MoU) on AI infrastructure, global human phenotype ontology (HPO) is a priority for the US and the UK. The UK NHS Act of 1946 and the Medicare and Medicaid Act of 1965 classify using genomics as primary care, supporting international HPO aims for Population Health Management (PHM). The Higher Expert Medical Science Safety (HEMSS) proposes the NHS England, Genomics, and Biobank agile group developers. The HEMSS strategy executes the PHM of the HPO through digital records, pilot citizen predictor pre-eXams, and precise eXam intercept classifications, continuously improving public safety. PHM reform includes biobank opportunities for Value-Based Care (VBC) stratifying genomic and socio-environmental factors that risk HPO in disease segmentation. The author evaluated a standard approach to PHM for HPO with mature and advanced interoperable standards. A reform toolkit aligns adversarial, neural, and transformer models for Generative AI by utilizing multimodal data nuanced for fairness in Quantum Intelligence. The recommendations include HEMSS steps from well-being evaluations to the PHM strategy for HPO in the UK-US. Concepts involve piloting the scaling up of neighborhood clinics and federal centers through reform classification. Plans for citizen privacy facilitate data use with access to reference biobanks, ensuring DNA democratization and national cybersecurity. The UK NHSE corporate governance and US federal authorities monitor and reform the Integrated Care Board assessments and the Centers for Medicare and Medicaid Services surveys using agile methods. The UK-US MoU for AI safety is an international ideal for PHM, creating a safe space for HPO adherence to predictive and interceptive adoption for health and socioeconomic growth. HEMSS Agile Group Development impacts ethical and societal primary care debates. HEMSS discussions on global public health inclusiveness and national engagement aim to govern the classification phases for adherence. Therefore, debates on UK-US accreditation or regulation on the future of Artificial General Intelligence follow. The author concludes in support of the Population Health Management Expert Medical Science Safety Agile Group Development Program. The UK and US governments would benefit from this proposition, and international goals for well-being and socioeconomic growth would also be supported.},
  archive      = {J_FRAI},
  author       = {Henry, James Andrew},
  doi          = {10.3389/frai.2025.1496948},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1496948},
  shortjournal = {Front. Artif. Intell.},
  title        = {Global reform population health management as stewarded by higher expert medical science safety (HEMSS)},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on knowledge and information extraction from PDF documents and storage approaches. <em>FRAI</em>, <em>8</em>, 1466092. (<a href='https://doi.org/10.3389/frai.2025.1466092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionAutomating the extraction of information from Portable Document Format (PDF) documents represents a major advancement in information extraction, with applications in various domains such as healthcare, law, or biochemistry. However, existing solutions face challenges related to accuracy, domain adaptability, and implementation complexity.MethodsA systematic review of the literature was conducted using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to examine approaches and trends in PDF information extraction and storage approaches.ResultsThe review revealed three dominant methodological categories: rule-based systems, statistical learning models, and neural network-based approaches. Key limitations include the rigidity of rule-based methods, the lack of annotated domain-specific datasets for learning-based approaches, and issues such as hallucinations in large language models.DiscussionTo overcome these limitations, a conceptual framework is proposed comprising nine core components: project manager, document manager, document pre-processor, ontology manager, information extractor, annotation engine, question-answering tool, knowledge visualizer, and data exporter. This framework aims to improve the accuracy, adaptability, and usability of PDF information extraction systems.},
  archive      = {J_FRAI},
  author       = {Atagong, Salvador D. and Tonnang, Henri and Senagi, Kennedy and Wamalwa, Mark and Agboka, Komi M. and Odindi, John},
  doi          = {10.3389/frai.2025.1466092},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1466092},
  shortjournal = {Front. Artif. Intell.},
  title        = {A review on knowledge and information extraction from PDF documents and storage approaches},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
