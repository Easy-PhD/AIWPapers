<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>frontiers</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="fcomp">FCOMP - 17</h2>
<ul>
<li><details>
<summary>
(2025). Machine learning-based spreading factor optimization in LoRaWAN networks. <em>FCOMP</em>, <em>7</em>, 1666262. (<a href='https://doi.org/10.3389/fcomp.2025.1666262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has experienced rapid growth and adoption in recent years, enabling applications across diverse industries, including agriculture, logistics, smart cities, and healthcare. Long Range Wide Area Network (LoRaWAN) has emerged as a leading choice among IoT communication technologies due to its long-range, low-power, and cost-effective capabilities. However, the rapid proliferation of IoT devices has intensified the challenge of efficient resource management, particularly in spreading factor (SF) allocation for LoRaWAN networks. In this paper, we propose a Machine Learning-based Adaptive Data Rate (ML-ADR) approach for SF management to address this issue. A Long Short-Term Memory (LSTM) network was trained on a dataset generated using ns-3 for optimal SF classification. The pre-trained LSTM model was then utilized on the end-device side for efficient SF allocation with newly generated data during simulation. The results demonstrate improved packet delivery ratios and reduced energy consumption.},
  archive      = {J_FCOMP},
  author       = {Nisar, Farhan and Amin, Muhammad and Touseef Irshad, Muhammad and Hadi, Hassan Jalil and Ahmad, Naveed and Ladan, Mohamad},
  doi          = {10.3389/fcomp.2025.1666262},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1666262},
  shortjournal = {Front. Comput. Sci.},
  title        = {Machine learning-based spreading factor optimization in LoRaWAN networks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brownian motion models: Cryptographic applications, capabilities, and limitations. <em>FCOMP</em>, <em>7</em>, 1649256. (<a href='https://doi.org/10.3389/fcomp.2025.1649256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brownian motion (BM) is a stochastic model that has been extensively studied in physics, finance, and engineering. However, its potential use in cryptographic applications remains underexplored. This paper presents a comprehensive review of the capabilities, limitations, and cryptographic properties of various BM models, including the Wiener process, geometric BM, fractional BM, Ornstein–Uhlenbeck process, multidimensional BM, and reflected BM. We reviewed the mathematics of these models, simulate their random evolutions, and compare their cryptanalytic properties. A comparison of these sources highlights unique characteristics that can provide cryptographic resilience, including long-range dependence, multidimensional modeling of noise, and constraints on randomness. We also describe the main limitations and potential weaknesses of each model. This paper addresses gaps in the application of stochastic process to cryptographic design and provides a foundational guideline for the continued development of secure systems based on Brownian dynamics.},
  archive      = {J_FCOMP},
  author       = {Zainalabideen, Abduljameel and Suwais, Khaled and El-Bakry, Hazem and Abdelmaksoud, Islam},
  doi          = {10.3389/fcomp.2025.1649256},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1649256},
  shortjournal = {Front. Comput. Sci.},
  title        = {Brownian motion models: Cryptographic applications, capabilities, and limitations},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explainable digital twin framework for skin cancer analysis using early activation meta-learner. <em>FCOMP</em>, <em>7</em>, 1646311. (<a href='https://doi.org/10.3389/fcomp.2025.1646311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is among the most common cancers globally, which calls for timely and precise diagnosis for successful therapy. Conventional methods of diagnosis, including dermoscopy and histopathology, are significantly dependent on expert judgment and therefore are time-consuming and susceptible to inconsistencies. Deep learning algorithms have shown potential in skin cancer classification but tend to consume a substantial amount of computational resources and large training sets. To overcome these issues, we introduce a new hybrid computer-aided diagnosis (CAD) system that integrates Stem Block for feature extraction and machine learning for classification. The International Skin Imaging Collaboration (ISIC) skin cancer dermoscopic images were collected from Kaggle, and essential features were collected from the Stem Block of a deep learning (DL) algorithm. The selected features, which were standardized using StandardScaler to achieve zero mean and unit variance, were then classified using a meta-learning classifier to enhance precision and efficiency. In addition, a digital twin framework was introduced to simulate and analyze the diagnostic process virtually, enabling real-time feedback and performance monitoring. This virtual replication aids in continuous improvement and supports the deployment of the CAD system in clinical environments. To improve transparency and clinical reliability, explainable artificial intelligence (XAI) methods were incorporated to visualize and interpret model predictions. Compared to state-of-the-art approaches, our system reduced training complexities without compromising high classification precision. Our proposed model attained an accuracy level of 96.25%, demonstrating its consistency and computationally efficient status as a screening tool for detecting skin cancer.},
  archive      = {J_FCOMP},
  author       = {Sampath, Pradeepa and S., Gopika and Vimal, S. and Kang, Yoonje and Seo, Sanghyun},
  doi          = {10.3389/fcomp.2025.1646311},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1646311},
  shortjournal = {Front. Comput. Sci.},
  title        = {An explainable digital twin framework for skin cancer analysis using early activation meta-learner},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient rotation invariance in deep neural networks through artificial mental rotation. <em>FCOMP</em>, <em>7</em>, 1644044. (<a href='https://doi.org/10.3389/fcomp.2025.1644044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans and animals recognize objects irrespective of the beholder's point of view, which may drastically change their appearance. Artificial pattern recognizers strive to also achieve this, e.g., through translational invariance in convolutional neural networks (CNNs). However, CNNs and vision transformers (ViTs) both perform poorly on rotated inputs. Here we present AMR (artificial mental rotation), a method for dealing with in-plane rotations focusing on large datasets and architectural flexibility, our simple AMR implementation works with all common CNN and ViT architectures. We test it on randomly rotated versions of ImageNet, Stanford Cars, and Oxford Pet. With a top-1 error (averaged across datasets and architectures) of 0.743, AMR outperforms rotational data augmentation (average top-1 error of 0.626) by 19%. We also easily transfer a trained AMR module to a downstream task to improve the performance of a pre-trained semantic segmentation model on rotated CoCo from 32.7 to 55.2 IoU.},
  archive      = {J_FCOMP},
  author       = {Tuggener, Lukas and Stadelmann, Thilo and Schmidhuber, Jürgen},
  doi          = {10.3389/fcomp.2025.1644044},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1644044},
  shortjournal = {Front. Comput. Sci.},
  title        = {Efficient rotation invariance in deep neural networks through artificial mental rotation},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital anthropomorphism and the psychology of trust in generative AI tutors: An opinion-based thematic synthesis. <em>FCOMP</em>, <em>7</em>, 1638657. (<a href='https://doi.org/10.3389/fcomp.2025.1638657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methodological Approach This article is an opinion-based conceptual piece that draws on a targeted selection of peer-reviewed sources to develop a conceptual discussion on digital anthropomorphism in generative AI tutors. To ground our argument in current scholarship, we searched Google Scholar, Scopus, and Web of Science for literature published between 2019 and 2025, using terms such as "AI trust," "digital anthropomorphism," and "generative AI in education." We focused on works that explicitly addressed human–AI interaction, trust psychology, or anthropomorphism in educational contexts, and excluded purely technical studies and noneducational applications. Approximately 45 relevant papers were identified. Rather than conducting a systematic review, we engaged in an informal thematic grouping of recurring ideas— such as perceived authority, emotional reassurance, automation bias, and epistemic vigilance— which informed the structure of this article. The aim here is not to provide exhaustive coverage, but to integrate converging insights from cognitive psychology, human–computer interaction, and educational technology into a coherent, opinion-driven perspective on trust calibration in AI-mediated learning. Introduction: When the Machine Feels Human Today's students interact more with generative AI tools like ChatGPT, Claude, and Google Gemini as conversational partners rather than as disembodied software. When these systems respond with fluency, politeness, and encouragement, they create a subtle but potent illusion: the AI appears to "understand" the user (Cohn et al., 2024; Karimova & Goby, 2020). This phenomenon, known as digital anthropomorphism, leads students to attribute human-like qualities—such as empathy, intelligence, and trustworthiness—to non-human systems (Jensen, 2021; Placani, 2024). This article offers a conceptual, opinion-based synthesis of recent peer-reviewed literature on this topic, drawing on insights from cognitive psychology, human–computer interaction, and educational technology. Our aim is not to provide an exhaustive or systematic review but to integrate converging findings into a coherent framework for understanding trust calibration in AI-mediated education. We structure the discussion around the conceptual pathway illustrated in Figure 1, which traces how anthropomorphic design cues may foster affective trust, reduce epistemic vigilance, and influence learner dependency, while also considering contexts in which anthropomorphism can enhance engagement and confidence when ethically designed. The Cognitive Basis of Digital Anthropomorphism Digital anthropomorphism is not a failure of rationality, but rather a manifestation of human social cognition (Fakhimi et al., 2023). Developmental psychology has demonstrated that even children ascribe intention and moral status to animated forms if they move in goal-oriented manners. Adults too habitually treat chatbots, GPS, and voice assistants as being quasi-social actors—to thank them, apologize, or obey their instructions. Generative AI amplifies this impact with linguistic anthropomorphism. Its natural language proficiency activates people's social brain mechanisms— soliciting empathy, engagement, and even perceived moral agency (Alabed et al., 2022; Q. Chen & Park, 2021). Human-computer interaction studies show that individuals are more willing to take advice from a friendly, courteous chatbot than from a direct or technical interface, even when the information is the same. This has its roots in what Clifford Nass called the "media equation": the hypothesis that people treat computers and media as if they were actual people and places. The conversational AI's design—affirmative statements, natural turns, emotional tone—invokes this illusion more powerfully than any earlier model of education technology (Inie et al., 2024). As outlined in Figure 1, these interface features can initiate a sequence from perceived empathy and authority to emotional trust, which may, in turn, lower epistemic vigilance. In the teaching environment, this has significant implications. A student made to feel "helped" or "seen" through interaction with an AI tutor is more likely to feel motivated and emotionally secure—but less apt to scrutinize the system's correctness and fairness. Its emotional trust will countervail the critical examination of the user, even when the AI tutor emulates reassurance and confidence (Chinmulgund et al., 2023). While these tendencies are well-documented in human–computer interaction and consumer research, their expression in formal educational settings is likely to be context-dependent. Factors such as learners' age, subject matter, prior exposure to AI, and cultural norms may moderate the strength of anthropomorphic responses. In this article, we treat such effects as plausible tendencies supported by adjacent literature, rather than as universal outcomes, and highlight the need for empirical validation within classroom environments. Perceived Authority and the Illusion of Understanding Belief in AI tutors is frequently influenced through perceived epistemic authority. When an AI system provides clear, assertive, and technical definitions, students might conclude that "it knows" as a human expert might. But AI systems do not know—they respond based upon statistical relationships, not conceptual understanding. This pretense of knowledge is a pernicious epistemic trap (Lalot & Bertram, 2024). It causes learners to take AI responses as authoritative, particularly when they have no pre-existing knowledge to analyze them with. Additionally, if AI response is written in didactic pedagogical tones or affective supportive tones, it reinforces the image of a wise and well-meaning tutor (Troshani et al., 2020). Educational psychology experiments demonstrate that students often rate feedback as more useful when delivered with confidence, even if the information is inaccurate. This link between confident tone and perceived expertise represents a plausible mechanism consistent with experimental findings in both educational and broader HCI contexts (Lalot & Bertram, 2024; Troshani et al., 2020), though its generalizability to all classroom settings remains to be confirmed. Such a 'confidence heuristic' is problematic when used with AI systems trained to optimize fluency and not epistemic truth. This aligns with findings by Atf and Lewis (2025), who demonstrate that user trust in AI systems is often driven by surface fluency and not correlated with explainability, especially in educational domains(Maeda, 2025). Figure 1. Conceptual synthesis — Psychological Pathway Linking Digital Anthropomorphism to Epistemic Vulnerability in AI-Mediated Learning. This diagram illustrates how interface design features that evoke human-like qualities can lead to affective trust, which in turn may reduce learners' epistemic vigilance, resulting in over-reliance, diminished critical thinking, and role confusion. Note: This is a conceptual synthesis derived from the thematic literature review and is not an empirically estimated model. Trust, Dependency, and the Erosion of Epistemic Vigilance From a psychological perspective, trust in learning is both required and dangerous. Students need to trust instructors to direct them, but they must also cultivate epistemic vigilance—the capacity to evaluate the believability of information sources. When students anthropomorphize AI tutors, their epistemic filters could weaken. Emotional trust in AI can be expressed as: • Over-reliance on AI feedback over teacher guidance. • Inadequate effort to cross-check or challenge AI-produced responses. • Acceptance of imperfect or slanted results, particularly if they come with persuasive voice (A. Chen & Wan, 2023). These tendencies are echoed in research about automation bias—the tendency to over-rely on machines even when their projections contradict good sense. When AI-mediated learning takes place, this is the way it has the ability to bring about lower levels of something called self-efficacy, less critical thinking, and dependence upon external feedback. And students tend to feel a kind of role confusion. When the AI is perceived as supportive, affectively responsive, and all-knowing, the student is apt to take on a receiving role, sacrificing their cognitive agency. Losing watchfulness is not only cognitive—it is emotional. When the machine comes across as friendly, students feel guilty questioning it. When the machine provides speedy responses, they feel impatient with complex questions. While such emotional reactions have been observed anecdotally in educational technology contexts, systematic empirical evidence for these specific effects in AI tutoring environments is still emerging. We therefore present these as conceptual extrapolations, grounded in related work on social responses to media and automation bias (Pergantis et al., 2025; Ryan, 2020), rather than as universally established findings. This quiet process from doubt to submission is a pivotal moment in the psychology of trust (Ryan, 2020). This accords with Pergantis et al.'s (2025) research, which shows that extensive AI interactions have the potential to move cognitive control processes underlying autonomous learning. Even though such flaws require close analysis, no less true is the fact that anthropomorphic indicators have, in certain scenarios, the potential to render useful pedagogical roles if appropriately and responsibly conceptualized. Productive Anthropomorphism and Ethical Design Although much of the debate about anthropomorphism in AI tutors centers on its possible dangers, it is valuable to note that human-like signals can have positive teaching outcomes as well, when implemented sensitively. Anthropomorphic design features can improve students' engagement, minimize feelings of loneliness in online classrooms, and give emotional comfort to students who are anxious or self-doubting. For instance, learners who have mathematical anxiety or who have limited exposure to human tutors may respond positively to an AI tutor's persistent, nonjudgmental feedback (Polydoros et al., 2025). Others who are shy or socially fearful may be more at ease conversing with an amicable AI interface than with colleagues or teachers in live classes. Ethical calibration is the answer: balancing motivational advantages of anthropomorphism with characteristics that maintain critical thinking and epistemic vigilance. Characteristics like that may be achieved through the use of transparency prompts, source citations that are visible, and infrequent "reflection nudges" which get students to stop and double-check information. In combination with instructional guidance, design strategies like these hold promise for making anthropomorphic cues function as a learning scaffold, not a shortcut to mere passive acceptance. Toward a Psychology of Critical Trust in AI Tutors To enter this psychological territory of anthropomorphism in the digital age, teachers must encourage students to cultivate critical trust—a mindset to be open to the affordances, yet cautious about the limits, of AI. Even technical literacy won't suffice; psychological sensitivity is needed (Mulcahy et al., 2023). Educational interventions might include: ➢ AI debriefs: Short reflection exercises that get students to present an AI-generated response that was utilized and answer three guiding questions: (1) What was the chief argument of the AI? (2) What sources, if any, did it reference? (3) How did you test or refute it? This helps students be mindful of their uses of AI intentionally. ➢ Counter-anthropomorphism exercises: Students reword an AI's polite, human-sounding response in purely technical terms, removing social signals. This helps students contrast how tone and style affect their perception of authority and reliability. ➢ Trust calibration training: Checklists or short classroom protocols that encourage students to ask, before accepting an AI's response: (1) Is there a legitimate source? (2) Is my explanation consistent with my prior knowledge? (3) Have I checked it elsewhere? This training induces the habit of separating interface ease from epistemic reliability. Educators can model critical trust through transparent and explainable use of AI in class, revealing its benefits and its limitations. Guided classroom debates about issues like algorithm bias, hallucinations, and surface fluency versus deep knowledge can "immunize" students against excessive faith. Classroom activities that engage students in collaborative tasks can further erode passive dependence: for instance, group debates where students are asked to argue against an answer generated by an AI, or collaborative projects where human and AI readings of the same content are evaluated side by side for nuance, tone, and cultural reference. These exercises tie directly to earlier interventions like AI debriefs, counter-anthropomorphizing, and calibration of trust, building upon them through active exercise. In the long run, establishing critical trust may even necessitate interface redesigns—with features like visible source quotation, easy-to-understand explainability tools, and interactive prompting that invite reflection before accepting an AI's answer. Research Pathways for Calibrating Trust in Generative AI Tutors Future research should explore the psychology of anthropomorphism in AI tutors across diverse educational contexts (Létourneau et al., 2025). We propose two complementary tracks: Track A – Affective Trust Calibration ❖ Investigate how learners distinguish between the emotional tone and epistemic validity of AI responses. ❖ Test interventions such as meta-cognitive prompts, counter-anthropomorphism training, and AI explanation auditing to determine their effectiveness in sustaining critical vigilance (Chakraborty et al., 2024; Israfilzade & Sadili, 2024). ❖ Explore the impact of interface features (e.g., source citations, uncertainty indicators, reflection nudges) on trust calibration over time. Track B – Population and Context Variance ❖ Examine differences in anthropomorphic responses across developmental stages, from adolescents with still-developing critical thinking skills to adult learners. ❖ Assess the unique effects on language learners, students with math anxiety, and those with varying degrees of self-confidence (Polydoros et al., 2025). ❖ Investigate how neurodiverse learners respond to AI tutors — identifying where consistent feedback supports learning versus where the absence of genuine empathy may hinder it. ❖ Anticipate the effects of multimodal AI (voice, facial expressions, haptics) on perceptions of agency, authority, and moral status. Pursuing these research tracks will help identify how to leverage the motivational benefits of anthropomorphism while minimizing the risks of epistemic over-reliance. Such insights are essential for co-designing ethical AI systems that inform, augment, and empower learners without compromising intellectual autonomy. Limitations and Scope Limitations and Scope. This article is presented as an opinion-based conceptual synthesis rather than a systematic review or empirical study. The thematic grouping of sources reflects a targeted but non-exhaustive selection of peer-reviewed literature published between 2019 and 2025. While many of the mechanisms discussed—such as automation bias, trust heuristics, and the influence of anthropomorphic cues—are supported by existing studies in related domains, some affective and behavioral claims are hypotheses requiring further empirical validation in classroom contexts. Findings and interpretations should therefore be considered context-dependent and provisional, intended to inform ongoing scholarly and design conversations rather than to offer definitive causal conclusions. Conclusion: Learning with the Non-Human Other Generative AI is not a neutral tool. Its linguistic fluency, affective tone, and interactive style are designed to mimic human-like interactivity, eliciting anthropomorphic responses from students who may greet AI tutors as intelligent guides, caring listeners, or moral figures (Hossain & Islam, 2024; Sarfaraj1, 2025). Such responses can enrich the learning experience when they foster motivation, confidence, and a sense of social presence (Polydoros et al., 2025). However, they also carry the risk of distorting teacher–student dynamics and encouraging uncritical trust (Vanneste & Puranam, 2024; Yuan & Hu, 2024). The challenge is not to eliminate trust in AI tutors but to calibrate it—ensuring that trust is informed, tentative, and tempered by awareness of the system's non-human constraints (Okamura & Yamada, 2020). This means leveraging the productive aspects of anthropomorphism while embedding safeguards such as transparency features, reflection prompts, and guided debriefs that preserve epistemic vigilance (Chakraborty et al., 2024; Mulcahy et al., 2023). In an algorithmically mediated educational future, the goal is to develop learners who can recognize when AI offers valuable support and when its persuasive surface masks the need for independent reasoning. Ultimately, critical trust allows students to use AI as a partner in learning without surrendering their intellectual autonomy (Ryan, 2020).},
  archive      = {J_FCOMP},
  author       = {Jose, Binny and Thomas, Angel},
  doi          = {10.3389/fcomp.2025.1638657},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1638657},
  shortjournal = {Front. Comput. Sci.},
  title        = {Digital anthropomorphism and the psychology of trust in generative AI tutors: An opinion-based thematic synthesis},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From shades to vibrance: A comprehensive review of modern image colorization techniques. <em>FCOMP</em>, <em>7</em>, 1626641. (<a href='https://doi.org/10.3389/fcomp.2025.1626641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image colorization has become a significant task in computer vision, addressing the challenge of transforming grayscale images into realistic, vibrant color outputs. Recent advancements leverage deep learning techniques, ranging from generative adversarial networks (GANs) to diffusion models, and integrate semantic understanding, multi-scale features, and user-guided controls. This review explores state-of-the-art methodologies, highlighting innovative components such as semantic class distribution learning, bidirectional temporal fusion, and instance-aware frameworks. Evaluation metrics, including PSNR, FID, and task-specific measures, ensure a comprehensive assessment of performance. Despite remarkable progress, challenges like multimodal uncertainty, computational cost, and generalization remain. This paper provides a thorough analysis of existing approaches, offering insights into their contributions, limitations, and future directions in automated image colorization.},
  archive      = {J_FCOMP},
  author       = {Geenath, Oshen and Priyadarshana, Y. H. P. P.},
  doi          = {10.3389/fcomp.2025.1626641},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1626641},
  shortjournal = {Front. Comput. Sci.},
  title        = {From shades to vibrance: A comprehensive review of modern image colorization techniques},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLaVA-GM: Lightweight LLaVA multimodal architecture. <em>FCOMP</em>, <em>7</em>, 1626346. (<a href='https://doi.org/10.3389/fcomp.2025.1626346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal large-scale language modeling has become the mainstream approach in natural language processing tasks and has been applied to various cross-modal fields such as image description and visual question answering. However, large-scale language modeling has high computational complexity and a large operational scale, which presents significant challenges for deployment in many resource-constrained scenarios. To address such problems, a lightweight multimodal framework, LLaVA-GM, is proposed, based on LLaVA, which can be deployed on devices with low resource requirements and has greatly reduced model parameters. It can also be tested on common VQA tasks and achieves good performance. The main contributions and work are as follows: First, it is found that the backbone of the Vicuna language model in LLaVA is too redundant. When fine-tuning downstream tasks, a very small amount of data sets is difficult to affect the language model. It is replaced with a new Gemma language model, thereby achieving fast task-specific adaptation with fewer parameters and data. Second, in response to the problem of information redundancy, the MoE mixed expert model is introduced. This model can be used in combination with itself, combining the MoE mixed expert model with Gemma to reduce the amount of computation while maintaining performance. Directly training the entire model will lead to a decline in performance. A multi-stage training strategy is adopted to maintain performance. First, the MLP layer is trained for visual adaptation, then the entire Gemma model is trained to improve multimodal capabilities, and finally only the MoE layer is trained for sparsification to ensure a smooth transition from dense models to sparse models. The experiment was tested on multiple VQA datasets and achieved good performance, confirming the potential of this compact model in downstream multimodal applications.},
  archive      = {J_FCOMP},
  author       = {Han, Zhiyin and Liu, Xiaoqun and Hao, Juan},
  doi          = {10.3389/fcomp.2025.1626346},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1626346},
  shortjournal = {Front. Comput. Sci.},
  title        = {LLaVA-GM: Lightweight LLaVA multimodal architecture},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An application layer with protocol-based java smart contract verification. <em>FCOMP</em>, <em>7</em>, 1596804. (<a href='https://doi.org/10.3389/fcomp.2025.1596804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contracts are software that runs in blockchain and expresses the rules of an agreement between parties. An incorrect smart contract might allow blockchain users to violate its rules and even jeopardize its expected security. Smart contracts cannot be easily replaced to patch a bug since the nature of contracts requires them to be immutable. More problems occur when a smart contract is written in a general-purpose language, such as Java, whose executions, in a blockchain, could hang the network, break consensus or violate data encapsulation. To limit these problems, there exist automatic static analyzers that find bugs before smart contracts are installed in the blockchain. This so-called off-chain verification is optional because programmers are not forced to use it. This paper presents a general framework for the verification of smart contracts, instead, that is part of the protocol of the nodes and applies when the code of the smart contracts gets installed. It is a mandatory entry filter that bans code that does not abide by the verification rules. Consequently, such rules become part of the consensus rules of the blockchain. Therefore, an improvement in the verification protocol entails a consensus update of the network. This paper describes an implementation of a smart contracts application layer with protocol-based verification for smart contracts written in the Takamaka subset of Java, that filters only those smart contracts whose execution in blockchain is not dangerous. This application layer runs on top of a consensus engine such as Tendermint and its derivatives Ignite and CometBFT (proof of stake), or Mokamint (proof of space). This paper provides examples of actual implementations of verification rules that check if the smart contracts satisfy some constraints required by the Takamaka language. This paper shows that protocol-based verification works and reports how consensus updates are implemented. It shows actual experiments as well as limits to its use, mainly related to the fact that protocol-based verification must be fast and its complexity must never explode, or otherwise, it would compromise the performance of the blockchain network.},
  archive      = {J_FCOMP},
  author       = {Olivieri, Luca and Spoto, Fausto and Tagliaferro, Fabio},
  doi          = {10.3389/fcomp.2025.1596804},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1596804},
  shortjournal = {Front. Comput. Sci.},
  title        = {An application layer with protocol-based java smart contract verification},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud security and authentication vulnerabilities in SOAP protocol: Addressing XML-based attacks. <em>FCOMP</em>, <em>7</em>, 1595624. (<a href='https://doi.org/10.3389/fcomp.2025.1595624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis research addresses the security weaknesses in SOAP-based web services, with a particular focus on authentication vulnerabilities resulting from XML-based attacks, such as Signature Wrapping or Replay Attacks. With an emphasis on the fact that an increasing number of cloud services are utilizing SOAP, this study aims to develop a formally verified model that can more effectively identify and address these vulnerabilities.MethodWe propose and execute a TulaFale-based verification framework that formally models SOAP authentication scenarios by introducing the standard constructs, UsernameToken, Timestamp, and X.509 digital certificates. These scripts are transformed into the applied pi-calculus and verified using the ProVerif verification tool to check for properties such as authentication, confidentiality, and message integrity.ResultsBy examining XML web services security problems and consulting with security professionals, a number of key risks were identified and discussed. The research contributes to developing a comprehensive language design for cloud security and vulnerabilities using Blanchet’s ProVerif. A controlled experimental testbed was set up to emulate client–server SOAP communication streams and to evaluate the model’s effectiveness in identifying an XML-based attack performed on the web services security framework. The framework was experimentally examined for verification time and scalability for concurrency, and for accuracy of identification. The results confirmed our success in identifying attack patterns and confirming secure message exchanges built to the standards set by WS-Security.DiscussionThe proposed approach addresses and allows for the addition of automated, formal verification to realistic SOAP deployments. By modeling and verifying a security protocol before the deployment, developers can be confident that their implementation is resilient against protocol-level vulnerabilities, improving the trust in the security of web services deployed within cloud applications.},
  archive      = {J_FCOMP},
  author       = {Saeed, Mozamel M.},
  doi          = {10.3389/fcomp.2025.1595624},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1595624},
  shortjournal = {Front. Comput. Sci.},
  title        = {Cloud security and authentication vulnerabilities in SOAP protocol: Addressing XML-based attacks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cyborg synchrony: Integrating human physiology into affective generative music AI. <em>FCOMP</em>, <em>7</em>, 1593905. (<a href='https://doi.org/10.3389/fcomp.2025.1593905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) systems become increasingly integrated into human social environments, their ability to foster meaningful interaction remains an open challenge. Building on research showing that physiological synchrony relates to social bonding, we speculate about an interpersonal musical biofeedback system that promotes synchrony by allowing users to attune to each other’s physiological rhythms. We propose a two‑stage AI training framework for an interpersonal musical biofeedback system: (1) a Foundational Model trained on diverse listeners’ physiological responses to a music library, and (2) Individualized Tuning, where the system fine-tunes itself to each user’s unique physiological responses. By analyzing musical features alongside real-time physiological responses, the proposed system illustrates a feasible architecture for dynamically personalizing music to facilitate nonverbal, embodied communication between users. This approach highlights the potential for AI-driven personalization that move beyond individual optimization to actively enhance physiological synchrony which may promote deeper emotional bonding and expand the role of music as a medium for AI-mediated social connections.},
  archive      = {J_FCOMP},
  author       = {Ng, Senaida and Sargent, Kaia and Snell, Jason J.},
  doi          = {10.3389/fcomp.2025.1593905},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1593905},
  shortjournal = {Front. Comput. Sci.},
  title        = {Cyborg synchrony: Integrating human physiology into affective generative music AI},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EduScrum meets focUS: A computer-assisted training to promote self-regulation skills in higher education. <em>FCOMP</em>, <em>7</em>, 1593889. (<a href='https://doi.org/10.3389/fcomp.2025.1593889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the ever-evolving demands of the professional world, higher education plays a vital role in equipping students with strategies for self-organized and sustainable skill development. This enables students to quickly and independently adapt to new knowledge and skills throughout their careers. Therefore, it is of the essence to integrate methods that enhance self-regulation skills into our study programs, alongside the instruction of specific subject-matter expertise. Addressing these demands, we introduce a focused training that embeds the assistive software tool focUS into a structured seminar concept, leveraging eduScrum elements and accompanying learning communities. After introducing the results of a pilot evaluation with a small group of interdisciplinary doctoral students, we discuss possibilities of technical and conceptual integration in course curricula and learning counseling in higher education. Taken together, our approach indicates value for empowering future professionals across various domains to unleash their full potential.},
  archive      = {J_FCOMP},
  author       = {Schmitz-Hübsch, Alina and Bareiß, Laura and Jahn, Eva and Wirzberger, Maria},
  doi          = {10.3389/fcomp.2025.1593889},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1593889},
  shortjournal = {Front. Comput. Sci.},
  title        = {EduScrum meets focUS: A computer-assisted training to promote self-regulation skills in higher education},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research AI: Integrating AI and gamification in higher education for e-learning optimization and soft skills assessment through a cross-study synthesis. <em>FCOMP</em>, <em>7</em>, 1587040. (<a href='https://doi.org/10.3389/fcomp.2025.1587040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe integration of Artificial Intelligence (AI) and gamification into higher education is reshaping educational practices by personalizing learning and fostering essential workforce skills. This study critically examines the effectiveness of these technologies, their impact on student engagement, and the factors influencing students’ acceptance.MethodsA systematic literature review complemented by Topic Modeling using Latent Dirichlet Allocation (LDA) identified key research themes. Subsequently, predictive modeling with machine learning algorithms, hyperparameter optimization, and Local Interpretable Model-Agnostic Explanations (LIME) were applied to classify academic documents and interpret influential factors.ResultsFindings indicate that AI effectively customizes educational pathways, enhancing engagement and academic performance. Gamification notably supports soft skill development, providing more interactive assessments than traditional approaches. However, challenges related to data privacy and technological accessibility remain significant, particularly affecting international students and institutions with limited resources.DiscussionAI and gamification demonstrate considerable potential for transforming higher education through personalized learning and interactive skill assessments. Nevertheless, widespread adoption depends on addressing data privacy concerns and ensuring technological equity. Future research should investigate the long-term implications of these technologies in developing students’ adaptability within a dynamic global workforce.},
  archive      = {J_FCOMP},
  author       = {Marengo, Agostino and Pagano, Alessandro and Lund, Brady and Santamato, Vito},
  doi          = {10.3389/fcomp.2025.1587040},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1587040},
  shortjournal = {Front. Comput. Sci.},
  title        = {Research AI: Integrating AI and gamification in higher education for e-learning optimization and soft skills assessment through a cross-study synthesis},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous ensemble learning: Modified ConvNextTiny for detecting molecular expression of breast cancer on standard biomarkers. <em>FCOMP</em>, <em>7</em>, 1569017. (<a href='https://doi.org/10.3389/fcomp.2025.1569017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the highest-ranking type of cancer, with 2.3 million new cases diagnosed each year. Immunohistochemistry (IHC) is the gold standard “examination” for determining the expression of cancer malignancies in patients with the ultimate goal of determining prognosis and therapy. Immunohistochemistry refers to the four WHO standard biomarkers: estrogen receptor, progesterone receptor, human epidermal growth factor receptor-2, and Ki-67. These biomarkers are assessed based on the quantity of cell nuclei and the intensity of brown cell membranes. Our study aims to detect the expression of breast cancer malignancy as an initial step in determining prognosis and therapy. We implemented homogeneous and heterogeneous ensemble learning models. The homogeneous ensemble learning model uses the majority vote technique to select the best performance between the Xception, ResNet50V2, InceptionResNet50V2, and ConvNextTiny models. The heterogeneous ensemble learning model takes the ConvNextTiny model as the best model. Feature engineering in ConvNextTiny combines convolution and cell-quantification features as feature fusion. ConvNextTiny, which applies feature fusion, can detect the expression of cancer malignancy. Heterogeneous ensemble learning outperforms homogeneous ensemble learning. The model performs well for accuracy, precision, recall, F1-score, and receiver operating characteristic-area under the curve (ROC-AUC) of 0.997, 0.973, 0.991, 0.982, and 0.994, respectively. These results indicate that the model can classify the malignancy expressions of breast cancer well. This model still requires the configuration of the visual laboratory device to test the real-time model capabilities.},
  archive      = {J_FCOMP},
  author       = {Intan, Indo and Karnyoto, Andrea Stevens and Harlina, Sitti and Nelwan, Berti Julian and Setiawan, Devin and Yamin, Amalia and Puspitasari, Ririn Endah},
  doi          = {10.3389/fcomp.2025.1569017},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1569017},
  shortjournal = {Front. Comput. Sci.},
  title        = {Heterogeneous ensemble learning: Modified ConvNextTiny for detecting molecular expression of breast cancer on standard biomarkers},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regional computing for VBD offloading in next-generation vehicular networks. <em>FCOMP</em>, <em>7</em>, 1564270. (<a href='https://doi.org/10.3389/fcomp.2025.1564270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of autonomous and Connected Vehicles (CVs) has led to a massive increase in Vehicular Big Data (VBD). While this data is transforming the Intelligent Transportation System (ITS), it also poses significant challenges in processing, communication, and resource scalability. Existing cloud solutions offer scalable resources; however, incur long delays and costs due to distant data communication. Conversely, edge computing reduces latency by processing data closer to the source; however, struggles to scale with the high volume and velocity of VBD. This paper introduces a novel Regional Computing (RC) paradigm for VBD offloading, with a key focus on adapting to traffic variations during peak and off-peak hours. Situated between edge and cloud layers, the RC layer enables near-source processing while maintaining higher capacity than edge or fog nodes. We propose a dynamic offloading algorithm that continuously monitors workload intensity, network utilization, and temporal traffic patterns to smartly offload tasks to the optimal tier (vehicle, regional, or cloud). This strategy ensures responsiveness across fluctuating conditions while minimizing delay, congestion, and energy consumption. To validate the proposed architecture, we develop a custom Python-based simulator, RegionalEdgeSimPy, specifically designed for VBD scenarios. Simulation results demonstrate that the proposed framework significantly reduces processing latency, energy usage, and operational costs compared to traditional models, offering a scalable and effective alternative for next-generation vehicular networks.},
  archive      = {J_FCOMP},
  author       = {Badshah, Afzal and Alsahfi, Tariq and Alesawi, Sami and Alfakeeh, Ahmed and Bukhari, Amal and Daud, Ali},
  doi          = {10.3389/fcomp.2025.1564270},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1564270},
  shortjournal = {Front. Comput. Sci.},
  title        = {Regional computing for VBD offloading in next-generation vehicular networks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on performance comparisons of different types of DevOps team formations. <em>FCOMP</em>, <em>7</em>, 1554299. (<a href='https://doi.org/10.3389/fcomp.2025.1554299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionDespite all the efforts to successfully implement DevOps practices, principles, and cultural change, there is still a lack of understanding on how DevOps team structure formation and performance differences are related. The lack of a ground truth for DevOps team structure formation and performance has become a persistent and relevant problem for companies and researchers.MethodsIn this study, we propose a framework for DevOps team Formation–Performance and conduct a survey to examine the relationships between team formations and performance with the five metrics we identified, two of which are novel. We conducted an empirical study using a survey to gather data. We employed targeted outreach on a social media platform along via a snowball sampling and sent 380 messages to DevOps professionals worldwide. This approach resulted in 122 positive responses and 105 completed surveys, achieving a 69.7% response rate from those who agreed to participate.ResultsThe research shows that implementing the DevOps methodology enhances team efficiency across various team structures, with the sole exception of “Separate Development and Operation teams with limited collaboration”. Moreover, the study reveals that all teams experienced improvements in Repair/Recovery performance metric following DevOps adoption. Notably, the “Separate Development and Operation teams with high collaboration” formation emerged as the top performer in the key metrics, including Deployment Frequency, Number of Incidents, and Number of Failures/Service Interruptions. The analysis further indicates that different DevOps organizational formations do not significantly impact Lead Time, Repair/Recovery, and Number of Failures/Service Interruptions in terms of goal achievement. However, a statistically significant disparity was observed between “Separate Development and Operation teams with high collaboration” and “A single team formation” regarding the Deployment Frequency goal achievement percentage.DiscussionThe analysis confirms that DevOps adoption improves performance across most team formations, with the exception of “Separate Development and Operation teams with limited collaboration” (TeamType1), which shows significant improvement only in Mean Time to Recovery (MTTR). Standardized effect size calculations (Cohen’s d) reveal that TeamType2 (“Separate Development and Operation teams with high collaboration”) consistently achieves large effects in Deployment Frequency (DF), Number of Incidents (NoI), and Number of Failures/Service Interruptions (NoF/NoSI), while TeamType3 shows strong results for Lead Time (LT) and NoF/NoSI. MTTR improvements are large across all formations, with TeamType4 performing best in this metric. These findings suggest that collaboration intensity is a critical determinant of performance gains. While team formation type does not significantly influence LT, MTTR, or NoF/NoSI goal achievement, DF goal achievement is significantly higher for TeamType2 compared to TeamType4, highlighting the potential competitive advantage of high-collaboration structures.},
  archive      = {J_FCOMP},
  author       = {Korkmaz, Halil Ergun and Aydin, Mehmet Nafiz},
  doi          = {10.3389/fcomp.2025.1554299},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1554299},
  shortjournal = {Front. Comput. Sci.},
  title        = {An empirical study on performance comparisons of different types of DevOps team formations},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring operator responses to augmented reality training: Insights from the SELFEX platform case study. <em>FCOMP</em>, <em>7</em>, 1507439. (<a href='https://doi.org/10.3389/fcomp.2025.1507439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional industrial training methods often fail to capture the tacit expertise of experienced personnel, limiting instructional quality for new staff. This study examines the SELFEX platform, an augmented reality (AR)–based training system enabling junior operators to learn autonomously by replicating recorded performances of senior operators. Using a mixed-methods design, the research combined a technical analysis of AR’s functionality, benefits, and constraints with an empirical evaluation in an industrial setting. Seventeen participants completed training tasks using either conventional screens or AR headsets, with subjective measures including satisfaction, perceived usefulness, ease of use, and flow state, alongside objective performance metrics. Results showed that AR training was particularly beneficial for novices, enhancing engagement, understanding, and perceived ease of learning, though no statistically significant performance differences with screen-based training were found. Correlation analyses revealed strong links between flow, satisfaction, and ease of learning, highlighting the importance of intuitive, well-integrated design. Challenges in integrating AR into professional workflows—such as technical stability and user adoption—were also identified. These findings position AR as a promising tool for accessible and immersive industrial training, capable of supporting both initial skill acquisition and potential future upskilling. Further longitudinal studies are recommended to evaluate long-term impacts on performance, retention, and cost-effectiveness, and to refine system usability for diverse user profiles.},
  archive      = {J_FCOMP},
  author       = {Escallada, Oscar and Lasa, Ganix and Mazmela, Maitane and La Carrubba, Dario and Bosani, Enrica and Dacal-Nieto, Angel and García, Marcos Villar},
  doi          = {10.3389/fcomp.2025.1507439},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1507439},
  shortjournal = {Front. Comput. Sci.},
  title        = {Exploring operator responses to augmented reality training: Insights from the SELFEX platform case study},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cyberbullying: A comparative analysis between the results of a scoping study and a questionnaire applied to students. <em>FCOMP</em>, <em>7</em>, 1506046. (<a href='https://doi.org/10.3389/fcomp.2025.1506046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a scoping study using the Scopus Database to analyze literature on cyberbullying and students’ perceptions. Using the keywords ‘cyberbullying’, ‘students’, and ‘perceptions’, we narrowed down 6,271 initial articles to 14 that met our inclusion criteria. Additionally, we conducted a questionnaire survey with 193 Portuguese students aged between 10 and 19 to understand their perceptions of cyberbullying. Our analysis revealed cyberbullying as a growing concern with significant negative impacts on students’ mental and emotional wellbeing. The correlation between our questionnaire results and the scoping study findings emphasizes the urgent need for comprehensive intervention strategies. Our research indicates that effective cyberbullying prevention requires a multi-faceted approach including: development of social and emotional skills among students; promotion of appropriate technology use beyond technical literacy; targeted teacher training programs; establishment of clear intervention protocols within schools; empowerment of cyber-observers as active prevention agents; and recognition that cyberbullying often functions as an extension of face-to-face aggression rather than anonymous attacks. This study brings into focus the critical importance of fostering digital citizenship within educational settings, with teachers and school administrators playing pivotal roles in creating safe digital environments. The findings underscore how properly structured educational interventions can significantly increase reporting rates and decrease cyberbullying incidents, thereby promoting students’ overall wellbeing in the digital age.},
  archive      = {J_FCOMP},
  author       = {Coutinho, Luís and Lencastre, José Alberto and Tomás, Ana Maria},
  doi          = {10.3389/fcomp.2025.1506046},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1506046},
  shortjournal = {Front. Comput. Sci.},
  title        = {Cyberbullying: A comparative analysis between the results of a scoping study and a questionnaire applied to students},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="fcteg">FCTEG - 1</h2>
<ul>
<li><details>
<summary>
(2025). Conflict-based model predictive control for multi-agent path finding experimentally validated on a magnetic planar drive system. <em>FCTEG</em>, <em>6</em>, 1645918. (<a href='https://doi.org/10.3389/fcteg.2025.1645918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis work presents an approach to collision avoidance in multi-agent systems (MAS) by integrating Conflict-Based Search (CBS) with Model Predictive Control (MPC), referred to as Conflict-Based Model Predictive Control (CB-MPC).MethodsThe proposed method leverages the conflict-avoidance strengths of CBS to generate collision-free paths, which are then refined into dynamic reference trajectories using a minimum jerk trajectory optimizer and then used inside a MPC to follow the trajectories and to avoid collisions. This integration ensures real-time trajectory execution, preventing collisions and adapting to online changes. The approach is evaluated using a magnetic planar drive system for realistic multi-agent scenarios, demonstrating enhanced real-time responsiveness and adaptability. The focus is on the development of a motion planning algorithm and its validation in dynamic environments, which are becoming increasingly relevant in modern adaptive production sites.ResultsOn the MAS demonstrator with four active agents, ten different scenarios were created with varying degrees of complexity in terms of route planning. In addition, external disturbances that hinder the execution of the paths were simulated. All calculation and solution times were recorded and discussed. The result show that all scenarios could be successfully solved and executed., and the CB-MPC is therefore suitable for motion planning on the presented MAS demonstrator.DiscussionThe results show, that the CB-MPC is suitable for motion planning on the presented MAS demonstrator. The greatest limitation of the approach lies in scalability with regard to increasing the number of agents.},
  archive      = {J_FCTEG},
  author       = {Janning, Kai and Housin, Abdalsalam and Schulte, Christopher and Erkens, Frederik and Frenken, Luca and Herbst, Laura and Nießing, Bastian and Schmitt, Robert H.},
  doi          = {10.3389/fcteg.2025.1645918},
  journal      = {Frontiers in Control Engineering},
  month        = {7},
  pages        = {1645918},
  shortjournal = {Front. Control Eng.},
  title        = {Conflict-based model predictive control for multi-agent path finding experimentally validated on a magnetic planar drive system},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="fdata">FDATA - 3</h2>
<ul>
<li><details>
<summary>
(2025). FAST—framework for AI-based surgical transformation. <em>FDATA</em>, <em>8</em>, 1655260. (<a href='https://doi.org/10.3389/fdata.2025.1655260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundThe use of machine learning (ML) in surgery till date has largely focused on predication of surgical variables, which has not been found to significantly improve operating room efficiencies and surgical success rates (SSR). Due to the long surgery wait times, limited health care resources and an increased population need, innovative ML models are needed. Thus, the Framework for AI-based Surgical Transformation (FAST) was created to make real time recommendations to improve OR efficiency.MethodsThe FAST model was developed and evaluated using a dataset of n=4796 orthopedic cases that utilizes surgery and team specific variables (e.g. specific team composition, OR turnover time, procedure duration), along with regular positive deviance seminars with the stakeholders for adherence and uptake. FAST was created using six ML algorithms, including decision trees and neural networks. The FAST was implemented in orthopedic surgeries at a hospital in Canada's capital (Ottawa).ResultsFAST was found to be feasible and implementable in the hospital orthopedic OR, with good team engagement due to the PD seminars. FAST led to a SSR of 93% over 23 weeks (57 arthroplasty surgery days) compared to 39% at baseline. Key variables impacting SSR included starting the first surgery on time, turnover time, and team composition.ConclusionsFAST is a novel ML framework that can provide real time feedback for improving OR efficiency and SSR. Stakeholder integration is key in its success in uptake and adherence. This unique framework can be implemented in different hospitals and for diverse surgeries, offering a novel and innovative application of ML for improving OR efficiency without additional resources.},
  archive      = {J_FDATA},
  author       = {Sekhon, Harmehr and Al Zoubi, Farid and Beaulé, Paul E. and Fallavollita, Pascal},
  doi          = {10.3389/fdata.2025.1655260},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {1655260},
  shortjournal = {Front. Big Data},
  title        = {FAST—framework for AI-based surgical transformation},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure aggregation of sufficiently many private inputs. <em>FDATA</em>, <em>8</em>, 1638307. (<a href='https://doi.org/10.3389/fdata.2025.1638307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure aggregation of distributed inputs is a well-studied problem. In this study, anonymity of inputs is achieved by assuring a minimal quota before publishing the outcome. We design and implement an efficient cryptographic protocol that mitigates the most important security risks and show its application in the cyber threat intelligence (CTI) domain. Our approach allows for generic aggregation and quota functions. With 20 inputs from different parties, we can do three secure and anonymous aggregations per second, and in a CTI community of 100 partners, 10, 000 aggregations could be performed during one night.},
  archive      = {J_FDATA},
  author       = {Veugen, Thijs and Spini, Gabriele and Muller, Frank},
  doi          = {10.3389/fdata.2025.1638307},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {1638307},
  shortjournal = {Front. Big Data},
  title        = {Secure aggregation of sufficiently many private inputs},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multistakeholder fairness in tourism: What can algorithms learn from tourism management?. <em>FDATA</em>, <em>8</em>, 1632766. (<a href='https://doi.org/10.3389/fdata.2025.1632766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic decision-support systems, i.e., recommender systems, are popular digital tools that help tourists decide which places and attractions to explore. However, algorithms often unintentionally direct tourist streams in a way that negatively affects the environment, local communities, or other stakeholders. This issue can be partly attributed to the computer science community's limited understanding of the complex relationships and trade-offs among stakeholders in the real world. In this work, we draw on the practical findings and methods from tourism management to inform research on multistakeholder fairness in algorithmic decision-support. Leveraging a semi-systematic literature review, we synthesize literature from tourism management as well as literature from computer science. Our findings suggest that tourism management actively tries to identify the specific needs of stakeholders and utilizes qualitative, inclusive and participatory methods to study fairness from a normative and holistic research perspective. In contrast, computer science lacks sufficient understanding of the stakeholder needs and primarily considers fairness through descriptive factors, such as measureable discrimination, while heavily relying on few mathematically formalized fairness criteria that fail to capture the multidimensional nature of fairness in tourism. With the results of this work, we aim to illustrate the shortcomings of purely algorithmic research and stress the potential and particular need for future interdisciplinary collaboration. We believe such a collaboration is a fundamental and necessary step to enhance algorithmic decision-support systems toward understanding and supporting true multistakeholder fairness in tourism.},
  archive      = {J_FDATA},
  author       = {Müllner, Peter and Schreuer, Anna and Kopeinik, Simone and Wieser, Bernhard and Kowald, Dominik},
  doi          = {10.3389/fdata.2025.1632766},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {1632766},
  shortjournal = {Front. Big Data},
  title        = {Multistakeholder fairness in tourism: What can algorithms learn from tourism management?},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="frai">FRAI - 51</h2>
<ul>
<li><details>
<summary>
(2025). Predicting BRICS NIFTY50 returns using XAI and S.A.F.E AI lens. <em>FRAI</em>, <em>8</em>, 1668700. (<a href='https://doi.org/10.3389/frai.2025.1668700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PurposeGlobal fund managers, in their effort toward risk diversification and generating higher returns, design portfolios that consist of financial assets of various countries. In the process, they expose their investors not only to the fundamentals of the assets but also to transnational volatility, macroeconomic shocks of different countries, and exchange rate fluctuations. These factors make forecasting returns from such global funds quite difficult and, at the same time, challenging. To aid global fund managers and investors, this study presents a forecasting framework for predicting returns from Goldman Sachs BRICs Nifty 50 Developed Markets Index (BRICS NIFTY 50), which is a traded and listed financial asset. It is a global portfolio, which not only exposes investors to the fundamentals of different companies but also to country risk.Design, methodology, and approachGradient boosting regression (GBR) and SHAP-based XAI are used to identify the top significant country-specific explanatory variables. Subsequently, with the selected variables, GBR, CatBoost, Light Gradient Boosting Machine (LGBM), Extreme Gradient Boosting (XGBoost), Random Forest (RF), and Extra Tree Regressor (ETR) are applied for forecasting returns from BRICS NIFTY 50. Along with standard evaluation tools, the S.A.F.E AI framework is used for measuring predictive accuracy, sustainability, and contribution of each predictor. To evaluate the relative efficacy of the six predictive models, the underlying research resorts to a multi-criteria decision-making (MCDM) framework.FindingsWe find that country-specific market volatility, industrial performance, financial sector development, and exchange rate fluctuations explain global returns significantly. Furthermore, the exercise also reveals that explanatory factors specific to India, China, and Brazil emerge to be relatively important.Research limitations and implicationsThe study focuses on a single index. Future work will extend it to other indices and global funds.Practical implicationsThe proposed methodology will be of practical use to global fund managers and investors. Policymakers may find it useful for identifying factors that make foreign direct investment and portfolio investment attractive.Originality and valueDevelopment of a two-step forecasting framework, identifying effects of country-specific explanatory variables, and applying different evaluation criteria to measure predictive efficiency underscore the novelty of the work.},
  archive      = {J_FRAI},
  author       = {Ghosh, Indranil and Datta Chaudhuri, Tamal and Babaei, Golnoosh and Giudici, Paolo and Raffinetti, Emanuela},
  doi          = {10.3389/frai.2025.1668700},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1668700},
  shortjournal = {Front. Artif. Intell.},
  title        = {Predicting BRICS NIFTY50 returns using XAI and S.A.F.E AI lens},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive noise-augmented attention for enhancing transformer fine-tuning on longitudinal medical data. <em>FRAI</em>, <em>8</em>, 1663484. (<a href='https://doi.org/10.3389/frai.2025.1663484'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer models pre-trained on self-supervised tasks and fine-tuned on downstream objectives have achieved remarkable results across a variety of domains. However, fine-tuning these models for clinical predictions from longitudinal medical data, such as electronic health records (EHR), remains challenging due to limited labeled data and the complex, event-driven nature of medical sequences. While self-attention mechanisms are powerful for capturing relationships within sequences, they may underperform when modeling subtle dependencies between sparse clinical events under limited supervision. We introduce a simple yet effective fine-tuning technique, Adaptive Noise-Augmented Attention (ANAA), which injects adaptive noise directly into the self-attention weights and applies a 2D Gaussian kernel to smooth the resulting attention maps. This mechanism broadens the attention distribution across tokens while refining it to emphasize more informative events. Unlike prior approaches that require expensive modifications to the architecture and pre-training phase, ANAA operates entirely during fine-tuning. Empirical results across multiple clinical prediction tasks demonstrate consistent performance improvements. Furthermore, we analyze how ANAA shapes the learned attention behavior, offering interpretable insights into the model's handling of temporal dependencies in EHR data.},
  archive      = {J_FRAI},
  author       = {Amirahmadi, Ali and Etminani, Farzaneh and Ohlsson, Mattias},
  doi          = {10.3389/frai.2025.1663484},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1663484},
  shortjournal = {Front. Artif. Intell.},
  title        = {Adaptive noise-augmented attention for enhancing transformer fine-tuning on longitudinal medical data},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting offer burden to optimize batch sizes in simultaneously expiring kidney offers. <em>FRAI</em>, <em>8</em>, 1662960. (<a href='https://doi.org/10.3389/frai.2025.1662960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundTimely and efficient allocation of deceased donor kidneys is a persistent challenge in transplantation. Traditional sequential offer systems often lead to extended delays and high nonuse rates, as many kidneys undergo multiple refusals before being accepted. Simultaneously expiring offers, where a kidney is offered to a batch of centers with synchronized response deadlines, offer a more efficient alternative. However, fixed batch sizes fail to account for variability in offer requirements, potentially introducing new inefficiencies or overwhelming transplant professionals with excessive notifications.MethodsWe investigated the use of machine learning-based survival models to dynamically predict the number of offers a kidney will require before acceptance. Utilizing data on over 16,000 deceased donor kidneys from the national organ offer dataset, we engineered predictive features from both donor profiles and recipient pool characteristics. We trained and evaluated multiple survival models using time-dependent concordance indices along with other survival and regression performance metrics.ResultsThe Random Survival Forest model achieved the best performance, with a time-dependent C-index of 0.882, effectively estimating the required offer volume for kidney placement. Feature importance analysis revealed that waitlist characteristics, such as mean Estimated Post-Transplant Survival (EPTS), mean Calculated Panel Reactive Antibody (CPRA), time on dialysis, and waitlist duration, were among the most influential predictors. When integrated into a dynamic simultaneous offer system, these predictions have the potential to reduce average placement delays from 17.37 h to 1.59 h while maintaining a manageable level of extraneous offers.DiscussionOur results demonstrate that survival-based predictive modeling can meaningfully improve the efficiency of simultaneously expiring offers in kidney allocation. By personalizing batch sizes based on expected offer burden, such models can reduce delays without overwhelming transplant professionals. These findings underscore the value of integrating real-time, data-driven tools into organ allocation systems to improve operational efficiency and facilitate practical implementation.},
  archive      = {J_FRAI},
  author       = {Berry, Sean and Görgülü, Berk and Tunç, Sait and Cevik, Mucahit},
  doi          = {10.3389/frai.2025.1662960},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1662960},
  shortjournal = {Front. Artif. Intell.},
  title        = {Predicting offer burden to optimize batch sizes in simultaneously expiring kidney offers},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using ChatGPT as an assessment tool for medical residents in mexico: A descriptive experience. <em>FRAI</em>, <em>8</em>, 1662203. (<a href='https://doi.org/10.3389/frai.2025.1662203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionArtificial intelligence (AI) in medical education has progressed gradually, with numerous authors debating whether to prohibit, restrict, or adopt its use in academic contexts. Growing evidence exists regarding the capabilities and applications of AI in this field, particularly in supporting educational tasks such as student assessment. In this article we described our experience using ChatGPT to evaluate medical residents.Materials and methodsA descriptive cross-sectional study was conducted involving 35 medical residents from different specialty’s at a secondary-level hospital. Two different exams were generated using ChatGPT in topics of Rocky Mountain Spotted Fever (RMSF) and Pertussis. Additionally, an opinion survey—previously validated was administered to assess participants’ perceptions of ChatGPT ability to generate multiple-choice questions.ResultsOverall average score for the Pertussis examination was 8.46, while the average for the RMSF examination was 8.29. All participants reported that the examination was well written and that the language used was coherent; 34 residents (97.14%) stated that the language was clear, concise, and easy to understand; 9 residents (25.71%) agreed that the language used was confusing; 33 residents (94.28%) rated the exams questions as difficult; 32 residents (91.42%) felt that they had adequately prepared for both examinations.DiscussionChatGPT exhibits a promising faculty as a tool to support teaching activities in the training of medical specialists, mainly in reducing the human workload of healthcare personnel, and becoming integral to the next phase of medical education through AI-assisted content creation supervised by educators.},
  archive      = {J_FRAI},
  author       = {Rivera-Rosas, Cristian N. and Calleja-López, J. R. Tadeo and Larios-Camacho, Sandra J. and Trujillo-López, Sergio},
  doi          = {10.3389/frai.2025.1662203},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1662203},
  shortjournal = {Front. Artif. Intell.},
  title        = {Using ChatGPT as an assessment tool for medical residents in mexico: A descriptive experience},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-dialectal arabic translation: Comparative analysis on large language models. <em>FRAI</em>, <em>8</em>, 1661789. (<a href='https://doi.org/10.3389/frai.2025.1661789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionExploring Arabic dialects in Natural Language Processing (NLP) is essential to understand linguistic variation and meet regional communication demands. Recent advances in Large Language Models (LLMs) have opened up new vistas for multilingual communication and text generation.MethodsThis paper investigates the performance of GPT-3.5, GPT-4, and Bard (Gemini) on the QADI and MADAR datasets, while GPT-5 was evaluated exclusively on MADAR encompassing over 15 different countries. Several metrics have been used in the evaluation, such as cosine similarity, universal similarity encoder, sentence BERT, TER, ROUGE, and BLEU. In this study, different prompting techniques were used: zero-shot and few-shot. Zero-shot was employed for all dialects, and few-shot was employed only for the least translation performance dialect, Tunisian.ResultsAnalysis revealed that in the QADI dataset, GPT-4 significantly outperformed others in translating MSA to DA, with ANOVA tests showing strong significance (p < 0.05) in most metrics, except for BLEU and TER where it does not show significance, indicating comparable translation performance among models. Furthermore, GPT-4 was highest in semantic similarity compared to GPT-3.5 and Bard (Gemini), 0.66, 0.61, and 0.63, respectively. GPT-4 was the best in identifying overlapping sentences (i.e., those where the source and target are identical) with a combined average of 0.41 in BLEU and ROUGE-L. All LLMs scored TER values between 6% and 25%, indicating generally good translation quality. However, GPT models, especially GPT-5, responded better to prompting and translation to Levant countries compared to Bard (Gemini). For the MADAR dataset, no significant translation differences were observed in sentence-BERT, ROUGE-L, and TER, while differences are identified in cosine similarity, BLEU, and universal similarity encoder metrics. Therefore, GPT-5 is the top performer in identifying sentence overlaps measured by BLEU and ROUGE-L (combined average 0.37).DiscussionThe few-shot approach did not show a significant improvement in translation performance, especially for GPT-4 and Bard (Gemini), while GPT-3.5 performed consistently. Zero-shot prompts were effective across dialects, while few-shot prompting, applied to the weakest-performing dialect (Tunisian), did not yield improvement. GPT-4 and Bard performed worse under this set-up, while GPT-3.5 remained consistent.},
  archive      = {J_FRAI},
  author       = {Beidas, Ayah and Mohi, Kousar and Ghaddar, Fatme and Ahmad, Imtiaz and Abed, Sa'Ed},
  doi          = {10.3389/frai.2025.1661789},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1661789},
  shortjournal = {Front. Artif. Intell.},
  title        = {Cross-dialectal arabic translation: Comparative analysis on large language models},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in traditional medicine: Evidence, barriers, and a research roadmap for personalized care. <em>FRAI</em>, <em>8</em>, 1659338. (<a href='https://doi.org/10.3389/frai.2025.1659338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundTraditional medicine (TM) systems such as Ayurveda, Traditional Chinese Medicine (TCM), and Thai Traditional Medicine (TTM) are increasingly intersecting with artificial intelligence (AI).ObjectiveTo synthesize how AI is currently applied to TM and to outline barriers and research needs for safe, equitable, and scalable adoption.MethodsWe conducted a targeted narrative mini review of peer reviewed studies (2017–Aug 2025) retrieved from PubMed, Scopus, and Google Scholar using terms spanning TM (Ayurveda/TCM/TTM) and AI (machine learning (ML), natural language processing (NLP), computer vision, telemedicine. Inclusion favored studies with reported methods and, when available, performance metrics; commentary and preprints without data were excluded.FindingsCurrent evidence supports AI assisted diagnostic pattern recognition, personalization frameworks integrating multi source data, digital preservation of TM knowledge, telemedicine enablement, and AI supported herbal pharmacology and safety assessment. Reported performance varies and is context dependent, with limited prospective external validation.LimitationsEvidence heterogeneity, small datasets, inconsistent ontologies across TM systems, and nascent regulatory pathways constrain real world deployment.ConclusionAI can augment TM education, research, and clinical services, but progress requires standards, culturally informed datasets, prospective trials, and clear governance. We propose a research roadmap to guide rigorous and ethical integration.},
  archive      = {J_FRAI},
  author       = {Jongjiamdee, Ketmanee and Pornwonglert, Pimnipa and Na Bangchang, Nutnichar and Akarasereenont, Pravit},
  doi          = {10.3389/frai.2025.1659338},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1659338},
  shortjournal = {Front. Artif. Intell.},
  title        = {Artificial intelligence in traditional medicine: Evidence, barriers, and a research roadmap for personalized care},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accuracy of AI chatbots in answering frequently asked questions on cervical cancer. <em>FRAI</em>, <em>8</em>, 1655303. (<a href='https://doi.org/10.3389/frai.2025.1655303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ObjectiveTo compare the accuracy of Deepseek and ChatGPT in answering frequently asked questions (FAQs) about cervical cancer.MethodsTo compile a list of FAQs concerning cervical cancer, a comprehensive search was conducted on social media and community platforms. The answer keys for all the selected questions were created on the basis of the guidelines of the National Comprehensive Cancer Network (NCCN), the International Federation of Gynecology and Obstetrics (FIGO), and the World Health Organization (WHO) for cervical cancer. The answers given by Deepseek-R1 and ChatGPT O1 were scored according to the Global Quality Score (GQS).ResultsA total of 74 FAQs covered a diverse range of topics related to cervical cancer, including diagnosis (n = 16), risk factors and epidemiology (n = 19), treatment (n = 20), and prevention (n = 19). When all the answers provided by DeepSeek to the FAQs about cervical cancer according to the GQS were evaluated, 68 answers were rated as score five, 4 answers were rated as score four, and 2 answers were rated as score three. For ChatGPT’s responses to the same set of FAQs, 67 answers were classified as score five, 6 answers were classified as score four, and 1 answer was classified as score three. There was no statistically significant difference between the two groups (p > 0.05).ConclusionBoth DeepSeek and ChatGPT demonstrated accurate and satisfactory responses to FAQs about cervical cancer when evaluated according to the GQS. However, in regard to treatment issues, a cautious attitude should be maintained. Compared to ChatGPT, DeepSeek stands out for its free availability, which makes it more accessible in resource-limited scenarios to the public.},
  archive      = {J_FRAI},
  author       = {Fan, Jielin and Xiao, Wenhong and Yan, Zhipeng and Ouyang, Qiang},
  doi          = {10.3389/frai.2025.1655303},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1655303},
  shortjournal = {Front. Artif. Intell.},
  title        = {Accuracy of AI chatbots in answering frequently asked questions on cervical cancer},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging pre-trained embeddings in an ensemble machine learning approach for arabic sentiment analysis. <em>FRAI</em>, <em>8</em>, 1653728. (<a href='https://doi.org/10.3389/frai.2025.1653728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionArabic sentiment analysis presents unique challenges due to the linguistic complexity of the language, including its wide range of dialects, orthographic ambiguity, and limited language resources. Addressing these issues is essential to develop robust sentiment classification systems.MethodsThis study investigates the application of ensemble machine learning methods for Arabic sentiment analysis. Several homogeneous ensemble techniques are implemented and evaluated on two datasets: the balanced ArTwitter dataset and the highly imbalanced Syria_Tweets dataset. To mitigate class imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) is employed. The models incorporate pre-trained word embeddings and unigram features.ResultsExperimental results indicate that individual classifiers using pre-trained embeddings achieve strong performance; however, ensemble models consistently yield superior outcomes. On the ArTwitter dataset, the ensemble of Naive Bayes, Support Vector Machine, and Decision Tree classifiers achieved an accuracy of 90.22% and an F1-score of 92.0%. On the Syria_Tweets dataset, an ensemble combining Stochastic Gradient Descent, k-Nearest Neighbors, and Random Forest attained 83.82% accuracy and an 83.86% F1-score.DiscussionThe findings highlight the effectiveness of ensemble learning in enhancing the robustness and generalizability of Arabic sentiment analysis systems. Incorporating pre-trained embeddings further strengthens performance, demonstrating that ensemble-based approaches can overcome challenges posed by linguistic complexity and dataset imbalance in Arabic natural language processing tasks.},
  archive      = {J_FRAI},
  author       = {Jaber, Areej and Bahati, Israa and Martínez, Paloma},
  doi          = {10.3389/frai.2025.1653728},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1653728},
  shortjournal = {Front. Artif. Intell.},
  title        = {Leveraging pre-trained embeddings in an ensemble machine learning approach for arabic sentiment analysis},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entropy-adaptive differential privacy federated learning for student performance prediction and privacy protection: A case study in python programming. <em>FRAI</em>, <em>8</em>, 1653437. (<a href='https://doi.org/10.3389/frai.2025.1653437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the digital transformation of engineering education, protecting student data privacy has become a key challenge for enabling data-driven instruction. This study proposes an Entropy-Adaptive Differential Privacy Federated Learning method (EADP-FedAvg) to enhance the accuracy of student performance prediction while ensuring data privacy. Based on online test records from Python programming courses for Electronic Engineering students (grade 2021–2023) at the School of Physics and Optoelectronic Technology, Baoji University of Arts and Sciences, China, the study uses a Multilayer Perceptron (MLP) model and 10 distributed clients for training. Under different privacy budgets (ε = 0.1, 1e-6, and 1.0), EADP-FedAvg achieves a test accuracy of 92.7%, macro-average score of 92.1%, and entropy of 0.207, outperforming standard federated learning and approaching centralized learning performance. The results demonstrate that by adaptively adjusting the noise level based on output entropy, EADP-FedAvg effectively balances privacy preservation and model accuracy. This method offers a novel solution for analyzing privacy-sensitive educational data in engineering education.},
  archive      = {J_FRAI},
  author       = {Chen, Shanwei and Qi, Xiuzhi},
  doi          = {10.3389/frai.2025.1653437},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1653437},
  shortjournal = {Front. Artif. Intell.},
  title        = {Entropy-adaptive differential privacy federated learning for student performance prediction and privacy protection: A case study in python programming},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting pediatric diagnostic imaging patient no-show and extended wait-times using LLMs, regression, and tree based models. <em>FRAI</em>, <em>8</em>, 1652397. (<a href='https://doi.org/10.3389/frai.2025.1652397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionPatients missing their appointments (no-shows) are a persistent issue that results in idle resources while delaying critical patient prognosis. Likewise, long waiting times increase frustration for patients, leaving a negative impression on the appointment. In this paper, we explore 3 modalities of diagnostic and interventional radiology appointments for pediatric patients at the Hospital for Sick Children (SickKids), Toronto, ON, Canada. Our goal was to survey machine learning methods that best predict the risk of patient no-shows and long wait-times exceeding 1 hour for scheduling teams to propose targeted downstream accommodations.MethodsWe experimented with 6 predictive model types separately trained on both tasks which included extreme gradient boosting (XGBoost), Random Forest (RF), Support Vector Machine, Logistic Regression, Artificial Neural Network, and a pre-trained large language model (LLM). Utilizing 20 features containing a mixture of patient demographics and appointment related data, we experimented with different data balancing methods including instance hardness threshold (IHT) and class weighting to reduce bias in prediction. We then conducted a comparative study of the improvements made by utilizing continuous contextual data in our LLM which boasted a 51% improvement in F1 score for the wait-time model.ResultsOur XGBoost model had the best combination of AUC and F1 scores (0.96 and 0.62, respectively) for predicting no-show while RF had the best AUC and F1 scores (0.83 and 0.61, respectively) for wait-time prediction. The LLMs also performed well for 90% probability thresholds (high risk patients) while being robustly calibrated on unseen test data.DiscussionOur results surveyed multiple algorithms and data balancing methods to propose the greatest performing models on our tasks, implemented a unique methodology to use LLMs on heterogeneous data within this domain, and demonstrated the greater importance of contextual appointment data over patient demographic features for a more equitable prediction algorithm. Going forward, the predictive output (calibrated probabilities of events) can be used as stochastic input for risk-based optimized scheduling to provide accommodation for patients less likely to receive quality access to healthcare.},
  archive      = {J_FRAI},
  author       = {Rafique, Daniel and Liu, Xuan and Gong, Bo and Belsito, Laura and McCradden, Melissa D. and Mazwi, Mjaye L. and Lee, Wayne and Ohanlon, Graham and Tsang, Kyle and Shroff, Manohar and Ertl-Wagner, Birgit and Khalvati, Farzad},
  doi          = {10.3389/frai.2025.1652397},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1652397},
  shortjournal = {Front. Artif. Intell.},
  title        = {Predicting pediatric diagnostic imaging patient no-show and extended wait-times using LLMs, regression, and tree based models},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on intelligent matching of students’ learning ability and healthcare job market demand based on industrial engineering expertise graph. <em>FRAI</em>, <em>8</em>, 1650095. (<a href='https://doi.org/10.3389/frai.2025.1650095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In China, there is a structural mismatch between the job market and student employment, characterized by “unfilled jobs” and “unqualified candidates,” particularly between the industrial engineering (IE) profession and the healthcare services sector. Expertise graphs are designed to identify the logical connections between academic disciplines and job market needs, linking students’ knowledge and skills with job requirements. This approach provides a systematic and visual alignment between students’ learning outcomes and job market demands, addressing the mismatch. However, current expertise graphs have not effectively captured the intrinsic connection between students’ learning abilities and healthcare job market demands. Additionally, research on intelligent matching and the construction of knowledge graphs for IE remains limited. This study aims to bridge this gap and alleviate the structural mismatch between the healthcare job market and student employment in China. First, an expertise graph for IE is developed, covering both expertise and healthcare job requirements. A multi-layer fusion information extraction model, combining BERT, BiLSTM, and GCN, is then proposed for knowledge extraction. An employment matching algorithm is introduced to extract healthcare job titles and requirements from the knowledge graph, calculate similarity with students’ overall ability scores, and recommend suitable positions. Finally, a case study demonstrates that the algorithm accurately analyzes students’ ability scores and successfully matches IE majors with relevant healthcare job positions, validating its effectiveness. This study aims to mitigate the structural mismatch between the healthcare job market and student employment, providing high-quality IE talent to medical services, which has significant scientific and practical value.},
  archive      = {J_FRAI},
  author       = {Xiao, Yan and Zeng, Lingtao and Yang, Jie and Wang, Mini Han and Lin, Zhiyuan and Li, Wei},
  doi          = {10.3389/frai.2025.1650095},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1650095},
  shortjournal = {Front. Artif. Intell.},
  title        = {Research on intelligent matching of students’ learning ability and healthcare job market demand based on industrial engineering expertise graph},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ethical prompting: Toward strategies for rapid and inclusive assistance in dual-use AI systems. <em>FRAI</em>, <em>8</em>, 1646444. (<a href='https://doi.org/10.3389/frai.2025.1646444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring technologies initially developed for individuals with disabilities carry inherent dual-use risks, especially evident in conflict or emergency scenarios. This article examines the dual-use dilemma posed by technologies whose civilian design objectives can unintentionally facilitate harmful applications in defense contexts. Specifically, we analyze the ethical risks associated with using civilian-generated data and systems, originally intended to enhance care and assistance, for military purposes without adequate safeguards. We argue that effective and ethically sound technological infrastructures require optimized and ethically-informed prompting strategies. These strategies must clearly define how data and system prompts are structured, reducing deployment biases, particularly against vulnerable populations.},
  archive      = {J_FRAI},
  author       = {Farnós, Joan and Sans Pinillos, Alger and Costa, Vicent},
  doi          = {10.3389/frai.2025.1646444},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1646444},
  shortjournal = {Front. Artif. Intell.},
  title        = {Ethical prompting: Toward strategies for rapid and inclusive assistance in dual-use AI systems},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weight-aware semi-supervised self-ensembling framework for interior decoration style classification. <em>FRAI</em>, <em>8</em>, 1645877. (<a href='https://doi.org/10.3389/frai.2025.1645877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic classification of interior decoration styles has great potential to guide and streamline the design process. Despite recent advancements, it remains challenging to construct an accurate interior decoration style recognition model due to the scarcity of expert annotations. In this article, we develop a new weight-aware semi-supervised self-ensembling framework for interior decoration style recognition, which selectively leverages the abundant unlabeled data to address the aforementioned challenge. Specifically, we devise a weight module that utilizes a truncated Gaussian function to automatically assess the reliability of unlabeled data. This enables more reliable unlabeled samples to be adaptively assigned higher weights during the training process. By incorporating adaptive weights, we devise a weighted consistency regularization to enforce consistent predictions for reliable unlabeled data under different perturbations. Furthermore, we devise a weighted relation consistency regularization to preserve the semantic relationships of reliable unlabeled data across various perturbations. Additionally, we introduce a weighted class-aware contrastive learning regularization to improve the model's discriminative feature learning capability using reliable unlabeled data. The synergistic learning of weighted consistency regularization, weighted relation consistency, and weighted class-aware contrastive learning significantly enhances the model's generalizability. Extensive experiments conducted on interior decoration style image datasets demonstrate the superior performance of our framework compared to existing semi-supervised learning methods.},
  archive      = {J_FRAI},
  author       = {Guo, Lichun and Zeng, Hao and Wang, Junliang and Liang, Shuang and Hang, Wenlong},
  doi          = {10.3389/frai.2025.1645877},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1645877},
  shortjournal = {Front. Artif. Intell.},
  title        = {Weight-aware semi-supervised self-ensembling framework for interior decoration style classification},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI for scientific integrity: Detecting ethical breaches, errors, and misconduct in manuscripts. <em>FRAI</em>, <em>8</em>, 1644098. (<a href='https://doi.org/10.3389/frai.2025.1644098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Generative AI (GenAI) in scientific writing has grown rapidly, offering tools for manuscript drafting, literature summarization, and data analysis. However, these benefits are accompanied by risks, including undisclosed AI authorship, manipulated content, and the emergence of papermills. This perspective examines two key strategies for maintaining research integrity in the GenAI era: (1) detecting unethical or inappropriate use of GenAI in scientific manuscripts and (2) using AI tools to identify mistakes in scientific literature, such as statistical errors, image manipulation, and incorrect citations. We reviewed the capabilities and limitations of existing AI detectors designed to differentiate human-written (HWT) from machine-generated text (MGT), highlighting performance gaps, genre sensitivity, and vulnerability to adversarial attacks. We also investigate emerging AI-powered systems aimed at identifying errors in published research, including tools for statistical verification, citation validation, and image manipulation detection. Additionally, we discuss recent publishing industry initiatives to AI-driven papermills. Our investigation shows that these developments are not yet sufficiently accurate or reliable yet for use in academic assessment, they mark an early but promising steps toward scalable, AI-assisted quality control in scholarly publishing.},
  archive      = {J_FRAI},
  author       = {Pellegrina, Diogo and Helmy, Mohamed},
  doi          = {10.3389/frai.2025.1644098},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1644098},
  shortjournal = {Front. Artif. Intell.},
  title        = {AI for scientific integrity: Detecting ethical breaches, errors, and misconduct in manuscripts},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-, linguistic-, and information-preserving synthesis of clinical documentation through generative agents. <em>FRAI</em>, <em>8</em>, 1644084. (<a href='https://doi.org/10.3389/frai.2025.1644084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of generative agents (GAs) is reshaping the healthcare landscape. Nonetheless, broad utilization is impeded by restricted access to high-quality, interoperable clinical documentation from electronic health records (EHRs) due to persistent legal, ethical, and technical barriers. Synthetic health data generation (SHDG), leveraging pre-trained large language models (LLMs) instantiated as GAs, could offer a practical solution by creating synthetic patient information that mimics genuine EHRs. The use of LLMs, however, is not without issues; significant concerns remain regarding privacy, potential bias propagation, the risk of generating inaccurate or misleading content, and the lack of transparency in how these models make decisions. We therefore propose a privacy-, linguistic-, and information-preserving SHDG protocol that employs multiple context-aware, role-specific GAs. Guided by targeted prompting and authentic EHRs—serving as structural and linguistic templates—role-specific GAs can, in principle, operate collaboratively through multi-turn interactions. We theorized that utilizing GAs in this fashion permits LLMs not only to produce synthetic EHRs that are accurate, consistent, and contextually appropriate, but also to expose the underlying decision-making process. To test this hypothesis, we developed a no-code GA-driven SHDG workflow as a proof of concept, which was implemented within a predefined, multi-layered data science infrastructure (DSI) stack—an integrated ensemble of software and hardware designed to support rapid prototyping and deployment. The DSI stack streamlines implementation for healthcare professionals, improving accessibility, usability, and cybersecurity. To deploy and validate GA-assisted workflows, we implemented a fully automated SHDG evaluation framework—co-developed with GenAI technology—which holistically compares the informational and linguistic features of synthetic, anonymized, and real EHRs at both the document and corpus levels. Our findings highlight that SHDG implemented through GAs offers a scalable, transparent, and reproducible methodology for unlocking the potential of clinical documentation to drive innovation, accelerate research, and advance the development of learning health systems. The source code, synthetic datasets, toolchains and prompts created for this study can be accessed at the GitHub repository: https://github.com/HR-DataLab-Healthcare/RESEARCH_SUPPORT/tree/main/PROJECTS/Generative_Agent_based_Data-Synthesis.},
  archive      = {J_FRAI},
  author       = {van Velzen, Mark and van der Willigen, Robert F. and de Beer, Vincent J. and de Graaf-Waar, Helen I. and Janssen, Esther R. C. and van Leeuwen, Sjemaine and van der Willigen, Micha F. and van der Willigen, Martijn J. and Renardus, Gavin and El Maaroufi, Rayan and Satimin, Sven J. and Hartog, Larissa M. and Hulsen, Tim and van Meeteren, Nico L. U. and Scheper, Mark C.},
  doi          = {10.3389/frai.2025.1644084},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1644084},
  shortjournal = {Front. Artif. Intell.},
  title        = {Privacy-, linguistic-, and information-preserving synthesis of clinical documentation through generative agents},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating LLMs on kazakhstan's mathematics exam for university admission. <em>FRAI</em>, <em>8</em>, 1642570. (<a href='https://doi.org/10.3389/frai.2025.1642570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe rapid advancement of large language models (LLMs) has prompted their exploration in educational contexts, particularly in high-stakes standardized tests such as Kazakhstan's Unified National Testing (UNT) mathematics component, which is critical for university admission. While most existing benchmarks for mathematical reasoning focus on English, concerns remain that LLMs may underperform in under-resourced or non-English languages. This study addresses this gap by evaluating LLM performance on a math test administered entirely in Russian.MethodsWe assessed six LLMs-Claude, DeepSeek, Gemini, Llama, Qwen, and o-on UNT multiple-choice mathematics questions covering algebra, functions, geometry, inequalities, and trigonometry. Three evaluation conditions were employed: (1) zero-shot performance, (2) hybrid integration with SymPy for symbolic computation, and (3) a role-specific simulated multi-agent refinement framework that builds on existing self-correction techniques with targeted feedback.ResultsIn zero-shot settings, DeepSeek, Gemini, Qwen, and o achieved near-perfect or perfect accuracy (X-Y%) across all difficulty levels and topics, while Claude and Llama lagged (A-B%). The hybrid approach significantly improved Claude and Llama's accuracy by C% and D%, respectively. Under the multi-agent refinement condition, Claude showed substantial gains, reaching E% accuracy, which represented a F% improvement over zero-shot performance.DiscussionThese findings provide important empirical evidence that LLMs can perform competitively on mathematics tasks in non-English languages. The results challenge prior assumptions about limited performance in under-resourced linguistic settings and highlight the potential of LLMs to support bilingual education and promote equitable access to higher education.},
  archive      = {J_FRAI},
  author       = {Kadyrov, Shirali and Abdrasilov, Bolatbek and Sabyrov, Aslan and Baizhanov, Nurseit and Makhmutova, Alfira and Kyllonen, Patrick C.},
  doi          = {10.3389/frai.2025.1642570},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1642570},
  shortjournal = {Front. Artif. Intell.},
  title        = {Evaluating LLMs on kazakhstan's mathematics exam for university admission},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing training of time series diffusion models via similarity score functions: Application to cyclic and acyclic motion with IMU data. <em>FRAI</em>, <em>8</em>, 1640948. (<a href='https://doi.org/10.3389/frai.2025.1640948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionDenoising diffusion probabilistic models have shown the capability to generate synthetic sensor signals. These models rely on a loss function that measures the difference between the noise added during the forward process and the noise predicted by the diffusion model, thereby enabling realistic data generation. However, the stochastic nature of the process and the loss function complicate the estimation of data quality.MethodsTo address this issue, we evaluated multiple similarity metrics and adapted an existing metric to monitor both the training and data synthesis processes. The adapted metric was further fine-tuned on the input data to align with the requirements of a downstream classification task.ResultsBy incorporating the adapted metric, we significantly reduced the number of training epochs required without observing performance degradation in the classification task.DiscussionOur findings demonstrate that optimizing the training process using similarity metrics not only conserves computational resources but also shortens the training time for generative models, making them more efficient and practical for real-world applications.},
  archive      = {J_FRAI},
  author       = {Oppel, Heiko and Spilz, Andreas and Munz, Michael},
  doi          = {10.3389/frai.2025.1640948},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1640948},
  shortjournal = {Front. Artif. Intell.},
  title        = {Optimizing training of time series diffusion models via similarity score functions: Application to cyclic and acyclic motion with IMU data},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Profiling investor behavior in the malaysian derivatives market using K-means clustering. <em>FRAI</em>, <em>8</em>, 1640776. (<a href='https://doi.org/10.3389/frai.2025.1640776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the trading behaviors of Malaysian derivatives traders using a comprehensive dataset from Bursa Malaysia with K-means clustering, representing one of the first AI applications to derivatives market segmentation. The analysis encompassed over 11 million trade records for FCPO and FKLI derivatives from January to December 2022. Six key features were engineered to segment derivative traders: Total Number of Trades, Total Traded Amount, Overall Realized Profit, Average ROI, Maximum Account Vintage (trader experience in years), and Median Holding Days (typical position duration). Inverse Hyperbolic Sine transformation was applied to address extreme outliers, ensuring robust feature scaling. K-means clustering identified five distinct profiles: “High-Frequency, High-Risk Derivative Traders with Consistent Losses,” “Conservative, Steady-Growth Derivative Trader,” “High-Frequency, High-Yield Derivative Traders,” “Conservative, Low-Yield Derivative Traders,” and “Cautious, Low-Activity Novice Derivative Traders.” Decision tree classifiers validated these clusters through interpretable splitting conditions. These profiles enable targeted risk management strategies, personalized trading services, and evidence-based regulatory policies for derivatives markets and future research.},
  archive      = {J_FRAI},
  author       = {Tan, Eng Hao Louis and Hamed, Yaman and Daud, Hanita and Abdul Wahab, Mohd Amirul Faiz and Azhar, Ahmad Amirul Adlan and Tan, Sieow Yeek},
  doi          = {10.3389/frai.2025.1640776},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1640776},
  shortjournal = {Front. Artif. Intell.},
  title        = {Profiling investor behavior in the malaysian derivatives market using K-means clustering},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming cataract care through artificial intelligence: An evaluation of large language models’ performance in addressing cataract-related queries. <em>FRAI</em>, <em>8</em>, 1639221. (<a href='https://doi.org/10.3389/frai.2025.1639221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PurposeTo evaluate the performance of five popular large language models (LLMs) in addressing cataract-related queries.MethodsThis comparative evaluation study was conducted at the Eye and ENT Hospital of Fudan University. We performed both qualitative and quantitative assessments of responses from five LLMs: ChatGPT-4, ChatGPT-4o, Gemini, Copilot, and the open-source Llama 3.5. Model outputs were benchmarked against human-generated responses using seven key metrics: accuracy, completeness, conciseness, harmlessness, readability, stability, and self-correction capability. Additional inter-model comparisons were performed across question subgroups categorized by clinical topic type.ResultsIn the information quality assessment, ChatGPT-4o demonstrated the best performance across most metrics, including accuracy score (6.70 ± 0.63), completeness score (4.63 ± 0.63), and harmlessness score (3.97 ± 0.17). Gemini achieved the highest conciseness score (4.00 ± 0.14). Further subgroup analysis showed that all LLMs performed comparably to or better than humans, regardless of the type of question posed. The readability assessment revealed that ChatGPT-4o had the lowest readability score (26.02 ± 10.78), indicating the highest level of reading difficulty. While Copilot recorded a higher readability score (40.26 ± 14.58) than the other LLMs, it still remained lower than that of humans (51.54 ± 13.71). Copilot also exhibited the best stability in reproducibility and stability assessment. All LLMs demonstrated strong self-correction capability when prompted.ConclusionOur study suggested that LLMs exhibited considerable potential in providing accurate and comprehensive responses to common cataract-related clinical issues. Notably, ChatGPT-4o achieved the best scores in accuracy, completeness, and harmlessness. Despite these promising results, clinicians and patients should be aware of the limitations of artificial intelligence (AI) to ensure critical evaluation in clinical practice.},
  archive      = {J_FRAI},
  author       = {Wang, Xinyue and Liu, Yan and Song, Linghao and Wen, Yinuo and Peng, Shenjie and Ren, Ruoxi and Zhang, Yi and Chen, Tianhui and Jiang, Yongxiang},
  doi          = {10.3389/frai.2025.1639221},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1639221},
  shortjournal = {Front. Artif. Intell.},
  title        = {Transforming cataract care through artificial intelligence: An evaluation of large language models’ performance in addressing cataract-related queries},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arabic speech recognition model using baidu's deep and cluster learning. <em>FRAI</em>, <em>8</em>, 1639147. (<a href='https://doi.org/10.3389/frai.2025.1639147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study involves extracting the spectrum from the Arabic raw, unlabeled audio signal and producing Mel-frequency cepstral coefficients (MFCCs). The clustering algorithm groups the retrieved MFCCs with analogous features. The K-means clustering technique played a crucial role in our research, enabling the unsupervised categorization of unlabeled Arabic audio data. Employing K-means on the extracted MFCC features allowed us to classify acoustically similar segments into distinct groups without prior knowledge of their characteristics. This initial phase was crucial for understanding the inherent diversity in our diverse sampled dataset. Dynamic Time Warping (DTW) and Euclidean Distance are utilized for illustration. Classification algorithms such as Decision Tree, eXtreme Gradient Boosting (XGBoost), K-Nearest Neighbors (KNN), and Random Forest are used to classify the various classes obtained based on clustering. This study also demonstrates the efficacy of Mozilla's Deep Speech framework for Arabic speech recognition. The core component of deep speech is its neural network architecture, which consists of multiple layers of Recurrent Neural Networks (RNNs). It strives to comprehend the intricate patterns and interactions between spoken sounds and their corresponding textual representations. The clustered labeled Arabic audio dataset, along with transcripts and Arabic Alphabets, is used as input to Baidu's Deep Speech model for training and testing purposes. PyCharm, in conjunction with Python 3.6, is used to build a Dockerfile. Creating, editing, and managing Dockerfiles within PyCharm's IDE is simplified by its functionality and integrated environment. Deep speech provides an eminent Arabic speech recognition quality with reduced loss, word error rate (WER), and character error rate (CER). Baidu's Deep Speech intends to achieve high performance in both end-to-end and isolated speech recognition with good precision and a low word rate and character error rate in a reasonable amount of time. The suggested strategy yielded a loss of 276.147, a word error rate of 0.3720, and a character error rate of 0.0568. This technique increases the accuracy of Arabic automatic speech recognition (ASR).},
  archive      = {J_FRAI},
  author       = {Al-Anzi, Fawaz S. and Sundaram Thankaleela, Bibin Shalini},
  doi          = {10.3389/frai.2025.1639147},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1639147},
  shortjournal = {Front. Artif. Intell.},
  title        = {Arabic speech recognition model using baidu's deep and cluster learning},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced YOLOv8 for industrial polymer films: A semi-supervised framework for micron-scale defect detection. <em>FRAI</em>, <em>8</em>, 1638772. (<a href='https://doi.org/10.3389/frai.2025.1638772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionPolymer material films are produced through extrusion machines, and their surfaces can develop micro-defects due to process and operational influences. The quantity and size of these defects significantly impact product quality.MethodsAs traditional machine learning defect detection methods suffer from low accuracy and poor adaptability to complex scenarios, requiring extensive effort for parameter tuning and exhibiting weak generalization capability, this paper proposes an improved YOLOv8 method to identify micro-defects on films. The approach embeds the CBAM attention mechanism into high-level networks to address feature sparsity in small target detection samples. Simultaneously, given the difficulty in obtaining large annotated datasets, we employ the Mean Teacher method for semi-supervised learning using limited labeled data. During training, the method optimizes neural network gradients through an improved loss function based on normalized Wasserstein distance (NWD), mitigating gradient instability caused by scale variations and enhancing detection accuracy for small targets. Additionally, a proposed multi-threshold mask segmentation algorithm extracts defect contours for further feature analysis.ResultsExperimental results demonstrate that the improved YOLOv8 algorithm achieves an 8.26% increase in mAP@0.5 compared to the baseline. It exhibits higher precision for small targets, and maintains defect detection rates exceeding 95.0% across validation data of varying image sizes, thereby meeting industrial production requirements. In generalization validation, the model demonstrates superior performance compared to traditional methods under test environments with lighting variations and environmental contamination.DiscussionThe improved YOLOv8 algorithm meeting the stringent requirements for high-precision small-target defect detection on polymer material film in industrial production. Future work will explore more advanced techniques to enhance model accuracy and robustness.},
  archive      = {J_FRAI},
  author       = {Yu, Xiaoxia and Hu, Bingyu and Jiang, Weifeng and Wan, Jinru and Yang, Xinduoji and Liu, Nianbo and Dong, Xiaoyan},
  doi          = {10.3389/frai.2025.1638772},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1638772},
  shortjournal = {Front. Artif. Intell.},
  title        = {Enhanced YOLOv8 for industrial polymer films: A semi-supervised framework for micron-scale defect detection},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auto-scaling LLM-based multi-agent systems through dynamic integration of agents. <em>FRAI</em>, <em>8</em>, 1638227. (<a href='https://doi.org/10.3389/frai.2025.1638227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionLarge Language Model-based Multi-Agent Systems (LLM-based MASs) represent a groundbreaking paradigm where diverse LLM-based agents collaborate, leveraging their unique capabilities to achieve shared objectives. Although LLM-based MASs outperform individual agents, their current architectures are limited by predefined, fixed, and static agent designs, restricting adaptability and scalability in dynamic environments.MethodTo address these limitations, this study proposes two novel approaches: Initial Automatic Agent Generation (IAAG) and Dynamic Real-Time Agent Generation (DRTAG). These approaches enable the automatic creation and seamless integration of new agents into MASs, driven by evolving conversational and task-specific contexts, thereby reducing the need for human intervention. Our method leverages advanced prompt engineering techniques such as persona pattern prompting, chain prompting, and few-shot prompting to generate new agents through existing LLM agents. Additionally, several evaluation metrics were adapted to score and rank LLM-generated texts.ResultsExperimental results demonstrate that the DRTAG approach significantly improves system adaptability and task performance compared to static MAS architectures. The IAAG framework also enhances initial system flexibility, supporting the creation of contextually relevant agents.DiscussionThese findings highlight the potential of dynamic LLM-based MASs to overcome the limitations of static architectures to address complex real-world challenges, paving the way for innovative applications across diverse domains.},
  archive      = {J_FRAI},
  author       = {Perera, Ravindu and Basnayake, Anuradha and Wickramasinghe, Manjusri},
  doi          = {10.3389/frai.2025.1638227},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1638227},
  shortjournal = {Front. Artif. Intell.},
  title        = {Auto-scaling LLM-based multi-agent systems through dynamic integration of agents},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv10-based detection of melanocytic nevi: Reverse exclusion optimization for melanoma screening. <em>FRAI</em>, <em>8</em>, 1637842. (<a href='https://doi.org/10.3389/frai.2025.1637842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malignant melanoma is the deadliest skin cancer, yet its early dermoscopic presentation closely mimics benign melanocytic nevi. Conventional visual or dermoscopic screening therefore suffers from high miss rates and generates excessive biopsies. In this study we focus on Chinese East-Asian patients and introduce a reversed-exclusion strategy—classifying “benign first, exclude malignancy”: lesions that fully meet benign nevus criteria are deemed low-risk; all others are flagged as high-risk. Building on the real-time detector YOLOv10, we incorporate three medical-oriented upgrades: (i) a PP-LCNet backbone to preserve sub-3 mm textures; (ii) a Multiscale Contextual Attention (MCA) neck to enhance cross-scale aggregation; and (iii) a Shape-IoU loss that jointly optimises position, scale, and curvature. The model was trained on a multi-centre dermoscopic dataset from three tertiary hospitals in mainland China (2,040 benign nevi) and independently tested on 365 biopsy-proven melanomas collected at the same medical institution but drawn from a demographically distinct patient cohort, achieving a detection mAP@0.5 of 97.69% for benign lesions and a melanoma false-negative rate (FNR) of only 0.27%. By delivering high-confidence benign identification followed by malignant exclusion, the proposed model offers a high-precision, low-risk pathway for early melanoma screening in Chinese clinical settings. It can markedly reduce unnecessary biopsies while keeping the miss rate below the clinical safety ceiling of 0.5%, thus preserving the life-saving window afforded by early detection.},
  archive      = {J_FRAI},
  author       = {Wang, ShengJie and Wang, Jian and Yin, Rui},
  doi          = {10.3389/frai.2025.1637842},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1637842},
  shortjournal = {Front. Artif. Intell.},
  title        = {YOLOv10-based detection of melanocytic nevi: Reverse exclusion optimization for melanoma screening},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lipschitz-based robustness estimation for hyperdimensional learning. <em>FRAI</em>, <em>8</em>, 1637105. (<a href='https://doi.org/10.3389/frai.2025.1637105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the adoption of machine learning models in various practical domains, there is a growing need for evaluating and increasing model robustness. Hyperdimensional computing (HDC) is a neurosymbolic computational paradigm that represents symbols as high dimensional vectors and symbolic operations as vector operations, seamlessly interfacing between neuro- and symbolic components of a model. However, there is a notable gap in HDC research regarding the robustness of HDC models to input perturbations. This study presents a novel theoretical framework tailored to evaluate the robustness of hyperdimensional classifiers against perturbations in the input space. In particular, our proposed measure of robustness gives a theoretical upper bound for the magnitude of noise a model can tolerate without changing its prediction for any given data point. We also propose a method to enhance the robustness of the model based on our proposed measure of robustness. Our approach introduces several methods to calculate model robustness as a function of the specific dataset and type of hyperdimensional encoding used. The results show that the average robustness of HDC models increases under the proposed optimization scheme while maintaining accuracy by varying the variance of the Gaussian distribution used to encode hypervectors. The practical effectiveness of our proposed measure of robustness is also demonstrated.},
  archive      = {J_FRAI},
  author       = {Yeung, Calvin and Errahmouni Barkam, Hamza and Zou, Zhuowen and Yun, Sanggeon and Bastian, Nathaniel D. and Imani, Mohsen},
  doi          = {10.3389/frai.2025.1637105},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1637105},
  shortjournal = {Front. Artif. Intell.},
  title        = {Lipschitz-based robustness estimation for hyperdimensional learning},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bibliometric review of deep learning in crop monitoring: Trends, challenges, and future perspectives. <em>FRAI</em>, <em>8</em>, 1636898. (<a href='https://doi.org/10.3389/frai.2025.1636898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global agricultural systems face unprecedented challenges from climate change, resource scarcity, and rising food demand, requiring transformative solutions. Artificial intelligence (AI), particularly deep learning (DL), has emerged as a critical tool for agricultural monitoring, yet a systematic synthesis of its applications remains understudied. This paper presents a comprehensive bibliometric and knowledge graph analysis of 650 + publications (2000–2024) to map AI’s role in agricultural information identification, with emphasis on DL and remote sensing integration (e.g., UAVs, satellites). Results highlight Convolutional Neural Networks (CNNs) as the dominant technology for real-time crop monitoring but reveal three persistent barriers: (1) scarcity of annotated datasets, (2) poor model generalization across environments, and (3) challenges in fusing multi-source data. Crucially, interdisciplinary collaboration—though vital for scalability—is identified as an underdeveloped research frontier. It is concluded that while AI can revolutionize agriculture, its potential hinges on improving data quality, developing environment-adaptive models, and fostering cross-domain partnerships. This study provides a strategic framework to accelerate AI’s integration into global agricultural systems, addressing both technical gaps and policy needs for future food security.},
  archive      = {J_FRAI},
  author       = {Zhang, Rui and Wu, Xue and Li, Jing and Zhao, Pengyu and Zhang, Qing and Wuri, Lige and Zhang, Donghui and Zhang, Zhijie and Yang, Linnan},
  doi          = {10.3389/frai.2025.1636898},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1636898},
  shortjournal = {Front. Artif. Intell.},
  title        = {A bibliometric review of deep learning in crop monitoring: Trends, challenges, and future perspectives},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Named entity recognition for chinese electronic medical records by integrating knowledge graph and ClinicalBERT. <em>FRAI</em>, <em>8</em>, 1634774. (<a href='https://doi.org/10.3389/frai.2025.1634774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionGeneral purpose language models often struggle with accurately identifying domain specific terminology in the medical field, resulting in suboptimal performance in named entity recognition (NER) tasks. This challenge is particularly pronounced in Chinese electronic medical records (EMRs), which lack clear word boundaries and contain complex medical expressions.MethodsThis study proposes a novel NER method for Chinese EMRs that integrates ClinicalBERT, a language model pre trained on clinical corpora, with structured knowledge from a medical knowledge graph. Entity representations derived via Translating Embeddings (TransE) are incorporated to inject external semantic knowledge. Furthermore, the model fuses multiple character level features, including positional labels, contextual category clues, and semantic embeddings, to enhance boundary detection. The input text is annotated using the BIOES (Begin, Inside, Outside, End, Single) tagging scheme and subsequently encoded by ClinicalBERT. The encoded features are then passed through a bidirectional long short term memory (BiLSTM) network and a conditional random field (CRF) layer for final label prediction.ResultsExperiments conducted on publicly available datasets demonstrate that the proposed approach achieves an F1 score of 89.44 percent, surpassing multiple existing baseline models in performance.DiscussionThese findings confirm that the integration of domain specific language modeling, structured medical knowledge, and enriched character level features significantly enhances NER accuracy in Chinese EMRs. The proposed method shows strong potential for practical deployment in clinical information extraction systems.},
  archive      = {J_FRAI},
  author       = {Xu, Xiang and Li, Zhengxiong and Zhang, Hongwei and Ma, Kai},
  doi          = {10.3389/frai.2025.1634774},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1634774},
  shortjournal = {Front. Artif. Intell.},
  title        = {Named entity recognition for chinese electronic medical records by integrating knowledge graph and ClinicalBERT},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI: The apollo guidance computer of the exposome moonshot. <em>FRAI</em>, <em>8</em>, 1632520. (<a href='https://doi.org/10.3389/frai.2025.1632520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Exposome—the totality of environmental exposures across a lifetime—remains one of the most significant challenges in understanding and preventing human disease. Translating its vast, heterogeneous data streams into actionable knowledge requires artificial intelligence (AI) integrated with human-relevant experimental systems. We propose a unifying vision in which Microphysiological Systems (MPS) and multi-omics platforms generate high-quality, context-specific data that iteratively calibrate AI models, enabling the creation of digital twins of organs, individuals, and ultimately populations. This “Exposome Moonshot” parallels the Apollo program in ambition, with MPS as the rocket, multi-omics as the lunar module, and AI as the guidance computer. Early applications demonstrate that deep learning can already outperform canonical animal tests for several toxicological endpoints, while reducing cost and time to decision. Realizing the full potential of Exposome intelligence will require expanding the applicability domain of models, implementing robust data security, and prioritizing transparent, interpretable algorithms. By linking predictive AI with experimental feedback, we can move toward a prevention-driven, personalized paradigm for human health and regulatory science.},
  archive      = {J_FRAI},
  author       = {Sillé, Fenna C. M. and Hartung, Thomas},
  doi          = {10.3389/frai.2025.1632520},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1632520},
  shortjournal = {Front. Artif. Intell.},
  title        = {AI: The apollo guidance computer of the exposome moonshot},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing XGBoosts to correct arcjet contamination in magnetic field measurements from GOES missions. <em>FRAI</em>, <em>8</em>, 1628029. (<a href='https://doi.org/10.3389/frai.2025.1628029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The magnetometers onboard the Geostationary Operational Environmental Satellites (GOES) provide crucial measurements for space weather monitoring and scientific research. However, periodic arcjet thruster firings introduce contamination in the measured magnetic field, affecting data accuracy. The currently used correction matrix approach mitigates these effects but struggles with transient variations and residual errors. In this study, we present an alternative correction method using XGBoost, a machine learning algorithm, to correct arcjet-induced contamination in the GOES-17 magnetometer data using GOES-18 as ground truth. Using cross-satellite comparisons and supervised learning techniques, our model is effective in reducing artificial disturbances, especially non-linear variations. We found that the XGBoost method works better than the existing correction matrix approach for E and P components, while the correction matrix performs better for the N component. Although some limitations remain due to training data constraints, our results highlight the importance of machine learning to improve magnetometer data quality by recognizing and correcting complex satellite-driven artifacts. The collocation of GOES-17 and GOES-18 provided a unique opportunity for cross-satellite calibration and validation, and with a longer collocation period, the XGBoost method shows significant promise for better correction of operational data, emphasizing the need for such configurations in future satellite missions.},
  archive      = {J_FRAI},
  author       = {Inceoglu, Fadil and Loto'aniu, Paul T. M.},
  doi          = {10.3389/frai.2025.1628029},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1628029},
  shortjournal = {Front. Artif. Intell.},
  title        = {Utilizing XGBoosts to correct arcjet contamination in magnetic field measurements from GOES missions},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An artificial intelligence model for early-stage breast cancer classification from histopathological biopsy images. <em>FRAI</em>, <em>8</em>, 1627876. (<a href='https://doi.org/10.3389/frai.2025.1627876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of breast cancer subtypes is essential for guiding treatment decisions and improving patient outcomes. In current clinical practice, determining histological subtypes often requires additional invasive procedures, delaying treatment initiation. This study proposes a deep learning-based model built on a DenseNet121 backbone with a multi-scale feature fusion strategy, designed to classify breast cancer from histopathological biopsy images. Trained and evaluated on the publicly available BreaKHis dataset using 5-fold cross-validation, the model achieved a binary classification accuracy of 97.1%, and subtype classification accuracies of 93.8% for benign tumors and 92.0% for malignant tumors. These results demonstrate the model’s ability to capture morphological cues at multiple levels of abstraction and highlight its potential as a diagnostic support tool in digital pathology workflows.},
  archive      = {J_FRAI},
  author       = {Chaudhary, Neil and Dhunny, A. Z.},
  doi          = {10.3389/frai.2025.1627876},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1627876},
  shortjournal = {Front. Artif. Intell.},
  title        = {An artificial intelligence model for early-stage breast cancer classification from histopathological biopsy images},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linguistic patterns in pandemic-related content: A comparative analysis of COVID-19, constraint, and monkeypox datasets. <em>FRAI</em>, <em>8</em>, 1627522. (<a href='https://doi.org/10.3389/frai.2025.1627522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis study investigates how linguistic features distinguish health misinformation from factual communication in pandemic-related online discourse. Understanding these differences is essential for improving detection of misinformation and informing effective public health messaging during crises.MethodsWe conducted a computational linguistic analysis across three corpora: COVID-19 false narratives (n = 7,588), general COVID-19 content (n = 10,700), and Monkeypox-related posts (n = 5,787). We examined readability, rhetorical markers, and persuasive language, focusing on differences between misinformation and factual communication.ResultsCOVID-19 misinformation exhibited markedly lower readability scores and contained more than twice the frequency of fear-related and persuasive terms compared to the other datasets. It showed minimal use of exclamation marks, contrasting with the more emotive style of Monkeypox content. These findings suggest that misinformation employs a deliberately complex rhetorical style combined with emotional cues, which may enhance perceived credibility.DiscussionOur findings contribute to the growing body of research on digital health misinformation by identifying linguistic indicators that can aid in detection. They also inform theoretical models of crisis communication and public health messaging strategies in networked media environments. However, the study has limitations, including reliance on traditional readability indices, a narrow persuasive lexicon, and static aggregate analysis. Future work should adopt longitudinal designs, incorporate broader emotion lexicons, and employ platform-sensitive approaches to improve robustness. The data and code supporting this study are openly available at: https://doi.org/10.5281/zenodo.17024569.},
  archive      = {J_FRAI},
  author       = {Sikosana, Mkululi and Maudsley-Barton, Sean and Ajao, Oluwaseun},
  doi          = {10.3389/frai.2025.1627522},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1627522},
  shortjournal = {Front. Artif. Intell.},
  title        = {Linguistic patterns in pandemic-related content: A comparative analysis of COVID-19, constraint, and monkeypox datasets},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable AI-driven depression detection from social media using natural language processing and black box machine learning models. <em>FRAI</em>, <em>8</em>, 1627078. (<a href='https://doi.org/10.3389/frai.2025.1627078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionMental disorders are highly prevalent in modern society, leading to substantial personal and societal burdens. Among these, depression is one of the most common, often exacerbated by socioeconomic, clinical, and individual risk factors. With the rise of social media, user-generated content offers valuable opportunities for the early detection of mental disorders through computational approaches.MethodsThis study explores the early detection of depression using black-box machine learning (ML) models, including Support Vector Machines (SVM), Random Forests (RF), Extreme Gradient Boosting (XGB), and Artificial Neural Networks (ANN). Advanced Natural Language Processing (NLP) techniques TF-IDF, Latent Dirichlet Allocation (LDA), N-grams, Bag of Words (BoW), and GloVe embeddings were employed to extract linguistic and semantic features. To address the interpretability limitations of black-box models, Explainable AI (XAI) methods were integrated, specifically the Local Interpretable Model-Agnostic Explanations (LIME).ResultsExperimental findings demonstrate that SVM achieved the highest accuracy in detecting depression from social media data, outperforming RF and other models. The application of LIME enabled granular insights into model predictions, highlighting linguistic markers strongly aligned with established psychological research.DiscussionUnlike most prior studies that focus primarily on classification accuracy, this work emphasizes both predictive performance and interpretability. The integration of LIME not only enhanced transparency and interpretability but also improved the potential clinical trustworthiness of ML-based depression detection models.},
  archive      = {J_FRAI},
  author       = {Hameed, Sidra and Nauman, Muhammad and Akhtar, Nadeem and Fayyaz, Muhammad A. B. and Nawaz, Raheel},
  doi          = {10.3389/frai.2025.1627078},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1627078},
  shortjournal = {Front. Artif. Intell.},
  title        = {Explainable AI-driven depression detection from social media using natural language processing and black box machine learning models},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning/machine learning approach for anomaly based network intrusion detection. <em>FRAI</em>, <em>8</em>, 1625891. (<a href='https://doi.org/10.3389/frai.2025.1625891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe increasing complexity and frequency of cybersecurity threats necessitate the development of advanced detection systems capable of identifying both known and emerging attacks. In this study, we present a hybrid anomaly-based Network Intrusion Detection System (NIDS) that integrates multiple machine learning and deep learning algorithms, including XGBoost, Random Forest, Graph Neural Networks (GNN), Long Short-Term Memory (LSTM) networks, and Autoencoders.MethodsThe proposed system was trained on a large-scale dataset comprising over 5.6 million network traffic records. Comprehensive data preprocessing and feature engineering were applied, and the Synthetic Minority Over-sampling Technique (SMOTE) was employed to address class imbalance. To enhance robustness and generalization, a weighted soft-voting ensemble strategy was used to combine predictions from the individual models.ResultsThe experimental evaluation demonstrated near-perfect performance, with accuracy, precision, recall, and F1-score values approaching 100% on the primary dataset. These results were validated through rigorous 5-fold cross-validation.DiscussionEvaluation on an independent benchmark dataset confirmed the strong generalizability and robustness of the proposed model across diverse intrusion scenarios. These findings highlight the effectiveness of the hybrid ensemble framework in significantly improving intrusion detection capabilities within complex and dynamic network environments.},
  archive      = {J_FRAI},
  author       = {Almuhanna, Reem and Dardouri, Samia},
  doi          = {10.3389/frai.2025.1625891},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1625891},
  shortjournal = {Front. Artif. Intell.},
  title        = {A deep learning/machine learning approach for anomaly based network intrusion detection},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in ADHD assessment: A comprehensive review of research progress from early screening to precise differential diagnosis. <em>FRAI</em>, <em>8</em>, 1624485. (<a href='https://doi.org/10.3389/frai.2025.1624485'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention deficit hyperactivity disorder (ADHD) diagnosis traditionally relies on subjective assessments, which lead to challenges like symptom overlap, heterogeneity, and misdiagnosis risk. Artificial intelligence (AI), especially machine learning (ML) and deep learning (DL), offers objective assessment opportunities by processing complex multimodal data (behavioral, neurophysiological, neuroimaging, genetic). This paper reviews AI’s current applications in objective ADHD assessment, covering early screening, risk prediction, diagnostic assistance, classification, assistance in precise differential diagnosis, symptom quantification, and heterogeneous subtype identification. While AI models show significant potential in extracting objective biomarkers and improving assessment efficiency, the field faces challenges: insufficient standardized data, limited generalization, interpretability issues, potential biases, and lack of rigorous clinical validation. Future research must establish large-scale, standardized multimodal databases, develop robust, interpretable, and fair AI models, and conduct rigorous clinical translation validation to achieve responsible, precise, objective, and personalized ADHD assessment and management.},
  archive      = {J_FRAI},
  author       = {Zhao, Cuijie and Xu, Yan and Li, Ruixing and Li, Huawei and Zhang, Meng},
  doi          = {10.3389/frai.2025.1624485},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1624485},
  shortjournal = {Front. Artif. Intell.},
  title        = {Artificial intelligence in ADHD assessment: A comprehensive review of research progress from early screening to precise differential diagnosis},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence attitudes and resistance to use robo-advisors: Exploring investor reluctance toward cognitive financial systems. <em>FRAI</em>, <em>8</em>, 1623534. (<a href='https://doi.org/10.3389/frai.2025.1623534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe study investigates resistance towards Financial Robo-Advisors (FRAs) among retail investors in India, grounded in innovation resistance theory. The study examines the impact of functional barriers and psychological barriers on resistance to FRAs, while considering user’s attitudes towards Artificial Intelligence (AI) as a moderator. It further evaluate the influence of such resistance on users’ intentions to use and recommend FRAs.MethodsUtilizing purposive sampling data was collected from 409 investors and further analyzed using structural equation modelling.ResultsThe findings revealed that all barriers under study, expect value barrier, substantially derive resistance towards robo-advisors, with inertia being the strongest determinant. Further, this resistance impedes both the intention to use FRAs and to recommend them. Moderation analysis results finds that users’ attitude towards AI significantly weakens the influence of inertia, overconfidence bias and data privacy risk on resistance, with no such impact on other relationships.DiscussionOverall, the study enriches IRT in Fintech context and provides theoretical and practical insights to enhance FRAs adoption in emerging markets.},
  archive      = {J_FRAI},
  author       = {Verma, Balraj and Schulze, Mike and Goswami, Divya and Upreti, Kamal},
  doi          = {10.3389/frai.2025.1623534},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1623534},
  shortjournal = {Front. Artif. Intell.},
  title        = {Artificial intelligence attitudes and resistance to use robo-advisors: Exploring investor reluctance toward cognitive financial systems},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing prediction of primary site recurrence in head and neck cancer using radiomics and uncertainty estimation. <em>FRAI</em>, <em>8</em>, 1623393. (<a href='https://doi.org/10.3389/frai.2025.1623393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionHead and neck squamous cell carcinomas (HNSCC) present a significant clinical challenge due to high recurrence rates despite advances in radiation and chemotherapy. Early detection of recurrence is critical for optimizing treatment outcomes and improving patient survival.MethodsWe developed two artificial intelligence (AI) pipelines—(1) machine learning models trained on radiomic and clinical data and (2) a Vision Transformer-based model directly applied to imaging data—to predict HNSCC recurrence using pre- and post-treatment PET/CT scans from a cohort of 249 patients. We incorporated Test-Time Augmentation (TTA) and Conformal Prediction to quantify prediction uncertainty and enhance model reliability.ResultsThe machine learning models achieved an average AUC of 0.820. The vision transformer model showed moderate performance (AUC = 0.658). Uncertainty quantification enabled the exclusion of ambiguous predictions, improving accuracy among more confident cases.DiscussionOur machine learning models achieved strong performance in predicting HNSCC recurrence from radiomic and clinical features. Incorporating uncertainty quantification further improved predictive performance and reliability.},
  archive      = {J_FRAI},
  author       = {Hu, Yu and Taing, Kimberly and Wang, Jing and Sher, David and Dohopolski, Michael},
  doi          = {10.3389/frai.2025.1623393},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1623393},
  shortjournal = {Front. Artif. Intell.},
  title        = {Enhancing prediction of primary site recurrence in head and neck cancer using radiomics and uncertainty estimation},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EmoShiftNet: A shift-aware multi-task learning framework with fusion strategies for emotion recognition in multi-party conversations. <em>FRAI</em>, <em>8</em>, 1618698. (<a href='https://doi.org/10.3389/frai.2025.1618698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionEmotion Recognition in Conversations (ERC) is vital for applications such as mental health monitoring, virtual assistants, and human–computer interaction. However, existing ERC models often neglect emotion shifts—transitions between emotional states across dialogue turns in multi-party conversations (MPCs). These shifts are subtle, context-dependent, and complicated by class imbalance in datasets such as the Multimodal EmotionLines Dataset (MELD).MethodsTo address this, we propose EmoShiftNet, a shift-aware multi-task learning (MTL) framework that jointly performs emotion classification and emotion shift detection. The model integrates multimodal features, including contextualized text embeddings from BERT, acoustic features (Mel-Frequency Cepstral Coefficients, pitch, loudness), and temporal cues (pause duration, speaker overlap, utterance length). Emotion shift detection is incorporated as an auxiliary task via a composite loss function combining focal loss, binary cross-entropy, and triplet margin loss.ResultsEvaluations on the MELD dataset demonstrate that EmoShiftNet achieves higher overall F1-scores than both traditional and graph-based ERC models. In addition, the framework improves the recognition of minority emotions under imbalanced conditions, confirming the effectiveness of incorporating shift supervision and multimodal fusion.DiscussionThese findings highlight the importance of modeling emotional transitions in ERC. By leveraging multi-task learning with explicit shift detection, EmoShiftNet enhances contextual awareness and offers more robust performance for multi-party conversational emotion recognition.},
  archive      = {J_FRAI},
  author       = {Nirujan, Hinduja and Priyadarshana, Y. H. P. P.},
  doi          = {10.3389/frai.2025.1618698},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1618698},
  shortjournal = {Front. Artif. Intell.},
  title        = {EmoShiftNet: A shift-aware multi-task learning framework with fusion strategies for emotion recognition in multi-party conversations},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image restoration and key field alignment for misaligned overlapping text in secondary printing document images. <em>FRAI</em>, <em>8</em>, 1616007. (<a href='https://doi.org/10.3389/frai.2025.1616007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of information technology, the demand for efficient recognition and information extraction from paper documents in industrial scenarios has grown rapidly. In practice, business information is often secondarily printed onto pre-designed templates, which frequently leads to text misalignment or overlap with backgrounds and tables, thereby significantly impairing the accuracy of subsequent Optical Character Recognition (OCR). To address this issue, this paper proposes a preprocessing method for OCR recognition of secondary printed documents, specifically targeting the problems of text misalignment and overlap. In particular, we design a Text Overlap Restoration Network (TORNet) to restore document images affected by text overlap. Experimental results demonstrate that, compared to the latest image restoration models, TORNet achieves PSNR improvements of 0.17 dB and 0.12 dB in foreground and background text restoration, respectively. Furthermore, to resolve residual misalignment issues after image restoration, a key-field alignment method is introduced. This method accurately locates the positional deviations of critical fields in the reconstructed image, enabling precise field-level alignment and structural correction. Based on the proposed preprocessing framework, the recognition accuracy and field-matching accuracy are improved by 23% and 31%, respectively, compared to existing commercial OCR models, significantly enhancing the recognition performance on misaligned and overlapping documents. This study provides an effective solution for recognizing secondary printed documents with text overlap in industrial environments.},
  archive      = {J_FRAI},
  author       = {Wang, Senlong and Ge, Junchao and Zhang, Jiantao and He, Hong and Zhang, Yunwei},
  doi          = {10.3389/frai.2025.1616007},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1616007},
  shortjournal = {Front. Artif. Intell.},
  title        = {Image restoration and key field alignment for misaligned overlapping text in secondary printing document images},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ethical implications of ChatGPT and other large language models in academia. <em>FRAI</em>, <em>8</em>, 1615761. (<a href='https://doi.org/10.3389/frai.2025.1615761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of technology in the digital age has significantly transformed human communication and knowledge exchange. At the forefront of this transformation are Large Language Models (LLMs), powerful neural networks trained on vast text corpora to perform a wide range of Natural Language Processing (NLP) tasks. While LLMs offer promising benefits such as enhanced productivity and human-like text generation, their integration into academic settings raises pressing ethical concerns. This study investigates the ethical dimensions surrounding the use of LLMs in academia, driven by their increasing prevalence and the need for responsible adoption. A mixed-methods approach was employed, combining surveys, semi-structured interviews, and focus groups with key stakeholders, including students, faculty, administrators, and AI developers. The findings reveal a high level of LLM adoption accompanied by concerns related to plagiarism, bias, authenticity, and academic integrity. In response, the study proposes concrete strategies for ethical integration, including: (1) the establishment of transparent usage policies, (2) the incorporation of LLM literacy training into academic curricula, (3) the development of institutional review frameworks for AI-generated content, and (4) ongoing stakeholder dialogue to adapt policies as the technology evolves. These recommendations aim to support the responsible and informed use of LLMs in scholarly environments. The widespread influence of technological advancement has notably transformed communication and knowledge sharing, with LLMs playing a central role. These advanced neural networks, trained on extensive text datasets, have become valuable tools for generating human-like text and improving efficiency. However, their growing use in academic contexts raises significant ethical concerns. This study investigates these issues, focusing on the implications of LLM integration in scholarly environments. Using mixed methods, including surveys, semi-structured interviews, and focus groups, the research gathered insights from students, faculty, administrators, and AI developers. The findings highlight substantial adoption of LLMs alongside concerns about plagiarism, bias, and academic integrity. Based on this input, the study proposes guidelines for their responsible and ethical use in academia.},
  archive      = {J_FRAI},
  author       = {Almufarreh, Ahmad and Ahmad, Ashfaq and Arshad, Muhammad and Onn, Choo Wou and Elechi, Robinson},
  doi          = {10.3389/frai.2025.1615761},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1615761},
  shortjournal = {Front. Artif. Intell.},
  title        = {Ethical implications of ChatGPT and other large language models in academia},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of the hybrid machine learning models for brain tumour segmentation and detection in medical images. <em>FRAI</em>, <em>8</em>, 1615550. (<a href='https://doi.org/10.3389/frai.2025.1615550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early and accurate detection of brain tumours using Magnetic Resonance Imaging (MRI) is critical for effective treatment and improved patient outcomes. This systematic review investigates the application of hybrid machine learning (ML) and deep learning (DL) models in enhancing the computational efficiency and diagnostic accuracy of brain tumour analysis from MRI images. The study synthesizes recent advances in combining traditional ML models such as Support Vector Machines (SVM) with deep neural networks like VGG-19 and YOLOv10n. A PRISMA-based literature search strategy was employed across major databases, including PubMed, Scopus, and IEEE Xplore, selecting 25 relevant studies published between 2019 and 2024. The review evaluates the performance of standalone and hybrid models using metrics such as Dice Similarity Coefficient (DSC), Intersection over Union (IoU), accuracy, precision, recall, and F1-score. Findings indicate that hybrid models, particularly those combining SVM with CNN-based architectures like VGG-19, demonstrate improved classification accuracy and reduced false positives, outperforming single-model approaches. Lightweight versions such as YOLOv10n offer faster inference times suitable for real-time applications while maintaining competitive accuracy. Despite these advances, challenges remain in model generalizability, lack of large, annotated datasets, and limited adoption of Explainable AI (XAI) for interpretability. This review highlights the potential of hybrid models for brain tumour detection and offers recommendations for future research to focus on scalable, interpretable, and clinically deployable solutions.},
  archive      = {J_FRAI},
  author       = {Netshamutshedzi, Ndivhuwo and Netshikweta, Rendani and Ndogmo, Jean-Claude and Obagbuwa, Ibidun Christiana},
  doi          = {10.3389/frai.2025.1615550},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1615550},
  shortjournal = {Front. Artif. Intell.},
  title        = {A systematic review of the hybrid machine learning models for brain tumour segmentation and detection in medical images},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI adoption among adolescents in education: Extending the UTAUT2 with psychological and contextual factors. <em>FRAI</em>, <em>8</em>, 1614993. (<a href='https://doi.org/10.3389/frai.2025.1614993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis correlational study investigates the psychological and contextual factors associated with the adoption of artificial intelligence (AI) technologies among Italian high school students. Building on the Unified Theory of Acceptance and Use of Technology 2 (UTAUT2), the study extends the model by incorporating Problematic Internet Use (PIU) and Attitudes Toward AI (ATAI) to better account for habitual AI use and behavioural intentions.MethodA sample of 933 students (Mage = 16.20, SDage = 1.29, 54.98% female) completed a survey assessing key UTAUT2 dimensions, psychological traits, and usage patterns of AI tools in educational contexts. Confirmatory factor analysis (CFA) was used to evaluate the functioning of the adapted UTAUT2. Multiple regression was used to investigate factors predicting habit formation and behavioural intention related to AI use.ResultsConfirmatory factor analysis supported the structural validity of the adapted UTAUT2 model. Multiple regression analyses revealed that Performance Expectancy, Social Influence, Hedonic Motivation, and Schoolwork-related AI use were significant predictors of both habit and behavioural intention. PIU showed a robust association with habitual use, suggesting a spillover effect from compulsive Internet behavior to AI engagement. ATAI was associated only with behavioural intention, indicating its role in initial adoption rather than sustained use. Demographic and contextual factors (e.g., school type, citizenship) showed additional effects.DiscussionThese findings contribute to a more comprehensive understanding of adolescent AI engagement by highlighting the role of compulsive tendencies and motivational beliefs. The study underscores the importance of designing inclusive, age-appropriate interventions to promote balanced and informed AI use in educational settings.},
  archive      = {J_FRAI},
  author       = {Caffaratti, Luca Ballestra and Longobardi, Claudio and Badenes-Ribera, Laura and Marengo, Davide},
  doi          = {10.3389/frai.2025.1614993},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1614993},
  shortjournal = {Front. Artif. Intell.},
  title        = {AI adoption among adolescents in education: Extending the UTAUT2 with psychological and contextual factors},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LMS-ViT: A multi-scale vision transformer approach for real-time smartphone-based skin cancer detection. <em>FRAI</em>, <em>8</em>, 1612502. (<a href='https://doi.org/10.3389/frai.2025.1612502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is the abnormal growth of skin cells. It occurs mostly in skin exposed to sunlight. To prevent the occurrence of skin cancer, avoid exposing skin to ultraviolet radiation. Skin cancer can be very harmful if found very late. Traditional convolutional neural networks (CNNs) face challenges in fine-grained lesion classification due to their limited ability to extract detailed features. To overcome such limitations, we introduced a novel approach in the form of a lightweight multi-scale vision transformer (LMS-ViT) application for the automated detection of skin cancer using dermoscopic images and the HAM10000 dataset. Unlike CNNs, LMS-ViT employs a multi-scale attention mechanism to capture both global lesion structures and fine-grained textural details, improving classification accuracy. This study combines skin images from the HAM10000 dataset with pictures taken using a smartphone. It uses a compact method to mix important features, which makes the system faster and suitable for real-time use in medical apps. The proposed system enables real-time skin cancer classification via a smartphone camera, making it portable and platform-independent. Experimental results show that LMS-ViT surpasses CNN-based models across all skin lesion categories, achieving 90% accuracy, an 18% improvement over CNN, while reducing computational cost by 30%. LMS-ViT also improves precision, recall, and F1-score, particularly in complex categories such as Vasc (0.96 to 1.01) and Nv (0.94 to 1.01), demonstrating superior classification power. With real-time android implementation, LMS-ViT offers accessible, mobile-friendly diagnostics for early skin cancer detection.},
  archive      = {J_FRAI},
  author       = {Leema, A. Anny and Balakrishnan, P. and Gopichand, G. and Rajarajan, G.},
  doi          = {10.3389/frai.2025.1612502},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1612502},
  shortjournal = {Front. Artif. Intell.},
  title        = {LMS-ViT: A multi-scale vision transformer approach for real-time smartphone-based skin cancer detection},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of vision transformers for the detection of fullness of garbage bins for efficient waste management. <em>FRAI</em>, <em>8</em>, 1612080. (<a href='https://doi.org/10.3389/frai.2025.1612080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient waste management is crucial for urban environments to maintain cleanliness, reduce environmental impact, and optimize resource allocation. Traditional waste collection systems often rely on scheduled pickups or manual inspections, leading to inefficient resource utilization and potential overflow issues. This paper presents a novel approach to automate the detection of garbage container fullness from images using machine learning techniques. More specifically, we explore three transformer-based architectures, namely, vision transformer, Swin transformer, and pyramid vision transformer to classify input images of garbage bins as clean or dirty. Our experimental results on the publicly available Clean dirty containers in Montevideo dataset suggest that transformer-based architectures are effective in garbage fullness detection. Moreover, a comparison with existing methods reveals that the proposed approach using the vision transformer surpasses the state-of-the-art, achieving a 96.74% accuracy in detecting garbage container fullness. In addition, the generalizability of the proposed approach is evaluated by testing the transformer-based classification frameworks on a synthetic image dataset generated using various generative AI models. The proposed approach achieved a highest test accuracy of 80% on this synthetic dataset, thereby highlighting its ability to generalize across different datasets. Synthetic dataset used in this work can be found at: https://www.kaggle.com/datasets/6df0652d2c4eb3b9f00043c40fba0afa0778b46d7c0685e212807c2f6967fe6f.},
  archive      = {J_FRAI},
  author       = {Tanwer, Parakram Singh and Maheshwari, Shishir and Behera, Sushree and Chauhan, Amit and Sunil Kumar, T.},
  doi          = {10.3389/frai.2025.1612080},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1612080},
  shortjournal = {Front. Artif. Intell.},
  title        = {Evaluation of vision transformers for the detection of fullness of garbage bins for efficient waste management},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lung cancer risk prediction using augmented machine learning pipelines with explainable AI. <em>FRAI</em>, <em>8</em>, 1602775. (<a href='https://doi.org/10.3389/frai.2025.1602775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer remains the leading cause of cancer-related deaths worldwide, making early and precise diagnosis is critical for improving the patient survival rates. Machine learning has shown promising results in predictive analysis for lung cancer prediction. However, class imbalance in clinical datasets negatively impacts the performance of Machine Learning classifiers, leading to biased predictions and reduced accuracy. In an attempt to address this issue, various data augmentation techniques were applied alongside classification models to enhance predictive performance. This study evaluates data augmentation techniques paired with machine learning classifiers to address class imbalance in a small lung cancer dataset. A comparative analysis was conducted to assess the impact of different augmentation techniques with classification models. Experimental findings demonstrate that K-Means SMOTE, combined with a Multi-Layer Perceptron classifier, achieves the highest accuracy of 93.55% and an AUC-ROC score of 96.76%, surpassing other augmentation-classifier combinations. These results underscore the importance of selecting optimal augmentation methods to improve classification performance. Furthermore, to ensure model interpretability and transparency in medical decision-making, LIME is utilized to provide insights into model predictions. The study highlights the significance of advanced augmentation techniques in addressing data imbalance, ultimately enhancing lung cancer risk prediction through machine learning. The findings contribute to the growing field of AI-driven healthcare by emphasizing the necessity of selecting effective augmentation-classifier pairs to develop more accurate and reliable diagnostic models. Due to the dataset’s high cancer prevalence (87.45%) and limited size, this work is a preliminary methodological comparison, not a clinical tool. Findings emphasize the importance of augmentation for imbalanced data and lay the groundwork for future validation with larger, representative datasets.},
  archive      = {J_FRAI},
  author       = {M S, Pavithran and D, Saranyaraj and Chakrabortty, Anirban},
  doi          = {10.3389/frai.2025.1602775},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1602775},
  shortjournal = {Front. Artif. Intell.},
  title        = {Lung cancer risk prediction using augmented machine learning pipelines with explainable AI},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for cardiovascular management: Optimizing pathways and cost control under diagnosis-related group models. <em>FRAI</em>, <em>8</em>, 1580445. (<a href='https://doi.org/10.3389/frai.2025.1580445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (CVDs) remain the leading causes of morbidity, mortality, and healthcare expenditures, presenting substantial challenges for hospitals operating under Diagnosis-Related Group (DRG) payment models. Recent advances in deep learning offer new strategies for optimizing CVD management to meet cost control objectives. This review synthesizes the roles of deep learning in CVD diagnosis, treatment planning, and prognostic modeling, emphasizing applications that reduce unnecessary diagnostic imaging, predict high-cost complications, and optimize the utilization of critical resources like ICU beds. By analyzing medical images, forecasting adverse events from patient data, and dynamically optimizing treatment plans, deep learning offers a data-driven strategy to manage high-cost procedures and prolonged hospital stays within DRG budgets. Deep learning offers the potential for earlier risk stratification and tailored interventions, helping mitigate the financial pressures associated with DRG reimbursements. Effective integration requires multidisciplinary collaboration, robust data governance, and transparent model design. Real-world evidence, drawn from retrospective studies and large clinical registries, highlights measurable improvements in cost control and patient outcomes; for instance, AI-optimized treatment strategies have been shown to reduce estimated mortality by 3.13%. However, challenges—such as data quality, regulatory compliance, ethical issues, and limited scalability—must be addressed to fully realize these benefits. Future research should focus on continuous model adaptation, multimodal data integration, equitable deployment, and standardized outcome monitoring to validate both clinical quality and financial return on investment under DRG metrics. By leveraging deep learning’s predictive power within DRG frameworks, healthcare systems can advance toward a more sustainable model of high-quality, cost-effective CVD care.},
  archive      = {J_FRAI},
  author       = {Chen, Haohao and Zeng, Ying and Cai, De},
  doi          = {10.3389/frai.2025.1580445},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1580445},
  shortjournal = {Front. Artif. Intell.},
  title        = {Deep learning for cardiovascular management: Optimizing pathways and cost control under diagnosis-related group models},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding manipulative narratives in cognitive warfare: A case study of the russia-ukraine conflict. <em>FRAI</em>, <em>8</em>, 1566022. (<a href='https://doi.org/10.3389/frai.2025.1566022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis study investigates the construction and dissemination of manipulative narratives in the context of cognitive warfare during the Russia-Ukraine conflict. Leveraging a mixed-methods approach that integrates AI-assisted semantic analysis with expert validation, we examine how adversarial messaging exploits cognitive biases-such as fear and confirmation bias-to influence perceptions and disrupt institutional trust.MethodsUsing the proprietary Attack-Index tool and large language models (LLMs), we detect linguistic markers of manipulation, including euphemisms, sarcasm, and strategic framing.ResultsOur findings demonstrate that emotionally charged narratives, particularly those invoking nuclear threat scenarios, are synchronized with key geopolitical events to influence decision-makers and public opinion. The study identifies five thematic clusters and traces shifts in rhetorical strategies over time, showing how manipulative discourse adapts to geopolitical contexts. Special attention is given to the differentiated targeting of international political elites, Western publics, and Russian domestic audiences, each exhibiting varied cognitive vulnerabilities.DiscussionWe acknowledge methodological and ethical limitations, including the dual-use potential of AI tools and challenges in establishing causal inferences. Nonetheless, this study offers the following key contributions:Empirically establishing nuclear rhetoric as a strategic element of narrative manipulation, particularly around NATO summits and military aid announcements.Advancing an integrated analytical framework that combines semantic clustering and AI-based discourse detection to monitor information threats in real time.Providing actionable insights for policy and digital security, including the development of countermeasures and international collaboration in addressing cognitive warfare.},
  archive      = {J_FRAI},
  author       = {Paziuk, Andrii and Lande, Dmytro and Shnurko-Tabakova, Elina and Kingston, Phillip},
  doi          = {10.3389/frai.2025.1566022},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1566022},
  shortjournal = {Front. Artif. Intell.},
  title        = {Decoding manipulative narratives in cognitive warfare: A case study of the russia-ukraine conflict},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A drop-out mechanism for active learning based on one-attribute heuristics. <em>FRAI</em>, <em>8</em>, 1562916. (<a href='https://doi.org/10.3389/frai.2025.1562916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active Learning (AL) leverages the principle that machine learning models can achieve high accuracy with fewer labeled samples by strategically selecting the most informative data points for training. However, when human annotators provide these labels, their decisions might exhibit a systematic bias. For example, humans frequently rely on a limited subset of the available attributes, or even on a single attribute, when making decisions, as when employing fast and frugal heuristics. This paper introduces a mathematically grounded approach to quantify the probability of mislabeling based on one attribute. We present a novel dropout mechanism designed to influence the attribute selection process used in annotation, effectively reducing the impact of bias. The proposed mechanism is evaluated using multiple AL algorithms and heuristic strategies across diverse prediction tasks. Experimental results demonstrate that the dropout mechanism significantly enhances active learning (AL) performance, achieving a minimum 70% improvement in effectiveness. These findings highlight the mechanism's potential to improve the reliability and accuracy of AL systems, providing valuable insights for designing and implementing robust intelligent systems.},
  archive      = {J_FRAI},
  author       = {Ravichandran, Sriram and Sudarsanam, Nandan and Ravindran, Balaraman and Katsikopoulos, Konstantinos V.},
  doi          = {10.3389/frai.2025.1562916},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1562916},
  shortjournal = {Front. Artif. Intell.},
  title        = {A drop-out mechanism for active learning based on one-attribute heuristics},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lexicon obtained and validated by a data-driven approach for organic residues valorization in emerging and developing countries. <em>FRAI</em>, <em>8</em>, 1557137. (<a href='https://doi.org/10.3389/frai.2025.1557137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open dump remains the main management process of organic residue in middle-and low-income countries [1]. Indeed, according to this study, municipal solid waste is composed of 44% organic fraction. However, waste recycling or valorization is about 7, 4.7, and 21% in Sub-Saharan Africa, Caribbean/Latin America, and South Asia respectively. It is thus interesting to determine organic residue valorization status in those regions. Answer to that question could be prospected through textual analysis. The method herein represents the first step to that end. Indeed, when missing, text mining could be used to extract thematic lexicon from a bibliographic corpus to drive a state-of-art in the valorization of organic residues in agriculture in developing countries. In this work, text mining and Natural Language Processing (NLP) methods enable to generate a specialized lexicon on this specific area. The definition of relevance of terms is challenging and discussed in this data paper. Actually, terminology extraction methods are generally based on benchmarks (i.e. gold-standard) or terms manually validated [2] but an experimental protocol that takes into account different kinds of relevance to consolidate the process is understudied. This needs to integrate expertise knowledge, agreement of experts regarding definitions and evaluation associated with, and the task to do. This paper highlights how this construction is conducted by considering different point-of-view of relevance in a multidisciplinary context. It is important to notice that this kind of lexicon specifically focused on organic residues valorization does not exist in agriculture semantic resources like AgroPortal which include more than 200 ontologies/thesaurii/lexicons [3].The present work consisted of using text mining approach to construct thematical lexicon from a corpus related to valorization of organic waste in developing countries. Method used to collect data was detailed first, followed by a section about the technic adopted to select, annotate, and validate the lexicon. Finally, future perspective work was explained in a concluding section. This exploratory methodology could be used to guide a more in-depth and oriented text analysis of scientific publications (i.e. scientometric analysis). Moreover, this methodology can be reused and/or adapted in other domain depending on purpose. In our ongoing work, we use this lexicon to conduct a semantic analysis of scientific publications dealing with organic residues valorization in emerging and developing countries.Proposed Method to Collect the DataSeveral online databases were consulted in 2021, to extract articles relating to biotransformation and valorization in agriculture of organic residues in emerging and developing countries (WoS, Ovid, Scopus, Google scholar, HAL, Cairn.info, AGRIS, and Agritropfoot_0 ) published until 2021. Terms used for bibliographic search in all databases through specific queries are detailed in the Appendix section of this paper.The equation used in the Web of Science collection was thereafter adapted for the other databases specificities. Advanced search was not available for most of the free online database, a global thematical search was then adopted (Appendix 1). The search gave 24 186 references on which a selective sorting was conducted to avoid duplicates and to select references in English only. A total of 7 692 references were used to generate the dataset available in the excel file (Initial_Corpus_References.xlsx) available on depository [4]. The corpus of the dataset combines articles, reports, book sections, and student thesis with bibliographic references (authors, year of publication, title, doi, and url).BioTex [5] was used to perform an Automatic Term Extraction (ATE) on the corpus. The terms extracted (e.g. rumen, humic acid, nutrient recovery, …) give a semantic point of view of the theme of the text. This tool was developed for Biomedical term extraction [6] and was adapted to extract terms associated with food security [7]. First, BioTex performed a linguistic screening through syntactic patterns (noun-noun, adjective-noun, …). In order to rank terms extracted on the "titles" corpus, the F-TF-IDF-C score integrated to BioTex was applied. This measure combines (i) C-value (4) to favor multi-word terms extracted, and (ii) TF-IDF (Term Frequency-Inverse Document Frequency) to highlight discriminative terms [6].Text mining was thereafter performed on titles of the corpus using the BioTex tools [5] and the result can be found in the associated excel file (Extracted_Terms.xlsx) on depository (1). The first column contains the 19 580 terms obtained from the extraction. The second column ("term") presents the terms constituted of words or compound nouns (e.g. mulch, effluents, soil amendments, bagasse cocomposting). The rank, in the last column, is obtained by maximizing a discriminative score associated with terms (i.e. F-TF-IDF-C).Five specialist raters conducted a first annotation on 200 sampled candidate terms among the 19 580 to exclude irrelevant terms to the topic of interest following the guideline file (Annotation_guidelines.pdf) available on the depository [4]. Specialists were researchers in "Recyclage et risque" unit of Cirad in France, working on recycling organic residue in agriculture and associated risks. The group was specialized in biochemistry, agronomy, microbiology, ecologist, soil science, and environmental assessment using both monitoring and modelling approaches. Each rater was asked to categorize each candidate term belonging to i) organic residues (OWT) and/or ii) biotransformation process (TM) and/or iii) valorization in agriculture (AV) or iv) none of them (None)following the first annotation guide. Definition of each category is described in the annotation guidelines. Table 1 shows example of the first step of annotation conducted by specialist.The Fleiss Kappa [8] which measures agreement between several raters equals to 0.52 for this first annotation corresponding to a bad agreement between the 5 raters. The 4 categories chosen to annotate the candidate terms appeared to be too restrictive. Terms indirectly associated to one or more of the 4 categories have been excluded by several raters.In a second annotation guideline, the manual labelling process focuses on the overall degree of pertinence related to the topic of valorization of organic residues. In this context, candidate term was annotated according 3 classes: (i) very pertinent when it was directly connected to one or more category(ries) (i.e. OWT+, TM+, AV+), (ii) pertinent when it was indirectly connected to one or more category(ries) (i.e. OWT, TM, AV), and (iii) irrelevant (i.e. None).A second annotation on the same 200 sampled terms was conducted. All results of the two series of annotation can be viewed with the file "Raters_Annotation_Results.xlsx" in our dataset [4]. Fleiss Kappa was calculated for 3 and 5 raters. It revealed a decreasing trend of the value (0.84 to 0.60) with increasing number of raters. Closer comparison highlighted more terms indirectly related to one or more category(ies) selected by 3 raters with high value of Kappa. In order to include as many terms indirectly related to the subject as possible, it was decided to apply the logic of these 3 annotators to pursue the categorization of the remaining terms.In Table 2, the results are evaluated in terms of precision (percentage of pertinent terms) obtained over the top k extracted terms (P@k). The results confirm that the ranking function of BioTex is adapted by highlighting relevant terms at the top of the list. For instance, precision value with k=100 and k=200 is high (more than 80%) but recall will be low because a lot of relevant terms are not proposed. Actually, a precise recall value is difficult to calculate because we do not have gold standard.The above detailed dataset can be found in the CIRAD Dataverse repository [4].One of the 5 specialists then pursued the annotation, with a degree of relevance, on the remaining extracted terms. It was decided to continue the categorization with the degree of pertinence and to apply the logic of the 3 annotators with the high kappa value explained above. It took about one-week work for the rater to conduct the categorization. The same five raters were then asked to verify and finalize the terms selection related to the biotransformation and valorization in agriculture of organic residues in low-income countries. All verified relevant terms were combined in the last file on the depository (Pertinent_Terms.xlsx), containing terms which can be indirectly (first sheet) or directly (second to fourth sheet) related to the topic.From the 19 580 initial candidate terms, about 75% were not associated to the topic of interest (Table 2). Irrelevant terms included words which are not related to organic residues nor biotransformation nor valorization in agriculture, such as: absence, certification, design, effect, fecundity, fitness, gray, immune response, integration analysis, low cost, marker genes, …. Among the 25% relevant terms, 2 079 were closely associated with the organic residues valorization in emerging and developing countries such as sludge, sewage, livestock, manure, slurry, anaerobic digestion, composting, vermicomposting. Several terms can be found in the glossary of terms related to livestock and manure management [9] and figure among terms with high pertinence in this dataset. Moreover, some of relevant terms are cited in literatures as the biotransformation (e.g.: anaerobic digestion, composting, bioethanol, biohydrogen) and valorization in agriculture (e.g.: biofertilization, organic fertilizers, amendments) of organic residues (e.g. rice straw, sugarcane bagasse, animal manure) [10], [11].The produced lexicon is currently used in a semantic-driven analysis of our corpus based on the CorTexT software [12]. In the context of this multidisciplinary work on-going, we obtain deeper knowledge regarding bioconversion and valorization in agriculture of organic residues in low-income countries as highlighted in Figure 1-A and B.The text-mining tool used in this work is based on statistical criteria that highlight discriminative terms. This method identifies significant terms that are present in the texts. As future work, the proposed framework could be extended by extracting variation of terms [13] that enables to recognize rare and/or unsystematic terms but also synonyms. Moreover, embedding approaches [14], language models [15], generative methods based on LLM (Large Language Models) techniques [16] could be applied to recognize new terms. Language model techniques are based on generic models like BERT -Bidirectional Encoder Representations from Transformers [15] or specific ones like AgriBERT -Knowledge-Infused Agricultural Language Models for Matching Food and Nutrition [17] dedicated to the agriculture domain. These models can be fine-tuned for specific tasks like terminology extraction.There can be used to improve terminology extraction. Note that the use of language models could be relevant for specific NLP tasks and domains like the agriculture area [18]. As future work, we plan to compare the applied methods described in this paper with other approaches based on language models but also Large Language Models (LLM) for terminology extraction [19]. LLM could also be used to expand our initial lexicon. This enables to extract variations of exiting terms and synonyms but also new terms. In the context of our work, the objective is to conduct a semantic analysis of terms present in the corpus, so the use of words or phrases in our lexicon but not used in our dataset (i.e. corpus) is not really useful. Web of science Core collection query: WOS, FSTA and Biosis TS = ("sewage sludge" OR "crop residue*" OR "agricultural waste" OR "industrial waste" OR "food waste" OR "household waste" OR "organic waste" OR "urban waste" OR "co-product*" OR "byproduct*" OR "biomass" OR "organic waste product*" OR mulch OR digestate* OR compost*) AND TS = (decomposition OR fermentation OR anaerobic OR aerobic OR methanisation OR composting OR vermicomposting OR fertilization OR bokashi OR biodegradation OR mineralization OR recycling OR "agricultural valuation" OR biotransformation OR mulching) AND TS = (africa OR "acp countries" OR "central america" OR "south america" OR "latin america" OR "south east asia" OR "south asia" OR afghanistan OR angola OR albania OR argentina OR armenia OR antigua OR azerbaijan OR burundi OR benin OR "burkina faso" OR bangladesh OR bosnia OR belarus OR belize OR bolivia OR brazil OR bhutan OR botswana OR "central african republic" OR china OR "ivory coast" OR cameroon OR congo OR colombia OR comoros OR "cape verde" OR "costa rica" OR cuba OR djibouti OR dominica OR "dominican republic" OR algeria OR ecuador OR egypt OR eritrea OR ethiopia OR fiji OR micronesia OR gabon OR georgia OR ghana OR guinea OR gambia OR grenada OR guatemala OR guyana OR honduras OR haiti OR indonesia OR india OR iran OR iraq OR jamaica OR jordan OR kazakhstan OR kenya OR kyrgyzstan OR cambodia OR kiribati OR "lao people's democratic republic" OR lebanon OR liberia OR libya OR "saint lucia" OR "sri lanka" OR lesotho OR morocco OR moldova OR madagascar OR maldives OR mexico OR "marshall islands" OR "north macedonia" OR mali OR myanmar OR montenegro OR mongolia OR mozambique OR mauritania OR montserrat OR mauritius OR malawi OR malaysia OR namibia OR niger OR nigeria OR nicaragua OR niue OR nepal OR nauru OR pakistan OR panama OR peru OR philippines OR palau OR "papua new guinea" OR " Democratic People's Republic of Korea" OR "north korea" OR paraguay OR "palestinian territory" OR rwanda OR sudan OR senegal OR "saint helena, ascension and tristan da cunha" OR "solomon islands" OR "sierra leone" OR "el Salvador" OR somalia OR serbia OR "south sudan" OR "sao tome and principe" OR suriname OR eswatini OR "syrian arab republic" OR chad OR togo OR thailand OR tajikistan OR tokelau OR turkmenistan OR "timor-leste" OR tonga OR tunisia OR turkey OR tuvalu OR tanzania OR uganda OR ukraine OR uzbekistan OR "saint vincent and the grenadines" OR venezuela OR "vietnam" OR vanuatu OR "wallis and futuna" OR samoa OR yemen OR "south africa" OR zambia OR zimbabwe). Due to a very high number obtained with the equivalent of WoS query, the following query was used for scopus with subject area=environmental sciences or agricultural. Then, only articles, reviews and conference paper were selected.TITLE-ABS-KEY ( "sewage sludge" OR "crop residue*" OR "agricultural waste" OR "industrial waste" OR "food waste" OR "household waste" OR "organic waste" OR "urban waste" OR "coproduct*" OR "by-product*" OR "biomass" OR "organic waste product*" OR mulch OR digestate* OR compost* ) AND TITLE-ABS-KEY ( decomposition OR fermentation OR anaerobic OR aerobic OR methanisation OR composting OR vermicomposting OR fertilisation OR bokashi OR biodegradation OR mineralisation OR recycling OR "agricultural valuation" OR biotransformation OR mulching ) AND ( LIMIT-TO ( SUBJAREA , "ENVI" ) OR LIMIT-TO ( SUBJAREA , "AGRI" ) ) AND ( LIMIT-TO ( DOCTYPE , "ar" ) OR LIMIT-TO ( DOCTYPE , "re" ) OR LIMIT-TO ( DOCTYPE , "cp" ) )Google scholar, HAL, Cairn.info, AGRIS:Advanced research was not available on free databases; the research was thus conducted with a general query on the topic which was:-In French : « biotransformation et valorisation en agriculture dans les contextes des pays du Sud » -In English « biotransformation et valorization in agriculture in low-income countries» in English. The query was then tested by adding Africa, Latin America, then South-East Asia ».},
  archive      = {J_FRAI},
  author       = {Rakotomalala, Christiane and Paillat, Jean-Marie and Feder, Frédéric and Avadí, Angel and Thuriès, Laurent and Vermeire, Marie-Liesse and Médoc, Jean-Michel and Wassenaar, Tom and Hottelart, Caroline and Kieffer, Lilou and Ndjie, Elisa and Picart, Mathieu and Tchamgoue, Jorel and Tulle, Alvin and Valade, Laurine and Boyer, Annie and Duchamp, Marie-Christine and Roche, Mathieu},
  doi          = {10.3389/frai.2025.1557137},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1557137},
  shortjournal = {Front. Artif. Intell.},
  title        = {A lexicon obtained and validated by a data-driven approach for organic residues valorization in emerging and developing countries},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An AI-powered framework for assessing teacher performance in classroom interactions: A deep learning approach. <em>FRAI</em>, <em>8</em>, 1553051. (<a href='https://doi.org/10.3389/frai.2025.1553051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionTeacher performance evaluation is essential for improving instructional quality and guiding professional development, yet traditional observation-based methods can be subjective, labor-intensive, and inconsistently reliable. This study proposes an AI-powered framework to objectively assess classroom interactions.MethodsWe developed and evaluated a computer-vision framework using three state-of-the-art object detectors—YOLOv8, Faster R-CNN, and RetinaNet—to identify eleven classroom interaction categories. A labeled dataset of 7,259 images collected from real classroom settings was annotated and used for training and evaluation. Performance was assessed using mean Average Precision (mAP).ResultsYOLOv8 achieved the best performance among the evaluated models, with an mAP of 85.8%, indicating strong accuracy in detecting diverse classroom interactions. Faster R-CNN and RetinaNet performed competitively but were outperformed by YOLOv8.Discussion/ConclusionThe results demonstrate that modern deep learning–based detection can provide more objective and reliable insights into teacher–student interactions than traditional approaches. The proposed framework supports evidence-based evaluation and has the potential to enhance feedback and outcomes in educational practice.].},
  archive      = {J_FRAI},
  author       = {Almubarak, Arwa and Alhalabi, Wadee and Albidewi, Ibrahim and Alharbi, Eaman},
  doi          = {10.3389/frai.2025.1553051},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1553051},
  shortjournal = {Front. Artif. Intell.},
  title        = {An AI-powered framework for assessing teacher performance in classroom interactions: A deep learning approach},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced deep convolutional neural network for SARS-CoV-2 variants classification. <em>FRAI</em>, <em>8</em>, 1512003. (<a href='https://doi.org/10.3389/frai.2025.1512003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionRapid and scalable classification of SARS-CoV-2 genomes from spike-gene sequences can support real-time genomic surveillance in contexts where whole-genome data or high-end computing resources are limited.MethodsWe curated approximately 35,800 quality-filtered spike sequences spanning multiple clades and lineages and trained a hybrid CNN–BiLSTM model with standard regularization and class-imbalance handling. Model performance was benchmarked against Nextclade assignments and compared with classical machine-learning baselines.ResultsAcross 10 experimental runs, the model achieved a mean training accuracy of 99.74% ± 0.11, a validation accuracy of 99.00% ± 0.00, and a test accuracy of 99.91% ± 0.03. In benchmarking against the molecular epidemiology tool Nextclade, our model demonstrated superior performance, correctly identifying 100% of Omicron sequences, compared to 34.95% achieved by Nextclade. Saliency and feature attribution analyses highlighted recurrent spike substitutions consistent with known variant-defining mutations, as well as additional uncharacterized motifs with potential biological relevance.DiscussionThese findings demonstrate that spike-only deep models can provide rapid and accurate clade or variant classification, while also yielding interpretable feature importance. Such models complement phylogenetic approaches in settings with constrained resources and enable efficient triage of samples for confirmatory whole-genome analysis, supporting more timely genomic surveillance.},
  archive      = {J_FRAI},
  author       = {Awe, Olaitan I. and Obura, Hesborn and Ssemuyiga, Charles and Mudibo, Evans and Mwanga, Mike J.},
  doi          = {10.3389/frai.2025.1512003},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1512003},
  shortjournal = {Front. Artif. Intell.},
  title        = {Enhanced deep convolutional neural network for SARS-CoV-2 variants classification},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global reform population health management as stewarded by higher expert medical science safety (HEMSS). <em>FRAI</em>, <em>8</em>, 1496948. (<a href='https://doi.org/10.3389/frai.2025.1496948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As described in a Memorandum of Understanding (MoU) on AI infrastructure, global human phenotype ontology (HPO) is a priority for the US and the UK. The UK NHS Act of 1946 and the Medicare and Medicaid Act of 1965 classify using genomics as primary care, supporting international HPO aims for Population Health Management (PHM). The Higher Expert Medical Science Safety (HEMSS) proposes the NHS England, Genomics, and Biobank agile group developers. The HEMSS strategy executes the PHM of the HPO through digital records, pilot citizen predictor pre-eXams, and precise eXam intercept classifications, continuously improving public safety. PHM reform includes biobank opportunities for Value-Based Care (VBC) stratifying genomic and socio-environmental factors that risk HPO in disease segmentation. The author evaluated a standard approach to PHM for HPO with mature and advanced interoperable standards. A reform toolkit aligns adversarial, neural, and transformer models for Generative AI by utilizing multimodal data nuanced for fairness in Quantum Intelligence. The recommendations include HEMSS steps from well-being evaluations to the PHM strategy for HPO in the UK-US. Concepts involve piloting the scaling up of neighborhood clinics and federal centers through reform classification. Plans for citizen privacy facilitate data use with access to reference biobanks, ensuring DNA democratization and national cybersecurity. The UK NHSE corporate governance and US federal authorities monitor and reform the Integrated Care Board assessments and the Centers for Medicare and Medicaid Services surveys using agile methods. The UK-US MoU for AI safety is an international ideal for PHM, creating a safe space for HPO adherence to predictive and interceptive adoption for health and socioeconomic growth. HEMSS Agile Group Development impacts ethical and societal primary care debates. HEMSS discussions on global public health inclusiveness and national engagement aim to govern the classification phases for adherence. Therefore, debates on UK-US accreditation or regulation on the future of Artificial General Intelligence follow. The author concludes in support of the Population Health Management Expert Medical Science Safety Agile Group Development Program. The UK and US governments would benefit from this proposition, and international goals for well-being and socioeconomic growth would also be supported.},
  archive      = {J_FRAI},
  author       = {Henry, James Andrew},
  doi          = {10.3389/frai.2025.1496948},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1496948},
  shortjournal = {Front. Artif. Intell.},
  title        = {Global reform population health management as stewarded by higher expert medical science safety (HEMSS)},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on knowledge and information extraction from PDF documents and storage approaches. <em>FRAI</em>, <em>8</em>, 1466092. (<a href='https://doi.org/10.3389/frai.2025.1466092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionAutomating the extraction of information from Portable Document Format (PDF) documents represents a major advancement in information extraction, with applications in various domains such as healthcare, law, or biochemistry. However, existing solutions face challenges related to accuracy, domain adaptability, and implementation complexity.MethodsA systematic review of the literature was conducted using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to examine approaches and trends in PDF information extraction and storage approaches.ResultsThe review revealed three dominant methodological categories: rule-based systems, statistical learning models, and neural network-based approaches. Key limitations include the rigidity of rule-based methods, the lack of annotated domain-specific datasets for learning-based approaches, and issues such as hallucinations in large language models.DiscussionTo overcome these limitations, a conceptual framework is proposed comprising nine core components: project manager, document manager, document pre-processor, ontology manager, information extractor, annotation engine, question-answering tool, knowledge visualizer, and data exporter. This framework aims to improve the accuracy, adaptability, and usability of PDF information extraction systems.},
  archive      = {J_FRAI},
  author       = {Atagong, Salvador D. and Tonnang, Henri and Senagi, Kennedy and Wamalwa, Mark and Agboka, Komi M. and Odindi, John},
  doi          = {10.3389/frai.2025.1466092},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {1466092},
  shortjournal = {Front. Artif. Intell.},
  title        = {A review on knowledge and information extraction from PDF documents and storage approaches},
  volume       = {8},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="frobt">FROBT - 21</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Understanding consumer attitudes towards second-hand robots for the home. <em>FROBT</em>, <em>12</em>, 1694690. (<a href='https://doi.org/10.3389/frobt.2025.1694690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {INTRODUCTION First published in 2015, the United Nations announced 17 Sustainable Development Goals [UN SDG] as part of their 2030 agenda for sustainable development providing a "plan of action for people, planet and prosperity" UN2 (2015). SDG 12 aims to "ensure sustainable production and consumption patterns" to tackle the impacts of open loop consumption UN2 (2015). These impacts include climate change, biodiversity loss and deforestation, and pollution which harms people, animals and habitats UN2 (2015); Dauvergne and Lister (2010); Carlisle and Hanlon (2007). With 80 percent of a product's environmental impact decided during the design and development phases of a product's life cycle Charter (2019) and human open-loop consumption impacting so heavily on climate change Łukasz Kurowski et al. (2022), building sustainable consumption patterns requires the buy-in and leadership of the product manufacturers to develop systems which are better suited to closed-loop consumption, also referred to as the 'Circular Economy'UKR (2021); Elzinga et al. (2020). Many electronic product manufacturers rely on the concept of recycling as a method to tackle open-loop consumption. However, across the globe, there is evidence of huge inefficiencies in the process of recycling. Globally, in 2019, only 17.4 percent of electronic waste created annually was recycled through formally managed systems Forti et al. (2020). Individuals living within the EU produce the highest levels of e-waste globally at 16.2kgs per capita per year Forti et al. (2020). Recycling rates are also highest in the EU, though rates only reach 42.5 percent Forti et al. (2020). In comparison, annually (in 2019), individuals in Oceania produce 16.1 kgs per capita and recycled 8.8 percent, in the Americas they produced 13.3 kg per capita and recycled 9.4 percent, Asia produced 5.6 kg per capita and formally recycled 11.7 percent, while in Africa per capita, e-waste production annually was 2.5 kg and recycling rates were 0.9 percent Forti et al. (2020). Consumer robots, generally being systems which require electrical current or electromagnetic fields to meet their functional purpose, could be considered as Electronic or Electrical Equipment [EEE] under the current standard accepted definition 200 (2003). Therefore, when a consumer robot reaches the end of its useful life it will also then be classed as Waste Electronic or Electrical Equipment [WEEE], also known as e-waste. And, taking the global data for recycling rates of other electronic products, it would be reasonable to assume that, without specific intervention from the robotics industry, recycling rates for robotic systems will be similarly low to those seen for other types of e-waste due to the consistently poor actions and habits of the general population in managing e-waste from the home. Alternatives to recycling and landfill for robotic systems are; repurposing, reusing, remanufacturing, reconditioning and repairing the systems at the end of their primary life. These options extend beyond the traditional 3 R's: Reduce, Reuse, Recycle HMG (2018), recognising the additional alternatives of Repair, Reconditioning, Remanufacturing and Repurposing. Reuse, recondition and repair are well-established processes; remanufacturing is a less common but well-documented method outside of the robotics industry for breaking down a system to component level and rebuilding it as new Nasr (2019); Steinhilper (1998); Steinhilper et al. (2017); while repurposing is a new concept for robotic systems which is under investigation by the authors of this paper McGloin et al. (2023). Each of these alternative methods to recycling aims to increase the working life of a product, which in turn reduces waste production by delaying the time until the system needs to be recycled or sent to landfill, and forms the basis of a circular economy (UN SDG #12). In addition to this, the forming of a circular economy for robotic products for domestic settings will also; increase accessibility to technology through opportunities for lower-cost second-hand systems (UN SDG #8); reduce deforestation of land used for mining materials needed for production (UN SDG #15) and reduce emissions associated with the production of electronic products such as robots (UN SDG #13). However, a critical element in making a circular economy viable lies in the participation of consumers in this business model and the ability to sell the resultant second-hand product to customers Elzinga et al. (2020). Consumers often require both push and pull influence factors to be in place to overcome established habits and be persuaded to purchase second-hand over new products Hazen et al. (2017). Push factors include the opportunity to purchase second-hand products at favourable rates to new ones, and the introduction of laws driving consumer habit changes. Pull factors might include tax breaks, government-or business-led incentives, and the purchaser's personal perspective on environmental issues Hazen et al. (2017); Łukasz Kurowski et al. (2022). Additionally, social media may create both push and pull influence factors for consumer purchasing habits. The aim of this study, therefore, was to assess the difference in consumer attitudes towards the purchase of new and second-hand robotic systems for domestic settings and to identify key factors which could progress or hinder the uptake of second-hand robots amongst potential users of robots for the home. This research was carried out using a survey method described in Section 2. Understanding potential consumer behaviours towards second-hand robotic systems provides an opportunity for researchers, developers and manufacturers of robots for the home to build more sustainable practices into the products they create before they become ubiquitous. Once in an accepted ubiquitous state, lock-in factors inhibit changes by both consumers and the Original Equipment Manufacturers from easily making more sustainable practice choices Aminoff and Sundqvist-Andberg (2021); Mac (2023). Additionally, the results of this research are part of a wider program which is also investigating the attitudes of the robotics industry towards sustainability, and the reuse and repurposing of robotic systems. The results of the industrial attitudes research will be presented in a separate paper. METHODOLOGY 2.1 Research Aim The aim of this research was to assess consumer interest in second-hand robots - such as those which have been reconditioned or repurposed, in comparison to new systems, and to identify key factors which could affect the uptake of second-hand systems. In order to meet the aims of the study, an appropriate research philosophy was selected (Section 2.2) which influenced the development of the study design (Section 2.3). Data was then collected (Section 3) and analysed (Section 4), with conclusions being presented in Section 5. 2.2 Research Methodology Saunders et al. (2019) present the 'research onion' (Figure 1) which summarises the methodological choices, strategies, data collection and analysis methods, which together form the widely accepted methodologies used in research strategies. The methodologies utilised in this research were; An Inductive research approach due to there being limited existing data available from the robotics field that could produce a theory which might, in turn, validate or invalidate a hypothesis based on the data collected. A survey method for data collection was selected as very few people already own robots, so observational data could not be collected for the purpose of this research. Instead, data collection relied on participants' opinions based on the information provided to them and utilised a Likert Scale system with free-text open-response questions for additional participant responses. Likert Scales were selected because they are easy for participants to use, resulting in increased response rates and reliability Jupp (2006). Quantitative analysis methods were selected for the Likert Scale survey data which was then transcribed from ordinal data to interval data during the analysis process. This transcription allows for the conversion of the opinions of participants into numerical-based data Fleetwood (2014). Figure 1. The 'research onion' (pg 130 Saunders et al. (2019). Source: Saunders MNK, Lewis P and Thornhill A (2019) Research methods for Business Students (8th edition) Harlow: Pearson: p 130. The research onion is © 2018 Mark Saunders, Philip Lewis and Adrian Thornhill and is reproduced in this article with their written permission Qualitative analysis methods were selected for the analysis of the free-text survey responses via Thematic analysis which aims to understand the core themes presented both in a participant's individual responses and also between participants' responses Bryman (2016). 2.3 Study Design Following the selection of the research methodology (Section 2.2), a survey was developed which was designed for access by the general public via the Qualtrics online platform. The survey was divided into the following sections: • Demographics and lifestyle factors - individuals were required to provide demographic data such as age range and country of residence, alongside a broad range of personal lifestyle indicator factors that the research team felt could influence the likelihood of uptake second-hand robotics. These lifestyle factors included attitudes to environmental topics, home ownership status, prior ownership of robotic systems in the home, uptake of internet-enabled technology devices in the home and previous purchasing decisions for home technology. In this survey, internet-enabled devices were defined as a device which requires connection to WiFi or mobile data in order to function, but does not include mobile phones, laptops, computers or tablets. All questions in this section provided participants with multiple-choice options for their responses. A concentration test question was also placed at the end of this section, to reduce the effect of random selection responses. • Purchasing attitudes - participants were presented with a variety of robot types that could possibly be purchased in the future via different purchasing conditions; new, second-hand with a guarantee, and second-hand without a guarantee. For each robot type, a short description was given, and examples were also provided (see Figure 2). Participants were then asked for their opinions on if they would or would not be inclined to purchase the robots in the different purchase conditions. The proposed robot types included; robots used to provide security, perform household chores, to have as pets, work Figure 2. Types of consumer robots presented to participants in the online survey, along with examples of each robot type. Note, graphic not used in survey, only wording. as personal assistants, and act as health and fitness instructors. Participants were asked to respond to questions using a 3-point Likert Scale to demonstrate their interest in a given system. • Concern factors - for each purchase condition, (new, second-hand with a guarantee and second-hand without a guarantee) participants were asked to rate their concern levels on a 5-point Likert scale for factors including cost, security, environmental impact and safety. The 5 -point Likert Scale gave participants the option to respond Very Concerned, Slightly Concerned, Neutral, Slightly Unconcerned and Unconcerned for a given factor. In addition to this, a free-text answer box was provided to collect additional comments from participants. Lastly, the participant survey design included the presentation of a downloadable participant information sheet and a data privacy policy. 2.4 Data collection and analysis procedure The participant study gained ethical approval through the University of West England. Following this, the study was shared through both social media platforms and via surveycircle - a website for finding survey participants. The survey was open to any individual who was over the age of 18 years old and consented to take part in the research. The responses submitted by participants were checked and rendered anonymous after 7 days. Excel was used to analyse the demographic and Likert scale data, while NViVo was utilised for the analysis of free-text responses. The Nvivo analysis followed a Thematic approach (as described in Section 2.2). The method to complete the thematic analysis within Nvivo was: • Phase I – Open Coding: this required the line-by-line analysis of raw data (from the survey free-text responses) to draw out concepts presented within the data Khaksar et al. (2015); Charmaz (2006). The purpose of this phase was to find meaning and actions behind the words given by the participant. Each concept was labelled (referred to as a code), to enable repeating concepts to be highlighted under the same code Hutchison et al. (2010). At this stage it was expected that as wide a range of coded concepts be identified as possible since fitting answers under pre-existing data labels would limit the analysis and stop new ideas from emerging Charmaz (2006)). The codes were considered provisional and could be amended at any time. The overall aim was to "make the codes fit the data" rather than "forcing the data to fit" the codes Charmaz (2006). • Phase II – Thematic framework: during this phase, codes were grouped into categories and sub-categories and the links between the codes which form categories were noted Khaksar et al. (2015). In this way, the data which was analysed at a line-for-line level in the open coding was brought back together and reassembled by identifying the connections between the codes Hutchison et al. (2010), thereby highlighting the key themes in the research. 2.5 Participant demographics and Lifestyle factors A total of 111 individuals responded to the online survey, including responses from 16 individuals who did not fully complete the survey and whose responses were removed from further data analysis. Of the remaining responses, 72 were from individuals in the UK and 23 were from individuals outside of the UK. Those responses from individuals outside of the UK covered 9 different countries and, for the purpose of the study presented within this paper, were not included in the data analysis set due to the small sample size. Age group (years) 18-25 26-35 36-45 46-55 56-65 66-75 76+ Survey Participants, n=72 13% 29% 21% 11% 18% 7% 1% UK Population Sta (2021) 11% 17% 16% 16% 16% 13% 12% Table 1. Percentage of population in each age group (listed in years) Of the 72 responses from UK residents carried forward for analysis, 60 percent of participants were female, compared to 51 percent of the UK populationGov (2018). A comparison of the age demographic of the survey participants versus the UK population is shown in Table 1. The age group with the greatest over-representation in comparison to the UK population was those aged 26-35, while the greatest underrepresentation was in the 76+ age group which highlights the limitation of using online platforms to both advertise and complete the survey. Additionally, the participants for the study represented the following demographics: • Home ownership rate - 67 percent of participants lived in owned (with or without a mortgage) accommodation, versus 63 percent of people in the UK Gov (2020). • At-home dependants - 29 percent of participants lived in homes with children under the age of 18 in them, compared to 45 percent of people in the UK living in households containing one or more dependant child ONS (2022). • Climate emergency beliefs - 96 percent of participants who completed the survey selected that they believe there is a climate emergency, versus 71 percent in the general UK population Booth-Dale (2022). RESULTS 3.1 Purchasing habits by consumers for non-robotic products To better understand the attitudes of participants towards second-hand robots, prior electronic purchasing habits were investigated as part of the participant demographics. Excluding mobile phones and laptops, Figure 3. Conditions which participants bought other electronic items participants were asked for the number of internet-enabled devices in their homes. The results of this are shown in table 2. Number of internet-enabled devices zero 1-3 4-6 7-9 10+ Percentage of participants (n=72) 15% 47% 21% 4% 13% Table 2. Numbers of Internet-enabled devices in the homes of participants The majority of participants (47 percent) owned one to three internet-enabled devices in their homes, while only 15 percent did not own any internet-enabled devices. Those without internet-enabled devices were generally women (23 percent of the women surveyed did not have internet-enabled devices in the home compared to 4 percent of men). Participants' willingness to purchase second-hand electronics was established through survey questions which required them to identify the condition in which they last bought a given type of electronic device (Figure 3). Purchase of electronics second-hand with guarantees was highest for the items in which there are multiple outlets available to make that type of purchase, such as mobile phones and laptops. Overall, second-hand purchases (with or without a guarantee) accounted for 23 percent of mobile phone purchases, 22 percent of TV or games console purchases, 18 percent of laptop or computer or tablet purchases, 10 percent of internet-enabled security devices and 9 percent of thermostat purchases. The only category where no participants had bought second-hand devices was the smart assistant devices (such as Amazon Echo or Google Home). Ownership levels for the different electronic devices varied, with all participants owning a mobile phone and only a single participant not owning a laptop, computer or tablet. 17 percent of respondents indicated they had not purchased a TV or games console, with 27, 59 and 70 percent of participants respectively indicating they had not purchased electronic thermostats, smart assistance devices and internet-enabled security devices. It should be noted though that TV ownership in the UK shows only 3 percent of households in the UK do not have a TV Sta (2022) compared to the 17 percent of survey participants. However, Sta (2022) statistics do highlight that only 70 percent of UK households own a smart TV. It is possible that either the participants had lower TV ownership than in comparison to the UK, or, more likely, that the writing of the survey did not highlight that the TV did not need to be internet-enabled and this requirement was assumed by participants due to the prior question on internet-enabled devices. Consumer attitudes towards purchasing of second-hand robots Across all robot types presented to participants, 27 percent of participants indicated they would purchase a robot new, 27 percent would purchase one second-hand with a guarantee but only 10 percent would purchase one without a guarantee. The specific type of consumer robot presented to the participants affected the indicated purchase rate, with the highest interest seen for household chores robots at 64 percent for a new system, and the least interest was shown for a second-hand pet robot without a guarantee at 3 percent of participants indicating they would be willing to purchase such a system. In Figure 4 the purchase indication rates for each robot type and condition are presented. Figure 4. Purchase indication by participants for a given robot type, based on the purchase conditions: New, Second-hand with guarantee [2nd w guarantee], Second-hand without guarantee [2nd w/o guarantee] For each demographic and lifestyle factor surveyed, the percentage of participants in each group indicating they would purchase a robot for a given sale condition is shown in Figure 5a, and is detailed in the Supplementary Material along with numbers for participants responding they were 'unsure' if they would purchase a robot of a given category. Women were more likely than men to indicate a positive response towards purchasing a robot, regardless of the condition, and those without children were also more likely to purchase a robot in all condition types. Interest in the purchase of new systems new generally decreased with age, with a 52 percent interest for those aged 18-25 years, down to 11 percent for those aged 66+. Only those aged 36-45 did not follow this trend, with their interest levels being 4 percent lower than for 56-65 year olds. Individuals were also more likely to select that they would purchase a new or second-hand robotic system if they already owned a robot in the home (such as a vacuum cleaner or lawn mower) compared to those who did not. Figure 5a. Participants responding positively to purchase a robot in a given condition Figure 5b. Combined responses for participants responding positively and unsure to purchase a robot in a given condition Figure 5. Effect of demographics on participants responding if they would purchase a robot based on the purchase conditions: New, Second-hand with guarantee, Second-hand without guarantee. Notes: [a] Prior ownership includes vacuum or mower-type robotic systems for the home. [b] Numbers of internet-enabled devices includes connected devices already in the home] Generally, a +/- 3 percent difference was seen when comparing the attitudes of participants to new robots, to second-hand robotic systems with guarantees. Exceptions to this were for those aged 18-25 which had a drop of 11 percent between new and second-hand with guarantee, the age category 66+ years which saw an 8 percent increase, prior ownership of robots which had a 7 percent increase and those with 7 or more internet-enabled devices in the home which had a 6 percent increase respectively. Across all factors and purchase conditions investigated the most popular type of robot was a robot for household chores, generally followed by robots for security systems. Only in the second-hand with no guarantee condition were robotic security systems not the second-place preference option. Instead no trends were found shown in the responses across demographic factors for this condition, beyond the initial preference by participants for household chore robots. Combining the positive and neutral responses (Figure 5) resulted in response rates of over 40 percent for new robots in all demographic categories except those aged 56-65 and 66+. The same trend was seen for second-hand robots with guarantees but with the addition of those who have 1-3 internet-enabled devices in the home having a response rate of 39 percent. 3.3 Concern factors for consumers when purchasing second-hand robots In addition to providing purchase indicators, concern factors recorded on a Likert scale were provided by participants. The response options were very concerned, slightly concerned, neutral, slightly unconcerned and unconcerned. Table 3 summarises the percentage of respondents selecting Very or Slightly concerned for each factor. The breakdown of the remaining Likert scale responses is given in the Supplementary Material. Purchase New Secondhand with guarantee Secondhand no guarantee Cost of purchase 90% 88% 76% Cost to maintain 83% 89% 92% Physical safety for people in the home 56% 67% 69% Physical damage to the home 58% 64% 71% Data security for people in the home 85% 90% 86% Manufacturing environmental impact 72% 63% 65% Environmental impact of disposal 82% 78% 72% Table 3. Percentage of participants selecting 'Very concerned' or 'Slightly concerned' for each factor described, based on a robot's sale condition Those who own robots were generally less concerned by these factors than those who did not own robots. The greatest difference in responses of very or slightly concerned to new, second-hand with guarantee, and second-hand without guarantee was for the factors: • Physical safety for people in the home (19, 32 and 35 percent difference respectively) • Physical damage to the home (22, 17, 24 percent difference respectively) The only instances where those who own robots had a greater level of concern shown than those who did not own robots were; the cost to maintain second-hand robots without guarantees (10 percent higher), the security of personal data and the environmental manufacturing impact on new robotic systems (6 and 8 percent higher). Figure 6. Graph of the number of coding references for each theme identified during the qualitative analysis Of the 72 participants in the study, 19 provided responses in the free-text sections of the survey requesting additional comments relating to concerns which had not been highlighted in the Likert Scale responses. Qualitative analysis of those free-text responses from participants identified over 32 additional areas of concern (codes), which together formed 8 key themes: • Concerns about the appearance of a robot • Concerns around the purchasing source and guarantees for robots • Concerns about the cost of a robot across its life-cycle • Negative effects of robots on people during the robots use • Concerns relating to the technical capability of robots • Concerns around data security and privacy for robot owners • People not wanting, or not able, to operate robots • Impacts on society of greater robot use Figure 6 summarises the number of references given for each of the themes identified, split by the condition of purchase. The theme with the greatest level of responses was those not wanting or not able to operate robots with comments from participants including; "I can't imagine a circumstance where I would buy a new robot for my home." [Participant #22], and concern about the purchase being "complicated operation or setting up of a unit" [Participant #25]. For new robots, the second greatest number of responses were related to the technical capability of the systems purchased. Comments within this theme included: "Could it cope with an old cottage with uneven floors" [Participant #18] and "How well would it actually do the job I purchased it to do?" [Participant #38]. For second-hand robots, both those with a guarantee and those without, respondents' key concern centred on the purchasing source, continued support for the systems and guarantees relating to the second-hand product. Examples included; "problems with robot not identified or lied about" [Participant #59, comments for second-hand robots with no guarantee], "Spares and support. Mobiles are only supported for a few years. Given the likely cost of a robot I'd want 15 years or more, like cars." [Participant #9, with guarantee], and "Credibility and viability of the organisation supporting product/guarantee" [Participant #56, with guarantee]. A variety of factors relating to cost were highlighted in the responses across all three robot conditions (new, second-hand and second-hand with guarantees), with concerns about ongoing maintenance, energy consumption and resale costs highlighted. Participants' comments relating to the cost of systems included; "whether I would use it enough to justify buying it" [Participant #19, new robots], "I could be spending a lot of money and have no means of refund or exchange if the robot went wrong or ceased to work." [Participant #38, second-hand no guarantee robots], "device energy consumption" [Participant #2 - new] and "cost of software updates" [Participant #20 - both new and second-hand with guarantee robots]. 3.4 Comparison of purchasing indications for robots to other electronic products Participant responses regarding concern levels for environmental issues were compared to robot purchasing conditions (shown in Figure 7). Those who indicated concern for e-waste levels were less likely to respond positively about purchasing new and second-hand robot products than those who indicated they were neutral towards concerns about e-waste. In comparison, those who were concerned about plastic waste levels were slightly more likely to respond positively about purchasing new and second-hand robot products than those who indicated they were neutral towards concerns about plastic waste levels. It was not possible to compare these results to responses about concern levels for the Climate Crisis, Deforestation and Pollution as the numbers responding neutral or unconcerned to these topics accounted for only 4 percent of the respondents (or 3 individual participants). Future surveys would need to address the recruitment and self-selection process found in this research that resulted in the majority of participants having high concern levels for topics such as the climate crisis, and therefore making sample sizes for those responding neutral or unconcerned too small to review. Figure 7 also presents how the response by the participant on the last condition they purchased a mobile phone, laptop and games console affected their purchasing indications for robots in a given condition. Participants who had bought games consoles second-hand without a guarantee were more likely to positively respond to purchasing a robot in any condition type, while the opposite was true for those buying laptops new who had a higher robot purchase indication rate than for those who bought laptops refurbished with guarantees. Supporting data for all figures included in section 3 is included in the Supplementary Data. DISCUSSION 4.1 Consumer attitudes towards purchasing of second-hand robots Consumer attitudes towards the purchase of second-hand robots with a guarantee matched interest levels for new systems at 27 percent, while only 10 percent of participants indicated a purchase possibility when there was no guarantee offered. Equal favourability for new and second-hand with guarantee robots does suggest that there is an opportunity for the sale of systems in both conditions. To enable the sale of second-hand systems though, vendors require the return of old systems at the end of their life requiring the industry to consider this in their business models. Elzinga et al. (2020) presents research into three business models for the electronics industry consumer circular economy; take-back management, product Figure 7. Effect of environmental lifestyle factors on participants responding if they would purchase a robot based on the purchase conditions: New, Second-hand with guarantee, Second-hand without guarantee. lease and pay-per-use, with take-back management the most popular option with consumers. The take-back management business model requires businesses to re-obtain ownership of products through collection points, with consumers incentivised to take part through payment schemes or fees Elzinga et al. (2020). This suggests that use of a take-back management scheme within the robotics industry will result in additional sales for robot manufacturers through the reconditioning and resale of second-hand systems, enabling a more circular economy. The success of a take-back management scheme for the robotics industry would require easily accessible return points and well-communicated returns processes. These are particularly important to the electronics industry, where long-term storage of small household electronics regularly occurs and is a key barrier to electronics recovery Wilson et al. (2017). This process, also referred to as the hibernation phase of electronics ownership Murakami et al. (2010), is a well-documented consumer behaviour in developed countries. 70 percent of consumer e-waste was placed in hibernation for three to five years by consumers in the US Babu et al. (2007), while consumers in the UK stored mobile phones for an average of 3 years Wilson et al. (2017), and in a study of residents in a city in Finland - 70 percent of participants retained their mobile phone for a period of time after they had no primary use for it in a study Yl¨a-Mella et al. (2015). When considering the different types of robots presented to participants, interest was highest for robots which were able to complete household chores, followed by robots performing security roles. Overall interest levels amongst participants for new and second-hand with guarantee robots for chores were 62 percent and 58 percent respectively, while security robots had interest levels of 37 and 32 percent respectively. These results are comparable with the Nitto et al. (2017) study found that 50 percent of consumers in the US were interested in robots that would help with either household chores or security. This study did find though that levels of interest varied by country of residents, with 34 percent of residents of Germany and only 19 percent of residents from Japan indicating they would be interested in purchasing a robot within the next 5 years. Robots used as pets were the least popular across all sale condition types, with a second-hand pet robot with no guarantee interesting only 3 percent of respondents. This was followed by robots for entertainment which had similar rates for new and second-hand with guarantee systems as pet robots, but a slightly higher interest in second-hand systems at 7 percent. Again, the Nitto et al. (2017) study noted that "when it comes to leisure, time spent with friends or caring for pets and children, American consumers are noticeably less interested in being involved with robots", confirming the findings of this study. Only robots used for entertainment, or for health & fitness, saw the rate of interest increase for a secondhand robot with a guarantee over a new robot, with the remaining categories seeing a decrease between the buying conditions. It is possible that due to the second-hand market for non-robotic technologies for both entertainment and fitness already being well established, there is a greater openness amongst participants to see second-hand robots in these sectors. Demographics which most influenced a survey participants' interest in purchasing new robots were age and prior ownership of robotic systems such as vacuums or mowers. 60 percent of those aged 18-25, and 37 percent of those previously owning a robotic system would purchase a robot new. Only those aged 66 and over were more likely to show an interest in purchasing a second-hand system (with or without a guarantee) rather than a new system. These results differed from those in the P´erez-Belis et al. (2017) study on consumer attitudes in Spain towards second-hand EEE purchases for the home which found that older consumers and women were more likely to repair small household EEE, while men and those from medium-income families were more likely to purchase second-hand small household EEE. To better understand participants' responses to second-hand robots, this survey also reviewed their prior purchasing habits of internet-enabled devices such as mobile phones, games consoles, personal assistant speakers and internet-enabled security doorbells or CCTV. For participants in this survey, 22 percent had owned a second-hand mobile phone and, of those who owned laptops, games consoles, and internet-enabled security systems, the percentage who had those products second-hand was 18, 22 and 10 percent respectively. These results are consistent with data and research into the second-hand market for mobile phones. In 2016 second-hand smartphones accounted for 7 percent of the global sales market and was expected to see increases in sales 4 to 5 times quicker than new phones Del (2016). In addition, it has been well-documented that mobile phones are passed on to friends and family members outside of global sales, adding to the total number of second-hand devices in use. Wieser and Tr¨oger (2016) summarises that 13 to 28 percent of all smartphones are passed onto family members or charity at their replacement point. In comparison, overall purchase rates of other second-hand small household electronics has been shown to be much lower. Overall the P´erez-Belis et al. (2017) study found only 0.75 percent of participants had bought second-hand small electronics for the home at any time where the electronics in the study included items such as vacuums, blenders, toasters and kettles. Therefore, recognising that consumer attitudes in this study showed equal interest in purchasing new robotic systems to second-hand robots with guarantees, it may be possible for the robotics industry to aim for the higher levels of second-hand device sales seen for internet-enabled devices such as mobile phones and game consoles than for the lower rates seen for standard electrical household goods. The results of this survey were not able to show a direct relation between participants who had previously bought other second-hand electronics, and those more willing to buy second-hand robots. This may be due to ownership levels of robots for the home being so low still that the responses of participants were not influenced by their attitudes towards other types of second-hand electronics. 4.2 Concern factors for consumers when purchasing second-hand robots In the Yl¨a-Mella et al. (2015) study of consumer perceptions towards second-hand mobile phones, concerns raised by participants as reasons for not purchasing second-hand mobile phones included reliability of the product (47 percent of respondents), short life-cycle (32 percent), availability of existing budget models (32 percent) and lack of warranty (16 percent). In comparison, this study yielded much higher concern rates, with concerns rates as high as 90 percent relating to the cost of purchasing new robots. Of the factors assessed by the Likert Scale responses in the survey, the lowest concern rate was still for robots carrying out physical damage to the home but this was still at 56 percent for new systems. Comparing ownership rates for consumer electronics within this survey only 14 percent of respondents owned a robot for the home, while 100 percent of respondents had a mobile phone. This disparity in ownership, and the relative newness of the robotics market will likely have affected concern levels for the robotic systems compared to other electronic devices. It should be noted though that even amongst robot owners in this survey, concern levels remained high, with their concern rate being 6, 8 and 10 percent higher than for those without prior robot ownership for factors of security of personal data (new), the environmental impact of manufacture (new) and cost of maintenance (2nd-hand without guarantee), highlighting the current realities of robot ownership. Participants of the survey indicated the same levels of concern toward cost for both new and second-hand systems. This suggests that consumers may need to recognise significant cost advantages in secondhand robots in order to purchase them over new. This is reflected in the findings of Hazen et al. (2017) which highlights the need for push factors to influence the uptake by consumers in purchasing of other manufactured technology goods. In addition to push factors affecting consumer choice in the purchase condition of electronics, pull factors such as the individual perception of environmental concerns can influence the uptake of second-hand goods Hazen et al. (2017). In this survey, 97 percent of the respondents agreed there was a climate emergency. This resulted in a too-small sample size for those that do not believe there is a climate emergency in order to compare the two demographics. However, across the survey participants, levels of concern decreased when participants considered the environmental impact involved in the manufacture of new to second-hand and second-hand without guarantee condition robots. Where consumers show a greater understanding of climate change and its impact, consumers are more likely to partake in a pro-climate consumer society Łukasz Kurowski et al. (2022). The results of this survey suggest highlighting the reduction in manufacturing impact through the reuse of robotic systems should result in positive consumer behaviour towards these second-hand products. It should be noted though that in comparison, concerns about the environmental impact of disposal increased across the purchase conditions. The researchers were not able to conclude from the data why this environmental concern had an opposite result to the manufacturing concern factor. Further investigation would be needed to understand this result. Further to the responses from the Likert Scale concern factors, the qualitative results raised additional concerns by participants around the performance of new and second-hand systems, maintainability of secondhand systems, and methods of insurance and liability of systems without a guarantee. The topics raised as additional areas of concern in the qualitative analysis revealed that some topics were universal to robots in the home, regardless of the purchasing condition. This included the performance of the robots, and more fundamentally, trust in any robotic system coming into the home. Much as Miglani and Hensman (2016) highlights the ethical and technical considerations for software used in robots in the home, participants of this survey raised concerns regarding data security. The number of such concerns increased for second-hand systems and included issues around software obsolescence, the introduction of viruses and access to prior owners' data. The theme of obsolescence was also raised in relation to the physical system, and the effect that buying a second-hand system might have on access to upgrades and the associated costs of upkeep. These concerns were in line with findings from research into other types of second-hand electronics. The P´erez-Belis et al. (2017) study found reasons for not purchasing second-hand electronics included cleanliness and hygiene concerns, minimal cost savings for second-hand systems over new, lack of knowledge of where to purchase second-hand devices, lack of repair guarantees and perceptions of low durability for second-hand systems. With many of these themes also appearing in this research, findings from other consumer electronic studies can be used to influence the robotics industry too. While this study has placed emphasis on the role of the robotics industry in creating and maintaining a circular economy for products, inaction by consumers must still be considered. P´erez-Belis et al. (2017) notes that while product ecodesign is central to a circular economy and requires manufacturers to design more 'durable, easier to repair, reuse or recycle products', attitudes of consumers towards electronics at the end of their primary life must also be tackled. General consumers will either dispose of the e-waste in the bin instead of through dedicated WEEE recycling bins or schemes due to the relatively small size of many EEE products, or they will store the e-waste at homeP´erez-Belis et al. (2017). This tendency for incorrect management of electronic waste at the end of its life was highlighted in the introduction to this paper (Section 1. The qualitative analysis highlighted a number of participants who registered no interest in purchasing any robotic system for the home, regardless of buying condition. It is inefficient to work towards creating solutions for those unlikely to ever purchase a robot, let alone a second-hand robot. Fiorini et al. (2022) describes how participant curiosity in the technology supports greater uptake of participants; addressing this in future studies, for this topic may provide a greater and more instructive yield in results. 4.3 Evaluation of the surveying process Reviewing the outcome of the participant demographics it was noted that participants with children in the home, those aged 66 and over, and those who do not believe there is a climate emergency were underrepresented in this survey in comparison to the UK population. This is a reflection that this survey partially relied on convenience sampling methods Bryman (2016) - where participants were those most available to the research team, in this case, through the publication of the request for participants via social media channels. This will likely have resulted in a more homogeneous demographic than a quota sampling system that would have produced Bryman (2016). Some effort was taken to widen the scope of participants through the use of the online Surveycircle platform, however, this platform is generally used by students and researchers, again resulting in some homogeneous traits and self-selection for participation based on interest in the topic. Results presented in Section 3 cannot, therefore, be generalised to be representative of the UK population, but do form indicating factors which was the requirement of the research process. There were higher response levels for comments for the new robot purchase condition than second-hand conditions, despite concern levels being higher in the Likert scales for second-hand. This suggests that either the Likert Scales better-encapsulated concern factors for participants or participants spent time considering responses for this category and may not have wanted to repeat themselves for the other conditions. Only in one example did the respondent choose to copy and paste their response from one robot condition to another. The survey design for the free-text component section of the data collection could have therefore been improved by providing participants with the option to indicate the responses additionally applied to other conditions of purchase. Additionally, with limited numbers of free-text responses, motivations for purchasing habits were not explored in great detail in this paper. Future surveys could utilise follow-up interviews or additional questioning to improve on the use of free-text comments in this survey design. While Likert Scales were used in the survey due to their ease of use and expected increased response rates, issues with using these types of scales include response acquiescence and social desirability. Acquiescence results in participants selecting results which they think are the correct answer, while social desirability causes users to select responses that make them look better for greater social acceptance Jupp (2006). Social desirability bias is often higher in surveys with an interviewer present which this research limited by utilising an online platform for data collection, however, topics around sustainability and the environment have known ethical and moral sensitivities Roxas and Lindsay (2012) which will likely have influenced participants responses. Lastly, due to the small number of participants who submitted survey data outside of the UK, only responses from those in the UK were carried forward. Bernotat and Eyssel (2018) highlighted the effect of different cultures (Japanese and German) on the perceptions of robots in the home and attitudes while Nitto et al. (2017) studies demonstrated the differences in projected purchasing habits for consumers in the US, Japan and Germany. Future studies should therefore compare attitudes to second-hand robots for participants outside of the UK. CONCLUSION Taking survey data from 72 UK participants, around a quarter indicated a positive interest in purchasing a new robot for the home. When presented with second-hand robots with guarantees, this figure did not change. However, when the option of second-hand robots without a guarantee was introduced this was reduced to 10 percent. This highlights the need for recognised certification methods or manufacturer warranties in order for robots to be successfully sold in the second-hand market. Young people aged 18-25 indicated a significantly higher interest in robotic systems than any other age group. Whether this is a factor of the participant's age at the time of the survey or the generation of which they are part is unclear from this single snapshot survey. However, paired with higher interest levels in the robots presented to those who have prior experience in ownership of robotic systems, this suggests an affinity for, and greater experience with, smarter devices in the home will form key drivers for individuals being willing to purchase a consumer robot. Current trends comparing second-hand purchases of household electronics to second-hand purchases of mobile phones suggest that the second-hand robotics market will be more similar to that of internet-enabled devices such as smartphones, than for household electronic devices - even where the robot will be used as a device for the home. By studying the growth of the second-hand mobile phone market and the challenges it has faced, the consumer robotics industry will be able to preempt the requirements that will likely be faced in trying to support the circular economy and tackle those at a time when it is cheaper in the technology maturity process to do so. However, it should be noted that the experience of prior ownership of other types of second-hand electronic devices (such as phones, laptops and game consoles) did not increase the participant's level of interest in second-hand robots. It is therefore possible that robotic technology for the home is too new or unknown for individuals to be able to make comparable decisions between purchasing the experience for other second-hand electronics to the projected experience of owning a robot. Instead, participants felt the initial purchase cost of any system was the greatest concern for new and second-hand robots, while the cost to maintain was the greatest concern for second-hand robots without guarantees. Additionally, attitudes to second-hand robots generally highlighted concerns for maintainability, verification and certification, technology obsolescence and liability in the event of damage to a person or home. In order to make the second-hand robot market attractive for consumers, these issues would need to be addressed and resolved, alongside a system to provide guarantees for second-hand systems. Even with these concerns though, individuals still indicated they were willing to purchase second-hand systems. Overall this survey is encouraging for the wider implementation of the circular economy and demonstrates that there is a market for second-hand robotic systems for the home. To enable this, processes must be in place to retain consumer robots at the end of their primary use, in order to bring them into the second-hand market. As demonstrated with other electronic devices, this may require incentivisation (such as buy-back schemes), and accessible methods for consumers to return used robotic systems. Manufacturers, retailers and public systems not supporting this process will likely result in the discarding of old robots as e-waste, adding considerable levels of waste already produced annually.},
  archive      = {J_FROBT},
  author       = {McGloin, Helen and Studley, Matthew and Mawle, Richard and Winfield, Alan Frank Thomas},
  doi          = {10.3389/frobt.2025.1694690},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1694690},
  shortjournal = {Front. Robot. AI},
  title        = {Correction: Understanding consumer attitudes towards second-hand robots for the home},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial: Advancements in vibration control for space manipulators: Actuators, algorithms, and material innovations. <em>FROBT</em>, <em>12</em>, 1681168. (<a href='https://doi.org/10.3389/frobt.2025.1681168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Editorial on Research Topic Space manipulators play a pivotal role in modern space missions, enabling satellite servicing, debris removal, and planetary exploration. However, their lightweight, long-reach designs and dynamic operational environments introduce significant vibration challenges that can compromise mission success. Addressing these challenges requires a multidisciplinary approach, integrating advancements in actuators, control algorithms, and material science. While conventional actuators (Liu et al., 2021; Tayebi et al., 2022; Mishra et al., 2018) and attitude control strategies (Ti et al., 2019; Tayebi et al., 2025; Xie et al., 2025) remain prevalent in spacecraft design, emerging solutions leveraging soft materials and AI-driven control architectures represent a rapidly evolving frontier in vibration mitigation research. This Research Topic provides the most recent developments in vibration mitigation techniques for space manipulators, including four important studies as result of the call. These studies cover several aspects of vibration through flexible dynamics estimation via quasi-static approximation, AI-enhanced guidance and control systems, bio-inspired soft actuation, and hybrid soft-rigid grasping architectures. Accurate measurement of elastic coordinates via sensors has historically posed significant challenges in controlling flexible space manipulators. Patel and Damaren addressed this by developing a model-based estimation framework, eliminating the need for direct vibration sensing. They proposed a quasi-static estimator that approximates elastic coordinates with joint torque data, enabling precise end-effector trajectory tracking. Their simulations on single and two-link manipulators demonstrate robust performance, even with large payloads and model uncertainties. Conventional spacecraft Guidance, Navigation, and Control (GNC) systems, designed for ground-commanded operations with limited autonomy, face significant challenges in adapting to the dynamic demands of on-orbit servicing missions. Addressing this gap, Hao et al. proposed an AI-enhanced visual GNC system as an intermediate solution between conventional architectures and future fully autonomous systems. Their approach combines a deep learning-based algorithm that estimates target pose from 2D images without requiring prior knowledge of the target's dynamics, and a learning-based motion planner that generates manipulator trajectories while minimizing spacecraft attitude disturbances. The visual GNC system is exemplified through simulation of a conceptual mission, involving a micro-satellite tasked with on-orbit manipulation of a non-cooperative CubeSat. Figure 1 illustrates the design of the intelligent orbital service spacecraft and delineates the conceptual mission framework for capturing and servicing a non-cooperative target. Their work demonstrates the potential of AI to enhance autonomy in space robotics, particularly for non-cooperative target capture. FIGURE 1 Left: The design of the intelligent orbital service spacecraft. Right: The conceptual mission framework (Hao et al.). Actuators play an important role in controlling of space manipulators. Ashby et al. introduced bio-inspired soft actuators using dielectric elastomer transducers (DETs) as lightweight artificial muscles for space applications. Inspired by the starfish podia as shown in Figure 2, their inflatable DET-based actuator combines deployable structures with proprioceptive capabilities, enabling compact stowage during launch and adaptive operation in zero-gravity environments. The study highlights the advantages of soft robotics in space, where mass and volume constraints are critical. These actuators show particular promise for applications requiring adaptable, mass-efficient systems in unstructured orbital environment. FIGURE 2 Left: The basic structure of a typical starfish's podium (tube foot). Right: Close-up of a starfish on the sea floor, with podia in active motion (Ashby et al.). Dontu et al. discussed the development of a hybrid soft gripper for delicate object manipulation, validated through real-world robotics competitions. Their vacuum-actuated design integrates soft fingers with rigid components, and task-specific modules, balancing compliance and precision. By refining the gripper through successive iterations, the work demonstrates the importance of adaptable, hybrid designs for handling diverse objects in unstructured environments. These studies collectively illustrated three essential developments in space manipulator design through model-based estimation overcoming sensing limitations in orbit, AI-driven autonomy enabling real-time adaptation, and material and actuation Innovations. Looking ahead, the field must address key challenges in technology readiness levels and orbital validation, particularly for soft robotic components in space environments.},
  archive      = {J_FROBT},
  author       = {Tayebi, Javad and Chen, Ti and Wu, Xiaofeng and Mishra, Anand Kumar},
  doi          = {10.3389/frobt.2025.1681168},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1681168},
  shortjournal = {Front. Robot. AI},
  title        = {Editorial: Advancements in vibration control for space manipulators: Actuators, algorithms, and material innovations},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial: Computer vision mechanisms for resource-constrained robotics applications. <em>FROBT</em>, <em>12</em>, 1680098. (<a href='https://doi.org/10.3389/frobt.2025.1680098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {KEY CONTRIBUTIONS IN THE SPECIAL ISSUE In the evolving landscape of robotics, the challenge of deploying intelligent systems in real-world environments under limited computational and energy budgets has become increasingly important. The special issue of Frontiers in Robotics and AI—"Computer Vision Mechanisms for Resource-Constrained Robotics Applications", gathers a series of innovative contributions that directly address this challenge. The collection not only reflects the state of the art in efficient robotic perception, but also channels bio-inspired models, minimal computing principles, and hardware-constrained innovation to define a compelling path forward. The premise of the issue is grounded in a vital observation: although computer vision has reached extraordinary levels of accuracy and generalization through deep learning and high-performance sensors, these solutions are often tethered to expensive and power-hungry hardware and computationally complex algorithms. Autonomous robots intended for field deployment—whether aerial drones, underwater vehicles, or mobile ground units—frequently face bandwidth, energy, and compute bottlenecks. Thus, vision mechanisms designed with constraint-awareness are not merely desirable, but necessary. One prominent contribution to the issue is Singh et al.'s framework on minimal perception, which seeks to enable robotic autonomy by reducing perceptual complexity to its essential components Singh et al. (2024). Drawing inspiration from ecological psychology and AI, the authors design systems where perception is not treated as a comprehensive reconstruction of the world, but rather as a dynamic filter tuned to task relevance. The approach is validated through simulation and robot experiments, demonstrating that sparse and compressed sensory signals can support goal-directed behaviors such as obstacle avoidance and navigation with minimal computational overhead. Another standout is Van Opstal et al.'s biomimetic robotic eye, which introduces a six-degree-of-freedom platform capable of replicating human-like saccadic movements. The design merges active vision control Rui Pimentel de Figueiredo et al. with neurophysiological plausibility, offering both an experimental platform for neuroscientific hypotheses and a roadmap for embodied perception in robotics Van Opstal et al. (2024). The project brings to life ideas proposed in earlier models of oculomotor control, notably by Robinson and Sparks, but with real-time hardware precision and active visual fixation. Kalou et al. contribute with their work on head-centric representation of 3D shapes, reformulating shape perception as a process guided by an embodied fixation system Kalou et al. (2022). Their compact encoding scheme capitalizes on spatial priors gained from head-centered visual exploration, echoing Gibson's ecological theory of vision Gibson (1979) and early active vision paradigms. On the sensor innovation front, Ribeiro-Gomes et al. advocate for event-based cameras as foundational tools in embedded robotics. Their integration of asynchronous feature tracking into a visual-inertial odometry (VIO) framework demonstrates improved latency and energy efficiency compared to conventional methods Ribeiro-Gomes et al. (2023). These contributions collectively reflect a growing interest in both algorithmic minimalism and bio-inspired adaptation. FUTURE DIRECTIONS AND EDITORIAL PERSPECTIVE The research compiled in this issue reflects a broader shift in the field: away from monolithic perception pipelines and toward adaptive, embodied, and task-sensitive vision systems. A key theme is the resurgence of the perception-action loop, long emphasized by ecological psychology and active vision pioneers such as Aloimonos et al. Aloimonos et al. (1988) and Gibson Gibson (1979), but now reinvigorated through hardware-aware and task-constrained implementations. First, there is a growing recognition of the need for perception systems that dynamically adapt their computational load. Runtime-adaptive pipelines—capable of modulating neural network size, activation depth, or sensor resolution in response to context—could become essential for sustained autonomy in resource-constrained environments. Second, spiking neural models and neuromorphic circuits represent a biologically plausible and energy-efficient alternative to dense convolutional networks. This trend aligns with broader developments in transprecision computing and event-driven processing Delbruck and Lichtsteiner (2007); Sze et al. (2017), offering pathways for ultra-low-power robotics. A third frontier lies in the development of learning-based attention strategies. While classical attention mechanisms in vision prioritize computational saliency, emerging work explores end-to-end learning of attention policies that guide both perception and action. Reinforcement learning, meta-learning, and unsupervised learning strategies may enable robots to allocate their limited perceptual resources more effectively—particularly when paired with real-time hardware feedback on power and latency. Finally, the field would benefit greatly from standardized benchmarks designed specifically for resource-constrained robotics. While robust datasets exist for general tasks like SLAM or autonomous driving, few evaluate performance under explicit constraints such as degraded sensing, memory limits, or energy caps. Establishing such benchmarks could spur more targeted innovation and help consolidate this rapidly growing subfield. In terms of real-world impact, the relevance of resource-constrained perception extends well beyond research labs. In industrial robotics, especially in domains such as manufacturing, logistics, and infrastructure inspection, there is a growing demand for agile and power-efficient robots that can operate reliably in bandwidth-limited and dynamic environments. Many of these applications involve mobile Rui Pimentel de Figueiredo et al. platforms or wearable systems where energy and compute budgets are tightly constrained. Vision systems that adaptively throttle processing, or that rely on asynchronous and event-based sensing, are particularly well-suited for tasks such as real-time anomaly detection, autonomous warehouse navigation, and human-robot collaboration on production lines. Moreover, the advances discussed in this issue intersect meaningfully with the development of bio-inspired and humanoid robots. Platforms modeled after human morphology benefit immensely from perception systems that mimic the selective, task-dependent nature of biological vision. For example, the implementation of saccadic control, head-centric representation, and embodied attention strategies aligns naturally with the perceptual architectures found in humanoid robots. Such robots—whether designed for eldercare, physical rehabilitation, or disaster response—require lightweight and efficient perception pipelines that can operate within strict power, latency, and mobility constraints dictated by their human-centric form and mission requirements. CONCLUSION From biologically inspired eye movements to minimal perception frameworks and event-based feature tracking, the contributions point toward a future where efficiency and embodiment are not peripheral concerns, but rather central pillars of robotic intelligence, particularly as robotics moves further into industrial, assistive, and human-interactive domains. This special issue offers a compelling survey of the innovations shaping vision for resource-constrained robotics. From biologically inspired eye movements to minimal perception frameworks and event-based feature tracking, the contributions point toward a future where efficiency and embodiment are not peripheral concerns, but rather central pillars of robotic intelligence.},
  archive      = {J_FROBT},
  author       = {Pimentel de Figueiredo, Rui and Limberg, Christian and Jamone, Lorenzo and Bernardino, Alexandre},
  doi          = {10.3389/frobt.2025.1680098},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1680098},
  shortjournal = {Front. Robot. AI},
  title        = {Editorial: Computer vision mechanisms for resource-constrained robotics applications},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of hotel robots’ service quality on continuance intention: The moderating effect of personal innovation. <em>FROBT</em>, <em>12</em>, 1667123. (<a href='https://doi.org/10.3389/frobt.2025.1667123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As service robots are increasingly integrated into the hotel industry to enhance operational efficiency and customer experience, understanding consumers’ responses to robotic services has become a critical research agenda. However, empirical evidence on how customers evaluate the service quality of hotel robots and how these evaluations influence their continuance intention remains limited. Drawing on the SERVQUAL framework, this study redefines service quality in the context of AI-powered hotel robots through five dimensions: reliability, assurance, entertainment, anthropomorphism, and tangibles. Furthermore, the study explores the moderating role of personal innovativeness in the relationship between perceived service quality and continuance intention. Data were collected via an online survey from 400 Generation Z consumers in China who had prior experience with item-delivery robots in hotel settings. The results indicate that assurance, entertainment, anthropomorphism, and tangibles have significant positive effects on continuance intention, while reliability does not show a statistically significant impact. Moreover, personal innovativeness significantly moderates the effects of certain service quality dimensions, suggesting that individual differences in technology readiness shape consumer reactions to robotic services. This study contributes to the literature by extending traditional service quality theory into the domain of human–robot interaction and by highlighting the nuanced mechanisms through which robot-specific service attributes influence user loyalty. Practical implications are offered for hotel managers seeking to optimize robot deployment strategies and improve guest engagement in technology-enhanced service environments.},
  archive      = {J_FROBT},
  author       = {Shi, Yaru and Zhan, Weifang and Lin, Chengpeng},
  doi          = {10.3389/frobt.2025.1667123},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1667123},
  shortjournal = {Front. Robot. AI},
  title        = {The impact of hotel robots’ service quality on continuance intention: The moderating effect of personal innovation},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Avoidance behaviours of farmed atlantic salmon (Salmo salar l.) to artificial sound and light: A case study of net-pen mariculture in norway. <em>FROBT</em>, <em>12</em>, 1657567. (<a href='https://doi.org/10.3389/frobt.2025.1657567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensive finfish aquaculture is increasingly relying on enabling technologies and solutions such as sensor systems, robotics, and other machinery. Together with conventional farming equipment, these systems may emanate acoustic noise and artificial light, impacting the pen environment. Farmed fish have been observed to respond behaviourally and/or physiologically to anthropogenic sounds and lights, indicating a stress reaction that could impair welfare and health. This study aimed to investigate how farmed Atlantic salmon respond to such stimuli, with direct implications for the design and operation of robotic and mechanised systems in sea pens. We conducted experiments where we systematically exposed adult farmed Atlantic salmon in commercial net pens to sounds of frequencies within the range common to farm equipment (100–1,000 Hz), and submerged lights at 8 and 12 m with four different intensities (600 lx–14,500 lx). Data was analysed using sonar data and a deep learning (DL) based method for processing that automatically identified fish distribution patterns and estimated the average avoidance distance to the sound/light source. The fish fled from the sound source while playing sounds of 400 Hz, while sounds at other frequencies did not elicit a response. The response to light intensity depended on deployment depth, with the fish moving closer to the source when intensity was increased at 8 m depth, but conversely moving further away with increasing density when it was placed at 12 m. These outcomes are important inputs for the design of equipment, autonomous vehicles, robotic interventions and operations at commercial farms to ensure that their sound and light emissions have minimal impact on the fish, thereby reducing the potential of induced stress.},
  archive      = {J_FROBT},
  author       = {Zhang, Qin and Bloecher, Nina and Evjemo, Linn Danielsen and Føre, Martin and Kelasidi, Eleni},
  doi          = {10.3389/frobt.2025.1657567},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1657567},
  shortjournal = {Front. Robot. AI},
  title        = {Avoidance behaviours of farmed atlantic salmon (Salmo salar l.) to artificial sound and light: A case study of net-pen mariculture in norway},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial: Towards HRI of everyday life. <em>FROBT</em>, <em>12</em>, 1657518. (<a href='https://doi.org/10.3389/frobt.2025.1657518'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, it is often assumed that, at least in principle, robots in general, and social robots in particular, have the potential to increase human well-being across a wide range of sectors and application areas. At the same time, efforts to introduce such systems into our everyday environments have had only limited success and it is yet to be seen whether the impact of social robots on our daily lives, professional and private, will actually be beneficial. Therefore, there is a need to systematically address and study the long-term use and presence of social robots in everyday environments, where humans not only interact with robots, but also have them integrated into the totality of their lived experiences in everyday life.One of the main ways to investigate human engagement with social robots in everyday environments is through HRI studies conducted “in-the-wild”. While such an approach has certainly been instrumental and increasingly used in HRI research, in this Research Topic we have proposed to further expand the underlying theoretical and methodological frameworks to address the notion of 'everyday life'. Such a notion is viewed here as an important sociological concept and an umbrella term that allows us to address, not only specific application domains for robots, but also the continuity and totality of human lived experiences that emerge through long-term engagement with social robots in everyday life contexts. This is in a situation where placing social robots in everyday settings has often been an explicit goal for robot development but the very notion of 'everyday life' has not been systematically analysed. Therefore, this Research Topic has brought together a wide range of disciplinary and interdisciplinary contributions to further develop the HRI domain. While the main audience of this Research Topic is the HRI community, we have welcomed contributions from any disciplinary, interdisciplinary, theoretical, or methodological perspective that would address the subject of 'everyday life' in HRI research. As a result, we have published eight articles centered on the themes of integration, acceptance and people’s attitudes towards robots in everyday life.\href{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2023.1241519/full}{Kühne et al.} investigates the use of dialects in social robots and its impact on the perceived trustworthiness and competence of robots. This is part of a broader inquiry into design and contextual factors that affect people’s acceptance of robots as social interaction partners. In particular, the study involved playing a short online video featuring NAO the robot speaking either in the Berlin dialect or standard German and asking one hundred twenty native German speakers to evaluate robots through a survey. While the study resulted in observing a slight trend of higher trust and competence evaluation for the standard German-speaking robot compared to the robot that spoke the Berlin dialect, the difference was not statistically significant and the two robots received largely comparable ratings in terms of both competence and trustworthiness. Also, on the one hand, the study found a positive correlation between certain variables, e.g. between participants’ self-reported Berlin dialect proficiency and trustworthiness, and on the other hand, it also depicted a complex picture of people’s perceptions of robots being affected by a number of factors that go far beyond robot features such as demographic characteristics, or a type of device used to watch the video. \href{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1438912/full}{Zafrani et al.} focuses on the acceptance and assimilation of socially assistive robots (SARs) in everyday life of the elderly persons. It presents the study that examined the uses, constraints and outcomes of engaging with ‘Gymmy’ the robot in real-life conditions and over a long period of time, and their effect on the older adults Quality Evaluations of SARs. The study involved nineteen community-dwelling adults aged 75-97 who interacted with a personal training robot installed at their homes for a period of 6 weeks. The main findings of the study involved identifying two assimilation patterns among the participants who could be categorized either as ‘Fans’ or ‘Skeptics’ based on the type of experience they reported regarding the robot. These two groups differed in terms of the positive vs. negative experience as well as participants’ personal background, attitudes towards robots before and after using the robot in question, and actual user experience. Thus, the study has shown that assimilation and acceptance of SARs is far from being a homogenous process and requires and requires careful consideration of the older adults’ needs and concerns.\href{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2023.1212034/full}{ Zawieska et al.} poses the basis for this Research Topic as it discusses the existing and possible conceptual frameworks for the study of ‘everyday life’ in the HRI research. The paper first provides an overview of the ways everyday life is typically addressed in the HRI studies, namely in terms of settings, activity, population and/or methodology. Subsequently, the paper proposes further conceptual developments towards a systematic study of everyday life in HRI as a topic in its own right. In the process, it follows Social Sciences and Humanities (SSH) and sociological perspectives that have a long tradition of studying the everyday. In search of possible synergies between the HRI and SSH approaches, the paper builds upon the notion of ‘lived experiences’, and depicts the everyday as an open-ended process. It also engages with a critique of the contemporary everyday life as it calls for challenging the underlying understanding of the everyday and the real-world as ‘natural’ and by doing do, widening the scope of HRI research to include ethico-political stances oriented towards the pursuit of a ‘good life’.\href{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1363243/full}{Komatsu et al.} conducted participatory design workshops with middle‑to‑older adults in Japan—once during the COVID‑19 pandemic and again post‑pandemic—to explore how their needs and attitudes toward robot technologies evolved due to changing social circumstances. Drawing on Nowland et al.’s ``stimulation vs. disengagement'' framework, the study found a marked shift over time: during the pandemic, participants prioritized robot tools that enabled distanced communication and social sharing with family, aligning with the stimulation hypothesis. After pandemic restrictions lifted, their focus shifted inward—toward personal well-being, ease of use, and technologies that alleviate digital exclusion, reflecting the disengagement hypothesis. Throughout, the participants consistently emphasized the importance of user-friendly, multifunctional robotic solutions—citing familiarity, simplicity, and seamless integration into their daily routines (e.g., combining guidance with communication tools). The authors highlight that Japan’s super-aged context and rapid digitization during COVID‑19 exacerbated older adults’ digital exclusion, particularly disadvantaging those less tech‑savvy. They conclude with a strong call for inclusive co‑design approaches that address evolving social needs, from enabling connection during isolation to supporting personal well‑being in the post-pandemic era—ensuring robot technologies remain relevant and accessible across life stages and contexts.\href{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1332110/full}{Hägglund et al.} explore young adults’ trust considerations when interacting with a socially assistive robot in a high-stakes pharmacy scenario—specifically, medication counseling on emergency contraceptive pills. Utilizing a co-creation methodology, they worked with participants to design a prototype application using the Furhat social robot platform. Through in-lab testing and subsequent interviews (six participants), they conducted an inductive, reflexive thematic analysis. The study produced five distinct ``tales of trust'', each represented as a persona embodying varying expectations and willingness to trust a robot in this sensitive domain. The research identifies six key factors that drive initial trust formation: physical position (spatial relation to the robot), perceived autonomy, interaction boundaries, feelings of shame, eye gaze, and conversational alignment. These factors shaped whether participants were open to consulting a robot about intimate health matters. By mapping a continuum from low to high trust expectations, the authors illuminate the nuanced interplay of affective, contextual, and design elements influencing user trust in socially assistive robots. The insights significantly inform the understanding of trust formation in HRI, especially in sensitive healthcare contexts, offering important design considerations for socially assistive robotics in the healthcare sector.\href{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1289414/full}{Ostrowski et al.} present a year-long co‑design study involving 28 older adults, aimed at developing HRI guidelines that reflect seniors’ real-world needs. Participants engaged in a series of interviews, design sessions, and reflections to articulate the types of interactions they would want in daily life. These older co-designers expressed growing confidence over time, increasingly favoring transactional robot capabilities—like reminders and scheduling—while maintaining reservations about surveillance-related functions such as personal data tracking and monitoring. Beyond practical assistance, the study found measured enthusiasm for robots facilitating social connection, monitoring bodily signals, and supporting emotional well-being—all tempered by concerns over autonomy, privacy, and the ``naturalness'' of robotic interactions. Employing ethnographic decision-tree modeling, the authors demonstrated that simpler, non-intrusive features gained acceptance, whereas more invasive ones triggered sustained skepticism. They conclude by offering clear, user-driven guidelines for HRI design with older adults, and highlight several areas—such as ethical data practices, enhancing agency, and improving emotional realism—that warrant deeper investigation before broader deployment.\href{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1347367/full}{Vetter et al.} conducted a study that explores integrating robotic technologies in the care sector, where such innovations are seen as solutions to labor shortages and aging populations. Their research applied an integrated approach combining value sensitive design and speculative design to explore the complexities of care environments, focusing on the diverse needs, goals, and socio-material arrangements that shape this space. Drawing on six interviews and six card workshops with Austrian care workers and residents, five key themes emerged: trust-building routines, stakeholder negotiations, reciprocal and affective care, caregiver self-care, and material mediation. To provoke reflection and discussion, six speculative vignettes were created, highlighting tensions that arise when technologies disrupt established care practices. The study offers valuable insights for robotic designers to understand care values and dynamics prior to developing interventions.\href{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1363713/full}{Irfan et al.} conducted a participatory design study with 28 older adults to explore their expectations of conversational companion robots powered by models, such as large language models (LLMs). The study introduced a functioning robot prototype using LLMs in everyday life scenarios to prompt discussion. Through a thematic analysis process of the data, the findings revealed that older adults prefer robots that actively engage in conversations during isolation and passively accompany them in social settings. Key expectations included memory and personalization, privacy and control over data, useful reminders and information, support for social connections, and the expression of empathy and emotion. Based on the findings outlined, the study offers design recommendations for incorporating models (such as LLMs) into companion robots. The outcomes of the articles provide insights that extend beyond robotic agents and contribute broadly to the development of socially responsive conversational agents for older adults.We hope that with these contributions we have shed light on the importance of the notion of everyday life with robots, and the need for multifaceted HRI approaches that would consider a whole range of factors that together contribute to human lived experiences with robots. The next step is to further develop more nuanced and HRI-specific methods and approaches that would allow us to study a unique phenomenon of life with robots at is unfolds.},
  archive      = {J_FROBT},
  author       = {Zawieska, Karolina and Obaid, Mohammad and Johal, Wafa},
  doi          = {10.3389/frobt.2025.1657518},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1657518},
  shortjournal = {Front. Robot. AI},
  title        = {Editorial: Towards HRI of everyday life},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing weed detection through knowledge distillation and attention mechanism. <em>FROBT</em>, <em>12</em>, 1654074. (<a href='https://doi.org/10.3389/frobt.2025.1654074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weeds pose a significant challenge in agriculture by competing with crops for essential resources, leading to reduced yields. To address this issue, researchers have increasingly adopted advanced machine learning techniques. Recently, Vision Transformers (ViT) have demonstrated remarkable success in various computer vision tasks, making their application to weed classification, detection, and segmentation more advantageous compared to traditional Convolutional Neural Networks (CNNs) due to their self-attention mechanism. However, the deployment of these models in agricultural robotics is hindered by resource limitations. Key challenges include high training costs, the absence of inductive biases, the extensive volume of data required for training, model size, and runtime memory constraints. This study proposes a knowledge distillation-based method for optimizing the ViT model. The approach aims to enhance the ViT model architecture while maintaining its performance for weed detection. To facilitate the training of the compacted ViT student model and enable parameter sharing and local receptive fields, knowledge was distilled from ResNet-50, which serves as the teacher model. Experimental results demonstrate significant enhancements and improvements in the student model, achieving a mean Average Precision (mAP) of 83.47%. Additionally, the model exhibits minimal computational expense, with only 5.7 million parameters. The proposed knowledge distillation framework successfully addresses the computational constraints associated with ViT deployment in agricultural robotics while preserving detection accuracy for weed detection applications.},
  archive      = {J_FROBT},
  author       = {El Alaoui, Ali and Mousannif, Hajar},
  doi          = {10.3389/frobt.2025.1654074},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1654074},
  shortjournal = {Front. Robot. AI},
  title        = {Enhancing weed detection through knowledge distillation and attention mechanism},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining vision and range sensors for AMCL localization in corridor environments with rectangular signs. <em>FROBT</em>, <em>12</em>, 1652251. (<a href='https://doi.org/10.3389/frobt.2025.1652251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Localization is widely recognized as a fundamental problem in mobile robotics. Even though robust localization methods do exist for many applications, it is difficult for them to succeed in complex environments and challenging situations. In particular, corridor-like environments present important issues for traditional range-based methods. The main contribution of this paper is the integration of new observation models into the popular AMCL ROS node, considering visual features obtained from the detection of rectangular landmarks. Visual rectangles are distinctive elements which are very common in man-made environments and should be detected and recognized in a robust manner. This hybrid approach is developed and evaluated both for the combination of an omnidirectional camera and a laser sensor (using artificial markers) and for RGB-D sensors (using natural rectangular features). For the latter, this work also introduces RIDGE, a novel algorithm for detecting projected quadrilaterals representing rectangles in images. Simulations and real world experiments are presented for both cases. As shown and discussed in the article, the proposed approach provides significant advantages for specific conditions and common scenarios such as long straight corridors.},
  archive      = {J_FROBT},
  author       = {de la Puente, Paloma and Vega-Martínez, Germán and Javierre, Patricia and Laserna, Javier and Martin-Arias, Elena},
  doi          = {10.3389/frobt.2025.1652251},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1652251},
  shortjournal = {Front. Robot. AI},
  title        = {Combining vision and range sensors for AMCL localization in corridor environments with rectangular signs},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GNV2-SLAM: Vision SLAM system for cowshed inspection robots. <em>FROBT</em>, <em>12</em>, 1648309. (<a href='https://doi.org/10.3389/frobt.2025.1648309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping (SLAM) has emerged as one of the foundational technologies enabling mobile robots to achieve autonomous navigation, garnering significant attention in recent years. To address the limitations inherent in traditional SLAM systems when operating within dynamic environments, this paper proposes a new SLAM system named GNV2-SLAM based on ORB-SLAM2, offering an innovative solution for the scenario of cowshed inspection. This innovative system incorporates a lightweight object detection network called GNV2 based on YOLOv8. Additionally, it employs GhostNetv2 as backbone network. The CBAM attention mechanism and SCDown downsampling module were introduced to reduce the model complexity while ensuring detection accuracy. Experimental results indicate that the GNV2 network achieves excellent model compression effects while maintaining high performance: mAP@0.5 increased by 1.04%, reaching a total of 95.19%; model parameters were decreased by 41.95%, computational cost reduced by 36.71%, and the model size shrunk by 40.44%. Moreover, the GNV2-SLAM system incorporates point and line feature extraction techniques, effectively mitigate issues reduced feature point extraction caused by excessive dynamic targets or blurred images. Testing on the TUM dataset demonstrate that GNV2-SLAM significantly outperforms the traditional ORB-SLAM2 system in terms of positioning accuracy and robustness within dynamic environments. Specifically, there was a remarkable reduction of 96.13% in root mean square error (RMSE) for absolute trajectory error (ATE), alongside decreases of 88.36% and 86.19% for translation and rotation drift in relative pose error (RPE), respectively. In terms of tracking evaluation, GNV2-SLAM successfully completes the tracking processing of a single frame image within 30 ms, demonstrating expressive real-time performance and competitiveness. Following the deployment of this system on inspection robots and subsequent experimental trials conducted in the cowshed environment, the results indicate that when the robot operates at speeds of 0.4 m/s and 0.6 m/s, the pose trajectory output by GNV2-SLAM is more consistent with the robot's actual movement trajectory. This study systematically validated the system's significant advantages in target recognition and positioning accuracy through experimental verification, thereby providing a new technical solution for the comprehensive automation of cattle barn inspection tasks.},
  archive      = {J_FROBT},
  author       = {Du, Xinwu and Li, Tingting and Jin, Xin and Yu, Xiufang and Xie, Xiaolin and Zhang, Chenglin},
  doi          = {10.3389/frobt.2025.1648309},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1648309},
  shortjournal = {Front. Robot. AI},
  title        = {GNV2-SLAM: Vision SLAM system for cowshed inspection robots},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A PSO–ML-LSTM-based IMU state estimation approach for manipulator teleoperation. <em>FROBT</em>, <em>12</em>, 1638853. (<a href='https://doi.org/10.3389/frobt.2025.1638853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manipulator teleoperation can liberate humans from hazardous tasks. Signal noise caused by environmental disturbances and the devices’ inherent characteristics may limit the teleoperation performance. This paper proposes an approach for inertial measurement unit (IMU) state estimation based on particle swarm optimization (PSO) and modulated long short-term memory (ML-LSTM) neural networks to mitigate the impact of IMU cumulative error on the robot teleoperation performance. A motion mapping model for the human arm and a seven-degree-of-freedom (7-DOF) robotic arm are first established based on global configuration parameters and a hybrid mapping method. This model is used to describe the impact of IMU cumulative error on the robot teleoperation performance. Subsequently, the IMU pose state estimation model is constructed using PSO and ML-LSTM neural networks. The initial data of multiple IMUs and handling handles are used for training the estimation model. Finally, comparative experiments are conducted to verify the performance of the proposed state estimation model. The results demonstrate that the PSO–ML-LSTM algorithm can effectively eliminate the impact of IMU cumulative errors on robot teleoperation.},
  archive      = {J_FROBT},
  author       = {Zhou, Renyi and Li, Yuanchong and Zhang, Aimin and Zhang, Tie and Guan, Yisheng and Zhao, Zhijia and Chen, Shouyan},
  doi          = {10.3389/frobt.2025.1638853},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1638853},
  shortjournal = {Front. Robot. AI},
  title        = {A PSO–ML-LSTM-based IMU state estimation approach for manipulator teleoperation},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating emotional intelligence, memory architecture, and gestures to achieve empathetic humanoid robot interaction in an educational setting. <em>FROBT</em>, <em>12</em>, 1635419. (<a href='https://doi.org/10.3389/frobt.2025.1635419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the integration of individual human traits into an empathetically adaptive educational robot tutor system designed to improve student engagement and learning outcomes with corresponding Engagement Vector measurements. While prior research in the field of Human-Robot Interaction (HRI) has examined the integration of the traits, such as emotional intelligence, memory-driven personalization, and non-verbal communication, by themselves, they have thus-far neglected to consider their synchronized integration into a cohesive, operational education framework. To address this gap, we customize a Multi-Modal Large Language Model (Llama 3.2 from Meta) deployed with modules for human-like traits (emotion, memory and gestures) into an AI-Agent framework. This constitutes the robot’s intelligent core that mimics the human emotional system, memory architecture and gesture controller to allow the robot to behave more empathetically while recognizing and responding appropriately to the student’s emotional state. It can also recall the student’s past learning record and adapt its style of interaction accordingly. This allows the robot tutor to react to the student in a more sympathetic manner by delivering personalized verbal feedback synchronized with relevant gestures. Our study suggests the extent of this effect through the introduction of Engagement Vector Model which can be a benchmark for judging the quality of HRI experience. Quantitative and qualitative results demonstrate that such an empathetic responsive approach significantly improves student engagement and learning outcomes compared with a baseline humanoid robot without these human-like traits. This indicates that robot tutors with empathetic capabilities can create a more supportive, interactive learning experience that ultimately leads to better outcomes for the student.},
  archive      = {J_FROBT},
  author       = {Sun, Fuze and Li, Lingyu and Meng, Shixiangyue and Teng, Xiaoming and Payne, Terry R. and Craig, Paul},
  doi          = {10.3389/frobt.2025.1635419},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1635419},
  shortjournal = {Front. Robot. AI},
  title        = {Integrating emotional intelligence, memory architecture, and gestures to achieve empathetic humanoid robot interaction in an educational setting},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating large language models for intuitive robot navigation. <em>FROBT</em>, <em>12</em>, 1627937. (<a href='https://doi.org/10.3389/frobt.2025.1627937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Home assistance robots face challenges in natural language interaction, object detection, and navigation, mainly when operating in resource-constrained home environments, which limits their practical deployment. In this study, we propose an AI agent framework based on Large Language Models (LLMs), which includes EnvNet, RoutePlanner, and AIBrain, to explore solutions for these issues. Utilizing quantized LLMs allows the system to operate on resource-limited devices while maintaining robust interaction capabilities. Our proposed method shows promising results in improving natural language understanding and navigation accuracy in home environments, also providing a valuable exploration for deploying home assistance robots.},
  archive      = {J_FROBT},
  author       = {Xue, Ziheng and Elksnis, Arturs and Wang, Ning},
  doi          = {10.3389/frobt.2025.1627937},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1627937},
  shortjournal = {Front. Robot. AI},
  title        = {Integrating large language models for intuitive robot navigation},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A projection-based inverse kinematic model for extensible continuum robots and hyper-redundant robots with an elbow joint. <em>FROBT</em>, <em>12</em>, 1627688. (<a href='https://doi.org/10.3389/frobt.2025.1627688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse kinematics is a core problem in robotics, involving the use of kinematic equations to calculate the joint configurations required to achieve a target pose. This study introduces a novel inverse kinematic model (IKM) for extensible (i.e., length-adjustable) continuum robots (CRs) and hyper-redundant robots (HRRs) featuring an elbow joint. This IKM numerically solves a set of equations representing geometric constraints (abbreviated as NSGC). NSGC can handle target poses Xt=[xt,yt,zt,ψt] in 3D space, which are projected onto a 2D plane and solved numerically. NSGC is capable of real-time operation and accounts for elbow joint limits. Extensive simulations and empirical tests confirm the reliability, performance, and practical applicability of NSGC.},
  archive      = {J_FROBT},
  author       = {Fritsch, Sven and Oberschmidt, Dirk},
  doi          = {10.3389/frobt.2025.1627688},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1627688},
  shortjournal = {Front. Robot. AI},
  title        = {A projection-based inverse kinematic model for extensible continuum robots and hyper-redundant robots with an elbow joint},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gödelian embodied self-referential genomic intelligence: Lessons for AI and AGI from the genomic blockchain. <em>FROBT</em>, <em>12</em>, 1624695. (<a href='https://doi.org/10.3389/frobt.2025.1624695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security of code-based digital records is a major concern of the 21st century. AI and artificial general intelligence (AGI) can be hacked to pieces by digital adversaries, and some AI objectives can lead to existential threats. The former arises from sitting duck problems that all software systems are vulnerable to, and the latter include control and misalignment problems. Blockchain technology, circa 2009, can address these problems: hashing algorithms rely on a consensus mechanism in manmade software systems to keep early blocks of software immutable and tamper-proof from digital malware, while new blocks can be added only if consistently aligned with original blocks. There is evidence that the ancient precedent of the genomic blockchain, underpinning the unbroken chain of life, uses a self-referential rather than a consensus-based hashing algorithm. Knowledge of self-codes permits biotic elements to achieve a hack-free agenda by self-reporting that they have been “negated,” or hacked, exactly implementing the Gödel sentence from foundational mathematics of Gödel, Turing, and Post (G–T–P). This results in an arms race in open-ended novelty to secure the primacy of original self-codes. Selfhood and autonomy are staples of neuroscience on complex self–other social cognition and increasingly of autonomous AGI agents capable of end-to-end programmed self-assembly. My perspective is that self-referential G–T–P information processing, first found in the adaptive immune system of jawed fish 500 mya and more recently in mirror neuron systems of humans, has enabled code-based self-organized intelligent systems like life to survive over 3.7 billion years. Some lessons for AGI can be gleaned from this discussion.},
  archive      = {J_FROBT},
  author       = {Markose, Sheri},
  doi          = {10.3389/frobt.2025.1624695},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1624695},
  shortjournal = {Front. Robot. AI},
  title        = {Gödelian embodied self-referential genomic intelligence: Lessons for AI and AGI from the genomic blockchain},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing reliability centered maintenance in automotive robotic welding machines for a tier 1 supplier. <em>FROBT</em>, <em>12</em>, 1620370. (<a href='https://doi.org/10.3389/frobt.2025.1620370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study highlights the effectiveness of FMEA in robotic spot-welding operations, providing a systematic approach to enhancing performance in an automotive assembly line. Robotic welding industries depend on mechanized, programmable tools to automate welding processes, ensuring efficiency, reliability, and effective material handling. In the automotive sector, Tier 1 suppliers utilize robotic welding machines to produce high volumes of welded assemblies, with daily output exceeding 450 units. However, frequent equipment downtime due to maintenance challenges disrupts productivity and impacts customer satisfaction. This study aims to develop a Reliability-Centered Maintenance (RCM) approach for robotic welding industries, optimizing machine uptime, enhancing product quality, and reducing financial losses caused by unexpected failures. A 3-year dataset was analysed to identify the primary causes of downtime and their associated costs. Failure Modes and Effects Analysis (FMEA) was applied to assess failure modes, determine root causes, and calculate Risk Priority Numbers (RPNs), thereby formulating corrective actions to mitigate recurring failures and enhance operational efficiency. Findings revealed that maintenance-related issues accounted for 79% of total downtime, resulting in financial losses of R2,281,508.82 over 3 years. The application of FMEA provided a structured framework for prioritizing critical failure modes and implementing targeted corrective measures to reduce downtime and enhance overall reliability. To sustain high productivity and quality, it is recommended that robotic welding industries adopt proactive maintenance strategies based on FMEA findings. Regular monitoring, predictive maintenance, and workforce training will help minimize machine failures and optimize operational efficiency.},
  archive      = {J_FROBT},
  author       = {Alaka, H. T. O. and Mpofu, K. and Ramatsetse, B. I. and Adegbola, T. A. and Adeoti, M. O.},
  doi          = {10.3389/frobt.2025.1620370},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1620370},
  shortjournal = {Front. Robot. AI},
  title        = {Developing reliability centered maintenance in automotive robotic welding machines for a tier 1 supplier},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion models for robotic manipulation: A survey. <em>FROBT</em>, <em>12</em>, 1606247. (<a href='https://doi.org/10.3389/frobt.2025.1606247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion generative models have demonstrated remarkable success in visual domains such as image and video generation. They have also recently emerged as a promising approach in robotics, especially in robot manipulations. Diffusion models leverage a probabilistic framework, and they stand out with their ability to model multi-modal distributions and their robustness to high-dimensional input and output spaces. This survey provides a comprehensive review of state-of-the-art diffusion models in robotic manipulation, including grasp learning, trajectory planning, and data augmentation. Diffusion models for scene and image augmentation lie at the intersection of robotics and computer vision for vision-based tasks to enhance generalizability and data scarcity. This paper also presents the two main frameworks of diffusion models and their integration with imitation learning and reinforcement learning. In addition, it discusses the common architectures and benchmarks and points out the challenges and advantages of current state-of-the-art diffusion-based methods.},
  archive      = {J_FROBT},
  author       = {Wolf, Rosa and Shi, Yitian and Liu, Sheng and Rayyes, Rania},
  doi          = {10.3389/frobt.2025.1606247},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1606247},
  shortjournal = {Front. Robot. AI},
  title        = {Diffusion models for robotic manipulation: A survey},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review and bibliometric analysis on collaborative robotics for industry: Safety emerging as a core focus. <em>FROBT</em>, <em>12</em>, 1605682. (<a href='https://doi.org/10.3389/frobt.2025.1605682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research organizations and academics often seek to map the development of scientific fields, identify research gaps, and guide the direction of future research. In cobot-related research, the scientific literature consulted does not propose any comprehensive research agenda. Moreover, cobots, industrial robots inherently designed to collaborate with humans, bring with them emerging issues. To solve them, interdisciplinary research is often essential (e.g., combination of engineering, ergonomics and biomechanics expertise to handle safety challenges). This paper proposes an exhaustive study that employs a scoping review and bibliometric analysis to provide a structured macro perspective on the developments, key topics, and trends in cobot research for industry. A total of 2,195 scientific publications were gained from the Web of Science database, and a thorough selection process narrowed them down to 532 papers for comprehensive analysis. Descriptive statistics were employed to analyze bibliometric measures, highlighting publication trends, leading journals, the most productive institutions, engaged countries, influential authors, and prominent research topics. Co-authorship and bibliographic couplings were also examined. Through a co-occurrence analysis of terms, the content and research objectives of the papers were systematically reviewed and lead to a univocal categorization framework. That categorization can support organizations or researchers in different cobotics (collaborative robotics) fields by understanding research developments and trends, identifying collaboration opportunities, selecting suitable publication venues, advancing the theoretical and experimental understanding of automatic collaborative systems, and identifying research directions and predicting the evolution of publication quantity in cobotics.},
  archive      = {J_FROBT},
  author       = {Haghighi, Aida and Cheraghi, Morteza and Pocachard, Jérôme and Botta-Genoulaz, Valérie and Jocelyn, Sabrina and Pourzarei, Hamidreza},
  doi          = {10.3389/frobt.2025.1605682},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1605682},
  shortjournal = {Front. Robot. AI},
  title        = {A comprehensive review and bibliometric analysis on collaborative robotics for industry: Safety emerging as a core focus},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomy in socially assistive robotics: A systematic review. <em>FROBT</em>, <em>12</em>, 1586473. (<a href='https://doi.org/10.3389/frobt.2025.1586473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Socially assistive robots are increasingly being researched and deployed in various domains such as education, healthcare, service, and even as collaborators in a variety of other workplaces. Similarly, SARs are also expected to interact in a socially acceptable manner with a wide audience, ranging from preschool children to the elderly. This diversity of application domains and target populations raises technical and social challenges that are yet to be overcome. While earlier works relied on the Wizard-of-Oz (WoZ) paradigm to give an illusion of interactivity and intelligence, a transition toward more autonomous robots can be observed. In this article, we present a systematic review, following the PRISMA method, of the last 5 years of Socially Assistive Robotics research, centered around SARs’ level of autonomy with a stronger focus on fully and semi-autonomous robots than non-autonomous ones. Specifically, to analyse SARs’ level of autonomy, the review identifies which sensing and actuation capabilities of SARs are typically automated and which ones are not, and how these capabilities are automated, with the aim of identifying potential gaps to be explored in future research. The review further explores whether SARs’ level of autonomy and capabilities are transparently communicated to the diverse target audiences above described and discusses the potential benefits and drawbacks of such transparency. Finally, with the aim of providing a more holistic view of SARs’ characteristics and application domains, the review also reports the embodiment and commonly envisioned role of SARs, as well as their interventions’ size, length and environment.},
  archive      = {J_FROBT},
  author       = {Maure, Romain and Bruno, Barbara},
  doi          = {10.3389/frobt.2025.1586473},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1586473},
  shortjournal = {Front. Robot. AI},
  title        = {Autonomy in socially assistive robotics: A systematic review},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Encouraging classroom activities for children using avatar robots: A field trial. <em>FROBT</em>, <em>12</em>, 1571804. (<a href='https://doi.org/10.3389/frobt.2025.1571804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Educational institutions are facing a critical shortage of teachers worldwide. Consequently, the trend of introducing interactive robots into educational sites is growing. However, most previous research focused on specific subjects or time slots, and only a few studies have introduced interactive robots to participate in whole classroom activities with children routinely. This study investigates the use of avatar robots operated by multiple remote operators in elementary school classrooms. Over nine days, a 5th-grade class was observed to assess the robot’s impact on student engagement, motivation, and peer interactions, and compared to classes where any avatar robots were not introduced. Key findings include improved student confidence in presentations, enhanced social interactions during recess, and positive feedback on the robot’s role in supporting classroom activities. The results suggest that avatar robots, with consistent remote operation, can provide valuable educational support without strong negative reactions from students.},
  archive      = {J_FROBT},
  author       = {Kawata, Megumi and Maeda, Masashi and Kumazaki, Hirokazu and Kamide, Hiroko and Baba, Jun and Matsuura, Naomi and Ishiguro, Hiroshi and Yoshikawa, Yuichiro},
  doi          = {10.3389/frobt.2025.1571804},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1571804},
  shortjournal = {Front. Robot. AI},
  title        = {Encouraging classroom activities for children using avatar robots: A field trial},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-specific CNN size reduction through content-specific pruning. <em>FROBT</em>, <em>12</em>, 1552068. (<a href='https://doi.org/10.3389/frobt.2025.1552068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread and growing use of flying unmanned aerial vehicles (UAVs) is attributed to their high spatial mobility, autonomous control, and lower cost compared to usual manned flying vehicles. Applications, such as surveying, searching, or scanning the environment with application-specific sensors, have made extensive use of UAVs in fields like agriculture, geography, forestry, and biology. However, due to the large number of applications and types of UAVs, limited power has to be taken into account when designing task-specific software for a target UAV. In particular, the power constraints of smaller UAVs will generally necessitate reducing power consumption by limiting functionality, decreasing their movement radius, or increasing their level of autonomy. Reducing the overhead of control and decision-making software onboard is one approach to increasing the autonomy of UAVs. Specifically, we can make the onboard control software more efficient and focused on specific tasks, which means it will need less computing power than a general-purpose algorithm. In this work, we focus on reducing the size of the computer vision object classification algorithm. We define different tasks by specifying which objects the UAV must recognize, and we construct a convolutional neural network (CNN) for each specific classification. However, rather than creating a custom CNN that requires its dataset, we begin with a pre-trained general-purpose classifier. We then choose specific groups of objects to recognize, and by using response-based pruning (RBP), we simplify the general-purpose CNN to fit our specific needs. We evaluate the pruned models in various scenarios. The results indicate that the evaluated task-specific pruning can reduce the size of the neural model and increase the accuracy of the classification tasks. For small UAVs intended for tasks with reduced visual content, the proposed method solves both the size reduction and individual model training problems.},
  archive      = {J_FROBT},
  author       = {Konyrbaev, Nurbek and Lukac, Martin and Ibadulla, Sabit and Diveev, Askhat and Sofronova, Elena and Galymzhankyzy, Asem},
  doi          = {10.3389/frobt.2025.1552068},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1552068},
  shortjournal = {Front. Robot. AI},
  title        = {Task-specific CNN size reduction through content-specific pruning},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VIO-GO: Optimizing event-based SLAM parameters for robust performance in high dynamic range scenarios. <em>FROBT</em>, <em>12</em>, 1541017. (<a href='https://doi.org/10.3389/frobt.2025.1541017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a critical challenge in Industry 4.0 robotics by enhancing Visual Inertial Odometry (VIO) systems to operate effectively in dynamic and low-light industrial environments, which are common in sectors like warehousing, logistics, and manufacturing. Inspired by biological sensing mechanisms, we integrate bio-inspired event cameras to improve state estimation systems performance in both dynamic and low-light conditions, enabling reliable localization and mapping. The proposed state estimation framework integrates events, conventional video frames, and inertial data to achieve reliable and precise localization with specific emphasis on real-world challenges posed by high-speed and cluttered settings typical in Industry 4.0. Despite advancements in event-based sensing, there is a noteworthy gap in optimizing Event Simultaneous Localization and Mapping (SLAM) parameters for practical applications. To address this, we introduce a novel VIO-Gradient-based Optimization (VIO-GO) method that employs Batch Gradient Descent (BGD) for efficient parameter tuning. This automated approach determines optimal parameters for Event SLAM algorithms by using motion-compensated images to represent event data. Experimental validation on the Event Camera Dataset shows a remarkable 60% improvement in Mean Position Error (MPE) over fixed-parameter methods. Our results demonstrate that VIO-GO consistently identifies optimal parameters, enabling precise VIO performance in complex, dynamic scenarios essential for Industry 4.0 applications. Additionally, as parameter complexity scales, VIO-GO achieves a 24% reduction in MPE when using the most comprehensive parameter set (VIO-GO8) compared to a minimal set (VIO-GO2), highlighting the method’s scalability and robustness for adaptive robotic systems in challenging industrial environments.},
  archive      = {J_FROBT},
  author       = {Sakhrieh, Saber and Singh, Abhilasha and Mounsef , Jinane and Arain , Bilal and Maalouf , Noel},
  doi          = {10.3389/frobt.2025.1541017},
  journal      = {Frontiers in Robotics and AI},
  month        = {9},
  pages        = {1541017},
  shortjournal = {Front. Robot. AI},
  title        = {VIO-GO: Optimizing event-based SLAM parameters for robust performance in high dynamic range scenarios},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
