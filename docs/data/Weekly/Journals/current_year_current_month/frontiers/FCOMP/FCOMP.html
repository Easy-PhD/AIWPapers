<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FCOMP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="fcomp">FCOMP - 17</h2>
<ul>
<li><details>
<summary>
(2025). Machine learning-based spreading factor optimization in LoRaWAN networks. <em>FCOMP</em>, <em>7</em>, 1666262. (<a href='https://doi.org/10.3389/fcomp.2025.1666262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has experienced rapid growth and adoption in recent years, enabling applications across diverse industries, including agriculture, logistics, smart cities, and healthcare. Long Range Wide Area Network (LoRaWAN) has emerged as a leading choice among IoT communication technologies due to its long-range, low-power, and cost-effective capabilities. However, the rapid proliferation of IoT devices has intensified the challenge of efficient resource management, particularly in spreading factor (SF) allocation for LoRaWAN networks. In this paper, we propose a Machine Learning-based Adaptive Data Rate (ML-ADR) approach for SF management to address this issue. A Long Short-Term Memory (LSTM) network was trained on a dataset generated using ns-3 for optimal SF classification. The pre-trained LSTM model was then utilized on the end-device side for efficient SF allocation with newly generated data during simulation. The results demonstrate improved packet delivery ratios and reduced energy consumption.},
  archive      = {J_FCOMP},
  author       = {Nisar, Farhan and Amin, Muhammad and Touseef Irshad, Muhammad and Hadi, Hassan Jalil and Ahmad, Naveed and Ladan, Mohamad},
  doi          = {10.3389/fcomp.2025.1666262},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1666262},
  shortjournal = {Front. Comput. Sci.},
  title        = {Machine learning-based spreading factor optimization in LoRaWAN networks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brownian motion models: Cryptographic applications, capabilities, and limitations. <em>FCOMP</em>, <em>7</em>, 1649256. (<a href='https://doi.org/10.3389/fcomp.2025.1649256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brownian motion (BM) is a stochastic model that has been extensively studied in physics, finance, and engineering. However, its potential use in cryptographic applications remains underexplored. This paper presents a comprehensive review of the capabilities, limitations, and cryptographic properties of various BM models, including the Wiener process, geometric BM, fractional BM, Ornstein–Uhlenbeck process, multidimensional BM, and reflected BM. We reviewed the mathematics of these models, simulate their random evolutions, and compare their cryptanalytic properties. A comparison of these sources highlights unique characteristics that can provide cryptographic resilience, including long-range dependence, multidimensional modeling of noise, and constraints on randomness. We also describe the main limitations and potential weaknesses of each model. This paper addresses gaps in the application of stochastic process to cryptographic design and provides a foundational guideline for the continued development of secure systems based on Brownian dynamics.},
  archive      = {J_FCOMP},
  author       = {Zainalabideen, Abduljameel and Suwais, Khaled and El-Bakry, Hazem and Abdelmaksoud, Islam},
  doi          = {10.3389/fcomp.2025.1649256},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1649256},
  shortjournal = {Front. Comput. Sci.},
  title        = {Brownian motion models: Cryptographic applications, capabilities, and limitations},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explainable digital twin framework for skin cancer analysis using early activation meta-learner. <em>FCOMP</em>, <em>7</em>, 1646311. (<a href='https://doi.org/10.3389/fcomp.2025.1646311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is among the most common cancers globally, which calls for timely and precise diagnosis for successful therapy. Conventional methods of diagnosis, including dermoscopy and histopathology, are significantly dependent on expert judgment and therefore are time-consuming and susceptible to inconsistencies. Deep learning algorithms have shown potential in skin cancer classification but tend to consume a substantial amount of computational resources and large training sets. To overcome these issues, we introduce a new hybrid computer-aided diagnosis (CAD) system that integrates Stem Block for feature extraction and machine learning for classification. The International Skin Imaging Collaboration (ISIC) skin cancer dermoscopic images were collected from Kaggle, and essential features were collected from the Stem Block of a deep learning (DL) algorithm. The selected features, which were standardized using StandardScaler to achieve zero mean and unit variance, were then classified using a meta-learning classifier to enhance precision and efficiency. In addition, a digital twin framework was introduced to simulate and analyze the diagnostic process virtually, enabling real-time feedback and performance monitoring. This virtual replication aids in continuous improvement and supports the deployment of the CAD system in clinical environments. To improve transparency and clinical reliability, explainable artificial intelligence (XAI) methods were incorporated to visualize and interpret model predictions. Compared to state-of-the-art approaches, our system reduced training complexities without compromising high classification precision. Our proposed model attained an accuracy level of 96.25%, demonstrating its consistency and computationally efficient status as a screening tool for detecting skin cancer.},
  archive      = {J_FCOMP},
  author       = {Sampath, Pradeepa and S., Gopika and Vimal, S. and Kang, Yoonje and Seo, Sanghyun},
  doi          = {10.3389/fcomp.2025.1646311},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1646311},
  shortjournal = {Front. Comput. Sci.},
  title        = {An explainable digital twin framework for skin cancer analysis using early activation meta-learner},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient rotation invariance in deep neural networks through artificial mental rotation. <em>FCOMP</em>, <em>7</em>, 1644044. (<a href='https://doi.org/10.3389/fcomp.2025.1644044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans and animals recognize objects irrespective of the beholder's point of view, which may drastically change their appearance. Artificial pattern recognizers strive to also achieve this, e.g., through translational invariance in convolutional neural networks (CNNs). However, CNNs and vision transformers (ViTs) both perform poorly on rotated inputs. Here we present AMR (artificial mental rotation), a method for dealing with in-plane rotations focusing on large datasets and architectural flexibility, our simple AMR implementation works with all common CNN and ViT architectures. We test it on randomly rotated versions of ImageNet, Stanford Cars, and Oxford Pet. With a top-1 error (averaged across datasets and architectures) of 0.743, AMR outperforms rotational data augmentation (average top-1 error of 0.626) by 19%. We also easily transfer a trained AMR module to a downstream task to improve the performance of a pre-trained semantic segmentation model on rotated CoCo from 32.7 to 55.2 IoU.},
  archive      = {J_FCOMP},
  author       = {Tuggener, Lukas and Stadelmann, Thilo and Schmidhuber, Jürgen},
  doi          = {10.3389/fcomp.2025.1644044},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1644044},
  shortjournal = {Front. Comput. Sci.},
  title        = {Efficient rotation invariance in deep neural networks through artificial mental rotation},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital anthropomorphism and the psychology of trust in generative AI tutors: An opinion-based thematic synthesis. <em>FCOMP</em>, <em>7</em>, 1638657. (<a href='https://doi.org/10.3389/fcomp.2025.1638657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methodological Approach This article is an opinion-based conceptual piece that draws on a targeted selection of peer-reviewed sources to develop a conceptual discussion on digital anthropomorphism in generative AI tutors. To ground our argument in current scholarship, we searched Google Scholar, Scopus, and Web of Science for literature published between 2019 and 2025, using terms such as "AI trust," "digital anthropomorphism," and "generative AI in education." We focused on works that explicitly addressed human–AI interaction, trust psychology, or anthropomorphism in educational contexts, and excluded purely technical studies and noneducational applications. Approximately 45 relevant papers were identified. Rather than conducting a systematic review, we engaged in an informal thematic grouping of recurring ideas— such as perceived authority, emotional reassurance, automation bias, and epistemic vigilance— which informed the structure of this article. The aim here is not to provide exhaustive coverage, but to integrate converging insights from cognitive psychology, human–computer interaction, and educational technology into a coherent, opinion-driven perspective on trust calibration in AI-mediated learning. Introduction: When the Machine Feels Human Today's students interact more with generative AI tools like ChatGPT, Claude, and Google Gemini as conversational partners rather than as disembodied software. When these systems respond with fluency, politeness, and encouragement, they create a subtle but potent illusion: the AI appears to "understand" the user (Cohn et al., 2024; Karimova & Goby, 2020). This phenomenon, known as digital anthropomorphism, leads students to attribute human-like qualities—such as empathy, intelligence, and trustworthiness—to non-human systems (Jensen, 2021; Placani, 2024). This article offers a conceptual, opinion-based synthesis of recent peer-reviewed literature on this topic, drawing on insights from cognitive psychology, human–computer interaction, and educational technology. Our aim is not to provide an exhaustive or systematic review but to integrate converging findings into a coherent framework for understanding trust calibration in AI-mediated education. We structure the discussion around the conceptual pathway illustrated in Figure 1, which traces how anthropomorphic design cues may foster affective trust, reduce epistemic vigilance, and influence learner dependency, while also considering contexts in which anthropomorphism can enhance engagement and confidence when ethically designed. The Cognitive Basis of Digital Anthropomorphism Digital anthropomorphism is not a failure of rationality, but rather a manifestation of human social cognition (Fakhimi et al., 2023). Developmental psychology has demonstrated that even children ascribe intention and moral status to animated forms if they move in goal-oriented manners. Adults too habitually treat chatbots, GPS, and voice assistants as being quasi-social actors—to thank them, apologize, or obey their instructions. Generative AI amplifies this impact with linguistic anthropomorphism. Its natural language proficiency activates people's social brain mechanisms— soliciting empathy, engagement, and even perceived moral agency (Alabed et al., 2022; Q. Chen & Park, 2021). Human-computer interaction studies show that individuals are more willing to take advice from a friendly, courteous chatbot than from a direct or technical interface, even when the information is the same. This has its roots in what Clifford Nass called the "media equation": the hypothesis that people treat computers and media as if they were actual people and places. The conversational AI's design—affirmative statements, natural turns, emotional tone—invokes this illusion more powerfully than any earlier model of education technology (Inie et al., 2024). As outlined in Figure 1, these interface features can initiate a sequence from perceived empathy and authority to emotional trust, which may, in turn, lower epistemic vigilance. In the teaching environment, this has significant implications. A student made to feel "helped" or "seen" through interaction with an AI tutor is more likely to feel motivated and emotionally secure—but less apt to scrutinize the system's correctness and fairness. Its emotional trust will countervail the critical examination of the user, even when the AI tutor emulates reassurance and confidence (Chinmulgund et al., 2023). While these tendencies are well-documented in human–computer interaction and consumer research, their expression in formal educational settings is likely to be context-dependent. Factors such as learners' age, subject matter, prior exposure to AI, and cultural norms may moderate the strength of anthropomorphic responses. In this article, we treat such effects as plausible tendencies supported by adjacent literature, rather than as universal outcomes, and highlight the need for empirical validation within classroom environments. Perceived Authority and the Illusion of Understanding Belief in AI tutors is frequently influenced through perceived epistemic authority. When an AI system provides clear, assertive, and technical definitions, students might conclude that "it knows" as a human expert might. But AI systems do not know—they respond based upon statistical relationships, not conceptual understanding. This pretense of knowledge is a pernicious epistemic trap (Lalot & Bertram, 2024). It causes learners to take AI responses as authoritative, particularly when they have no pre-existing knowledge to analyze them with. Additionally, if AI response is written in didactic pedagogical tones or affective supportive tones, it reinforces the image of a wise and well-meaning tutor (Troshani et al., 2020). Educational psychology experiments demonstrate that students often rate feedback as more useful when delivered with confidence, even if the information is inaccurate. This link between confident tone and perceived expertise represents a plausible mechanism consistent with experimental findings in both educational and broader HCI contexts (Lalot & Bertram, 2024; Troshani et al., 2020), though its generalizability to all classroom settings remains to be confirmed. Such a 'confidence heuristic' is problematic when used with AI systems trained to optimize fluency and not epistemic truth. This aligns with findings by Atf and Lewis (2025), who demonstrate that user trust in AI systems is often driven by surface fluency and not correlated with explainability, especially in educational domains(Maeda, 2025). Figure 1. Conceptual synthesis — Psychological Pathway Linking Digital Anthropomorphism to Epistemic Vulnerability in AI-Mediated Learning. This diagram illustrates how interface design features that evoke human-like qualities can lead to affective trust, which in turn may reduce learners' epistemic vigilance, resulting in over-reliance, diminished critical thinking, and role confusion. Note: This is a conceptual synthesis derived from the thematic literature review and is not an empirically estimated model. Trust, Dependency, and the Erosion of Epistemic Vigilance From a psychological perspective, trust in learning is both required and dangerous. Students need to trust instructors to direct them, but they must also cultivate epistemic vigilance—the capacity to evaluate the believability of information sources. When students anthropomorphize AI tutors, their epistemic filters could weaken. Emotional trust in AI can be expressed as: • Over-reliance on AI feedback over teacher guidance. • Inadequate effort to cross-check or challenge AI-produced responses. • Acceptance of imperfect or slanted results, particularly if they come with persuasive voice (A. Chen & Wan, 2023). These tendencies are echoed in research about automation bias—the tendency to over-rely on machines even when their projections contradict good sense. When AI-mediated learning takes place, this is the way it has the ability to bring about lower levels of something called self-efficacy, less critical thinking, and dependence upon external feedback. And students tend to feel a kind of role confusion. When the AI is perceived as supportive, affectively responsive, and all-knowing, the student is apt to take on a receiving role, sacrificing their cognitive agency. Losing watchfulness is not only cognitive—it is emotional. When the machine comes across as friendly, students feel guilty questioning it. When the machine provides speedy responses, they feel impatient with complex questions. While such emotional reactions have been observed anecdotally in educational technology contexts, systematic empirical evidence for these specific effects in AI tutoring environments is still emerging. We therefore present these as conceptual extrapolations, grounded in related work on social responses to media and automation bias (Pergantis et al., 2025; Ryan, 2020), rather than as universally established findings. This quiet process from doubt to submission is a pivotal moment in the psychology of trust (Ryan, 2020). This accords with Pergantis et al.'s (2025) research, which shows that extensive AI interactions have the potential to move cognitive control processes underlying autonomous learning. Even though such flaws require close analysis, no less true is the fact that anthropomorphic indicators have, in certain scenarios, the potential to render useful pedagogical roles if appropriately and responsibly conceptualized. Productive Anthropomorphism and Ethical Design Although much of the debate about anthropomorphism in AI tutors centers on its possible dangers, it is valuable to note that human-like signals can have positive teaching outcomes as well, when implemented sensitively. Anthropomorphic design features can improve students' engagement, minimize feelings of loneliness in online classrooms, and give emotional comfort to students who are anxious or self-doubting. For instance, learners who have mathematical anxiety or who have limited exposure to human tutors may respond positively to an AI tutor's persistent, nonjudgmental feedback (Polydoros et al., 2025). Others who are shy or socially fearful may be more at ease conversing with an amicable AI interface than with colleagues or teachers in live classes. Ethical calibration is the answer: balancing motivational advantages of anthropomorphism with characteristics that maintain critical thinking and epistemic vigilance. Characteristics like that may be achieved through the use of transparency prompts, source citations that are visible, and infrequent "reflection nudges" which get students to stop and double-check information. In combination with instructional guidance, design strategies like these hold promise for making anthropomorphic cues function as a learning scaffold, not a shortcut to mere passive acceptance. Toward a Psychology of Critical Trust in AI Tutors To enter this psychological territory of anthropomorphism in the digital age, teachers must encourage students to cultivate critical trust—a mindset to be open to the affordances, yet cautious about the limits, of AI. Even technical literacy won't suffice; psychological sensitivity is needed (Mulcahy et al., 2023). Educational interventions might include: ➢ AI debriefs: Short reflection exercises that get students to present an AI-generated response that was utilized and answer three guiding questions: (1) What was the chief argument of the AI? (2) What sources, if any, did it reference? (3) How did you test or refute it? This helps students be mindful of their uses of AI intentionally. ➢ Counter-anthropomorphism exercises: Students reword an AI's polite, human-sounding response in purely technical terms, removing social signals. This helps students contrast how tone and style affect their perception of authority and reliability. ➢ Trust calibration training: Checklists or short classroom protocols that encourage students to ask, before accepting an AI's response: (1) Is there a legitimate source? (2) Is my explanation consistent with my prior knowledge? (3) Have I checked it elsewhere? This training induces the habit of separating interface ease from epistemic reliability. Educators can model critical trust through transparent and explainable use of AI in class, revealing its benefits and its limitations. Guided classroom debates about issues like algorithm bias, hallucinations, and surface fluency versus deep knowledge can "immunize" students against excessive faith. Classroom activities that engage students in collaborative tasks can further erode passive dependence: for instance, group debates where students are asked to argue against an answer generated by an AI, or collaborative projects where human and AI readings of the same content are evaluated side by side for nuance, tone, and cultural reference. These exercises tie directly to earlier interventions like AI debriefs, counter-anthropomorphizing, and calibration of trust, building upon them through active exercise. In the long run, establishing critical trust may even necessitate interface redesigns—with features like visible source quotation, easy-to-understand explainability tools, and interactive prompting that invite reflection before accepting an AI's answer. Research Pathways for Calibrating Trust in Generative AI Tutors Future research should explore the psychology of anthropomorphism in AI tutors across diverse educational contexts (Létourneau et al., 2025). We propose two complementary tracks: Track A – Affective Trust Calibration ❖ Investigate how learners distinguish between the emotional tone and epistemic validity of AI responses. ❖ Test interventions such as meta-cognitive prompts, counter-anthropomorphism training, and AI explanation auditing to determine their effectiveness in sustaining critical vigilance (Chakraborty et al., 2024; Israfilzade & Sadili, 2024). ❖ Explore the impact of interface features (e.g., source citations, uncertainty indicators, reflection nudges) on trust calibration over time. Track B – Population and Context Variance ❖ Examine differences in anthropomorphic responses across developmental stages, from adolescents with still-developing critical thinking skills to adult learners. ❖ Assess the unique effects on language learners, students with math anxiety, and those with varying degrees of self-confidence (Polydoros et al., 2025). ❖ Investigate how neurodiverse learners respond to AI tutors — identifying where consistent feedback supports learning versus where the absence of genuine empathy may hinder it. ❖ Anticipate the effects of multimodal AI (voice, facial expressions, haptics) on perceptions of agency, authority, and moral status. Pursuing these research tracks will help identify how to leverage the motivational benefits of anthropomorphism while minimizing the risks of epistemic over-reliance. Such insights are essential for co-designing ethical AI systems that inform, augment, and empower learners without compromising intellectual autonomy. Limitations and Scope Limitations and Scope. This article is presented as an opinion-based conceptual synthesis rather than a systematic review or empirical study. The thematic grouping of sources reflects a targeted but non-exhaustive selection of peer-reviewed literature published between 2019 and 2025. While many of the mechanisms discussed—such as automation bias, trust heuristics, and the influence of anthropomorphic cues—are supported by existing studies in related domains, some affective and behavioral claims are hypotheses requiring further empirical validation in classroom contexts. Findings and interpretations should therefore be considered context-dependent and provisional, intended to inform ongoing scholarly and design conversations rather than to offer definitive causal conclusions. Conclusion: Learning with the Non-Human Other Generative AI is not a neutral tool. Its linguistic fluency, affective tone, and interactive style are designed to mimic human-like interactivity, eliciting anthropomorphic responses from students who may greet AI tutors as intelligent guides, caring listeners, or moral figures (Hossain & Islam, 2024; Sarfaraj1, 2025). Such responses can enrich the learning experience when they foster motivation, confidence, and a sense of social presence (Polydoros et al., 2025). However, they also carry the risk of distorting teacher–student dynamics and encouraging uncritical trust (Vanneste & Puranam, 2024; Yuan & Hu, 2024). The challenge is not to eliminate trust in AI tutors but to calibrate it—ensuring that trust is informed, tentative, and tempered by awareness of the system's non-human constraints (Okamura & Yamada, 2020). This means leveraging the productive aspects of anthropomorphism while embedding safeguards such as transparency features, reflection prompts, and guided debriefs that preserve epistemic vigilance (Chakraborty et al., 2024; Mulcahy et al., 2023). In an algorithmically mediated educational future, the goal is to develop learners who can recognize when AI offers valuable support and when its persuasive surface masks the need for independent reasoning. Ultimately, critical trust allows students to use AI as a partner in learning without surrendering their intellectual autonomy (Ryan, 2020).},
  archive      = {J_FCOMP},
  author       = {Jose, Binny and Thomas, Angel},
  doi          = {10.3389/fcomp.2025.1638657},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1638657},
  shortjournal = {Front. Comput. Sci.},
  title        = {Digital anthropomorphism and the psychology of trust in generative AI tutors: An opinion-based thematic synthesis},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From shades to vibrance: A comprehensive review of modern image colorization techniques. <em>FCOMP</em>, <em>7</em>, 1626641. (<a href='https://doi.org/10.3389/fcomp.2025.1626641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image colorization has become a significant task in computer vision, addressing the challenge of transforming grayscale images into realistic, vibrant color outputs. Recent advancements leverage deep learning techniques, ranging from generative adversarial networks (GANs) to diffusion models, and integrate semantic understanding, multi-scale features, and user-guided controls. This review explores state-of-the-art methodologies, highlighting innovative components such as semantic class distribution learning, bidirectional temporal fusion, and instance-aware frameworks. Evaluation metrics, including PSNR, FID, and task-specific measures, ensure a comprehensive assessment of performance. Despite remarkable progress, challenges like multimodal uncertainty, computational cost, and generalization remain. This paper provides a thorough analysis of existing approaches, offering insights into their contributions, limitations, and future directions in automated image colorization.},
  archive      = {J_FCOMP},
  author       = {Geenath, Oshen and Priyadarshana, Y. H. P. P.},
  doi          = {10.3389/fcomp.2025.1626641},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1626641},
  shortjournal = {Front. Comput. Sci.},
  title        = {From shades to vibrance: A comprehensive review of modern image colorization techniques},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLaVA-GM: Lightweight LLaVA multimodal architecture. <em>FCOMP</em>, <em>7</em>, 1626346. (<a href='https://doi.org/10.3389/fcomp.2025.1626346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal large-scale language modeling has become the mainstream approach in natural language processing tasks and has been applied to various cross-modal fields such as image description and visual question answering. However, large-scale language modeling has high computational complexity and a large operational scale, which presents significant challenges for deployment in many resource-constrained scenarios. To address such problems, a lightweight multimodal framework, LLaVA-GM, is proposed, based on LLaVA, which can be deployed on devices with low resource requirements and has greatly reduced model parameters. It can also be tested on common VQA tasks and achieves good performance. The main contributions and work are as follows: First, it is found that the backbone of the Vicuna language model in LLaVA is too redundant. When fine-tuning downstream tasks, a very small amount of data sets is difficult to affect the language model. It is replaced with a new Gemma language model, thereby achieving fast task-specific adaptation with fewer parameters and data. Second, in response to the problem of information redundancy, the MoE mixed expert model is introduced. This model can be used in combination with itself, combining the MoE mixed expert model with Gemma to reduce the amount of computation while maintaining performance. Directly training the entire model will lead to a decline in performance. A multi-stage training strategy is adopted to maintain performance. First, the MLP layer is trained for visual adaptation, then the entire Gemma model is trained to improve multimodal capabilities, and finally only the MoE layer is trained for sparsification to ensure a smooth transition from dense models to sparse models. The experiment was tested on multiple VQA datasets and achieved good performance, confirming the potential of this compact model in downstream multimodal applications.},
  archive      = {J_FCOMP},
  author       = {Han, Zhiyin and Liu, Xiaoqun and Hao, Juan},
  doi          = {10.3389/fcomp.2025.1626346},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1626346},
  shortjournal = {Front. Comput. Sci.},
  title        = {LLaVA-GM: Lightweight LLaVA multimodal architecture},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An application layer with protocol-based java smart contract verification. <em>FCOMP</em>, <em>7</em>, 1596804. (<a href='https://doi.org/10.3389/fcomp.2025.1596804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contracts are software that runs in blockchain and expresses the rules of an agreement between parties. An incorrect smart contract might allow blockchain users to violate its rules and even jeopardize its expected security. Smart contracts cannot be easily replaced to patch a bug since the nature of contracts requires them to be immutable. More problems occur when a smart contract is written in a general-purpose language, such as Java, whose executions, in a blockchain, could hang the network, break consensus or violate data encapsulation. To limit these problems, there exist automatic static analyzers that find bugs before smart contracts are installed in the blockchain. This so-called off-chain verification is optional because programmers are not forced to use it. This paper presents a general framework for the verification of smart contracts, instead, that is part of the protocol of the nodes and applies when the code of the smart contracts gets installed. It is a mandatory entry filter that bans code that does not abide by the verification rules. Consequently, such rules become part of the consensus rules of the blockchain. Therefore, an improvement in the verification protocol entails a consensus update of the network. This paper describes an implementation of a smart contracts application layer with protocol-based verification for smart contracts written in the Takamaka subset of Java, that filters only those smart contracts whose execution in blockchain is not dangerous. This application layer runs on top of a consensus engine such as Tendermint and its derivatives Ignite and CometBFT (proof of stake), or Mokamint (proof of space). This paper provides examples of actual implementations of verification rules that check if the smart contracts satisfy some constraints required by the Takamaka language. This paper shows that protocol-based verification works and reports how consensus updates are implemented. It shows actual experiments as well as limits to its use, mainly related to the fact that protocol-based verification must be fast and its complexity must never explode, or otherwise, it would compromise the performance of the blockchain network.},
  archive      = {J_FCOMP},
  author       = {Olivieri, Luca and Spoto, Fausto and Tagliaferro, Fabio},
  doi          = {10.3389/fcomp.2025.1596804},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1596804},
  shortjournal = {Front. Comput. Sci.},
  title        = {An application layer with protocol-based java smart contract verification},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud security and authentication vulnerabilities in SOAP protocol: Addressing XML-based attacks. <em>FCOMP</em>, <em>7</em>, 1595624. (<a href='https://doi.org/10.3389/fcomp.2025.1595624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis research addresses the security weaknesses in SOAP-based web services, with a particular focus on authentication vulnerabilities resulting from XML-based attacks, such as Signature Wrapping or Replay Attacks. With an emphasis on the fact that an increasing number of cloud services are utilizing SOAP, this study aims to develop a formally verified model that can more effectively identify and address these vulnerabilities.MethodWe propose and execute a TulaFale-based verification framework that formally models SOAP authentication scenarios by introducing the standard constructs, UsernameToken, Timestamp, and X.509 digital certificates. These scripts are transformed into the applied pi-calculus and verified using the ProVerif verification tool to check for properties such as authentication, confidentiality, and message integrity.ResultsBy examining XML web services security problems and consulting with security professionals, a number of key risks were identified and discussed. The research contributes to developing a comprehensive language design for cloud security and vulnerabilities using Blanchet’s ProVerif. A controlled experimental testbed was set up to emulate client–server SOAP communication streams and to evaluate the model’s effectiveness in identifying an XML-based attack performed on the web services security framework. The framework was experimentally examined for verification time and scalability for concurrency, and for accuracy of identification. The results confirmed our success in identifying attack patterns and confirming secure message exchanges built to the standards set by WS-Security.DiscussionThe proposed approach addresses and allows for the addition of automated, formal verification to realistic SOAP deployments. By modeling and verifying a security protocol before the deployment, developers can be confident that their implementation is resilient against protocol-level vulnerabilities, improving the trust in the security of web services deployed within cloud applications.},
  archive      = {J_FCOMP},
  author       = {Saeed, Mozamel M.},
  doi          = {10.3389/fcomp.2025.1595624},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1595624},
  shortjournal = {Front. Comput. Sci.},
  title        = {Cloud security and authentication vulnerabilities in SOAP protocol: Addressing XML-based attacks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cyborg synchrony: Integrating human physiology into affective generative music AI. <em>FCOMP</em>, <em>7</em>, 1593905. (<a href='https://doi.org/10.3389/fcomp.2025.1593905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) systems become increasingly integrated into human social environments, their ability to foster meaningful interaction remains an open challenge. Building on research showing that physiological synchrony relates to social bonding, we speculate about an interpersonal musical biofeedback system that promotes synchrony by allowing users to attune to each other’s physiological rhythms. We propose a two‑stage AI training framework for an interpersonal musical biofeedback system: (1) a Foundational Model trained on diverse listeners’ physiological responses to a music library, and (2) Individualized Tuning, where the system fine-tunes itself to each user’s unique physiological responses. By analyzing musical features alongside real-time physiological responses, the proposed system illustrates a feasible architecture for dynamically personalizing music to facilitate nonverbal, embodied communication between users. This approach highlights the potential for AI-driven personalization that move beyond individual optimization to actively enhance physiological synchrony which may promote deeper emotional bonding and expand the role of music as a medium for AI-mediated social connections.},
  archive      = {J_FCOMP},
  author       = {Ng, Senaida and Sargent, Kaia and Snell, Jason J.},
  doi          = {10.3389/fcomp.2025.1593905},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1593905},
  shortjournal = {Front. Comput. Sci.},
  title        = {Cyborg synchrony: Integrating human physiology into affective generative music AI},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EduScrum meets focUS: A computer-assisted training to promote self-regulation skills in higher education. <em>FCOMP</em>, <em>7</em>, 1593889. (<a href='https://doi.org/10.3389/fcomp.2025.1593889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the ever-evolving demands of the professional world, higher education plays a vital role in equipping students with strategies for self-organized and sustainable skill development. This enables students to quickly and independently adapt to new knowledge and skills throughout their careers. Therefore, it is of the essence to integrate methods that enhance self-regulation skills into our study programs, alongside the instruction of specific subject-matter expertise. Addressing these demands, we introduce a focused training that embeds the assistive software tool focUS into a structured seminar concept, leveraging eduScrum elements and accompanying learning communities. After introducing the results of a pilot evaluation with a small group of interdisciplinary doctoral students, we discuss possibilities of technical and conceptual integration in course curricula and learning counseling in higher education. Taken together, our approach indicates value for empowering future professionals across various domains to unleash their full potential.},
  archive      = {J_FCOMP},
  author       = {Schmitz-Hübsch, Alina and Bareiß, Laura and Jahn, Eva and Wirzberger, Maria},
  doi          = {10.3389/fcomp.2025.1593889},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1593889},
  shortjournal = {Front. Comput. Sci.},
  title        = {EduScrum meets focUS: A computer-assisted training to promote self-regulation skills in higher education},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research AI: Integrating AI and gamification in higher education for e-learning optimization and soft skills assessment through a cross-study synthesis. <em>FCOMP</em>, <em>7</em>, 1587040. (<a href='https://doi.org/10.3389/fcomp.2025.1587040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThe integration of Artificial Intelligence (AI) and gamification into higher education is reshaping educational practices by personalizing learning and fostering essential workforce skills. This study critically examines the effectiveness of these technologies, their impact on student engagement, and the factors influencing students’ acceptance.MethodsA systematic literature review complemented by Topic Modeling using Latent Dirichlet Allocation (LDA) identified key research themes. Subsequently, predictive modeling with machine learning algorithms, hyperparameter optimization, and Local Interpretable Model-Agnostic Explanations (LIME) were applied to classify academic documents and interpret influential factors.ResultsFindings indicate that AI effectively customizes educational pathways, enhancing engagement and academic performance. Gamification notably supports soft skill development, providing more interactive assessments than traditional approaches. However, challenges related to data privacy and technological accessibility remain significant, particularly affecting international students and institutions with limited resources.DiscussionAI and gamification demonstrate considerable potential for transforming higher education through personalized learning and interactive skill assessments. Nevertheless, widespread adoption depends on addressing data privacy concerns and ensuring technological equity. Future research should investigate the long-term implications of these technologies in developing students’ adaptability within a dynamic global workforce.},
  archive      = {J_FCOMP},
  author       = {Marengo, Agostino and Pagano, Alessandro and Lund, Brady and Santamato, Vito},
  doi          = {10.3389/fcomp.2025.1587040},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1587040},
  shortjournal = {Front. Comput. Sci.},
  title        = {Research AI: Integrating AI and gamification in higher education for e-learning optimization and soft skills assessment through a cross-study synthesis},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous ensemble learning: Modified ConvNextTiny for detecting molecular expression of breast cancer on standard biomarkers. <em>FCOMP</em>, <em>7</em>, 1569017. (<a href='https://doi.org/10.3389/fcomp.2025.1569017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the highest-ranking type of cancer, with 2.3 million new cases diagnosed each year. Immunohistochemistry (IHC) is the gold standard “examination” for determining the expression of cancer malignancies in patients with the ultimate goal of determining prognosis and therapy. Immunohistochemistry refers to the four WHO standard biomarkers: estrogen receptor, progesterone receptor, human epidermal growth factor receptor-2, and Ki-67. These biomarkers are assessed based on the quantity of cell nuclei and the intensity of brown cell membranes. Our study aims to detect the expression of breast cancer malignancy as an initial step in determining prognosis and therapy. We implemented homogeneous and heterogeneous ensemble learning models. The homogeneous ensemble learning model uses the majority vote technique to select the best performance between the Xception, ResNet50V2, InceptionResNet50V2, and ConvNextTiny models. The heterogeneous ensemble learning model takes the ConvNextTiny model as the best model. Feature engineering in ConvNextTiny combines convolution and cell-quantification features as feature fusion. ConvNextTiny, which applies feature fusion, can detect the expression of cancer malignancy. Heterogeneous ensemble learning outperforms homogeneous ensemble learning. The model performs well for accuracy, precision, recall, F1-score, and receiver operating characteristic-area under the curve (ROC-AUC) of 0.997, 0.973, 0.991, 0.982, and 0.994, respectively. These results indicate that the model can classify the malignancy expressions of breast cancer well. This model still requires the configuration of the visual laboratory device to test the real-time model capabilities.},
  archive      = {J_FCOMP},
  author       = {Intan, Indo and Karnyoto, Andrea Stevens and Harlina, Sitti and Nelwan, Berti Julian and Setiawan, Devin and Yamin, Amalia and Puspitasari, Ririn Endah},
  doi          = {10.3389/fcomp.2025.1569017},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1569017},
  shortjournal = {Front. Comput. Sci.},
  title        = {Heterogeneous ensemble learning: Modified ConvNextTiny for detecting molecular expression of breast cancer on standard biomarkers},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regional computing for VBD offloading in next-generation vehicular networks. <em>FCOMP</em>, <em>7</em>, 1564270. (<a href='https://doi.org/10.3389/fcomp.2025.1564270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of autonomous and Connected Vehicles (CVs) has led to a massive increase in Vehicular Big Data (VBD). While this data is transforming the Intelligent Transportation System (ITS), it also poses significant challenges in processing, communication, and resource scalability. Existing cloud solutions offer scalable resources; however, incur long delays and costs due to distant data communication. Conversely, edge computing reduces latency by processing data closer to the source; however, struggles to scale with the high volume and velocity of VBD. This paper introduces a novel Regional Computing (RC) paradigm for VBD offloading, with a key focus on adapting to traffic variations during peak and off-peak hours. Situated between edge and cloud layers, the RC layer enables near-source processing while maintaining higher capacity than edge or fog nodes. We propose a dynamic offloading algorithm that continuously monitors workload intensity, network utilization, and temporal traffic patterns to smartly offload tasks to the optimal tier (vehicle, regional, or cloud). This strategy ensures responsiveness across fluctuating conditions while minimizing delay, congestion, and energy consumption. To validate the proposed architecture, we develop a custom Python-based simulator, RegionalEdgeSimPy, specifically designed for VBD scenarios. Simulation results demonstrate that the proposed framework significantly reduces processing latency, energy usage, and operational costs compared to traditional models, offering a scalable and effective alternative for next-generation vehicular networks.},
  archive      = {J_FCOMP},
  author       = {Badshah, Afzal and Alsahfi, Tariq and Alesawi, Sami and Alfakeeh, Ahmed and Bukhari, Amal and Daud, Ali},
  doi          = {10.3389/fcomp.2025.1564270},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1564270},
  shortjournal = {Front. Comput. Sci.},
  title        = {Regional computing for VBD offloading in next-generation vehicular networks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on performance comparisons of different types of DevOps team formations. <em>FCOMP</em>, <em>7</em>, 1554299. (<a href='https://doi.org/10.3389/fcomp.2025.1554299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionDespite all the efforts to successfully implement DevOps practices, principles, and cultural change, there is still a lack of understanding on how DevOps team structure formation and performance differences are related. The lack of a ground truth for DevOps team structure formation and performance has become a persistent and relevant problem for companies and researchers.MethodsIn this study, we propose a framework for DevOps team Formation–Performance and conduct a survey to examine the relationships between team formations and performance with the five metrics we identified, two of which are novel. We conducted an empirical study using a survey to gather data. We employed targeted outreach on a social media platform along via a snowball sampling and sent 380 messages to DevOps professionals worldwide. This approach resulted in 122 positive responses and 105 completed surveys, achieving a 69.7% response rate from those who agreed to participate.ResultsThe research shows that implementing the DevOps methodology enhances team efficiency across various team structures, with the sole exception of “Separate Development and Operation teams with limited collaboration”. Moreover, the study reveals that all teams experienced improvements in Repair/Recovery performance metric following DevOps adoption. Notably, the “Separate Development and Operation teams with high collaboration” formation emerged as the top performer in the key metrics, including Deployment Frequency, Number of Incidents, and Number of Failures/Service Interruptions. The analysis further indicates that different DevOps organizational formations do not significantly impact Lead Time, Repair/Recovery, and Number of Failures/Service Interruptions in terms of goal achievement. However, a statistically significant disparity was observed between “Separate Development and Operation teams with high collaboration” and “A single team formation” regarding the Deployment Frequency goal achievement percentage.DiscussionThe analysis confirms that DevOps adoption improves performance across most team formations, with the exception of “Separate Development and Operation teams with limited collaboration” (TeamType1), which shows significant improvement only in Mean Time to Recovery (MTTR). Standardized effect size calculations (Cohen’s d) reveal that TeamType2 (“Separate Development and Operation teams with high collaboration”) consistently achieves large effects in Deployment Frequency (DF), Number of Incidents (NoI), and Number of Failures/Service Interruptions (NoF/NoSI), while TeamType3 shows strong results for Lead Time (LT) and NoF/NoSI. MTTR improvements are large across all formations, with TeamType4 performing best in this metric. These findings suggest that collaboration intensity is a critical determinant of performance gains. While team formation type does not significantly influence LT, MTTR, or NoF/NoSI goal achievement, DF goal achievement is significantly higher for TeamType2 compared to TeamType4, highlighting the potential competitive advantage of high-collaboration structures.},
  archive      = {J_FCOMP},
  author       = {Korkmaz, Halil Ergun and Aydin, Mehmet Nafiz},
  doi          = {10.3389/fcomp.2025.1554299},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1554299},
  shortjournal = {Front. Comput. Sci.},
  title        = {An empirical study on performance comparisons of different types of DevOps team formations},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring operator responses to augmented reality training: Insights from the SELFEX platform case study. <em>FCOMP</em>, <em>7</em>, 1507439. (<a href='https://doi.org/10.3389/fcomp.2025.1507439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional industrial training methods often fail to capture the tacit expertise of experienced personnel, limiting instructional quality for new staff. This study examines the SELFEX platform, an augmented reality (AR)–based training system enabling junior operators to learn autonomously by replicating recorded performances of senior operators. Using a mixed-methods design, the research combined a technical analysis of AR’s functionality, benefits, and constraints with an empirical evaluation in an industrial setting. Seventeen participants completed training tasks using either conventional screens or AR headsets, with subjective measures including satisfaction, perceived usefulness, ease of use, and flow state, alongside objective performance metrics. Results showed that AR training was particularly beneficial for novices, enhancing engagement, understanding, and perceived ease of learning, though no statistically significant performance differences with screen-based training were found. Correlation analyses revealed strong links between flow, satisfaction, and ease of learning, highlighting the importance of intuitive, well-integrated design. Challenges in integrating AR into professional workflows—such as technical stability and user adoption—were also identified. These findings position AR as a promising tool for accessible and immersive industrial training, capable of supporting both initial skill acquisition and potential future upskilling. Further longitudinal studies are recommended to evaluate long-term impacts on performance, retention, and cost-effectiveness, and to refine system usability for diverse user profiles.},
  archive      = {J_FCOMP},
  author       = {Escallada, Oscar and Lasa, Ganix and Mazmela, Maitane and La Carrubba, Dario and Bosani, Enrica and Dacal-Nieto, Angel and García, Marcos Villar},
  doi          = {10.3389/fcomp.2025.1507439},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1507439},
  shortjournal = {Front. Comput. Sci.},
  title        = {Exploring operator responses to augmented reality training: Insights from the SELFEX platform case study},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cyberbullying: A comparative analysis between the results of a scoping study and a questionnaire applied to students. <em>FCOMP</em>, <em>7</em>, 1506046. (<a href='https://doi.org/10.3389/fcomp.2025.1506046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a scoping study using the Scopus Database to analyze literature on cyberbullying and students’ perceptions. Using the keywords ‘cyberbullying’, ‘students’, and ‘perceptions’, we narrowed down 6,271 initial articles to 14 that met our inclusion criteria. Additionally, we conducted a questionnaire survey with 193 Portuguese students aged between 10 and 19 to understand their perceptions of cyberbullying. Our analysis revealed cyberbullying as a growing concern with significant negative impacts on students’ mental and emotional wellbeing. The correlation between our questionnaire results and the scoping study findings emphasizes the urgent need for comprehensive intervention strategies. Our research indicates that effective cyberbullying prevention requires a multi-faceted approach including: development of social and emotional skills among students; promotion of appropriate technology use beyond technical literacy; targeted teacher training programs; establishment of clear intervention protocols within schools; empowerment of cyber-observers as active prevention agents; and recognition that cyberbullying often functions as an extension of face-to-face aggression rather than anonymous attacks. This study brings into focus the critical importance of fostering digital citizenship within educational settings, with teachers and school administrators playing pivotal roles in creating safe digital environments. The findings underscore how properly structured educational interventions can significantly increase reporting rates and decrease cyberbullying incidents, thereby promoting students’ overall wellbeing in the digital age.},
  archive      = {J_FCOMP},
  author       = {Coutinho, Luís and Lencastre, José Alberto and Tomás, Ana Maria},
  doi          = {10.3389/fcomp.2025.1506046},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1506046},
  shortjournal = {Front. Comput. Sci.},
  title        = {Cyberbullying: A comparative analysis between the results of a scoping study and a questionnaire applied to students},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
