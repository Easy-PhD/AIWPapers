<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMODS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simods">SIMODS - 5</h2>
<ul>
<li><details>
<summary>
(2025). Complete and continuous invariants of 1-periodic sequences in polynomial time. <em>SIMODS</em>, <em>7</em>(4), 1643-1663. (<a href='https://doi.org/10.1137/25M1733574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Inevitable noise in real measurements motivates the challenging problem of continuously quantifying the similarity between rigid objects such as periodic time series and 1-dimensional materials considered under isometry maintaining interpoint distances. The past work developed many Hausdorff-like distances, which have slow or approximate algorithms due to minimizations over infinitely many isometries. For all finite and 1-periodic sequences under isometry and rigid motion in any high-dimensional Euclidean space, we introduce complete invariants and Lipschitz continuous metrics whose time complexities are polynomial in both input size and ambient dimension. The key novelty in the periodic case is the Lipschitz continuity under perturbations that discontinuously change a minimum period. The proven continuity is practically important for maintaining scientific integrity by real-time detection of near-duplicate structures in experimental and simulated materials datasets.},
  archive      = {J_SIMODS},
  author       = {Vitaliy A. Kurlin},
  doi          = {10.1137/25M1733574},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1643-1663},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Complete and continuous invariants of 1-periodic sequences in polynomial time},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The pontryagin maximum principle for training convolutional neural networks. <em>SIMODS</em>, <em>7</em>(4), 1616-1642. (<a href='https://doi.org/10.1137/24M1675369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A novel batch sequential quadratic Hamiltonian (bSQH) algorithm for training convolutional neural networks (CNNs) with -based regularization is presented. This methodology is based on a discrete-time Pontryagin maximum principle (PMP). It uses forward and backward sweeps together with the layerwise approximate maximization of an augmented Hamiltonian function, where the augmentation parameter is chosen adaptively. A technique for determining this augmentation parameter is proposed, and the loss-reduction and convergence properties of the bSQH algorithm are analyzed theoretically and validated numerically. Results of numerical experiments in the context of image classification with a sparsity-enforcing, -based regularizer demonstrate the effectiveness of the proposed method in full-batch and mini-batch modes.},
  archive      = {J_SIMODS},
  author       = {S. Hofmann and A. Borzì},
  doi          = {10.1137/24M1675369},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1616-1642},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The pontryagin maximum principle for training convolutional neural networks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear meta-learning can guarantee faster rates. <em>SIMODS</em>, <em>7</em>(4), 1594-1615. (<a href='https://doi.org/10.1137/24M1662977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Many recent theoretical works on meta-learning aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. The main aim of theoretical guarantees on the subject is to establish the extent to which convergence rates—in learning a common representation—may scale with the number of tasks (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks and task-specific regression functions are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonlinearity maps to an infinite dimensional reproducing kernel Hilbert space, we show that additional biases can be mitigated with careful regularization that leverages the smoothness of task-specific regression functions, yielding improved rates that scale with the number of tasks as desired.},
  archive      = {J_SIMODS},
  author       = {Dimitri Meunier and Zhu Li and Arthur Gretton and Samory Kpotufe},
  doi          = {10.1137/24M1662977},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1594-1615},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Nonlinear meta-learning can guarantee faster rates},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On neural network approximation of ideal adversarial attack and convergence of adversarial training. <em>SIMODS</em>, <em>7</em>(4), 1568-1593. (<a href='https://doi.org/10.1137/23M1590512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Adversarial attacks are usually expressed in terms of a gradient-based operation on the input data and model; this results in heavy computations every time an attack is generated. Recent empirical works exhibit attacks that can be approximated by neural networks. This work provides a general theoretical framework to represent adversarial attacks as a trainable function without further gradient computation. We first motivate that the theoretical best attacks, under proper conditions, can be represented as smooth piecewise functions (piecewise Hölder functions). Then we obtain an approximation result of such functions by a neural network. Subsequently, we emulate the ideal attack process by a neural network and reduce the adversarial training to a mathematical game between an attack network and a training model (a defense network). We also obtain convergence rates of adversarial loss in terms of the sample size for adversarial training in such a setting.},
  archive      = {J_SIMODS},
  author       = {Rajdeep Haldar and Qifan Song},
  doi          = {10.1137/23M1590512},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1568-1593},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On neural network approximation of ideal adversarial attack and convergence of adversarial training},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative modeling of lévy area for high order SDE simulation. <em>SIMODS</em>, <em>7</em>(4), 1541-1567. (<a href='https://doi.org/10.1137/23M161077X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. It is well known that when numerically simulating solutions to stochastic differential equations (SDEs), achieving a strong convergence rate better than (where is the step-size) usually requires the use of certain iterated integrals of Brownian motion, commonly referred to as its “Lévy areas,” However, these stochastic integrals are difficult to simulate due to their non-Gaussian nature. and for a -dimensional Brownian motion with , no fast almost-exact sampling algorithm is known. In this paper, we propose LévyGAN, a deep-learning-based model for generating approximate samples of Lévy area conditional on a Brownian increment. Due to our “bridge-flipping” operation, the output samples match all joint and conditional odd moments exactly. Our generator employs a tailored graph neural network (GNN)-inspired architecture, which enforces the correct dependency structure between the output distribution and the conditioning variable. Furthermore, we incorporate a mathematically principled characteristic-function-based discriminator. Lastly, we introduce a novel training mechanism, termed “Chen-training,” which circumvents the need for expensive-to-generate training data-sets. This new training procedure is underpinned by our two main theoretical results. For four-dimensional Brownian motion, we show that LévyGAN exhibits state-of-the-art performance across several metrics which measure both the joint and marginal distributions. We conclude with a numerical experiment on the log-Heston model, a popular SDE in mathematical finance, demonstrating that a high-quality synthetic Lévy area can lead to high order weak convergence and variance reduction when using multilevel Monte Carlo (MLMC).},
  archive      = {J_SIMODS},
  author       = {Andraž Jelinčič and Jiajie Tao and William F. Turner and Thomas Cass and James Foster and Hao Ni},
  doi          = {10.1137/23M161077X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1541-1567},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Generative modeling of lévy area for high order SDE simulation},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
