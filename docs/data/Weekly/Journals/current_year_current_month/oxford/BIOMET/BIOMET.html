<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMET</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomet">BIOMET - 69</h2>
<ul>
<li><details>
<summary>
(2025). Identifying and bounding the probability of necessity for causes of effects with ordinal outcomes. <em>BIOMET</em>, <em>112</em>(3), asaf049. (<a href='https://doi.org/10.1093/biomet/asaf049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the existing causal inference literature focuses on the forward-looking perspective by estimating effects of causes, the backward-looking perspective can provide insights into causes of effects. In backward-looking causal inference, the probability of necessity measures the probability that a certain event is caused by the treatment, given the observed treatment and outcome. Most existing results focus on binary outcomes. Motivated by applications with ordinal outcomes, we propose a general definition of the probability of necessity. However, identifying the probability of necessity is challenging because it involves the joint distribution of the potential outcomes. We propose the novel assumption of a monotonic incremental treatment effect to identify the probability of necessity with ordinal outcomes. We also discuss the testable implications of this key identification assumption. When it fails, we derive explicit formulas of the sharp large-sample bounds on the probability of necessity.},
  archive      = {J_BIOMET},
  author       = {Zhang, Chao and Geng, Zhi and Li, Wei and Ding, Peng},
  doi          = {10.1093/biomet/asaf049},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf049},
  shortjournal = {Biometrika},
  title        = {Identifying and bounding the probability of necessity for causes of effects with ordinal outcomes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent and scalable composite likelihood estimation of probit models with crossed random effects. <em>BIOMET</em>, <em>112</em>(3), asaf037. (<a href='https://doi.org/10.1093/biomet/asaf037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of crossed random effects models commonly incurs computational costs that grow faster than linearly in the sample size N ⁠ , often as fast as Ω ( N 3 / 2 ) ⁠ , making them unsuitable for large datasets. For non-Gaussian responses, integrating out the random effects to obtain a marginal likelihood poses significant challenges, especially for high-dimensional integrals for which the Laplace approximation may not be accurate. In this article we develop a composite likelihood approach to probit models that replaces the crossed random effects model with some hierarchical models that require only one-dimensional integrals. We show how to consistently estimate the crossed effects model parameters from the hierarchical model fits. We find that the computation scales linearly in the sample size. The method is illustrated by applying it to approximately five million observations from Stitch Fix, where the crossed effects formulation would require an integral of dimension larger than 700 000 ⁠ .},
  archive      = {J_BIOMET},
  author       = {Bellio, R and Ghosh, S and Owen, A B and Varin, C},
  doi          = {10.1093/biomet/asaf037},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf037},
  shortjournal = {Biometrika},
  title        = {Consistent and scalable composite likelihood estimation of probit models with crossed random effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive performance of power posteriors. <em>BIOMET</em>, <em>112</em>(3), asaf034. (<a href='https://doi.org/10.1093/biomet/asaf034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the impact of using tempered likelihoods in the production of posterior predictions. While the choice of temperature has an impact on predictive performance in small samples, we formally show that in moderate-to-large samples, tempering does not impact posterior predictions.},
  archive      = {J_BIOMET},
  author       = {McLatchie, Y and Fong, E and Frazier, D T and Knoblauch, J},
  doi          = {10.1093/biomet/asaf034},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf034},
  shortjournal = {Biometrika},
  title        = {Predictive performance of power posteriors},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bias correction of quadratic spectral estimators. <em>BIOMET</em>, <em>112</em>(3), asaf033. (<a href='https://doi.org/10.1093/biomet/asaf033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three cardinal, statistically consistent families of nonparametric estimators for the power spectral density of a time series are the lag-window, multitaper and Welch estimators. However, when estimating power spectral densities from a finite sample, each can be subject to nonignorable bias. Astfalck et al. (2024) developed a method that offers significant bias reduction for finite samples for Welch’s estimator, which this article extends to the larger family of quadratic estimators, thus providing similar theory for bias correction of lag-window and multitaper estimators as well as combinations thereof. Importantly, this theory may be used in conjunction with any and all tapers and lag-sequences designed for bias reduction, and so should be seen as an extension to valuable work in these fields, rather than a supplanting methodology. The order of computation is larger than the |$ {O}(n\log n) $| which is typical in spectral analyses, but it is not insurmountable in practice. Simulation studies support the theory with comparisons across variations of quadratic estimators.},
  archive      = {J_BIOMET},
  author       = {Astfalck, Lachlan C and Sykulski, Adam M and Cripps, Edward J},
  doi          = {10.1093/biomet/asaf033},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf033},
  shortjournal = {Biometrika},
  title        = {Bias correction of quadratic spectral estimators},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integer programming for learning directed acyclic graphs from nonidentifiable gaussian models. <em>BIOMET</em>, <em>112</em>(3), asaf032. (<a href='https://doi.org/10.1093/biomet/asaf032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of learning directed acyclic graphs from continuous observational data, generated according to a linear Gaussian structural equation model. State-of-the-art structure learning methods for this setting have at least one of the following shortcomings: (i) they cannot provide optimality guarantees and can suffer from learning suboptimal models; (ii) they rely on the stringent assumption that the noise is homoscedastic, and hence the underlying model is fully identifiable. We overcome these shortcomings and develop a computationally efficient mixed-integer programming framework for learning medium-sized problems that accounts for arbitrary heteroscedastic noise. We present an early stopping criterion under which we can terminate the branch-and-bound procedure to achieve an asymptotically optimal solution and establish the consistency of this approximate solution. In addition, we show via numerical experiments that our method outperforms state-of-the-art algorithms and is robust to noise heteroscedasticity, whereas the performance of some competing methods deteriorates under strong violations of the identifiability assumption. The software implementation of our method is available as the Python package micodag .},
  archive      = {J_BIOMET},
  author       = {Xu, Tong and Taeb, Armeen and Küçükyavuz, Simge and Shojaie, Ali},
  doi          = {10.1093/biomet/asaf032},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf032},
  shortjournal = {Biometrika},
  title        = {Integer programming for learning directed acyclic graphs from nonidentifiable gaussian models},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general form of covariate adjustment in clinical trials under covariate-adaptive randomization. <em>BIOMET</em>, <em>112</em>(3), asaf029. (<a href='https://doi.org/10.1093/biomet/asaf029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized clinical trials, adjusting for baseline covariates can improve credibility and efficiency for demonstrating and quantifying treatment effects. This article studies the augmented inverse propensity weighted estimator, which is a general form of covariate adjustment that uses linear, generalized linear and nonparametric or machine learning models for the conditional mean of the response given covariates. Under covariate-adaptive randomization, we establish general theorems that show a complete picture of the asymptotic normality, efficiency gain and applicability of augmented inverse propensity weighted estimators. In particular, we provide for the first time a rigorous theoretical justification of using machine learning methods with cross-fitting for dependent data under covariate-adaptive randomization. Based on the general theorems, we offer insights on the conditions for guaranteed efficiency gain and universal applicability under different randomization schemes, which also motivate a joint calibration strategy using some constructed covariates after applying augmented inverse propensity weighted estimators.},
  archive      = {J_BIOMET},
  author       = {Bannick, Marlena S and Shao, Jun and Liu, Jingyi and Du, Yu and Yi, Yanyao and Ye, Ting},
  doi          = {10.1093/biomet/asaf029},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf029},
  shortjournal = {Biometrika},
  title        = {A general form of covariate adjustment in clinical trials under covariate-adaptive randomization},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic factor analysis of high-dimensional recurrent events. <em>BIOMET</em>, <em>112</em>(3), asaf028. (<a href='https://doi.org/10.1093/biomet/asaf028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event time data arise in many studies, including in biomedicine, public health, marketing and social media analysis. High-dimensional recurrent event data involving many event types and observations have become prevalent with advances in information technology. This article proposes a semiparametric dynamic factor model for the dimension reduction of high-dimensional recurrent event data. The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies. A nearly rate-optimal smoothing-based estimator is proposed. An information criterion that consistently selects the number of factors is also developed. Simulation studies demonstrate the effectiveness of these inference tools. The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained.},
  archive      = {J_BIOMET},
  author       = {Chen, F and Chen, Y and Ying, Z and Zhou, K},
  doi          = {10.1093/biomet/asaf028},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf028},
  shortjournal = {Biometrika},
  title        = {Dynamic factor analysis of high-dimensional recurrent events},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving efficiency in transporting average treatment effects. <em>BIOMET</em>, <em>112</em>(3), asaf027. (<a href='https://doi.org/10.1093/biomet/asaf027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop flexible, semiparametric estimators of the average treatment effect transported to a new target population, which offer potential efficiency gains. Transport may be of value when the average treatment effect may differ across populations. We consider the setting where differences in the average treatment effect are due to differences in the distribution of effect modifiers, baseline covariates that modify the treatment effect. First, we propose a collaborative one-step semiparametric estimator that can improve efficiency. This approach does not require researchers to have knowledge about which covariates are effect modifiers and which differ in distribution between the populations, but it does require all covariates to be measured in the target population. Second, we propose two one-step semiparametric estimators that assume knowledge of which covariates are effect modifiers and which are both effect modifiers and differentially distributed between the populations. These estimators can be used even when not all covariates are observed in the target population; one estimator requires that only effect modifiers be observed, and the other requires that only those modifiers that are also differentially distributed be observed. We use simulations to compare the finite-sample performance of our proposed estimators and an existing semiparametric estimator of the transported average treatment effect, including in the presence of practical violations of the positivity assumption. Lastly, we apply our proposed estimators to a large-scale housing trial.},
  archive      = {J_BIOMET},
  author       = {Rudolph, K E and Williams, N T and Stuart, E A and Díaz, I},
  doi          = {10.1093/biomet/asaf027},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf027},
  shortjournal = {Biometrika},
  title        = {Improving efficiency in transporting average treatment effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general condition for bias attenuation by a nondifferentially mismeasured confounder. <em>BIOMET</em>, <em>112</em>(3), asaf026. (<a href='https://doi.org/10.1093/biomet/asaf026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world studies, the collected confounders may suffer from measurement error. Although mismeasurement of confounders is typically unintentional (originating from sources such as human oversight or imprecise machinery), deliberate mismeasurement also occurs and is becoming increasingly more common. For example, in the 2020 U.S. census, noise was added to measurements to assuage privacy concerns. Sensitive variables such as income or age are often partially censored and are only known up to a range of values. In such settings, obtaining valid estimates of the causal effect of a binary treatment can be impossible, as mismeasurement of confounders constitutes a violation of the no-unmeasured-confounding assumption. A natural question is whether the common practice of simply adjusting for the mismeasured confounder is justifiable. In this article, we answer this question in the affirmative and demonstrate that in many realistic scenarios not covered by previous literature, adjusting for the mismeasured confounders reduces bias compared to not adjusting.},
  archive      = {J_BIOMET},
  author       = {Zhang, Jeffrey and Lee, Junu},
  doi          = {10.1093/biomet/asaf026},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf026},
  shortjournal = {Biometrika},
  title        = {A general condition for bias attenuation by a nondifferentially mismeasured confounder},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: ‘Network cross-validation by edge sampling’. <em>BIOMET</em>, <em>112</em>(3), asaf023. (<a href='https://doi.org/10.1093/biomet/asaf023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asaf023},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf023},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Network cross-validation by edge sampling’},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning for piecewise-constant mean estimation: Optimality, ℓ1 and ℓ0 penalization. <em>BIOMET</em>, <em>112</em>(3), asaf018. (<a href='https://doi.org/10.1093/biomet/asaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study transfer learning for estimating piecewise-constant signals when source data, which may be relevant but disparate, are available in addition to target data. We first investigate transfer learning estimators that respectively employ ℓ 1 and ℓ 0 penalties for unisource data scenarios and then generalize these estimators to accommodate multisources. To further reduce estimation errors, especially when some sources significantly differ from the target, we introduce an informative source selection algorithm. We then examine these estimators with multisource selection and establish their minimax optimality. Unlike the common narrative in the transfer learning literature that the performance is enhanced through large source sample sizes, our approaches leverage higher observational frequencies and accommodate diverse frequencies across multiple sources. Our extensive numerical experiments show that the proposed transfer learning estimators significantly improve estimation performance compared to estimators that only use the target data.},
  archive      = {J_BIOMET},
  author       = {Wang, F and Yu, Y},
  doi          = {10.1093/biomet/asaf018},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf018},
  shortjournal = {Biometrika},
  title        = {Transfer learning for piecewise-constant mean estimation: Optimality, ℓ1 and ℓ0 penalization},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spike-and-slab prior for dimension selection in generalized linear network eigenmodels. <em>BIOMET</em>, <em>112</em>(3), asaf014. (<a href='https://doi.org/10.1093/biomet/asaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent space models are often used to model network data by embedding a network’s nodes into a low-dimensional latent space; however, choosing the dimension of this space remains a challenge. To this end, we begin by formalizing a class of latent space models we call generalized linear network eigenmodels that can model various edge types (binary, ordinal, nonnegative continuous) found in scientific applications. This model class subsumes the traditional eigenmodel by embedding it in a generalized linear model with an exponential dispersion family random component and fixes identifiability issues that hindered interpretability. We propose a Bayesian approach to dimension selection for generalized linear network eigenmodels based on an ordered spike-and-slab prior that provides improved dimension estimation and satisfies several appealing theoretical properties. We show that the model’s posterior is consistent and concentrates on low-dimensional models near the truth. We demonstrate our approach’s consistent dimension selection on simulated networks, and we use generalized linear network eigenmodels to study the effect of covariates on the formation of networks from biology, ecology and economics and the existence of residual latent structure.},
  archive      = {J_BIOMET},
  author       = {Loyal, Joshua D and Chen, Yuguo},
  doi          = {10.1093/biomet/asaf014},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf014},
  shortjournal = {Biometrika},
  title        = {A spike-and-slab prior for dimension selection in generalized linear network eigenmodels},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling directed networks with reciprocity. <em>BIOMET</em>, <em>112</em>(2), asaf035. (<a href='https://doi.org/10.1093/biomet/asaf035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetric relational data are becoming increasingly prevalent in diverse fields, underscoring the need for developing directed network models to address the complex challenges posed by the unique structure of such data. Unlike undirected models, directed models can capture reciprocity, the tendency of nodes to form mutual links. This work addresses a fundamental question: what is the effective sample size for modelling reciprocity? We examine this question by analysing the Bernoulli model with reciprocity, allowing for varying sparsity levels between non-reciprocal and reciprocal effects. We then extend this framework to a model that incorporates node-specific heterogeneity and link-specific reciprocity using covariates. Our findings reveal the intriguing interplay between non-reciprocal and reciprocal effects in sparse networks. We propose a straightforward inference procedure based on maximum likelihood estimation that operates without prior knowledge of sparsity levels, whether covariates are included or not.},
  archive      = {J_BIOMET},
  author       = {Feng, Rui and Leng, Chenlei},
  doi          = {10.1093/biomet/asaf035},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf035},
  shortjournal = {Biometrika},
  title        = {Modelling directed networks with reciprocity},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact sampling of spanning trees via fast-forwarded random walks. <em>BIOMET</em>, <em>112</em>(2), asaf031. (<a href='https://doi.org/10.1093/biomet/asaf031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree graphs are used routinely in statistics. When estimating a Bayesian model with a tree component, sampling the posterior remains a core difficulty. Existing Markov chain Monte Carlo methods tend to rely on local moves, often leading to poor mixing. A promising approach is to instead directly sample spanning trees on an auxiliary graph. Current spanning tree samplers, such as the celebrated Aldous–Broder algorithm, rely predominantly on simulating random walks that are required to visit all the nodes of the graph. Such algorithms are prone to getting stuck in certain subgraphs. We formalize this phenomenon using the bottlenecks in the random walk’s transition probability matrix. We then propose a novel fast-forwarded cover algorithm that can break free from bottlenecks. The core idea is a marginalization argument that leads to a closed-form expression that allows for fast-forwarding to the event of visiting a new node. Unlike many existing approximation algorithms, our algorithm yields exact samples. We demonstrate the enhanced efficiency of the fast-forwarded cover algorithm, and illustrate its application in fitting a Bayesian dendrogram model on a Massachusetts crime and community dataset.},
  archive      = {J_BIOMET},
  author       = {Tam, Edric and Dunson, David B and Duan, Leo L},
  doi          = {10.1093/biomet/asaf031},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf031},
  shortjournal = {Biometrika},
  title        = {Exact sampling of spanning trees via fast-forwarded random walks},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the minimax robustness against correlation and heteroscedasticity of ordinary least squares among generalized least squares estimates of regression. <em>BIOMET</em>, <em>112</em>(2), asaf025. (<a href='https://doi.org/10.1093/biomet/asaf025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit a result according to which certain functions of covariance matrices are maximized at scalar multiples of the identity matrix. In a statistical context in which such functions measure loss, this says that the least favourable form of dependence is in fact independence, so that a procedure optimal for independent and identically distributed data can be minimax. In particular, the ordinary least squares estimate of a correctly specified regression response is minimax among generalized least squares estimates, when the maximum is taken over certain classes of error covariance structures and the loss function possesses a natural monotonicity property. In regression models whose response function is possibly misspecified, ordinary least squares is minimax if the design is uniform on its support, but this often fails otherwise. An investigation of the interplay between minimax generalized least squares procedures and minimax designs leads us to extend, to robustness against dependencies, an existing observation: that robustness against model misspecifications is increased by splitting replicates into clusters of observations at nearby locations.},
  archive      = {J_BIOMET},
  author       = {Wiens, Douglas P},
  doi          = {10.1093/biomet/asaf025},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf025},
  shortjournal = {Biometrika},
  title        = {On the minimax robustness against correlation and heteroscedasticity of ordinary least squares among generalized least squares estimates of regression},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric data segmentation in multivariate time series via joint characteristic functions. <em>BIOMET</em>, <em>112</em>(2), asaf024. (<a href='https://doi.org/10.1093/biomet/asaf024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern time series data often exhibit complex dependence and structural changes that are not easily characterized by shifts in the mean or model parameters. We propose a nonparametric data segmentation methodology for multivariate time series. By considering joint characteristic functions between the time series and its lagged values, our proposed method is able to detect changepoints in the marginal distribution, but also those in possibly nonlinear serial dependence, all without the need to prespecify the type of changes. We show the theoretical consistency of our method in estimating the total number and the locations of the changepoints, and demonstrate its good performance against a variety of changepoint scenarios. We further demonstrate its usefulness in applications to seismology and economic time series.},
  archive      = {J_BIOMET},
  author       = {McGonigle, E T and Cho, H},
  doi          = {10.1093/biomet/asaf024},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf024},
  shortjournal = {Biometrika},
  title        = {Nonparametric data segmentation in multivariate time series via joint characteristic functions},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian inference for generalized linear models via quasi-posteriors. <em>BIOMET</em>, <em>112</em>(2), asaf022. (<a href='https://doi.org/10.1093/biomet/asaf022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized linear models are routinely used for modelling relationships between a response variable and a set of covariates. The simple form of a generalized linear model comes with easy interpretability, but also leads to concerns about model misspecification impacting inferential conclusions. A popular semiparametric solution adopted in the frequentist literature is quasilikelihood, which improves robustness by only requiring correct specification of the first two moments. We develop a robust approach to Bayesian inference in generalized linear models through quasi-posterior distributions. We show that quasi-posteriors provide a coherent generalized Bayes inference method, while also approximating so-called coarsened posteriors. In so doing, we obtain new insights into the choice of coarsening parameter. Asymptotically, the quasi-posterior converges in total variation to a normal distribution and has important connections with the loss-likelihood bootstrap posterior. We demonstrate that it is also well calibrated in terms of frequentist coverage. Moreover, the loss-scale parameter has a clear interpretation as a dispersion, and this leads to a consolidated method-of-moments estimator.},
  archive      = {J_BIOMET},
  author       = {Agnoletto, D and Rigon, T and Dunson, D B},
  doi          = {10.1093/biomet/asaf022},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf022},
  shortjournal = {Biometrika},
  title        = {Bayesian inference for generalized linear models via quasi-posteriors},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved estimator of the pair correlation function of a spatial point process. <em>BIOMET</em>, <em>112</em>(2), asaf021. (<a href='https://doi.org/10.1093/biomet/asaf021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pair correlation function, or two-point correlation, of a spatial point process is a fundamental tool in spatial statistics and astrostatistics, measuring the strength of spatial dependence between points. Interest is focused on the behaviour of this function at short distances, but this is the region in which existing estimators can be particularly unreliable. We propose a new estimator of the pair correlation function based on techniques from stochastic geometry and kernel density estimation. Theory and simulation experiments confirm that the new estimator is far superior to existing estimators, especially at short distances, when the underlying point process is clustered or completely spatially random. Extensions of the estimator are developed for inhomogeneous point processes, for spatially inhibited (negatively correlated) processes and for cases where the form of the pair correlation function is known approximately. We address practical issues including boundary correction, bandwidth selection and data-based choice of technique. Real data examples, of shelling in Ukraine and meningococcal disease in Germany, demonstrate that the new estimator has substantial impact on the interpretation of data.},
  archive      = {J_BIOMET},
  author       = {Baddeley, A and Davies, T M and Hazelton, M L},
  doi          = {10.1093/biomet/asaf021},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf021},
  shortjournal = {Biometrika},
  title        = {An improved estimator of the pair correlation function of a spatial point process},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The only admissible way of merging arbitrary e-values. <em>BIOMET</em>, <em>112</em>(2), asaf020. (<a href='https://doi.org/10.1093/biomet/asaf020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper it is proved that the only admissible way of merging arbitrary |$ e $| -values is to use a weighted arithmetic average. This result completes the picture of merging methods for arbitrary |$ e $| -values and generalizes the result of Vovk & Wang (2021) that the only admissible way of symmetrically merging |$ e $| -values is to use the arithmetic average combined with a constant. Although the proved statement is naturally anticipated, its proof relies on a sophisticated application of optimal transport duality and a minimax theorem.},
  archive      = {J_BIOMET},
  author       = {Wang, Ruodu},
  doi          = {10.1093/biomet/asaf020},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf020},
  shortjournal = {Biometrika},
  title        = {The only admissible way of merging arbitrary e-values},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the fundamental limitations of multi-proposal markov chain monte carlo algorithms. <em>BIOMET</em>, <em>112</em>(2), asaf019. (<a href='https://doi.org/10.1093/biomet/asaf019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study multi-proposal Markov chain Monte Carlo algorithms, such as multiple-try or generalized Metropolis–Hastings schemes, which have recently received renewed attention due to their amenability to parallel computing. First, we prove that no multi-proposal scheme can speed up convergence relative to the corresponding single-proposal scheme by more than a factor of |$ K $|⁠ , where |$ K $| denotes the number of proposals at each iteration. This result applies to arbitrary target distributions and it implies that common serial multi-proposal implementations are less efficient than single-proposal ones. Second, we consider log-concave distributions over Euclidean spaces, proving that, in this case, the speed-up is at most logarithmic in |$ K $|⁠ , which implies that even parallel multi-proposal implementations are fundamentally limited in the computational gain they can offer. Crucially, our results apply to arbitrary multi-proposal schemes and purely rely on the two-step structure of the associated kernels: first generate |$ K $| candidate points, then select one among those. Our theoretical findings are validated through numerical simulations.},
  archive      = {J_BIOMET},
  author       = {Pozza, F and Zanella, G},
  doi          = {10.1093/biomet/asaf019},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf019},
  shortjournal = {Biometrika},
  title        = {On the fundamental limitations of multi-proposal markov chain monte carlo algorithms},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample splitting and assessing goodness-of-fit of time series. <em>BIOMET</em>, <em>112</em>(2), asaf017. (<a href='https://doi.org/10.1093/biomet/asaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental and often final step in time series modelling is to assess the quality of fit of a proposed model to the data. Since the underlying distribution of the innovations that generate a model is often not prescribed, goodness-of-fit tests typically take the form of testing the fitted residuals for serial independence. However, these fitted residuals are intrinsically dependent since they are based on the same parameter estimates, and thus standard tests of serial independence, such as those based on the autocorrelation function or auto-distance correlation function of the fitted residuals, need to be adjusted. The sample-splitting procedure of Pfister et al. (2018) is one such fix for the case of models for independent data, but fails to work in the dependent setting. In this article, sample splitting is leveraged in the time series setting to perform tests of serial dependence of fitted residuals using the autocorrelation function and auto-distance correlation function. The first f n of the data points are used to estimate the parameters of the model and then, using these parameter estimates, the last l n of the data points are used to compute the estimated residuals. Tests for serial independence are then based on these l n residuals. As long as the overlap between the f n and l n data splits is asymptotically 1 / 2 ⁠ , the autocorrelation function and auto-distance correlation function tests of serial independence often have the same limit distributions as when the underlying residuals are indeed independent and identically distributed. In particular, if the first half of the data is used to estimate the parameters and the estimated residuals are computed for the entire dataset based on these parameter estimates, then the autocorrelation function and auto-distance correlation function can have the same limit distributions as if the residuals were independent and identically distributed. This procedure ameliorates the need for adjustment in the construction of confidence bounds for both the autocorrelation function and the auto-distance correlation function in goodness-of-fit testing.},
  archive      = {J_BIOMET},
  author       = {Davis, Richard A and Fernandes, Leon},
  doi          = {10.1093/biomet/asaf017},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf017},
  shortjournal = {Biometrika},
  title        = {Sample splitting and assessing goodness-of-fit of time series},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the partial autocorrelation function for locally stationary time series: Characterization, estimation and inference. <em>BIOMET</em>, <em>112</em>(2), asaf016. (<a href='https://doi.org/10.1093/biomet/asaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For stationary time series, it is common to use plots of the partial autocorrelation function (PACF) or PACF-based tests to explore the temporal dependence structure of the process. To the best of our knowledge, analogues for nonstationary time series have not yet been fully developed. This article aims to fill this gap for locally stationary time series with short-range dependence. First, we characterize the PACF locally in the time domain and show that the j th PACF decays with j at a rate that adapts to the temporal dependence of the time series |$ \{x_{i,n}\} $|⁠ . Second, at each time |$ i, $| inspired by Killick et al. (2020) . We show that the PACF can be efficiently approximated by the best linear prediction coefficients via the Yule–Walker equations. This allows us to study the PACF via ordinary least squares locally. Third, we show that the PACF is smooth in time for locally stationary time series. We use the sieve method with ordinary least squares to estimate the PACF and construct some statistics to test the PACF and infer the structure of the time series. These tests generalize and modify those used in Brockwell & Davis (1987) for stationary time series. Finally, a multiplier bootstrap algorithm is proposed for practical implementation and an R package Sie2nts is provided to implement the algorithm. Numerical simulations and real-data analysis confirm the usefulness of our results.},
  archive      = {J_BIOMET},
  author       = {Ding, Xiucai and Zhou, Zhou},
  doi          = {10.1093/biomet/asaf016},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf016},
  shortjournal = {Biometrika},
  title        = {On the partial autocorrelation function for locally stationary time series: Characterization, estimation and inference},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous-time locally stationary wavelet processes. <em>BIOMET</em>, <em>112</em>(2), asaf015. (<a href='https://doi.org/10.1093/biomet/asaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the class of continuous-time locally stationary wavelet processes. Continuous-time models enable us to properly provide scale-based time series models for irregularly spaced observations for the first time, while also permitting a spectral representation of the process over a continuous range of scales. We derive results for both the theoretical setting, where we assume access to the entire process sample path, and a more practical one, which develops methods for estimating the quantities of interest from sampled time series. The latter estimates are accurately computable in reasonable time by solving the relevant linear integral equation using the iterative soft-thresholding algorithm of Daubechies et al. (2004). Appropriate smoothing techniques are also developed and applied in this new setting. Comparisons to previous methods are conducted on the heart rate time series of a sleeping infant. Additionally, we exemplify our new methods by computing spectral and autocovariance estimates on irregularly spaced heart rate data obtained from a recent sleep-state study.},
  archive      = {J_BIOMET},
  author       = {Palasciano, H A and Knight, M I and Nason, G P},
  doi          = {10.1093/biomet/asaf015},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf015},
  shortjournal = {Biometrika},
  title        = {Continuous-time locally stationary wavelet processes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomization inference when n equals one. <em>BIOMET</em>, <em>112</em>(2), asaf013. (<a href='https://doi.org/10.1093/biomet/asaf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades, N -of-1 experiments, where a unit serves as its own control and treatment in different time windows, have been used in certain medical contexts. However, due to effects that accumulate over long time windows and interventions that have complex evolution, a lack of robust inference tools has limited the widespread applicability of such N -of-1 designs. This work combines techniques from experimental design in causal inference and system identification from control theory to provide such an inference framework. We derive a model of the dynamic interference effect that arises in linear time-invariant dynamical systems. We show that a family of causal estimands analogous to those studied in potential outcomes are estimable via a standard estimator derived from the method of moments. We derive formulae for higher moments of this estimator and describe conditions under which N -of-1 designs may provide faster ways to estimate the effects of interventions in dynamical systems. We also provide conditions under which our estimator is asymptotically normal and derive valid confidence intervals for this setting.},
  archive      = {J_BIOMET},
  author       = {Liang, Tengyuan and Recht, Benjamin},
  doi          = {10.1093/biomet/asaf013},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf013},
  shortjournal = {Biometrika},
  title        = {Randomization inference when n equals one},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient estimation for functional accelerated failure time models. <em>BIOMET</em>, <em>112</em>(2), asaf011. (<a href='https://doi.org/10.1093/biomet/asaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a functional accelerated failure time model to characterize the effects of both functional and scalar covariates on the time to event of interest, and provide regularity conditions to guarantee model identifiability. For efficient estimation of model parameters, we develop a sieve maximum likelihood approach where parametric and nonparametric coefficients are bundled with an unknown baseline hazard function in the likelihood function. Not only do the bundled parameters cause immense numerical difficulties, but they also result in new challenges in theoretical development. By developing a general theoretical framework, we overcome the challenges arising from the bundled parameters and derive the convergence rate of the proposed estimator. Additionally, we prove that the finite-dimensional estimator is root- n consistent, asymptotically normal and achieves the semiparametric information bound. Furthermore, we demonstrate the nonparametric optimality of the functional estimator and construct the asymptotic simultaneous confidence band. The proposed inference procedures are evaluated by extensive simulation studies and illustrated with an application to the National Health and Nutrition Examination Survey data.},
  archive      = {J_BIOMET},
  author       = {Liu, Changyu and Su, Wen and Liu, Kin-Yat and Yin, Guosheng and Zhao, Xingqiu},
  doi          = {10.1093/biomet/asaf011},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf011},
  shortjournal = {Biometrika},
  title        = {Efficient estimation for functional accelerated failure time models},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and reliable confidence intervals for a variance component. <em>BIOMET</em>, <em>112</em>(2), asaf010. (<a href='https://doi.org/10.1093/biomet/asaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that in a variance component model, confidence intervals with asymptotically correct uniform coverage probability can be obtained by inverting certain test statistics based on the score for the restricted likelihood. The results hold in settings where the variance component is near or at the boundary of the parameter set. Simulations indicate that the proposed test statistics are approximately pivotal and lead to confidence intervals with near-nominal coverage even in small samples. We illustrate the application of the proposed methods in spatially resolved transcriptomics, where we compute approximately 15 000 confidence intervals, used for gene ranking, in less than 4 minutes. In the settings we consider, the proposed method is between two and 28 000 times faster than popular alternatives, depending on how many confidence intervals are computed.},
  archive      = {J_BIOMET},
  author       = {Zhang, Yiqiao and Ekvall, Karl Oskar and Molstad, Aaron J},
  doi          = {10.1093/biomet/asaf010},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf010},
  shortjournal = {Biometrika},
  title        = {Fast and reliable confidence intervals for a variance component},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testable implications of outcome-independent missingness not at random in covariates. <em>BIOMET</em>, <em>112</em>(2), asaf009. (<a href='https://doi.org/10.1093/biomet/asaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common aim of empirical research is to regress an outcome on a set of covariates, when some covariates are subject to missingness. If the probability of missingness is conditionally independent of the outcome, given the covariates, then a complete-case analysis is unbiased for parameters conditional on covariates. We derive all testable constraints that such outcome-independent missingness not at random implies on the observed data distribution, for settings where both the outcome and covariates are categorical. By assessing if these constraints are violated for a particular observed data distribution, the analyst can infer whether the assumption of outcome-independent missingness not at random is violated for that distribution. The constraints are formulated implicitly, in terms of consistency requirements on certain linear equation systems. We also derive explicit inequality constraints that are more easily assessable, but also more permissive than the implicit constraints.},
  archive      = {J_BIOMET},
  author       = {Sjölander, A and Hägg, S},
  doi          = {10.1093/biomet/asaf009},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf009},
  shortjournal = {Biometrika},
  title        = {Testable implications of outcome-independent missingness not at random in covariates},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Super-efficient estimation of future conditional hazards based on time-homogeneous high-quality marker information. <em>BIOMET</em>, <em>112</em>(2), asaf008. (<a href='https://doi.org/10.1093/biomet/asaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new concept for forecasting future events based on marker information. The model is developed in the nonparametric counting process setting under the assumptions that the marker is of so-called high quality and with a time-homogeneous conditional distribution. Despite the model having nonparametric parts, it is established herein that it attains a parametric rate of uniform consistency and uniform asymptotic normality. In usual nonparametric scenarios, reaching such a fast convergence rate is not possible, so one can say that the proposed approach is super-efficient. These theoretical results are employed in the construction of simultaneous confidence bands directly for the hazard rate. Extensive simulation studies validate and compare the proposed methodology with the joint modelling approach and illustrate its robustness for mild violations of the assumptions. Its use in practice is illustrated in the computation of individual dynamic predictions in the context of primary biliary cirrhosis of the liver.},
  archive      = {J_BIOMET},
  author       = {Bagkavos, D and Isakson, A and Mammen, E and Nielsen, J P and Proust–Lima, C},
  doi          = {10.1093/biomet/asaf008},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf008},
  shortjournal = {Biometrika},
  title        = {Super-efficient estimation of future conditional hazards based on time-homogeneous high-quality marker information},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale multiple testing of composite null hypotheses under heteroskedasticity. <em>BIOMET</em>, <em>112</em>(2), asaf007. (<a href='https://doi.org/10.1093/biomet/asaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heteroskedasticity poses several methodological challenges in designing valid and powerful procedures for simultaneous testing of composite null hypotheses. In particular, the conventional practice of standardizing or rescaling heteroskedastic test statistics in this setting may severely affect the power of the underlying multiple testing procedure. Additionally, when the inferential parameter of interest is correlated with the variance of the test statistic, methods that ignore this dependence may fail to control the Type I error at the desired level. We propose a new heteroskedasticity-adjusted multiple testing procedure that avoids data reduction by standardization and directly incorporates the side information from the variances into the testing procedure. Our approach relies on an improved nonparametric empirical Bayes deconvolution estimator that offers a practical way of capturing the dependence between the inferential parameter of interest and the variance of the test statistic. We develop theory to establish that the proposed procedure is asymptotically valid and optimal for false discovery rate control. Simulation results demonstrate that our method outperforms existing procedures, with substantial power gains across many settings at the same false discovery rate level. The method is illustrated with an application involving the detection of engaged users on a mobile game app.},
  archive      = {J_BIOMET},
  author       = {Gang, B and Banerjee, T},
  doi          = {10.1093/biomet/asaf007},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf007},
  shortjournal = {Biometrika},
  title        = {Large-scale multiple testing of composite null hypotheses under heteroskedasticity},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric sieve estimation for survival data with two-layer censoring. <em>BIOMET</em>, <em>112</em>(2), asaf006. (<a href='https://doi.org/10.1093/biomet/asaf006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease registry data provide important information on the progression of disease conditions. However, reports of death or drop-out of patients enrolled in the registry are always subject to a noticeable delay. Reporting delays, together with the administrative censoring that arises from a freeze date in data collection, lead to two layers of right censoring in the data. The first layer results from random drop-out and acts on the survival time. The second layer is the administrative censoring, which acts on the sum of the reporting delay and the minimum of the survival time and random drop-out time. The heterogeneities among patients further complicate data analysis. This paper proposes a novel semiparametric sieve method based on phase-type distributions, in which covariates can be readily accommodated by the accelerated failure time model. A well-orchestrated EM algorithm is developed to compute the sieve maximum likelihood estimator. We establish the consistency and rate of convergence of the proposed sieve estimators, as well as the asymptotic normality and semiparametric efficiency of the estimators for the regression parameters. Comprehensive simulations and a real example of lung cancer registry data are used to demonstrate the proposed method. The results reveal substantial biases if reporting delays are overlooked.},
  archive      = {J_BIOMET},
  author       = {Wang, Yudong and Tong, Jiayi and Hu, Xiangbin and Ye, Zhi-Sheng and Tang, Cheng Yong and Chen, Yong},
  doi          = {10.1093/biomet/asaf006},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf006},
  shortjournal = {Biometrika},
  title        = {Semiparametric sieve estimation for survival data with two-layer censoring},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: ‘Efficient estimation under data fusion’. <em>BIOMET</em>, <em>112</em>(2), asaf005. (<a href='https://doi.org/10.1093/biomet/asaf005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asaf005},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf005},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Efficient estimation under data fusion’},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An omitted variable bias framework for sensitivity analysis of instrumental variables. <em>BIOMET</em>, <em>112</em>(2), asaf004. (<a href='https://doi.org/10.1093/biomet/asaf004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Cinelli, Carlos and Hazlett, Chad},
  doi          = {10.1093/biomet/asaf004},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf004},
  shortjournal = {Biometrika},
  title        = {An omitted variable bias framework for sensitivity analysis of instrumental variables},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise-induced randomization in regression discontinuity designs. <em>BIOMET</em>, <em>112</em>(2), asaf003. (<a href='https://doi.org/10.1093/biomet/asaf003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression discontinuity designs assess causal effects in settings where treatment is determined by whether an observed running variable crosses a prespecified threshold. Here, we propose a new approach to identification, estimation and inference in regression discontinuity designs that uses knowledge about exogenous noise (e.g., measurement error) in the running variable. In our strategy, we weight treated and control units to balance a latent variable, of which the running variable is a noisy measure. Our approach is driven by effective randomization provided by the noise in the running variable, and complements standard formal analyses that appeal to continuity arguments while ignoring the stochastic nature of the assignment mechanism.},
  archive      = {J_BIOMET},
  author       = {Eckles, Dean and Ignatiadis, Nikolaos and Wager, Stefan and Wu, Han},
  doi          = {10.1093/biomet/asaf003},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf003},
  shortjournal = {Biometrika},
  title        = {Noise-induced randomization in regression discontinuity designs},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomization-based Z-estimation for evaluating average and individual treatment effects. <em>BIOMET</em>, <em>112</em>(2), asaf002. (<a href='https://doi.org/10.1093/biomet/asaf002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized experiments have been the gold standard for drawing causal inference. The conventional model-based approach has been one of the most popular methods of analysing treatment effects from randomized experiments, which is often carried out through inference for certain model parameters. In this paper, we provide a systematic investigation of model-based analyses for treatment effects under the randomization-based inference framework. This framework does not impose any distributional assumptions on the outcomes, covariates and their dependence, and utilizes only randomization as the reasoned basis. We first derive the asymptotic theory for Z -estimation in completely randomized experiments, and propose sandwich-type conservative covariance estimation. We then apply the developed theory to analyse both average and individual treatment effects in randomized experiments. For the average treatment effect, we consider model-based, model-imputed and model-assisted estimation strategies, where the first two strategies can be sensitive to model misspecification or require specific methods for parameter estimation. The model-assisted approach is robust to arbitrary model misspecification and always provides consistent average treatment effect estimation. We propose optimal ways to conduct model-assisted estimation using generally nonlinear least squares for parameter estimation. For the individual treatment effects, we propose directly modelling the relationship between individual effects and covariates, and discuss the model’s identifiability, inference and interpretation allowing for model misspecification.},
  archive      = {J_BIOMET},
  author       = {Qu, Tianyi and Du, Jiangchuan and Li, Xinran},
  doi          = {10.1093/biomet/asaf002},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf002},
  shortjournal = {Biometrika},
  title        = {Randomization-based Z-estimation for evaluating average and individual treatment effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-sample distribution tests in high dimensions via max-sliced wasserstein distance and bootstrapping. <em>BIOMET</em>, <em>112</em>(2), asaf001. (<a href='https://doi.org/10.1093/biomet/asaf001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-sample hypothesis testing is a fundamental statistical problem for inference about two populations. In this paper, we construct a novel test statistic to detect high-dimensional distributional differences based on the max-sliced Wasserstein distance to mitigate the curse of dimensionality. By exploiting an intriguing link between the distance and suprema of empirical processes, we develop an effective bootstrapping procedure to approximate the null distribution of the test statistic. One distinctive feature of the proposed test is the ability to construct simultaneous confidence intervals for the max-sliced Wasserstein distances of projected distributions of interest. This enables, not only the detection of global distributional differences, but also the identification of significantly different marginal distributions between two populations, without the need for additional tests. We establish the convergence of Gaussian and bootstrap approximations of the proposed test, based on which we show that the test is asymptotically valid and powerful as long as the considered max-sliced Wasserstein distance is adequately large. The merits of our approach are illustrated via simulated and real data examples.},
  archive      = {J_BIOMET},
  author       = {Hu, Xiaoyu and Lin, Zhenhua},
  doi          = {10.1093/biomet/asaf001},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf001},
  shortjournal = {Biometrika},
  title        = {Two-sample distribution tests in high dimensions via max-sliced wasserstein distance and bootstrapping},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistency of common spatial estimators under spatial confounding. <em>BIOMET</em>, <em>112</em>(2), asae070. (<a href='https://doi.org/10.1093/biomet/asae070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the asymptotic performance of popular spatial regression estimators of the linear effect of an exposure on an outcome under spatial confounding, the presence of an unmeasured spatially structured variable influencing both the exposure and the outcome. We first show that the estimators from ordinary least squares and restricted spatial regression are asymptotically biased under spatial confounding. We then prove a novel result on the infill consistency of the generalized least squares estimator using a working covariance matrix from a Matérn or squared exponential kernel, in the presence of spatial confounding. The result holds under very mild assumptions, accommodating any exposure with some nonspatial variation, any spatially continuous fixed confounder function, and non-Gaussian errors in both the exposure and the outcome. Finally, we prove that spatial estimators from generalized least squares, Gaussian process regression and spline models that are consistent under confounding by a fixed function will also be consistent under endogeneity or confounding by a random function, i.e., a stochastic process. We conclude that, contrary to some claims in the literature on spatial confounding, traditional spatial estimators are capable of estimating linear exposure effects under spatial confounding as long as there is some noise in the exposure. We support our theoretical arguments with simulation studies.},
  archive      = {J_BIOMET},
  author       = {Gilbert, Brian and Ogburn, Elizabeth L and Datta, Abhirup},
  doi          = {10.1093/biomet/asae070},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae070},
  shortjournal = {Biometrika},
  title        = {Consistency of common spatial estimators under spatial confounding},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving randomized controlled trial analysis via data-adaptive borrowing. <em>BIOMET</em>, <em>112</em>(2), asae069. (<a href='https://doi.org/10.1093/biomet/asae069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, real-world external controls have grown in popularity as a tool to empower randomized placebo-controlled trials, particularly in rare diseases or cases where balanced randomization is unethical or impractical. However, as external controls are not always comparable to the trials, direct borrowing without scrutiny may heavily bias the treatment effect estimator. Our paper proposes a data-adaptive integrative framework capable of preventing unknown biases of the external controls. The adaptive nature is achieved by dynamically sorting out a comparable subset of external controls via bias penalization. Our proposed method can simultaneously achieve (a) the semiparametric efficiency bound when the external controls are comparable and (b) selective borrowing that mitigates the impact of the existence of incomparable external controls. Furthermore, we establish statistical guarantees, including consistency, asymptotic distribution and inference, providing Type-I error control and good power. Extensive simulations and two real-data applications show that the proposed method leads to improved performance over the trial-only estimator across various bias-generating scenarios.},
  archive      = {J_BIOMET},
  author       = {Gao, Chenyin and Yang, Shu and Shan, Mingyang and Ye, Wenyu and Lipkovich, Ilya and Faries, Douglas},
  doi          = {10.1093/biomet/asae069},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae069},
  shortjournal = {Biometrika},
  title        = {Improving randomized controlled trial analysis via data-adaptive borrowing},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric local variable selection under misspecification. <em>BIOMET</em>, <em>112</em>(2), asae068. (<a href='https://doi.org/10.1093/biomet/asae068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local variable selection aims to test for the effect of covariates on an outcome within specific regions. We outline a challenge that arises in the presence of nonlinear effects and model misspecification. Specifically, for common semiparametric methods, even slight model misspecification can result in a high false positive rate, in a manner that is highly sensitive to the chosen basis functions. We propose a method based on orthogonal cut splines that avoids false positive inflation for any choice of knots and achieves consistent local variable selection. Our approach offers simplicity, can handle both continuous and categorical covariates, and provides theory for high-dimensional covariates and model misspecification. We discuss settings with either independent or dependent data. The proposed method allows inclusion of adjustment covariates that do not undergo selection, enhancing the model’s flexibility. Our examples describe salary gaps associated with various discrimination factors at different ages and elucidate the effects of covariates on functional data measuring brain activation at different times.},
  archive      = {J_BIOMET},
  author       = {Rossell, D and Seong, A K and Saez, I and Guindani, M},
  doi          = {10.1093/biomet/asae068},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae068},
  shortjournal = {Biometrika},
  title        = {Semiparametric local variable selection under misspecification},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially factorized variational inference for high-dimensional mixed models. <em>BIOMET</em>, <em>112</em>(2), asae067. (<a href='https://doi.org/10.1093/biomet/asae067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While generalized linear mixed models are a fundamental tool in applied statistics, many specifications, such as those involving categorical factors with many levels or interaction terms, can be computationally challenging to estimate due to the need to compute or approximate high-dimensional integrals. Variational inference is a popular way to perform such computations, especially in the Bayesian context. However, naive use of such methods can provide unreliable uncertainty quantification. We show that this is indeed the case for mixed models, proving that standard mean-field variational inference dramatically underestimates posterior uncertainty in high dimensions. We then show how appropriately relaxing the mean-field assumption leads to methods whose uncertainty quantification does not deteriorate in high dimensions, and whose total computational cost scales linearly with the number of parameters and observations. Our theoretical and numerical results focus on mixed models with Gaussian or binomial likelihoods, and rely on connections to random graph theory to obtain sharp high-dimensional asymptotic analysis. We also provide generic results, which are of independent interest, relating the accuracy of variational inference to the convergence rate of the corresponding coordinate ascent algorithm that is used to find it. Our proposed methodology is implemented in the R package vglmer . Numerical results with simulated and real data examples illustrate the favourable computation cost versus accuracy trade-off of our approach compared to various alternatives.},
  archive      = {J_BIOMET},
  author       = {Goplerud, M and Papaspiliopoulos, O and Zanella, G},
  doi          = {10.1093/biomet/asae067},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae067},
  shortjournal = {Biometrika},
  title        = {Partially factorized variational inference for high-dimensional mixed models},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anytime-valid and asymptotically efficient inference driven by predictive recursion. <em>BIOMET</em>, <em>112</em>(2), asae066. (<a href='https://doi.org/10.1093/biomet/asae066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distinguishing two models is a fundamental and practically important statistical problem. Error rate control is crucial to the testing logic, but in complex nonparametric settings can be difficult to achieve, especially when the stopping rule that determines the data collection process is not available. This paper proposes an e -process construction based on the predictive recursion algorithm originally designed to recursively fit nonparametric mixture models. The resulting predictive recursion e -process affords anytime-valid inference and is asymptotically efficient in the sense that its growth rate is first-order optimal relative to the predictive recursion’s mixture model.},
  archive      = {J_BIOMET},
  author       = {Dixit, Vaidehi and Martin, Ryan},
  doi          = {10.1093/biomet/asae066},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae066},
  shortjournal = {Biometrika},
  title        = {Anytime-valid and asymptotically efficient inference driven by predictive recursion},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing variable importance in survival analysis using machine learning. <em>BIOMET</em>, <em>112</em>(2), asae061. (<a href='https://doi.org/10.1093/biomet/asae061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a collection of features available for inclusion in a predictive model, it may be of interest to quantify the relative importance of a subset of features for the prediction task at hand. For example, in HIV vaccine trials, participant baseline characteristics are used to predict the probability of HIV acquisition over the intended follow-up period, and investigators may wish to understand how much certain types of predictors, such as behavioural factors, contribute to overall predictiveness. Time-to-event outcomes such as time to HIV acquisition are often subject to right censoring, and existing methods for assessing variable importance are typically not intended to be used in this setting. We describe a broad class of algorithm-agnostic variable importance measures for prediction in the context of survival data. We propose a nonparametric efficient estimation procedure that incorporates flexible learning of nuisance parameters, yields asymptotically valid inference and enjoys double robustness. We assess the performance of our proposed procedure via numerical simulations and analyse data from the HVTN 702 vaccine trial to inform enrolment strategies for future HIV vaccine trials.},
  archive      = {J_BIOMET},
  author       = {Wolock, C J and Gilbert, P B and Simon, N and Carone, M},
  doi          = {10.1093/biomet/asae061},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae061},
  shortjournal = {Biometrika},
  title        = {Assessing variable importance in survival analysis using machine learning},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian clustering of high-dimensional data via latent repulsive mixtures. <em>BIOMET</em>, <em>112</em>(2), asae059. (<a href='https://doi.org/10.1093/biomet/asae059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based clustering of moderate- or large-dimensional data is notoriously difficult. We propose a model for simultaneous dimensionality reduction and clustering by assuming a mixture model for a set of latent scores, which are then linked to the observations via a Gaussian latent factor model. This approach was recently investigated by Chandra et al. (2023). The authors used a factor-analytic representation and assumed a mixture model for the latent factors. However, performance can deteriorate in the presence of model misspecification. Assuming a repulsive point process prior for the component-specific means of the mixture for the latent scores is shown to yield a more robust model that outperforms the standard mixture model for the latent factors in several simulated scenarios. The repulsive point process must be anisotropic to favour well-separated clusters of data, and its density should be tractable for efficient posterior inference. We address these issues by proposing a general construction for anisotropic determinantal point processes. We illustrate our model in simulations, as well as a plant species co-occurrence dataset.},
  archive      = {J_BIOMET},
  author       = {Ghilotti, L and Beraha, M and Guglielmi, A},
  doi          = {10.1093/biomet/asae059},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae059},
  shortjournal = {Biometrika},
  title        = {Bayesian clustering of high-dimensional data via latent repulsive mixtures},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust and heteroscedasticity-aware sample trimming for causal inference. <em>BIOMET</em>, <em>112</em>(2), asae053. (<a href='https://doi.org/10.1093/biomet/asae053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular method for variance reduction in causal inference is propensity-based trimming, the practice of removing units with extreme propensities from the sample. This practice has theoretical grounding when the data are homoscedastic and the propensity model is parametric (Crump et al., 2009; Yang & Ding, 2018), but in modern settings where heteroscedastic data are analysed with nonparametric models, existing theory fails to support current practice. In this work, we address this challenge by developing new methods and theory for sample trimming. Our contributions are three-fold. First, we describe novel procedures for selecting which units to trim. Our procedures differ from previous works in that we trim, not only units with small propensities, but also units with extreme conditional variances. Second, we give new theoretical guarantees for inference after trimming. In particular, we show how to perform inference on the trimmed subpopulation without requiring that our regressions converge at parametric rates. Instead, we make only fourth-root rate assumptions like those in the double machine learning literature. This result applies to conventional propensity-based trimming as well, and thus may be of independent interest. Finally, we propose a bootstrap-based method for constructing simultaneously valid confidence intervals for multiple trimmed subpopulations, which are valuable for navigating the trade-off between sample size and variance reduction inherent in trimming. We validate our methods in simulation, on the 2007–2008 National Health and Nutrition Examination Survey and on a semisynthetic Medicare dataset, and find promising results in all settings.},
  archive      = {J_BIOMET},
  author       = {Khan, S and Ugander, J},
  doi          = {10.1093/biomet/asae053},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae053},
  shortjournal = {Biometrika},
  title        = {Doubly robust and heteroscedasticity-aware sample trimming for causal inference},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust estimation under a possibly misspecified marginal structural cox model. <em>BIOMET</em>, <em>112</em>(1), asae065. (<a href='https://doi.org/10.1093/biomet/asae065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we consider the marginal structural Cox model, which has been widely used to analyse observational studies with survival outcomes. The standard inverse probability weighting method under the model hinges on a propensity score model for the treatment assignment and a censoring model that incorporates both the treatment and the covariates. In such settings model misspecification can often occur, and the Cox regression model’s non-collapsibility has historically posed challenges when striving to guard against model misspecification through augmentation. We introduce a novel joint augmentation to the martingale-based full-data estimating functions and develop rate double robustness, which allows the use of machine learning and nonparametric methods to overcome the challenges of non-collapsibility. We closely examine its theoretical properties to guarantee root- n inference for the estimand. The estimator extends naturally to estimating a time-average treatment effect when the proportional hazards assumption fails, and we show that it satisfies both the assumption-lean and the well-specification criteria in the context of a causal estimand for censoring survival data; that is, it is a functional of the potential outcome distributions only and does not depend on the treatment assignment mechanism, the covariate distribution or the censoring mechanism. The martingale-based augmentation approach is also applicable to many semiparametric failure time models. Finally, its application to a dataset provides insights into the impact of mid-life alcohol consumption on mortality in later life.},
  archive      = {J_BIOMET},
  author       = {Luo, Jiyu and Rava, Denise and Bradic, Jelena and Xu, Ronghui},
  doi          = {10.1093/biomet/asae065},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae065},
  shortjournal = {Biometrika},
  title        = {Doubly robust estimation under a possibly misspecified marginal structural cox model},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using negative controls to identify causal effects with invalid instrumental variables. <em>BIOMET</em>, <em>112</em>(1), asae064. (<a href='https://doi.org/10.1093/biomet/asae064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many proposals for the identification of causal effects require an instrumental variable that satisfies strong, untestable unconfoundedness and exclusion restriction assumptions. In this paper, we show how one can potentially identify causal effects under violations of these assumptions by harnessing a negative control population or outcome. This strategy allows one to leverage subpopulations for whom the exposure is degenerate, and requires that the instrument-outcome association satisfies a certain parallel trend condition. We develop semiparametric efficiency theory for a general instrumental variable model, and obtain a multiply robust, locally efficient estimator of the average treatment effect in the treated. The utility of the estimators is demonstrated in simulation studies and an analysis of the Life Span Study.},
  archive      = {J_BIOMET},
  author       = {Dukes, O and Richardson, D B and Shahn, Z and Robins, J M and Tchetgen Tchetgen, E J},
  doi          = {10.1093/biomet/asae064},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae064},
  shortjournal = {Biometrika},
  title        = {Using negative controls to identify causal effects with invalid instrumental variables},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invariant probabilistic prediction. <em>BIOMET</em>, <em>112</em>(1), asae063. (<a href='https://doi.org/10.1093/biomet/asae063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method for obtaining invariant probabilistic predictions and study the consistency of the underlying parameters. Finally, we demonstrate the empirical performance of our proposed procedure via simulations and analysis of single-cell data.},
  archive      = {J_BIOMET},
  author       = {Henzi, Alexander and Shen, Xinwei and Law, Michael and Bühlmann, Peter},
  doi          = {10.1093/biomet/asae063},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae063},
  shortjournal = {Biometrika},
  title        = {Invariant probabilistic prediction},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On rosenbaum’s rank-based matching estimator. <em>BIOMET</em>, <em>112</em>(1), asae062. (<a href='https://doi.org/10.1093/biomet/asae062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In two influential contributions, Rosenbaum (2005, 2020a) advocated for using the distances between componentwise ranks, instead of the original data values, to measure covariate similarity when constructing matching estimators of average treatment effects. While the intuitive benefits of using covariate ranks for matching estimation are apparent, there is no theoretical understanding of such procedures in the literature. We fill this gap by demonstrating that Rosenbaum’s rank-based matching estimator, when coupled with a regression adjustment, enjoys the properties of double robustness and semiparametric efficiency without the need to enforce restrictive covariate moment assumptions. Our theoretical findings further emphasize the statistical virtues of employing ranks for estimation and inference, more broadly aligning with the insights put forth by Peter Bickel in his 2004 Rietz lecture.},
  archive      = {J_BIOMET},
  author       = {Cattaneo, Matias D and Han, Fang and Lin, Zhexiao},
  doi          = {10.1093/biomet/asae062},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae062},
  shortjournal = {Biometrika},
  title        = {On rosenbaum’s rank-based matching estimator},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverting estimating equations for causal inference on quantiles. <em>BIOMET</em>, <em>112</em>(1), asae058. (<a href='https://doi.org/10.1093/biomet/asae058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The causal inference literature frequently focuses on estimating the mean of the potential outcome, whereas quantiles of the potential outcome may carry important additional information. We propose an inverse estimating equation framework to generalize a wide class of causal inference solutions from estimating the mean of the potential outcome to its quantiles. We assume that a moment function is available to identify the mean of the threshold-transformed potential outcome, based on which a convenient construction of the estimating equation of the quantiles of the potential outcome is proposed. In addition, we give a general construction of the efficient influence functions of the mean and quantiles of potential outcomes, and establish their connection. We motivate estimators for the quantile estimands with the efficient influence function, and develop their asymptotic properties when either parametric models or data-adaptive machine learners are used to estimate the nuisance functions. A broad implication of our results is that one can rework the existing result for mean causal estimands to facilitate causal inference on quantiles. Our general results are illustrated by several analytical and numerical examples.},
  archive      = {J_BIOMET},
  author       = {Cheng, Chao and Li, Fan},
  doi          = {10.1093/biomet/asae058},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae058},
  shortjournal = {Biometrika},
  title        = {Inverting estimating equations for causal inference on quantiles},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The phase diagram of kernel interpolation in large dimensions. <em>BIOMET</em>, <em>112</em>(1), asae057. (<a href='https://doi.org/10.1093/biomet/asae057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalization ability of kernel interpolation in large dimensions, ie, n ≍ d γ for some γ > 0 ⁠ , could be one of the most interesting problems in the recent renaissance of kernel regression, since it may help us understand the so-called benign overfitting phenomenon reported in the neural networks literature. Focusing on the inner product kernel on the unit sphere, we fully characterize the exact order of both the variance and the bias of large-dimensional kernel interpolation under various source conditions s ⩾ 0 ⁠ . Consequently, we obtain the ( s , γ ) phase diagram of large-dimensional kernel interpolation, ie, we determine the regions in the ( s , γ ) plane where the kernel interpolation is minimax optimal, suboptimal and inconsistent.},
  archive      = {J_BIOMET},
  author       = {Zhang, Haobo and Lu, Weihao and Lin, Qian},
  doi          = {10.1093/biomet/asae057},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae057},
  shortjournal = {Biometrika},
  title        = {The phase diagram of kernel interpolation in large dimensions},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised distribution learning. <em>BIOMET</em>, <em>112</em>(1), asae056. (<a href='https://doi.org/10.1093/biomet/asae056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the challenge of distribution estimation and inference in a semi-supervised setting. In contrast to prior research focusing on parameter inference, this work explores the complexities of semi-supervised distribution estimation, particularly the uniformity problem inherent in functional processes. To tackle this issue, we introduce a versatile framework designed to extract valuable information from unlabelled data by approximating a conditional distribution on covariates. The proposed estimator is derived using K -fold cross-fitting, and exhibits both consistency and asymptotic Gaussian process properties. Under mild conditions, the proposed estimator outperforms the empirical cumulative distribution function in terms of asymptotic efficiency. Several applications of the methodology are given, including parameter inference and goodness-of-fit tests.},
  archive      = {J_BIOMET},
  author       = {Wen, Mengtao and Jia, Yinxu and Ren, Haojie and Wang, Zhaojun and Zou, Changliang},
  doi          = {10.1093/biomet/asae056},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae056},
  shortjournal = {Biometrika},
  title        = {Semi-supervised distribution learning},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional principal component analysis with informative observation times. <em>BIOMET</em>, <em>112</em>(1), asae055. (<a href='https://doi.org/10.1093/biomet/asae055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional principal component analysis has been shown to be invaluable for revealing variation modes of longitudinal outcomes, which serve as important building blocks for forecasting and model building. Decades of research have advanced methods for functional principal component analysis, often assuming independence between the observation times and longitudinal outcomes. Yet such assumptions are fragile in real-world settings where observation times may be driven by outcome-related processes. Rather than ignoring the informative observation time process, we explicitly model the observational times by a general counting process dependent on time-varying prognostic factors. Identification of the mean, covariance function and functional principal components ensues via inverse intensity weighting. We propose using weighted penalized splines for estimation and establish consistency and convergence rates for the weighted estimators. Simulation studies demonstrate that the proposed estimators are substantially more accurate than the existing ones in the presence of a correlation between the observation time process and the longitudinal outcome process. We further examine the finite-sample performance of the proposed method using the Acute Infection and Early Disease Research Program study.},
  archive      = {J_BIOMET},
  author       = {Sang, Peijun and Kong, Dehan and Yang, Shu},
  doi          = {10.1093/biomet/asae055},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae055},
  shortjournal = {Biometrika},
  title        = {Functional principal component analysis with informative observation times},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). With random regressors, least squares inference is robust to correlated errors with unknown correlation structure. <em>BIOMET</em>, <em>112</em>(1), asae054. (<a href='https://doi.org/10.1093/biomet/asae054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear regression is arguably the most widely used statistical method. With fixed regressors and correlated errors, the conventional wisdom is to modify the variance-covariance estimator to accommodate the known correlation structure of the errors. We depart from existing literature by showing that with random regressors, linear regression inference is robust to correlated errors with unknown correlation structure. The existing theoretical analyses for linear regression are no longer valid because even the asymptotic normality of the least squares coefficients breaks down in this regime. We first prove the asymptotic normality of the t statistics by establishing their Berry–Esseen bounds based on a novel probabilistic analysis of self-normalized statistics. We then study the local power of the corresponding t tests and show that, perhaps surprisingly, error correlation can even enhance power in the regime of weak signals. Overall, our results show that linear regression is applicable more broadly than the conventional theory suggests, and they further demonstrate the value of randomization for ensuring robustness of inference.},
  archive      = {J_BIOMET},
  author       = {Zhang, Zifeng and Ding, Peng and Zhou, Wen and Wang, Haonan},
  doi          = {10.1093/biomet/asae054},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae054},
  shortjournal = {Biometrika},
  title        = {With random regressors, least squares inference is robust to correlated errors with unknown correlation structure},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the degrees of freedom of the smoothing parameter. <em>BIOMET</em>, <em>112</em>(1), asae052. (<a href='https://doi.org/10.1093/biomet/asae052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The smoothing parameters in a semiparametric model are estimated based on criteria such as generalized cross-validation or restricted maximum likelihood. As these parameters are estimated in a data-driven manner, they influence the degrees of freedom of a semiparametric model, based on Stein’s lemma. This allows us to associate parts of the degrees of freedom of a semiparametric model with the smoothing parameters. A framework is introduced that enables these degrees of freedom of the smoothing parameters to be derived analytically, based on the implicit function theorem. The degrees of freedom of the smoothing parameters are efficient to compute and have a geometrical interpretation. The practical importance of this finding is highlighted by a simulation study and an application, showing that ignoring the degrees of freedom of the smoothing parameters in Akaike information criterion-based model selection leads to an increase in the post-selection prediction error.},
  archive      = {J_BIOMET},
  author       = {Säfken, B and Kneib, T and Wood, S N},
  doi          = {10.1093/biomet/asae052},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae052},
  shortjournal = {Biometrika},
  title        = {On the degrees of freedom of the smoothing parameter},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive null proportion estimator for false discovery rate control. <em>BIOMET</em>, <em>112</em>(1), asae051. (<a href='https://doi.org/10.1093/biomet/asae051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The false discovery rate is a commonly used criterion in multiple testing, and the Benjamini–Hochberg procedure is a standard approach to false discovery rate control. To increase its power, adaptive Benjamini–Hochberg procedures, that use estimates of the null proportion, have been proposed. A particularly popular approach being that based on Storey’s estimator. The performance of Storey’s estimator hinges on a critical hyperparameter, such that a pre-fixed configuration may lack power and existing data-driven hyperparameters may compromise false discovery rate control. In this work, we propose a novel class of adaptive hyperparameters and establish the false discovery rate control of the associated adaptive Benjamini–Hochberg procedure using a martingale argument. Within this class of data-driven hyperparameters, we further present a specific configuration designed to maximize the number of rejections and characterize its convergence to the optimal hyperparameter under a mixture model. The proposed method exhibits significant power gains, particularly in cases with a conservative null distribution, which are common in composite null testing, or with a moderate proportion of weak nonnulls, as is typically observed in biological experiments with enrichment processes.},
  archive      = {J_BIOMET},
  author       = {Gao, Zijun},
  doi          = {10.1093/biomet/asae051},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae051},
  shortjournal = {Biometrika},
  title        = {An adaptive null proportion estimator for false discovery rate control},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A note on e-values and multiple testing. <em>BIOMET</em>, <em>112</em>(1), asae050. (<a href='https://doi.org/10.1093/biomet/asae050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discover a connection between the Benjamini–Hochberg procedure and the e -Benjamini–Hochberg procedure (Wang & Ramdas, 2022) with a suitably defined set of e -values. This insight extends to Storey’s procedure and generalized versions of the Benjamini–Hochberg procedure and the model-free multiple testing procedure of Barber & Candés (2015) with a general form of rejection rules. We further summarize these findings in a unified form. These connections open up new possibilities for designing multiple testing procedures in various contexts by aggregating e -values from different procedures or assembling e -values from different data subsets.},
  archive      = {J_BIOMET},
  author       = {Li, Guanxun and Zhang, Xianyang},
  doi          = {10.1093/biomet/asae050},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae050},
  shortjournal = {Biometrika},
  title        = {A note on e-values and multiple testing},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing the mean and variance by e-processes. <em>BIOMET</em>, <em>112</em>(1), asae049. (<a href='https://doi.org/10.1093/biomet/asae049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of testing the conditional mean and conditional variance for nonstationary data. We build e -values and p -values for four types of nonparametric composite hypothesis with specified mean and variance as well as other conditions on the shape of the data-generating distribution. These shape conditions include symmetry, unimodality and their combination. Using the obtained e -values and p -values, we construct tests via e -processes, also known as testing by betting, as well as some tests based on combining p -values for comparison. Although we mainly focus on one-sided tests, the two-sided test for the mean is also studied. Simulation and empirical studies are conducted under a few settings, and they illustrate features of the methods based on e -processes.},
  archive      = {J_BIOMET},
  author       = {Fan, Yixuan and Jiao, Zhanyi and Wang, Ruodu},
  doi          = {10.1093/biomet/asae049},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae049},
  shortjournal = {Biometrika},
  title        = {Testing the mean and variance by e-processes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting the power of kernel two-sample tests. <em>BIOMET</em>, <em>112</em>(1), asae048. (<a href='https://doi.org/10.1093/biomet/asae048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The kernel two-sample test based on the maximum mean discrepancy is one of the most popular methods for detecting differences between two distributions over general metric spaces. In this paper we propose a method to boost the power of the kernel test by combining maximum mean discrepancy estimates over multiple kernels using their Mahalanobis distance. We derive the asymptotic null distribution of the proposed test statistic and use a multiplier bootstrap approach to efficiently compute the rejection region. The resulting test is universally consistent and, since it is obtained by aggregating over a collection of kernels/bandwidths, is more powerful in detecting a wide range of alternatives in finite samples. We also derive the distribution of the test statistic for both fixed and local contiguous alternatives. The latter, in particular, implies that the proposed test is statistically efficient, that is, it has nontrivial asymptotic (Pitman) efficiency. The consistency properties of the Mahalanobis and other natural aggregation methods are also explored when the number of kernels is allowed to grow with the sample size. Extensive numerical experiments are performed on both synthetic and real-world datasets to illustrate the efficacy of the proposed method over single-kernel tests. The computational complexity of the proposed method is also studied, both theoretically and in simulations. Our asymptotic results rely on deriving the joint distribution of the maximum mean discrepancy estimates using the framework of multiple stochastic integrals, which is more broadly useful, specifically, in understanding the efficiency properties of recently proposed adaptive maximum mean discrepancy tests based on kernel aggregation and also in developing more computationally efficient, linear-time tests that combine multiple kernels. We conclude with an application of the Mahalanobis aggregation method for kernels with diverging scaling parameters.},
  archive      = {J_BIOMET},
  author       = {Chatterjee, A and Bhattacharya, B B},
  doi          = {10.1093/biomet/asae048},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae048},
  shortjournal = {Biometrika},
  title        = {Boosting the power of kernel two-sample tests},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: ‘A model-free variable screening method for optimal treatment regimes with high-dimensional survival data’. <em>BIOMET</em>, <em>112</em>(1), asae047. (<a href='https://doi.org/10.1093/biomet/asae047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asae047},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae047},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘A model-free variable screening method for optimal treatment regimes with high-dimensional survival data’},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local bootstrap for network data. <em>BIOMET</em>, <em>112</em>(1), asae046. (<a href='https://doi.org/10.1093/biomet/asae046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network analysis, one frequently needs to conduct inference for network parameters based on a single observed network. Since the sampling distribution of the statistic is often unknown, one has to rely on the bootstrap. However, because of the complex dependence structure among vertices, existing bootstrap methods often yield unsatisfactory performance, especially for small or moderate sample sizes. Here we propose a new network bootstrap procedure, termed the local bootstrap, for estimating the standard errors of network statistics. The method involves resampling the observed vertices along with their neighbour sets, and then reconstructing the edges between the resampled vertices by drawing from the set of edges connecting their neighbour sets. We justify the proposed method theoretically with desirable asymptotic properties for statistics such as motif density, and demonstrate its excellent numerical performance for small and moderate sample sizes. Our approach encompasses several existing methods, such as the empirical graphon bootstrap, as special cases. We investigate the advantages of the proposed method over existing methods in terms of edge randomness, vertex heterogeneity and neighbour set size, which can help to shed light on the complex issue of network bootstrapping.},
  archive      = {J_BIOMET},
  author       = {Zu, Tianhai and Qin, Yichen},
  doi          = {10.1093/biomet/asae046},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae046},
  shortjournal = {Biometrika},
  title        = {Local bootstrap for network data},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple bootstrap for chatterjee’s rank correlation. <em>BIOMET</em>, <em>112</em>(1), asae045. (<a href='https://doi.org/10.1093/biomet/asae045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that an m -out-of- n bootstrap procedure for Chatterjee’s rank correlation is consistent whenever asymptotic normality of Chatterjee’s rank correlation can be established. In particular, we prove that m -out-of- n bootstrap works for continuous as well as discrete data with independent coordinates; furthermore, simulations indicate that it also performs well for discrete data with dependent coordinates, and that it outperforms alternative estimation methods. Consistency of the bootstrap is proved in the Kolmogorov distance as well as in the Wasserstein distance.},
  archive      = {J_BIOMET},
  author       = {Dette, H and Kroll, M},
  doi          = {10.1093/biomet/asae045},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae045},
  shortjournal = {Biometrika},
  title        = {A simple bootstrap for chatterjee’s rank correlation},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity models and bounds under sequential unmeasured confounding in longitudinal studies. <em>BIOMET</em>, <em>112</em>(1), asae044. (<a href='https://doi.org/10.1093/biomet/asae044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider sensitivity analysis for causal inference in a longitudinal study with time-varying treatments and covariates. It is of interest to assess the worst-case possible values of counterfactual outcome means and average treatment effects under sequential unmeasured confounding. We formulate several multi-period sensitivity models to relax the corresponding versions of the assumption of sequential non-confounding. The primary sensitivity model involves only counterfactual outcomes, whereas the joint and product sensitivity models involve both counterfactual covariates and outcomes. We establish and compare explicit representations for the sharp and conservative bounds at the population level through convex optimization, depending only on the observed data. These results provide for the first time a satisfactory generalization from the marginal sensitivity model in the cross-sectional setting.},
  archive      = {J_BIOMET},
  author       = {Tan, Zhiqiang},
  doi          = {10.1093/biomet/asae044},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae044},
  shortjournal = {Biometrika},
  title        = {Sensitivity models and bounds under sequential unmeasured confounding in longitudinal studies},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Axiomatization of interventional probability distributions. <em>BIOMET</em>, <em>112</em>(1), asae043. (<a href='https://doi.org/10.1093/biomet/asae043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal intervention is an essential tool in causal inference. It is axiomatized under the rules of do-calculus in the case of structure causal models. We provide simple axiomatizations for families of probability distributions to be different types of interventional distributions. Our axiomatizations neatly lead to a simple and clear theory of causality that has several advantages: it does not need to make use of any modelling assumptions such as those imposed by structural causal models; it relies only on interventions on single variables; it includes most cases with latent variables and causal cycles; and, more importantly, it does not assume the existence of an underlying true causal graph as we do not take it as the primitive object; moreover, a causal graph is derived as a by-product of our theory. We show that, under our axiomatizations, the intervened distributions are Markovian to the defined intervened causal graphs, and an observed joint probability distribution is Markovian to the obtained causal graph; these results are consistent with the case of structural causal models, and as a result, the existing theory of causal inference applies. We also show that a large class of natural structural causal models satisfy the theory presented here. The aim of this paper is axiomatization of interventional families, which is subtly different from causal modelling.},
  archive      = {J_BIOMET},
  author       = {Sadeghi, Kayvan and Soo, Terry},
  doi          = {10.1093/biomet/asae043},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae043},
  shortjournal = {Biometrika},
  title        = {Axiomatization of interventional probability distributions},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple conditional randomization tests for lagged and spillover treatment effects. <em>BIOMET</em>, <em>112</em>(1), asae042. (<a href='https://doi.org/10.1093/biomet/asae042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of constructing multiple independent conditional randomization tests using a single dataset. Because the tests are independent, the randomization p -values can be interpreted individually and combined using standard methods for multiple testing. We give a simple, sequential construction of such tests and then discuss its application to three problems: Rosenbaum’s evidence factors for observational studies, lagged treatment effects in stepped-wedge trials, and spillover effects in randomized trials with interference. We compare the proposed approach with some existing methods using simulated and real datasets. Finally, we establish a more general sufficient condition for independent conditional randomization tests.},
  archive      = {J_BIOMET},
  author       = {Zhang, Yao and Zhao, Qingyuan},
  doi          = {10.1093/biomet/asae042},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae042},
  shortjournal = {Biometrika},
  title        = {Multiple conditional randomization tests for lagged and spillover treatment effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating causal effects under non-individualistic treatments due to network entanglement. <em>BIOMET</em>, <em>112</em>(1), asae041. (<a href='https://doi.org/10.1093/biomet/asae041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many observational studies, the treatment assignment mechanism is not individualistic, as it allows the probability of treatment of a unit to depend on quantities beyond the unit’s covariates. In such settings, unit treatments may be entangled in complex ways. In this article, we consider a particular instance of this problem where the treatments are entangled by a social network among units. For instance, when studying the effects of peer interaction on a social media platform, the treatment on a unit depends on the change of the interactions network over time. A similar situation is encountered in many economic studies, such as those examining the effects of bilateral trade partnerships on countries’ economic growth. The challenge in these settings is that individual treatments depend on a global network that may change in a way that is endogenous and cannot be manipulated experimentally. In this paper, we show that classical propensity score methods that ignore entanglement may lead to large bias and wrong inference of causal effects. We then propose a solution that involves calculating propensity scores by marginalizing over the network change. Under an appropriate ignorability assumption, this leads to unbiased estimates of the treatment effect of interest. We also develop a randomization-based inference procedure that takes entanglement into account. Under general conditions on network change, this procedure can deliver valid inference without explicitly modelling the network. We establish theoretical results for the proposed methods and illustrate their behaviour via simulation studies based on real-world network data. We also revisit a large-scale observational dataset on contagion of online user behaviour, showing that ignoring entanglement may inflate estimates of peer influence.},
  archive      = {J_BIOMET},
  author       = {Toulis, P and Volfovsky, A and Airoldi, E M},
  doi          = {10.1093/biomet/asae041},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae041},
  shortjournal = {Biometrika},
  title        = {Estimating causal effects under non-individualistic treatments due to network entanglement},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variance-based sensitivity analysis for weighting estimators results in more informative bounds. <em>BIOMET</em>, <em>112</em>(1), asae040. (<a href='https://doi.org/10.1093/biomet/asae040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighting methods are popular tools for estimating causal effects, and assessing their robustness under unobserved confounding is important in practice. Current approaches to sensitivity analyses rely on bounding a worst-case error from omitting a confounder. In this paper, we introduce a new sensitivity model called the variance-based sensitivity model , which instead bounds the distributional differences that arise in the weights from omitting a confounder. The variance-based sensitivity model can be parameterized by an R 2 parameter that is both standardized and bounded. We demonstrate, both empirically and theoretically, that the variance-based sensitivity model provides improvements on the stability of the sensitivity analysis procedure over existing methods. We show that by moving away from worst-case bounds, we are able to obtain more interpretable and informative bounds. We illustrate our proposed approach on a study examining blood mercury levels using the National Health and Nutrition Examination Survey.},
  archive      = {J_BIOMET},
  author       = {Huang, Melody and Pimentel, Samuel D},
  doi          = {10.1093/biomet/asae040},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae040},
  shortjournal = {Biometrika},
  title        = {Variance-based sensitivity analysis for weighting estimators results in more informative bounds},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Studies in the history of probability and statistics, LI: The first conditional logistic regression. <em>BIOMET</em>, <em>112</em>(1), asae038. (<a href='https://doi.org/10.1093/biomet/asae038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statisticians and epidemiologists generally cite the publications of Prentice & Breslow (1978) and Breslow et al. (1978) as the first description and use of conditional logistic regression, while economists cite the book chapter by Nobel laureate McFadden ( McFadden, 1973 ). We describe the until-now-unrecognized use of, and way of fitting, this model in 1934 by Lionel Penrose and Ronald Fisher.},
  archive      = {J_BIOMET},
  author       = {Hanley, J A},
  doi          = {10.1093/biomet/asae038},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae038},
  shortjournal = {Biometrika},
  title        = {Studies in the history of probability and statistics, LI: The first conditional logistic regression},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal inference with hidden mediators. <em>BIOMET</em>, <em>112</em>(1), asae037. (<a href='https://doi.org/10.1093/biomet/asae037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proximal causal inference was recently proposed as a framework to identify causal effects from observational data in the presence of hidden confounders for which proxies are available. In this paper, we extend the proximal causal inference approach to settings where identification of causal effects hinges upon a set of mediators that are not observed, yet error prone proxies of the hidden mediators are measured. Specifically, (i) we establish causal hidden mediation analysis, which extends classical causal mediation analysis methods for identifying natural direct and indirect effects under no unmeasured confounding to a setting where the mediator of interest is hidden, but proxies of it are available; (ii) we establish a hidden front-door criterion, criterion to allow for hidden mediators for which proxies are available; (iii) we show that the identification of a certain causal effect called the population intervention indirect effect remains possible with hidden mediators in settings where challenges in (i) and (ii) might co-exist. We view (i)–(iii) as important steps towards the practical application of front-door criteria and mediation analysis as mediators are almost always measured with error and, thus, the most one can hope for in practice is that the measurements are at best proxies of mediating mechanisms. We propose identification approaches for the parameters of interest in our considered models. For the estimation aspect, we propose an influence function-based estimation method and provide an analysis for the robustness of the estimators.},
  archive      = {J_BIOMET},
  author       = {Ghassami, Amiremad and Yang, Alan and Shpitser, Ilya and Tchetgen Tchetgen, Eric},
  doi          = {10.1093/biomet/asae037},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae037},
  shortjournal = {Biometrika},
  title        = {Causal inference with hidden mediators},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust covariate-balancing method for learning optimal individualized treatment regimes. <em>BIOMET</em>, <em>112</em>(1), asae036. (<a href='https://doi.org/10.1093/biomet/asae036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important problems in precision medicine is to find the optimal individualized treatment rule, which is designed to recommend treatment decisions and maximize overall clinical benefit to patients based on their individual characteristics. Typically, the expected clinical outcome is required to be estimated first, for which an outcome regression model or a propensity score model usually needs to be assumed with most existing statistical methods. However, if either model assumption is invalid, the estimated treatment regime will not be reliable. In this article, we first define a contrast value function, which forms the basis for the study of individualized treatment regimes. Then we construct a hybrid estimator of the contrast value function by combining two types of estimation methods. We further propose a robust covariate-balancing estimator of the contrast value function by combining the inverse probability weighted method and matching method, which is based on the covariate balancing propensity score proposed by Imai & Ratkovic (2014) . Theoretical results show that the proposed estimator is doubly robust, ie, it is consistent if either the propensity score model or the matching is correct. Based on a large number of simulation studies, we demonstrate that the proposed estimator outperforms existing methods. Application of the proposed method is illustrated through analysis of the SUPPORT study.},
  archive      = {J_BIOMET},
  author       = {Li, Canhui and Zeng, Donglin and Zhu, Wensheng},
  doi          = {10.1093/biomet/asae036},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae036},
  shortjournal = {Biometrika},
  title        = {A robust covariate-balancing method for learning optimal individualized treatment regimes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric efficiency gains from parametric restrictions on propensity scores. <em>BIOMET</em>, <em>112</em>(1), asae034. (<a href='https://doi.org/10.1093/biomet/asae034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore how much knowing a parametric restriction on propensity scores improves semiparametric efficiency bounds in the potential outcome framework. For stratified propensity scores, considered as a parametric model, we derive explicit formulas for the efficiency gain from knowing how the covariate space is split. Based on these, we find that the efficiency gain decreases as the partition of the stratification becomes finer. For general parametric models, where it is hard to obtain explicit representations of efficiency bounds, we propose a novel framework that enables us to see whether knowing a parametric model is valuable in terms of efficiency even when it is high dimensional. In addition to the intuitive fact that knowing the parametric model does not help much if it is sufficiently flexible, we discover that the efficiency gain can be nearly zero even though the parametric assumption significantly restricts the space of possible propensity scores.},
  archive      = {J_BIOMET},
  author       = {Kono, Haruki},
  doi          = {10.1093/biomet/asae034},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae034},
  shortjournal = {Biometrika},
  title        = {Semiparametric efficiency gains from parametric restrictions on propensity scores},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
