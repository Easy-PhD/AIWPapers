<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>oxford</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomet">BIOMET - 69</h2>
<ul>
<li><details>
<summary>
(2025). Identifying and bounding the probability of necessity for causes of effects with ordinal outcomes. <em>BIOMET</em>, <em>112</em>(3), asaf049. (<a href='https://doi.org/10.1093/biomet/asaf049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the existing causal inference literature focuses on the forward-looking perspective by estimating effects of causes, the backward-looking perspective can provide insights into causes of effects. In backward-looking causal inference, the probability of necessity measures the probability that a certain event is caused by the treatment, given the observed treatment and outcome. Most existing results focus on binary outcomes. Motivated by applications with ordinal outcomes, we propose a general definition of the probability of necessity. However, identifying the probability of necessity is challenging because it involves the joint distribution of the potential outcomes. We propose the novel assumption of a monotonic incremental treatment effect to identify the probability of necessity with ordinal outcomes. We also discuss the testable implications of this key identification assumption. When it fails, we derive explicit formulas of the sharp large-sample bounds on the probability of necessity.},
  archive      = {J_BIOMET},
  author       = {Zhang, Chao and Geng, Zhi and Li, Wei and Ding, Peng},
  doi          = {10.1093/biomet/asaf049},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf049},
  shortjournal = {Biometrika},
  title        = {Identifying and bounding the probability of necessity for causes of effects with ordinal outcomes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent and scalable composite likelihood estimation of probit models with crossed random effects. <em>BIOMET</em>, <em>112</em>(3), asaf037. (<a href='https://doi.org/10.1093/biomet/asaf037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of crossed random effects models commonly incurs computational costs that grow faster than linearly in the sample size N ⁠ , often as fast as Ω ( N 3 / 2 ) ⁠ , making them unsuitable for large datasets. For non-Gaussian responses, integrating out the random effects to obtain a marginal likelihood poses significant challenges, especially for high-dimensional integrals for which the Laplace approximation may not be accurate. In this article we develop a composite likelihood approach to probit models that replaces the crossed random effects model with some hierarchical models that require only one-dimensional integrals. We show how to consistently estimate the crossed effects model parameters from the hierarchical model fits. We find that the computation scales linearly in the sample size. The method is illustrated by applying it to approximately five million observations from Stitch Fix, where the crossed effects formulation would require an integral of dimension larger than 700 000 ⁠ .},
  archive      = {J_BIOMET},
  author       = {Bellio, R and Ghosh, S and Owen, A B and Varin, C},
  doi          = {10.1093/biomet/asaf037},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf037},
  shortjournal = {Biometrika},
  title        = {Consistent and scalable composite likelihood estimation of probit models with crossed random effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive performance of power posteriors. <em>BIOMET</em>, <em>112</em>(3), asaf034. (<a href='https://doi.org/10.1093/biomet/asaf034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the impact of using tempered likelihoods in the production of posterior predictions. While the choice of temperature has an impact on predictive performance in small samples, we formally show that in moderate-to-large samples, tempering does not impact posterior predictions.},
  archive      = {J_BIOMET},
  author       = {McLatchie, Y and Fong, E and Frazier, D T and Knoblauch, J},
  doi          = {10.1093/biomet/asaf034},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf034},
  shortjournal = {Biometrika},
  title        = {Predictive performance of power posteriors},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bias correction of quadratic spectral estimators. <em>BIOMET</em>, <em>112</em>(3), asaf033. (<a href='https://doi.org/10.1093/biomet/asaf033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three cardinal, statistically consistent families of nonparametric estimators for the power spectral density of a time series are the lag-window, multitaper and Welch estimators. However, when estimating power spectral densities from a finite sample, each can be subject to nonignorable bias. Astfalck et al. (2024) developed a method that offers significant bias reduction for finite samples for Welch’s estimator, which this article extends to the larger family of quadratic estimators, thus providing similar theory for bias correction of lag-window and multitaper estimators as well as combinations thereof. Importantly, this theory may be used in conjunction with any and all tapers and lag-sequences designed for bias reduction, and so should be seen as an extension to valuable work in these fields, rather than a supplanting methodology. The order of computation is larger than the |$ {O}(n\log n) $| which is typical in spectral analyses, but it is not insurmountable in practice. Simulation studies support the theory with comparisons across variations of quadratic estimators.},
  archive      = {J_BIOMET},
  author       = {Astfalck, Lachlan C and Sykulski, Adam M and Cripps, Edward J},
  doi          = {10.1093/biomet/asaf033},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf033},
  shortjournal = {Biometrika},
  title        = {Bias correction of quadratic spectral estimators},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integer programming for learning directed acyclic graphs from nonidentifiable gaussian models. <em>BIOMET</em>, <em>112</em>(3), asaf032. (<a href='https://doi.org/10.1093/biomet/asaf032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of learning directed acyclic graphs from continuous observational data, generated according to a linear Gaussian structural equation model. State-of-the-art structure learning methods for this setting have at least one of the following shortcomings: (i) they cannot provide optimality guarantees and can suffer from learning suboptimal models; (ii) they rely on the stringent assumption that the noise is homoscedastic, and hence the underlying model is fully identifiable. We overcome these shortcomings and develop a computationally efficient mixed-integer programming framework for learning medium-sized problems that accounts for arbitrary heteroscedastic noise. We present an early stopping criterion under which we can terminate the branch-and-bound procedure to achieve an asymptotically optimal solution and establish the consistency of this approximate solution. In addition, we show via numerical experiments that our method outperforms state-of-the-art algorithms and is robust to noise heteroscedasticity, whereas the performance of some competing methods deteriorates under strong violations of the identifiability assumption. The software implementation of our method is available as the Python package micodag .},
  archive      = {J_BIOMET},
  author       = {Xu, Tong and Taeb, Armeen and Küçükyavuz, Simge and Shojaie, Ali},
  doi          = {10.1093/biomet/asaf032},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf032},
  shortjournal = {Biometrika},
  title        = {Integer programming for learning directed acyclic graphs from nonidentifiable gaussian models},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general form of covariate adjustment in clinical trials under covariate-adaptive randomization. <em>BIOMET</em>, <em>112</em>(3), asaf029. (<a href='https://doi.org/10.1093/biomet/asaf029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized clinical trials, adjusting for baseline covariates can improve credibility and efficiency for demonstrating and quantifying treatment effects. This article studies the augmented inverse propensity weighted estimator, which is a general form of covariate adjustment that uses linear, generalized linear and nonparametric or machine learning models for the conditional mean of the response given covariates. Under covariate-adaptive randomization, we establish general theorems that show a complete picture of the asymptotic normality, efficiency gain and applicability of augmented inverse propensity weighted estimators. In particular, we provide for the first time a rigorous theoretical justification of using machine learning methods with cross-fitting for dependent data under covariate-adaptive randomization. Based on the general theorems, we offer insights on the conditions for guaranteed efficiency gain and universal applicability under different randomization schemes, which also motivate a joint calibration strategy using some constructed covariates after applying augmented inverse propensity weighted estimators.},
  archive      = {J_BIOMET},
  author       = {Bannick, Marlena S and Shao, Jun and Liu, Jingyi and Du, Yu and Yi, Yanyao and Ye, Ting},
  doi          = {10.1093/biomet/asaf029},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf029},
  shortjournal = {Biometrika},
  title        = {A general form of covariate adjustment in clinical trials under covariate-adaptive randomization},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic factor analysis of high-dimensional recurrent events. <em>BIOMET</em>, <em>112</em>(3), asaf028. (<a href='https://doi.org/10.1093/biomet/asaf028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event time data arise in many studies, including in biomedicine, public health, marketing and social media analysis. High-dimensional recurrent event data involving many event types and observations have become prevalent with advances in information technology. This article proposes a semiparametric dynamic factor model for the dimension reduction of high-dimensional recurrent event data. The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies. A nearly rate-optimal smoothing-based estimator is proposed. An information criterion that consistently selects the number of factors is also developed. Simulation studies demonstrate the effectiveness of these inference tools. The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained.},
  archive      = {J_BIOMET},
  author       = {Chen, F and Chen, Y and Ying, Z and Zhou, K},
  doi          = {10.1093/biomet/asaf028},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf028},
  shortjournal = {Biometrika},
  title        = {Dynamic factor analysis of high-dimensional recurrent events},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving efficiency in transporting average treatment effects. <em>BIOMET</em>, <em>112</em>(3), asaf027. (<a href='https://doi.org/10.1093/biomet/asaf027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop flexible, semiparametric estimators of the average treatment effect transported to a new target population, which offer potential efficiency gains. Transport may be of value when the average treatment effect may differ across populations. We consider the setting where differences in the average treatment effect are due to differences in the distribution of effect modifiers, baseline covariates that modify the treatment effect. First, we propose a collaborative one-step semiparametric estimator that can improve efficiency. This approach does not require researchers to have knowledge about which covariates are effect modifiers and which differ in distribution between the populations, but it does require all covariates to be measured in the target population. Second, we propose two one-step semiparametric estimators that assume knowledge of which covariates are effect modifiers and which are both effect modifiers and differentially distributed between the populations. These estimators can be used even when not all covariates are observed in the target population; one estimator requires that only effect modifiers be observed, and the other requires that only those modifiers that are also differentially distributed be observed. We use simulations to compare the finite-sample performance of our proposed estimators and an existing semiparametric estimator of the transported average treatment effect, including in the presence of practical violations of the positivity assumption. Lastly, we apply our proposed estimators to a large-scale housing trial.},
  archive      = {J_BIOMET},
  author       = {Rudolph, K E and Williams, N T and Stuart, E A and Díaz, I},
  doi          = {10.1093/biomet/asaf027},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf027},
  shortjournal = {Biometrika},
  title        = {Improving efficiency in transporting average treatment effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general condition for bias attenuation by a nondifferentially mismeasured confounder. <em>BIOMET</em>, <em>112</em>(3), asaf026. (<a href='https://doi.org/10.1093/biomet/asaf026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world studies, the collected confounders may suffer from measurement error. Although mismeasurement of confounders is typically unintentional (originating from sources such as human oversight or imprecise machinery), deliberate mismeasurement also occurs and is becoming increasingly more common. For example, in the 2020 U.S. census, noise was added to measurements to assuage privacy concerns. Sensitive variables such as income or age are often partially censored and are only known up to a range of values. In such settings, obtaining valid estimates of the causal effect of a binary treatment can be impossible, as mismeasurement of confounders constitutes a violation of the no-unmeasured-confounding assumption. A natural question is whether the common practice of simply adjusting for the mismeasured confounder is justifiable. In this article, we answer this question in the affirmative and demonstrate that in many realistic scenarios not covered by previous literature, adjusting for the mismeasured confounders reduces bias compared to not adjusting.},
  archive      = {J_BIOMET},
  author       = {Zhang, Jeffrey and Lee, Junu},
  doi          = {10.1093/biomet/asaf026},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf026},
  shortjournal = {Biometrika},
  title        = {A general condition for bias attenuation by a nondifferentially mismeasured confounder},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: ‘Network cross-validation by edge sampling’. <em>BIOMET</em>, <em>112</em>(3), asaf023. (<a href='https://doi.org/10.1093/biomet/asaf023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asaf023},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf023},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Network cross-validation by edge sampling’},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning for piecewise-constant mean estimation: Optimality, ℓ1 and ℓ0 penalization. <em>BIOMET</em>, <em>112</em>(3), asaf018. (<a href='https://doi.org/10.1093/biomet/asaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study transfer learning for estimating piecewise-constant signals when source data, which may be relevant but disparate, are available in addition to target data. We first investigate transfer learning estimators that respectively employ ℓ 1 and ℓ 0 penalties for unisource data scenarios and then generalize these estimators to accommodate multisources. To further reduce estimation errors, especially when some sources significantly differ from the target, we introduce an informative source selection algorithm. We then examine these estimators with multisource selection and establish their minimax optimality. Unlike the common narrative in the transfer learning literature that the performance is enhanced through large source sample sizes, our approaches leverage higher observational frequencies and accommodate diverse frequencies across multiple sources. Our extensive numerical experiments show that the proposed transfer learning estimators significantly improve estimation performance compared to estimators that only use the target data.},
  archive      = {J_BIOMET},
  author       = {Wang, F and Yu, Y},
  doi          = {10.1093/biomet/asaf018},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf018},
  shortjournal = {Biometrika},
  title        = {Transfer learning for piecewise-constant mean estimation: Optimality, ℓ1 and ℓ0 penalization},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spike-and-slab prior for dimension selection in generalized linear network eigenmodels. <em>BIOMET</em>, <em>112</em>(3), asaf014. (<a href='https://doi.org/10.1093/biomet/asaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent space models are often used to model network data by embedding a network’s nodes into a low-dimensional latent space; however, choosing the dimension of this space remains a challenge. To this end, we begin by formalizing a class of latent space models we call generalized linear network eigenmodels that can model various edge types (binary, ordinal, nonnegative continuous) found in scientific applications. This model class subsumes the traditional eigenmodel by embedding it in a generalized linear model with an exponential dispersion family random component and fixes identifiability issues that hindered interpretability. We propose a Bayesian approach to dimension selection for generalized linear network eigenmodels based on an ordered spike-and-slab prior that provides improved dimension estimation and satisfies several appealing theoretical properties. We show that the model’s posterior is consistent and concentrates on low-dimensional models near the truth. We demonstrate our approach’s consistent dimension selection on simulated networks, and we use generalized linear network eigenmodels to study the effect of covariates on the formation of networks from biology, ecology and economics and the existence of residual latent structure.},
  archive      = {J_BIOMET},
  author       = {Loyal, Joshua D and Chen, Yuguo},
  doi          = {10.1093/biomet/asaf014},
  journal      = {Biometrika},
  number       = {3},
  pages        = {asaf014},
  shortjournal = {Biometrika},
  title        = {A spike-and-slab prior for dimension selection in generalized linear network eigenmodels},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling directed networks with reciprocity. <em>BIOMET</em>, <em>112</em>(2), asaf035. (<a href='https://doi.org/10.1093/biomet/asaf035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetric relational data are becoming increasingly prevalent in diverse fields, underscoring the need for developing directed network models to address the complex challenges posed by the unique structure of such data. Unlike undirected models, directed models can capture reciprocity, the tendency of nodes to form mutual links. This work addresses a fundamental question: what is the effective sample size for modelling reciprocity? We examine this question by analysing the Bernoulli model with reciprocity, allowing for varying sparsity levels between non-reciprocal and reciprocal effects. We then extend this framework to a model that incorporates node-specific heterogeneity and link-specific reciprocity using covariates. Our findings reveal the intriguing interplay between non-reciprocal and reciprocal effects in sparse networks. We propose a straightforward inference procedure based on maximum likelihood estimation that operates without prior knowledge of sparsity levels, whether covariates are included or not.},
  archive      = {J_BIOMET},
  author       = {Feng, Rui and Leng, Chenlei},
  doi          = {10.1093/biomet/asaf035},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf035},
  shortjournal = {Biometrika},
  title        = {Modelling directed networks with reciprocity},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact sampling of spanning trees via fast-forwarded random walks. <em>BIOMET</em>, <em>112</em>(2), asaf031. (<a href='https://doi.org/10.1093/biomet/asaf031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree graphs are used routinely in statistics. When estimating a Bayesian model with a tree component, sampling the posterior remains a core difficulty. Existing Markov chain Monte Carlo methods tend to rely on local moves, often leading to poor mixing. A promising approach is to instead directly sample spanning trees on an auxiliary graph. Current spanning tree samplers, such as the celebrated Aldous–Broder algorithm, rely predominantly on simulating random walks that are required to visit all the nodes of the graph. Such algorithms are prone to getting stuck in certain subgraphs. We formalize this phenomenon using the bottlenecks in the random walk’s transition probability matrix. We then propose a novel fast-forwarded cover algorithm that can break free from bottlenecks. The core idea is a marginalization argument that leads to a closed-form expression that allows for fast-forwarding to the event of visiting a new node. Unlike many existing approximation algorithms, our algorithm yields exact samples. We demonstrate the enhanced efficiency of the fast-forwarded cover algorithm, and illustrate its application in fitting a Bayesian dendrogram model on a Massachusetts crime and community dataset.},
  archive      = {J_BIOMET},
  author       = {Tam, Edric and Dunson, David B and Duan, Leo L},
  doi          = {10.1093/biomet/asaf031},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf031},
  shortjournal = {Biometrika},
  title        = {Exact sampling of spanning trees via fast-forwarded random walks},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the minimax robustness against correlation and heteroscedasticity of ordinary least squares among generalized least squares estimates of regression. <em>BIOMET</em>, <em>112</em>(2), asaf025. (<a href='https://doi.org/10.1093/biomet/asaf025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit a result according to which certain functions of covariance matrices are maximized at scalar multiples of the identity matrix. In a statistical context in which such functions measure loss, this says that the least favourable form of dependence is in fact independence, so that a procedure optimal for independent and identically distributed data can be minimax. In particular, the ordinary least squares estimate of a correctly specified regression response is minimax among generalized least squares estimates, when the maximum is taken over certain classes of error covariance structures and the loss function possesses a natural monotonicity property. In regression models whose response function is possibly misspecified, ordinary least squares is minimax if the design is uniform on its support, but this often fails otherwise. An investigation of the interplay between minimax generalized least squares procedures and minimax designs leads us to extend, to robustness against dependencies, an existing observation: that robustness against model misspecifications is increased by splitting replicates into clusters of observations at nearby locations.},
  archive      = {J_BIOMET},
  author       = {Wiens, Douglas P},
  doi          = {10.1093/biomet/asaf025},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf025},
  shortjournal = {Biometrika},
  title        = {On the minimax robustness against correlation and heteroscedasticity of ordinary least squares among generalized least squares estimates of regression},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric data segmentation in multivariate time series via joint characteristic functions. <em>BIOMET</em>, <em>112</em>(2), asaf024. (<a href='https://doi.org/10.1093/biomet/asaf024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern time series data often exhibit complex dependence and structural changes that are not easily characterized by shifts in the mean or model parameters. We propose a nonparametric data segmentation methodology for multivariate time series. By considering joint characteristic functions between the time series and its lagged values, our proposed method is able to detect changepoints in the marginal distribution, but also those in possibly nonlinear serial dependence, all without the need to prespecify the type of changes. We show the theoretical consistency of our method in estimating the total number and the locations of the changepoints, and demonstrate its good performance against a variety of changepoint scenarios. We further demonstrate its usefulness in applications to seismology and economic time series.},
  archive      = {J_BIOMET},
  author       = {McGonigle, E T and Cho, H},
  doi          = {10.1093/biomet/asaf024},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf024},
  shortjournal = {Biometrika},
  title        = {Nonparametric data segmentation in multivariate time series via joint characteristic functions},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian inference for generalized linear models via quasi-posteriors. <em>BIOMET</em>, <em>112</em>(2), asaf022. (<a href='https://doi.org/10.1093/biomet/asaf022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized linear models are routinely used for modelling relationships between a response variable and a set of covariates. The simple form of a generalized linear model comes with easy interpretability, but also leads to concerns about model misspecification impacting inferential conclusions. A popular semiparametric solution adopted in the frequentist literature is quasilikelihood, which improves robustness by only requiring correct specification of the first two moments. We develop a robust approach to Bayesian inference in generalized linear models through quasi-posterior distributions. We show that quasi-posteriors provide a coherent generalized Bayes inference method, while also approximating so-called coarsened posteriors. In so doing, we obtain new insights into the choice of coarsening parameter. Asymptotically, the quasi-posterior converges in total variation to a normal distribution and has important connections with the loss-likelihood bootstrap posterior. We demonstrate that it is also well calibrated in terms of frequentist coverage. Moreover, the loss-scale parameter has a clear interpretation as a dispersion, and this leads to a consolidated method-of-moments estimator.},
  archive      = {J_BIOMET},
  author       = {Agnoletto, D and Rigon, T and Dunson, D B},
  doi          = {10.1093/biomet/asaf022},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf022},
  shortjournal = {Biometrika},
  title        = {Bayesian inference for generalized linear models via quasi-posteriors},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved estimator of the pair correlation function of a spatial point process. <em>BIOMET</em>, <em>112</em>(2), asaf021. (<a href='https://doi.org/10.1093/biomet/asaf021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pair correlation function, or two-point correlation, of a spatial point process is a fundamental tool in spatial statistics and astrostatistics, measuring the strength of spatial dependence between points. Interest is focused on the behaviour of this function at short distances, but this is the region in which existing estimators can be particularly unreliable. We propose a new estimator of the pair correlation function based on techniques from stochastic geometry and kernel density estimation. Theory and simulation experiments confirm that the new estimator is far superior to existing estimators, especially at short distances, when the underlying point process is clustered or completely spatially random. Extensions of the estimator are developed for inhomogeneous point processes, for spatially inhibited (negatively correlated) processes and for cases where the form of the pair correlation function is known approximately. We address practical issues including boundary correction, bandwidth selection and data-based choice of technique. Real data examples, of shelling in Ukraine and meningococcal disease in Germany, demonstrate that the new estimator has substantial impact on the interpretation of data.},
  archive      = {J_BIOMET},
  author       = {Baddeley, A and Davies, T M and Hazelton, M L},
  doi          = {10.1093/biomet/asaf021},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf021},
  shortjournal = {Biometrika},
  title        = {An improved estimator of the pair correlation function of a spatial point process},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The only admissible way of merging arbitrary e-values. <em>BIOMET</em>, <em>112</em>(2), asaf020. (<a href='https://doi.org/10.1093/biomet/asaf020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper it is proved that the only admissible way of merging arbitrary |$ e $| -values is to use a weighted arithmetic average. This result completes the picture of merging methods for arbitrary |$ e $| -values and generalizes the result of Vovk & Wang (2021) that the only admissible way of symmetrically merging |$ e $| -values is to use the arithmetic average combined with a constant. Although the proved statement is naturally anticipated, its proof relies on a sophisticated application of optimal transport duality and a minimax theorem.},
  archive      = {J_BIOMET},
  author       = {Wang, Ruodu},
  doi          = {10.1093/biomet/asaf020},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf020},
  shortjournal = {Biometrika},
  title        = {The only admissible way of merging arbitrary e-values},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the fundamental limitations of multi-proposal markov chain monte carlo algorithms. <em>BIOMET</em>, <em>112</em>(2), asaf019. (<a href='https://doi.org/10.1093/biomet/asaf019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study multi-proposal Markov chain Monte Carlo algorithms, such as multiple-try or generalized Metropolis–Hastings schemes, which have recently received renewed attention due to their amenability to parallel computing. First, we prove that no multi-proposal scheme can speed up convergence relative to the corresponding single-proposal scheme by more than a factor of |$ K $|⁠ , where |$ K $| denotes the number of proposals at each iteration. This result applies to arbitrary target distributions and it implies that common serial multi-proposal implementations are less efficient than single-proposal ones. Second, we consider log-concave distributions over Euclidean spaces, proving that, in this case, the speed-up is at most logarithmic in |$ K $|⁠ , which implies that even parallel multi-proposal implementations are fundamentally limited in the computational gain they can offer. Crucially, our results apply to arbitrary multi-proposal schemes and purely rely on the two-step structure of the associated kernels: first generate |$ K $| candidate points, then select one among those. Our theoretical findings are validated through numerical simulations.},
  archive      = {J_BIOMET},
  author       = {Pozza, F and Zanella, G},
  doi          = {10.1093/biomet/asaf019},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf019},
  shortjournal = {Biometrika},
  title        = {On the fundamental limitations of multi-proposal markov chain monte carlo algorithms},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample splitting and assessing goodness-of-fit of time series. <em>BIOMET</em>, <em>112</em>(2), asaf017. (<a href='https://doi.org/10.1093/biomet/asaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental and often final step in time series modelling is to assess the quality of fit of a proposed model to the data. Since the underlying distribution of the innovations that generate a model is often not prescribed, goodness-of-fit tests typically take the form of testing the fitted residuals for serial independence. However, these fitted residuals are intrinsically dependent since they are based on the same parameter estimates, and thus standard tests of serial independence, such as those based on the autocorrelation function or auto-distance correlation function of the fitted residuals, need to be adjusted. The sample-splitting procedure of Pfister et al. (2018) is one such fix for the case of models for independent data, but fails to work in the dependent setting. In this article, sample splitting is leveraged in the time series setting to perform tests of serial dependence of fitted residuals using the autocorrelation function and auto-distance correlation function. The first f n of the data points are used to estimate the parameters of the model and then, using these parameter estimates, the last l n of the data points are used to compute the estimated residuals. Tests for serial independence are then based on these l n residuals. As long as the overlap between the f n and l n data splits is asymptotically 1 / 2 ⁠ , the autocorrelation function and auto-distance correlation function tests of serial independence often have the same limit distributions as when the underlying residuals are indeed independent and identically distributed. In particular, if the first half of the data is used to estimate the parameters and the estimated residuals are computed for the entire dataset based on these parameter estimates, then the autocorrelation function and auto-distance correlation function can have the same limit distributions as if the residuals were independent and identically distributed. This procedure ameliorates the need for adjustment in the construction of confidence bounds for both the autocorrelation function and the auto-distance correlation function in goodness-of-fit testing.},
  archive      = {J_BIOMET},
  author       = {Davis, Richard A and Fernandes, Leon},
  doi          = {10.1093/biomet/asaf017},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf017},
  shortjournal = {Biometrika},
  title        = {Sample splitting and assessing goodness-of-fit of time series},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the partial autocorrelation function for locally stationary time series: Characterization, estimation and inference. <em>BIOMET</em>, <em>112</em>(2), asaf016. (<a href='https://doi.org/10.1093/biomet/asaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For stationary time series, it is common to use plots of the partial autocorrelation function (PACF) or PACF-based tests to explore the temporal dependence structure of the process. To the best of our knowledge, analogues for nonstationary time series have not yet been fully developed. This article aims to fill this gap for locally stationary time series with short-range dependence. First, we characterize the PACF locally in the time domain and show that the j th PACF decays with j at a rate that adapts to the temporal dependence of the time series |$ \{x_{i,n}\} $|⁠ . Second, at each time |$ i, $| inspired by Killick et al. (2020) . We show that the PACF can be efficiently approximated by the best linear prediction coefficients via the Yule–Walker equations. This allows us to study the PACF via ordinary least squares locally. Third, we show that the PACF is smooth in time for locally stationary time series. We use the sieve method with ordinary least squares to estimate the PACF and construct some statistics to test the PACF and infer the structure of the time series. These tests generalize and modify those used in Brockwell & Davis (1987) for stationary time series. Finally, a multiplier bootstrap algorithm is proposed for practical implementation and an R package Sie2nts is provided to implement the algorithm. Numerical simulations and real-data analysis confirm the usefulness of our results.},
  archive      = {J_BIOMET},
  author       = {Ding, Xiucai and Zhou, Zhou},
  doi          = {10.1093/biomet/asaf016},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf016},
  shortjournal = {Biometrika},
  title        = {On the partial autocorrelation function for locally stationary time series: Characterization, estimation and inference},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous-time locally stationary wavelet processes. <em>BIOMET</em>, <em>112</em>(2), asaf015. (<a href='https://doi.org/10.1093/biomet/asaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the class of continuous-time locally stationary wavelet processes. Continuous-time models enable us to properly provide scale-based time series models for irregularly spaced observations for the first time, while also permitting a spectral representation of the process over a continuous range of scales. We derive results for both the theoretical setting, where we assume access to the entire process sample path, and a more practical one, which develops methods for estimating the quantities of interest from sampled time series. The latter estimates are accurately computable in reasonable time by solving the relevant linear integral equation using the iterative soft-thresholding algorithm of Daubechies et al. (2004). Appropriate smoothing techniques are also developed and applied in this new setting. Comparisons to previous methods are conducted on the heart rate time series of a sleeping infant. Additionally, we exemplify our new methods by computing spectral and autocovariance estimates on irregularly spaced heart rate data obtained from a recent sleep-state study.},
  archive      = {J_BIOMET},
  author       = {Palasciano, H A and Knight, M I and Nason, G P},
  doi          = {10.1093/biomet/asaf015},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf015},
  shortjournal = {Biometrika},
  title        = {Continuous-time locally stationary wavelet processes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomization inference when n equals one. <em>BIOMET</em>, <em>112</em>(2), asaf013. (<a href='https://doi.org/10.1093/biomet/asaf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades, N -of-1 experiments, where a unit serves as its own control and treatment in different time windows, have been used in certain medical contexts. However, due to effects that accumulate over long time windows and interventions that have complex evolution, a lack of robust inference tools has limited the widespread applicability of such N -of-1 designs. This work combines techniques from experimental design in causal inference and system identification from control theory to provide such an inference framework. We derive a model of the dynamic interference effect that arises in linear time-invariant dynamical systems. We show that a family of causal estimands analogous to those studied in potential outcomes are estimable via a standard estimator derived from the method of moments. We derive formulae for higher moments of this estimator and describe conditions under which N -of-1 designs may provide faster ways to estimate the effects of interventions in dynamical systems. We also provide conditions under which our estimator is asymptotically normal and derive valid confidence intervals for this setting.},
  archive      = {J_BIOMET},
  author       = {Liang, Tengyuan and Recht, Benjamin},
  doi          = {10.1093/biomet/asaf013},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf013},
  shortjournal = {Biometrika},
  title        = {Randomization inference when n equals one},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient estimation for functional accelerated failure time models. <em>BIOMET</em>, <em>112</em>(2), asaf011. (<a href='https://doi.org/10.1093/biomet/asaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a functional accelerated failure time model to characterize the effects of both functional and scalar covariates on the time to event of interest, and provide regularity conditions to guarantee model identifiability. For efficient estimation of model parameters, we develop a sieve maximum likelihood approach where parametric and nonparametric coefficients are bundled with an unknown baseline hazard function in the likelihood function. Not only do the bundled parameters cause immense numerical difficulties, but they also result in new challenges in theoretical development. By developing a general theoretical framework, we overcome the challenges arising from the bundled parameters and derive the convergence rate of the proposed estimator. Additionally, we prove that the finite-dimensional estimator is root- n consistent, asymptotically normal and achieves the semiparametric information bound. Furthermore, we demonstrate the nonparametric optimality of the functional estimator and construct the asymptotic simultaneous confidence band. The proposed inference procedures are evaluated by extensive simulation studies and illustrated with an application to the National Health and Nutrition Examination Survey data.},
  archive      = {J_BIOMET},
  author       = {Liu, Changyu and Su, Wen and Liu, Kin-Yat and Yin, Guosheng and Zhao, Xingqiu},
  doi          = {10.1093/biomet/asaf011},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf011},
  shortjournal = {Biometrika},
  title        = {Efficient estimation for functional accelerated failure time models},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and reliable confidence intervals for a variance component. <em>BIOMET</em>, <em>112</em>(2), asaf010. (<a href='https://doi.org/10.1093/biomet/asaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that in a variance component model, confidence intervals with asymptotically correct uniform coverage probability can be obtained by inverting certain test statistics based on the score for the restricted likelihood. The results hold in settings where the variance component is near or at the boundary of the parameter set. Simulations indicate that the proposed test statistics are approximately pivotal and lead to confidence intervals with near-nominal coverage even in small samples. We illustrate the application of the proposed methods in spatially resolved transcriptomics, where we compute approximately 15 000 confidence intervals, used for gene ranking, in less than 4 minutes. In the settings we consider, the proposed method is between two and 28 000 times faster than popular alternatives, depending on how many confidence intervals are computed.},
  archive      = {J_BIOMET},
  author       = {Zhang, Yiqiao and Ekvall, Karl Oskar and Molstad, Aaron J},
  doi          = {10.1093/biomet/asaf010},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf010},
  shortjournal = {Biometrika},
  title        = {Fast and reliable confidence intervals for a variance component},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testable implications of outcome-independent missingness not at random in covariates. <em>BIOMET</em>, <em>112</em>(2), asaf009. (<a href='https://doi.org/10.1093/biomet/asaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common aim of empirical research is to regress an outcome on a set of covariates, when some covariates are subject to missingness. If the probability of missingness is conditionally independent of the outcome, given the covariates, then a complete-case analysis is unbiased for parameters conditional on covariates. We derive all testable constraints that such outcome-independent missingness not at random implies on the observed data distribution, for settings where both the outcome and covariates are categorical. By assessing if these constraints are violated for a particular observed data distribution, the analyst can infer whether the assumption of outcome-independent missingness not at random is violated for that distribution. The constraints are formulated implicitly, in terms of consistency requirements on certain linear equation systems. We also derive explicit inequality constraints that are more easily assessable, but also more permissive than the implicit constraints.},
  archive      = {J_BIOMET},
  author       = {Sjölander, A and Hägg, S},
  doi          = {10.1093/biomet/asaf009},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf009},
  shortjournal = {Biometrika},
  title        = {Testable implications of outcome-independent missingness not at random in covariates},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Super-efficient estimation of future conditional hazards based on time-homogeneous high-quality marker information. <em>BIOMET</em>, <em>112</em>(2), asaf008. (<a href='https://doi.org/10.1093/biomet/asaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new concept for forecasting future events based on marker information. The model is developed in the nonparametric counting process setting under the assumptions that the marker is of so-called high quality and with a time-homogeneous conditional distribution. Despite the model having nonparametric parts, it is established herein that it attains a parametric rate of uniform consistency and uniform asymptotic normality. In usual nonparametric scenarios, reaching such a fast convergence rate is not possible, so one can say that the proposed approach is super-efficient. These theoretical results are employed in the construction of simultaneous confidence bands directly for the hazard rate. Extensive simulation studies validate and compare the proposed methodology with the joint modelling approach and illustrate its robustness for mild violations of the assumptions. Its use in practice is illustrated in the computation of individual dynamic predictions in the context of primary biliary cirrhosis of the liver.},
  archive      = {J_BIOMET},
  author       = {Bagkavos, D and Isakson, A and Mammen, E and Nielsen, J P and Proust–Lima, C},
  doi          = {10.1093/biomet/asaf008},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf008},
  shortjournal = {Biometrika},
  title        = {Super-efficient estimation of future conditional hazards based on time-homogeneous high-quality marker information},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale multiple testing of composite null hypotheses under heteroskedasticity. <em>BIOMET</em>, <em>112</em>(2), asaf007. (<a href='https://doi.org/10.1093/biomet/asaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heteroskedasticity poses several methodological challenges in designing valid and powerful procedures for simultaneous testing of composite null hypotheses. In particular, the conventional practice of standardizing or rescaling heteroskedastic test statistics in this setting may severely affect the power of the underlying multiple testing procedure. Additionally, when the inferential parameter of interest is correlated with the variance of the test statistic, methods that ignore this dependence may fail to control the Type I error at the desired level. We propose a new heteroskedasticity-adjusted multiple testing procedure that avoids data reduction by standardization and directly incorporates the side information from the variances into the testing procedure. Our approach relies on an improved nonparametric empirical Bayes deconvolution estimator that offers a practical way of capturing the dependence between the inferential parameter of interest and the variance of the test statistic. We develop theory to establish that the proposed procedure is asymptotically valid and optimal for false discovery rate control. Simulation results demonstrate that our method outperforms existing procedures, with substantial power gains across many settings at the same false discovery rate level. The method is illustrated with an application involving the detection of engaged users on a mobile game app.},
  archive      = {J_BIOMET},
  author       = {Gang, B and Banerjee, T},
  doi          = {10.1093/biomet/asaf007},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf007},
  shortjournal = {Biometrika},
  title        = {Large-scale multiple testing of composite null hypotheses under heteroskedasticity},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric sieve estimation for survival data with two-layer censoring. <em>BIOMET</em>, <em>112</em>(2), asaf006. (<a href='https://doi.org/10.1093/biomet/asaf006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease registry data provide important information on the progression of disease conditions. However, reports of death or drop-out of patients enrolled in the registry are always subject to a noticeable delay. Reporting delays, together with the administrative censoring that arises from a freeze date in data collection, lead to two layers of right censoring in the data. The first layer results from random drop-out and acts on the survival time. The second layer is the administrative censoring, which acts on the sum of the reporting delay and the minimum of the survival time and random drop-out time. The heterogeneities among patients further complicate data analysis. This paper proposes a novel semiparametric sieve method based on phase-type distributions, in which covariates can be readily accommodated by the accelerated failure time model. A well-orchestrated EM algorithm is developed to compute the sieve maximum likelihood estimator. We establish the consistency and rate of convergence of the proposed sieve estimators, as well as the asymptotic normality and semiparametric efficiency of the estimators for the regression parameters. Comprehensive simulations and a real example of lung cancer registry data are used to demonstrate the proposed method. The results reveal substantial biases if reporting delays are overlooked.},
  archive      = {J_BIOMET},
  author       = {Wang, Yudong and Tong, Jiayi and Hu, Xiangbin and Ye, Zhi-Sheng and Tang, Cheng Yong and Chen, Yong},
  doi          = {10.1093/biomet/asaf006},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf006},
  shortjournal = {Biometrika},
  title        = {Semiparametric sieve estimation for survival data with two-layer censoring},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: ‘Efficient estimation under data fusion’. <em>BIOMET</em>, <em>112</em>(2), asaf005. (<a href='https://doi.org/10.1093/biomet/asaf005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asaf005},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf005},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘Efficient estimation under data fusion’},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An omitted variable bias framework for sensitivity analysis of instrumental variables. <em>BIOMET</em>, <em>112</em>(2), asaf004. (<a href='https://doi.org/10.1093/biomet/asaf004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Cinelli, Carlos and Hazlett, Chad},
  doi          = {10.1093/biomet/asaf004},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf004},
  shortjournal = {Biometrika},
  title        = {An omitted variable bias framework for sensitivity analysis of instrumental variables},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise-induced randomization in regression discontinuity designs. <em>BIOMET</em>, <em>112</em>(2), asaf003. (<a href='https://doi.org/10.1093/biomet/asaf003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression discontinuity designs assess causal effects in settings where treatment is determined by whether an observed running variable crosses a prespecified threshold. Here, we propose a new approach to identification, estimation and inference in regression discontinuity designs that uses knowledge about exogenous noise (e.g., measurement error) in the running variable. In our strategy, we weight treated and control units to balance a latent variable, of which the running variable is a noisy measure. Our approach is driven by effective randomization provided by the noise in the running variable, and complements standard formal analyses that appeal to continuity arguments while ignoring the stochastic nature of the assignment mechanism.},
  archive      = {J_BIOMET},
  author       = {Eckles, Dean and Ignatiadis, Nikolaos and Wager, Stefan and Wu, Han},
  doi          = {10.1093/biomet/asaf003},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf003},
  shortjournal = {Biometrika},
  title        = {Noise-induced randomization in regression discontinuity designs},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomization-based Z-estimation for evaluating average and individual treatment effects. <em>BIOMET</em>, <em>112</em>(2), asaf002. (<a href='https://doi.org/10.1093/biomet/asaf002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized experiments have been the gold standard for drawing causal inference. The conventional model-based approach has been one of the most popular methods of analysing treatment effects from randomized experiments, which is often carried out through inference for certain model parameters. In this paper, we provide a systematic investigation of model-based analyses for treatment effects under the randomization-based inference framework. This framework does not impose any distributional assumptions on the outcomes, covariates and their dependence, and utilizes only randomization as the reasoned basis. We first derive the asymptotic theory for Z -estimation in completely randomized experiments, and propose sandwich-type conservative covariance estimation. We then apply the developed theory to analyse both average and individual treatment effects in randomized experiments. For the average treatment effect, we consider model-based, model-imputed and model-assisted estimation strategies, where the first two strategies can be sensitive to model misspecification or require specific methods for parameter estimation. The model-assisted approach is robust to arbitrary model misspecification and always provides consistent average treatment effect estimation. We propose optimal ways to conduct model-assisted estimation using generally nonlinear least squares for parameter estimation. For the individual treatment effects, we propose directly modelling the relationship between individual effects and covariates, and discuss the model’s identifiability, inference and interpretation allowing for model misspecification.},
  archive      = {J_BIOMET},
  author       = {Qu, Tianyi and Du, Jiangchuan and Li, Xinran},
  doi          = {10.1093/biomet/asaf002},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf002},
  shortjournal = {Biometrika},
  title        = {Randomization-based Z-estimation for evaluating average and individual treatment effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-sample distribution tests in high dimensions via max-sliced wasserstein distance and bootstrapping. <em>BIOMET</em>, <em>112</em>(2), asaf001. (<a href='https://doi.org/10.1093/biomet/asaf001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-sample hypothesis testing is a fundamental statistical problem for inference about two populations. In this paper, we construct a novel test statistic to detect high-dimensional distributional differences based on the max-sliced Wasserstein distance to mitigate the curse of dimensionality. By exploiting an intriguing link between the distance and suprema of empirical processes, we develop an effective bootstrapping procedure to approximate the null distribution of the test statistic. One distinctive feature of the proposed test is the ability to construct simultaneous confidence intervals for the max-sliced Wasserstein distances of projected distributions of interest. This enables, not only the detection of global distributional differences, but also the identification of significantly different marginal distributions between two populations, without the need for additional tests. We establish the convergence of Gaussian and bootstrap approximations of the proposed test, based on which we show that the test is asymptotically valid and powerful as long as the considered max-sliced Wasserstein distance is adequately large. The merits of our approach are illustrated via simulated and real data examples.},
  archive      = {J_BIOMET},
  author       = {Hu, Xiaoyu and Lin, Zhenhua},
  doi          = {10.1093/biomet/asaf001},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asaf001},
  shortjournal = {Biometrika},
  title        = {Two-sample distribution tests in high dimensions via max-sliced wasserstein distance and bootstrapping},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistency of common spatial estimators under spatial confounding. <em>BIOMET</em>, <em>112</em>(2), asae070. (<a href='https://doi.org/10.1093/biomet/asae070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the asymptotic performance of popular spatial regression estimators of the linear effect of an exposure on an outcome under spatial confounding, the presence of an unmeasured spatially structured variable influencing both the exposure and the outcome. We first show that the estimators from ordinary least squares and restricted spatial regression are asymptotically biased under spatial confounding. We then prove a novel result on the infill consistency of the generalized least squares estimator using a working covariance matrix from a Matérn or squared exponential kernel, in the presence of spatial confounding. The result holds under very mild assumptions, accommodating any exposure with some nonspatial variation, any spatially continuous fixed confounder function, and non-Gaussian errors in both the exposure and the outcome. Finally, we prove that spatial estimators from generalized least squares, Gaussian process regression and spline models that are consistent under confounding by a fixed function will also be consistent under endogeneity or confounding by a random function, i.e., a stochastic process. We conclude that, contrary to some claims in the literature on spatial confounding, traditional spatial estimators are capable of estimating linear exposure effects under spatial confounding as long as there is some noise in the exposure. We support our theoretical arguments with simulation studies.},
  archive      = {J_BIOMET},
  author       = {Gilbert, Brian and Ogburn, Elizabeth L and Datta, Abhirup},
  doi          = {10.1093/biomet/asae070},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae070},
  shortjournal = {Biometrika},
  title        = {Consistency of common spatial estimators under spatial confounding},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving randomized controlled trial analysis via data-adaptive borrowing. <em>BIOMET</em>, <em>112</em>(2), asae069. (<a href='https://doi.org/10.1093/biomet/asae069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, real-world external controls have grown in popularity as a tool to empower randomized placebo-controlled trials, particularly in rare diseases or cases where balanced randomization is unethical or impractical. However, as external controls are not always comparable to the trials, direct borrowing without scrutiny may heavily bias the treatment effect estimator. Our paper proposes a data-adaptive integrative framework capable of preventing unknown biases of the external controls. The adaptive nature is achieved by dynamically sorting out a comparable subset of external controls via bias penalization. Our proposed method can simultaneously achieve (a) the semiparametric efficiency bound when the external controls are comparable and (b) selective borrowing that mitigates the impact of the existence of incomparable external controls. Furthermore, we establish statistical guarantees, including consistency, asymptotic distribution and inference, providing Type-I error control and good power. Extensive simulations and two real-data applications show that the proposed method leads to improved performance over the trial-only estimator across various bias-generating scenarios.},
  archive      = {J_BIOMET},
  author       = {Gao, Chenyin and Yang, Shu and Shan, Mingyang and Ye, Wenyu and Lipkovich, Ilya and Faries, Douglas},
  doi          = {10.1093/biomet/asae069},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae069},
  shortjournal = {Biometrika},
  title        = {Improving randomized controlled trial analysis via data-adaptive borrowing},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric local variable selection under misspecification. <em>BIOMET</em>, <em>112</em>(2), asae068. (<a href='https://doi.org/10.1093/biomet/asae068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local variable selection aims to test for the effect of covariates on an outcome within specific regions. We outline a challenge that arises in the presence of nonlinear effects and model misspecification. Specifically, for common semiparametric methods, even slight model misspecification can result in a high false positive rate, in a manner that is highly sensitive to the chosen basis functions. We propose a method based on orthogonal cut splines that avoids false positive inflation for any choice of knots and achieves consistent local variable selection. Our approach offers simplicity, can handle both continuous and categorical covariates, and provides theory for high-dimensional covariates and model misspecification. We discuss settings with either independent or dependent data. The proposed method allows inclusion of adjustment covariates that do not undergo selection, enhancing the model’s flexibility. Our examples describe salary gaps associated with various discrimination factors at different ages and elucidate the effects of covariates on functional data measuring brain activation at different times.},
  archive      = {J_BIOMET},
  author       = {Rossell, D and Seong, A K and Saez, I and Guindani, M},
  doi          = {10.1093/biomet/asae068},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae068},
  shortjournal = {Biometrika},
  title        = {Semiparametric local variable selection under misspecification},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially factorized variational inference for high-dimensional mixed models. <em>BIOMET</em>, <em>112</em>(2), asae067. (<a href='https://doi.org/10.1093/biomet/asae067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While generalized linear mixed models are a fundamental tool in applied statistics, many specifications, such as those involving categorical factors with many levels or interaction terms, can be computationally challenging to estimate due to the need to compute or approximate high-dimensional integrals. Variational inference is a popular way to perform such computations, especially in the Bayesian context. However, naive use of such methods can provide unreliable uncertainty quantification. We show that this is indeed the case for mixed models, proving that standard mean-field variational inference dramatically underestimates posterior uncertainty in high dimensions. We then show how appropriately relaxing the mean-field assumption leads to methods whose uncertainty quantification does not deteriorate in high dimensions, and whose total computational cost scales linearly with the number of parameters and observations. Our theoretical and numerical results focus on mixed models with Gaussian or binomial likelihoods, and rely on connections to random graph theory to obtain sharp high-dimensional asymptotic analysis. We also provide generic results, which are of independent interest, relating the accuracy of variational inference to the convergence rate of the corresponding coordinate ascent algorithm that is used to find it. Our proposed methodology is implemented in the R package vglmer . Numerical results with simulated and real data examples illustrate the favourable computation cost versus accuracy trade-off of our approach compared to various alternatives.},
  archive      = {J_BIOMET},
  author       = {Goplerud, M and Papaspiliopoulos, O and Zanella, G},
  doi          = {10.1093/biomet/asae067},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae067},
  shortjournal = {Biometrika},
  title        = {Partially factorized variational inference for high-dimensional mixed models},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anytime-valid and asymptotically efficient inference driven by predictive recursion. <em>BIOMET</em>, <em>112</em>(2), asae066. (<a href='https://doi.org/10.1093/biomet/asae066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distinguishing two models is a fundamental and practically important statistical problem. Error rate control is crucial to the testing logic, but in complex nonparametric settings can be difficult to achieve, especially when the stopping rule that determines the data collection process is not available. This paper proposes an e -process construction based on the predictive recursion algorithm originally designed to recursively fit nonparametric mixture models. The resulting predictive recursion e -process affords anytime-valid inference and is asymptotically efficient in the sense that its growth rate is first-order optimal relative to the predictive recursion’s mixture model.},
  archive      = {J_BIOMET},
  author       = {Dixit, Vaidehi and Martin, Ryan},
  doi          = {10.1093/biomet/asae066},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae066},
  shortjournal = {Biometrika},
  title        = {Anytime-valid and asymptotically efficient inference driven by predictive recursion},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing variable importance in survival analysis using machine learning. <em>BIOMET</em>, <em>112</em>(2), asae061. (<a href='https://doi.org/10.1093/biomet/asae061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a collection of features available for inclusion in a predictive model, it may be of interest to quantify the relative importance of a subset of features for the prediction task at hand. For example, in HIV vaccine trials, participant baseline characteristics are used to predict the probability of HIV acquisition over the intended follow-up period, and investigators may wish to understand how much certain types of predictors, such as behavioural factors, contribute to overall predictiveness. Time-to-event outcomes such as time to HIV acquisition are often subject to right censoring, and existing methods for assessing variable importance are typically not intended to be used in this setting. We describe a broad class of algorithm-agnostic variable importance measures for prediction in the context of survival data. We propose a nonparametric efficient estimation procedure that incorporates flexible learning of nuisance parameters, yields asymptotically valid inference and enjoys double robustness. We assess the performance of our proposed procedure via numerical simulations and analyse data from the HVTN 702 vaccine trial to inform enrolment strategies for future HIV vaccine trials.},
  archive      = {J_BIOMET},
  author       = {Wolock, C J and Gilbert, P B and Simon, N and Carone, M},
  doi          = {10.1093/biomet/asae061},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae061},
  shortjournal = {Biometrika},
  title        = {Assessing variable importance in survival analysis using machine learning},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian clustering of high-dimensional data via latent repulsive mixtures. <em>BIOMET</em>, <em>112</em>(2), asae059. (<a href='https://doi.org/10.1093/biomet/asae059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based clustering of moderate- or large-dimensional data is notoriously difficult. We propose a model for simultaneous dimensionality reduction and clustering by assuming a mixture model for a set of latent scores, which are then linked to the observations via a Gaussian latent factor model. This approach was recently investigated by Chandra et al. (2023). The authors used a factor-analytic representation and assumed a mixture model for the latent factors. However, performance can deteriorate in the presence of model misspecification. Assuming a repulsive point process prior for the component-specific means of the mixture for the latent scores is shown to yield a more robust model that outperforms the standard mixture model for the latent factors in several simulated scenarios. The repulsive point process must be anisotropic to favour well-separated clusters of data, and its density should be tractable for efficient posterior inference. We address these issues by proposing a general construction for anisotropic determinantal point processes. We illustrate our model in simulations, as well as a plant species co-occurrence dataset.},
  archive      = {J_BIOMET},
  author       = {Ghilotti, L and Beraha, M and Guglielmi, A},
  doi          = {10.1093/biomet/asae059},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae059},
  shortjournal = {Biometrika},
  title        = {Bayesian clustering of high-dimensional data via latent repulsive mixtures},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust and heteroscedasticity-aware sample trimming for causal inference. <em>BIOMET</em>, <em>112</em>(2), asae053. (<a href='https://doi.org/10.1093/biomet/asae053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular method for variance reduction in causal inference is propensity-based trimming, the practice of removing units with extreme propensities from the sample. This practice has theoretical grounding when the data are homoscedastic and the propensity model is parametric (Crump et al., 2009; Yang & Ding, 2018), but in modern settings where heteroscedastic data are analysed with nonparametric models, existing theory fails to support current practice. In this work, we address this challenge by developing new methods and theory for sample trimming. Our contributions are three-fold. First, we describe novel procedures for selecting which units to trim. Our procedures differ from previous works in that we trim, not only units with small propensities, but also units with extreme conditional variances. Second, we give new theoretical guarantees for inference after trimming. In particular, we show how to perform inference on the trimmed subpopulation without requiring that our regressions converge at parametric rates. Instead, we make only fourth-root rate assumptions like those in the double machine learning literature. This result applies to conventional propensity-based trimming as well, and thus may be of independent interest. Finally, we propose a bootstrap-based method for constructing simultaneously valid confidence intervals for multiple trimmed subpopulations, which are valuable for navigating the trade-off between sample size and variance reduction inherent in trimming. We validate our methods in simulation, on the 2007–2008 National Health and Nutrition Examination Survey and on a semisynthetic Medicare dataset, and find promising results in all settings.},
  archive      = {J_BIOMET},
  author       = {Khan, S and Ugander, J},
  doi          = {10.1093/biomet/asae053},
  journal      = {Biometrika},
  number       = {2},
  pages        = {asae053},
  shortjournal = {Biometrika},
  title        = {Doubly robust and heteroscedasticity-aware sample trimming for causal inference},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust estimation under a possibly misspecified marginal structural cox model. <em>BIOMET</em>, <em>112</em>(1), asae065. (<a href='https://doi.org/10.1093/biomet/asae065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we consider the marginal structural Cox model, which has been widely used to analyse observational studies with survival outcomes. The standard inverse probability weighting method under the model hinges on a propensity score model for the treatment assignment and a censoring model that incorporates both the treatment and the covariates. In such settings model misspecification can often occur, and the Cox regression model’s non-collapsibility has historically posed challenges when striving to guard against model misspecification through augmentation. We introduce a novel joint augmentation to the martingale-based full-data estimating functions and develop rate double robustness, which allows the use of machine learning and nonparametric methods to overcome the challenges of non-collapsibility. We closely examine its theoretical properties to guarantee root- n inference for the estimand. The estimator extends naturally to estimating a time-average treatment effect when the proportional hazards assumption fails, and we show that it satisfies both the assumption-lean and the well-specification criteria in the context of a causal estimand for censoring survival data; that is, it is a functional of the potential outcome distributions only and does not depend on the treatment assignment mechanism, the covariate distribution or the censoring mechanism. The martingale-based augmentation approach is also applicable to many semiparametric failure time models. Finally, its application to a dataset provides insights into the impact of mid-life alcohol consumption on mortality in later life.},
  archive      = {J_BIOMET},
  author       = {Luo, Jiyu and Rava, Denise and Bradic, Jelena and Xu, Ronghui},
  doi          = {10.1093/biomet/asae065},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae065},
  shortjournal = {Biometrika},
  title        = {Doubly robust estimation under a possibly misspecified marginal structural cox model},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using negative controls to identify causal effects with invalid instrumental variables. <em>BIOMET</em>, <em>112</em>(1), asae064. (<a href='https://doi.org/10.1093/biomet/asae064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many proposals for the identification of causal effects require an instrumental variable that satisfies strong, untestable unconfoundedness and exclusion restriction assumptions. In this paper, we show how one can potentially identify causal effects under violations of these assumptions by harnessing a negative control population or outcome. This strategy allows one to leverage subpopulations for whom the exposure is degenerate, and requires that the instrument-outcome association satisfies a certain parallel trend condition. We develop semiparametric efficiency theory for a general instrumental variable model, and obtain a multiply robust, locally efficient estimator of the average treatment effect in the treated. The utility of the estimators is demonstrated in simulation studies and an analysis of the Life Span Study.},
  archive      = {J_BIOMET},
  author       = {Dukes, O and Richardson, D B and Shahn, Z and Robins, J M and Tchetgen Tchetgen, E J},
  doi          = {10.1093/biomet/asae064},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae064},
  shortjournal = {Biometrika},
  title        = {Using negative controls to identify causal effects with invalid instrumental variables},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invariant probabilistic prediction. <em>BIOMET</em>, <em>112</em>(1), asae063. (<a href='https://doi.org/10.1093/biomet/asae063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method for obtaining invariant probabilistic predictions and study the consistency of the underlying parameters. Finally, we demonstrate the empirical performance of our proposed procedure via simulations and analysis of single-cell data.},
  archive      = {J_BIOMET},
  author       = {Henzi, Alexander and Shen, Xinwei and Law, Michael and Bühlmann, Peter},
  doi          = {10.1093/biomet/asae063},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae063},
  shortjournal = {Biometrika},
  title        = {Invariant probabilistic prediction},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On rosenbaum’s rank-based matching estimator. <em>BIOMET</em>, <em>112</em>(1), asae062. (<a href='https://doi.org/10.1093/biomet/asae062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In two influential contributions, Rosenbaum (2005, 2020a) advocated for using the distances between componentwise ranks, instead of the original data values, to measure covariate similarity when constructing matching estimators of average treatment effects. While the intuitive benefits of using covariate ranks for matching estimation are apparent, there is no theoretical understanding of such procedures in the literature. We fill this gap by demonstrating that Rosenbaum’s rank-based matching estimator, when coupled with a regression adjustment, enjoys the properties of double robustness and semiparametric efficiency without the need to enforce restrictive covariate moment assumptions. Our theoretical findings further emphasize the statistical virtues of employing ranks for estimation and inference, more broadly aligning with the insights put forth by Peter Bickel in his 2004 Rietz lecture.},
  archive      = {J_BIOMET},
  author       = {Cattaneo, Matias D and Han, Fang and Lin, Zhexiao},
  doi          = {10.1093/biomet/asae062},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae062},
  shortjournal = {Biometrika},
  title        = {On rosenbaum’s rank-based matching estimator},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverting estimating equations for causal inference on quantiles. <em>BIOMET</em>, <em>112</em>(1), asae058. (<a href='https://doi.org/10.1093/biomet/asae058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The causal inference literature frequently focuses on estimating the mean of the potential outcome, whereas quantiles of the potential outcome may carry important additional information. We propose an inverse estimating equation framework to generalize a wide class of causal inference solutions from estimating the mean of the potential outcome to its quantiles. We assume that a moment function is available to identify the mean of the threshold-transformed potential outcome, based on which a convenient construction of the estimating equation of the quantiles of the potential outcome is proposed. In addition, we give a general construction of the efficient influence functions of the mean and quantiles of potential outcomes, and establish their connection. We motivate estimators for the quantile estimands with the efficient influence function, and develop their asymptotic properties when either parametric models or data-adaptive machine learners are used to estimate the nuisance functions. A broad implication of our results is that one can rework the existing result for mean causal estimands to facilitate causal inference on quantiles. Our general results are illustrated by several analytical and numerical examples.},
  archive      = {J_BIOMET},
  author       = {Cheng, Chao and Li, Fan},
  doi          = {10.1093/biomet/asae058},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae058},
  shortjournal = {Biometrika},
  title        = {Inverting estimating equations for causal inference on quantiles},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The phase diagram of kernel interpolation in large dimensions. <em>BIOMET</em>, <em>112</em>(1), asae057. (<a href='https://doi.org/10.1093/biomet/asae057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalization ability of kernel interpolation in large dimensions, ie, n ≍ d γ for some γ > 0 ⁠ , could be one of the most interesting problems in the recent renaissance of kernel regression, since it may help us understand the so-called benign overfitting phenomenon reported in the neural networks literature. Focusing on the inner product kernel on the unit sphere, we fully characterize the exact order of both the variance and the bias of large-dimensional kernel interpolation under various source conditions s ⩾ 0 ⁠ . Consequently, we obtain the ( s , γ ) phase diagram of large-dimensional kernel interpolation, ie, we determine the regions in the ( s , γ ) plane where the kernel interpolation is minimax optimal, suboptimal and inconsistent.},
  archive      = {J_BIOMET},
  author       = {Zhang, Haobo and Lu, Weihao and Lin, Qian},
  doi          = {10.1093/biomet/asae057},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae057},
  shortjournal = {Biometrika},
  title        = {The phase diagram of kernel interpolation in large dimensions},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised distribution learning. <em>BIOMET</em>, <em>112</em>(1), asae056. (<a href='https://doi.org/10.1093/biomet/asae056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the challenge of distribution estimation and inference in a semi-supervised setting. In contrast to prior research focusing on parameter inference, this work explores the complexities of semi-supervised distribution estimation, particularly the uniformity problem inherent in functional processes. To tackle this issue, we introduce a versatile framework designed to extract valuable information from unlabelled data by approximating a conditional distribution on covariates. The proposed estimator is derived using K -fold cross-fitting, and exhibits both consistency and asymptotic Gaussian process properties. Under mild conditions, the proposed estimator outperforms the empirical cumulative distribution function in terms of asymptotic efficiency. Several applications of the methodology are given, including parameter inference and goodness-of-fit tests.},
  archive      = {J_BIOMET},
  author       = {Wen, Mengtao and Jia, Yinxu and Ren, Haojie and Wang, Zhaojun and Zou, Changliang},
  doi          = {10.1093/biomet/asae056},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae056},
  shortjournal = {Biometrika},
  title        = {Semi-supervised distribution learning},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional principal component analysis with informative observation times. <em>BIOMET</em>, <em>112</em>(1), asae055. (<a href='https://doi.org/10.1093/biomet/asae055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional principal component analysis has been shown to be invaluable for revealing variation modes of longitudinal outcomes, which serve as important building blocks for forecasting and model building. Decades of research have advanced methods for functional principal component analysis, often assuming independence between the observation times and longitudinal outcomes. Yet such assumptions are fragile in real-world settings where observation times may be driven by outcome-related processes. Rather than ignoring the informative observation time process, we explicitly model the observational times by a general counting process dependent on time-varying prognostic factors. Identification of the mean, covariance function and functional principal components ensues via inverse intensity weighting. We propose using weighted penalized splines for estimation and establish consistency and convergence rates for the weighted estimators. Simulation studies demonstrate that the proposed estimators are substantially more accurate than the existing ones in the presence of a correlation between the observation time process and the longitudinal outcome process. We further examine the finite-sample performance of the proposed method using the Acute Infection and Early Disease Research Program study.},
  archive      = {J_BIOMET},
  author       = {Sang, Peijun and Kong, Dehan and Yang, Shu},
  doi          = {10.1093/biomet/asae055},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae055},
  shortjournal = {Biometrika},
  title        = {Functional principal component analysis with informative observation times},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). With random regressors, least squares inference is robust to correlated errors with unknown correlation structure. <em>BIOMET</em>, <em>112</em>(1), asae054. (<a href='https://doi.org/10.1093/biomet/asae054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear regression is arguably the most widely used statistical method. With fixed regressors and correlated errors, the conventional wisdom is to modify the variance-covariance estimator to accommodate the known correlation structure of the errors. We depart from existing literature by showing that with random regressors, linear regression inference is robust to correlated errors with unknown correlation structure. The existing theoretical analyses for linear regression are no longer valid because even the asymptotic normality of the least squares coefficients breaks down in this regime. We first prove the asymptotic normality of the t statistics by establishing their Berry–Esseen bounds based on a novel probabilistic analysis of self-normalized statistics. We then study the local power of the corresponding t tests and show that, perhaps surprisingly, error correlation can even enhance power in the regime of weak signals. Overall, our results show that linear regression is applicable more broadly than the conventional theory suggests, and they further demonstrate the value of randomization for ensuring robustness of inference.},
  archive      = {J_BIOMET},
  author       = {Zhang, Zifeng and Ding, Peng and Zhou, Wen and Wang, Haonan},
  doi          = {10.1093/biomet/asae054},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae054},
  shortjournal = {Biometrika},
  title        = {With random regressors, least squares inference is robust to correlated errors with unknown correlation structure},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the degrees of freedom of the smoothing parameter. <em>BIOMET</em>, <em>112</em>(1), asae052. (<a href='https://doi.org/10.1093/biomet/asae052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The smoothing parameters in a semiparametric model are estimated based on criteria such as generalized cross-validation or restricted maximum likelihood. As these parameters are estimated in a data-driven manner, they influence the degrees of freedom of a semiparametric model, based on Stein’s lemma. This allows us to associate parts of the degrees of freedom of a semiparametric model with the smoothing parameters. A framework is introduced that enables these degrees of freedom of the smoothing parameters to be derived analytically, based on the implicit function theorem. The degrees of freedom of the smoothing parameters are efficient to compute and have a geometrical interpretation. The practical importance of this finding is highlighted by a simulation study and an application, showing that ignoring the degrees of freedom of the smoothing parameters in Akaike information criterion-based model selection leads to an increase in the post-selection prediction error.},
  archive      = {J_BIOMET},
  author       = {Säfken, B and Kneib, T and Wood, S N},
  doi          = {10.1093/biomet/asae052},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae052},
  shortjournal = {Biometrika},
  title        = {On the degrees of freedom of the smoothing parameter},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive null proportion estimator for false discovery rate control. <em>BIOMET</em>, <em>112</em>(1), asae051. (<a href='https://doi.org/10.1093/biomet/asae051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The false discovery rate is a commonly used criterion in multiple testing, and the Benjamini–Hochberg procedure is a standard approach to false discovery rate control. To increase its power, adaptive Benjamini–Hochberg procedures, that use estimates of the null proportion, have been proposed. A particularly popular approach being that based on Storey’s estimator. The performance of Storey’s estimator hinges on a critical hyperparameter, such that a pre-fixed configuration may lack power and existing data-driven hyperparameters may compromise false discovery rate control. In this work, we propose a novel class of adaptive hyperparameters and establish the false discovery rate control of the associated adaptive Benjamini–Hochberg procedure using a martingale argument. Within this class of data-driven hyperparameters, we further present a specific configuration designed to maximize the number of rejections and characterize its convergence to the optimal hyperparameter under a mixture model. The proposed method exhibits significant power gains, particularly in cases with a conservative null distribution, which are common in composite null testing, or with a moderate proportion of weak nonnulls, as is typically observed in biological experiments with enrichment processes.},
  archive      = {J_BIOMET},
  author       = {Gao, Zijun},
  doi          = {10.1093/biomet/asae051},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae051},
  shortjournal = {Biometrika},
  title        = {An adaptive null proportion estimator for false discovery rate control},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A note on e-values and multiple testing. <em>BIOMET</em>, <em>112</em>(1), asae050. (<a href='https://doi.org/10.1093/biomet/asae050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discover a connection between the Benjamini–Hochberg procedure and the e -Benjamini–Hochberg procedure (Wang & Ramdas, 2022) with a suitably defined set of e -values. This insight extends to Storey’s procedure and generalized versions of the Benjamini–Hochberg procedure and the model-free multiple testing procedure of Barber & Candés (2015) with a general form of rejection rules. We further summarize these findings in a unified form. These connections open up new possibilities for designing multiple testing procedures in various contexts by aggregating e -values from different procedures or assembling e -values from different data subsets.},
  archive      = {J_BIOMET},
  author       = {Li, Guanxun and Zhang, Xianyang},
  doi          = {10.1093/biomet/asae050},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae050},
  shortjournal = {Biometrika},
  title        = {A note on e-values and multiple testing},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing the mean and variance by e-processes. <em>BIOMET</em>, <em>112</em>(1), asae049. (<a href='https://doi.org/10.1093/biomet/asae049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of testing the conditional mean and conditional variance for nonstationary data. We build e -values and p -values for four types of nonparametric composite hypothesis with specified mean and variance as well as other conditions on the shape of the data-generating distribution. These shape conditions include symmetry, unimodality and their combination. Using the obtained e -values and p -values, we construct tests via e -processes, also known as testing by betting, as well as some tests based on combining p -values for comparison. Although we mainly focus on one-sided tests, the two-sided test for the mean is also studied. Simulation and empirical studies are conducted under a few settings, and they illustrate features of the methods based on e -processes.},
  archive      = {J_BIOMET},
  author       = {Fan, Yixuan and Jiao, Zhanyi and Wang, Ruodu},
  doi          = {10.1093/biomet/asae049},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae049},
  shortjournal = {Biometrika},
  title        = {Testing the mean and variance by e-processes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting the power of kernel two-sample tests. <em>BIOMET</em>, <em>112</em>(1), asae048. (<a href='https://doi.org/10.1093/biomet/asae048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The kernel two-sample test based on the maximum mean discrepancy is one of the most popular methods for detecting differences between two distributions over general metric spaces. In this paper we propose a method to boost the power of the kernel test by combining maximum mean discrepancy estimates over multiple kernels using their Mahalanobis distance. We derive the asymptotic null distribution of the proposed test statistic and use a multiplier bootstrap approach to efficiently compute the rejection region. The resulting test is universally consistent and, since it is obtained by aggregating over a collection of kernels/bandwidths, is more powerful in detecting a wide range of alternatives in finite samples. We also derive the distribution of the test statistic for both fixed and local contiguous alternatives. The latter, in particular, implies that the proposed test is statistically efficient, that is, it has nontrivial asymptotic (Pitman) efficiency. The consistency properties of the Mahalanobis and other natural aggregation methods are also explored when the number of kernels is allowed to grow with the sample size. Extensive numerical experiments are performed on both synthetic and real-world datasets to illustrate the efficacy of the proposed method over single-kernel tests. The computational complexity of the proposed method is also studied, both theoretically and in simulations. Our asymptotic results rely on deriving the joint distribution of the maximum mean discrepancy estimates using the framework of multiple stochastic integrals, which is more broadly useful, specifically, in understanding the efficiency properties of recently proposed adaptive maximum mean discrepancy tests based on kernel aggregation and also in developing more computationally efficient, linear-time tests that combine multiple kernels. We conclude with an application of the Mahalanobis aggregation method for kernels with diverging scaling parameters.},
  archive      = {J_BIOMET},
  author       = {Chatterjee, A and Bhattacharya, B B},
  doi          = {10.1093/biomet/asae048},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae048},
  shortjournal = {Biometrika},
  title        = {Boosting the power of kernel two-sample tests},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: ‘A model-free variable screening method for optimal treatment regimes with high-dimensional survival data’. <em>BIOMET</em>, <em>112</em>(1), asae047. (<a href='https://doi.org/10.1093/biomet/asae047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  doi          = {10.1093/biomet/asae047},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae047},
  shortjournal = {Biometrika},
  title        = {Correction to: ‘A model-free variable screening method for optimal treatment regimes with high-dimensional survival data’},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local bootstrap for network data. <em>BIOMET</em>, <em>112</em>(1), asae046. (<a href='https://doi.org/10.1093/biomet/asae046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network analysis, one frequently needs to conduct inference for network parameters based on a single observed network. Since the sampling distribution of the statistic is often unknown, one has to rely on the bootstrap. However, because of the complex dependence structure among vertices, existing bootstrap methods often yield unsatisfactory performance, especially for small or moderate sample sizes. Here we propose a new network bootstrap procedure, termed the local bootstrap, for estimating the standard errors of network statistics. The method involves resampling the observed vertices along with their neighbour sets, and then reconstructing the edges between the resampled vertices by drawing from the set of edges connecting their neighbour sets. We justify the proposed method theoretically with desirable asymptotic properties for statistics such as motif density, and demonstrate its excellent numerical performance for small and moderate sample sizes. Our approach encompasses several existing methods, such as the empirical graphon bootstrap, as special cases. We investigate the advantages of the proposed method over existing methods in terms of edge randomness, vertex heterogeneity and neighbour set size, which can help to shed light on the complex issue of network bootstrapping.},
  archive      = {J_BIOMET},
  author       = {Zu, Tianhai and Qin, Yichen},
  doi          = {10.1093/biomet/asae046},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae046},
  shortjournal = {Biometrika},
  title        = {Local bootstrap for network data},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple bootstrap for chatterjee’s rank correlation. <em>BIOMET</em>, <em>112</em>(1), asae045. (<a href='https://doi.org/10.1093/biomet/asae045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that an m -out-of- n bootstrap procedure for Chatterjee’s rank correlation is consistent whenever asymptotic normality of Chatterjee’s rank correlation can be established. In particular, we prove that m -out-of- n bootstrap works for continuous as well as discrete data with independent coordinates; furthermore, simulations indicate that it also performs well for discrete data with dependent coordinates, and that it outperforms alternative estimation methods. Consistency of the bootstrap is proved in the Kolmogorov distance as well as in the Wasserstein distance.},
  archive      = {J_BIOMET},
  author       = {Dette, H and Kroll, M},
  doi          = {10.1093/biomet/asae045},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae045},
  shortjournal = {Biometrika},
  title        = {A simple bootstrap for chatterjee’s rank correlation},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity models and bounds under sequential unmeasured confounding in longitudinal studies. <em>BIOMET</em>, <em>112</em>(1), asae044. (<a href='https://doi.org/10.1093/biomet/asae044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider sensitivity analysis for causal inference in a longitudinal study with time-varying treatments and covariates. It is of interest to assess the worst-case possible values of counterfactual outcome means and average treatment effects under sequential unmeasured confounding. We formulate several multi-period sensitivity models to relax the corresponding versions of the assumption of sequential non-confounding. The primary sensitivity model involves only counterfactual outcomes, whereas the joint and product sensitivity models involve both counterfactual covariates and outcomes. We establish and compare explicit representations for the sharp and conservative bounds at the population level through convex optimization, depending only on the observed data. These results provide for the first time a satisfactory generalization from the marginal sensitivity model in the cross-sectional setting.},
  archive      = {J_BIOMET},
  author       = {Tan, Zhiqiang},
  doi          = {10.1093/biomet/asae044},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae044},
  shortjournal = {Biometrika},
  title        = {Sensitivity models and bounds under sequential unmeasured confounding in longitudinal studies},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Axiomatization of interventional probability distributions. <em>BIOMET</em>, <em>112</em>(1), asae043. (<a href='https://doi.org/10.1093/biomet/asae043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal intervention is an essential tool in causal inference. It is axiomatized under the rules of do-calculus in the case of structure causal models. We provide simple axiomatizations for families of probability distributions to be different types of interventional distributions. Our axiomatizations neatly lead to a simple and clear theory of causality that has several advantages: it does not need to make use of any modelling assumptions such as those imposed by structural causal models; it relies only on interventions on single variables; it includes most cases with latent variables and causal cycles; and, more importantly, it does not assume the existence of an underlying true causal graph as we do not take it as the primitive object; moreover, a causal graph is derived as a by-product of our theory. We show that, under our axiomatizations, the intervened distributions are Markovian to the defined intervened causal graphs, and an observed joint probability distribution is Markovian to the obtained causal graph; these results are consistent with the case of structural causal models, and as a result, the existing theory of causal inference applies. We also show that a large class of natural structural causal models satisfy the theory presented here. The aim of this paper is axiomatization of interventional families, which is subtly different from causal modelling.},
  archive      = {J_BIOMET},
  author       = {Sadeghi, Kayvan and Soo, Terry},
  doi          = {10.1093/biomet/asae043},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae043},
  shortjournal = {Biometrika},
  title        = {Axiomatization of interventional probability distributions},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple conditional randomization tests for lagged and spillover treatment effects. <em>BIOMET</em>, <em>112</em>(1), asae042. (<a href='https://doi.org/10.1093/biomet/asae042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of constructing multiple independent conditional randomization tests using a single dataset. Because the tests are independent, the randomization p -values can be interpreted individually and combined using standard methods for multiple testing. We give a simple, sequential construction of such tests and then discuss its application to three problems: Rosenbaum’s evidence factors for observational studies, lagged treatment effects in stepped-wedge trials, and spillover effects in randomized trials with interference. We compare the proposed approach with some existing methods using simulated and real datasets. Finally, we establish a more general sufficient condition for independent conditional randomization tests.},
  archive      = {J_BIOMET},
  author       = {Zhang, Yao and Zhao, Qingyuan},
  doi          = {10.1093/biomet/asae042},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae042},
  shortjournal = {Biometrika},
  title        = {Multiple conditional randomization tests for lagged and spillover treatment effects},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating causal effects under non-individualistic treatments due to network entanglement. <em>BIOMET</em>, <em>112</em>(1), asae041. (<a href='https://doi.org/10.1093/biomet/asae041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many observational studies, the treatment assignment mechanism is not individualistic, as it allows the probability of treatment of a unit to depend on quantities beyond the unit’s covariates. In such settings, unit treatments may be entangled in complex ways. In this article, we consider a particular instance of this problem where the treatments are entangled by a social network among units. For instance, when studying the effects of peer interaction on a social media platform, the treatment on a unit depends on the change of the interactions network over time. A similar situation is encountered in many economic studies, such as those examining the effects of bilateral trade partnerships on countries’ economic growth. The challenge in these settings is that individual treatments depend on a global network that may change in a way that is endogenous and cannot be manipulated experimentally. In this paper, we show that classical propensity score methods that ignore entanglement may lead to large bias and wrong inference of causal effects. We then propose a solution that involves calculating propensity scores by marginalizing over the network change. Under an appropriate ignorability assumption, this leads to unbiased estimates of the treatment effect of interest. We also develop a randomization-based inference procedure that takes entanglement into account. Under general conditions on network change, this procedure can deliver valid inference without explicitly modelling the network. We establish theoretical results for the proposed methods and illustrate their behaviour via simulation studies based on real-world network data. We also revisit a large-scale observational dataset on contagion of online user behaviour, showing that ignoring entanglement may inflate estimates of peer influence.},
  archive      = {J_BIOMET},
  author       = {Toulis, P and Volfovsky, A and Airoldi, E M},
  doi          = {10.1093/biomet/asae041},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae041},
  shortjournal = {Biometrika},
  title        = {Estimating causal effects under non-individualistic treatments due to network entanglement},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variance-based sensitivity analysis for weighting estimators results in more informative bounds. <em>BIOMET</em>, <em>112</em>(1), asae040. (<a href='https://doi.org/10.1093/biomet/asae040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighting methods are popular tools for estimating causal effects, and assessing their robustness under unobserved confounding is important in practice. Current approaches to sensitivity analyses rely on bounding a worst-case error from omitting a confounder. In this paper, we introduce a new sensitivity model called the variance-based sensitivity model , which instead bounds the distributional differences that arise in the weights from omitting a confounder. The variance-based sensitivity model can be parameterized by an R 2 parameter that is both standardized and bounded. We demonstrate, both empirically and theoretically, that the variance-based sensitivity model provides improvements on the stability of the sensitivity analysis procedure over existing methods. We show that by moving away from worst-case bounds, we are able to obtain more interpretable and informative bounds. We illustrate our proposed approach on a study examining blood mercury levels using the National Health and Nutrition Examination Survey.},
  archive      = {J_BIOMET},
  author       = {Huang, Melody and Pimentel, Samuel D},
  doi          = {10.1093/biomet/asae040},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae040},
  shortjournal = {Biometrika},
  title        = {Variance-based sensitivity analysis for weighting estimators results in more informative bounds},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Studies in the history of probability and statistics, LI: The first conditional logistic regression. <em>BIOMET</em>, <em>112</em>(1), asae038. (<a href='https://doi.org/10.1093/biomet/asae038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statisticians and epidemiologists generally cite the publications of Prentice & Breslow (1978) and Breslow et al. (1978) as the first description and use of conditional logistic regression, while economists cite the book chapter by Nobel laureate McFadden ( McFadden, 1973 ). We describe the until-now-unrecognized use of, and way of fitting, this model in 1934 by Lionel Penrose and Ronald Fisher.},
  archive      = {J_BIOMET},
  author       = {Hanley, J A},
  doi          = {10.1093/biomet/asae038},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae038},
  shortjournal = {Biometrika},
  title        = {Studies in the history of probability and statistics, LI: The first conditional logistic regression},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal inference with hidden mediators. <em>BIOMET</em>, <em>112</em>(1), asae037. (<a href='https://doi.org/10.1093/biomet/asae037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proximal causal inference was recently proposed as a framework to identify causal effects from observational data in the presence of hidden confounders for which proxies are available. In this paper, we extend the proximal causal inference approach to settings where identification of causal effects hinges upon a set of mediators that are not observed, yet error prone proxies of the hidden mediators are measured. Specifically, (i) we establish causal hidden mediation analysis, which extends classical causal mediation analysis methods for identifying natural direct and indirect effects under no unmeasured confounding to a setting where the mediator of interest is hidden, but proxies of it are available; (ii) we establish a hidden front-door criterion, criterion to allow for hidden mediators for which proxies are available; (iii) we show that the identification of a certain causal effect called the population intervention indirect effect remains possible with hidden mediators in settings where challenges in (i) and (ii) might co-exist. We view (i)–(iii) as important steps towards the practical application of front-door criteria and mediation analysis as mediators are almost always measured with error and, thus, the most one can hope for in practice is that the measurements are at best proxies of mediating mechanisms. We propose identification approaches for the parameters of interest in our considered models. For the estimation aspect, we propose an influence function-based estimation method and provide an analysis for the robustness of the estimators.},
  archive      = {J_BIOMET},
  author       = {Ghassami, Amiremad and Yang, Alan and Shpitser, Ilya and Tchetgen Tchetgen, Eric},
  doi          = {10.1093/biomet/asae037},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae037},
  shortjournal = {Biometrika},
  title        = {Causal inference with hidden mediators},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust covariate-balancing method for learning optimal individualized treatment regimes. <em>BIOMET</em>, <em>112</em>(1), asae036. (<a href='https://doi.org/10.1093/biomet/asae036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important problems in precision medicine is to find the optimal individualized treatment rule, which is designed to recommend treatment decisions and maximize overall clinical benefit to patients based on their individual characteristics. Typically, the expected clinical outcome is required to be estimated first, for which an outcome regression model or a propensity score model usually needs to be assumed with most existing statistical methods. However, if either model assumption is invalid, the estimated treatment regime will not be reliable. In this article, we first define a contrast value function, which forms the basis for the study of individualized treatment regimes. Then we construct a hybrid estimator of the contrast value function by combining two types of estimation methods. We further propose a robust covariate-balancing estimator of the contrast value function by combining the inverse probability weighted method and matching method, which is based on the covariate balancing propensity score proposed by Imai & Ratkovic (2014) . Theoretical results show that the proposed estimator is doubly robust, ie, it is consistent if either the propensity score model or the matching is correct. Based on a large number of simulation studies, we demonstrate that the proposed estimator outperforms existing methods. Application of the proposed method is illustrated through analysis of the SUPPORT study.},
  archive      = {J_BIOMET},
  author       = {Li, Canhui and Zeng, Donglin and Zhu, Wensheng},
  doi          = {10.1093/biomet/asae036},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae036},
  shortjournal = {Biometrika},
  title        = {A robust covariate-balancing method for learning optimal individualized treatment regimes},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric efficiency gains from parametric restrictions on propensity scores. <em>BIOMET</em>, <em>112</em>(1), asae034. (<a href='https://doi.org/10.1093/biomet/asae034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore how much knowing a parametric restriction on propensity scores improves semiparametric efficiency bounds in the potential outcome framework. For stratified propensity scores, considered as a parametric model, we derive explicit formulas for the efficiency gain from knowing how the covariate space is split. Based on these, we find that the efficiency gain decreases as the partition of the stratification becomes finer. For general parametric models, where it is hard to obtain explicit representations of efficiency bounds, we propose a novel framework that enables us to see whether knowing a parametric model is valuable in terms of efficiency even when it is high dimensional. In addition to the intuitive fact that knowing the parametric model does not help much if it is sufficiently flexible, we discover that the efficiency gain can be nearly zero even though the parametric assumption significantly restricts the space of possible propensity scores.},
  archive      = {J_BIOMET},
  author       = {Kono, Haruki},
  doi          = {10.1093/biomet/asae034},
  journal      = {Biometrika},
  number       = {1},
  pages        = {asae034},
  shortjournal = {Biometrika},
  title        = {Semiparametric efficiency gains from parametric restrictions on propensity scores},
  volume       = {112},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="biomtc">BIOMTC - 39</h2>
<ul>
<li><details>
<summary>
(2025). Joint disease mapping for bivariate count data with residual correlation due to unknown number of common cases. <em>BIOMTC</em>, <em>81</em>(3), ujaf119. (<a href='https://doi.org/10.1093/biomtc/ujaf119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint spatial distribution of two count outcomes (eg, counts of two diseases) is usually studied using a Poisson shared component model (P-SCM), which uses geographically structured latent variables to model spatial variations that are specific and shared by both outcomes. In this model, the correlation between the outcomes is assumed to be fully accounted for by the latent variables. However, in this article, we show that when the outcomes have an unknown number of cases in common, the bivariate counts exhibit a positive “residual” correlation, which the P-SCM wrongly attributes to the covariance of the latent variables, leading to biased inference and degraded predictive performance. Accordingly, we propose a new SCM based on the Bivariate-Poisson distribution (BP-SCM hereafter) to study such correlated bivariate data. The BP-SCM decomposes each count into counts of common and distinct cases, and then models each of these three counts (two distinct and one common) using Gaussian Markov Random Fields. The model is formulated in a Bayesian framework using Hamiltonian Monte Carlo inference. Simulations and a real-world application showed the good inferential and predictive performances of the BP-SCM and confirm the bias in P-SCM. BP-SCM provides rich epidemiological information, such as the mean levels of the unknown counts of common and distinct cases, and their shared and specific spatial variations.},
  archive      = {J_BIOMTC},
  author       = {Chatignoux, Edouard and Uhry, Zoé and Remontet, Laurent and Albert, Isabelle},
  doi          = {10.1093/biomtc/ujaf119},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf119},
  shortjournal = {Biometrics},
  title        = {Joint disease mapping for bivariate count data with residual correlation due to unknown number of common cases},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric bayesian approach for dynamic borrowing of historical control data. <em>BIOMTC</em>, <em>81</em>(3), ujaf118. (<a href='https://doi.org/10.1093/biomtc/ujaf118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When incorporating historical control data into the analysis of current randomized controlled trial data, it is critical to account for differences between the datasets. When the cause of difference is an unmeasured factor and adjustment for only observed covariates is insufficient, it is desirable to use a dynamic borrowing method that reduces the impact of heterogeneous historical controls. We propose a nonparametric Bayesian approach that addresses between-trial heterogeneity and allows borrowing historical controls homogeneous with the current control. Additionally, to emphasize conflict resolution between historical controls and the current control, we introduce a method based on the dependent Dirichlet process (DP) mixture. The proposed methods can be implemented using the same procedure, regardless of whether the outcome data comprise aggregated study-level data or individual participant data. We also develop a novel index of similarity between the historical and current control data, based on the posterior distribution of the parameter of interest. We conduct a simulation study and analyze clinical trial examples to evaluate the performance of the proposed methods compared to existing methods. The proposed method, based on the dependent DP mixture, can accurately borrow from homogeneous historical controls while reducing the impact of heterogeneous historical controls compared to the typical DP mixture. The proposed methods outperform existing methods in scenarios with heterogeneous historical controls, in which the meta-analytic approach is ineffective.},
  archive      = {J_BIOMTC},
  author       = {Ohigashi, Tomohiro and Maruo, Kazushi and Sozu, Takashi and Gosho, Masahiko},
  doi          = {10.1093/biomtc/ujaf118},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf118},
  shortjournal = {Biometrics},
  title        = {Nonparametric bayesian approach for dynamic borrowing of historical control data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian inference for copy number intra-tumoral heterogeneity from single-cell RNA-sequencing data. <em>BIOMTC</em>, <em>81</em>(3), ujaf115. (<a href='https://doi.org/10.1093/biomtc/ujaf115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copy number alterations (CNA) are important drivers and markers of clonal structures within tumors. Understanding these structures at single-cell resolution is crucial to advancing cancer treatments. The objective is to cluster single cells into clones and identify CNA events in each clone. Early attempts often sacrifice the intrinsic link between cell clustering and clonal CNA detection for simplicity and rely heavily on human input for critical parameters such as the number of clones. Here, we develop a Bayesian model to utilize single-cell RNA sequencing (scRNA-seq) data for automatic analysis of intra-tumoral clonal structure concerning CNAs, without reliance on prior knowledge. The model clusters cells into sub-tumoral clones, identifies the number of clones, and simultaneously infers the clonal CNA profiles. It synergistically incorporates input from gene expression and germline single-nucleotide polymorphisms. A Gibbs sampling algorithm has been implemented and is available as an R package Chloris. We demonstrate that our new method compares strongly against existing software tools in terms of both cell clustering and CNA profile identification accuracy. Application to human metastatic melanoma and anaplastic thyroid tumor data demonstrates accurate clustering of tumor and non-tumor cells and reveals clonal CNA profiles that highlight functional gene expression differences between clones from the same tumor.},
  archive      = {J_BIOMTC},
  author       = {Qiao, PuXue and Kwok, Chun Fung and Qian, Guoqi and McCarthy, Davis J},
  doi          = {10.1093/biomtc/ujaf115},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf115},
  shortjournal = {Biometrics},
  title        = {Bayesian inference for copy number intra-tumoral heterogeneity from single-cell RNA-sequencing data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting optimal allocations for binary responses: Insights from considering type-I error rate control. <em>BIOMTC</em>, <em>81</em>(3), ujaf114. (<a href='https://doi.org/10.1093/biomtc/ujaf114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation—an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive 2 optimal allocation proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power, and the other minimizes the total number of failures in a trial while maintaining a fixed variance level. Through simulations based on an early phase and a confirmatory trial, we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials, and alternative measures of interest.},
  archive      = {J_BIOMTC},
  author       = {Pin, Lukas and Villar, Sofía S and Rosenberger, William F},
  doi          = {10.1093/biomtc/ujaf114},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf114},
  shortjournal = {Biometrics},
  title        = {Revisiting optimal allocations for binary responses: Insights from considering type-I error rate control},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised linear regression: Enhancing efficiency and robustness in high dimensions. <em>BIOMTC</em>, <em>81</em>(3), ujaf113. (<a href='https://doi.org/10.1093/biomtc/ujaf113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semi-supervised learning, the prevailing understanding suggests that observing additional unlabeled samples improves estimation accuracy for linear parameters only in the case of model misspecification. In this work, we challenge such a claim and show that additional unlabeled samples are beneficial in high-dimensional settings. Initially focusing on a dense scenario, we introduce robust semi-supervised estimators for the regression coefficient without relying on sparse structures in the population slope. Even when the true underlying model is linear, we show that leveraging information from large-scale unlabeled data helps reduce estimation bias, thereby improving both estimation accuracy and inference robustness. Moreover, we propose semi-supervised methods with further enhanced efficiency in scenarios with a sparse linear slope. The performance of the proposed methods is demonstrated through extensive numerical studies.},
  archive      = {J_BIOMTC},
  author       = {Chen, Kai and Zhang, Yuqian},
  doi          = {10.1093/biomtc/ujaf113},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf113},
  shortjournal = {Biometrics},
  title        = {Semi-supervised linear regression: Enhancing efficiency and robustness in high dimensions},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model robust designs for dose-response models. <em>BIOMTC</em>, <em>81</em>(3), ujaf112. (<a href='https://doi.org/10.1093/biomtc/ujaf112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An optimal experimental design is a structured data collection plan aimed at maximizing the amount of information gathered. Determining an optimal experimental design, however, relies on the assumption that a predetermined model structure, relating the response and covariates, is known a priori. In practical scenarios, such as dose-response modeling, the form of the model representing the “true” relationship is frequently unknown, although there exists a finite set or pool of potential alternative models. Designing experiments based on a single model from this set may lead to inefficiency or inadequacy if the “true” model differs from that assumed when calculating the design. One approach to minimize the impact of the uncertainty in the model on the experimental plan is known as model robust design . In this context, we systematically address the challenge of finding approximate optimal model robust experimental designs. Our focus is on locally optimal designs, so allowing some of the models in the pool to be nonlinear. We present three Semidefinite Programming-based formulations, each aligned with one of the classes of model robustness criteria introduced by Läuter. These formulations exploit the semidefinite representability of the robustness criteria, leading to the representation of the robust problem as a semidefinite program. To ensure comparability of information measures across various models, we employ standardized designs. To illustrate the application of our approach, we consider a dose-response study where, initially, seven models were postulated as potential candidates to describe the dose-response relationship.},
  archive      = {J_BIOMTC},
  author       = {Duarte, Belmiro P M and Atkinson, Anthony C and Oliveira, Nuno M C},
  doi          = {10.1093/biomtc/ujaf112},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf112},
  shortjournal = {Biometrics},
  title        = {Model robust designs for dose-response models},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Evaluating the effects of high-throughput structural neuroimaging predictors on whole-brain functional connectome outcomes via network-based matrix-on-vector regression. <em>BIOMTC</em>, <em>81</em>(3), ujaf111. (<a href='https://doi.org/10.1093/biomtc/ujaf111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1093/biomtc/ujaf111},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf111},
  shortjournal = {Biometrics},
  title        = {Correction to: Evaluating the effects of high-throughput structural neuroimaging predictors on whole-brain functional connectome outcomes via network-based matrix-on-vector regression},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mastering rare event analysis: Subsample-size determination in cox and logistic regressions. <em>BIOMTC</em>, <em>81</em>(3), ujaf110. (<a href='https://doi.org/10.1093/biomtc/ujaf110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of contemporary data analysis, the use of massive datasets has taken on heightened significance, albeit often entailing considerable demands on computational time and memory. While a multitude of existing works offer optimal subsampling methods for conducting analyses on subsamples with minimized efficiency loss, they notably lack tools for judiciously selecting the subsample size. To bridge this gap, our work introduces tools designed for choosing the subsample size. We focus on three settings: the Cox regression model for survival data with rare events, and logistic regression for both balanced and imbalanced datasets. Additionally, we present a new optimal subsampling procedure tailored to logistic regression with imbalanced data. The efficacy of these tools and procedures is demonstrated through an extensive simulation study and meticulous analyses of two sizable datasets: survival analysis of UK Biobank colorectal cancer data with about 350 million rows and logistic regression of linked birth and infant death data with about 28 million observations.},
  archive      = {J_BIOMTC},
  author       = {Agassi, Tal and Keret, Nir and Gorfine, Malka},
  doi          = {10.1093/biomtc/ujaf110},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf110},
  shortjournal = {Biometrics},
  title        = {Mastering rare event analysis: Subsample-size determination in cox and logistic regressions},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional multi-study multi-modality covariate-augmented generalized factor model. <em>BIOMTC</em>, <em>81</em>(3), ujaf107. (<a href='https://doi.org/10.1093/biomtc/ujaf107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent factor models that integrate data from multiple sources/studies or modalities have garnered considerable attention across various disciplines. However, existing methods predominantly focus either on multi-study integration or multi-modality integration, rendering them insufficient for analyzing the diverse modalities measured across multiple studies. To address this limitation and cater to practical needs, we introduce a high-dimensional generalized factor model that seamlessly integrates multi-modality data from multiple studies, while also accommodating additional covariates. We conduct a thorough investigation of the identifiability conditions to enhance the model’s interpretability. To tackle the complexity of high-dimensional nonlinear integration caused by 4 large latent random matrices, we utilize a variational lower bound to approximate the observed log-likelihood by employing a variational posterior distribution. By profiling the variational parameters, we establish the asymptotical properties of estimators for model parameters using M-estimation theory. Furthermore, we devise a computationally efficient variational expectation maximization (EM) algorithm to execute the estimation process and a criterion to determine the optimal number of both study-shared and study-specific factors. Extensive simulation studies and a real-world application show that the proposed method significantly outperforms existing methods in terms of estimation accuracy and computational efficiency.},
  archive      = {J_BIOMTC},
  author       = {Liu, Wei and Zhong, Qingzhi},
  doi          = {10.1093/biomtc/ujaf107},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf107},
  shortjournal = {Biometrics},
  title        = {High-dimensional multi-study multi-modality covariate-augmented generalized factor model},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A monotone single index model for spatially referenced multistate current status data. <em>BIOMTC</em>, <em>81</em>(3), ujaf105. (<a href='https://doi.org/10.1093/biomtc/ujaf105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessment of multistate disease progression is commonplace in biomedical research, such as in periodontal disease (PD). However, the presence of multistate current status endpoints, where only a single snapshot of each subject’s progression through disease states is available at a random inspection time after a known starting state, complicates the inferential framework. In addition, these endpoints can be clustered, and spatially associated, where a group of proximally located teeth (within subjects) may experience similar PD status, compared to those distally located. Motivated by a clinical study recording PD progression, we propose a Bayesian semiparametric accelerated failure time model with an inverse-Wishart proposal for accommodating (spatial) random effects, and flexible errors that follow a Dirichlet process mixture of Gaussians. For clinical interpretability, the systematic component of the event times is modeled using a monotone single index model, with the (unknown) link function estimated via a novel integrated basis expansion and basis coefficients endowed with constrained Gaussian process priors. In addition to establishing parameter identifiability, we present scalable computing via a combination of elliptical slice sampling, fast circulant embedding techniques, and smoothing of hard constraints, leading to straightforward estimation of parameters, and state occupation and transition probabilities. Using synthetic data, we study the finite sample properties of our Bayesian estimates and their performance under model misspecification. We also illustrate our method via application to the real clinical PD dataset.},
  archive      = {J_BIOMTC},
  author       = {Das, Snigdha and Chae, Minwoo and Pati, Debdeep and Bandyopadhyay, Dipankar},
  doi          = {10.1093/biomtc/ujaf105},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf105},
  shortjournal = {Biometrics},
  title        = {A monotone single index model for spatially referenced multistate current status data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric joint modeling to estimate the treatment effect on a longitudinal surrogate with application to chronic kidney disease trials. <em>BIOMTC</em>, <em>81</em>(3), ujaf104. (<a href='https://doi.org/10.1093/biomtc/ujaf104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials where long follow-up is required to measure the primary outcome of interest, there is substantial interest in using an accepted surrogate outcome that can be measured earlier in time or with less cost to estimate a treatment effect. For example, in clinical trials of chronic kidney disease, the effect of a treatment is often demonstrated on a longitudinal surrogate, the change of the longitudinal outcome (glomerular filtration rate, GFR) per year or GFR slope. However, estimating the effect of a treatment on the GFR slope is complicated by the fact that GFR measurement can be terminated by the occurrence of a terminal event, such as death or kidney failure. Thus, to estimate this effect, one must consider both the longitudinal GFR trajectory and the terminal event process. In this paper, we build a semiparametric framework to jointly model the longitudinal outcome and the terminal event, where the model for the longitudinal outcome is semiparametric, the relationship between the longitudinal outcome and the terminal event is nonparametric, and the terminal event is modeled via a semiparametric Cox model. The proposed semiparametric joint model is flexible and can be easily extended to include a nonlinear trajectory of the longitudinal outcome. An estimating equation based method is proposed to estimate the treatment effect on the longitudinal surrogate outcome (eg, GFR slope). Theoretical properties of the proposed estimators are derived, and finite sample performance is evaluated through simulation studies. We illustrate the proposed method using data from the Reduction of Endpoints in NIDDM with the Angiotensin II Antagonist Losartan (RENAAL) trial to examine the effect of Losartan on GFR slope.},
  archive      = {J_BIOMTC},
  author       = {Wang, Xuan and Zhou, Jie and Parast, Layla and Greene, Tom},
  doi          = {10.1093/biomtc/ujaf104},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf104},
  shortjournal = {Biometrics},
  title        = {Semiparametric joint modeling to estimate the treatment effect on a longitudinal surrogate with application to chronic kidney disease trials},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating longitudinal treatment effects for duchenne muscular dystrophy using dynamically enriched bayesian small sample, sequential, multiple assignment randomized trial (snSMART). <em>BIOMTC</em>, <em>81</em>(3), ujaf103. (<a href='https://doi.org/10.1093/biomtc/ujaf103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For progressive rare diseases like Duchenne muscular dystrophy (DMD), evaluating disease burden by measuring the totality of evidence from outcome data over time per patient can be highly informative, especially regarding how a new treatment impacts disease progression and functional outcomes. This paper focuses on new statistical approaches for analyzing data generated over time in a small sample, sequential, multiple assignment, randomized trial (snSMART), with an application to DMD. In addition, the use of external control data can enhance the statistical and operational efficiency in rare disease drug development by solving participant scarcity issues and ethical challenges. We employ a two-step robust meta-analytic approach to leverage external control data while adjusting for important baseline confounders and potential conflicts between external controls and trial data. Furthermore, our approach integrates important baseline covariates to account for patient heterogeneity and introduces a novel piecewise model to manage stage-wise treatment assignments. By applying this methodology to a case study in DMD research, we not only demonstrate the practical application and benefits of our approach but also highlight its potential to mitigate challenges in rare disease trials. Our findings advocate for a more nuanced and statistically robust analysis of treatment effects, thereby improving the reliability of clinical trial results.},
  archive      = {J_BIOMTC},
  author       = {Wang, Sidi and Roychoudhury, Satrajit and Kidwell, Kelley M},
  doi          = {10.1093/biomtc/ujaf103},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf103},
  shortjournal = {Biometrics},
  title        = {Evaluating longitudinal treatment effects for duchenne muscular dystrophy using dynamically enriched bayesian small sample, sequential, multiple assignment randomized trial (snSMART)},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analysis for attributable effects in case2 studies. <em>BIOMTC</em>, <em>81</em>(3), ujaf102. (<a href='https://doi.org/10.1093/biomtc/ujaf102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case 2 study, also referred to as the case–case study design, is a valuable approach for conducting inference for treatment effects. Unlike traditional case-control studies, the case 2 design compares treatment in cases of concern (the first type of case) to other cases (the second type of case). One of the quantities of interest is the attributable effect for the first type of case—that is, the number of the first type of case that would not have occurred had the treatment been withheld from all units. In some case 2 studies, a key quantity of interest is the attributable effect for the first type of case. Two key assumptions that are usually made for making inferences about this attributable effect in case 2 studies are (1) treatment does not cause the second type of case, and (2) the treatment does not alter an individual’s case type. However, these assumptions are not realistic in many real-data applications. In this article, we present a sensitivity analysis framework to scrutinize the impact of deviations from these assumptions on inferences for the attributable effect. We also include sensitivity analyses related to the assumption of unmeasured confounding, recognizing the potential bias introduced by unobserved covariates. The proposed methodology is exemplified through an investigation into whether having violent behavior in the last year of life increases suicide risk using the 1993 National Mortality Followback Survey dataset.},
  archive      = {J_BIOMTC},
  author       = {Chen, Kan and Ye, Ting and Small, Dylan S},
  doi          = {10.1093/biomtc/ujaf102},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf102},
  shortjournal = {Biometrics},
  title        = {Sensitivity analysis for attributable effects in case2 studies},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smooth and shape-constrained quantile distributed lag models. <em>BIOMTC</em>, <em>81</em>(3), ujaf101. (<a href='https://doi.org/10.1093/biomtc/ujaf101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposure to environmental pollutants during the gestational period can significantly impact infant health outcomes, such as birth weight and neurological development. Identifying critical windows of susceptibility, which are specific periods during pregnancy when exposure has the most profound effects, is essential for developing targeted interventions. Distributed lag models (DLMs) are widely used in environmental epidemiology to analyze the temporal patterns of exposure and their impact on health outcomes. However, traditional DLMs focus on modeling the conditional mean, which may fail to capture heterogeneity in the relationship between predictors and the outcome. Moreover, when modeling the distribution of health outcomes like gestational birth weight, it is the extreme quantiles that are of most clinical relevance. We introduce 2 new quantile distributed lag model (QDLM) estimators designed to address the limitations of existing methods by leveraging smoothness and shape constraints, such as unimodality and concavity, to enhance interpretability and efficiency. We apply our QDLM estimators to the Colorado birth cohort data, demonstrating their effectiveness in identifying critical windows of susceptibility and informing public health interventions.},
  archive      = {J_BIOMTC},
  author       = {Jin, Yisen and Molstad, Aaron J and Wilson, Ander and Antonelli, Joseph},
  doi          = {10.1093/biomtc/ujaf101},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf101},
  shortjournal = {Biometrics},
  title        = {Smooth and shape-constrained quantile distributed lag models},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regression analysis of interval-censored failure time data with change points and a cured subgroup. <em>BIOMTC</em>, <em>81</em>(3), ujaf100. (<a href='https://doi.org/10.1093/biomtc/ujaf100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There exists a substantial body of literature that discusses regression analysis of interval-censored failure time data and also many methods have been proposed for handling the presence of a cured subgroup. However, only limited research exists on the problems incorporating change points, with or without a cured subgroup, which can occur in various contexts such as clinical trials where disease risks may shift dramatically when certain biological indicators exceed specific thresholds. To fill this gap, we consider a class of partly linear transformation models within the mixture cure model framework and propose a sieve maximum likelihood estimation approach using Bernstein polynomials and piecewise linear functions for inference. Additionally, we provide a data-driven adaptive procedure to identify the number and locations of change points and establish the asymptotic properties of the proposed method. Extensive simulation studies demonstrate the effectiveness and practical utility of the proposed methods, which are applied to the real data from a breast cancer study that motivated this work.},
  archive      = {J_BIOMTC},
  author       = {Lou, Yichen and Du, Mingyue and Song, Xinyuan},
  doi          = {10.1093/biomtc/ujaf100},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf100},
  shortjournal = {Biometrics},
  title        = {Regression analysis of interval-censored failure time data with change points and a cured subgroup},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Negative binomial mixed effects location-scale models for intensive longitudinal count-type physical activity data provided by wearable devices. <em>BIOMTC</em>, <em>81</em>(3), ujaf099. (<a href='https://doi.org/10.1093/biomtc/ujaf099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of wearable devices, for example, accelerometers, have become increasingly prevalent. Wearable devices enable more accurate real-time tracking of a subject’s physical activity (PA) level, such as steps, number of activity bouts, or time in moderate-to-vigorous intensity PA (MVPA), which are important general health markers and can often be represented as counts. These intensive within-subject count data provided by wearable devices, for example, minutes in MVPA summarized per hour across days and even months, allow the possibility for modeling not only the mean PA level, but also the dispersion level for each subject. Especially in the context of daily PA, subjects’ dispersion levels are potentially informative in reflecting their exercise patterns: some subjects might exhibit consistent PA across time and can be considered “less dispersed” subjects; while others might have a large amount of PA at a particular time point, while being sedentary for most of the day, and can be considered “more dispersed” subjects. Thus, we propose a negative binomial mixed effects location-scale model to model these intensive longitudinal PA counts and to account for the heterogeneity in both the mean and dispersion level across subjects. Further, to handle the issue of inflated numbers of zeros in the PA data, we also propose a hurdle/zero-inflated version which additionally includes the modeling of the probability of having |$>$| 0 PA levels.},
  archive      = {J_BIOMTC},
  author       = {Ma, Qianheng and Dunton, Genevieve F and Hedeker, Donald},
  doi          = {10.1093/biomtc/ujaf099},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf099},
  shortjournal = {Biometrics},
  title        = {Negative binomial mixed effects location-scale models for intensive longitudinal count-type physical activity data provided by wearable devices},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal machine learning for heterogeneous treatment effects in the presence of missing outcome data. <em>BIOMTC</em>, <em>81</em>(3), ujaf098. (<a href='https://doi.org/10.1093/biomtc/ujaf098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When estimating heterogeneous treatment effects, missing outcome data can complicate treatment effect estimation, causing certain subgroups of the population to be poorly represented. In this work, we discuss this commonly overlooked problem and consider the impact that missing at random outcome data has on causal machine learning estimators for the conditional average treatment effect (CATE). We propose 2 de-biased machine learning estimators for the CATE, the mDR-learner, and mEP-learner, which address the issue of under-representation by integrating inverse probability of censoring weights into the DR-learner and EP-learner, respectively. We show that under reasonable conditions, these estimators are oracle efficient and illustrate their favorable performance through simulated data settings, comparing them to existing CATE estimators, including comparison to estimators that use common missing data techniques. We present an example of their application using the GBSG2 trial, exploring treatment effect heterogeneity when comparing hormonal therapies to non-hormonal therapies among breast cancer patients post surgery, and offer guidance on the decisions a practitioner must make when implementing these estimators.},
  archive      = {J_BIOMTC},
  author       = {Pryce, Matthew and Diaz-Ordaz, Karla and Keogh, Ruth H and Vansteelandt, Stijn},
  doi          = {10.1093/biomtc/ujaf098},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf098},
  shortjournal = {Biometrics},
  title        = {Causal machine learning for heterogeneous treatment effects in the presence of missing outcome data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariance-on-covariance regression. <em>BIOMTC</em>, <em>81</em>(3), ujaf097. (<a href='https://doi.org/10.1093/biomtc/ujaf097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A covariance-on-covariance regression model is introduced in this manuscript. It is assumed that there exists (at least) a pair of linear projections on outcome covariance matrices and predictor covariance matrices such that a log-linear model links the variances in the projection spaces, as well as additional covariates of interest. An ordinary least square type of estimator is proposed to simultaneously identify the projections and estimate model coefficients. Under regularity conditions, the proposed estimator is asymptotically consistent. The superior performance of the proposed approach over existing methods is demonstrated via simulation studies. Applying to data collected in the Human Connectome Project Aging study, the proposed approach identifies 3 pairs of brain networks, where functional connectivity within the resting-state network predicts functional connectivity within the corresponding task-state network. The 3 networks correspond to a global signal network, a task-related network, and a task-unrelated network. The findings are consistent with existing knowledge about brain function.},
  archive      = {J_BIOMTC},
  author       = {Zhao, Yi and Zhao, Yize},
  doi          = {10.1093/biomtc/ujaf097},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf097},
  shortjournal = {Biometrics},
  title        = {Covariance-on-covariance regression},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Valid and efficient inference for nonparametric variable importance in two-phase studies. <em>BIOMTC</em>, <em>81</em>(3), ujaf095. (<a href='https://doi.org/10.1093/biomtc/ujaf095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a common nonparametric regression setting, where the data consist of a response variable Y , some easily obtainable covariates X ⁠ , and a set of costly covariates Z ⁠ . Before establishing predictive models for Y , a natural question arises: Is it worthwhile to include Z as predictors, given the additional cost of collecting data on Z for both training the models and predicting Y for future individuals? Therefore, we aim to conduct preliminary investigations to infer importance of Z in predicting Y in the presence of X ⁠ . To achieve this goal, we propose a nonparametric variable importance measure for Z ⁠ . It is defined as a parameter that aggregates maximum potential contributions of Z in single or multiple predictive models, with contributions quantified by general loss functions. Considering two-phase data that provide a large number of observations for ( Y , X ) with the expensive Z measured only in a small subsample, we develop a novel approach to infer the proposed importance measure, accommodating missingness of Z in the sample by substituting functions of ( Y , X ) for each individual’s contribution to the predictive loss of models involving Z ⁠ . Our approach attains unified and efficient inference regardless of whether Z makes zero or positive contribution to predicting Y , a desirable yet surprising property owing to data incompleteness. As intermediate steps of our theoretical development, we establish novel results in two relevant research areas, semi-supervised inference and two-phase nonparametric estimation. Numerical results from both simulated and real data demonstrate superior performance of our approach.},
  archive      = {J_BIOMTC},
  author       = {Dai, Guorong and Carroll, Raymond J and Chen, Jinbo},
  doi          = {10.1093/biomtc/ujaf095},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf095},
  shortjournal = {Biometrics},
  title        = {Valid and efficient inference for nonparametric variable importance in two-phase studies},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved prediction and flagging of extreme random effects for non-gaussian outcomes using weighted methods. <em>BIOMTC</em>, <em>81</em>(3), ujaf094. (<a href='https://doi.org/10.1093/biomtc/ujaf094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigators often focus on predicting extreme random effects from mixed effects models fitted to longitudinal or clustered data, and on identifying or “flagging” outliers such as poorly performing hospitals or rapidly deteriorating patients. Our recent work with Gaussian outcomes showed that weighted prediction methods can substantially reduce mean square error of prediction for extremes and substantially increase correct flagging rates compared to previous methods, while controlling the incorrect flagging rates. This paper extends the weighted prediction methods to non-Gaussian outcomes such as binary and count data. Closed-form expressions for predicted random effects and probabilities of correct and incorrect flagging are not available for the usual non-Gaussian outcomes, and the computational challenges are substantial. Therefore, our results include the development of theory to support algorithms that tune predictors that we call “self-calibrated” (which control the incorrect flagging rate using very simple flagging rules) and innovative numerical methods to calculate weighted predictors as well as to evaluate their performance. Comprehensive numerical evaluations show that the novel weighted predictors for non-Gaussian outcomes have substantially lower mean square error of prediction at the extremes and considerably higher correct flagging rates than previously proposed methods, while controlling the incorrect flagging rates. We illustrate our new methods using data on emergency room readmissions for children with asthma.},
  archive      = {J_BIOMTC},
  author       = {Neuhaus, John and McCulloch, Charles and Boylan, Ross},
  doi          = {10.1093/biomtc/ujaf094},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf094},
  shortjournal = {Biometrics},
  title        = {Improved prediction and flagging of extreme random effects for non-gaussian outcomes using weighted methods},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage estimators for spatial confounding with point-referenced data. <em>BIOMTC</em>, <em>81</em>(3), ujaf093. (<a href='https://doi.org/10.1093/biomtc/ujaf093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public health data are often spatially dependent, but standard spatial regression methods can suffer from bias and invalid inference when the independent variable is associated with spatially correlated residuals. This could occur if, for example, there is an unmeasured environmental contaminant associated with the independent and outcome variables in a spatial regression analysis. Geoadditive structural equation modeling (gSEM), in which an estimated spatial trend is removed from both the explanatory and response variables before estimating the parameters of interest, has previously been proposed as a solution but there has been little investigation of gSEM’s properties with point-referenced data. We link gSEM to results on double machine learning and semiparametric regression based on two-stage procedures. We propose using these semiparametric estimators for spatial regression using Gaussian processes with Matèrn covariance to estimate the spatial trends and term this class of estimators double spatial regression (DSR). We derive regularity conditions for root- n asymptotic normality and consistency and closed-form variance estimation, and show that in simulations where standard spatial regression estimators are highly biased and have poor coverage, DSR can mitigate bias more effectively than competitors and obtain nominal coverage.},
  archive      = {J_BIOMTC},
  author       = {Wiecha, Nate and Hoppin, Jane A and Reich, Brian J},
  doi          = {10.1093/biomtc/ujaf093},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf093},
  shortjournal = {Biometrics},
  title        = {Two-stage estimators for spatial confounding with point-referenced data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using model-assisted calibration methods to improve efficiency of regression analyses using two-phase samples or pooled samples under complex survey designs. <em>BIOMTC</em>, <em>81</em>(3), ujaf092. (<a href='https://doi.org/10.1093/biomtc/ujaf092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase sampling designs are frequently applied in epidemiological studies and large-scale health surveys. In such designs, certain variables are collected exclusively within a second-phase random subsample of the initial first-phase sample, often due to factors such as high costs, response burden, or constraints on data collection or assessment. Consequently, second-phase sample estimators can be inefficient due to the diminished sample size. Model-assisted calibration methods have been used to improve the efficiency of second-phase estimators in regression analysis. However, limited literature provides valid finite population inferences of the calibration estimators that use appropriate calibration auxiliary variables while simultaneously accounting for the complex sample designs in the first- and second-phase samples. Moreover, no literature considers the “pooled design” where some covariates are measured exclusively in certain repeated survey cycles. This paper proposes calibrating the sample weights for the second-phase sample to the weighted first-phase sample based on score functions of the regression model that uses predictions of the second-phase variable for the first-phase sample. We establish the consistency of estimation using calibrated weights and provide variance estimation for the regression coefficients under the two-phase design or the pooled design nested within complex survey designs. Empirical evidence highlights the efficiency and robustness of the proposed calibration compared to existing calibration and imputation methods. Data examples from the National Health and Nutrition Examination Survey are provided.},
  archive      = {J_BIOMTC},
  author       = {Wang, Lingxiao},
  doi          = {10.1093/biomtc/ujaf092},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf092},
  shortjournal = {Biometrics},
  title        = {Using model-assisted calibration methods to improve efficiency of regression analyses using two-phase samples or pooled samples under complex survey designs},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to “Propensity weighting plus adjustment in proportional hazards model is not doubly robust,” by erin e. gabriel, michael c. sachs, ingeborg waernbaum, els goetghebeur, paul f. blanche, stijn vansteelandt, arvid sjölander, and thomas scheike; volume 80, issue 3, september 2024, https://doi.org/10.1093/biomtc/ujae069. <em>BIOMTC</em>, <em>81</em>(3), ujaf091. (<a href='https://doi.org/10.1093/biomtc/ujaf091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Gabriel, Erin E and Sachs, Michael C and Waernbaum, Ingeborg and Goetghebeur, Els and Blanche, Paul F and Vansteelandt, Stijn and Sjölander, Arvid and Scheike, Thomas},
  doi          = {10.1093/biomtc/ujaf091},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf091},
  shortjournal = {Biometrics},
  title        = {Correction to “Propensity weighting plus adjustment in proportional hazards model is not doubly robust,” by erin e. gabriel, michael c. sachs, ingeborg waernbaum, els goetghebeur, paul f. blanche, stijn vansteelandt, arvid sjölander, and thomas scheike; volume 80, issue 3, september 2024, https://doi.org/10.1093/biomtc/ujae069},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjusted predictions for generalized estimating equations. <em>BIOMTC</em>, <em>81</em>(3), ujaf090. (<a href='https://doi.org/10.1093/biomtc/ujaf090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized estimating equations (GEEs) are a popular statistical method for longitudinal data analysis, requiring specification of the first 2 marginal moments of the response along with a working correlation matrix to capture temporal correlations within a cluster. When it comes to prediction at future/new time points using GEEs, a standard approach adopted by practitioners and software is to base it simply on the marginal mean model. In this article, we propose an alternative approach to prediction for independent cluster GEEs. By viewing the GEE as solving an iterative working linear model, we borrow ideas from universal kriging to construct an adjusted predictor that exploits working cross-correlations between the current and new observations within the same cluster. We establish theoretical conditions for the adjusted GEE predictor to outperform the standard GEE predictor. Simulations and an application to longitudinal data on the growth of sitka spruces demonstrate that, even when we misspecify the working correlation structure, adjusted GEE predictors can achieve better performance relative to standard GEE predictors, the so-called “oracle” GEE predictor using all time points, and potentially even cluster-specific predictions from a generalized linear mixed model.},
  archive      = {J_BIOMTC},
  author       = {Hui, Francis K C and Muller, Samuel and Welsh, Alan H},
  doi          = {10.1093/biomtc/ujaf090},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf090},
  shortjournal = {Biometrics},
  title        = {Adjusted predictions for generalized estimating equations},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tree-based additive noise directed acyclic graphical models for nonlinear causal discovery with interactions. <em>BIOMTC</em>, <em>81</em>(3), ujaf089. (<a href='https://doi.org/10.1093/biomtc/ujaf089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed acyclic graphical models with additive noises are essential in nonlinear causal discovery and have numerous applications in various domains, such as social science and systems biology. Most such models further assume that structural causal functions are additive to ensure causal identifiability and computational feasibility, which may be too restrictive in the presence of causal interactions. Some methods consider general nonlinear causal functions represented by, for example, Gaussian processes and neural networks, to accommodate interactions. However, they are either computationally intensive or lack interpretability. We propose a highly interpretable and computationally feasible approach using trees to incorporate interactions in nonlinear causal discovery, termed tree-based additive noise models. The nature of the tree construction leads to piecewise constant causal functions, making existing causal identifiability results of additive noise models with continuous and smooth causal functions inapplicable. Therefore, we provide new conditions under which the proposed model is identifiable. We develop a recursive algorithm for source node identification and a score-based ordering search algorithm. Through extensive simulations, we demonstrate the utility of the proposed model and algorithms benchmarking against existing additive noise models, especially when there are strong causal interactions. Our method is applied to infer a protein–protein interaction network for breast cancer, where proteins may form protein complexes to perform their functions.},
  archive      = {J_BIOMTC},
  author       = {Zhou, Fangting and He, Kejun and Ni, Yang},
  doi          = {10.1093/biomtc/ujaf089},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf089},
  shortjournal = {Biometrics},
  title        = {Tree-based additive noise directed acyclic graphical models for nonlinear causal discovery with interactions},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simple simulation based reconstruction of incidence rates from death data. <em>BIOMTC</em>, <em>81</em>(3), ujaf088. (<a href='https://doi.org/10.1093/biomtc/ujaf088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Daily deaths from an infectious disease provide a means for retrospectively inferring daily incidence, given knowledge of the infection-to-death interval distribution. Existing methods for doing so rely either on fitting simplified non-linear epidemic models to the deaths data or on spline based deconvolution approaches. The former runs the risk of introducing unintended artefacts via the model formulation, while the latter may be viewed as technically obscure, impeding uptake by practitioners. This note proposes a simple simulation based approach to inferring fatal incidence from deaths that requires minimal assumptions, is easy to understand, and allows testing of alternative hypothesized incidence trajectories. The aim is that in any future situation similar to the COVID pandemic, the method can be easily, rapidly, transparently, and uncontroversially deployed as an input to management.},
  archive      = {J_BIOMTC},
  author       = {Wood, Simon N},
  doi          = {10.1093/biomtc/ujaf088},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf088},
  shortjournal = {Biometrics},
  title        = {Simple simulation based reconstruction of incidence rates from death data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A flexible framework for N-mixture occupancy models: Applications to breeding bird surveys. <em>BIOMTC</em>, <em>81</em>(3), ujaf087. (<a href='https://doi.org/10.1093/biomtc/ujaf087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating species abundance under imperfect detection is a key challenge in biodiversity conservation. The N -mixture model, widely recognized for its ability to distinguish between abundance and individual detection probability without marking individuals, is constrained by its stringent closure assumption, which leads to biased estimates when violated in real-world settings. To address this limitation, we propose an extended framework based on a development of the mixed Gamma-Poisson model, incorporating a community parameter that represents the proportion of individuals consistently present throughout the survey period. This flexible framework generalizes both the zero-inflated type occupancy model and the standard N -mixture model as special cases, corresponding to community parameter values of 0 and 1, respectively. The model’s effectiveness is validated through simulations and applications to real-world datasets, specifically with 5 species from the North American Breeding Bird Survey and 46 species from the Swiss Breeding Bird Survey, demonstrating its improved accuracy and adaptability in settings where strict closure may not hold.},
  archive      = {J_BIOMTC},
  author       = {Huynh, Huu-Dinh and Royle, J Andrew and Hwang, Wen-Han},
  doi          = {10.1093/biomtc/ujaf087},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf087},
  shortjournal = {Biometrics},
  title        = {A flexible framework for N-mixture occupancy models: Applications to breeding bird surveys},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple tests for restricted mean time lost with competing risks data. <em>BIOMTC</em>, <em>81</em>(3), ujaf086. (<a href='https://doi.org/10.1093/biomtc/ujaf086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Easy-to-interpret effect estimands are highly desirable in survival analysis. In the competing risks framework, one good candidate is the restricted mean time lost (RMTL). It is defined as the area under the cumulative incidence function up to a prespecified time point and, thus, it summarizes the cumulative incidence function into a meaningful estimand. While existing RMTL-based tests are limited to 2-sample comparisons and mostly to 2 event types, we aim to develop general contrast tests for factorial designs and an arbitrary number of event types based on a Wald-type test statistic. Furthermore, we avoid the often-made, rather restrictive continuity assumption on the event time distribution. This allows for ties in the data, which often occur in practical applications, for example, when event times are measured in whole days. In addition, we develop more reliable tests for RMTL comparisons that are based on a permutation approach to improve the small sample performance. In a second step, multiple tests for RMTL comparisons are developed to test several null hypotheses simultaneously. Here, we incorporate the asymptotically exact dependence structure between the local test statistics to gain more power. The small sample performance of the proposed testing procedures is analyzed in simulations and finally illustrated by analyzing a real-data example about leukemia patients who underwent bone marrow transplantation.},
  archive      = {J_BIOMTC},
  author       = {Munko, Merle and Dobler, Dennis and Ditzhaus, Marc},
  doi          = {10.1093/biomtc/ujaf086},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf086},
  shortjournal = {Biometrics},
  title        = {Multiple tests for restricted mean time lost with competing risks data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A positivity robust strategy to study effects of switching treatment. <em>BIOMTC</em>, <em>81</em>(3), ujaf085. (<a href='https://doi.org/10.1093/biomtc/ujaf085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies of medical treatments, individuals often experience post-treatment events that predict their future outcomes. In this work, we study how to use initial observations of a recurrent event—a type of post-treatment event—to offer updated treatment recommendations in settings where no, or few, individuals are observed to switch between treatment arms. Specifically, we formulate an estimand quantifying the average effect of switching treatment on subsequent events. We derive bounds on the value of this estimand under plausible conditions and propose non-parametric estimators of the bounds. Furthermore, we define a value and regret function for a dynamic treatment-switching regime, and use these to determine 3 types of optimal regimes under partial identification: the pessimist (maximin value), optimist (maximax value), and opportunist (minimax regret) regimes. The pessimist regime is guaranteed to perform at least as well as the standard of care. We apply our methods to data from the Systolic Blood Pressure Intervention Trial.},
  archive      = {J_BIOMTC},
  author       = {Janvin, Matias and Ryalen, Pål C and Sarvet, Aaron L and Stensrud, Mats J},
  doi          = {10.1093/biomtc/ujaf085},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf085},
  shortjournal = {Biometrics},
  title        = {A positivity robust strategy to study effects of switching treatment},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust nonparametric estimators of the predictive value of covariates for survival data. <em>BIOMTC</em>, <em>81</em>(3), ujaf084. (<a href='https://doi.org/10.1093/biomtc/ujaf084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The predictive value of a covariate is often of interest in studies with a survival endpoint. A common situation is that there are some well established predictors and a potential valuable new marker. The challenge is how to judge the potentially added predictive value of this new marker. We propose to use the positive predictive value (PPV) curve based on a nonparametric scoring rule. The estimand of interest is viewed as a single transformation of the underlying data generating probability measure, which allows us to develop a robust nonparametric estimator of the PPV by first calculating the corresponding efficient influence function. We provide asymptotic results and illustrate the approach with numerical studies and with 2 cancer data studies.},
  archive      = {J_BIOMTC},
  author       = {Martinussen, Torben and van der Laan, Mark J},
  doi          = {10.1093/biomtc/ujaf084},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf084},
  shortjournal = {Biometrics},
  title        = {Doubly robust nonparametric estimators of the predictive value of covariates for survival data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency band analysis of nonstationary multivariate time series. <em>BIOMTC</em>, <em>81</em>(3), ujaf083. (<a href='https://doi.org/10.1093/biomtc/ujaf083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information from frequency bands in biomedical time series provides useful summaries of the observed signal. Many existing methods consider summaries of the time series obtained over a few well-known, pre-defined frequency bands of interest. However, there is a dearth of data-driven methods for identifying frequency bands that optimally summarize frequency-domain information in the time series. A new method to identify partition points in the frequency space of a multivariate locally stationary time series is proposed. These partition points signify changes across frequencies in the time-varying behavior of the signal and provide frequency band summary measures that best preserve nonstationary dynamics of the observed series. An |$L_2$| -norm based discrepancy measure that finds differences in the time-varying spectral density matrix is constructed, and its asymptotic properties are derived. New nonparametric bootstrap tests are also provided to identify significant frequency partition points and to identify components and cross-components of the spectral matrix exhibiting changes over frequencies. Finite-sample performance of the proposed method is illustrated via simulations. The proposed method is used to develop optimal frequency band summary measures for characterizing time-varying behavior in resting-state electroencephalography time series, as well as identifying components and cross-components associated with each frequency partition point.},
  archive      = {J_BIOMTC},
  author       = {Sundararajan, Raanju R and Bruce, Scott A},
  doi          = {10.1093/biomtc/ujaf083},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf083},
  shortjournal = {Biometrics},
  title        = {Frequency band analysis of nonstationary multivariate time series},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse 2-stage bayesian meta-analysis for individualized treatments. <em>BIOMTC</em>, <em>81</em>(3), ujaf082. (<a href='https://doi.org/10.1093/biomtc/ujaf082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment rules tailor treatments to patients based on clinical, demographic, and other characteristics. Estimation of individualized treatment rules requires the identification of individuals who benefit most from the particular treatments and thus the detection of variability in treatment effects. To develop an effective individualized treatment rule, data from multisite studies may be required due to the low power provided by smaller datasets for detecting the often small treatment-covariate interactions. However, sharing of individual-level data is sometimes constrained. Furthermore, sparsity may arise in 2 senses: different data sites may recruit from different populations, making it infeasible to estimate identical models or all parameters of interest at all sites, and the number of non-zero parameters in the model for the treatment rule may be small. To address these issues, we adopt a 2-stage Bayesian meta-analysis approach to estimate individualized treatment rules which optimize expected patient outcomes using multisite data without disclosing individual-level data beyond the sites. Simulation results demonstrate that our approach can provide consistent estimates of the parameters which fully characterize the optimal individualized treatment rule. We estimate the optimal Warfarin dose strategy using data from the International Warfarin Pharmacogenetics Consortium, where data sparsity and small treatment-covariate interaction effects pose additional statistical challenges.},
  archive      = {J_BIOMTC},
  author       = {Shen, Junwei and Moodie, Erica E M and Golchi, Shirin},
  doi          = {10.1093/biomtc/ujaf082},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf082},
  shortjournal = {Biometrics},
  title        = {Sparse 2-stage bayesian meta-analysis for individualized treatments},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on age-specific fertility in ecology and evolution. learning from other disciplines and improving the state of the art. <em>BIOMTC</em>, <em>81</em>(3), ujaf081. (<a href='https://doi.org/10.1093/biomtc/ujaf081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the importance of age-specific fertility for ecology and evolution, the methods for modeling and inference have proven considerably limited. However, other disciplines have long focused on exploring and developing a vast number of models. Here, I provide an overview of the different models proposed since the 1940s by formal demographers, statisticians, and social scientists, most of which are unknown to the ecological and evolutionary communities. I describe how these fall into 2 main categories, namely polynomials and those based on probability density functions. I discuss their merits in terms of their overall behavior and how well they represent the different stages of fertility. Despite many alternative models, inference on age-specific fertility has usually been limited to simple least squares. Although this might be sufficient for human data, I hope to demonstrate that inference requires more sophisticated approaches for ecological and evolutionary datasets. To illustrate how inference and model choice can be achieved on different types of typical ecological and evolutionary data, I present the new R package Bayesian Fertility Trajectory Analysis, which I apply to published aggregated data for lions and baboons. I then conduct a simulation study to test its performance on individual-level data. I show that appropriate inference and model selection can be achieved even when a small number of parents are followed.},
  archive      = {J_BIOMTC},
  author       = {Colchero, Fernando},
  doi          = {10.1093/biomtc/ujaf081},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf081},
  shortjournal = {Biometrics},
  title        = {Inference on age-specific fertility in ecology and evolution. learning from other disciplines and improving the state of the art},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to environmental data science by jerry d. davis, chapman and Hall/CRC, 2023, ISBN: 9781003317821 https://www.routledge.com/Introduction-to-environmental-data-science/Davis/p/book/9781003317821. <em>BIOMTC</em>, <em>81</em>(3), ujaf079. (<a href='https://doi.org/10.1093/biomtc/ujaf079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Sheppard, Lianne and Blanco, Magali N},
  doi          = {10.1093/biomtc/ujaf079},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf079},
  shortjournal = {Biometrics},
  title        = {Introduction to environmental data science by jerry d. davis, chapman and Hall/CRC, 2023, ISBN: 9781003317821 https://www.routledge.com/Introduction-to-environmental-data-science/Davis/p/book/9781003317821},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Change point analysis for time series by lajos horváth and gregory rice, springer, 2024, ISBN: 9783031516085https://link.springer.com/book/10.1007/978-3-031-51609-2. <em>BIOMTC</em>, <em>81</em>(3), ujaf078. (<a href='https://doi.org/10.1093/biomtc/ujaf078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Han, Fang},
  doi          = {10.1093/biomtc/ujaf078},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf078},
  shortjournal = {Biometrics},
  title        = {Change point analysis for time series by lajos horváth and gregory rice, springer, 2024, ISBN: 9783031516085https://link.springer.com/book/10.1007/978-3-031-51609-2},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of gene therapies: Strategic, scientific, regulatory, and access considerations by avery McIntosh and oleksandr sverdlov, chapman and Hall/CRC, 2024, ISBN: 9781032136554 https://www.routledge.com/Development-of-gene-therapies-strategic-scientific-regulatory-and-access-Considerations/McIntosh-sverdlov/p/book/9781032136554. <em>BIOMTC</em>, <em>81</em>(3), ujaf070. (<a href='https://doi.org/10.1093/biomtc/ujaf070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yuan, Ying},
  doi          = {10.1093/biomtc/ujaf070},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf070},
  shortjournal = {Biometrics},
  title        = {Development of gene therapies: Strategic, scientific, regulatory, and access considerations by avery McIntosh and oleksandr sverdlov, chapman and Hall/CRC, 2024, ISBN: 9781032136554 https://www.routledge.com/Development-of-gene-therapies-strategic-scientific-regulatory-and-access-Considerations/McIntosh-sverdlov/p/book/9781032136554},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cumulative incidence function estimation using population-based biobank data. <em>BIOMTC</em>, <em>81</em>(3), ujaf049. (<a href='https://doi.org/10.1093/biomtc/ujaf049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many countries have established population-based biobanks, which are being used increasingly in epidemiological and clinical research. These biobanks offer opportunities for large-scale studies addressing questions beyond the scope of traditional clinical trials or cohort studies. However, using biobank data poses new challenges. Typically, biobank data are collected from a study cohort recruited over a defined calendar period, with subjects entering the study at various ages falling between c L and c U ⁠ . This work focuses on biobank data with individuals reporting disease-onset age upon recruitment, termed prevalent data, along with individuals initially recruited as healthy, and their disease onset observed during the follow-up period. We propose a novel cumulative incidence function (CIF) estimator that efficiently incorporates prevalent cases, in contrast to existing methods, providing two advantages: (1) increased efficiency and (2) CIF estimation for ages before the lower limit, c L ⁠ .},
  archive      = {J_BIOMTC},
  author       = {Gorfine, Malka and Zucker, David M and Shoham, Shoval},
  doi          = {10.1093/biomtc/ujaf049},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf049},
  shortjournal = {Biometrics},
  title        = {Cumulative incidence function estimation using population-based biobank data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precision generalized phase I-II designs. <em>BIOMTC</em>, <em>81</em>(3), ujaf043. (<a href='https://doi.org/10.1093/biomtc/ujaf043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new family of precision Bayesian dose optimization designs, PGen I-II, based on early efficacy, early toxicity, and long-term time to treatment failure is proposed. A PGen I-II design refines a Gen I-II design by accounting for patient heterogeneity characterized by subgroups that may be defined by prognostic levels, disease subtypes, or biomarker categories. The design makes subgroup-specific decisions, which may be to drop an unacceptably toxic or inefficacious dose, randomize patients among acceptable doses, or identify a best dose in terms of treatment success defined in terms of time to failure over long-term follow-up. A piecewise exponential distribution for failure time is assumed, including subgroup-specific effects of dose, response, and toxicity. Latent variables are used to adaptively cluster subgroups found to have similar dose-outcome distributions, with the model simplified to borrow strength between subgroups in the same cluster. Guidelines and user-friendly computer software for implementing the design are provided. A simulation study is reported that shows the PGen I-II design is superior to similarly structured designs that either assume patient homogeneity or conduct separate trials within subgroups.},
  archive      = {J_BIOMTC},
  author       = {Zhao, Saijun and Thall, Peter F and Yuan, Ying and Lee, Juhee and Msaouel, Pavlos and Zang, Yong},
  doi          = {10.1093/biomtc/ujaf043},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf043},
  shortjournal = {Biometrics},
  title        = {Precision generalized phase I-II designs},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture models: Parametric, semiparametric, and new directions by weixin yao and sijia xiang, chapman and Hall/CRC, 2024, ISBN: 9780367481827 https://www.routledge.com/Mixture-models-parametric-semiparametric-and-new-directions/Yao-xiang/p/book/9780367481827. <em>BIOMTC</em>, <em>81</em>(3), ujaf031. (<a href='https://doi.org/10.1093/biomtc/ujaf031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Chen, Li-Pang},
  doi          = {10.1093/biomtc/ujaf031},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf031},
  shortjournal = {Biometrics},
  title        = {Mixture models: Parametric, semiparametric, and new directions by weixin yao and sijia xiang, chapman and Hall/CRC, 2024, ISBN: 9780367481827 https://www.routledge.com/Mixture-models-parametric-semiparametric-and-new-directions/Yao-xiang/p/book/9780367481827},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="biostat">BIOSTAT - 63</h2>
<ul>
<li><details>
<summary>
(2025). The winner’s curse under dependence: Repairing empirical bayes using convoluted densities. <em>BIOSTAT</em>, <em>26</em>(1), kxaf025. (<a href='https://doi.org/10.1093/biostatistics/kxaf025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The winner’s curse is a form of selection bias that arises when estimates are obtained for a large number of features, but only a subset of most extreme estimates is reported. It occurs in large scale significance testing as well as in rank-based selection, and imperils reproducibility of findings and follow-up study design. Several methods correcting for this selection bias have been proposed, but questions remain on their susceptibility to dependence between features since theoretical analyses and comparative studies are few. We prove that estimation through Tweedie’s formula is biased in presence of strong dependence, and propose a convolution of its density estimator to restore its competitive performance, which also aids other empirical Bayes methods. Furthermore, we perform a comprehensive simulation study comparing different classes of winner’s curse correction methods for point estimates as well as confidence intervals under dependence. We find a bootstrap method and empirical Bayes methods with density convolution to perform best at correcting the selection bias, although this correction generally does not improve the feature ranking. Finally, we apply the methods to a comparison of single-feature versus multi-feature prediction models in predicting Brassica napus phenotypes from gene expression data, demonstrating that the superiority of the best single-feature model may be illusory.},
  archive      = {J_BIOSTAT},
  author       = {Hawinkel, Stijn and Thas, Olivier and Maere, Steven},
  doi          = {10.1093/biostatistics/kxaf025},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf025},
  shortjournal = {Biostatistics},
  title        = {The winner’s curse under dependence: Repairing empirical bayes using convoluted densities},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based dimensionality reduction for single-cell RNA-seq using generalized bilinear models. <em>BIOSTAT</em>, <em>26</em>(1), kxaf024. (<a href='https://doi.org/10.1093/biostatistics/kxaf024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction is a critical step in the analysis of single-cell RNA-seq (scRNA-seq) data. The standard approach is to apply a transformation to the count matrix followed by principal components analysis (PCA). However, this approach can induce spurious heterogeneity and mask true biological variability. An alternative approach is to directly model the counts, but existing methods tend to be computationally intractable on large datasets and do not quantify uncertainty in the low-dimensional representation. To address these problems, we develop scGBM, a novel method for model-based dimensionality reduction of scRNA-seq data using a Poisson bilinear model. We introduce a fast estimation algorithm to fit the model using iteratively reweighted singular value decompositions, enabling the method to scale to datasets with millions of cells. Furthermore, scGBM quantifies the uncertainty in each cell’s latent position and leverages these uncertainties to assess the confidence associated with a given cell clustering. On real and simulated single-cell data, we find that scGBM produces low-dimensional embeddings that better capture relevant biological information while removing unwanted variation.},
  archive      = {J_BIOSTAT},
  author       = {Nicol, Phillip B and Miller, Jeffrey W},
  doi          = {10.1093/biostatistics/kxaf024},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf024},
  shortjournal = {Biostatistics},
  title        = {Model-based dimensionality reduction for single-cell RNA-seq using generalized bilinear models},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust transfer learning for individualized treatment rules in the presence of missing data. <em>BIOSTAT</em>, <em>26</em>(1), kxaf023. (<a href='https://doi.org/10.1093/biostatistics/kxaf023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment rule (ITR) is a stepping stone to precision medicine. To ensure validity, ITRs are ideally derived from randomized trial data, but the use cases of ITRs extend beyond these trial populations. Transferring knowledge from experimental data to real-world data is of interest, while experimental data with selective inclusion criteria reflect a population distribution that may differ from the real-world target. In well-designed experiments, granular information crucial to decision making can be thoroughly collected. However, part of this may not be accessible in real-world scenarios. We propose a learning scheme for ITR that simultaneously addresses the issues of covariate shift and missing covariates with a quantile-based optimal treatment objective. Specifically, we compare the outcome uncertainty across treatment arms that is due to missing covariates and use it to guide treatment selection to reduce the likelihood of worse outcomes. The performance of this method is evaluated in simulations and a sepsis data application.},
  archive      = {J_BIOSTAT},
  author       = {Sui, Zhiyu and Ding, Ying and Tang, Lu},
  doi          = {10.1093/biostatistics/kxaf023},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf023},
  shortjournal = {Biostatistics},
  title        = {Robust transfer learning for individualized treatment rules in the presence of missing data},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging population information in brain connectivity via bayesian ICA with a novel informative prior for correlation matrices. <em>BIOSTAT</em>, <em>26</em>(1), kxaf022. (<a href='https://doi.org/10.1093/biostatistics/kxaf022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain functional connectivity (FC), the temporal synchrony between brain networks, is essential to understand the functional organization of the brain and to identify changes due to neurological disorders, development, treatment, and other phenomena. Independent component analysis (ICA) is a matrix decomposition method used extensively for simultaneous estimation of functional brain topography and connectivity. However, estimation of FC via ICA is often sub-optimal due to the use of ad hoc estimation methods or temporal dimension reduction prior to ICA. Bayesian ICA can avoid dimension reduction, estimate latent variables and model parameters more accurately, and facilitate posterior inference. In this article, we develop a novel, computationally feasible Bayesian ICA method with population-derived priors on both the spatial ICs and their temporal correlation (that is, their FC). For the latter, we consider two priors: the inverse-Wishart, which is conjugate but is not ideally suited for modeling correlation matrices; and a novel informative prior for correlation matrices. For each prior, we derive a variational Bayes algorithm to estimate the model variables and facilitate posterior inference. Through extensive simulation studies, we evaluate the performance of the proposed methods and benchmark against existing approaches. We also analyze fMRI data from over 400 healthy adults in the Human Connectome Project. We find that our Bayesian ICA model and algorithms result in more accurate measures of functional connectivity and spatial brain features. Our novel prior for correlation matrices is more computationally intensive than the inverse-Wishart but provides improved accuracy and inference. The proposed framework is applicable to single-subject analysis, making it potentially clinically viable.},
  archive      = {J_BIOSTAT},
  author       = {Mejia, Amanda F and Bolin, David and Spencer, Daniel A and Eloyan, Ani},
  doi          = {10.1093/biostatistics/kxaf022},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf022},
  shortjournal = {Biostatistics},
  title        = {Leveraging population information in brain connectivity via bayesian ICA with a novel informative prior for correlation matrices},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control arm augmentation and hierarchical modeling in time-to-event trials: Advantages and pitfalls. <em>BIOSTAT</em>, <em>26</em>(1), kxaf021. (<a href='https://doi.org/10.1093/biostatistics/kxaf021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, it is often valuable to borrow information from external data sources. Unfortunately, when the external data are fully or partially incompatible with the current trial data, type I error rates can be highly inflated under traditional blanket discounting schemes such as power priors, commensurate priors, and meta-analytic predictive priors. However, such inflation of the probability of a false positive can be necessary, as the alternative is to have an underpowered study. For clinical trials with time-to-event (TTE) outcomes, this problem is exacerbated since many observations are censored. In this paper, we develop the latent exchangeability prior for TTE data. We also present a novel framework to borrow information about the treatment effect between groups as well as incorporate information from external controls. Simulation results suggest that, although efficiency gains can be achieved by borrowing information among external controls, operating characteristics in general can be quite poor under a lack of exchangeability. We apply our approach to a real clinical trial in second-line metastatic colorectal cancer.},
  archive      = {J_BIOSTAT},
  author       = {Alt, Ethan M and Chang, Xiuya and Liu, Qing and Jiang, Xun and Mo, May and Xia, H Amy and Ibrahim, Joseph G},
  doi          = {10.1093/biostatistics/kxaf021},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf021},
  shortjournal = {Biostatistics},
  title        = {Control arm augmentation and hierarchical modeling in time-to-event trials: Advantages and pitfalls},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mediation with external summary statistic information. <em>BIOSTAT</em>, <em>26</em>(1), kxaf020. (<a href='https://doi.org/10.1093/biostatistics/kxaf020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental health studies are increasingly measuring endogenous omics data ( ⁠|$ \boldsymbol{M} $|⁠ ) to study intermediary biological pathways by which an exogenous exposure ( ⁠|$ \boldsymbol{A} $|⁠ ) affects a health outcome ( ⁠|$ \boldsymbol{Y} $|⁠ ), given confounders ( ⁠|$ \boldsymbol{C} $|⁠ ). Mediation analysis is frequently performed to understand such mechanisms. If intermediary pathways are of interest, then there is likely literature establishing statistical and biological significance of the total effect, defined as the effect of |$ \boldsymbol{A} $| on |$ \boldsymbol{Y} $| given |$ \boldsymbol{C} $|⁠ . For mediation models with continuous outcomes and mediators, we show that leveraging external summary-level information on the total effect can improve estimation efficiency of the direct and indirect effects. Moreover, the efficiency gain depends on the asymptotic partial |$ R^{2} $| between the outcome ( ⁠|$ \boldsymbol{Y}\mid\boldsymbol{M},\boldsymbol{A},\boldsymbol{C} $|⁠ ) and total effect ( ⁠|$ \boldsymbol{Y}\mid\boldsymbol{A},\boldsymbol{C} $|⁠ ) models, with smaller (larger) values benefiting direct (indirect) effect estimation. We propose a robust data-adaptive estimation procedure, Mediation with External Summary Statistic Information, to improve estimation efficiency in settings with congenial external information, while simultaneously protecting against bias in settings with incongenial external information. In congenial simulation scenarios, we observe relative efficiency gains for mediation effect estimation of up to 40%. We illustrate our methodology using data from the Puerto Rico Testsite for Exploring Contamination Threats, where Cytochrome p450 metabolites are hypothesized to mediate the effect of phthalate exposure on gestational age at delivery. External summary information on the total effect comes from a recently published pooled analysis of 16 studies. The proposed framework blends mediation analysis with emerging data integration techniques.},
  archive      = {J_BIOSTAT},
  author       = {Boss, Jonathan and Hao, Wei and Cathey, Amber and Welch, Barrett M and Ferguson, Kelly K and Meeker, John D and Zhou, Xiang and Kang, Jian and Mukherjee, Bhramar},
  doi          = {10.1093/biostatistics/kxaf020},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf020},
  shortjournal = {Biostatistics},
  title        = {Mediation with external summary statistic information},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal functional mediation analysis with an application to functional magnetic resonance imaging data. <em>BIOSTAT</em>, <em>26</em>(1), kxaf019. (<a href='https://doi.org/10.1093/biostatistics/kxaf019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primary goal of task-based functional magnetic resonance imaging (fMRI) studies is to quantify the effective connectivity between brain regions when stimuli are presented. Assessing the dynamics of effective connectivity has attracted increasing attention. Causal mediation analysis serves as a widely implemented tool aiming to delineate the mechanism between task stimuli and brain activations. However, the case, where the treatment, mediator, and outcome are continuous functions, has not been studied. Causal mediation analysis for functional data is considered. Semiparametric functional linear structural equation models are introduced and causal assumptions are discussed. The proposed models allow for the estimation of individual effect curves. The models are applied to a task-based fMRI study, providing a new perspective of studying dynamic brain connectivity. The R package cfma for implementation is available on CRAN.},
  archive      = {J_BIOSTAT},
  author       = {Zhao, Yi and Luo, Xi and Sobel, Michael E and Lindquist, Martin A and Caffo, Brian S},
  doi          = {10.1093/biostatistics/kxaf019},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf019},
  shortjournal = {Biostatistics},
  title        = {Causal functional mediation analysis with an application to functional magnetic resonance imaging data},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A surrogate endpoint-based provisional approval causal roadmap, illustrated by vaccine development. <em>BIOSTAT</em>, <em>26</em>(1), kxaf018. (<a href='https://doi.org/10.1093/biostatistics/kxaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many rare diseases with no approved preventive interventions, promising interventions exist. However, it has proven difficult to conduct a pivotal phase 3 trial that could provide direct evidence demonstrating a beneficial effect of the intervention on the target disease outcome. When a promising putative surrogate endpoint(s) for the target outcome is available, surrogate-based provisional approval of an intervention may be pursued. Following the general Causal Roadmap rubric, we describe a surrogate endpoint-based provisional approval causal roadmap. Based on an observational study data set and a phase 3 randomized trial data set, this roadmap defines an approach to analyze the combined data set to draw a conservative inference about the treatment effect (TE) on the target outcome in the phase 3 study population. The observational study enrolls untreated individuals and collects baseline covariates, surrogate endpoints, and the target outcome, and is used to estimate the surrogate index—the regression of the target outcome on the surrogate endpoints and baseline covariates. The phase 3 trial randomizes participants to treated vs. untreated and collects the same data but is much smaller and hence very underpowered to directly assess TE, such that inference on TE is based on the surrogate index. This inference is made conservative by specifying 2 bias functions: one that expresses an imperfection of the surrogate index as a surrogate endpoint in the phase 3 study, and the other that expresses imperfect transport of the surrogate index in the untreated from the observational to the phase 3 study. Plug-in and nonparametric efficient one-step estimators of TE, with inferential procedures, are developed. The finite-sample performance of the estimators is evaluated in simulation studies. The causal roadmap is motivated by and illustrated with contemporary Group B Streptococcus vaccine development.},
  archive      = {J_BIOSTAT},
  author       = {Gilbert, Peter B and Peng, James and Han, Larry and Lange, Theis and Lu, Yun and Nie, Lei and Shih, Mei-Chiung and Waddy, Salina P and Wiley, Ken and Yann, Margot and Zafari, Zafar and Ghosh, Debashis and Follmann, Dean and Juraska, Michal and Díaz, Iván},
  doi          = {10.1093/biostatistics/kxaf018},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf018},
  shortjournal = {Biostatistics},
  title        = {A surrogate endpoint-based provisional approval causal roadmap, illustrated by vaccine development},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed lag interaction model with index modification. <em>BIOSTAT</em>, <em>26</em>(1), kxaf017. (<a href='https://doi.org/10.1093/biostatistics/kxaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiological evidence supports an association between exposure to air pollution during pregnancy and birth and child health outcomes. Typically, such associations are estimated by regressing an outcome on daily or weekly measures of exposure during pregnancy using a distributed lag model. However, these associations may be modified by multiple factors. We propose a distributed lag interaction model with index modification that allows for effect modification of a functional predictor by a weighted average of multiple modifiers. Our model allows for simultaneous estimation of modifier index weights and the exposure–time–response function via a spline cross-basis in a Bayesian hierarchical framework. Through simulations, we showed that our model out-performs competing methods when there are multiple modifiers of unknown importance. We applied our proposed method to a Colorado birth cohort to estimate the association between birth weight and air pollution modified by a neighborhood-vulnerability index and to a Mexican birth cohort to estimate the association between birthing-parent cardio-metabolic endpoints and air pollution modified by a birthing-parent lifetime stress index.},
  archive      = {J_BIOSTAT},
  author       = {Demateis, Danielle and India-Aldana, Sandra and Wright, Robert O and Wright, Rosalind J and Baccarelli, Andrea and Colicino, Elena and Wilson, Ander and Keller, Kayleigh P},
  doi          = {10.1093/biostatistics/kxaf017},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf017},
  shortjournal = {Biostatistics},
  title        = {Distributed lag interaction model with index modification},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating historic information to further improve power when conducting bayesian information borrowing in basket trials. <em>BIOSTAT</em>, <em>26</em>(1), kxaf016. (<a href='https://doi.org/10.1093/biostatistics/kxaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In basket trials a single therapeutic treatment is tested on several patient populations simultaneously, each of which forming a basket, where patients across all baskets on the trial share a common genetic aberration. These trials allow testing of treatments on small groups of patients, however, limited basket sample sizes can result in inadequate precision and power of estimates. It is well known that Bayesian information borrowing models such as the exchangeability-nonexchangeability (EXNEX) model can be implemented to tackle such a problem, drawing on information from one basket when making inference in another. An alternative approach to improve power of estimates, is to incorporate any historical or external information available. This paper considers models that amalgamate both forms of information borrowing, allowing borrowing between baskets in the ongoing trial whilst also drawing on response data from historical sources, with the aim to further improve treatment effect estimates. We propose several Bayesian information borrowing approaches that incorporate historical information into the model. These methods are data-driven, updating the degree of borrowing based on the level of homogeneity between information sources. A thorough simulation study is presented to draw comparisons between the proposed approaches, whilst also comparing to the standard EXNEX model in which no historical information is utilized. The models are also applied to a real-life trial example to demonstrate their performance in practice. We show that the incorporation of historic data under the novel approaches can lead to a substantial improvement in precision and power of treatment effect estimates when such data is homogeneous to the responses in the ongoing trial. Under some approaches, this came alongside an inflation in type I error rate in cases of heterogeneity. However, the use of a power prior in the EXNEX model is shown to increase power and precision, whilst maintaining similar error rates to the standard EXNEX model.},
  archive      = {J_BIOSTAT},
  author       = {Daniells, Libby and Mozgunov, Pavel and Barnett, Helen and Bedding, Alun and Jaki, Thomas},
  doi          = {10.1093/biostatistics/kxaf016},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf016},
  shortjournal = {Biostatistics},
  title        = {Incorporating historic information to further improve power when conducting bayesian information borrowing in basket trials},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and estimation of causal effects with confounders missing not at random. <em>BIOSTAT</em>, <em>26</em>(1), kxaf015. (<a href='https://doi.org/10.1093/biostatistics/kxaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making causal inferences from observational studies can be challenging when confounders are missing not at random. In such cases, identifying causal effects is often not guaranteed. Motivated by a real example, we consider a treatment-independent missingness assumption under which we establish the identification of causal effects when confounders are missing not at random. We propose a weighted estimating equation approach for estimating model parameters and introduce three estimators for the average causal effect, based on regression, propensity score weighting, and doubly robust estimation. We evaluate the performance of these estimators through simulations, and provide a real data analysis to illustrate our proposed method.},
  archive      = {J_BIOSTAT},
  author       = {Sun, Jian and Fu, Bo},
  doi          = {10.1093/biostatistics/kxaf015},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf015},
  shortjournal = {Biostatistics},
  title        = {Identification and estimation of causal effects with confounders missing not at random},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biomarker-assisted reporting in nutritional epidemiology: Addressing measurement error in exposure–disease associations. <em>BIOSTAT</em>, <em>26</em>(1), kxaf014. (<a href='https://doi.org/10.1093/biostatistics/kxaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In nutritional epidemiology, self-reported dietary data are commonly used to investigate diet–disease relationships. However, the resulting association estimates are often subject to biases due to random and systematic measurement errors. Regression calibration has emerged as a crucial method for addressing these biases by refining self-reported nutrient intake with objective biomarkers, which differ from the true values only by a random “noise” component. This paper presents methodological tools for analyzing nutritional epidemiology cohort studies involving time-to-event data when a biomarker subsample is available alongside dietary assessments. We introduce novel regression calibration methods to tackle two common challenges in this field. First, a widely used approach assumes that the log hazard ratio (HR) follows a linear function of dietary exposure. However, assessing whether this assumption holds—or if a more flexible model is needed to capture potential deviations from linearity—is often necessary. Second, another prevalent analytical strategy involves estimating HRs based on categorized dietary exposure variables. New methods are critically needed to minimize bias in defining category boundaries and estimating hazard ratios within exposure categories, both of which can be distorted by measurement error. We apply these methods to reassess the relationship between sodium and potassium intake and cardiovascular disease risk using data from the Women’s Health Initiative.},
  archive      = {J_BIOSTAT},
  author       = {Huang, Ying and Prentice, Ross L},
  doi          = {10.1093/biostatistics/kxaf014},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf014},
  shortjournal = {Biostatistics},
  title        = {Biomarker-assisted reporting in nutritional epidemiology: Addressing measurement error in exposure–disease associations},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting distributions of physical activity profiles in the national health and nutrition examination survey database using a partially linear fréchet single index model. <em>BIOSTAT</em>, <em>26</em>(1), kxaf013. (<a href='https://doi.org/10.1093/biostatistics/kxaf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object-oriented data analysis is a fascinating and evolving field in modern statistical science, with the potential to make significant contributions to biomedical applications. This statistical framework facilitates the development of new methods to analyze complex data objects that capture more information than traditional clinical biomarkers. This paper applies the object-oriented framework to analyze physical activity levels, measured by accelerometers, as response objects in a regression model. Unlike traditional summary metrics, we utilize a recently proposed representation of physical activity data as a distributional object, providing a more nuanced and complete profile of individual energy expenditure across all ranges of monitoring intensity. A novel hybrid Fréchet regression model is proposed and applied to US population accelerometer data from National Health and Nutrition Examination Survey (NHANES) 2011 to 2014. The semi-parametric nature of the model allows for the inclusion of nonlinear effects for critical variables, such as age, which are biologically known to have subtle impacts on physical activity. Simultaneously, the inclusion of linear effects preserves interpretability for other variables, particularly categorical covariates such as ethnicity and sex. The results obtained are valuable from a public health perspective and could lead to new strategies for optimizing physical activity interventions in specific American subpopulations.},
  archive      = {J_BIOSTAT},
  author       = {Matabuena, Marcos and Ghosal, Aritra and Meiring, Wendy and Petersen, Alexander},
  doi          = {10.1093/biostatistics/kxaf013},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf013},
  shortjournal = {Biostatistics},
  title        = {Predicting distributions of physical activity profiles in the national health and nutrition examination survey database using a partially linear fréchet single index model},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing the mean–variance relationship in spatially resolved transcriptomics data with spoon. <em>BIOSTAT</em>, <em>26</em>(1), kxaf012. (<a href='https://doi.org/10.1093/biostatistics/kxaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important task in the analysis of spatially resolved transcriptomics (SRT) data is to identify spatially variable genes (SVGs), or genes that vary in a 2D space. Current approaches rank SVGs based on either |$ P $| -values or an effect size, such as the proportion of spatial variance. However, previous work in the analysis of RNA-sequencing data identified a technical bias with log-transformation, violating the “mean–variance relationship” of gene counts, where highly expressed genes are more likely to have a higher variance in counts but lower variance after log-transformation. Here, we demonstrate the mean–variance relationship in SRT data. Furthermore, we propose spoon , a statistical framework using empirical Bayes techniques to remove this bias, leading to more accurate prioritization of SVGs. We demonstrate the performance of spoon in both simulated and real SRT data. A software implementation of our method is available at https://bioconductor.org/packages/spoon .},
  archive      = {J_BIOSTAT},
  author       = {Shah, Kinnary and Guo, Boyi and Hicks, Stephanie C},
  doi          = {10.1093/biostatistics/kxaf012},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf012},
  shortjournal = {Biostatistics},
  title        = {Addressing the mean–variance relationship in spatially resolved transcriptomics data with spoon},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analysis for the probability of benefit in randomized controlled trials with a binary treatment and a binary outcome. <em>BIOSTAT</em>, <em>26</em>(1), kxaf011. (<a href='https://doi.org/10.1093/biostatistics/kxaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a comprehensive understanding of the effect of a given treatment on an outcome of interest, quantification of individual treatment heterogeneity is essential, alongside estimation of the average causal effect. However, even in randomized controlled trials, quantities such as the probability of benefit or the probability of harm are not identifiable, since multiple potential outcomes cannot be observed simultaneously for the same individual. We propose a sensitivity analysis for the probability of benefit in randomized controlled trial settings with a binary treatment and a binary outcome, by quantifying the deviation from conditional independence of the two potential outcomes, given a set of measured prognostic baseline covariates. We do this using a marginal sensitivity analysis parameter that does not depend on the number or complexity of the measured covariates. We provide a guide to estimation and interpretation, and illustrate our method in simulations, as well as using a real data example from a randomized controlled trial studying the effect of umbilical vein oxytocin administration on the need for manual removal of the placenta during birth.},
  archive      = {J_BIOSTAT},
  author       = {Ciocănea-Teodorescu, Iuliana and Gabriel, Erin E and Sjölander, Arvid},
  doi          = {10.1093/biostatistics/kxaf011},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf011},
  shortjournal = {Biostatistics},
  title        = {Sensitivity analysis for the probability of benefit in randomized controlled trials with a binary treatment and a binary outcome},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic clustering using shared latent variable model for assessing alzheimer’s disease biomarkers. <em>BIOSTAT</em>, <em>26</em>(1), kxaf010. (<a href='https://doi.org/10.1093/biostatistics/kxaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The preclinical stage of many neurodegenerative diseases can span decades before symptoms become apparent. Understanding the sequence of preclinical biomarker changes provides a critical opportunity for early diagnosis and effective intervention prior to significant loss of patients’ brain functions. The main challenge to early detection lies in the absence of direct observation of the disease state and the considerable variability in both biomarkers and disease dynamics among individuals. Recent research hypothesized the existence of subgroups with distinct biomarker patterns due to co-morbidities and degrees of brain resilience. Our ability to diagnose early and intervene during the preclinical stage of neurodegenerative diseases will be enhanced by further insights into heterogeneity in the biomarker–disease relationship. In this article, we focus on Alzheimer’s disease (AD) and attempt to identify the systematic patterns within the heterogeneous AD biomarker–disease cascade. Specifically, we quantify the disease progression using a dynamic latent variable whose mixture distribution represents patient subgroups. Model estimation uses Hamiltonian Monte Carlo with the number of clusters determined by the Bayesian Information Criterion. We report simulation studies that investigate the performance of the proposed model in finite sample settings that are similar to our motivating application. We apply the proposed model to the Biomarkers of Cognitive Decline Among Normal Individuals data, a longitudinal study that was conducted over 2 decades among individuals who were initially cognitively normal. Our application yields evidence consistent with the hypothetical model of biomarker dynamics presented in Jack Jr et al. In addition, our analysis identified 2 subgroups with distinct disease-onset patterns. Finally, we develop a dynamic prediction approach to improve the precision of prognoses.},
  archive      = {J_BIOSTAT},
  author       = {Xu, Yizhen and Zeger, Scott and Wang, Zheyu},
  doi          = {10.1093/biostatistics/kxaf010},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf010},
  shortjournal = {Biostatistics},
  title        = {Probabilistic clustering using shared latent variable model for assessing alzheimer’s disease biomarkers},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and inference for causal spillover effects in egocentric-network randomized trials in the presence of network membership misclassification. <em>BIOSTAT</em>, <em>26</em>(1), kxaf009. (<a href='https://doi.org/10.1093/biostatistics/kxaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To leverage peer influence and increase population behavioral changes, behavioral interventions often rely on peer-based strategies. A common study design that assesses such strategies is the egocentric-network randomized trial (ENRT), where index participants receive a behavioral training and are encouraged to disseminate information to their peers. Under this design, a crucial estimand of interest is the Average Spillover Effect (ASpE), which measures the impact of the intervention on participants who do not receive it, but whose outcomes may be affected by others who do. The assessment of the ASpE relies on assumptions about, and correct measurement of, interference sets within which individuals may influence one another’s outcomes. It can be challenging to properly specify interference sets, such as networks in ENRTs, and when mismeasured, intervention effects estimated by existing methods will be biased. In studies where social networks play an important role in disease transmission or behavior change, correcting ASpE estimates for bias due to network misclassification is critical for accurately evaluating the full impact of interventions. We combined measurement error and causal inference methods to bias-correct the ASpE estimate for network misclassification in ENRTs, when surrogate networks are recorded in place of true ones, and validation data that relate the misclassified to the true networks are available. We investigated finite sample properties of our methods in an extensive simulation study and illustrated our methods in the HIV Prevention Trials Network (HPTN) 037 study.},
  archive      = {J_BIOSTAT},
  author       = {Chao, Ariel and Spiegelman, Donna and Buchanan, Ashley and Forastiere, Laura},
  doi          = {10.1093/biostatistics/kxaf009},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf009},
  shortjournal = {Biostatistics},
  title        = {Estimation and inference for causal spillover effects in egocentric-network randomized trials in the presence of network membership misclassification},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric mixture regression for asynchronous longitudinal data using multivariate functional principal component analysis. <em>BIOSTAT</em>, <em>26</em>(1), kxaf008. (<a href='https://doi.org/10.1093/biostatistics/kxaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transitional phase of menopause induces significant hormonal fluctuations, exerting a profound influence on the long-term well-being of women. In an extensive longitudinal investigation of women’s health during mid-life and beyond, known as the Study of Women’s Health Across the Nation (SWAN), hormonal biomarkers are repeatedly assessed, following an asynchronous schedule compared to other error-prone covariates, such as physical and cardiovascular measurements. We conduct a subgroup analysis of the SWAN data employing a semiparametric mixture regression model, which allows us to explore how the relationship between hormonal responses and other time-varying or time-invariant covariates varies across subgroups. To address the challenges posed by asynchronous scheduling and measurement errors, we model the time-varying covariate trajectories as functional data with reduced-rank Karhunen–Loéve expansions, where splines are employed to capture the mean and eigenfunctions. Treating the latent subgroup membership and the functional principal component (FPC) scores as missing data, we propose an Expectation-Maximization algorithm to effectively fit the joint model, combining the mixture regression for the hormonal response and the FPC model for the asynchronous, time-varying covariates. In addition, we explore data-driven methods to determine the optimal number of subgroups within the population. Through our comprehensive analysis of the SWAN data, we unveil a crucial subgroup structure within the aging female population, shedding light on important distinctions and patterns among women undergoing menopause.},
  archive      = {J_BIOSTAT},
  author       = {Lu, Ruihan and Li, Yehua and Yao, Weixin},
  doi          = {10.1093/biostatistics/kxaf008},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf008},
  shortjournal = {Biostatistics},
  title        = {Semiparametric mixture regression for asynchronous longitudinal data using multivariate functional principal component analysis},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random forest for dynamic risk prediction of recurrent events: A pseudo-observation approach. <em>BIOSTAT</em>, <em>26</em>(1), kxaf007. (<a href='https://doi.org/10.1093/biostatistics/kxaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events are common in clinical, healthcare, social, and behavioral studies, yet methods for dynamic risk prediction of these events are limited. To overcome some long-standing challenges in analyzing censored recurrent event data, a recent regression analysis framework constructs a censored longitudinal dataset consisting of times to the first recurrent event in multiple pre-specified follow-up windows of length |$ \tau $| (XMT models). Traditional regression models struggle with nonlinear and multiway interactions, with success depending on the skill of the statistical programmer. With a staggering number of potential predictors being generated from genetic, -omic, and electronic health records sources, machine learning approaches such as the random forest regression are growing in popularity, as they can nonparametrically incorporate information from many predictors with nonlinear and multiway interactions involved in prediction. In this article, we (i) develop a random forest approach for dynamically predicting probabilities of remaining event-free during a subsequent |$ \tau $| -duration follow-up period from a reconstructed censored longitudinal data set, (ii) modify the XMT regression approach to predict these same probabilities, subject to the limitations that traditional regression models typically have, and (iii) demonstrate how to incorporate patient-specific history of recurrent events for prediction in settings where this information may be partially missing. We show the increased ability of our random forest algorithm for predicting the probability of remaining event-free over a |$ \tau $| -duration follow-up window when compared to our modified XMT method for prediction in settings where association between predictors and recurrent event outcomes is complex in nature. We also show the importance of incorporating past recurrent event history in prediction algorithms when event times are correlated within a subject. The proposed random forest algorithm is demonstrated using recurrent exacerbation data from the trial of Azithromycin for the Prevention of Exacerbations of Chronic Obstructive Pulmonary Disease.},
  archive      = {J_BIOSTAT},
  author       = {Loe, Abigail and Murray, Susan and Wu, Zhenke},
  doi          = {10.1093/biostatistics/kxaf007},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf007},
  shortjournal = {Biostatistics},
  title        = {Random forest for dynamic risk prediction of recurrent events: A pseudo-observation approach},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized likelihood optimization for censored missing value imputation in proteomics. <em>BIOSTAT</em>, <em>26</em>(1), kxaf006. (<a href='https://doi.org/10.1093/biostatistics/kxaf006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label-free bottom-up proteomics using mass spectrometry and liquid chromatography has long been established as one of the most popular high-throughput analysis workflows for proteome characterization. However, it produces data hindered by complex and heterogeneous missing values, which imputation has long remained problematic. To cope with this, we introduce Pirat, an algorithm that harnesses this challenge using an original likelihood maximization strategy. Notably, it models the instrument limit by learning a global censoring mechanism from the data available. Moreover, it estimates the covariance matrix between enzymatic cleavage products (ie peptides or precursor ions), while offering a natural way to integrate complementary transcriptomic information when multi-omic assays are available. Our benchmarking on several datasets covering a variety of experimental designs (number of samples, acquisition mode, missingness patterns, etc.) and using a variety of metrics (differential analysis ground truth or imputation errors) shows that Pirat outperforms all pre-existing imputation methods. Beyond the interest of Pirat as an imputation tool, these results pinpoint the need for a paradigm change in proteomics imputation, as most pre-existing strategies could be boosted by incorporating similar models to account for the instrument censorship or for the correlation structures, either grounded to the analytical pipeline or arising from a multi-omic approach.},
  archive      = {J_BIOSTAT},
  author       = {Etourneau, Lucas and Fancello, Laura and Wieczorek, Samuel and Varoquaux, Nelle and Burger, Thomas},
  doi          = {10.1093/biostatistics/kxaf006},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf006},
  shortjournal = {Biostatistics},
  title        = {Penalized likelihood optimization for censored missing value imputation in proteomics},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariate-adjusted estimators of diagnostic accuracy in randomized trials. <em>BIOSTAT</em>, <em>26</em>(1), kxaf005. (<a href='https://doi.org/10.1093/biostatistics/kxaf005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials evaluating the diagnostic accuracy of a marker frequently collect information on baseline covariates in addition to information on the marker and the reference standard. However, standard estimators of sensitivity and specificity do not use data on baseline covariates and restrict the analysis to data from participants with a positive reference standard in the intervention arm being evaluated. Covariate-adjusted estimators for marginal treatment effects have been developed and been advocated for by regulatory agencies because they can improve power compared to unadjusted estimators. Despite this, similar covariate-adjusted estimators for marginal sensitivity and specificity have not yet been developed. In this manuscript, we address this gap by developing covariate-adjusted estimators for marginal sensitivity and specificity of a diagnostic test that leverage baseline covariate information. The estimators also use data from all participants, not just participants with a positive reference standard in the intervention arm being evaluated. We derive the asymptotic properties of the estimators and evaluate the finite sample properties of the estimators using simulations and by analyzing data on lung cancer screening.},
  archive      = {J_BIOSTAT},
  author       = {Steingrimsson, Jon A},
  doi          = {10.1093/biostatistics/kxaf005},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf005},
  shortjournal = {Biostatistics},
  title        = {Covariate-adjusted estimators of diagnostic accuracy in randomized trials},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mediation analysis with graph mediator. <em>BIOSTAT</em>, <em>26</em>(1), kxaf004. (<a href='https://doi.org/10.1093/biostatistics/kxaf004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a mediation analysis framework when the mediator is a graph. A Gaussian covariance graph model is assumed for graph presentation. Causal estimands and assumptions are discussed under this presentation. With a covariance matrix as the mediator, a low-rank representation is introduced and parametric mediation models are considered under the structural equation modeling framework. Assuming Gaussian random errors, likelihood-based estimators are introduced to simultaneously identify the low-rank representation and causal parameters. An efficient computational algorithm is proposed and asymptotic properties of the estimators are investigated. Via simulation studies, the performance of the proposed approach is evaluated. Applying to a resting-state fMRI study, a brain network is identified within which functional connectivity mediates the sex difference in the performance of a motor task.},
  archive      = {J_BIOSTAT},
  author       = {Xu, Yixi and Zhao, Yi},
  doi          = {10.1093/biostatistics/kxaf004},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf004},
  shortjournal = {Biostatistics},
  title        = {Mediation analysis with graph mediator},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Within-trial data borrowing for sequential multiple assignment randomized trials. <em>BIOSTAT</em>, <em>26</em>(1), kxaf003. (<a href='https://doi.org/10.1093/biostatistics/kxaf003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sequential Multiple Assignment Randomized Trial (SMART) is a complex trial design that involves randomizing a single participant multiple times in a sequential manner. This results in the branching nature of a SMART, which represents several distinct groups defined by different combinations of treatments, response statuses, etc. A SMART can then answer various scientific questions of interest, eg, the optimal dynamic treatment regime (DTR) for treating a chronic illness, what intervention to offer first, and what intervention to offer to nonresponders (or suboptimal responders). However, the analysis of a SMART can suffer from low precision, as the potentially widely branching structure can lead to reduced sample sizes in some groups of interest. In this paper, we propose a novel analysis method for a SMART in which dynamic borrowing is used to borrow strength across groups with similar expected outcomes, thus providing increased precision for the estimation of the expected outcomes of DTRs. We apply our method to a SMART evaluating various weight loss strategies using a binary endpoint of clinically significant weight loss and show by simulation that our method can improve the precision of the estimated expected outcome of a DTR, aid in the identification of the optimal DTR, and produce a clustering analysis of DTRs embedded in a SMART.},
  archive      = {J_BIOSTAT},
  author       = {Kotalik, Ales and Vock, David M and Sherwood, Nancy E and Hobbs, Brian P and Koopmeiners, Joseph S},
  doi          = {10.1093/biostatistics/kxaf003},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf003},
  shortjournal = {Biostatistics},
  title        = {Within-trial data borrowing for sequential multiple assignment randomized trials},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connectivity regression. <em>BIOSTAT</em>, <em>26</em>(1), kxaf002. (<a href='https://doi.org/10.1093/biostatistics/kxaf002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing how brain functional connectivity networks vary across individuals promises to uncover important scientific questions such as patterns of healthy brain aging through the lifespan or dysconnectivity associated with disease. In this article, we introduce a general regression framework, Connectivity Regression ( ConnReg ), for regressing subject-specific functional connectivity networks on covariates while accounting for within-network inter-edge dependence. ConnReg utilizes a multivariate generalization of Fisher’s transformation to project network objects into an alternative space where Gaussian assumptions are justified and positive semidefinite constraints are automatically satisfied. Penalized multivariate regression is fit in the transformed space to simultaneously induce sparsity in regression coefficients and in covariance elements, which capture within network inter-edge dependence. We use permutation tests to perform multiplicity-adjusted inference to identify covariates associated with connectivity, and stability selection scores to identify network edges that vary with selected covariates. Simulation studies validate the inferential properties of our proposed method and demonstrate how estimating and accounting for within-network inter-edge dependence leads to more efficient estimation, more powerful inference, and more accurate selection of covariate-dependent network edges. We apply ConnReg to the Human Connectome Project Young Adult study, revealing insights into how connectivity varies with language processing covariates and structural brain features.},
  archive      = {J_BIOSTAT},
  author       = {Desai, Neel and Baladandayuthapani, Veera and Shinohara, Russell T and Morris, Jeffrey S},
  doi          = {10.1093/biostatistics/kxaf002},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf002},
  shortjournal = {Biostatistics},
  title        = {Connectivity regression},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable randomized kernel methods for multiview data integration and prediction with application to coronavirus disease. <em>BIOSTAT</em>, <em>26</em>(1), kxaf001. (<a href='https://doi.org/10.1093/biostatistics/kxaf001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is still more to learn about the pathobiology of coronavirus disease (COVID-19) despite 4 years of the pandemic. A multiomics approach offers a comprehensive view of the disease and has the potential to yield deeper insight into the pathogenesis of the disease. Previous multiomics integrative analysis and prediction studies for COVID-19 severity and status have assumed simple relationships (ie linear relationships) between omics data and between omics and COVID-19 outcomes. However, these linear methods do not account for the inherent underlying nonlinear structure associated with these different types of data. The motivation behind this work is to model nonlinear relationships in multiomics and COVID-19 outcomes, and to determine key multidimensional molecules associated with the disease. Toward this goal, we develop scalable randomized kernel methods for jointly associating data from multiple sources or views and simultaneously predicting an outcome or classifying a unit into one of 2 or more classes. We also determine variables or groups of variables that best contribute to the relationships among the views. We use the idea that random Fourier bases can approximate shift-invariant kernel functions to construct nonlinear mappings of each view and we use these mappings and the outcome variable to learn view-independent low-dimensional representations. We demonstrate the effectiveness of the proposed methods through extensive simulations. When the proposed methods were applied to gene expression, metabolomics, proteomics, and lipidomics data pertaining to COVID-19, we identified several molecular signatures for COVID-19 status and severity. Our results agree with previous findings and suggest potential avenues for future research. Our algorithms are implemented in Pytorch and interfaced in R and available at: https://github.com/lasandrall/RandMVLearn .},
  archive      = {J_BIOSTAT},
  author       = {Safo, Sandra E and Lu, Han},
  doi          = {10.1093/biostatistics/kxaf001},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxaf001},
  shortjournal = {Biostatistics},
  title        = {Scalable randomized kernel methods for multiview data integration and prediction with application to coronavirus disease},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking the power of time-since-infection models: Data augmentation for improved instantaneous reproduction number estimation. <em>BIOSTAT</em>, <em>26</em>(1), kxae054. (<a href='https://doi.org/10.1093/biostatistics/kxae054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-since-infection (TSI) models, which use disease surveillance data to model infectious diseases, have become increasingly popular due to their flexibility and capacity to address complex disease control questions. However, a notable limitation of TSI models is their primary reliance on incidence data. Even when hospitalization data are available, existing TSI models have not been crafted to improve the estimation of disease transmission or to estimate hospitalization-related parameters—metrics crucial for understanding a pandemic and planning hospital resources. Moreover, their dependence on reported infection data makes them vulnerable to variations in data quality. In this study, we advance TSI models by integrating hospitalization data, marking a significant step forward in modeling with TSI models. We introduce hospitalization propensity parameters to jointly model incidence and hospitalization data. We use a composite likelihood function to accommodate complex data structure and a Monte Carlo expectation–maximization algorithm to estimate model parameters. We analyze COVID-19 data to estimate disease transmission, assess risk factor impacts, and calculate hospitalization propensity. Our model improves the accuracy of estimating the instantaneous reproduction number in TSI models, particularly when hospitalization data is of higher quality than incidence data. It enables the estimation of key infectious disease parameters without relying on contact tracing data and provides a foundation for integrating TSI models with other infectious disease models.},
  archive      = {J_BIOSTAT},
  author       = {Shi, Jiasheng and Zhou, Yizhao and Huang, Jing},
  doi          = {10.1093/biostatistics/kxae054},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae054},
  shortjournal = {Biostatistics},
  title        = {Unlocking the power of time-since-infection models: Data augmentation for improved instantaneous reproduction number estimation},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recurrent events modeling based on a reflected brownian motion with application to hypoglycemia. <em>BIOSTAT</em>, <em>26</em>(1), kxae053. (<a href='https://doi.org/10.1093/biostatistics/kxae053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with type 2 diabetes need to closely monitor blood sugar levels as their routine diabetes self-management. Although many treatment agents aim to tightly control blood sugar, hypoglycemia often stands as an adverse event. In practice, patients can observe hypoglycemic events more easily than hyperglycemic events due to the perception of neurogenic symptoms. We propose to model each patient’s observed hypoglycemic event as a lower boundary crossing event for a reflected Brownian motion with an upper reflection barrier. The lower boundary is set by clinical standards. To capture patient heterogeneity and within-patient dependence, covariates and a patient level frailty are incorporated into the volatility and the upper reflection barrier. This framework provides quantification for the underlying glucose level variability, patients heterogeneity, and risk factors’ impact on glucose. We make inferences based on a Bayesian framework using Markov chain Monte Carlo. Two model comparison criteria, the deviance information criterion and the logarithm of the pseudo-marginal likelihood, are used for model selection. The methodology is validated in simulation studies. In analyzing a dataset from the diabetic patients in the DURABLE trial, our model provides adequate fit, generates data similar to the observed data, and offers insights that could be missed by other models.},
  archive      = {J_BIOSTAT},
  author       = {Xie, Yingfa and Fu, Haoda and Huang, Yuan and Pozdnyakov, Vladimir and Yan, Jun},
  doi          = {10.1093/biostatistics/kxae053},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae053},
  shortjournal = {Biostatistics},
  title        = {Recurrent events modeling based on a reflected brownian motion with application to hypoglycemia},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the opioid syndemic in north carolina: A novel approach to modeling and identifying factors. <em>BIOSTAT</em>, <em>26</em>(1), kxae052. (<a href='https://doi.org/10.1093/biostatistics/kxae052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The opioid epidemic is a significant public health challenge in North Carolina, but limited data restrict our understanding of its complexity. Examining trends and relationships among different outcomes believed to reflect opioid misuse provides an alternative perspective to understand the opioid epidemic. We use a Bayesian dynamic spatial factor model to capture the interrelated dynamics within six different county-level outcomes, such as illicit opioid overdose deaths, emergency department visits related to drug overdose, treatment counts for opioid use disorder, patients receiving prescriptions for buprenorphine, and newly diagnosed cases of acute and chronic hepatitis C virus and human immunodeficiency virus. We design the factor model to yield meaningful interactions among predefined subsets of these outcomes, causing a departure from the conventional lower triangular structure in the loadings matrix and leading to familiar identifiability issues. To address this challenge, we propose a novel approach that involves decomposing the loadings matrix within a Markov chain Monte Carlo algorithm, allowing us to estimate the loadings and factors uniquely. As a result, we gain a better understanding of the spatio-temporal dynamics of the opioid epidemic in North Carolina.},
  archive      = {J_BIOSTAT},
  author       = {Murphy, Eva and Kline, David and Egan, Kathleen L and Lancaster, Kathryn E and Miller, William C and Waller, Lance A and Hepler, Staci A},
  doi          = {10.1093/biostatistics/kxae052},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae052},
  shortjournal = {Biostatistics},
  title        = {Understanding the opioid syndemic in north carolina: A novel approach to modeling and identifying factors},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bipartite interference and air pollution transport: Estimating health effects of power plant interventions. <em>BIOSTAT</em>, <em>26</em>(1), kxae051. (<a href='https://doi.org/10.1093/biostatistics/kxae051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating air quality interventions is confronted with the challenge of interference since interventions at a particular pollution source likely impact air quality and health at distant locations, and air quality and health at any given location are likely impacted by interventions at many sources. The structure of interference in this context is dictated by complex atmospheric processes governing how pollution emitted from a particular source is transformed and transported across space and can be cast with a bipartite structure reflecting the two distinct types of units: (i) interventional units on which treatments are applied or withheld to change pollution emissions; and (ii) outcome units on which outcomes of primary interest are measured. We propose new estimands for bipartite causal inference with interference that construe two components of treatment: a “key-associated” (or “individual”) treatment and an “upwind” (or “neighborhood”) treatment. Estimation is carried out using a covariate adjustment approach based on a joint propensity score. A reduced-complexity atmospheric model characterizes the structure of the interference network by modeling the movement of air parcels through time and space. The new methods are deployed to evaluate the effectiveness of installing flue-gas desulfurization scrubbers on 472 coal-burning power plants (the interventional units) in reducing Medicare hospitalizations among 21,577,552 Medicare beneficiaries residing across 25,553 ZIP codes in the United States (the outcome units).},
  archive      = {J_BIOSTAT},
  author       = {Zigler, Corwin and Liu, Vera and Mealli, Fabrizia and Forastiere, Laura},
  doi          = {10.1093/biostatistics/kxae051},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae051},
  shortjournal = {Biostatistics},
  title        = {Bipartite interference and air pollution transport: Estimating health effects of power plant interventions},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Scalable kernel balancing weights in a nationwide observational study of hospital profit status and heart attack outcomes. <em>BIOSTAT</em>, <em>26</em>(1), kxae050. (<a href='https://doi.org/10.1093/biostatistics/kxae050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxae050},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae050},
  shortjournal = {Biostatistics},
  title        = {Correction to: Scalable kernel balancing weights in a nationwide observational study of hospital profit status and heart attack outcomes},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling schizophrenia: A study with generalized functional linear mixed model via the investigation of functional random effects. <em>BIOSTAT</em>, <em>26</em>(1), kxae049. (<a href='https://doi.org/10.1093/biostatistics/kxae049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies have identified attenuated pre-speech activity and speech sound suppression in individuals with Schizophrenia, with similar patterns observed in basic tasks entailing button-pressing to perceive a tone. However, it remains unclear whether these patterns are uniform across individuals or vary from person to person. Motivated by electroencephalographic (EEG) data from a Schizophrenia study, we develop a generalized functional linear mixed model (GFLMM) for repeated measurements by incorporating subject-specific functional random effects associated with multiple functional predictors. To assess the significance of these functional effects, we employ two different multivariate functional principal component analysis methods, which transform the GFLMM into a conventional generalized linear mixed model, thereby facilitating its implementation with standard software. Furthermore, we introduce a cutting-edge testing approach utilizing working responses to detect both subject-specific and predictor-specific functional random effects. Monte Carlo simulation studies demonstrate the effectiveness of our proposed testing method. Application of the proposed methods to the Schizophrenia data reveals significant subject-specific effects of human brain activity in the frontal zone (Fz) and the central zone (Cz), providing valuable insights into the potential variations among individuals, from healthy controls to those diagnosed with Schizophrenia.},
  archive      = {J_BIOSTAT},
  author       = {Rui, Rongxiang and Xiong, Wei and Pan, Jianxin and Tian, Maozai},
  doi          = {10.1093/biostatistics/kxae049},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae049},
  shortjournal = {Biostatistics},
  title        = {Unveiling schizophrenia: A study with generalized functional linear mixed model via the investigation of functional random effects},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian thresholded modeling for integrating brain node and network predictors. <em>BIOSTAT</em>, <em>26</em>(1), kxae048. (<a href='https://doi.org/10.1093/biostatistics/kxae048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Progress in neuroscience has provided unprecedented opportunities to advance our understanding of brain alterations and their correspondence to phenotypic profiles. With data collected from various imaging techniques, studies have integrated different types of information ranging from brain structure, function, or metabolism. More recently, an emerging way to categorize imaging traits is through a metric hierarchy, including localized node-level measurements and interactive network-level metrics. However, limited research has been conducted to integrate these different hierarchies and achieve a better understanding of the neurobiological mechanisms and communications. In this work, we address this literature gap by proposing a Bayesian regression model under both vector-variate and matrix-variate predictors. To characterize the interplay between different predicting components, we propose a set of biologically plausible prior models centered on an innovative joint thresholded prior. This captures the coupling and grouping effect of signal patterns, as well as their spatial contiguity across brain anatomy. By developing a posterior inference, we can identify and quantify the uncertainty of signaling node- and network-level neuromarkers, as well as their predictive mechanism for phenotypic outcomes. Through extensive simulations, we demonstrate that our proposed method outperforms the alternative approaches substantially in both out-of-sample prediction and feature selection. By implementing the model to study children’s general mental abilities, we establish a powerful predictive mechanism based on the identified task contrast traits and resting-state sub-networks.},
  archive      = {J_BIOSTAT},
  author       = {Sun, Zhe and Xu, Wanwan and Li, Tianxi and Kang, Jian and Alanis-Lobato, Gregorio and Zhao, Yize},
  doi          = {10.1093/biostatistics/kxae048},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae048},
  shortjournal = {Biostatistics},
  title        = {Bayesian thresholded modeling for integrating brain node and network predictors},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BAMITA: Bayesian multiple imputation for tensor arrays. <em>BIOSTAT</em>, <em>26</em>(1), kxae047. (<a href='https://doi.org/10.1093/biostatistics/kxae047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data increasingly take the form of a multi-way array, or tensor, in several biomedical domains. Such tensors are often incompletely observed. For example, we are motivated by longitudinal microbiome studies in which several timepoints are missing for several subjects. There is a growing literature on missing data imputation for tensors. However, existing methods give a point estimate for missing values without capturing uncertainty. We propose a multiple imputation approach for tensors in a flexible Bayesian framework, that yields realistic simulated values for missing entries and can propagate uncertainty through subsequent analyses. Our model uses efficient and widely applicable conjugate priors for a CANDECOMP/PARAFAC (CP) factorization, with a separable residual covariance structure. This approach is shown to perform well with respect to both imputation accuracy and uncertainty calibration, for scenarios in which either single entries or entire fibers of the tensor are missing. For two microbiome applications, it is shown to accurately capture uncertainty in the full microbiome profile at missing timepoints and used to infer trends in species diversity for the population. Documented R code to perform our multiple imputation approach is available at https://github.com/lockEF/MultiwayImputation .},
  archive      = {J_BIOSTAT},
  author       = {Jiang, Ziren and Li, Gen and Lock, Eric F},
  doi          = {10.1093/biostatistics/kxae047},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae047},
  shortjournal = {Biostatistics},
  title        = {BAMITA: Bayesian multiple imputation for tensor arrays},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for a difference in means of a single feature after clustering. <em>BIOSTAT</em>, <em>26</em>(1), kxae046. (<a href='https://doi.org/10.1093/biostatistics/kxae046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many applications, it is critical to interpret and validate groups of observations obtained via clustering. A common interpretation and validation approach involves testing differences in feature means between observations in two estimated clusters. In this setting, classical hypothesis tests lead to an inflated Type I error rate. To overcome this problem, we propose a new test for the difference in means in a single feature between a pair of clusters obtained using hierarchical or k -means clustering. The test controls the selective Type I error rate in finite samples and can be efficiently computed. We further illustrate the validity and power of our proposal in simulation and demonstrate its use on single-cell RNA-sequencing data.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Yiqun T and Gao, Lucy L},
  doi          = {10.1093/biostatistics/kxae046},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae046},
  shortjournal = {Biostatistics},
  title        = {Testing for a difference in means of a single feature after clustering},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian subtyping for multi-state brain functional connectome with application on preadolescent brain cognition. <em>BIOSTAT</em>, <em>26</em>(1), kxae045. (<a href='https://doi.org/10.1093/biostatistics/kxae045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Converging evidence indicates that the heterogeneity of cognitive profiles may arise through detectable alternations in brain functional connectivity. Despite an unprecedented opportunity to uncover neurobiological subtypes through clustering or subtyping analyses on multi-state functional connectivity, few existing approaches are applicable to accommodate the network topology and unique biological architecture. To address this issue, we propose an innovative Bayesian nonparametric network-variate clustering analysis to uncover subgroups of individuals with homogeneous brain functional network patterns under multiple cognitive states. In light of the existing neuroscience literature, we assume there are unknown state-specific modular structures within functional connectivity. Concurrently, we identify informative network features essential for defining subtypes. To further facilitate practical use, we develop a computationally efficient variational inference algorithm to approximate posterior inference with satisfactory estimation accuracy. Extensive simulations show the superiority of our method. We apply the method to the Adolescent Brain Cognitive Development (ABCD) study, and identify neurodevelopmental subtypes and brain sub-network phenotypes under each state to signal neurobiological heterogeneity, suggesting promising directions for further exploration and investigation in neuroscience.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Tianqi and Zhao, Hongyu and Tan, Chichun and Constable, Todd and Yip, Sarah and Zhao, Yize},
  doi          = {10.1093/biostatistics/kxae045},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae045},
  shortjournal = {Biostatistics},
  title        = {Bayesian subtyping for multi-state brain functional connectome with application on preadolescent brain cognition},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recoverability of causal effects under presence of missing data: A longitudinal case study. <em>BIOSTAT</em>, <em>26</em>(1), kxae044. (<a href='https://doi.org/10.1093/biostatistics/kxae044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data in multiple variables is a common issue. We investigate the applicability of the framework of graphical models for handling missing data to a complex longitudinal pharmacological study of children with HIV treated with an efavirenz-based regimen as part of the CHAPAS-3 trial. Specifically, we examine whether the causal effects of interest, defined through static interventions on multiple continuous variables, can be recovered (estimated consistently) from the available data only. So far, no general algorithms are available to decide on recoverability, and decisions have to be made on a case-by-case basis. We emphasize the sensitivity of recoverability to even the smallest changes in the graph structure, and present recoverability results for three plausible missingness-directed acyclic graphs (m-DAGs) in the CHAPAS-3 study, informed by clinical knowledge. Furthermore, we propose the concept of a “closed missingness mechanism”: if missing data are generated based on this mechanism, an available case analysis is admissible for consistent estimation of any statistical or causal estimand, even if data are missing not at random. Both simulations and theoretical considerations demonstrate how, in the assumed MNAR setting of our study, a complete or available case analysis can be superior to multiple imputation, and estimation results vary depending on the assumed missingness DAG. Our analyses demonstrate an innovative application of missingness DAGs to complex longitudinal real-world data, while highlighting the sensitivity of the results with respect to the assumed causal model.},
  archive      = {J_BIOSTAT},
  author       = {Holovchak, Anastasiia and McIlleron, Helen and Denti, Paolo and Schomaker, Michael},
  doi          = {10.1093/biostatistics/kxae044},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae044},
  shortjournal = {Biostatistics},
  title        = {Recoverability of causal effects under presence of missing data: A longitudinal case study},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast standard error estimation for joint models of longitudinal and time-to-event data based on stochastic EM algorithms. <em>BIOSTAT</em>, <em>26</em>(1), kxae043. (<a href='https://doi.org/10.1093/biostatistics/kxae043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum likelihood inference can often become computationally intensive when performing joint modeling of longitudinal and time-to-event data, due to the intractable integrals in the joint likelihood function. The computational challenges escalate further when modeling HIV-1 viral load data, owing to the nonlinear trajectories and the presence of left-censored data resulting from the assay’s lower limit of quantification. In this paper, for a joint model comprising a nonlinear mixed-effect model and a Cox Proportional Hazards model, we develop a computationally efficient Stochastic EM (StEM) algorithm for parameter estimation. Furthermore, we propose a novel technique for fast standard error estimation, which directly estimates standard errors from the results of StEM iterations and is broadly applicable to various joint modeling settings, such as those containing generalized linear mixed-effect models, parametric survival models, or joint models with more than two submodels. We evaluate the performance of the proposed methods through simulation studies and apply them to HIV-1 viral load data from six AIDS Clinical Trials Group studies to characterize viral rebound trajectories following the interruption of antiretroviral therapy (ART), accounting for the informative duration of off-ART periods.},
  archive      = {J_BIOSTAT},
  author       = {Yu, Tingting and Wu, Lang and Bosch, Ronald J and Smith, Davey M and Wang, Rui},
  doi          = {10.1093/biostatistics/kxae043},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae043},
  shortjournal = {Biostatistics},
  title        = {Fast standard error estimation for joint models of longitudinal and time-to-event data based on stochastic EM algorithms},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of coarsening an exposure on partial identifiability in instrumental variable settings. <em>BIOSTAT</em>, <em>26</em>(1), kxae042. (<a href='https://doi.org/10.1093/biostatistics/kxae042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In instrumental variable (IV) settings, such as imperfect randomized trials and observational studies with Mendelian randomization, one may encounter a continuous exposure, the causal effect of which is not of true interest. Instead, scientific interest may lie in a coarsened version of this exposure. Although there is a lengthy literature on the impact of coarsening of an exposure with several works focusing specifically on IV settings, all methods proposed in this literature require parametric assumptions. Instead, just as in the standard IV setting, one can consider partial identification via bounds making no parametric assumptions. This was first pointed out in Alexander Balke’s PhD dissertation. We extend and clarify his work and derive novel bounds in several settings, including for a three-level IV, which will most likely be the case in Mendelian randomization. We demonstrate our findings in two real data examples, a randomized trial for peanut allergy in infants and a Mendelian randomization setting investigating the effect of homocysteine on cardiovascular disease.},
  archive      = {J_BIOSTAT},
  author       = {Gabriel, Erin E and Sachs, Michael C and Sjölander, Arvid},
  doi          = {10.1093/biostatistics/kxae042},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae042},
  shortjournal = {Biostatistics},
  title        = {The impact of coarsening an exposure on partial identifiability in instrumental variable settings},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shared parameter modeling of longitudinal data allowing for possibly informative visiting process and terminal event. <em>BIOSTAT</em>, <em>26</em>(1), kxae041. (<a href='https://doi.org/10.1093/biostatistics/kxae041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modeling of longitudinal and time-to-event data, particularly through shared parameter models (SPMs), is a common approach for handling longitudinal marker data with an informative terminal event. A critical but often neglected assumption in this context is that the visiting/observation process is noninformative, depending solely on past marker values and visit times. When this assumption fails, the visiting process becomes informative, resulting potentially to biased SPM estimates. Existing methods generally rely on a conditional independence assumption, positing that the marker model, visiting process, and time-to-event model are independent given shared or correlated random effects. Moreover, they are typically built on an intensity-based visiting process using calendar time. This study introduces a unified approach for jointly modeling a normally distributed marker, the visiting process, and time-to-event data in the form of competing risks. Our model conditions on the history of observed marker values, prior visit times, the marker’s random effects, and possibly a frailty term independent of the random effects. While our approach aligns with the shared-parameter framework, it does not presume conditional independence between the processes. Additionally, the visiting process can be defined on either a gap time scale, via proportional hazard models, or a calendar time scale, via proportional intensity models. Through extensive simulation studies, we assess the performance of our proposed methodology. We demonstrate that disregarding an informative visiting process can yield significantly biased marker estimates. However, misspecification of the visiting process can also lead to biased estimates. The gap time formulation exhibits greater robustness compared to the intensity-based model when the visiting process is misspecified. In general, enriching the visiting process with prior visit history enhances performance. We further apply our methodology to real longitudinal data from HIV, where visit frequency varies substantially among individuals.},
  archive      = {J_BIOSTAT},
  author       = {Thomadakis, Christos and Meligkotsidou, Loukia and Pantazis, Nikos and Touloumi, Giota},
  doi          = {10.1093/biostatistics/kxae041},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae041},
  shortjournal = {Biostatistics},
  title        = {Shared parameter modeling of longitudinal data allowing for possibly informative visiting process and terminal event},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional quantile principal component analysis. <em>BIOSTAT</em>, <em>26</em>(1), kxae040. (<a href='https://doi.org/10.1093/biostatistics/kxae040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces functional quantile principal component analysis (FQPCA), a dimensionality reduction technique that extends the concept of functional principal components analysis (FPCA) to the examination of participant-specific quantiles curves. Our approach borrows strength across participants to estimate patterns in quantiles, and uses participant-level data to estimate loadings on those patterns. As a result, FQPCA is able to capture shifts in the scale and distribution of data that affect participant-level quantile curves, and is also a robust methodology suitable for dealing with outliers, heteroscedastic data or skewed data. The need for such methodology is exemplified by physical activity data collected using wearable devices. Participants often differ in the timing and intensity of physical activity behaviors, and capturing information beyond the participant-level expected value curves produced by FPCA is necessary for a robust quantification of diurnal patterns of activity. We illustrate our methods using accelerometer data from the National Health and Nutrition Examination Survey, and produce participant-level 10%, 50%, and 90% quantile curves over 24 h of activity. The proposed methodology is supported by simulation results, and is available as an R package.},
  archive      = {J_BIOSTAT},
  author       = {Méndez-Civieta, Álvaro and Wei, Ying and Diaz, Keith M. and Goldsmith, Jeff},
  doi          = {10.1093/biostatistics/kxae040},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae040},
  shortjournal = {Biostatistics},
  title        = {Functional quantile principal component analysis},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection processes, transportability, and failure time analysis in life history studies. <em>BIOSTAT</em>, <em>26</em>(1), kxae039. (<a href='https://doi.org/10.1093/biostatistics/kxae039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In life history analysis of data from cohort studies, it is important to address the process by which participants are identified and selected. Many health studies select or enrol individuals based on whether they have experienced certain health related events, for example, disease diagnosis or some complication from disease. Standard methods of analysis rely on assumptions concerning the independence of selection and a person’s prospective life history process, given their prior history. Violations of such assumptions are common, however, and can bias estimation of process features. This has implications for the internal and external validity of cohort studies, and for the transportabilty of results to a population. In this paper, we study failure time analysis by proposing a joint model for the cohort selection process and the failure process of interest. This allows us to address both independence assumptions and the transportability of study results. It is shown that transportability cannot be guaranteed in the absence of auxiliary information on the population. Conditions that produce dependent selection and types of auxiliary data are discussed and illustrated in numerical studies. The proposed framework is applied to a study of the risk of psoriatic arthritis in persons with psoriasis.},
  archive      = {J_BIOSTAT},
  author       = {Cook, Richard J and Lawless, Jerald F},
  doi          = {10.1093/biostatistics/kxae039},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae039},
  shortjournal = {Biostatistics},
  title        = {Selection processes, transportability, and failure time analysis in life history studies},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scalable two-stage bayesian approach accounting for exposure measurement error in environmental epidemiology. <em>BIOSTAT</em>, <em>26</em>(1), kxae038. (<a href='https://doi.org/10.1093/biostatistics/kxae038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accounting for exposure measurement errors has been recognized as a crucial problem in environmental epidemiology for over two decades. Bayesian hierarchical models offer a coherent probabilistic framework for evaluating associations between environmental exposures and health effects, which take into account exposure measurement errors introduced by uncertainty in the estimated exposure as well as spatial misalignment between the exposure and health outcome data. While two-stage Bayesian analyses are often regarded as a good alternative to fully Bayesian analyses when joint estimation is not feasible, there has been minimal research on how to properly propagate uncertainty from the first-stage exposure model to the second-stage health model, especially in the case of a large number of participant locations along with spatially correlated exposures. We propose a scalable two-stage Bayesian approach, called a sparse multivariate normal (sparse MVN) prior approach, based on the Vecchia approximation for assessing associations between exposure and health outcomes in environmental epidemiology. We compare its performance with existing approaches through simulation. Our sparse MVN prior approach shows comparable performance with the fully Bayesian approach, which is a gold standard but is impossible to implement in some cases. We investigate the association between source-specific exposures and pollutant (nitrogen dioxide [NO 2 ])-specific exposures and birth weight of full-term infants born in 2012 in Harris County, Texas, using several approaches, including the newly developed method.},
  archive      = {J_BIOSTAT},
  author       = {Lee, Changwoo J and Symanski, Elaine and Rammah, Amal and Kang, Dong Hun and Hopke, Philip K and Park, Eun Sug},
  doi          = {10.1093/biostatistics/kxae038},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae038},
  shortjournal = {Biostatistics},
  title        = {A scalable two-stage bayesian approach accounting for exposure measurement error in environmental epidemiology},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speeding up interval estimation for r2-based mediation effect of high-dimensional mediators via cross-fitting. <em>BIOSTAT</em>, <em>26</em>(1), kxae037. (<a href='https://doi.org/10.1093/biostatistics/kxae037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is a useful tool in investigating how molecular phenotypes such as gene expression mediate the effect of exposure on health outcomes. However, commonly used mean-based total mediation effect measures may suffer from cancellation of component-wise mediation effects in opposite directions in the presence of high-dimensional omics mediators. To overcome this limitation, we recently proposed a variance-based R-squared total mediation effect measure that relies on the computationally intensive nonparametric bootstrap for confidence interval estimation. In the work described herein, we formulated a more efficient two-stage, cross-fitted estimation procedure for the R 2 measure. To avoid potential bias, we performed iterative Sure Independence Screening (iSIS) in two subsamples to exclude the non-mediators, followed by ordinary least squares regressions for the variance estimation. We then constructed confidence intervals based on the newly derived closed-form asymptotic distribution of the R 2 measure. Extensive simulation studies demonstrated that this proposed procedure is much more computationally efficient than the resampling-based method, with comparable coverage probability. Furthermore, when applied to the Framingham Heart Study, the proposed method replicated the established finding of gene expression mediating age-related variation in systolic blood pressure and identified the role of gene expression profiles in the relationship between sex and high-density lipoprotein cholesterol level. The proposed estimation procedure is implemented in R package CFR2M .},
  archive      = {J_BIOSTAT},
  author       = {Xu, Zhichao and Li, Chunlin and Chi, Sunyi and Yang, Tianzhong and Wei, Peng},
  doi          = {10.1093/biostatistics/kxae037},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae037},
  shortjournal = {Biostatistics},
  title        = {Speeding up interval estimation for r2-based mediation effect of high-dimensional mediators via cross-fitting},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic and concordance-assisted learning for risk stratification with application to alzheimer’s disease. <em>BIOSTAT</em>, <em>26</em>(1), kxae036. (<a href='https://doi.org/10.1093/biostatistics/kxae036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic prediction models capable of retaining accuracy by evolving over time could play a significant role for monitoring disease progression in clinical practice. In biomedical studies with long-term follow up, participants are often monitored through periodic clinical visits with repeat measurements until an occurrence of the event of interest (e.g. disease onset) or the study end. Acknowledging the dynamic nature of disease risk and clinical information contained in the longitudinal markers, we propose an innovative concordance-assisted learning algorithm to derive a real-time risk stratification score. The proposed approach bypasses the need to fit regression models, such as joint models of the longitudinal markers and time-to-event outcome, and hence enjoys the desirable property of model robustness. Simulation studies confirmed that the proposed method has satisfactory performance in dynamically monitoring the risk of developing disease and differentiating high-risk and low-risk population over time. We apply the proposed method to the Alzheimer’s Disease Neuroimaging Initiative data and develop a dynamic risk score of Alzheimer’s Disease for patients with mild cognitive impairment using multiple longitudinal markers and baseline prognostic factors.},
  archive      = {J_BIOSTAT},
  author       = {Li, Wen and Li, Ruosha and Feng, Ziding and Ning, Jing and For the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1093/biostatistics/kxae036},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae036},
  shortjournal = {Biostatistics},
  title        = {Dynamic and concordance-assisted learning for risk stratification with application to alzheimer’s disease},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the addams family of discrete frailty distributions for modeling multivariate case i interval-censored data. <em>BIOSTAT</em>, <em>26</em>(1), kxae035. (<a href='https://doi.org/10.1093/biostatistics/kxae035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random effect models for time-to-event data, also known as frailty models, provide a conceptually appealing way of quantifying association between survival times and of representing heterogeneities resulting from factors which may be difficult or impossible to measure. In the literature, the random effect is usually assumed to have a continuous distribution. However, in some areas of application, discrete frailty distributions may be more appropriate. The present paper is about the implementation and interpretation of the Addams family of discrete frailty distributions. We propose methods of estimation for this family of densities in the context of shared frailty models for the hazard rates for case I interval-censored data. Our optimization framework allows for stratification of random effect distributions by covariates. We highlight interpretational advantages of the Addams family of discrete frailty distributions and the K -point distribution as compared to other frailty distributions. A unique feature of the Addams family and the K -point distribution is that the support of the frailty distribution depends on its parameters. This feature is best exploited by imposing a model on the distributional parameters, resulting in a model with non-homogeneous covariate effects that can be analyzed using standard measures such as the hazard ratio. Our methods are illustrated with applications to multivariate case I interval-censored infection data.},
  archive      = {J_BIOSTAT},
  author       = {Bardo, Maximilian and Hens, Niel and Unkel, Steffen},
  doi          = {10.1093/biostatistics/kxae035},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae035},
  shortjournal = {Biostatistics},
  title        = {On the addams family of discrete frailty distributions for modeling multivariate case i interval-censored data},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian pharmacokinetics integrated phase I–II design to optimize dose-schedule regimes. <em>BIOSTAT</em>, <em>26</em>(1), kxae034. (<a href='https://doi.org/10.1093/biostatistics/kxae034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The schedule of administering a drug has profound impact on the toxicity and efficacy profiles of the drug through changing its pharmacokinetics (PK). PK is an innate and indispensable component of the dose-schedule optimization. Motivated by this, we propose a Bayesian PK integrated dose-schedule finding (PKIDS) design to identify the optimal dose-schedule regime by integrating PK, toxicity, and efficacy data. Based on the causal pathway that dose and schedule affect PK, which in turn affects efficacy and toxicity, we jointly model the three endpoints by first specifying a Bayesian hierarchical model for the marginal distribution of the longitudinal dose-concentration process. Conditional on the drug concentration in plasma, we jointly model toxicity and efficacy as a function of the concentration. We quantify the risk-benefit of regimes using utility—continuously updating the estimates of PK, toxicity, and efficacy based on interim data—and make adaptive decisions to assign new patients to appropriate dose-schedule regimes via adaptive randomization. The simulation study shows that the PKIDS design has desirable operating characteristics.},
  archive      = {J_BIOSTAT},
  author       = {Lu, Mengyi and Yuan, Ying and Liu, Suyu},
  doi          = {10.1093/biostatistics/kxae034},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae034},
  shortjournal = {Biostatistics},
  title        = {A bayesian pharmacokinetics integrated phase I–II design to optimize dose-schedule regimes},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMM for discovering decision-making dynamics using reinforcement learning experiments. <em>BIOSTAT</em>, <em>26</em>(1), kxae033. (<a href='https://doi.org/10.1093/biostatistics/kxae033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major depressive disorder (MDD), a leading cause of years of life lived with disability, presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes, such as gains or losses in the laboratory. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing (e.g. reward sensitivity) to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task within the Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we propose a novel RL-HMM (hidden Markov model) framework for analyzing reward-based decision-making. Our model accommodates decision-making strategy switching between two distinct approaches under an HMM: subjects making decisions based on the RL model or opting for random choices. We account for continuous RL state space and allow time-varying transition probabilities in the HMM. We introduce a computationally efficient Expectation-maximization (EM) algorithm for parameter estimation and use a nonparametric bootstrap for inference. Extensive simulation studies validate the finite-sample performance of our method. We apply our approach to the EMBARC study to show that MDD patients are less engaged in RL compared to the healthy controls, and engagement is associated with brain activities in the negative affect circuitry during an emotional conflict task.},
  archive      = {J_BIOSTAT},
  author       = {Guo, Xingche and Zeng, Donglin and Wang, Yuanjia},
  doi          = {10.1093/biostatistics/kxae033},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae033},
  shortjournal = {Biostatistics},
  title        = {HMM for discovering decision-making dynamics using reinforcement learning experiments},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pooling controls from nested case–control studies with the proportional risks model. <em>BIOSTAT</em>, <em>26</em>(1), kxae032. (<a href='https://doi.org/10.1093/biostatistics/kxae032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard approach to regression modeling for cause-specific hazards with prospective competing risks data specifies separate models for each failure type. An alternative proposed by Lunn and McNeil (1995) assumes the cause-specific hazards are proportional across causes. This may be more efficient than the standard approach, and allows the comparison of covariate effects across causes. In this paper, we extend Lunn and McNeil (1995) to nested case–control studies, accommodating scenarios with additional matching and non-proportionality. We also consider the case where data for different causes are obtained from different studies conducted in the same cohort. It is demonstrated that while only modest gains in efficiency are possible in full cohort analyses, substantial gains may be attained in nested case–control analyses for failure types that are relatively rare. Extensive simulation studies are conducted and real data analyses are provided using the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial (PLCO) study.},
  archive      = {J_BIOSTAT},
  author       = {Chang, Yen and Ivanova, Anastasia and Albanes, Demetrius and Fine, Jason P and Shin, Yei Eun},
  doi          = {10.1093/biostatistics/kxae032},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae032},
  shortjournal = {Biostatistics},
  title        = {Pooling controls from nested case–control studies with the proportional risks model},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exposure proximal immune correlates analysis. <em>BIOSTAT</em>, <em>26</em>(1), kxae031. (<a href='https://doi.org/10.1093/biostatistics/kxae031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immune response decays over time, and vaccine-induced protection often wanes. Understanding how vaccine efficacy changes over time is critical to guiding the development and application of vaccines in preventing infectious diseases. The objective of this article is to develop statistical methods that assess the effect of decaying immune responses on the risk of disease and on vaccine efficacy, within the context of Cox regression with sparse sampling of immune responses, in a baseline-naive population. We aim to further disentangle the various aspects of the time-varying vaccine effect, whether direct on disease or mediated through immune responses. Based on time-to-event data from a vaccine efficacy trial and sparse sampling of longitudinal immune responses, we propose a weighted estimated induced likelihood approach that models the longitudinal immune response trajectory and the time to event separately. This approach assesses the effects of the decaying immune response, the peak immune response, and/or the waning vaccine effect on the risk of disease. The proposed method is applicable not only to standard randomized trial designs but also to augmented vaccine trial designs that re-vaccinate uninfected placebo recipients at the end of the standard trial period. We conducted simulation studies to evaluate the performance of our method and applied the method to analyze immune correlates from a phase III SARS-CoV-2 vaccine trial.},
  archive      = {J_BIOSTAT},
  author       = {Huang, Ying and Follmann, Dean},
  doi          = {10.1093/biostatistics/kxae031},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae031},
  shortjournal = {Biostatistics},
  title        = {Exposure proximal immune correlates analysis},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive gaussian markov random fields for child mortality estimation. <em>BIOSTAT</em>, <em>26</em>(1), kxae030. (<a href='https://doi.org/10.1093/biostatistics/kxae030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The under-5 mortality rate (U5MR), a critical health indicator, is typically estimated from household surveys in lower and middle income countries. Spatio-temporal disaggregation of household survey data can lead to highly variable estimates of U5MR, necessitating the usage of smoothing models which borrow information across space and time. The assumptions of common smoothing models may be unrealistic when certain time periods or regions are expected to have shocks in mortality relative to their neighbors, which can lead to oversmoothing of U5MR estimates. In this paper, we develop a spatial and temporal smoothing approach based on Gaussian Markov random field models which incorporate knowledge of these expected shocks in mortality. We demonstrate the potential for these models to improve upon alternatives not incorporating knowledge of expected shocks in a simulation study. We apply these models to estimate U5MR in Rwanda at the national level from 1985 to 2019, a time period which includes the Rwandan civil war and genocide.},
  archive      = {J_BIOSTAT},
  author       = {Aleshin-Guendel, Serge and Wakefield, Jon},
  doi          = {10.1093/biostatistics/kxae030},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae030},
  shortjournal = {Biostatistics},
  title        = {Adaptive gaussian markov random fields for child mortality estimation},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction. <em>BIOSTAT</em>, <em>26</em>(1), kxae029. (<a href='https://doi.org/10.1093/biostatistics/kxae029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxae029},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae029},
  shortjournal = {Biostatistics},
  title        = {Correction},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating prior information in gene expression network-based cancer heterogeneity analysis. <em>BIOSTAT</em>, <em>26</em>(1), kxae028. (<a href='https://doi.org/10.1093/biostatistics/kxae028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is molecularly heterogeneous, with seemingly similar patients having different molecular landscapes and accordingly different clinical behaviors. In recent studies, gene expression networks have been shown as more effective/informative for cancer heterogeneity analysis than some simpler measures. Gene interconnections can be classified as “direct” and “indirect,” where the latter can be caused by shared genomic regulators (such as transcription factors, microRNAs, and other regulatory molecules) and other mechanisms. It has been suggested that incorporating the regulators of gene expressions in network analysis and focusing on the direct interconnections can lead to a deeper understanding of the more essential gene interconnections. Such analysis can be seriously challenged by the large number of parameters (jointly caused by network analysis, incorporation of regulators, and heterogeneity) and often weak signals. To effectively tackle this problem, we propose incorporating prior information contained in the published literature. A key challenge is that such prior information can be partial or even wrong. We develop a two-step procedure that can flexibly accommodate different levels of prior information quality. Simulation demonstrates the effectiveness of the proposed approach and its superiority over relevant competitors. In the analysis of a breast cancer dataset, findings different from the alternatives are made, and the identified sample subgroups have important clinical differences.},
  archive      = {J_BIOSTAT},
  author       = {Li, Rong and Xu, Shaodong and Li, Yang and Tang, Zuojian and Feng, Di and Cai, James and Ma, Shuangge},
  doi          = {10.1093/biostatistics/kxae028},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae028},
  shortjournal = {Biostatistics},
  title        = {Incorporating prior information in gene expression network-based cancer heterogeneity analysis},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Direct estimation and inference of higher-level correlations from lower-level measurements with applications in gene-pathway and proteomics studies. <em>BIOSTAT</em>, <em>26</em>(1), kxae027. (<a href='https://doi.org/10.1093/biostatistics/kxae027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the challenge of estimating correlations between higher-level biological variables (e.g. proteins and gene pathways) when only lower-level measurements are directly observed (e.g. peptides and individual genes). Existing methods typically aggregate lower-level data into higher-level variables and then estimate correlations based on the aggregated data. However, different data aggregation methods can yield varying correlation estimates as they target different higher-level quantities. Our solution is a latent factor model that directly estimates these higher-level correlations from lower-level data without the need for data aggregation. We further introduce a shrinkage estimator to ensure the positive definiteness and improve the accuracy of the estimated correlation matrix. Furthermore, we establish the asymptotic normality of our estimator, enabling efficient computation of P -values for the identification of significant correlations. The effectiveness of our approach is demonstrated through comprehensive simulations and the analysis of proteomics and gene expression datasets. We develop the R package highcor for implementing our method.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Yue and Shi, Haoran},
  doi          = {10.1093/biostatistics/kxae027},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae027},
  shortjournal = {Biostatistics},
  title        = {Direct estimation and inference of higher-level correlations from lower-level measurements with applications in gene-pathway and proteomics studies},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regression and alignment for functional data and network topology. <em>BIOSTAT</em>, <em>26</em>(1), kxae026. (<a href='https://doi.org/10.1093/biostatistics/kxae026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the brain, functional connections form a network whose topological organization can be described by graph-theoretic network diagnostics. These include characterizations of the community structure, such as modularity and participation coefficient, which have been shown to change over the course of childhood and adolescence. To investigate if such changes in the functional network are associated with changes in cognitive performance during development, network studies often rely on an arbitrary choice of preprocessing parameters, in particular the proportional threshold of network edges. Because the choice of parameter can impact the value of the network diagnostic, and therefore downstream conclusions, we propose to circumvent that choice by conceptualizing the network diagnostic as a function of the parameter. As opposed to a single value, a network diagnostic curve describes the connectome topology at multiple scales—from the sparsest group of the strongest edges to the entire edge set. To relate these curves to executive function and other covariates, we use scalar-on-function regression, which is more flexible than previous functional data-based models used in network neuroscience. We then consider how systematic differences between networks can manifest in misalignment of diagnostic curves, and consequently propose a supervised curve alignment method that incorporates auxiliary information from other variables. Our algorithm performs both functional regression and alignment via an iterative, penalized, and nonlinear likelihood optimization. The illustrated method has the potential to improve the interpretability and generalizability of neuroscience studies where the goal is to study heterogeneity among a mixture of function- and scalar-valued measures.},
  archive      = {J_BIOSTAT},
  author       = {Tu, Danni and Wrobel, Julia and Satterthwaite, Theodore D and Goldsmith, Jeff and Gur, Ruben C and Gur, Raquel E and Gertheiss, Jan and Bassett, Dani S and Shinohara, Russell T},
  doi          = {10.1093/biostatistics/kxae026},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae026},
  shortjournal = {Biostatistics},
  title        = {Regression and alignment for functional data and network topology},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating causal effects for binary outcomes using per-decision inverse probability weighting. <em>BIOSTAT</em>, <em>26</em>(1), kxae025. (<a href='https://doi.org/10.1093/biostatistics/kxae025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-randomized trials are commonly conducted for optimizing mobile health interventions such as push notifications for behavior change. In analyzing such trials, causal excursion effects are often of primary interest, and their estimation typically involves inverse probability weighting (IPW). However, in a micro-randomized trial, additional treatments can often occur during the time window over which an outcome is defined, and this can greatly inflate the variance of the causal effect estimator because IPW would involve a product of numerous weights. To reduce variance and improve estimation efficiency, we propose two new estimators using a modified version of IPW, which we call “per-decision IPW.” The second estimator further improves efficiency using the projection idea from the semiparametric efficiency theory. These estimators are applicable when the outcome is binary and can be expressed as the maximum of a series of sub-outcomes defined over sub-intervals of time. We establish the estimators’ consistency and asymptotic normality. Through simulation studies and real data applications, we demonstrate substantial efficiency improvement of the proposed estimator over existing estimators. The new estimators can be used to improve the precision of primary and secondary analyses for micro-randomized trials with binary outcomes.},
  archive      = {J_BIOSTAT},
  author       = {Bao, Yihan and Bell, Lauren and Williamson, Elizabeth and Garnett, Claire and Qian, Tianchen},
  doi          = {10.1093/biostatistics/kxae025},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae025},
  shortjournal = {Biostatistics},
  title        = {Estimating causal effects for binary outcomes using per-decision inverse probability weighting},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian estimation of covariate assisted principal regression for brain functional connectivity. <em>BIOSTAT</em>, <em>26</em>(1), kxae023. (<a href='https://doi.org/10.1093/biostatistics/kxae023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Bayesian reformulation of covariate-assisted principal regression for covariance matrix outcomes to identify low-dimensional components in the covariance associated with covariates. By introducing a geometric approach to the covariance matrices and leveraging Euclidean geometry, we estimate dimension reduction parameters and model covariance heterogeneity based on covariates. This method enables joint estimation and uncertainty quantification of relevant model parameters associated with heteroscedasticity. We demonstrate our approach through simulation studies and apply it to analyze associations between covariates and brain functional connectivity using data from the Human Connectome Project.},
  archive      = {J_BIOSTAT},
  author       = {Park, Hyung G},
  doi          = {10.1093/biostatistics/kxae023},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae023},
  shortjournal = {Biostatistics},
  title        = {Bayesian estimation of covariate assisted principal regression for brain functional connectivity},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A modeling framework for detecting and leveraging node-level information in bayesian network inference. <em>BIOSTAT</em>, <em>26</em>(1), kxae021. (<a href='https://doi.org/10.1093/biostatistics/kxae021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian graphical models are powerful tools to infer complex relationships in high dimension, yet are often fraught with computational and statistical challenges. If exploited in a principled way, the increasing information collected alongside the data of primary interest constitutes an opportunity to mitigate these difficulties by guiding the detection of dependence structures. For instance, gene network inference may be informed by the use of publicly available summary statistics on the regulation of genes by genetic variants. Here we present a novel Gaussian graphical modeling framework to identify and leverage information on the centrality of nodes in conditional independence graphs. Specifically, we consider a fully joint hierarchical model to simultaneously infer (i) sparse precision matrices and (ii) the relevance of node-level information for uncovering the sought-after network structure. We encode such information as candidate auxiliary variables using a spike-and-slab submodel on the propensity of nodes to be hubs, which allows hypothesis-free selection and interpretation of a sparse subset of relevant variables. As efficient exploration of large posterior spaces is needed for real-world applications, we develop a variational expectation conditional maximization algorithm that scales inference to hundreds of samples, nodes and auxiliary variables. We illustrate and exploit the advantages of our approach in simulations and in a gene network study which identifies hub genes involved in biological pathways relevant to immune-mediated diseases.},
  archive      = {J_BIOSTAT},
  author       = {Xi, Xiaoyue and Ruffieux, Hélène},
  doi          = {10.1093/biostatistics/kxae021},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae021},
  shortjournal = {Biostatistics},
  title        = {A modeling framework for detecting and leveraging node-level information in bayesian network inference},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based multifacet clustering with high-dimensional omics applications. <em>BIOSTAT</em>, <em>26</em>(1), kxae020. (<a href='https://doi.org/10.1093/biostatistics/kxae020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional omics data often contain intricate and multifaceted information, resulting in the coexistence of multiple plausible sample partitions based on different subsets of selected features. Conventional clustering methods typically yield only one clustering solution, limiting their capacity to fully capture all facets of cluster structures in high-dimensional data. To address this challenge, we propose a model-based multifacet clustering (MFClust) method based on a mixture of Gaussian mixture models, where the former mixture achieves facet assignment for gene features and the latter mixture determines cluster assignment of samples. We demonstrate superior facet and cluster assignment accuracy of MFClust through simulation studies. The proposed method is applied to three transcriptomic applications from postmortem brain and lung disease studies. The result captures multifacet clustering structures associated with critical clinical variables and provides intriguing biological insights for further hypothesis generation and discovery.},
  archive      = {J_BIOSTAT},
  author       = {Zong, Wei and Li, Danyang and Seney, Marianne L and Mcclung, Colleen A and Tseng, George C},
  doi          = {10.1093/biostatistics/kxae020},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae020},
  shortjournal = {Biostatistics},
  title        = {Model-based multifacet clustering with high-dimensional omics applications},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A marginal structural model for normal tissue complication probability. <em>BIOSTAT</em>, <em>26</em>(1), kxae019. (<a href='https://doi.org/10.1093/biostatistics/kxae019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of radiation therapy for cancer is to deliver prescribed radiation dose to the tumor while minimizing dose to the surrounding healthy tissues. To evaluate treatment plans, the dose distribution to healthy organs is commonly summarized as dose-volume histograms (DVHs). Normal tissue complication probability (NTCP) modeling has centered around making patient-level risk predictions with features extracted from the DVHs, but few have considered adapting a causal framework to evaluate the safety of alternative treatment plans. We propose causal estimands for NTCP based on deterministic and stochastic interventions, as well as propose estimators based on marginal structural models that impose bivariable monotonicity between dose, volume, and toxicity risk. The properties of these estimators are studied through simulations, and their use is illustrated in the context of radiotherapy treatment of anal canal cancer patients.},
  archive      = {J_BIOSTAT},
  author       = {Tang, Thai-Son and Liu, Zhihui and Hosni, Ali and Kim, John and Saarela, Olli},
  doi          = {10.1093/biostatistics/kxae019},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae019},
  shortjournal = {Biostatistics},
  title        = {A marginal structural model for normal tissue complication probability},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic EM algorithm for partially observed stochastic epidemics with individual heterogeneity. <em>BIOSTAT</em>, <em>26</em>(1), kxae018. (<a href='https://doi.org/10.1093/biostatistics/kxae018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a stochastic epidemic model progressing over dynamic networks, where infection rates are heterogeneous and may vary with individual-level covariates. The joint dynamics are modeled as a continuous-time Markov chain such that disease transmission is constrained by the contact network structure, and network evolution is in turn influenced by individual disease statuses. To accommodate partial epidemic observations commonly seen in real-world data, we propose a stochastic EM algorithm for inference, introducing key innovations that include efficient conditional samplers for imputing missing infection and recovery times which respect the dynamic contact network. Experiments on both synthetic and real datasets demonstrate that our inference method can accurately and efficiently recover model parameters and provide valuable insight at the presence of unobserved disease episodes in epidemic data.},
  archive      = {J_BIOSTAT},
  author       = {Bu, Fan and Aiello, Allison E and Volfovsky, Alexander and Xu, Jason},
  doi          = {10.1093/biostatistics/kxae018},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae018},
  shortjournal = {Biostatistics},
  title        = {Stochastic EM algorithm for partially observed stochastic epidemics with individual heterogeneity},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous clustering and estimation of networks in multiple graphical models. <em>BIOSTAT</em>, <em>26</em>(1), kxae015. (<a href='https://doi.org/10.1093/biostatistics/kxae015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models are widely used to study the dependence structure among variables. When samples are obtained from multiple conditions or populations, joint analysis of multiple graphical models are desired due to their capacity to borrow strength across populations. Nonetheless, existing methods often overlook the varying levels of similarity between populations, leading to unsatisfactory results. Moreover, in many applications, learning the population-level clustering structure itself is of particular interest. In this article, we develop a novel method, called S imultaneous C lustering and E stimation of N etworks via T ensor decomposition (SCENT), that simultaneously clusters and estimates graphical models from multiple populations. Precision matrices from different populations are uniquely organized as a three-way tensor array, and a low-rank sparse model is proposed for joint population clustering and network estimation. We develop a penalized likelihood method and an augmented Lagrangian algorithm for model fitting. We also establish the clustering accuracy and norm consistency of the estimated precision matrices. We demonstrate the efficacy of the proposed method with comprehensive simulation studies. The application to the Genotype-Tissue Expression multi-tissue gene expression data provides important insights into tissue clustering and gene coexpression patterns in multiple brain tissues.},
  archive      = {J_BIOSTAT},
  author       = {Li, Gen and Wang, Miaoyan},
  doi          = {10.1093/biostatistics/kxae015},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae015},
  shortjournal = {Biostatistics},
  title        = {Simultaneous clustering and estimation of networks in multiple graphical models},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A joint normal-ordinal (probit) model for ordinal and continuous longitudinal data. <em>BIOSTAT</em>, <em>26</em>(1), kxae014. (<a href='https://doi.org/10.1093/biostatistics/kxae014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, continuous and ordinal longitudinal variables are frequently encountered. In many of these studies it is of interest to estimate the effect of one of these longitudinal variables on the other. Time-dependent covariates have, however, several limitations; they can, for example, not be included when the data is not collected at fixed intervals. The issues can be circumvented by implementing joint models, where two or more longitudinal variables are treated as a response and modeled with a correlated random effect. Next, by conditioning on these response(s), we can study the effect of one or more longitudinal variables on another. We propose a normal-ordinal(probit) joint model. First, we derive closed-form formulas to estimate the model-based correlations between the responses on their original scale. In addition, we derive the marginal model, where the interpretation is no longer conditional on the random effects. As a consequence, we can make predictions for a subvector of one response conditional on the other response and potentially a subvector of the history of the response. Next, we extend the approach to a high-dimensional case with more than two ordinal and/or continuous longitudinal variables. The methodology is applied to a case study where, among others, a longitudinal ordinal response is predicted with a longitudinal continuous variable.},
  archive      = {J_BIOSTAT},
  author       = {Delporte, Margaux and Molenberghs, Geert and Fieuws, Steffen and Verbeke, Geert},
  doi          = {10.1093/biostatistics/kxae014},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae014},
  shortjournal = {Biostatistics},
  title        = {A joint normal-ordinal (probit) model for ordinal and continuous longitudinal data},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semiparametric gaussian mixture model for chest CT-based 3D blood vessel reconstruction. <em>BIOSTAT</em>, <em>26</em>(1), kxae013. (<a href='https://doi.org/10.1093/biostatistics/kxae013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) has been a powerful diagnostic tool since its emergence in the 1970s. Using CT data, 3D structures of human internal organs and tissues, such as blood vessels, can be reconstructed using professional software. This 3D reconstruction is crucial for surgical operations and can serve as a vivid medical teaching example. However, traditional 3D reconstruction heavily relies on manual operations, which are time-consuming, subjective, and require substantial experience. To address this problem, we develop a novel semiparametric Gaussian mixture model tailored for the 3D reconstruction of blood vessels. This model extends the classical Gaussian mixture model by enabling nonparametric variations in the component-wise parameters of interest according to voxel positions. We develop a kernel-based expectation–maximization algorithm for estimating the model parameters, accompanied by a supporting asymptotic theory. Furthermore, we propose a novel regression method for optimal bandwidth selection. Compared to the conventional cross-validation-based (CV) method, the regression method outperforms the CV method in terms of computational and statistical efficiency. In application, this methodology facilitates the fully automated reconstruction of 3D blood vessel structures with remarkable accuracy.},
  archive      = {J_BIOSTAT},
  author       = {Zeng, Qianhan and Zhou, Jing and Ji, Ying and Wang, Hansheng},
  doi          = {10.1093/biostatistics/kxae013},
  journal      = {Biostatistics},
  number       = {1},
  pages        = {kxae013},
  shortjournal = {Biostatistics},
  title        = {A semiparametric gaussian mixture model for chest CT-based 3D blood vessel reconstruction},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="comjnl">COMJNL - 15</h2>
<ul>
<li><details>
<summary>
(2025). A dual protection technology to thwart hardware trojan insertion based on observability. <em>COMJNL</em>, <em>68</em>(8), 1074-1085. (<a href='https://doi.org/10.1093/comjnl/bxaf050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The globalization of the integrated circuit design industry makes it easier for the adversary to pirate intellectual property and insert hardware Trojans (HTs). Although many HT protection methods have been proposed, some malicious elements can still be inserted into vulnerable nodes. Trojans are usually inserted in the rare nodes with the lowest observability, which makes it hard for testers to discover them. In this paper, we propose a dual-protection technology to protect the circuit against HT attacks based on observability. First, we propose an algorithm to increase the low observabilities of the circuit, so as to make it difficult for attackers to implant HTs. Several existing approaches try to identify the low observability by setting a threshold artificially, which is not generic. We do not need to set thresholds when targeting the low observability. Second, we present another logic locking algorithm to enhance the entire circuit’s security further. Simulation results on ISCAS85 benchmark and several larger circuits show that the proposed HT protection method has increased the lowest observability of the circuit by an average of 370.76 times. Furthermore, the logic locking technique maximizes the ambiguity for an attacker. Compared with the state of the art, the proposed logic locking can obtain better results, achieving a Hamming distance that is closer to 50% between the correct and incorrect outputs when a wrong key is applied.},
  archive      = {J_COMJNL},
  author       = {Wang, Zhen and Lv, Jinfeng and Zhou, Yuhao and Jiang, Jianhui and Wang, Yong},
  doi          = {10.1093/comjnl/bxaf050},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {1074-1085},
  shortjournal = {Comput. J.},
  title        = {A dual protection technology to thwart hardware trojan insertion based on observability},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compression cryptosystem. <em>COMJNL</em>, <em>68</em>(8), 1062-1073. (<a href='https://doi.org/10.1093/comjnl/bxaf023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A compression cryptosystem is a single coding process, the output of which is both reduced in space and secure against unauthorized decoding. Considering both Huffman and arithmetic coding, this paper proposes to apply repeatedly minor changes to the compression model, with negligible deterioration of its optimality. The cumulative impact of a large number of such changes leads to completely different ciphertexts, which can be decrypted only if a given secret key is known. The security of the system is based on the NP-completeness of a problem related to breaking the code. Several variants are suggested, and their results are tested in various settings, including for security against chosen plaintext attacks.},
  archive      = {J_COMJNL},
  author       = {Gross, Yoav and Klein, Shmuel T and Opalinsky, Elina and Revivo, Rivka and Shapira, Dana},
  doi          = {10.1093/comjnl/bxaf023},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {1062-1073},
  shortjournal = {Comput. J.},
  title        = {Compression cryptosystem},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel curriculum learning framework for multi-label emotion classification. <em>COMJNL</em>, <em>68</em>(8), 1050-1061. (<a href='https://doi.org/10.1093/comjnl/bxaf022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Curriculum learning (CL) is a training strategy that imitates how humans learn, by gradually introducing more complex samples and information to the model. However, in multi-label emotion classification (MEC) tasks, using a traditional CL approach can result in overfitting on easy samples and lead to biased training. Additionally, the sample difficulty varies as the model trains. To address these challenges, we propose a novel CL framework for MEC tasks called CLF-MEC. Unlike traditional approaches that assess difficulty at the sample level, we utilize category-level assessment to determine the difficulty level of samples. As the model identifies a category well, the score for that category’s samples is reduced, ensuring dynamic changes in the sample difficulty are accounted for. Our CL framework employs two training modes, namely “learning” and “tackling.” These two processes are trained alternatively to imitate the “learning-tackling” process in human learning. This ensures that samples from hard-to-learn categories receive more attention. During the “tackling” process, our method transforms the task of dealing with hard samples into an “easy” learning task by utilizing contrastive learning to enhance the semantic representation of those hard samples. Experimental results demonstrate that our CLF-MEC framework has achieved significant improvements in MEC.},
  archive      = {J_COMJNL},
  author       = {Lin, Nankai and Wu, Hongyan and Zeng, Peijian and Bai, Qifeng and Zhou, Dong and Yang, Aimin},
  doi          = {10.1093/comjnl/bxaf022},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {1050-1061},
  shortjournal = {Comput. J.},
  title        = {A novel curriculum learning framework for multi-label emotion classification},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new DDoS attack detection model based on improved stacked autoencoder and gated recurrent unit for software defined network. <em>COMJNL</em>, <em>68</em>(8), 1028-1049. (<a href='https://doi.org/10.1093/comjnl/bxaf021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of Software Defined Networking (SDN), detecting Distributed Denial of Service (DDoS) attacks has become an urgent challenge in SDN maintenance and Security. Given the diversity of DDoS attack types, we face significant challenges. This paper proposes a model called ARSAE-QGRU, which is based on integrating attention mechanisms and residual connections within a stacked autoencoder for DDoS attack detection. By introducing attention mechanisms and residual connections into the stacked autoencoder (SAE), the model effectively conveys more valuable information and facilitates gradient propagation, allowing it to learn low-dimensional representations better. It also combines the learned low-dimensional representations with traffic features to generate data for DDoS attack training. Furthermore, incorporating Gated Recurrent Unit aids in a more in-depth understanding of the temporal characteristics of traffic data, resulting in improved detection accuracy. This model demonstrates outstanding performance on the CICDDoS2019 and CICIDS2017 datasets, achieving accuracy rates of 97.2% and 97.9%, respectively. Moreover, when applied to datasets in SDN environments, it reaches an even higher accuracy rate of 99.8%. This research provides a reliable solution for high-dimensional data processing and DDoS attack detection within SDN, addressing the urgent challenges in these domains.},
  archive      = {J_COMJNL},
  author       = {Wang, Haizhen and Jia, Na and He, Yang and Lian, Zuozheng},
  doi          = {10.1093/comjnl/bxaf021},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {1028-1049},
  shortjournal = {Comput. J.},
  title        = {A new DDoS attack detection model based on improved stacked autoencoder and gated recurrent unit for software defined network},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted twin support vector machine with rescaled hinge loss. <em>COMJNL</em>, <em>68</em>(8), 1013-1027. (<a href='https://doi.org/10.1093/comjnl/bxaf020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighted twin support vector machine (WTSVM) has been proved to be effective for classification problems. However, it is sensitive to noises, especially for data corrupted by outliers. In this paper, we propose an improved classifier termed as weighted twin support vector machine with rescaled hinge loss (RHWTSVM). Similar to WTSVM, it uses the intra-class KNN technique to extract structural information in the same class. It uses the inter-class KNN technique to reduce the redundant constraints to improve the computational speed. Furthermore, we introduce the regularization term into the objective function to make the proposed RHWTSVM implement the principles of structural risk minimization and empirical risk minimization simultaneously. Besides, we use the rescaled hinge loss function which is a monotonic, bounded, and nonconvex loss to replace the traditional hinge loss function in WTSVM to make the proposed classifier more robust. Therefore, the RHWTSVM is less sensitive to outliers. Because the model is a nonconvex optimization problem, we use the half-quadratic optimization method to solve it and find that the new method is equivalent to an iterative WTSVM. Numerical experiments on datasets with various levels of noise demonstrate that RHWTSVM is reasonable and effective.},
  archive      = {J_COMJNL},
  author       = {Zhang, Siyuan and Zhang, Yixuan and Feng, Jianying},
  doi          = {10.1093/comjnl/bxaf020},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {1013-1027},
  shortjournal = {Comput. J.},
  title        = {Weighted twin support vector machine with rescaled hinge loss},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). F2PQNN: A fast and secure two-party inference on quantized convolutional neural networks. <em>COMJNL</em>, <em>68</em>(8), 998-1012. (<a href='https://doi.org/10.1093/comjnl/bxaf019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The machine learning as a service (MLaaS) paradigm has been widely adopted across various applications. However, it also raises significant privacy concerns, particularly regarding the exposure of input data and trained models. Two-party computation in convolutional neural network (CNN) inference has emerged as a promising solution to address these privacy issues in MLaaS. Nevertheless, most existing privacy-preserving CNN architectures rely on computationally expensive encryption methods, resulting in prolonged inference times and increased communication overhead. In this paper, we propose F2PQNN, a fast and secure two-party inference framework for quantized CNNs. To minimize reliance on computationally intensive encryption, F2PQNN utilizes two non-colluding servers and integrates secret sharing with oblivious transfer techniques. Furthermore, F2PQNN incorporates quantization techniques, along with batching and asynchronous computation, to significantly accelerate inference predictions. We evaluate the performance of F2PQNN on the MNIST, Fashion-MNIST, CIFAR-10, and STL-10 datasets. Experimental results demonstrate that F2PQNN outperforms existing solutions, achieving a 9.14 × speedup and reducing communication overhead by 59.8 × on the MNIST dataset.},
  archive      = {J_COMJNL},
  author       = {Li, Jinguo and Yuan, Peichun and Zhang, Jin and Shen, Sheng and He, Yin and Xiao, Ruyang},
  doi          = {10.1093/comjnl/bxaf019},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {998-1012},
  shortjournal = {Comput. J.},
  title        = {F2PQNN: A fast and secure two-party inference on quantized convolutional neural networks},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault tolerance assessment of the data center network DPCell based on g-good-neighbor conditions. <em>COMJNL</em>, <em>68</em>(8), 985-997. (<a href='https://doi.org/10.1093/comjnl/bxaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data center networks (DCNs) provide critical data storage and computing services for cloud computing. The continuous increase in demand for cloud computing has led to a surge in data volume, necessitating the continual expansion of DCNs. However, this expansion also heightens the risk of device failures. Therefore, it is particularly important to study the fault tolerance of DCNs, which refers to their ability to ensure reliable communication even in the presence of device failures. Among DCNs constructed using dual-port servers, DPCell achieves higher scalability and bisection width while maintaining a smaller diameter. This paper assesses the fault tolerance of DPCell using two metrics: connectivity and diagnosability. Recognizing the limitations of traditional connectivity and diagnosability, we investigate the connectivity and diagnosability of DPCell under the condition that each fault-free node in the network has at least |$g$| fault-free neighbors. The results indicate that, under this condition, the connectivity and diagnosability of DPCell exceed its traditional metrics by more than |$g$| times.},
  archive      = {J_COMJNL},
  author       = {Dong, Hui and Wang, Huaqun and Lv, Mengjie and Fan, Weibei},
  doi          = {10.1093/comjnl/bxaf018},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {985-997},
  shortjournal = {Comput. J.},
  title        = {Fault tolerance assessment of the data center network DPCell based on g-good-neighbor conditions},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing an intelligent framework with blockchain capabilities for environmental monitoring using a CubeSat. <em>COMJNL</em>, <em>68</em>(8), 968-984. (<a href='https://doi.org/10.1093/comjnl/bxaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellites have revolutionised the way that the planet’s environment is monitored via a unique perspective from above. Indeed, environmental monitoring is crucial for understanding and addressing the complex challenges facing the planet, which helps in decision-making and ensuring a sustainable future. Thus, this work aims to develop an intelligent model that includes artificial neural networks and deep learning approaches that are coupled with Blockchain capabilities for secure environmental monitoring using a CubeSat. The CubeSat, which is a small satellite platform, is equipped with a designed communication payload, including an adaptive Multiple-Input Multiple-Output antenna as well as an High Definition (HD) camera for better connectivity and precision aerial imaging. The proposed solution is simulated, tested, and validated from four scenarios, namely, water detection, tree counting and vegetation assessment, and oil spill detection. Ensuring the security and integrity of the data transmitted between the CubeSat and the ground station is of paramount importance; this is where Blockchain technology comes into play. The obtained results show high accuracy in monitoring environmental surfaces like water, trees, and coasts in an effective and rapid deployment fashion. Also, performance indicators of the Blockchain ensure data integrity and retrieval efficiency. Combining these technologies provides a valuable contribution to environmental monitoring.},
  archive      = {J_COMJNL},
  author       = {Almalki, Faris A},
  doi          = {10.1093/comjnl/bxaf017},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {968-984},
  shortjournal = {Comput. J.},
  title        = {Developing an intelligent framework with blockchain capabilities for environmental monitoring using a CubeSat},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BLFair: Enabling proportional I/O sharing for NVMe SSD in SPDK para-virtualization architecture. <em>COMJNL</em>, <em>68</em>(8), 953-967. (<a href='https://doi.org/10.1093/comjnl/bxaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data centers, the Storage Performance Development Kit (SPDK) para-virtualization architecture is an efficient solution for non-volatile memory express (NVMe) solid-state drive (SSD) virtualization but faces challenges in maintaining performance fairness and isolation due to storage resource competition among multi-tenants. However, the existing Quality of Service method in SPDK fails to ensure proportional I/O sharing among multi-tenants. Providing fairness and isolation while maintaining high storage utilization in SPDK remains a challenge. In this paper, we propose BLFair to address this problem. Specifically, BLFair implements proportional I/O sharing for multi-tenants in the SPDK. The design of BLFair can effectively reduce the high time complexity caused by the ordering and the overhead of maintaining the virtual clock. Moreover, BLFair allows for achieving a trade-off between proportional I/O sharing and maximizing storage utilization. BLFair also uses the lockless ring mechanism to achieve scalability for cross-core operation. We have implemented a prototype system of BLFair in SPDK. Finally, we conduct evaluations with different workloads in both local storage and NVMe over RDMA fabric environments. The results show that our method can achieve fairness and scalability. BLFair can achieve up to 7.09x 99.99th latency reduction compared to the system with no fairness. Evaluation results in realistic workloads also show that BLFair outperforms other methods.},
  archive      = {J_COMJNL},
  author       = {Feng, Yanchang and Ma, Junchao and Xia, Haojun and Zhang, Da and Tu, Bibo},
  doi          = {10.1093/comjnl/bxaf016},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {953-967},
  shortjournal = {Comput. J.},
  title        = {BLFair: Enabling proportional I/O sharing for NVMe SSD in SPDK para-virtualization architecture},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards effective privacy preservation in federated learning with automatic gradient clipping and gradient transformation perturbation. <em>COMJNL</em>, <em>68</em>(8), 939-952. (<a href='https://doi.org/10.1093/comjnl/bxaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy can effectively help federated learning resist privacy attacks from various parties. However, existing approaches that use differential privacy for privacy protection greatly decrease the model performance of federated learning, especially in scenarios with complex model structures and large parameters. In this paper, we propose a novel privacy preservation scheme for federated learning that combines automatic gradient clipping and gradient transformation perturbation. Our approach primarily reduces the impact of differential privacy on federated learning from two aspects. Firstly, we efficiently control the gradient sensitivity by using automatic gradient clipping instead of traditional threshold clipping. Secondly, we utilize the space transformation technique to alleviate the dramatic accuracy degradation of the model caused by the insertion noise. Extensive experiments on various benchmark datasets demonstrate that our approach achieves a good trade-off between data privacy and effectiveness under the same privacy budget.},
  archive      = {J_COMJNL},
  author       = {Wang, Chuanyin and Zhang, Yifei and Gao, Neng},
  doi          = {10.1093/comjnl/bxaf015},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {939-952},
  shortjournal = {Comput. J.},
  title        = {Towards effective privacy preservation in federated learning with automatic gradient clipping and gradient transformation perturbation},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DuoSQL: Towards elastic data warehousing via separated data management and processing. <em>COMJNL</em>, <em>68</em>(8), 926-938. (<a href='https://doi.org/10.1093/comjnl/bxaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving data warehouses (DWs) to the cloud is what today’s companies consider a trend towards cost-effective data management. To fully achieve the goal, the cloud DW system is supposed to adjust its resource provisioning to adapt to changing workload requirements. However, traditional data warehousing architecture lacks the flexibility for on-demand resource control, which severely restricts cost optimization and quality of service for both cloud providers and users. To build cloud DWs, new architectures are needed. This paper explores an architecture that decouples data management and processing to enable on-demand resource control. This optimized design enhances system elasticity and adaptability. However, this separation design is not without cost, as cooperation overhead can be high if not well optimized. For proof of concept, we build a prototype system, DuoSQL, using PostgreSQL for data management and Spark for data processing. To optimize cooperation, we conduct joint parameter tuning to improve overall system performance. We validate the system with the TPC-H benchmark. Results show the decoupling approach is flexible and offers significant performance potential.},
  archive      = {J_COMJNL},
  author       = {zhang, Weikang and Liu, Zhi and Bai, Tongxin and Zheng, Furong and Jin, Wenming and Wang, Yang},
  doi          = {10.1093/comjnl/bxaf014},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {926-938},
  shortjournal = {Comput. J.},
  title        = {DuoSQL: Towards elastic data warehousing via separated data management and processing},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to optimize based on rate decay. <em>COMJNL</em>, <em>68</em>(8), 908-925. (<a href='https://doi.org/10.1093/comjnl/bxaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to optimize (L2O) is a technique that uses neural networks to learn optimization algorithms automatically. While it holds promise for diverse optimization problems, achieving consistently ideal results remains a challenge. Typically, L2O through a parameterized optimization method (i.e. “ optimizer”) learns from training samples and generalizes to test tasks with the same distribution. However, the new test tasks usually have some deviation from the training set distribution. In this case, the generic L2O methods may not produce good optimization results. Thus, we introduce a step-size control mechanism based on the generic L2O to solve the common problem of insufficient control of the iteration amplitude in L2O and adopt different update strategies for various optimization problems to adapt to complex optimization scenarios. Additionally, we also innovatively use the gated recurrent unit network as the core model of the optimizer to achieve better optimization results. Finally, the experimental outcomes from numerical simulations and real-world datasets show that our proposed methods are significantly better than other optimization algorithms.},
  archive      = {J_COMJNL},
  author       = {Ma, Wenmin},
  doi          = {10.1093/comjnl/bxaf012},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {908-925},
  shortjournal = {Comput. J.},
  title        = {Learning to optimize based on rate decay},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MuST-GAN MFAS: Multi-semantic spoof tracer GAN with transformer layers for multi-modal face anti-spoofing. <em>COMJNL</em>, <em>68</em>(8), 891-907. (<a href='https://doi.org/10.1093/comjnl/bxaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-modal face anti-spoofing (MFAS), where RGB, depth, and infrared data are integrated, remarkable advancements have been seen. However, despite the advancement, there still exist challenges when it comes to adaptability, particularly in dealing with unseen attacks. In this paper, a novel model called MuST-GAN MFAS is presented. This model employs a generative network that incorporates modality-specific encoders and transformer layers. It is significant that the model efficiently disentangles multi-semantic spoof traces by utilizing the power of cross-modal attention mechanisms and a transformer-based spoof trace generator. The training process involves bidirectional adversarial learning, ensuring identity consistency, intensity, center, and classification losses are taken into consideration. Through precise evaluations, it has been shown that the proposed model surpasses existing frameworks, showing remarkable performance when evaluating several modal samples. In the end, MuST-GAN MFAS makes an impressive contribution to the field of face anti-spoofing by offering results that are easy to interpret and emphasizing how important it is to learn multi-semantic spoof traces in order to improve generalization and adaptability to unseen attacks. The code is available at https://github.com/ZainUlAbideenMalik/Must-GAN-MFAS .},
  archive      = {J_COMJNL},
  author       = {Liu, Shu and Ul Abideen, Zain and Wan, Tongming and Shahzad, Inzamam and Waseem, Abbas and Pan, Yushan},
  doi          = {10.1093/comjnl/bxaf011},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {891-907},
  shortjournal = {Comput. J.},
  title        = {MuST-GAN MFAS: Multi-semantic spoof tracer GAN with transformer layers for multi-modal face anti-spoofing},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zk-DCIAExchange: SGX protected fair exchange with distributed zero knowledge proof for data confidentiality and authentication. <em>COMJNL</em>, <em>68</em>(8), 872-890. (<a href='https://doi.org/10.1093/comjnl/bxaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based data transaction protocols augmented with zero-knowledge proofs offer fairness to the participants, yet they encounter challenges pertaining to both security and efficiency. We propose the zk-DSTARK, a zk-STARK-based protocol that enables distributed generation of zero-knowledge proofs, significantly reducing the computational burden. And zk-DSTARK inherits zk-STARK’s feature of single proof generation for multiple uses, improving the efficiency of successive transactions. Furthermore, we propose a fair exchange system named zk-DCIAExchange for off-chain verification, which is based on zk-DSTARK and intel software guard extensions (SGX). This system not only minimizes on-chain overhead but also ensures the security and fairness of the transaction. Experimental results show that, in continuous transactions scenarios, the time overhead for subsequent transactions is diminished by 99.9% compared to the first transaction; compared to zero knowledge contingent payment (ZKCSP), our scheme achieves a remarkable 92% reduction in time overhead, and a 26.3% reduction when compared to FairSwap; with 32 distributed nodes and a trace length of 2 16 , the proof generation time is reduced by ~85.45%; the additional verification time introduced by the SGX is ~0.45 s, which is deemed acceptable, and the on-chain verification overhead is reduced by ~7.2% compared to the ZKCSP and ~54.4% compared to FairSwap.},
  archive      = {J_COMJNL},
  author       = {Zhan, Jing and Li, Bo and Zhao, Jiang and Zhao, Yong},
  doi          = {10.1093/comjnl/bxaf010},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {872-890},
  shortjournal = {Comput. J.},
  title        = {Zk-DCIAExchange: SGX protected fair exchange with distributed zero knowledge proof for data confidentiality and authentication},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing arrhythmia identification using chaos theory and deep learning analysis. <em>COMJNL</em>, <em>68</em>(8), 859-871. (<a href='https://doi.org/10.1093/comjnl/bxaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal heart function is crucial for quality life. Electrocardiograms are indispensable for diagnosing cardiac irregularities and analyzing heartbeat signals. This study unveils three novel machine learning (ML) techniques enhancing diagnostic precision. The first method scrutinizes raw ECG data and its time-series metrics, while the second incorporates historical patient health data for direct ECG classification. The third technique transforms ECG signals into insightful features, focusing on QRS complex waves. Uniquely combining these strategies, our research pioneers in advanced feature selection for cardiac diagnosis. Experiments were conducted to compare the integration of clinical data, QRS characteristics, and chaos preprocessing impact on the diagnosis. For classification assessment, five algorithms were utilized: Support Vector Machine, Decision Tree, Naïve Bayes, Neural Network, and Convolutional Neural Networks (CNNs). Implementing chaos theory, we converted QRS features into deterministic metrics. Notably, our Chaos-Enhanced CNN model exhibited outstanding performance, achieving a remarkable 99.8% accuracy rate, outshining other models.},
  archive      = {J_COMJNL},
  author       = {Eldakhly, Nabil M},
  doi          = {10.1093/comjnl/bxaf009},
  journal      = {The Computer Journal},
  month        = {8},
  number       = {8},
  pages        = {859-871},
  shortjournal = {Comput. J.},
  title        = {Optimizing arrhythmia identification using chaos theory and deep learning analysis},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="evlett">EVLETT - 11</h2>
<ul>
<li><details>
<summary>
(2025). Repeated evolution of reduced visual investment at the onset of ecological speciation in high-altitude heliconius butterflies. <em>EVLETT</em>, <em>9</em>(4), 502-510. (<a href='https://doi.org/10.1093/evlett/qraf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colonization of new habitats is typically followed by divergent selection acting on traits that are immediately important for fitness. For example, differences between sensory environments are often associated with variation in sensory traits critical for navigation and foraging. However, the extent to which the initial response to novel sensory conditions is mediated by phenotypic plasticity, and the contribution of sensory or neural adaptation to early species divergence remains unclear. We took advantage of repeated cases of speciation in Heliconius butterflies with independent allopatric distributions in the west of the Colombian and Ecuadorian Andes. Using volumetric brain measurements, we analyzed patterns of investment in primary sensory processing areas of the brain across different localities and habitats. We find that a higher altitude species, Heliconius chestertonii , differs in levels of investment in visual and olfactory brain components compared with its lower altitude relative H. erato venus , mainly attributable to broad-sense heritable variation as inferred from comparisons between wild and common-garden-reared individuals. We provide evidence that this variation is consistent with divergent selection, and compare these shifts with those reported for another high-altitude species, H. himera , and its parapatric lowland counterpart, H. erato cyrbia , to demonstrate parallel reductions in the size of specific optic lobe neuropils. Conversely, for the antennal lobe, we detected different trait shifts in H. himera and H. chestertonii relative to their lowland H. erato neighbors. Overall, our findings add weight to the adaptive potential of neuroanatomical divergence related to sensory processing during early species formation. Repeated associations between trait variation and environmental shifts may indicate adaptation to local sources of natural selection. For instance, in sticklebacks, independent populations in separate freshwater habitats have evolved reduced armored plating, suggesting equivalent phenotypic responses to shared sources of natural selection. We compared independent cases of ecological divergence in Heliconius butterflies distributed along altitudinal gradients from sea level to mid-elevation in the west of the Colombian and Ecuadorian Andes. Shifts in altitude involve repeated, abrupt transitions from wet, large-leaved, warm forests to higher altitude scrub forests that are drier and more exposed. We tested hypotheses about the role of these ecological shifts in driving adaptive evolution in neuroanatomical traits during speciation. We showed that in Heliconius , independent changes in forest type have been accompanied by heritable, parallel patterns of divergence in investment in visual processing in the brain. We propose these differences likely facilitate species divergence in the face of ongoing gene flow.},
  archive      = {J_EVLETT},
  author       = {Rivas-Sánchez, David F and Morris, Jake and Salazar, Camilo and Pardo-Díaz, Carolina and Merrill, Richard M and Montgomery, Stephen H},
  doi          = {10.1093/evlett/qraf017},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {502-510},
  shortjournal = {Evol. Lett.},
  title        = {Repeated evolution of reduced visual investment at the onset of ecological speciation in high-altitude heliconius butterflies},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The evolution of reversible plasticity in stable environments. <em>EVLETT</em>, <em>9</em>(4), 491-501. (<a href='https://doi.org/10.1093/evlett/qraf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible plasticity, i.e., the ability to deconstruct phenotypic specializations based on environmental conditions, is widespread in nature. Despite its ubiquity, few mathematical models have explored the evolutionary selection pressures that favor trait reversibility. Therefore, many scenarios remain to be examined. In particular, existing theory has modeled trait development as an instantaneous process. These models do not capture the fact that trait development is often a constructive process, in which phenotypes incrementally adjust to local ecologies. Here, we present an optimality model of the evolution of reversible plasticity in which organisms build traits incrementally. In our model, organisms repeatedly sample cues to infer the environmental state—which can vary between generations but not within generations—and incrementally tailor their phenotypes to match their environments. Organisms also have the option to deconstruct phenotypic adjustments. We investigate two different modes of phenotypic deconstruction: Organisms can either deconstruct phenotypic adjustments incrementally or completely deconstruct all phenotypic adjustments in one time period. We highlight two results. First, early-life sensitive periods in construction precede mid-ontogeny sensitive periods in deconstruction. Intriguingly, although organisms typically only deconstruct toward the end of ontogeny, environmental cues in mid-ontogeny have the strongest impact on deconstruction. Second, in contrast to previous models, we find that reversibility often evolves in environments that are stable within generations. Thus, our model shows that reversibility does not require environmental change during development—as long as organisms are initially uncertain about environmental conditions. Our model provides new insights into the capacity for reversibility in species that have evolved in ontogenetically stable environments.},
  archive      = {J_EVLETT},
  author       = {Walasek, Nicole and Panchanathan, Karthik and Frankenhuis, Willem E},
  doi          = {10.1093/evlett/qraf015},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {491-501},
  shortjournal = {Evol. Lett.},
  title        = {The evolution of reversible plasticity in stable environments},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continent-wide differentiation of fitness traits and patterns of climate adaptation among european populations of drosophila melanogaster. <em>EVLETT</em>, <em>9</em>(4), 473-490. (<a href='https://doi.org/10.1093/evlett/qraf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A particularly well-studied evolutionary model is the vinegar fly Drosophila melanogaster , a cosmopolitan insect of ancestral southern-central African origin. Recent work suggests that it expanded out of Africa ∼9,000 years ago, and spread from the Middle East into Europe ∼1,800 years ago. During its global expansion, this human commensal adapted to novel climate zones and habitats. Despite much work on phenotypic differentiation and adaptation on several continents (especially North America and Australia), typically in the context of latitudinal clines, little is known about phenotypic divergence among European populations. Here, we sought to provide a continent-wide study of phenotypic differentiation among European populations of D. melanogaster . In a consortium-wide phenomics effort, we assayed 16 fitness-related traits on a panel of 173 isofemale lines from 9 European populations, with the majority of traits measured by several groups using semi-standardized protocols. For most fitness-related traits, we found significant differentiation among populations on a continental scale. Despite inevitable differences in assay conditions among labs, the reproducibility and hence robustness of our measurements were overall remarkably good. Several fitness components (e.g., viability, development time) exhibited significant latitudinal or longitudinal clines, and populations differed markedly in multivariate trait structure. Notably, populations experiencing higher humidity/rainfall and lower maximum temperature showed higher viability, fertility, starvation resistance, and lifespan at the expense of lower heat-shock survival, suggesting a pattern of local adaptation. Our results indicate that derived populations of this tropical fly have been shaped by pervasive spatially varying multivariate selection and adaptation to different climates on the European continent.},
  archive      = {J_EVLETT},
  author       = {Durmaz Mitchell, Esra and Kerdaffrec, Envel and Harney, Ewan and Paulo, Tânia F and Veselinovic, Marija Savic and Tanaskovic, Marija and Tyukmaeva, Venera and de Arcaya, Teresa Abaurrea Fernandez and Aksoy, Cansu and Argyridou, Eliza and Bailly, Tiphaine P M and Can, Dogus and Cobanoglu, Ezgi and Cook, Nicola and Coşkun, Seda and Davidovic, Slobodan and Demir, Ekin and Dias, Tânia and Rasouli-Dogaheh, Somayeh and Duque, Pedro and Eric, Katarina and Eric, Pavle and Erickson, Priscilla and Filipovski, Filip and Fishman, Bettina and Glaser-Schmitt, Amanda and Goldfischer, August and Green, Llewellyn and Janillon, Sonia and Jelic, Mihailo and Kostic, Hristina and Kreiman, Lucas E and Kremer, Natacha and Lyrakis, Manolis and Maistrenko, Oleksandr M and Marti, Sapho-Lou and McGunnigle, Megan and Merenciano, Miriam and Mira, Mário S and Montbel, Vincent and Mouton, Laurence and Mukha, Dmitry V and Murali, Siddharth and Patenkovic, Aleksandra and Protsenko, Oleksandra and Putero, Florencia A and Reis, Micael and Roshina, Natalia V and Rybina, Olga Y and Schou, Mads F and Schowing, Thibault and Senkal, Senel Selin and Serga, Svitlana and Trieu, Virginie and Symonenko, Alexander V and Trostnikov, Mikhail V and Tsybul'ko, Evgenia A and van den Heuvel, Joost and van Waarde, David and Veselkina, Ekaterina R and Vieira, Cristina P and Wang, Xiaocui and Zandveld, Jelle and Abbott, Jessica and Billeter, Jean-Christophe and Colinet, Hervé and Ebrahimi, Mehregan and Gibert, Patricia and Hrcek, Jan and Kankare, Maaria and Kozeretska, Iryna and Loeschcke, Volker and Mensch, Julián and Onder, Banu Sebnem and Parsch, John and Pasyukova, Elena G and Stamenkovic-Radak, Marina and Tauber, Eran and Vieira, Cristina and Wegener, Christian and Hoedjes, Katja M and Zwaan, Bas J and Betancourt, Andrea J and Fricke, Claudia and Grath, Sonja and Posnien, Nico and Vieira, Jorge and Kapun, Martin and Schlötterer, Christian and Schmidt, Paul and Sucena, Élio and González, Josefa and Bergland, Alan and Ritchie, Michael G and Flatt, Thomas},
  doi          = {10.1093/evlett/qraf014},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {473-490},
  shortjournal = {Evol. Lett.},
  title        = {Continent-wide differentiation of fitness traits and patterns of climate adaptation among european populations of drosophila melanogaster},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverging arabidopsis populations quickly accumulate pollen-acting genetic incompatibilities. <em>EVLETT</em>, <em>9</em>(4), 461-472. (<a href='https://doi.org/10.1093/evlett/qraf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process by which species diverge from one another, gradually accumulate genetic incompatibilities, and eventually reach full-fledged reproductive isolation is a key question in evolutionary biology. However, the nature of reproductive barriers, the pace at which they accumulate, and their genomic distribution remain poorly documented. The disruption of co-adapted epistatic interactions in hybrids and the accumulation of selfish genetic elements are proposed contributors to this process, and can lead to the distortion of the Mendelian segregation of the affected loci across the genome. In this study we detect and quantify segregation distortion across the genomes of crosses produced from a diverse sampling of Arabidopsis lyrata and A. halleri populations, 2 species at the early stages of speciation and that can still interbreed. We observe no distortion loci in crosses with geographically and genetically similar parents, but both their frequency of occurrence and their magnitude become highly variable in more distant crosses. We also observe that distorter loci evolve rapidly, as they occur not only in interspecific hybrids, but also in intraspecific hybrids produced by crossing individuals from 2 isolated regions. Finally, we identify both genome-wide nonindependence and 2 specific genomic regions on different chromosomes where opposite distortion effects are repeatedly observed across multiple F1 individuals, suggesting negative epistasis is a major contributor to the evolution of hybrid segregation distortion. Our study demonstrates that pollen-acting segregation distortion is ubiquitous, and may contribute not only to the ongoing reproductive isolation between A. halleri and A. lyrata , but also between recently diverged populations of the same species.},
  archive      = {J_EVLETT},
  author       = {Condon, Christopher and Carpentier, Fantin and Tabourin, Marie and Wozniak, Natalia and Takou, Margarita and Blassiau, Christelle and Kumar, Vinod and Pietzenuk, Björn and Habert, Rémi and De Meaux, Juliette and Krämer, Ute and Roux, Camille and Corbett-Detig, Russell and Castric, Vincent},
  doi          = {10.1093/evlett/qraf013},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {461-472},
  shortjournal = {Evol. Lett.},
  title        = {Diverging arabidopsis populations quickly accumulate pollen-acting genetic incompatibilities},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary trends in the emergence of skeletal cell types. <em>EVLETT</em>, <em>9</em>(4), 446-460. (<a href='https://doi.org/10.1093/evlett/qraf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell types are fundamental functional units of multicellular organisms. The evolutionary emergence of new cell types is underpinned by genetic changes, such as gene co-option and cis-regulatory evolution, that propel the assembly or rewiring of molecular networks and give rise to new cell types with specialized functions. Here, we integrate genomic phylostratigraphy with single-cell transcriptomics to explore the evolutionary trends in the assembly of the skeletal cell type-specific gene expression programs. In particular, we investigate how the emergence of lineage-specific genes contributed to this process. We show that osteoblasts and hypertrophic chondrocytes (HC) express evolutionary younger transcriptomes compared to immature chondrocytes that resemble the ancestral skeletogenic program. We demonstrate that the recruitment of lineage-specific genes resulted in subsequent elaboration and individuation of the ancestral chondrogenic gene expression program, propelling the emergence of osteoblasts and HC. Notably, osteoblasts show significant enrichment of vertebrate-specific genes, while HC is enriched in gnathostome-specific genes. By identifying the functional properties of the recruited genes, coupled with the recently discovered fossil evidence, our study challenges the long-standing view on the evolution of vertebrate skeletal structures by suggesting that endochondral ossification and chondrocyte hypertrophy may have already evolved in the last common ancestors of gnathostomes.},
  archive      = {J_EVLETT},
  author       = {Damatac, Amor and Koska, Sara and Ullrich, Kristian K and Domazet-Lošo, Tomislav and Klimovich, Alexander and Kaucká, Markéta},
  doi          = {10.1093/evlett/qraf012},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {446-460},
  shortjournal = {Evol. Lett.},
  title        = {Evolutionary trends in the emergence of skeletal cell types},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-mer-based diversity scales with population size proxies more than nucleotide diversity in a meta-analysis of 98 plant species. <em>EVLETT</em>, <em>9</em>(4), 434-445. (<a href='https://doi.org/10.1093/evlett/qraf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key prediction of neutral theory is that the level of genetic diversity in a population should scale with population size. However, as was noted by Richard Lewontin in 1974 and reaffirmed by later studies, the slope of the population size-diversity relationship in nature is much weaker than expected under neutral theory. We hypothesize that one contributor to this paradox is that current methods relying on single nucleotide polymorphisms (SNPs) called from aligning short reads to a reference genome underestimate levels of genetic diversity in many species. As a first step to testing this idea, we calculated nucleotide diversity ( ⁠ π ⁠ ) and k -mer-based metrics of genetic diversity across 112 plant species, amounting to over 205 terabases of DNA sequencing data from 27,488 individuals. After excluding 14 species with low coverage or no variant sites called, we compared how different diversity metrics correlated with proxies of population size that account for both range size and population density variation across species. We found that our population size proxies scaled anywhere from about 3 to over 20 times faster with k -mer diversity than nucleotide diversity after adjusting for evolutionary history, mating system, life cycle habit, cultivation status, and invasiveness. The relationship between k -mer diversity and population size proxies also remains significant after correcting for genome size, whereas the analogous relationship for nucleotide diversity does not. These results are consistent with the possibility that variation not captured by common SNP-based analyses explains part of Lewontin’s paradox in plants, but larger scale pangenomic studies are needed to definitively address this question. Even after many revolutions in our ability to sequence and understand DNA, many important biological questions remain unsolved. One such problem is Lewontin’s paradox, named after Richard Lewontin who first described it in 1974. The core of the paradox is a simple idea: species with more individuals should be more genetically diverse. The reasoning is that more individuals means more replication of DNA, and thus more opportunities for mutation to create new variation. However, species that differ massively in population size often have similar diversity levels. Lewontin’s paradox has several potential, previously investigated mechanisms but what if one contributor is simply that our measurements of genetic diversity are off? Most studies estimate diversity by comparing sample genomes to a standard reference genome. While this approach is useful, it is impossible to measure variation in DNA that is not represented in the reference—a phenomenon known as reference bias. We estimate metrics of diversity that are free of reference-bias and re-investigate Lewontin’s paradox in plants. Overall, we find that reference-free diversity metrics scale more with population size, compared with the reference-biased approach. While it is unlikely that reference-bias fully explains Lewontin’s paradox and larger-scale pangenomic studies are needed, our analyses are consistent with reference-bias contributing to Lewontin’s paradox.},
  archive      = {J_EVLETT},
  author       = {Roberts, Miles D and Josephs, Emily B},
  doi          = {10.1093/evlett/qraf011},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {434-445},
  shortjournal = {Evol. Lett.},
  title        = {K-mer-based diversity scales with population size proxies more than nucleotide diversity in a meta-analysis of 98 plant species},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Replicate geographic transects across a hybrid zone reveal parallelism and differences in the genetic architecture of reproductive isolation. <em>EVLETT</em>, <em>9</em>(4), 421-433. (<a href='https://doi.org/10.1093/evlett/qraf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the genetic architecture of traits involved in adaptation and speciation is one of the key components of understanding the evolutionary mechanisms behind biological diversification. Hybrid zones provide a unique opportunity to use genetic admixture to identify traits and loci contributing to partial reproductive barriers between taxa. Many studies have focused on the temporal dynamics of hybrid zones, but geographical variation in hybrid zones that span distinct ecological contexts has received less attention. We address this knowledge gap by analyzing hybridization and introgression between black-capped and Carolina chickadees in two geographically remote transects across their extensive hybrid zone, one located in eastern and one in central North America. Previous studies demonstrated that this hybrid zone is moving northward as a result of climate change but is staying consistently narrow due to selection against hybrids. In addition, the hybrid zone is moving ~5× slower in central North America compared to more eastern regions, reflecting continent-wide variation in the rate of climate change. We use whole genome sequencing of 259 individuals to assess whether variation in the rate of hybrid zone movement is reflected in patterns of hybridization and introgression, and which genes and genomic regions show consistently restricted introgression in distinct ecological contexts. Our results highlight substantial similarities between geographically remote transects and reveal large Z-linked chromosomal rearrangements that generate measurable differences in the degree of gene flow between transects. We further use simulations and analyses of climatic data to examine potential factors contributing to continental-scale nuances in selection pressures. We discuss our findings in the context of speciation mechanisms and the importance of sex chromosome inversions in chickadees and other species.},
  archive      = {J_EVLETT},
  author       = {Semenov, Georgy and Kenyon, Haley and Funk, Erik and Anderson, William and McQuillan, Michael and Spinelli, Joan and Russell, Austin and Martinez, Noel and Van Huynh, Alex and Alexander, Alana and Schweizer, Rena and Linck, Ethan and Cheviron, Zachary and Carling, Matt and Roth, Timothy and Robbins, Mark and Rice, Amber and Taylor, Scott},
  doi          = {10.1093/evlett/qraf009},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {421-433},
  shortjournal = {Evol. Lett.},
  title        = {Replicate geographic transects across a hybrid zone reveal parallelism and differences in the genetic architecture of reproductive isolation},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drosophila melanogaster pigmentation demonstrates adaptive phenotypic parallelism over multiple spatiotemporal scales. <em>EVLETT</em>, <em>9</em>(4), 408-420. (<a href='https://doi.org/10.1093/evlett/qraf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Populations are capable of responding to environmental change over ecological timescales via adaptive tracking. However, the translation from patterns of allele frequency change to rapid adaptation of complex traits remains unresolved. We used abdominal pigmentation in Drosophila melanogaster as a model phenotype to address the nature, genetic architecture, and repeatability of rapid adaptation in the field. We show that D. melanogaster pigmentation evolves as a highly parallel and deterministic response to shared environmental variation across latitude and season in natural North American populations. We then experimentally evolved replicate, genetically diverse fly populations in field mesocosms to remove any confounding effects of demography and/or cryptic structure that may drive patterns in wild populations; we show that pigmentation rapidly responds, in parallel, in fewer than 15 generations. Thus, pigmentation evolves concordantly in response to spatial and temporal climatic axes. We next examined whether phenotypic differentiation was associated with allele frequency change at loci with established links to genetic variance in pigmentation in natural populations. We found that across all spatial and temporal scales, phenotypic patterns were associated with variation at pigmentation-related loci, and the sets of genes we identified at each scale were largely nonoverlapping. Therefore, our findings suggest that parallel phenotypic evolution is associated with distinct components of the polygenic architecture shifting across each environmental axis to produce redundant adaptive patterns.},
  archive      = {J_EVLETT},
  author       = {Berardi, Skyler and Rhodes, Jessica A and Berner, Mary Catherine and Greenblum, Sharon I and Bitter, Mark C and Behrman, Emily L and Betancourt, Nicolas J and Bergland, Alan O and Petrov, Dmitri A and Rajpurohit, Subhash and Schmidt, Paul},
  doi          = {10.1093/evlett/qraf008},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {408-420},
  shortjournal = {Evol. Lett.},
  title        = {Drosophila melanogaster pigmentation demonstrates adaptive phenotypic parallelism over multiple spatiotemporal scales},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting fitness in future climates: Insights from temporally replicated field experiments in arabidopsis thaliana. <em>EVLETT</em>, <em>9</em>(4), 392-407. (<a href='https://doi.org/10.1093/evlett/qraf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organisms are already facing climate change. To understand and mitigate the negative effects of climate change on wild and cultivated species, recent research has focused on predicting the fitness of organisms or populations in future climates. The accuracy of these predictions is, however, seldom tested. To test such predictions, we grew a set of 800 genetic families of the annual plant Arabidopsis thaliana in the same field site located in Northern France for 2 consecutive years with contrasted climates. Despite observing, in both years, a clear association between fitness and climatic distance between our field site and the climate of origin of these genetic families, the diverse set of methods we used failed to accurately predict fitness from a year to another. This low accuracy can be explained by the fact that different climatic factors contributed to climate adaptation in different years, which impeded the definition of a meaningful climate descriptor across years. Our results also suggest that populations of A. thaliana from Northern France already suffer from an adaptational lag with respect to climate, and that vegetative growth seems to be a more important trait for climate adaptation than phenology. We discuss the implications of our results for predicting the fitness of wild organisms in future climates and for breeding programs.},
  archive      = {J_EVLETT},
  author       = {Villoutreix, Romain and Faure, Nathalie and Glorieux, Cédric and Roux, Fabrice},
  doi          = {10.1093/evlett/qraf007},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {392-407},
  shortjournal = {Evol. Lett.},
  title        = {Predicting fitness in future climates: Insights from temporally replicated field experiments in arabidopsis thaliana},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics of infection and immunity over 50 years as marine stickleback adapt to freshwater. <em>EVLETT</em>, <em>9</em>(4), 383-391. (<a href='https://doi.org/10.1093/evlett/qraf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a species colonizes a new environment, it may encounter new parasites to which its immune system is poorly adapted. After an initial spike in infection rates in the naïve founder population, the host may subsequently evolve increased immunity, thereby reducing infection rates. Here, we present an example of this eco-evolutionary process in a population of threespine stickleback ( Gasterosteus aculeatus ) that was founded in Heisholt Quarry, a man-made quarry pond, in 1967. Marine stickleback rarely encounter Schistocephalus solidus tapeworms (which require freshwater to hatch), and so remain highly susceptible to infection. Initially, introduced marine fish were heavily infected by S. solidus . They exhibited low levels of fibrosis, a heritable immune trait that some genotypes activate in response to infection, thereby suppressing tapeworm growth and viability. By the 1990s, the Heisholt Quarry population exhibited high rates of fibrosis, which partly suppressed S. solidus infection. This increased immune response led to reduced infection rates, and the tapeworm was apparently extirpated by 2021. Because fibrosis has a strong genetic basis in other stickleback populations, we infer that the newly founded stickleback–parasite interaction exhibits an eco-evolutionary process of increased immunity that effectively reduced infection. The infection and immune dynamics documented here closely match those expected from a simple eco-evo dynamic model presented here.},
  archive      = {J_EVLETT},
  author       = {Sriramulu, Pranav and Schluter, Dolph and Bolnick, Daniel I},
  doi          = {10.1093/evlett/qraf016},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {383-391},
  shortjournal = {Evol. Lett.},
  title        = {Dynamics of infection and immunity over 50 years as marine stickleback adapt to freshwater},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial: Accountability, voice, and trust - Responsible use of GenAI in scientific publishing. <em>EVLETT</em>, <em>9</em>(4), 381-382. (<a href='https://doi.org/10.1093/evlett/qraf027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EVLETT},
  doi          = {10.1093/evlett/qraf027},
  journal      = {Evolution Letters},
  month        = {8},
  number       = {4},
  pages        = {381-382},
  shortjournal = {Evol. Lett.},
  title        = {Editorial: Accountability, voice, and trust - Responsible use of GenAI in scientific publishing},
  volume       = {9},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jrssig">JRSSIG - 17</h2>
<ul>
<li><details>
<summary>
(2025). Q&A: Elizabeth ndjendja. <em>JRSSIG</em>, <em>22</em>(5), 48. (<a href='https://doi.org/10.1093/jrssig/qmaf065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An expert in educational examinations and assessment, Elizabeth obtained her PhD at Rhodes University, South Africa and serves as deputy director in the Directorate of National Examinations and Assessment, in the Ministry of Education, Namibia. Read about her work to overhaul Namibia's exam data management system on page 12},
  archive      = {J_JRSSIG},
  author       = {Britten, Anna},
  doi          = {10.1093/jrssig/qmaf065},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {48},
  shortjournal = {Significance},
  title        = {Q&A: Elizabeth ndjendja},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crossword. <em>JRSSIG</em>, <em>22</em>(5), 47. (<a href='https://doi.org/10.1093/jrssig/qmaf064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSIG},
  author       = {Buttrey, Sam},
  doi          = {10.1093/jrssig/qmaf064},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {47},
  shortjournal = {Significance},
  title        = {Crossword},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Letters. <em>JRSSIG</em>, <em>22</em>(5), 46-47. (<a href='https://doi.org/10.1093/jrssig/qmaf063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSIG},
  doi          = {10.1093/jrssig/qmaf063},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {46-47},
  shortjournal = {Significance},
  title        = {Letters},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review. <em>JRSSIG</em>, <em>22</em>(5), 45. (<a href='https://doi.org/10.1093/jrssig/qmaf062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSIG},
  author       = {King, Tom},
  doi          = {10.1093/jrssig/qmaf062},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {45},
  shortjournal = {Significance},
  title        = {Book review},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bad stats: A regular series exploring slip-ups, snafus and salutary lessons from the world of statistics. <em>JRSSIG</em>, <em>22</em>(5), 42-44. (<a href='https://doi.org/10.1093/jrssig/qmaf061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSIG},
  author       = {Ramos, Mark Louie F},
  doi          = {10.1093/jrssig/qmaf061},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {42-44},
  shortjournal = {Significance},
  title        = {Bad stats: A regular series exploring slip-ups, snafus and salutary lessons from the world of statistics},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The hidden use of statistics. <em>JRSSIG</em>, <em>22</em>(5), 40-41. (<a href='https://doi.org/10.1093/jrssig/qmaf060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The general public grabs statistics from a variety of not necessarily reliable online sources when making big life decisions from baby names to house moves. Johnny Runge considers how the official stats world should feel about that},
  archive      = {J_JRSSIG},
  author       = {Runge, Johnny},
  doi          = {10.1093/jrssig/qmaf060},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {40-41},
  shortjournal = {Significance},
  title        = {The hidden use of statistics},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Where is africa in the AI conversation?. <em>JRSSIG</em>, <em>22</em>(5), 36-38. (<a href='https://doi.org/10.1093/jrssig/qmaf059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While wealthier continents get excited about ChatGPT et al ., many bright minds in Africa can only watch in frustration from the sidelines. Alex Mirugwe looks at what needs to happen now},
  archive      = {J_JRSSIG},
  author       = {Mirugwe, Alex},
  doi          = {10.1093/jrssig/qmaf059},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {36-38},
  shortjournal = {Significance},
  title        = {Where is africa in the AI conversation?},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interview: Tinfissi-joseph ilboudo. <em>JRSSIG</em>, <em>22</em>(5), 33-35. (<a href='https://doi.org/10.1093/jrssig/qmaf058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do you modernise national statistical systems across a whole continent? You need skilled people, reliable technology and an optimistic attitude towards reduced donor funding, Tinfissi-Joseph Ilboudo, head of the African Centre for Statistics (UNECA) in Addis Ababa, Ethiopia, tells Sandra Alba},
  archive      = {J_JRSSIG},
  author       = {Alba, Sandra},
  doi          = {10.1093/jrssig/qmaf058},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {33-35},
  shortjournal = {Significance},
  title        = {Interview: Tinfissi-joseph ilboudo},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interview: Muhammed semakula. <em>JRSSIG</em>, <em>22</em>(5), 30-32. (<a href='https://doi.org/10.1093/jrssig/qmaf057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Muhammed Semakula is a medical statistician and permanent secretary in the Ministry of Health in Rwanda. He tells Sandra Alba about his work on anti-epidemic robots, maternal mortality and a platform offering a real-time picture of his country's health},
  archive      = {J_JRSSIG},
  author       = {Alba, Sandra},
  doi          = {10.1093/jrssig/qmaf057},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {30-32},
  shortjournal = {Significance},
  title        = {Interview: Muhammed semakula},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). People power: Citizen data and policy change in the global south. <em>JRSSIG</em>, <em>22</em>(5), 26-29. (<a href='https://doi.org/10.1093/jrssig/qmaf056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-gathering apps help Ghanaians and Kenyans understand the extent of problems from gender-based violence to health to waste management – and, although findings may not always be truly representative, they are essential tools for equitable policy-making, say Karen Bett , Fredy Rodriguez and Victor Ohuruogu},
  archive      = {J_JRSSIG},
  author       = {Bett, Karen and Rodriguez, Fredy and Ohuruogu, Victor},
  doi          = {10.1093/jrssig/qmaf056},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {26-29},
  shortjournal = {Significance},
  title        = {People power: Citizen data and policy change in the global south},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deforestation decoded: Data science and decision-making in cameroon. <em>JRSSIG</em>, <em>22</em>(5), 20-25. (<a href='https://doi.org/10.1093/jrssig/qmaf055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Congo Basin, the second largest forest in the world, is vanishing. Amandine Debus examines how satellite data could help affected countries understand and address the root causes},
  archive      = {J_JRSSIG},
  author       = {Debus, Amandine},
  doi          = {10.1093/jrssig/qmaf055},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {20-25},
  shortjournal = {Significance},
  title        = {Deforestation decoded: Data science and decision-making in cameroon},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring need in mali: Bridging the social protection data gap. <em>JRSSIG</em>, <em>22</em>(5), 16-19. (<a href='https://doi.org/10.1093/jrssig/qmaf054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Mali, one of the largest countries of the African continent, social protection programmes are a lifeline for vulnerable people. However, data-informed policy-making can be tough. Enter a digital game changer built despite countless practical obstacles – Jeeveeta Soobarah Agnihotri takes us behind the scenes},
  archive      = {J_JRSSIG},
  author       = {Agnihotri, Jeeveeta Soobarah},
  doi          = {10.1093/jrssig/qmaf054},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {16-19},
  shortjournal = {Significance},
  title        = {Measuring need in mali: Bridging the social protection data gap},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). No learner left behind: Modernising examination data in namibia. <em>JRSSIG</em>, <em>22</em>(5), 12-15. (<a href='https://doi.org/10.1093/jrssig/qmaf053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As African education systems embrace digital transformation, Namibia is one of the African countries setting a benchmark by modernising its examination data management processes, explain Elizabeth Ndjendja and Karl Turnbull},
  archive      = {J_JRSSIG},
  author       = {Ndjendja, Elizabeth and Turnbull, Karl},
  doi          = {10.1093/jrssig/qmaf053},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {12-15},
  shortjournal = {Significance},
  title        = {No learner left behind: Modernising examination data in namibia},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hungry for data: How agricultural data can tackle food insecurity. <em>JRSSIG</em>, <em>22</em>(5), 6-11. (<a href='https://doi.org/10.1093/jrssig/qmaf052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millions face food insecurity globally, with the highest proportion of people facing hunger in Africa. James Henderson and Victor Ohuruogu report on why data is the key to ensuring resources reach those in need and policies are evidence-based and effective},
  archive      = {J_JRSSIG},
  author       = {Henderson, James and Ohuruogu, Victor},
  doi          = {10.1093/jrssig/qmaf052},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {6-11},
  shortjournal = {Significance},
  title        = {Hungry for data: How agricultural data can tackle food insecurity},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial. <em>JRSSIG</em>, <em>22</em>(5), 5. (<a href='https://doi.org/10.1093/jrssig/qmaf051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSIG},
  author       = {Britten, Anna},
  doi          = {10.1093/jrssig/qmaf051},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {5},
  shortjournal = {Significance},
  title        = {Editorial},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Puzzle. <em>JRSSIG</em>, <em>22</em>(5), 4. (<a href='https://doi.org/10.1093/jrssig/qmaf050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSIG},
  author       = {Fletcher, Michael},
  doi          = {10.1093/jrssig/qmaf050},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {4},
  shortjournal = {Significance},
  title        = {Puzzle},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). News. <em>JRSSIG</em>, <em>22</em>(5), 2-3. (<a href='https://doi.org/10.1093/jrssig/qmaf049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSIG},
  author       = {Britten, Anna},
  doi          = {10.1093/jrssig/qmaf049},
  journal      = {Significance},
  month        = {9},
  number       = {5},
  pages        = {2-3},
  shortjournal = {Significance},
  title        = {News},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jrsssa">JRSSSA - 36</h2>
<ul>
<li><details>
<summary>
(2025). Correction to: Ranking, and other properties, of elite swimmers using extreme value theory. <em>JRSSSA</em>, <em>188</em>(3), 960. (<a href='https://doi.org/10.1093/jrsssa/qnae154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  doi          = {10.1093/jrsssa/qnae154},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {960},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Correction to: Ranking, and other properties, of elite swimmers using extreme value theory},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational aspects of psychometric methods with r. <em>JRSSSA</em>, <em>188</em>(3), 958. (<a href='https://doi.org/10.1093/jrsssa/qnae112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {McCulloch, Andrew},
  doi          = {10.1093/jrsssa/qnae112},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {958},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Computational aspects of psychometric methods with r},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demand forecasting for executives and professionals. <em>JRSSSA</em>, <em>188</em>(3), 958-959. (<a href='https://doi.org/10.1093/jrsssa/qnaf024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Bhat, Smita},
  doi          = {10.1093/jrsssa/qnaf024},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {958-959},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Demand forecasting for executives and professionals},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ANOVA and mixed models: A short introduction using r. <em>JRSSSA</em>, <em>188</em>(3), 957. (<a href='https://doi.org/10.1093/jrsssa/qnae110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Addy, John W G},
  doi          = {10.1093/jrsssa/qnae110},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {957},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {ANOVA and mixed models: A short introduction using r},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alan geoffrey hawkes, 1938–2023. <em>JRSSSA</em>, <em>188</em>(3), 954-956. (<a href='https://doi.org/10.1093/jrsssa/qnaf064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Sykes, Alan and Watkins, Alan},
  doi          = {10.1093/jrsssa/qnaf064},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {954-956},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Alan geoffrey hawkes, 1938–2023},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ian pieter schagen 1947–2025. <em>JRSSSA</em>, <em>188</em>(3), 953-954. (<a href='https://doi.org/10.1093/jrsssa/qnaf038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Hutchison, Dougal},
  doi          = {10.1093/jrsssa/qnaf038},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {953-954},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Ian pieter schagen 1947–2025},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impacts of innovation school system in korea: A latent space item response model with Neyman–Scott point process. <em>JRSSSA</em>, <em>188</em>(3), 935-952. (<a href='https://doi.org/10.1093/jrsssa/qnae087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {South Korea’s educational system has faced criticism for its lack of focus on critical thinking and creativity, resulting in high levels of stress and anxiety among students. As part of the government’s effort to improve the educational system, the innovation school system was introduced in 2009, which aims to develop students’ creativity as well as their non-cognitive skills. To better understand the differences between innovation and regular school systems in South Korea, we propose a novel method that combines the latent space item response model with the Neyman–Scott point process model. Our method accounts for the heterogeneity of items and students, captures relationships between respondents and items, and identifies item and student clusters that can provide a comprehensive understanding of students’ behaviours/perceptions on non-cognitive outcomes. Our analysis reveals that students in the innovation school system show a higher sense of citizenship, while those in the regular school system tend to associate confidence in appearance with social ability. A comparison with exploratory item factor analysis highlights our method’s advantages in terms of uncertainty quantification of the clustering process and more detailed and nuanced clustering results. Our method is made available to an existing R package, lsirm12pl.},
  archive      = {J_JRSSSA},
  author       = {Yi, Seorim and Kim, Minkyu and Park, Jaewoo and Jeon, Minjeong and Jin, Ick Hoon},
  doi          = {10.1093/jrsssa/qnae087},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {935-952},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Impacts of innovation school system in korea: A latent space item response model with Neyman–Scott point process},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal risk-assessment scheduling for primary prevention of cardiovascular disease. <em>JRSSSA</em>, <em>188</em>(3), 920-934. (<a href='https://doi.org/10.1093/jrsssa/qnae086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a personalized and age-specific net benefit function, composed of benefits and costs, to recommend optimal timing of risk assessments for cardiovascular disease (CVD) prevention. We extend the 2-stage landmarking model to estimate patient-specific CVD risk profiles, adjusting for time-varying covariates. We apply our model to data from the Clinical Practice Research Datalink, comprising primary care electronic health records from the UK. We find that people at lower risk could be recommended an optimal risk-assessment interval of 5 years or more. Time-varying risk factors are required to discriminate between more frequent schedules for high-risk people.},
  archive      = {J_JRSSSA},
  author       = {Gasperoni, Francesca and Jackson, Christopher H and Wood, Angela M and Sweeting, Michael J and Newcombe, Paul J and Stevens, David and Barrett, Jessica K},
  doi          = {10.1093/jrsssa/qnae086},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {920-934},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Optimal risk-assessment scheduling for primary prevention of cardiovascular disease},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework for regression with mismatched data based on mixture modelling. <em>JRSSSA</em>, <em>188</em>(3), 896-919. (<a href='https://doi.org/10.1093/jrsssa/qnae083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of the information age has revolutionized data collection and has led to a rapid expansion of available data sources. Methods of data integration are indispensable when a question of interest cannot be addressed using a single data source. Record linkage (RL) is at the forefront of such data integration efforts. Incentives for sharing linked data for secondary analysis have prompted the need for methodology accounting for possible errors at the RL stage. Mismatch error is a common consequence resulting from the use of nonunique or noisy identifiers at that stage. In this paper, we present a framework to enable valid postlinkage inference in the secondary analysis setting in which only the linked file is given. The proposed framework covers a variety of statistical models and can flexibly incorporate information about the underlying RL process. We propose a mixture model for linked records whose two components reflect distributions conditional on match status, i.e. correct or false match. Regarding inference, we develop a method based on composite likelihood and the expectation-maximization algorithm that is implemented in the R package pldamixture . Extensive simulations and case studies involving contemporary RL applications corroborate the effectiveness of our framework.},
  archive      = {J_JRSSSA},
  author       = {Slawski, Martin and West, Brady T and Bukke, Priyanjali and Wang, Zhenbang and Diao, Guoqing and Ben-David, Emanuel},
  doi          = {10.1093/jrsssa/qnae083},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {896-919},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {A general framework for regression with mismatched data based on mixture modelling},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kpop: A kernel balancing approach for reducing specification assumptions in survey weighting. <em>JRSSSA</em>, <em>188</em>(3), 875-895. (<a href='https://doi.org/10.1093/jrsssa/qnae082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the precipitous decline in response rates, researchers and pollsters have been left with highly nonrepresentative samples, relying on constructed weights to make these samples representative of the desired target population. Though practitioners employ valuable expert knowledge to choose what variables X must be adjusted for, they rarely defend particular functional forms relating these variables to the response process or the outcome. Unfortunately, commonly used calibration weights—which make the weighted mean of X in the sample equal that of the population—only ensure correct adjustment when the portion of the outcome and the response process left unexplained by linear functions of X are independent. To alleviate this functional form dependency, we describe kernel balancing for population weighting ( kpop ). This approach replaces the design matrix X with a kernel matrix, K encoding high-order information about X ⁠ . Weights are then found to make the weighted average row of K among sampled units approximately equal to that of the target population. This produces good calibration on a wide range of smooth functions of X , without relying on the user to decide which X or what functions of them to include. We describe the method and illustrate it by application to polling data from the 2016 US presidential election.},
  archive      = {J_JRSSSA},
  author       = {Hartman, Erin and Hazlett, Chad and Sterbenz, Ciara},
  doi          = {10.1093/jrsssa/qnae082},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {875-895},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Kpop: A kernel balancing approach for reducing specification assumptions in survey weighting},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mapping socio-economic status using mixed data: A hierarchical bayesian approach. <em>JRSSSA</em>, <em>188</em>(3), 859-874. (<a href='https://doi.org/10.1093/jrsssa/qnae080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian hierarchical model to estimate a socio-economic status (SES) index based on mixed dichotomous and continuous variables. In particular, we extend Quinn’s ([2004]. Bayesian factor analysis for mixed ordinal and continuous responses. Political Analysis , 12 (4), 338–353. https://doi.org/10.1093/pan/mph022 ) and Schliep and Hoeting’s ([2013]. Multilevel latent Gaussian process model for mixed discrete and continuous multivariate response data. Journal of Agricultural, Biological, and Environmental Statistics , 18 (4), 492–513. https://doi.org/10.1007/s13253-013-0136-z ) factor analysis models for mixed dichotomous and continuous variables by allowing a spatial hierarchical structure of key parameters of the model. Unlike most SES assessment models proposed in the literature, the hierarchical nature of this model enables the use of census observations at the household level without needing to aggregate any information a priori . Therefore, it better accommodates the variability of the SES between census tracts and the number of households per area. The proposed model is used in the estimation of a socio-economic index using 10% of the 2010 Ghana census in the Greater Accra Metropolitan area. Out of the 20 observed variables, the number of people per room, access to water piping and flushable toilets differentiated high and low SES areas the best.},
  archive      = {J_JRSSSA},
  author       = {Virgili-Gervais, Gabrielle and Schmidt, Alexandra M and Bixby, Honor and Cavanaugh, Alicia and Owusu, George and Agyei-Mensah, Samuel and Robinson, Brian and Baumgartner, Jill},
  doi          = {10.1093/jrsssa/qnae080},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {859-874},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Mapping socio-economic status using mixed data: A hierarchical bayesian approach},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian spatial–temporal varying coefficients model for estimating excess deaths associated with respiratory infections. <em>JRSSSA</em>, <em>188</em>(3), 843-858. (<a href='https://doi.org/10.1093/jrsssa/qnae079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease surveillance data are used for monitoring and understanding disease burden, which provides valuable information in allocating health programme resources. Statistical methods play an important role in estimating disease burden since disease surveillance systems are prone to undercounting. This paper is motivated by the challenge of estimating mortality associated with respiratory infections (e.g. influenza and COVID-19) that are not ascertained from death certificates. We propose a Bayesian spatial–temporal model incorporating measures of infection activity to estimate excess deaths. Particularly, the inclusion of time-varying coefficients allows us to better characterize associations between infection activity and mortality counts time series. Software to implement this method is available in the R package NBRegAD . Applying our modelling framework to weekly state-wide COVID-19 data in the US from 8 March 2020 to 3 July 2022, we identified temporal and spatial differences in excess deaths between different age groups. We estimated the total number of COVID-19 deaths in the US to be 1,168,481 (95% CI: 1,148,953 1,187,187) compared to the 1,022,147 from using only death certificate information. The analysis also suggests that the most severe undercounting was in the 18–49 years age group with an estimated underascertainment rate of 0.21 (95% CI: 0.16, 0.25).},
  archive      = {J_JRSSSA},
  author       = {Zhang, Yuzi and Chang, Howard H and Iuliano, Angela D and Reed, Carrie},
  doi          = {10.1093/jrsssa/qnae079},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {843-858},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {A bayesian spatial–temporal varying coefficients model for estimating excess deaths associated with respiratory infections},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The heterogeneous response of real estate prices during the covid-19 pandemic. <em>JRSSSA</em>, <em>188</em>(3), 819-842. (<a href='https://doi.org/10.1093/jrsssa/qnae078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We estimate the transmission of the pandemic shock in 2020 to the residential and commercial real estate market by causal machine learning, using granular data for Germany. We exploit differences in the incidence of Covid infections and short-time work at the municipal level for the identification of epidemiological and economic effects of the pandemic. We find that (i) a larger incidence of Covid infections temporarily reduced rents for retail real estate; (ii) a larger incidence of short-time work temporarily reduced rents of office real estate; (iii) the pandemic increased prices, particularly in the top price segment of commercial real estate.},
  archive      = {J_JRSSSA},
  author       = {Heiniger, Sandro and Koeniger, Winfried and Lechner, Michael},
  doi          = {10.1093/jrsssa/qnae078},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {819-842},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {The heterogeneous response of real estate prices during the covid-19 pandemic},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal estimators for incorporating external controls in randomized trials with longitudinal outcomes. <em>JRSSSA</em>, <em>188</em>(3), 791-818. (<a href='https://doi.org/10.1093/jrsssa/qnae075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating external data, such as external controls, holds the promise of improving the efficiency of traditional randomized controlled trials especially when treating rare diseases or diseases with unmet needs. To this end, we propose novel weighting estimators grounded in the causal inference framework. As an alternative framework, Bayesian methods are also discussed. From trial design perspective, operating characteristics including Type I error and power are particularly important and are assessed in our realistic simulation studies representing a variety of practical scenarios. Our proposed weighting estimators achieve significant power gain, while maintaining Type I error close to the nominal value of 0.05. An empirical application of the methods is demonstrated through a Phase III clinical trial in rare disease.},
  archive      = {J_JRSSSA},
  author       = {Zhou, Xiner and Zhu, Jiawen and Drake, Christiana and Pang, Herbert},
  doi          = {10.1093/jrsssa/qnae075},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {791-818},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Causal estimators for incorporating external controls in randomized trials with longitudinal outcomes},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving instrumental variable estimators with poststratification. <em>JRSSSA</em>, <em>188</em>(3), 765-790. (<a href='https://doi.org/10.1093/jrsssa/qnae073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experiments studying get-out-the-vote (GOTV) efforts estimate causal effects of various mobilization efforts on voter turnout. However, there is often substantial noncompliance in these studies. A usual approach is to use an instrumental variable (IV) analysis to estimate impacts for compliers, here being those actually contacted by the investigators. Unfortunately, popular IV estimators can be unstable in studies with a small fraction of compliers. We explore poststratifying the data (e.g. taking a weighted average of IV estimates within each stratum) using variables that predict complier status (and, potentially, the outcome) to mitigate this. We present the benefits of poststratification in terms of bias, variance, and improved standard error estimates, and provide a finite-sample asymptotic variance formula. We also compare the performance of different IV approaches and discuss the advantages of our design-based poststratification approach over incorporating compliance-predictive covariates into the two-stage least squares (2SLS) estimator. In the end, we show that covariates predictive of compliance can increase precision, but only if one is willing to make a bias-variance trade-off by down-weighting or dropping strata with few compliers. By contrast, standard approaches such as 2SLS fail to use such information. We finally examine the benefits of our approach in two GOTV applications.},
  archive      = {J_JRSSSA},
  author       = {Pashley, Nicole E and Keele, Luke and Miratrix, Luke W},
  doi          = {10.1093/jrsssa/qnae073},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {765-790},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Improving instrumental variable estimators with poststratification},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Receiver operating characteristic analysis for paired comparison data. <em>JRSSSA</em>, <em>188</em>(3), 741-764. (<a href='https://doi.org/10.1093/jrsssa/qnae072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paired comparison models are used for analysing data that involves pairwise comparisons among a set of objects. When the outcomes of the pairwise comparisons have no ties, the paired comparison models can be generalized as a class of binary response models. Receiver operating characteristic (ROC) curves and their corresponding areas under the curves are commonly used as performance metrics to evaluate the discriminating ability of binary response models. Despite their individual wide range of usage and their close connection to binary response models, ROC analysis to our knowledge has never been extended to paired comparison models since the problem of using different objects as the reference in paired comparison models prevents traditional ROC approach from generating unambiguous and interpretable curves. We focus on addressing this problem by proposing two novel methods to construct ROC curves for paired comparison data which provide interpretable statistics and maintain desired asymptotic properties. The methods are then applied and analysed on head-to-head professional sports competition data.},
  archive      = {J_JRSSSA},
  author       = {Huo, Ran and Glickman, Mark E},
  doi          = {10.1093/jrsssa/qnae072},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {741-764},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Receiver operating characteristic analysis for paired comparison data},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Authors’ reply to the discussion of ‘Extreme-value modelling of migratory bird arrival dates: Insights from citizen-science data’. <em>JRSSSA</em>, <em>188</em>(3), 730-740. (<a href='https://doi.org/10.1093/jrsssa/qnaf058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We respond to the discussion comments of the Proposer and Seconder of the vote of thanks, and to the eight other contributions discussing our work.},
  archive      = {J_JRSSSA},
  author       = {Koh, Jonathan and Opitz, Thomas},
  doi          = {10.1093/jrsssa/qnaf058},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {730-740},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Authors’ reply to the discussion of ‘Extreme-value modelling of migratory bird arrival dates: Insights from citizen-science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Authors’ reply to the discussion of ‘Frequentist prediction sets for species abundance using indirect information’ at the ‘Discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 727-729. (<a href='https://doi.org/10.1093/jrsssa/qnaf040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Bersson, Elizabeth and Hoff, Peter D},
  doi          = {10.1093/jrsssa/qnaf040},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {727-729},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Authors’ reply to the discussion of ‘Frequentist prediction sets for species abundance using indirect information’ at the ‘Discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Authors’ reply to the discussion of ‘Efficient statistical inference methods for assessing changes in species’ populations using citizen science data’ at the ‘Discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 722-727. (<a href='https://doi.org/10.1093/jrsssa/qnaf041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Dennis, Emily B and Diana, Alex and Matechou, Eleni and Morgan, Byron J T},
  doi          = {10.1093/jrsssa/qnaf041},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {722-727},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Authors’ reply to the discussion of ‘Efficient statistical inference methods for assessing changes in species’ populations using citizen science data’ at the ‘Discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maozai tian’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 720-722. (<a href='https://doi.org/10.1093/jrsssa/qnaf021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Tian, Maozai},
  doi          = {10.1093/jrsssa/qnaf021},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {720-722},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Maozai tian’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stefano rizzelli’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 719-720. (<a href='https://doi.org/10.1093/jrsssa/qnaf020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Rizzelli, Stefano},
  doi          = {10.1093/jrsssa/qnaf020},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {719-720},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Stefano rizzelli’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Allan reese’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 718. (<a href='https://doi.org/10.1093/jrsssa/qnaf019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Reese, R Allan},
  doi          = {10.1093/jrsssa/qnaf019},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {718},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Allan reese’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subhash lele’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 717. (<a href='https://doi.org/10.1093/jrsssa/qnaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Lele, Subhash},
  doi          = {10.1093/jrsssa/qnaf015},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {717},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Subhash lele’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kuldeep kumar’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 716. (<a href='https://doi.org/10.1093/jrsssa/qnaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Kumar, Kuldeep},
  doi          = {10.1093/jrsssa/qnaf014},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {716},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Kuldeep kumar’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Raphaël huser and andrew zammit-mangion’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 714-716. (<a href='https://doi.org/10.1093/jrsssa/qnaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Huser, Raphaël and Zammit-Mangion, Andrew},
  doi          = {10.1093/jrsssa/qnaf012},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {714-716},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Raphaël huser and andrew zammit-mangion’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dani gamerman’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 713-714. (<a href='https://doi.org/10.1093/jrsssa/qnaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Gamerman, Dani},
  doi          = {10.1093/jrsssa/qnaf010},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {713-714},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Dani gamerman’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Abdelaati daouia and gilles stupfler’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 712-713. (<a href='https://doi.org/10.1093/jrsssa/qnaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Daouia, Abdelaati and Stupfler, Gilles},
  doi          = {10.1093/jrsssa/qnaf018},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {712-713},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Abdelaati daouia and gilles stupfler’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Léo r. belzile and rishikesh yadav’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 711-712. (<a href='https://doi.org/10.1093/jrsssa/qnaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Belzile, Léo R and Yadav, Rishikesh},
  doi          = {10.1093/jrsssa/qnaf016},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {711-712},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Léo r. belzile and rishikesh yadav’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jafet belmont, sara martino, janine illian, and håvard rue’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 708-710. (<a href='https://doi.org/10.1093/jrsssa/qnaf022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Belmont, Jafet and Martino, Sara and Illian, Janine and Rue, Håvard},
  doi          = {10.1093/jrsssa/qnaf022},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {708-710},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Jafet belmont, sara martino, janine illian, and håvard rue’s contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benjamin r. baer’s contribution to the discussion of ‘the discussion meeting on analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 706-708. (<a href='https://doi.org/10.1093/jrsssa/qnaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Baer, Benjamin R},
  doi          = {10.1093/jrsssa/qnaf011},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {706-708},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Benjamin r. baer’s contribution to the discussion of ‘the discussion meeting on analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Andrej srakar’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 705-706. (<a href='https://doi.org/10.1093/jrsssa/qnaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Srakar, Andrej},
  doi          = {10.1093/jrsssa/qnaf017},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {705-706},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Andrej srakar’s contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seconder of the vote of thanks and contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 703-705. (<a href='https://doi.org/10.1093/jrsssa/qnaf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Mengersen, Kerrie},
  doi          = {10.1093/jrsssa/qnaf013},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {703-705},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Seconder of the vote of thanks and contribution to the discussion of ‘the discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proposer of the vote of thanks and contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’. <em>JRSSSA</em>, <em>188</em>(3), 700-703. (<a href='https://doi.org/10.1093/jrsssa/qnaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSA},
  author       = {Swallow, Ben},
  doi          = {10.1093/jrsssa/qnaf009},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {700-703},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Proposer of the vote of thanks and contribution to the discussion of the ‘Discussion meeting on the analysis of citizen science data’},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extreme-value modelling of migratory bird arrival dates: Insights from citizen science data. <em>JRSSSA</em>, <em>188</em>(3), 674-699. (<a href='https://doi.org/10.1093/jrsssa/qnae108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Citizen science mobilizes many observers and gathers huge datasets but often without strict sampling protocols, resulting in observation biases due to heterogeneous sampling effort, which can lead to biased predictions. We develop a spatio-temporal Bayesian hierarchical model for bias-corrected estimation of arrival dates of the first migratory bird individuals at their breeding sites. Higher sampling effort could be correlated with earlier observed dates. We implement data fusion of two citizen-science datasets with fundamentally different protocols (Breeding Bird Survey, eBird) and obtain posterior distributions of the latent process, which contains four spatial components endowed with Gaussian process priors: species niche; sampling effort; position and scale parameters of annual first arrival date. The data layer consists of four response variables: counts of observed eBird locations (Poisson); presence–absence at observed eBird locations (Binomial); BBS occurrence counts (Poisson); first arrival dates (generalized extreme-value). We devise a Markov chain Monte Carlo scheme and check by simulation that the latent process components are identifiable. We apply our model to several migratory bird species in the northeastern US for 2001–2021 and find that the sampling effort significantly modulates the observed first arrival dates. We exploit this relationship to effectively bias-correct predictions of the true first arrivals.},
  archive      = {J_JRSSSA},
  author       = {Koh, Jonathan and Opitz, Thomas},
  doi          = {10.1093/jrsssa/qnae108},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {674-699},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Extreme-value modelling of migratory bird arrival dates: Insights from citizen science data},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequentist prediction sets for species abundance using indirect information. <em>JRSSSA</em>, <em>188</em>(3), 658-673. (<a href='https://doi.org/10.1093/jrsssa/qnae096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Citizen science databases that consist of volunteer-led sampling efforts of species communities are relied on as essential sources of data in ecology. Summarizing such data across counties with frequentist-valid prediction sets for each county provides an interpretable comparison across counties of varying size or composition. As citizen science data often feature unequal sampling efforts across a spatial domain, prediction sets constructed with indirect methods that share information across counties may be used to improve precision. In this article, we present a nonparametric framework to obtain precise prediction sets for a multinomial random sample based on indirect information that maintain frequentist coverage for each county. We detail a simple algorithm to obtain prediction sets for each county using indirect information where the computation time does not depend on the sample size and scales nicely with the number of species considered. The indirect information may be estimated by a proposed empirical Bayes procedure based on information from auxiliary data. Our approach makes inference for under-sampled counties more precise, while maintaining area-specific frequentist validity for each county. Our method is used to provide a useful description of avian species abundance in North Carolina, USA based on citizen science data from the eBird database.},
  archive      = {J_JRSSSA},
  author       = {Bersson, Elizabeth and Hoff, Peter D},
  doi          = {10.1093/jrsssa/qnae096},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {658-673},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Frequentist prediction sets for species abundance using indirect information},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient statistical inference methods for assessing changes in species’ populations using citizen science data. <em>JRSSSA</em>, <em>188</em>(3), 641-657. (<a href='https://doi.org/10.1093/jrsssa/qnae105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global decline of biodiversity, driven by habitat degradation and climate breakdown, is a significant concern. Accurate measures of change are crucial to provide reliable evidence of species’ population changes. Meanwhile citizen science data have witnessed a remarkable expansion in both quantity and sources and serve as the foundation for assessing species’ status. The growing data reservoir presents opportunities for novel and improved inference but often comes with computational costs: computational efficiency is paramount, especially as regular analysis updates are necessary. Building upon recent research, we present illustrations of computationally efficient methods for fitting new models, applied to three major citizen science data sets for butterflies. We extend a method for modelling abundance changes of seasonal organisms, firstly to accommodate multiple years of count data efficiently, and secondly for application to counts from a snapshot mass-participation survey. We also present a variational inference approach for fitting occupancy models efficiently to opportunistic citizen science data. The continuous growth of citizen science data offers unprecedented opportunities to enhance our understanding of how species respond to anthropogenic pressures. Efficient techniques in fitting new models are vital for accurately assessing species’ status, supporting policy-making, setting measurable targets, and enabling effective conservation efforts.},
  archive      = {J_JRSSSA},
  author       = {Dennis, Emily B and Diana, Alex and Matechou, Eleni and Morgan, Byron J T},
  doi          = {10.1093/jrsssa/qnae105},
  journal      = {Journal of the Royal Statistical Society Series a: Statistics in Society},
  month        = {7},
  number       = {3},
  pages        = {641-657},
  shortjournal = {J. R. Stat. Soc. Ser. A},
  title        = {Efficient statistical inference methods for assessing changes in species’ populations using citizen science data},
  volume       = {188},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jrsssb">JRSSSB - 16</h2>
<ul>
<li><details>
<summary>
(2025). Correction to: X-vine models for multivariate extremes. <em>JRSSSB</em>, <em>87</em>(3), 908. (<a href='https://doi.org/10.1093/jrsssb/qkaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf011},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {908},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: X-vine models for multivariate extremes},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust evaluation of longitudinal surrogate markers with censored data. <em>JRSSSB</em>, <em>87</em>(3), 891-907. (<a href='https://doi.org/10.1093/jrsssb/qkae119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of statistical methods to evaluate surrogate markers is an active area of research. In many clinical settings, the surrogate marker is not simply a single measurement but is instead a longitudinal trajectory of measurements over time, e.g. fasting plasma glucose measured every 6 months for 3 years. In general, available methods developed for the single-surrogate setting cannot accommodate a longitudinal surrogate marker. Furthermore, many of the methods have not been developed for use with primary outcomes that are time-to-event outcomes and/or subject to censoring. In this paper, we propose robust methods to evaluate a longitudinal surrogate marker in a censored time-to-event outcome setting. Specifically, we propose a method to define and estimate the proportion of the treatment effect on a censored primary outcome that is explained by the treatment effect on a longitudinal surrogate marker measured up to time t 0 ⁠ . We accommodate both potential censoring of the primary outcome and of the surrogate marker. A simulation study demonstrates a good finite-sample performance of our proposed methods. We illustrate our procedures by examining repeated measures of fasting plasma glucose, a surrogate marker for diabetes diagnosis, using data from the diabetes prevention programme.},
  archive      = {J_JRSSSB},
  author       = {Agniel, Denis and Parast, Layla},
  doi          = {10.1093/jrsssb/qkae119},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {891-907},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robust evaluation of longitudinal surrogate markers with censored data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial effect detection regression for large-scale spatio-temporal covariates. <em>JRSSSB</em>, <em>87</em>(3), 872-890. (<a href='https://doi.org/10.1093/jrsssb/qkae118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a Spatial Effect Detection Regression (SEDR) model to capture the nonlinear and irregular effects of high-dimensional spatio-temporal predictors on a scalar outcome. Specifically, we assume that both the component and the coefficient functions in the SEDR are unknown smooth functions of location and time. This allows us to leverage spatially and temporally correlated information, transforming the curse of dimensionality into a blessing, as confirmed by our theoretical and numerical results. Moreover, we introduce a set of 0–1 regression coefficients to automatically identify the boundaries of the spatial effect, implemented via a novel penalty. A simple iterative algorithm, with explicit forms at each update step, is developed, and we demonstrate that it converges from the initial values given in the paper. Furthermore, we establish the convergence rate and selection consistency of the proposed estimator under various scenarios involving dimensionality and the effect space. Through simulation studies, we thoroughly evaluate the superior performance of our method in terms of bias and empirical efficiency. Finally, we apply the method to analyse and forecast data from environmental monitoring and Alzheimer’s Disease Neuroimaging Initiative study, revealing interesting findings and achieving smaller out-of-sample prediction errors compared to existing methods.},
  archive      = {J_JRSSSB},
  author       = {Zhang, Chenlin and Zhou, Ling and Guo, Bin and Lin, Huazhen},
  doi          = {10.1093/jrsssb/qkae118},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {872-890},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Spatial effect detection regression for large-scale spatio-temporal covariates},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction sets for high-dimensional mixture of experts models. <em>JRSSSB</em>, <em>87</em>(3), 850-871. (<a href='https://doi.org/10.1093/jrsssb/qkae117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large datasets make it possible to build predictive models that can capture heterogenous relationships between the response variable and features. The mixture of high-dimensional linear experts model posits that observations come from a mixture of high-dimensional linear regression models, where the mixture weights are themselves feature-dependent. In this article, we show how to construct valid prediction sets for an ℓ 1 -penalized mixture of experts model in the high-dimensional setting. We make use of a debiasing procedure to account for the bias induced by the penalization and propose a novel strategy for combining intervals to form a prediction set with coverage guarantees in the mixture setting. Synthetic examples and an application to the prediction of critical temperatures of superconducting materials show our method to have reliable practical performance.},
  archive      = {J_JRSSSB},
  author       = {Javanmard, Adel and Shao, Simeng and Bien, Jacob},
  doi          = {10.1093/jrsssb/qkae117},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {850-871},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Prediction sets for high-dimensional mixture of experts models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic modelling of sparse longitudinal data and functional snippets with stochastic differential equations. <em>JRSSSB</em>, <em>87</em>(3), 833-849. (<a href='https://doi.org/10.1093/jrsssb/qkae116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse functional/longitudinal data have attracted widespread interest due to the prevalence of such data in social and life sciences. A prominent scenario where such data are routinely encountered are accelerated longitudinal studies, where subjects are enrolled in the study at a random time and are only tracked for a short amount of time relative to the domain of interest. The statistical analysis of such functional snippets is challenging since information for far-off-diagonal regions of the covariance structure is missing. Our main methodological contribution is to address this challenge by bypassing covariance estimation and instead modelling the underlying process as the solution of a data-adaptive stochastic differential equation. Taking advantage of the interface between Gaussian functional data and stochastic differential equations makes it possible to efficiently reconstruct the target process by estimating its dynamic distribution. The proposed approach allows one to consistently recover forward sample paths from functional snippets at the subject level. We establish the existence and uniqueness of the solution to the proposed data-driven stochastic differential equation and derive rates of convergence for the corresponding estimators. The finite sample performance is demonstrated with simulation studies and functional snippets arising from a growth study and spinal bone mineral density data.},
  archive      = {J_JRSSSB},
  author       = {Zhou, Yidong and Müller, Hans-Georg},
  doi          = {10.1093/jrsssb/qkae116},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {833-849},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Dynamic modelling of sparse longitudinal data and functional snippets with stochastic differential equations},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moving beyond population variable importance: Concept, theory and applications of individual variable importance. <em>JRSSSB</em>, <em>87</em>(3), 816-832. (<a href='https://doi.org/10.1093/jrsssb/qkae115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a non-parametric regression setting, we introduce a novel concept of ‘individual variable importance’, which assesses the relevance of certain covariates to an outcome variable among individuals with specific characteristics. This concept holds practical importance for both risk assessment and association identification. For example, it can represent (i) the usefulness of expensive biomarkers in risk prediction for individuals at a specified baseline risk, or (ii) age-specific associations between physiological indicators. We quantify individual variable importance using a ratio parameter between two conditional mean squared errors. To estimate and infer this parameter, we develop fully non-parametric estimators and establish their asymptotic properties. Our method performs well in simulation studies. Applying our approach to analyse a real dataset reveals a scientifically interesting result: the association between body shape and systolic blood pressure diminishes with increasing age. Our finding aligns with the medical literature based on standard parametric regression techniques, but our approach is more reliable due to its robustness to model misspecification. More importantly, the fully non-parametric nature of our method allows it to be applied in settings with complex relationships between variables, which cannot be correctly characterized by traditional parametric interaction analyses.},
  archive      = {J_JRSSSB},
  author       = {Dai, Guorong and Shao, Lingxuan and Chen, Jinbo},
  doi          = {10.1093/jrsssb/qkae115},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {816-832},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Moving beyond population variable importance: Concept, theory and applications of individual variable importance},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive conformal classification with noisy labels. <em>JRSSSB</em>, <em>87</em>(3), 796-815. (<a href='https://doi.org/10.1093/jrsssb/qkae114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a conformal prediction method for classification tasks that can adapt to random label contamination in the calibration sample, often leading to more informative prediction sets with stronger coverage guarantees compared to existing approaches. This is obtained through a precise characterization of the coverage inflation (or deflation) suffered by standard conformal inferences in the presence of label contamination, which is then made actionable through a new calibration algorithm. Our solution can leverage different modelling assumptions about the contamination process, while requiring no knowledge of the underlying data distribution or of the inner workings of the classification model. The empirical performance of the proposed method is demonstrated through simulations and an application to object classification with the CIFAR-10H image data set.},
  archive      = {J_JRSSSB},
  author       = {Sesia, Matteo and Wang, Y X Rachel and Tong, Xin},
  doi          = {10.1093/jrsssb/qkae114},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {796-815},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Adaptive conformal classification with noisy labels},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable couplings for the random walk metropolis algorithm. <em>JRSSSB</em>, <em>87</em>(3), 772-795. (<a href='https://doi.org/10.1093/jrsssb/qkae113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a recent surge of interest in coupling methods for Markov chain Monte Carlo algorithms: they facilitate convergence quantification and unbiased estimation, while exploiting embarrassingly parallel computing capabilities. Motivated by these, we consider the design and analysis of couplings of the random walk Metropolis algorithm which scale well with the dimension of the target measure. Methodologically, we introduce a low-rank modification of the synchronous coupling that is provably optimally contractive in standard high-dimensional asymptotic regimes. We expose a shortcoming of the reflection coupling, the state of the art at the time of writing, and we propose a modification which mitigates the issue. Our analysis bridges the gap to the optimal scaling literature and builds a framework of asymptotic optimality which may be of independent interest. We illustrate the applicability of our proposed couplings, and the potential for extending our ideas, with various numerical experiments.},
  archive      = {J_JRSSSB},
  author       = {Papp, Tamás P and Sherlock, Chris},
  doi          = {10.1093/jrsssb/qkae113},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {772-795},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Scalable couplings for the random walk metropolis algorithm},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). α-separability and adjustable combination of amplitude and phase model for functional data. <em>JRSSSB</em>, <em>87</em>(3), 746-771. (<a href='https://doi.org/10.1093/jrsssb/qkae112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider separating and joint modelling amplitude and phase variations for functional data in an identifiable manner. To rigorously address this separability issue, we introduce the notion of α-separability upon constructing a family of α -indexed metrics. We bridge α -separability with the uniqueness of Fréchet mean, leading to the proposed adjustable combination of amplitude and phase model. The parameter α allows user-defined modelling emphasis between vertical and horizontal features and provides a novel viewpoint on the identifiability issue. We prove the consistency of the sample Fréchet mean and variance, and the proposed estimators. Our method is illustrated in simulations and COVID-19 infection rate data.},
  archive      = {J_JRSSSB},
  author       = {Wang, Tian and Ding, Jimin},
  doi          = {10.1093/jrsssb/qkae112},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {746-771},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {α-separability and adjustable combination of amplitude and phase model for functional data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust angle-based transfer learning in high dimensions. <em>JRSSSB</em>, <em>87</em>(3), 723-745. (<a href='https://doi.org/10.1093/jrsssb/qkae111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning improves target model performance by leveraging data from related source populations, especially when target data are scarce. This study addresses the challenge of training high-dimensional regression models with limited target data in the presence of heterogeneous source populations. We focus on a practical setting where only parameter estimates of pretrained source models are available, rather than individual-level source data. For a single source model, we propose a novel angle-based transfer learning (angleTL) method that leverages concordance between source and target model parameters. AngleTL adapts to the signal strength of the target model, unifies several benchmark methods, and mitigates negative transfer when between-population heterogeneity is large. We extend angleTL to incorporate multiple source models, accounting for varying levels of relevance among them. Our high-dimensional asymptotic analysis provides insights into when a source model benefits the target model and demonstrates the superiority of angleTL over other methods. Extensive simulations validate these findings and highlight the feasibility of applying angleTL to transfer genetic risk prediction models across multiple biobanks.},
  archive      = {J_JRSSSB},
  author       = {Gu, Tian and Han, Yi and Duan, Rui},
  doi          = {10.1093/jrsssb/qkae111},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {723-745},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robust angle-based transfer learning in high dimensions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequentist inference for semi-mechanistic epidemic models with interventions. <em>JRSSSB</em>, <em>87</em>(3), 701-722. (<a href='https://doi.org/10.1093/jrsssb/qkae110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effect of public health interventions on an epidemic are often estimated by adding the intervention to epidemic models. During the Covid-19 epidemic, numerous papers used such methods for making scenario predictions. The majority of these papers use Bayesian methods to estimate the parameters of the model. In this article, we show how to use frequentist methods for estimating these effects which avoids having to specify prior distributions. We also use model-free shrinkage methods to improve estimation when there are many different geographic regions. This allows us to borrow strength from different regions while still getting confidence intervals with correct coverage and without having to specify a hierarchical model. Throughout, we focus on a semi-mechanistic model which provides a simple, tractable alternative to compartmental methods.},
  archive      = {J_JRSSSB},
  author       = {Bong, Heejong and Ventura, Valérie and Wasserman, Larry},
  doi          = {10.1093/jrsssb/qkae110},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {701-722},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Frequentist inference for semi-mechanistic epidemic models with interventions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal mediation analysis: Selection with asymptotically valid inference. <em>JRSSSB</em>, <em>87</em>(3), 678-700. (<a href='https://doi.org/10.1093/jrsssb/qkae109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers are often interested in learning not only the effect of treatments on outcomes, but also the mechanisms that transmit these effects. A mediator is a variable that is affected by treatment and subsequently affects outcome. Existing methods for penalized mediation analyses may lead to ignoring important mediators and either assume that finite-dimensional linear models are sufficient to remove confounding bias, or perform no confounding control at all. In practice, these assumptions may not hold. We propose a method that considers the confounding functions as nuisance parameters to be estimated using data-adaptive methods. We then use a novel regularization method applied to this objective function to identify a set of important mediators. We consider natural direct and indirect effects as our target parameters. We then proceed to derive the asymptotic properties of our estimators and establish the oracle property under specific assumptions. Asymptotic results are also presented in a local setting, which contrast the proposal with the standard adaptive lasso. We also propose a perturbation bootstrap technique to provide asymptotically valid postselection inference for the mediated effects of interest. The performance of these methods will be discussed and demonstrated through simulation studies.},
  archive      = {J_JRSSSB},
  author       = {Jones, Jeremiah and Ertefaie, Ashkan and Strawderman, Robert L},
  doi          = {10.1093/jrsssb/qkae109},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {678-700},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Causal mediation analysis: Selection with asymptotically valid inference},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Engression: Extrapolation through the lens of distributional regression. <em>JRSSSB</em>, <em>87</em>(3), 653-677. (<a href='https://doi.org/10.1093/jrsssb/qkae108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributional regression aims to estimate the full conditional distribution of a target variable, given covariates. Popular methods include linear and tree ensemble based quantile regression. We propose a neural network-based distributional regression methodology called ‘engression’. An engression model is generative in the sense that we can sample from the fitted conditional distribution and is also suitable for high-dimensional outcomes. Furthermore, we find that modelling the conditional distribution on training data can constrain the fitted function outside of the training support, which offers a new perspective to the challenging extrapolation problem in nonlinear regression. In particular, for ‘preadditive noise’ models, where noise is added to the covariates before applying a nonlinear transformation, we show that engression can successfully perform extrapolation under some assumptions such as monotonicity, whereas traditional regression approaches such as least-squares or quantile regression fall short under the same assumptions. Our empirical results, from both simulated and real data, validate the effectiveness of the engression method. The software implementations of engression are available in both R and Python.},
  archive      = {J_JRSSSB},
  author       = {Shen, Xinwei and Meinshausen, Nicolai},
  doi          = {10.1093/jrsssb/qkae108},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {653-677},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Engression: Extrapolation through the lens of distributional regression},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness, model checking, and hierarchical models. <em>JRSSSB</em>, <em>87</em>(3), 632-652. (<a href='https://doi.org/10.1093/jrsssb/qkae107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model checking is essential to evaluate the adequacy of statistical models and the validity of inferences drawn from them. Particularly, hierarchical models such as latent Gaussian models (LGMs) pose unique challenges as it is difficult to check assumptions on the latent parameters. Diagnostic statistics are often used to quantify the degree to which a model fit deviates from the observed data. We construct diagnostic statistics by (a) defining an alternative model with relaxed assumptions and (b) deriving the diagnostic statistic most sensitive to discrepancies induced by this alternative model. We also promote a workflow for model criticism that combines model checking with subsequent robustness analysis. As a result, we obtain a general recipe to check assumptions in hierarchical models and the impact of these assumptions on the results. We demonstrate the ideas by assessing the latent Gaussianity assumption, a crucial but often overlooked assumption in LGMs. We illustrate the methods via examples utilizing Stan and provide functions for easy usage of the methods for general models fitted through R-INLA.},
  archive      = {J_JRSSSB},
  author       = {Cabral, Rafael and Bolin, David and Rue, Håvard},
  doi          = {10.1093/jrsssb/qkae107},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {632-652},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robustness, model checking, and hierarchical models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive functional principal components analysis. <em>JRSSSB</em>, <em>87</em>(3), 603-631. (<a href='https://doi.org/10.1093/jrsssb/qkae106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis almost always involves smoothing discrete observations into curves, because they are never observed in continuous time and rarely without error. Although smoothing parameters affect the subsequent inference, data-driven methods for selecting these parameters are not well-developed, frustrated by the difficulty of using all the information shared by curves while being computationally efficient. On the one hand, smoothing individual curves in an isolated, albeit sophisticated way, ignores useful signals present in other curves. On the other hand, bandwidth selection by automatic procedures such as cross-validation after pooling all the curves together quickly become computationally unfeasible due to the large number of data points. In this paper, we propose a new data-driven, adaptive kernel smoothing, specifically tailored for functional principal components analysis through the derivation of sharp, explicit risk bounds for the eigen-elements. The minimization of these quadratic risk bounds provides refined, yet computationally efficient bandwidth rules for each eigen-element separately. Both common and independent design cases are allowed. Rates of convergence for the estimators are derived. An extensive simulation study, designed in a versatile manner to closely mimic the characteristics of real data sets supports our methodological contribution. An illustration on a real data application is provided.},
  archive      = {J_JRSSSB},
  author       = {Wang, Sunny G W and Patilea, Valentin and Klutchnikoff, Nicolas},
  doi          = {10.1093/jrsssb/qkae106},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {603-631},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Adaptive functional principal components analysis},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). X-vine models for multivariate extremes. <em>JRSSSB</em>, <em>87</em>(3), 579-602. (<a href='https://doi.org/10.1093/jrsssb/qkae105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular vine sequences permit the organization of variables in a random vector along a sequence of trees. Vine-based dependence models have become greatly popular as a way to combine arbitrary bivariate copulas into higher-dimensional ones, offering flexibility, parsimony, and tractability. In this project, we use regular vine sequences to decompose and construct the exponent measure density of a multivariate extreme value distribution, or, equivalently, the tail copula density. Although these densities pose theoretical challenges due to their infinite mass, their homogeneity property offers simplifications. The theory sheds new light on existing parametric families and facilitates the construction of new ones, called X-vines. Computations proceed via recursive formulas in terms of bivariate model components. We develop simulation algorithms for X-vine multivariate Pareto distributions as well as methods for parameter estimation and model selection on the basis of threshold exceedances. The methods are illustrated by Monte Carlo experiments and a case study on US flight delay data.},
  archive      = {J_JRSSSB},
  author       = {Kiriliouk, Anna and Lee, Jeongjin and Segers, Johan},
  doi          = {10.1093/jrsssb/qkae105},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {579-602},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {X-vine models for multivariate extremes},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="jrsssc">JRSSSC - 12</h2>
<ul>
<li><details>
<summary>
(2025). A multivariate spatial statistical model for statistical downscaling of sea surface temperature in the great barrier reef region. <em>JRSSSC</em>, <em>74</em>(4), 1183-1213. (<a href='https://doi.org/10.1093/jrsssc/qlaf019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a statistical downscaling method to produce fine-resolution climate projections. A multivariate spatial statistical model is developed to jointly analyse high-resolution remote sensing data and coarse-resolution climate model outputs. With a basis function representation, the resulting model can achieve efficient computation and describe potentially nonstationary spatial dependence. We implement our method to produce downscaled sea surface temperature projections over the Great Barrier Reef region from CMIP6 Earth system models. Compared with the state of the art, our method reduces the mean squared predictive error substantially and produces a predictive distribution enabling holistic uncertainty quantification analyses.},
  archive      = {J_JRSSSC},
  author       = {Ekanayaka, Ayesha and Kang, Emily L and Braverman, Amy and Kalmus, Peter},
  doi          = {10.1093/jrsssc/qlaf019},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1183-1213},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {A multivariate spatial statistical model for statistical downscaling of sea surface temperature in the great barrier reef region},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian mortality modelling with pandemics: A vanishing jump approach. <em>JRSSSC</em>, <em>74</em>(4), 1150-1182. (<a href='https://doi.org/10.1093/jrsssc/qlaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends the Lee–Carter (LC) model for single- and multi-populations to account for pandemic jump effects of vanishing kind, allowing for a more comprehensive and accurate representation of mortality rates during a pandemic, characterized by a high impact at the beginning and gradually vanishing effects over subsequent periods. While the LC model is effective in capturing mortality trends, it may not always be able to account for large, unexpected jumps in mortality rates caused by pandemics or wars. Existing models allow either for transient jumps with an effect of one period only or persistent jumps. However, there is no literature on estimating mortality time series with jumps having an effect over a small number of periods, as is typically observed in pandemics. The Bayesian approach allows to quantify the uncertainty around the parameter estimates. Empirical data from the COVID-19 pandemic show the superiority of the proposed approach, compared with models with a transitory shock effect.},
  archive      = {J_JRSSSC},
  author       = {Goes, Julius and Barigou, Karim and Leucht, Anne},
  doi          = {10.1093/jrsssc/qlaf018},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1150-1182},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Bayesian mortality modelling with pandemics: A vanishing jump approach},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical modelling of on-street parking spot occupancy in smart cities. <em>JRSSSC</em>, <em>74</em>(4), 1128-1149. (<a href='https://doi.org/10.1093/jrsssc/qlaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies suggest that searching for parking is associated with significant direct and indirect costs. Therefore, it is appealing to reduce the time that car drivers spend on finding an available parking spot, especially in urban areas where the space for all road users is limited. The prediction of on-street parking spot occupancy can provide drivers with guidance on where clear parking spaces are likely to be found. This field of research has gained more and more attention in the last decade through the increasing availability of real-time parking spot occupancy data. In this paper, we pursue a statistical approach for the prediction of parking spot occupancy, where we make use of time-to-event models and semi-Markov process theory. The latter involves the employment of Laplace transformations as well as their inversion, which is an ambitious numerical task. We apply our methodology to data from the City of Melbourne in Australia. Our main result is that the semi-Markov model outperforms a Markov model in terms of both true negative rate and true positive rate while this is essentially achieved by respecting the current duration that a parking space already spends in its initial state.},
  archive      = {J_JRSSSC},
  author       = {Schneble, Marc and Kauermann, Göran},
  doi          = {10.1093/jrsssc/qlaf017},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1128-1149},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Statistical modelling of on-street parking spot occupancy in smart cities},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A flexible model for record linkage. <em>JRSSSC</em>, <em>74</em>(4), 1100-1127. (<a href='https://doi.org/10.1093/jrsssc/qlaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining data from various sources empowers researchers to explore innovative questions, for example those raised by conducting healthcare monitoring studies. However, the lack of a unique identifier often poses challenges. Record linkage procedures determine whether pairs of observations collected on different occasions belong to the same individual using partially identifying variables (e.g. birth year, postal code). Existing methodologies typically involve a compromise between computational efficiency and accuracy. Traditional approaches simplify this task by condensing information, yet they neglect dependencies among linkage decisions and disregard the one-to-one relationship required to establish coherent links. Modern approaches offer a comprehensive representation of the data generation process, at the expense of computational overhead and reduced flexibility. We propose a flexible method, that adapts to varying data complexities, addressing registration errors and accommodating changes of the identifying information over time. Our approach balances accuracy and scalability, estimating the linkage using a Stochastic Expectation Maximization algorithm on a latent variable model. We illustrate the ability of our methodology to connect observations using large real data applications and demonstrate the robustness of our model to the linking variables quality in a simulation study. The proposed algorithm FlexRL is implemented and available in an open source R package.},
  archive      = {J_JRSSSC},
  author       = {Robach, Kayané and van der Pas, Stéphanie L and van de Wiel, Mark A and Hof, Michel H},
  doi          = {10.1093/jrsssc/qlaf016},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1100-1127},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {A flexible model for record linkage},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven segmentation of observation-level logistic regression models. <em>JRSSSC</em>, <em>74</em>(4), 1077-1099. (<a href='https://doi.org/10.1093/jrsssc/qlaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a data-adaptive method to segment individual observation-based logistic regression models, focusing on motivating binary landslide data. Our method assigns observation-specific regression models and utilizes a grouped fused lasso penalty for data-adaptive model fusion when common regression coefficients are desired. However, when inherent differences persist, the models remain separate, resulting in distinct regression coefficients. To handle the large number of parameters arising from individual observation-based models, we develop a novel alternating direction method of multipliers-based algorithm. Our numerical study demonstrates improved prediction performance over conventional logistic regression models by leveraging heterogeneous data characteristics.},
  archive      = {J_JRSSSC},
  author       = {Choi, Yunjin and Park, No-Wook and Lee, Woojoo},
  doi          = {10.1093/jrsssc/qlaf015},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1077-1099},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Data-driven segmentation of observation-level logistic regression models},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying regions of concomitant compound precipitation and wind speed extremes over europe. <em>JRSSSC</em>, <em>74</em>(4), 1057-1076. (<a href='https://doi.org/10.1093/jrsssc/qlaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of simplifying the complex spatio-temporal variables associated with climate modelling is of utmost importance and comes with significant challenges. In this research, our primary objective is to develop a clustering framework to handle compound extreme events within gridded climate data across Europe. Specifically, we intend to identify subregions that display asymptotic independence between precipitation and wind speed extremes, meaning that occurrences of extreme rain and wind speed in one subregion do not affect those in the other. To achieve this, we utilize daily precipitation sums and daily maximum wind speed data derived from the ERA5 reanalysis dataset spanning from 1979 to 2022. Our approach hinges on a tuning parameter and the application of a divergence measure to spotlight disparities in extremal dependence structures without relying on specific parametric assumptions. We propose a data-driven approach to determine the tuning parameter. This enables us to generate clusters that are spatially concentrated, which can provide more insightful information about the regional distribution of compound precipitation and wind speed extremes. The proposed method is able to extract valuable information about extreme compound events while also significantly reducing the size of the dataset within reasonable computational timeframes.},
  archive      = {J_JRSSSC},
  author       = {Boulin, Alexis and Di Bernardino, Elena and Laloë, Thomas and Toulemonde, Gwladys},
  doi          = {10.1093/jrsssc/qlaf014},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1057-1076},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Identifying regions of concomitant compound precipitation and wind speed extremes over europe},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data fusion model for meteorological data using the INLA-SPDE method. <em>JRSSSC</em>, <em>74</em>(4), 1021-1056. (<a href='https://doi.org/10.1093/jrsssc/qlaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a data fusion model designed to address the problem of sparse observational data by incorporating numerical forecast models as an additional data source to improve predictions of key variables. This model is applied to two main meteorological data sources in the Philippines. The data fusion approach assumes that different data sources are imperfect representations of a common underlying process. Observations from weather stations follow a classical error model, while numerical weather forecasts involve both a constant multiplicative bias and an additive bias, which is spatially structured and time-varying. To perform inference, we use a Bayesian model averaging technique combined with integrated nested Laplace approximation. The model’s performance is evaluated through a simulation study, where it consistently results in better predictions and more accurate parameter estimates than models using only weather stations data or regression calibration, particularly in cases of sparse observational data. In the meteorological data application, the proposed data fusion model also outperforms these benchmark approaches, as demonstrated by leave-group-out cross-validation.},
  archive      = {J_JRSSSC},
  author       = {Villejo, Stephen Jun and Martino, Sara and Lindgren, Finn and Illian, Janine B},
  doi          = {10.1093/jrsssc/qlaf012},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {1021-1056},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {A data fusion model for meteorological data using the INLA-SPDE method},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D-vine generalized additive model copula-based quantile regression with application to ensemble postprocessing. <em>JRSSSC</em>, <em>74</em>(4), 994-1020. (<a href='https://doi.org/10.1093/jrsssc/qlaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The D-vine copula-based quantile regression (DVQR) is a powerful tool for weather forecasting, as it is able to select informative predictor variables from a large set and takes account of nonlinear relationships among them. However, DVQR shows in its current form a lack in adaptively modelling strongly varying effects among variables, such as temporal and/or spatial effects. Consequently, we propose an extension of the current DVQR, where we specify the parameters of the bivariate copulas in the D-vine copula through Kendall’s τ to which additional covariates are linked. The parameterization of the correlation parameter allows generalized additive models (GAMs) to incorporate, e.g. linear, nonlinear, and spatial effects as well as interactions. The new method is called GAM-DVQR, and its performance is illustrated in a case study on postprocessing 2 m surface temperature ensemble weather forecasts. We investigate constant as well as time-dependent Kendall’s τ correlation models. The results indicate that the GAM-DVQR models are able to identify time-dependent correlations and significantly outperform state-of-the-art postprocessing methods. Furthermore, the introduced temporal parameterization allows a more economical and faster model estimation in comparison to DVQR using a sliding training window. To complement this article, we provide an R -package for our method called gamvinereg .},
  archive      = {J_JRSSSC},
  author       = {Jobst, David and Möller, Annette and Groß, Jürgen},
  doi          = {10.1093/jrsssc/qlaf011},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {994-1020},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {D-vine generalized additive model copula-based quantile regression with application to ensemble postprocessing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correcting for bias due to mismeasured exposure in mediation analysis with a survival outcome. <em>JRSSSC</em>, <em>74</em>(4), 969-993. (<a href='https://doi.org/10.1093/jrsssc/qlaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the impact of exposure measurement error on assessing mediation with a survival outcome modelled by Cox regression. We first derive the bias formulas of natural indirect and direct effects with a rare outcome and no exposure–mediator interaction. We then develop several calibration approaches to correct for the measurement error-induced bias, and generalize our methods to accommodate a common outcome and an exposure–mediator interaction. We apply the proposed methods to analyse the Health Professionals Follow-up Study (1986–2016) and evaluate the extent to which reduced body mass index mediates the protective effect of physical activity on risk of cardiovascular diseases.},
  archive      = {J_JRSSSC},
  author       = {Cheng, Chao and Spiegelman, Donna and Li, Fan},
  doi          = {10.1093/jrsssc/qlaf010},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {969-993},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Correcting for bias due to mismeasured exposure in mediation analysis with a survival outcome},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A gaussian sliding windows regression model for hydrological inference. <em>JRSSSC</em>, <em>74</em>(4), 946-968. (<a href='https://doi.org/10.1093/jrsssc/qlaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models are an essential tool to model, forecast, and understand the hydrological processes in watersheds. In particular, the understanding of time lags associated with the delay between rainfall occurrence and subsequent changes in streamflow is of high practical importance. Since water can take a variety of flow paths to generate streamflow, a series of distinct runoff pulses may combine to create the observed streamflow time series. Current state-of-the-art models are not able to sufficiently confront the problem complexity with interpretable parametrization, thus preventing novel insights about the dynamics of distinct flow paths from being formed. The proposed Gaussian Sliding Windows Regression Model targets this problem by combining the concept of multiple windows sliding along the time axis with multiple linear regression. The window kernels, which indicate the weights applied to different time lags, are implemented via Gaussian-shaped kernels. As a result, straightforward process inference can be achieved since each window can represent one flow path. Experiments on simulated and real-world scenarios underline that the proposed model achieves accurate parameter estimates and competitive predictive performance, while fostering explainable and interpretable hydrological modelling.},
  archive      = {J_JRSSSC},
  author       = {Schrunner, Stefan and Pishrobat, Parham and Janssen, Joseph and Jenul, Anna and Cao, Jiguo and Ameli, Ali A and Welch, William J},
  doi          = {10.1093/jrsssc/qlaf009},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {946-968},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {A gaussian sliding windows regression model for hydrological inference},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic self-learning approach to establishing individualized treatment rules from multiple benefit outcomes in a calcium supplementation trial. <em>JRSSSC</em>, <em>74</em>(4), 925-945. (<a href='https://doi.org/10.1093/jrsssc/qlaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In utero lead exposure poses risks to children’s neurobehavioral development. The Early Life Exposure in Mexico to ENvironmental Toxicants’ calcium supplementation trial studies the effect of calcium supplement in reducing maternal lead exposure to infants during pregnancy. An individualized treatment rule (ITR) is needed to guide pregnant women on taking calcium supplement. This article introduces a statistical learning method, synergistic self-learning (SS-learning), to tackle two challenges in deriving ITR with multiple outcomes, including heterogeneous multidimensional outcomes and complex missing data patterns. Applying SS-learning to the trial, important covariates were identified to form an ITR, expected to lead to higher lead reduction if implemented across the study population.},
  archive      = {J_JRSSSC},
  author       = {Zhou, Yiwang and Song, Peter X K},
  doi          = {10.1093/jrsssc/qlaf008},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {925-945},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {Synergistic self-learning approach to establishing individualized treatment rules from multiple benefit outcomes in a calcium supplementation trial},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-resolution urban air quality monitoring from citizen science data with echo-state transformer networks. <em>JRSSSC</em>, <em>74</em>(4), 905-924. (<a href='https://doi.org/10.1093/jrsssc/qlaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Citizen science data for monitoring air pollution have recently emerged as a powerful yet under-explored resource to complement expensive and sparse national air quality monitors. In urban environments, these new data have the potential to allow for high-resolution and high-frequency forecasts, and thereby to provide an assessment of population exposure at neighbourhood level. The complex spatio-temporal structure of these data, however, requires new flexible methods that are also able to provide timely forecasts. In this work, we propose a novel method that first provides forecasts with a reservoir computing approach, an echo-state network, adjusts the forecast with a transformer network with attention mechanism and then merges the echo-state and transformer forecast into a combined network. The stochastic nature of the method allows for a fast and more accurate forecast then individual predictors as well as standard statistical methods. Simulation and application to San Francisco air pollution show how the proposed method is able to produce high-resolution urban maps of air quality. Additionally, we show how these forecasts can be used to provide neighbour-level exposure assessment using population data, a task that would not be achievable with sparse government-sponsored air quality networks.},
  archive      = {J_JRSSSC},
  author       = {Bonas, Matthew and Castruccio, Stefano},
  doi          = {10.1093/jrsssc/qlaf007},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  month        = {11},
  number       = {4},
  pages        = {905-924},
  shortjournal = {J. R. Stat. Soc. Ser. C},
  title        = {High-resolution urban air quality monitoring from citizen science data with echo-state transformer networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

<h2 id="nsr">NSR - 3</h2>
<ul>
<li><details>
<summary>
(2025). The origin and fate of subslab partial melts at convergent margins. <em>NSR</em>, <em>12</em>(10), nwaf314. (<a href='https://doi.org/10.1093/nsr/nwaf314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channelized thin seismic low-velocity zones (LVZs) are observed beneath subducting slabs, extending from the lithosphere-asthenosphere boundary to the mantle transition zone (MTZ). While LVZs above slabs are well-explained by slab dehydration and flux melting, the origin and fate of these enigmatic subslab LVZs—persisting far deeper than expected—remain elusive. Here we use geodynamic modeling to show that subduction-induced wet upwellings from a water-bearing MTZ that generates dehydration melting feed the base of the lithosphere forming seismically detected LVZs. A similar process may occur when a pre-existing partially molten layer atop the 410-km discontinuity is displaced upward in response to subduction. These thin partially molten layers are successively entrained alongside the subducting slab and recycled back to MTZ depths. This mechanism supports a globally widespread scenario where subduction-induced upwelling of a water-rich MTZ can account for the observed mantle heterogeneities. Since minute quantities of melts may dramatically reduce viscosity, this process is likely to have non-negligible implications for the Earth's dynamics and recycling of the lithosphere into the deep mantle.},
  archive      = {J_NSR},
  author       = {Yang, Jianfeng and Faccenda, Manuele and Chen, Ling and Wang, Xin and Shen, Hao and VanderBeek, Brandon P and Zhao, Liang},
  doi          = {10.1093/nsr/nwaf314},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf314},
  shortjournal = {Nat. Sci. Rev.},
  title        = {The origin and fate of subslab partial melts at convergent margins},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid indian ocean warming fuels more frequent extreme pre-flood season rainfall over southern china. <em>NSR</em>, <em>12</em>(10), nwaf298. (<a href='https://doi.org/10.1093/nsr/nwaf298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2024, southern China faced its worst flooding during the pre-flood season (April–June), the first major rainfall season in East Asia, with considerable socioeconomic consequences. This extreme flooding is fueled by the unprecedented warming in the Indian Ocean, with a decaying moderate El Niño in the Pacific contributing weakly. Alarmingly, similar pre-flood season flooding events have become increasingly frequent in southern China over recent decades, posing unexpected risks to local communities. We demonstrate that the recent rapid Indian Ocean warming enhances local convection efficiency, leading to more frequent intense pre-flood season rainfall. As sea surface temperature in the Indian Ocean continues to rise in a warming world, it becomes increasingly crucial to understand its role in shaping regional extreme weather patterns for future climate adaptation and disaster management.},
  archive      = {J_NSR},
  author       = {Hou, Ruiqin and Zhang, Wenjun and Hu, Suqiong and Xu, Rongrong},
  doi          = {10.1093/nsr/nwaf298},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf298},
  shortjournal = {Nat. Sci. Rev.},
  title        = {Rapid indian ocean warming fuels more frequent extreme pre-flood season rainfall over southern china},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medial preoptic CCKAR mediates anxiety and aggression induced by chronic emotional stress in male mice. <em>NSR</em>, <em>12</em>(10), nwaf152. (<a href='https://doi.org/10.1093/nsr/nwaf152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anxiety disorders frequently accompany aggression, with their co-occurrence predicting greater functional impairment and poor prognosis. Nevertheless, the underlying neural mechanisms remain elusive, primarily due to a lack of appropriate animal models. Here, we designed a chronic conspecific outsider stress (CCS) model in which male mice underwent perceived social threats and exhibited increased anxiety-like behaviors accompanied by aggression. CCS led to Fos activation and hyperexcitability of GABAergic neurons in the medial preoptic area (mPOA). Inhibition of mPOA GABAergic (mPOA Gad2 ) neurons rescued CCS-induced anxiety-like and aggressive behaviors, whereas activating these cells induced susceptibility to CCS. Moreover, CCS upregulated the mRNA and protein expression of the sexual-dimorphic gene, cholecystokinin A receptor (CCKAR)-encoding Cckar gene in the mPOA. Importantly, the knock-down and overexpression of CCKAR in the mPOA Gad2 neurons had alleviating and promoting effects on anxiety-like and aggressive behaviors, aligning with decreased and increased excitability by the anxiolytic CCKAR antagonist MK-329 and the anxiogenic CCKAR agonist A71623 in mPOA Gad2 neurons, respectively. Overall, our study characterizes a novel mouse model of anxiety disorders accompanied by aggression and the neuronal subpopulation and molecular mediator of the aberrant behaviors provide potential targets of intervention for anxiety disorders with aggression.},
  archive      = {J_NSR},
  author       = {Tang, Meng-Yu and Zhang, Yan-Yi and Lin, Lin and Wu, Lin-Lin and Hu, Meng-Ting and Tan, Li-Heng and Yu, Chen-Xi and Wang, Hao and Yu, Yan-Qin and Ding, Yu and Han, Jia-Xuan and Hu, Hailan and Li, Xiao-Ming and Lian, Hong},
  doi          = {10.1093/nsr/nwaf152},
  journal      = {National Science Review},
  month        = {10},
  number       = {10},
  pages        = {nwaf152},
  shortjournal = {Nat. Sci. Rev.},
  title        = {Medial preoptic CCKAR mediates anxiety and aggression induced by chronic emotional stress in male mice},
  volume       = {12},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
