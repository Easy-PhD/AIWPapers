<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRSSSB</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrsssb">JRSSSB - 16</h2>
<ul>
<li><details>
<summary>
(2025). Correction to: X-vine models for multivariate extremes. <em>JRSSSB</em>, <em>87</em>(3), 908. (<a href='https://doi.org/10.1093/jrsssb/qkaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf011},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {908},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: X-vine models for multivariate extremes},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust evaluation of longitudinal surrogate markers with censored data. <em>JRSSSB</em>, <em>87</em>(3), 891-907. (<a href='https://doi.org/10.1093/jrsssb/qkae119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of statistical methods to evaluate surrogate markers is an active area of research. In many clinical settings, the surrogate marker is not simply a single measurement but is instead a longitudinal trajectory of measurements over time, e.g. fasting plasma glucose measured every 6 months for 3 years. In general, available methods developed for the single-surrogate setting cannot accommodate a longitudinal surrogate marker. Furthermore, many of the methods have not been developed for use with primary outcomes that are time-to-event outcomes and/or subject to censoring. In this paper, we propose robust methods to evaluate a longitudinal surrogate marker in a censored time-to-event outcome setting. Specifically, we propose a method to define and estimate the proportion of the treatment effect on a censored primary outcome that is explained by the treatment effect on a longitudinal surrogate marker measured up to time t 0 ⁠ . We accommodate both potential censoring of the primary outcome and of the surrogate marker. A simulation study demonstrates a good finite-sample performance of our proposed methods. We illustrate our procedures by examining repeated measures of fasting plasma glucose, a surrogate marker for diabetes diagnosis, using data from the diabetes prevention programme.},
  archive      = {J_JRSSSB},
  author       = {Agniel, Denis and Parast, Layla},
  doi          = {10.1093/jrsssb/qkae119},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {891-907},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robust evaluation of longitudinal surrogate markers with censored data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial effect detection regression for large-scale spatio-temporal covariates. <em>JRSSSB</em>, <em>87</em>(3), 872-890. (<a href='https://doi.org/10.1093/jrsssb/qkae118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a Spatial Effect Detection Regression (SEDR) model to capture the nonlinear and irregular effects of high-dimensional spatio-temporal predictors on a scalar outcome. Specifically, we assume that both the component and the coefficient functions in the SEDR are unknown smooth functions of location and time. This allows us to leverage spatially and temporally correlated information, transforming the curse of dimensionality into a blessing, as confirmed by our theoretical and numerical results. Moreover, we introduce a set of 0–1 regression coefficients to automatically identify the boundaries of the spatial effect, implemented via a novel penalty. A simple iterative algorithm, with explicit forms at each update step, is developed, and we demonstrate that it converges from the initial values given in the paper. Furthermore, we establish the convergence rate and selection consistency of the proposed estimator under various scenarios involving dimensionality and the effect space. Through simulation studies, we thoroughly evaluate the superior performance of our method in terms of bias and empirical efficiency. Finally, we apply the method to analyse and forecast data from environmental monitoring and Alzheimer’s Disease Neuroimaging Initiative study, revealing interesting findings and achieving smaller out-of-sample prediction errors compared to existing methods.},
  archive      = {J_JRSSSB},
  author       = {Zhang, Chenlin and Zhou, Ling and Guo, Bin and Lin, Huazhen},
  doi          = {10.1093/jrsssb/qkae118},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {872-890},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Spatial effect detection regression for large-scale spatio-temporal covariates},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction sets for high-dimensional mixture of experts models. <em>JRSSSB</em>, <em>87</em>(3), 850-871. (<a href='https://doi.org/10.1093/jrsssb/qkae117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large datasets make it possible to build predictive models that can capture heterogenous relationships between the response variable and features. The mixture of high-dimensional linear experts model posits that observations come from a mixture of high-dimensional linear regression models, where the mixture weights are themselves feature-dependent. In this article, we show how to construct valid prediction sets for an ℓ 1 -penalized mixture of experts model in the high-dimensional setting. We make use of a debiasing procedure to account for the bias induced by the penalization and propose a novel strategy for combining intervals to form a prediction set with coverage guarantees in the mixture setting. Synthetic examples and an application to the prediction of critical temperatures of superconducting materials show our method to have reliable practical performance.},
  archive      = {J_JRSSSB},
  author       = {Javanmard, Adel and Shao, Simeng and Bien, Jacob},
  doi          = {10.1093/jrsssb/qkae117},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {850-871},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Prediction sets for high-dimensional mixture of experts models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic modelling of sparse longitudinal data and functional snippets with stochastic differential equations. <em>JRSSSB</em>, <em>87</em>(3), 833-849. (<a href='https://doi.org/10.1093/jrsssb/qkae116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse functional/longitudinal data have attracted widespread interest due to the prevalence of such data in social and life sciences. A prominent scenario where such data are routinely encountered are accelerated longitudinal studies, where subjects are enrolled in the study at a random time and are only tracked for a short amount of time relative to the domain of interest. The statistical analysis of such functional snippets is challenging since information for far-off-diagonal regions of the covariance structure is missing. Our main methodological contribution is to address this challenge by bypassing covariance estimation and instead modelling the underlying process as the solution of a data-adaptive stochastic differential equation. Taking advantage of the interface between Gaussian functional data and stochastic differential equations makes it possible to efficiently reconstruct the target process by estimating its dynamic distribution. The proposed approach allows one to consistently recover forward sample paths from functional snippets at the subject level. We establish the existence and uniqueness of the solution to the proposed data-driven stochastic differential equation and derive rates of convergence for the corresponding estimators. The finite sample performance is demonstrated with simulation studies and functional snippets arising from a growth study and spinal bone mineral density data.},
  archive      = {J_JRSSSB},
  author       = {Zhou, Yidong and Müller, Hans-Georg},
  doi          = {10.1093/jrsssb/qkae116},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {833-849},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Dynamic modelling of sparse longitudinal data and functional snippets with stochastic differential equations},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moving beyond population variable importance: Concept, theory and applications of individual variable importance. <em>JRSSSB</em>, <em>87</em>(3), 816-832. (<a href='https://doi.org/10.1093/jrsssb/qkae115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a non-parametric regression setting, we introduce a novel concept of ‘individual variable importance’, which assesses the relevance of certain covariates to an outcome variable among individuals with specific characteristics. This concept holds practical importance for both risk assessment and association identification. For example, it can represent (i) the usefulness of expensive biomarkers in risk prediction for individuals at a specified baseline risk, or (ii) age-specific associations between physiological indicators. We quantify individual variable importance using a ratio parameter between two conditional mean squared errors. To estimate and infer this parameter, we develop fully non-parametric estimators and establish their asymptotic properties. Our method performs well in simulation studies. Applying our approach to analyse a real dataset reveals a scientifically interesting result: the association between body shape and systolic blood pressure diminishes with increasing age. Our finding aligns with the medical literature based on standard parametric regression techniques, but our approach is more reliable due to its robustness to model misspecification. More importantly, the fully non-parametric nature of our method allows it to be applied in settings with complex relationships between variables, which cannot be correctly characterized by traditional parametric interaction analyses.},
  archive      = {J_JRSSSB},
  author       = {Dai, Guorong and Shao, Lingxuan and Chen, Jinbo},
  doi          = {10.1093/jrsssb/qkae115},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {816-832},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Moving beyond population variable importance: Concept, theory and applications of individual variable importance},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive conformal classification with noisy labels. <em>JRSSSB</em>, <em>87</em>(3), 796-815. (<a href='https://doi.org/10.1093/jrsssb/qkae114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a conformal prediction method for classification tasks that can adapt to random label contamination in the calibration sample, often leading to more informative prediction sets with stronger coverage guarantees compared to existing approaches. This is obtained through a precise characterization of the coverage inflation (or deflation) suffered by standard conformal inferences in the presence of label contamination, which is then made actionable through a new calibration algorithm. Our solution can leverage different modelling assumptions about the contamination process, while requiring no knowledge of the underlying data distribution or of the inner workings of the classification model. The empirical performance of the proposed method is demonstrated through simulations and an application to object classification with the CIFAR-10H image data set.},
  archive      = {J_JRSSSB},
  author       = {Sesia, Matteo and Wang, Y X Rachel and Tong, Xin},
  doi          = {10.1093/jrsssb/qkae114},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {796-815},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Adaptive conformal classification with noisy labels},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable couplings for the random walk metropolis algorithm. <em>JRSSSB</em>, <em>87</em>(3), 772-795. (<a href='https://doi.org/10.1093/jrsssb/qkae113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a recent surge of interest in coupling methods for Markov chain Monte Carlo algorithms: they facilitate convergence quantification and unbiased estimation, while exploiting embarrassingly parallel computing capabilities. Motivated by these, we consider the design and analysis of couplings of the random walk Metropolis algorithm which scale well with the dimension of the target measure. Methodologically, we introduce a low-rank modification of the synchronous coupling that is provably optimally contractive in standard high-dimensional asymptotic regimes. We expose a shortcoming of the reflection coupling, the state of the art at the time of writing, and we propose a modification which mitigates the issue. Our analysis bridges the gap to the optimal scaling literature and builds a framework of asymptotic optimality which may be of independent interest. We illustrate the applicability of our proposed couplings, and the potential for extending our ideas, with various numerical experiments.},
  archive      = {J_JRSSSB},
  author       = {Papp, Tamás P and Sherlock, Chris},
  doi          = {10.1093/jrsssb/qkae113},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {772-795},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Scalable couplings for the random walk metropolis algorithm},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). α-separability and adjustable combination of amplitude and phase model for functional data. <em>JRSSSB</em>, <em>87</em>(3), 746-771. (<a href='https://doi.org/10.1093/jrsssb/qkae112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider separating and joint modelling amplitude and phase variations for functional data in an identifiable manner. To rigorously address this separability issue, we introduce the notion of α-separability upon constructing a family of α -indexed metrics. We bridge α -separability with the uniqueness of Fréchet mean, leading to the proposed adjustable combination of amplitude and phase model. The parameter α allows user-defined modelling emphasis between vertical and horizontal features and provides a novel viewpoint on the identifiability issue. We prove the consistency of the sample Fréchet mean and variance, and the proposed estimators. Our method is illustrated in simulations and COVID-19 infection rate data.},
  archive      = {J_JRSSSB},
  author       = {Wang, Tian and Ding, Jimin},
  doi          = {10.1093/jrsssb/qkae112},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {746-771},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {α-separability and adjustable combination of amplitude and phase model for functional data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust angle-based transfer learning in high dimensions. <em>JRSSSB</em>, <em>87</em>(3), 723-745. (<a href='https://doi.org/10.1093/jrsssb/qkae111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning improves target model performance by leveraging data from related source populations, especially when target data are scarce. This study addresses the challenge of training high-dimensional regression models with limited target data in the presence of heterogeneous source populations. We focus on a practical setting where only parameter estimates of pretrained source models are available, rather than individual-level source data. For a single source model, we propose a novel angle-based transfer learning (angleTL) method that leverages concordance between source and target model parameters. AngleTL adapts to the signal strength of the target model, unifies several benchmark methods, and mitigates negative transfer when between-population heterogeneity is large. We extend angleTL to incorporate multiple source models, accounting for varying levels of relevance among them. Our high-dimensional asymptotic analysis provides insights into when a source model benefits the target model and demonstrates the superiority of angleTL over other methods. Extensive simulations validate these findings and highlight the feasibility of applying angleTL to transfer genetic risk prediction models across multiple biobanks.},
  archive      = {J_JRSSSB},
  author       = {Gu, Tian and Han, Yi and Duan, Rui},
  doi          = {10.1093/jrsssb/qkae111},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {723-745},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robust angle-based transfer learning in high dimensions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequentist inference for semi-mechanistic epidemic models with interventions. <em>JRSSSB</em>, <em>87</em>(3), 701-722. (<a href='https://doi.org/10.1093/jrsssb/qkae110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effect of public health interventions on an epidemic are often estimated by adding the intervention to epidemic models. During the Covid-19 epidemic, numerous papers used such methods for making scenario predictions. The majority of these papers use Bayesian methods to estimate the parameters of the model. In this article, we show how to use frequentist methods for estimating these effects which avoids having to specify prior distributions. We also use model-free shrinkage methods to improve estimation when there are many different geographic regions. This allows us to borrow strength from different regions while still getting confidence intervals with correct coverage and without having to specify a hierarchical model. Throughout, we focus on a semi-mechanistic model which provides a simple, tractable alternative to compartmental methods.},
  archive      = {J_JRSSSB},
  author       = {Bong, Heejong and Ventura, Valérie and Wasserman, Larry},
  doi          = {10.1093/jrsssb/qkae110},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {701-722},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Frequentist inference for semi-mechanistic epidemic models with interventions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal mediation analysis: Selection with asymptotically valid inference. <em>JRSSSB</em>, <em>87</em>(3), 678-700. (<a href='https://doi.org/10.1093/jrsssb/qkae109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers are often interested in learning not only the effect of treatments on outcomes, but also the mechanisms that transmit these effects. A mediator is a variable that is affected by treatment and subsequently affects outcome. Existing methods for penalized mediation analyses may lead to ignoring important mediators and either assume that finite-dimensional linear models are sufficient to remove confounding bias, or perform no confounding control at all. In practice, these assumptions may not hold. We propose a method that considers the confounding functions as nuisance parameters to be estimated using data-adaptive methods. We then use a novel regularization method applied to this objective function to identify a set of important mediators. We consider natural direct and indirect effects as our target parameters. We then proceed to derive the asymptotic properties of our estimators and establish the oracle property under specific assumptions. Asymptotic results are also presented in a local setting, which contrast the proposal with the standard adaptive lasso. We also propose a perturbation bootstrap technique to provide asymptotically valid postselection inference for the mediated effects of interest. The performance of these methods will be discussed and demonstrated through simulation studies.},
  archive      = {J_JRSSSB},
  author       = {Jones, Jeremiah and Ertefaie, Ashkan and Strawderman, Robert L},
  doi          = {10.1093/jrsssb/qkae109},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {678-700},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Causal mediation analysis: Selection with asymptotically valid inference},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Engression: Extrapolation through the lens of distributional regression. <em>JRSSSB</em>, <em>87</em>(3), 653-677. (<a href='https://doi.org/10.1093/jrsssb/qkae108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributional regression aims to estimate the full conditional distribution of a target variable, given covariates. Popular methods include linear and tree ensemble based quantile regression. We propose a neural network-based distributional regression methodology called ‘engression’. An engression model is generative in the sense that we can sample from the fitted conditional distribution and is also suitable for high-dimensional outcomes. Furthermore, we find that modelling the conditional distribution on training data can constrain the fitted function outside of the training support, which offers a new perspective to the challenging extrapolation problem in nonlinear regression. In particular, for ‘preadditive noise’ models, where noise is added to the covariates before applying a nonlinear transformation, we show that engression can successfully perform extrapolation under some assumptions such as monotonicity, whereas traditional regression approaches such as least-squares or quantile regression fall short under the same assumptions. Our empirical results, from both simulated and real data, validate the effectiveness of the engression method. The software implementations of engression are available in both R and Python.},
  archive      = {J_JRSSSB},
  author       = {Shen, Xinwei and Meinshausen, Nicolai},
  doi          = {10.1093/jrsssb/qkae108},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {653-677},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Engression: Extrapolation through the lens of distributional regression},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness, model checking, and hierarchical models. <em>JRSSSB</em>, <em>87</em>(3), 632-652. (<a href='https://doi.org/10.1093/jrsssb/qkae107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model checking is essential to evaluate the adequacy of statistical models and the validity of inferences drawn from them. Particularly, hierarchical models such as latent Gaussian models (LGMs) pose unique challenges as it is difficult to check assumptions on the latent parameters. Diagnostic statistics are often used to quantify the degree to which a model fit deviates from the observed data. We construct diagnostic statistics by (a) defining an alternative model with relaxed assumptions and (b) deriving the diagnostic statistic most sensitive to discrepancies induced by this alternative model. We also promote a workflow for model criticism that combines model checking with subsequent robustness analysis. As a result, we obtain a general recipe to check assumptions in hierarchical models and the impact of these assumptions on the results. We demonstrate the ideas by assessing the latent Gaussianity assumption, a crucial but often overlooked assumption in LGMs. We illustrate the methods via examples utilizing Stan and provide functions for easy usage of the methods for general models fitted through R-INLA.},
  archive      = {J_JRSSSB},
  author       = {Cabral, Rafael and Bolin, David and Rue, Håvard},
  doi          = {10.1093/jrsssb/qkae107},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {632-652},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robustness, model checking, and hierarchical models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive functional principal components analysis. <em>JRSSSB</em>, <em>87</em>(3), 603-631. (<a href='https://doi.org/10.1093/jrsssb/qkae106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis almost always involves smoothing discrete observations into curves, because they are never observed in continuous time and rarely without error. Although smoothing parameters affect the subsequent inference, data-driven methods for selecting these parameters are not well-developed, frustrated by the difficulty of using all the information shared by curves while being computationally efficient. On the one hand, smoothing individual curves in an isolated, albeit sophisticated way, ignores useful signals present in other curves. On the other hand, bandwidth selection by automatic procedures such as cross-validation after pooling all the curves together quickly become computationally unfeasible due to the large number of data points. In this paper, we propose a new data-driven, adaptive kernel smoothing, specifically tailored for functional principal components analysis through the derivation of sharp, explicit risk bounds for the eigen-elements. The minimization of these quadratic risk bounds provides refined, yet computationally efficient bandwidth rules for each eigen-element separately. Both common and independent design cases are allowed. Rates of convergence for the estimators are derived. An extensive simulation study, designed in a versatile manner to closely mimic the characteristics of real data sets supports our methodological contribution. An illustration on a real data application is provided.},
  archive      = {J_JRSSSB},
  author       = {Wang, Sunny G W and Patilea, Valentin and Klutchnikoff, Nicolas},
  doi          = {10.1093/jrsssb/qkae106},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {603-631},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Adaptive functional principal components analysis},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). X-vine models for multivariate extremes. <em>JRSSSB</em>, <em>87</em>(3), 579-602. (<a href='https://doi.org/10.1093/jrsssb/qkae105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular vine sequences permit the organization of variables in a random vector along a sequence of trees. Vine-based dependence models have become greatly popular as a way to combine arbitrary bivariate copulas into higher-dimensional ones, offering flexibility, parsimony, and tractability. In this project, we use regular vine sequences to decompose and construct the exponent measure density of a multivariate extreme value distribution, or, equivalently, the tail copula density. Although these densities pose theoretical challenges due to their infinite mass, their homogeneity property offers simplifications. The theory sheds new light on existing parametric families and facilitates the construction of new ones, called X-vines. Computations proceed via recursive formulas in terms of bivariate model components. We develop simulation algorithms for X-vine multivariate Pareto distributions as well as methods for parameter estimation and model selection on the basis of threshold exceedances. The methods are illustrated by Monte Carlo experiments and a case study on US flight delay data.},
  archive      = {J_JRSSSB},
  author       = {Kiriliouk, Anna and Lee, Jeongjin and Segers, Johan},
  doi          = {10.1093/jrsssb/qkae105},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {579-602},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {X-vine models for multivariate extremes},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
