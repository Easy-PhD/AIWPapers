<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FTML</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ftml">FTML - 1</h2>
<ul>
<li><details>
<summary>
(2025). Hyperparameter optimization in machine learning. <em>FTML</em>, <em>18</em>(6), 1054-1201. (<a href='https://doi.org/10.1561/2200000088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameters are configuration variables controlling the behavior of machine learning algorithms. They are ubiquitous in machine learning and artificial intelligence and the choice of their values determines the effectiveness of systems based on these technologies. Manual hyperparameter search is often time-consuming and becomes infeasible when the number of hyperparameters is large. Automating the search is an important step towards advancing, streamlining, and systematizing machine learning, freeing researchers and practitioners alike from the burden of finding a good set of hyperparameters by trial and error. In this survey, we present a unified treatment of hyperparameter optimization, providing the reader with examples, insights into the state-of-the-art, and numerous links to further reading. We cover the main families of techniques to automate hyperparameter search, often referred to as hyperparameter optimization or tuning, including random and quasi-random search, bandit-, model-, population-, and gradient-based approaches. We further discuss extensions, including online, constrained, and multi-objective formulations, touch upon connections with other fields, such as meta-learning and neural architecture search, and conclude with open questions and future research directions.},
  archive      = {J_FTML},
  author       = {Luca Franceschi and Michele Donini and Valerio Perrone and Aaron Klein and Cédric Archambeau and Matthias Seeger and Massimiliano Pontil and Paolo Frasconi},
  doi          = {10.1561/2200000088},
  journal      = {Foundations and Trends® in Machine Learning},
  month        = {10},
  number       = {6},
  pages        = {1054-1201},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Hyperparameter optimization in machine learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
