<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms">THMS - 18</h2>
<ul>
<li><details>
<summary>
(2025). A method for improving online active engagement during lower limb rehabilitation training based on EEG signals. <em>THMS</em>, <em>55</em>(4), 650-659. (<a href='https://doi.org/10.1109/THMS.2025.3569194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing online active engagement has been shown to effectively improve rehabilitation outcomes. This article introduces a method for improving online active engagement during lower limb rehabilitation training based on electroencephalogram (EEG) signals using an artificial neural network (ANN) recognition model as well as a virtual reality-based induced active engagement rehabilitation training system (VATS). The VATS consists of a bicycle-style rehabilitation instrument and a virtual reality game. Experiments were performed to train the ANN model and to validate the effectiveness of the VATS. We compared the accuracy of the ANN with that of k-nearest neighbor and support vector machine models, where active engagement results from eye tracker analysis were used as the gold standard. In addition, we used the surface electromyography engagement index (Er) to compare the VATS with and without active engagement feedback. The results showed that the ANN model had the highest average accuracy of 76.95% and a maximum accuracy of 81.48%. A comparison of the ANN, KNN, and SVM models revealed statistically significant differences in accuracy (p < 0.05). Significant differences were observed in the Er values between the VATS with and without active engagement feedback. The results indicated that the ANN has an apparent advantage in active engagement recognition. Moreover, the VATS was effective in improving active engagement. The proposed method utilized EEG signals to characterize active engagement intention and achieved real-time enhancement of active engagement through VATS. This method could be further used to improve the rehabilitation training effect in clinical applications.},
  archive      = {J_THMS},
  author       = {Mingyu Du and Yiyun Yao and Jiayao Xiang and Xin Chen and Yinan Jin and Kewen Zhang and Guanjun Bao and Shibo Cai},
  doi          = {10.1109/THMS.2025.3569194},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {650-659},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A method for improving online active engagement during lower limb rehabilitation training based on EEG signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of bundle branch block from 12-lead ECG using fifth-order tensor-domain machine learning. <em>THMS</em>, <em>55</em>(4), 639-649. (<a href='https://doi.org/10.1109/THMS.2025.3563292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle branch block (BBB) is a cardiac disease that occurs due to the delay in the heart’s electrical activity during a heartbeat. The early detection of BBB using 12-lead electrocardiogram (ECG) is crucial in clinical studies for monitoring the progress of this disease and initiation of treatment. This article proposes a fifth-order tensor-domain machine learning (FOTDML) approach for the automated detection of BBB using 12-lead ECG recordings. The entire duration of the 12-lead ECG recording of each subject is initially segmented into 12-lead beats using a multilead fusion-based QRS complex detection method. Multivariate fast iterative filtering (MVFIF) decomposes each 12-lead beat into intrinsic mode functions or local components. Then, the continuous wavelet transform is utilized to evaluate the time-frequency representation of the MVFIF mode of the 12-lead beat. A fifth-order tensor containing the information on 12-lead ECG beats, leads, MVFIF-domain local components, frequencies (or scales), and samples is evaluated from the entire duration of the 12-lead ECG recording of each subject. Multilinear singular value decomposition is employed to extract features from the fifth-order tensor. Different machine learning (ML)-based methods are utilized to detect BBB from the fifth-order tensor-domain features of each subject’s entire duration 12-lead ECG recording. The suggested approach is evaluated using 12-lead ECG recordings of subjects from two public databases. The results show that the proposed FOTDML approach has yielded the classification accuracy values of 99.88% and 100%, respectively, for healthy control (HC) versus left BBB versus right BBB and HC versus BBB schemes with the subject-independent hold-out validation strategy. The suggested FOTDML approach has demonstrated higher classification performance than the existing methods to detect BBB using 12-lead ECG.},
  archive      = {J_THMS},
  author       = {Chhaviraj Chauhan and Rajesh Kumar Tripathy and Monika Agrawal},
  doi          = {10.1109/THMS.2025.3563292},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {639-649},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Detection of bundle branch block from 12-lead ECG using fifth-order tensor-domain machine learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effect and sensitivity analysis of VR gaming on human contact force perception. <em>THMS</em>, <em>55</em>(4), 629-638. (<a href='https://doi.org/10.1109/THMS.2025.3566497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging evidence suggests that prolonged virtual reality (VR) exposure may impair human sensory systems. Most research has focused on the visual, proprioceptive, and vestibular systems, but the impact of VR on haptic perception remains unclear. In this study, we investigated alterations in human sensitivity to contact force following VR gaming. A force perception task was designed to assess changes in contact force across six difficulty levels with step sizes ranging from 0.5 to 5 N. A total of 18 participants performed the task before VR, after 10 min, and after an additional 20 min of VR. The perceptual accuracy of correctly perceiving force changes at each difficulty level was measured across three test periods. The results indicated that 66.67% of participants experienced a negative impact from VR at the 1-N change step. Perceptual accuracy significantly decreased in this group, with a 9.17% reduction after 10 min and a 17.50% reduction after an additional 20 min. In contrast, minimal effects were observed in the remaining participants. These findings suggest that even short-term VR exposure can impair force discrimination in certain users, with the effects becoming more pronounced over time.},
  archive      = {J_THMS},
  author       = {Yi-Feng Chen and Han Zi and Jing Zhong and Changqi Zhang and Mingjie Dong and Jin Yuan and Mingming Zhang},
  doi          = {10.1109/THMS.2025.3566497},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {629-638},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Effect and sensitivity analysis of VR gaming on human contact force perception},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formalizing motion plan legibility using empirical manual takeover data in autonomous spacecraft docking. <em>THMS</em>, <em>55</em>(4), 619-628. (<a href='https://doi.org/10.1109/THMS.2025.3573243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spacecraft rendezvous and docking maneuvers are becoming highly automated in an attempt to decrease astronaut workload, but still require a human supervisor to continually monitor the system and manually take over control when systems are not performing as expected. This requirement shifts astronaut workload to a monitoring and failure-mitigation task, which must be characterized to assess the influence of this task shift for automated rendezvous and docking (ARD). The physical performance of manual takeover maneuvers are well studied in other fields, such as automated vehicles, but less is known about the factors that influence the decision leading to takeover. This study operationalizes the concept of automation legibility (i.e., intent-expression) to gain insight into when and where supervisors initiate manual takeover. We hypothesized that fundamental aspects of autonomous agent path planning of initial condition and path curvature influence path legibility and takeover decision-making. The study had $N=33$ participants who performed an ARD monitoring task. Metrics for legibility were defined using the positions, where the human initiated a manual takeover along the ARD path. Results support that initial condition, path curvature, and autonomous agent heading were interacting predictors of path legibility and takeover decision timing and location. Increased legibility was correlated to supervisor perceptions of path-appropriateness. The most legible paths aligned the supervisor’s egocentric viewpoint to the path goal, while putting targets of avoidance in the supervisor’s periphery. Characterizations of cooperative performance in human-automation interaction systems from this study can inform future ARD system design that mitigates workload in ARD task performance.},
  archive      = {J_THMS},
  author       = {Hannah Larson and Leia Stirling},
  doi          = {10.1109/THMS.2025.3573243},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {619-628},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Formalizing motion plan legibility using empirical manual takeover data in autonomous spacecraft docking},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When do drivers maneuver: Experimental investigation and inference of perception-response time for tailored safety systems in intelligent vehicles. <em>THMS</em>, <em>55</em>(4), 609-618. (<a href='https://doi.org/10.1109/THMS.2025.3567611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sudden traffic hazards trigger collision-avoidance behaviors in drivers that can significantly impact vehicle dynamics, potentially conflicting with the existing advanced driver assistance systems (ADAS), such as autonomous emergency braking and steering. This behavior can lead to unexpected vehicle movements, further complicating the situation and elevating the risk of accidents. Understanding and tailoring the inference of drivers’ perception-response time (PRT) is essential for optimizing ADAS activation in intelligent vehicles. This approach allows customization for individual drivers, improving safety and ensuring that interventions are personalized and minimally disruptive to normal driving patterns. To achieve this objective, this study performs high-fidelity simulation experiments to gather a comprehensive multidimensional dataset on drivers’ responses in safety-critical scenarios, primarily focusing on PRT and its influencing factors. Using the collision-avoidance behavior data, a driver evidence accumulation model is created to explain PRT distribution and facilitate real-time personalized inferences. We also analyze the relationship between model parameters and real-world physical significance, demonstrating that driver decisions rely on visual evidence accumulation influenced by dynamic interactions in different scenarios. Our proposed model, by offering a detailed understanding of drivers’ perceptual and decision-making processes, aids in developing personalized driver assistance system activation recommendations. This approach seeks to create personalized and adaptive systems within intelligent vehicles, thereby reducing human-machine conflicts and improving the overall safety of intelligent transportation systems.},
  archive      = {J_THMS},
  author       = {Detong Qin and Qingfan Wang and Quan Li and Tianle Lu and Chen Chen and Hong Wang and Bingbing Nie},
  doi          = {10.1109/THMS.2025.3567611},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {609-618},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {When do drivers maneuver: Experimental investigation and inference of perception-response time for tailored safety systems in intelligent vehicles},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMPD: A dual-modality fusion method for cross-spectral pedestrian detection. <em>THMS</em>, <em>55</em>(4), 599-608. (<a href='https://doi.org/10.1109/THMS.2025.3566102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In urban safety, intelligent transportation, and smart security applications, robust pedestrian detection is paramount. Methods that rely solely on visible light imaging struggle in low-light or adverse weather conditions. To address these challenges, we propose dual-modality pedestrian detection (DMPD)—a novel dual-modality pedestrian detection framework that fuses visible and infrared imaging through innovative fusion strategies. The method integrates a modal alignment module to reduce pixel-level misalignment, a differential modal fusion module to effectively combine complementary features while suppressing noise, and a mix module that enhances multiscale feature extraction via integrated convolution and self-attention mechanisms. Furthermore, the enhanced YOLOv7 is used to further boost feature representation and detection accuracy. Experimental results on the public dataset demonstrate that DMPD achieves a detection $mA{{P}_{50}}$ of 97.1% and a real-time speed of 118 FPS, outperforming state-of-the-art methods under both normal and adverse conditions, including fog, rain, and snow. These results confirm the effectiveness of the proposed fusion strategy in harnessing the complementary strengths of visible and infrared modalities, thereby offering a highly robust and scalable solution for pedestrian detection in complex urban environments.},
  archive      = {J_THMS},
  author       = {Huanyu Yang and Jun Wang and Mengchu Tian and Yuming Bo},
  doi          = {10.1109/THMS.2025.3566102},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {599-608},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {DMPD: A dual-modality fusion method for cross-spectral pedestrian detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noninvasive blood glucose monitoring system based on deep learning and multiwavelength near-infrared technology. <em>THMS</em>, <em>55</em>(4), 589-598. (<a href='https://doi.org/10.1109/THMS.2025.3579000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent blood glucose monitoring is crucial for patients with diabetes. However, current blood glucose measurement methods are primarily invasive and, thus, cause discomfort and infection risks. Accordingly, this study developed a noninvasive real-time glucose monitoring system based on a deep learning (DL) model (comprising convolutional neural network and long short-term memory network models) and multiwavelength near-infrared (NIR) light technology. The system is equipped with a portable finger gripper with NIR light-emitting diodes emitting at three distinct wavelengths (810, 860, and 940 nm) to illuminate the finger. A triad spectroscopy sensor is then used to capture finger photoplethysmography (PPG) signals within only 6 s. The captured signals are then transmitted to a server-side DL model for glucose prediction. This DL model analyzes the three-wavelength PPG data to predict a user’s blood glucose value. The predicted glucose value is subsequently displayed on a dedicated smartphone app. In contrast to previously proposed systems relying on machine learning for feature extraction, the proposed system uses a DL model to automatically extract glucose-related features from the three-band PPG signals, ultimately leading to blood glucose predictions with a root-mean-square error of 6.62. Furthermore, the proposed system prioritizes user comfort, portability, and stability, thereby offering a convenient and accessible blood glucose monitoring experience.},
  archive      = {J_THMS},
  author       = {Chih-Wei Peng and Bor-Shyh Lin and Hsin-Yen Lin and Yu-Ching Shau and Bor-Shing Lin},
  doi          = {10.1109/THMS.2025.3579000},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {589-598},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Noninvasive blood glucose monitoring system based on deep learning and multiwavelength near-infrared technology},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BreathRelax: A game-based breathing system combines stress relief and engagement. <em>THMS</em>, <em>55</em>(4), 579-588. (<a href='https://doi.org/10.1109/THMS.2025.3582068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breathing exercises have gained popularity as a stress management technique. They provide benefits such as improved lung function, reduced stress, and better quality of life. However, the repetitive nature of these exercises can become boring over time, decreasing people’s motivation to continue practicing them. In this work, we developed BreathRelax, a breath-based video game that integrates a temperature sensor to detect respiratory signals and provides real-time feedback through computer gaming. BreathRelax can achieve an accuracy rate of 95% in recognizing breathing patterns, with a response time of 0.23 s, demonstrating its reliable and responsive performance. To evaluate the effectiveness of BreathRelax, 60 participants were invited for a comprehensive experiment. ANCOVA analysis demonstrated BreathRelax’s superior stress reduction through significantly improved RMSSD, SDNN, and pNN50 stress indices. In addition, the usability evaluation revealed that participants experienced high enjoyment and engagement in the game while finding it easy to operate. BreathRelax delivered promising results in both relaxation effects and user experience.},
  archive      = {J_THMS},
  author       = {Chang-Yu Lin and Edward T.-H. Chu and Chia-Rong Lee},
  doi          = {10.1109/THMS.2025.3582068},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {579-588},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {BreathRelax: A game-based breathing system combines stress relief and engagement},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MADDPG based distributed multirequest pricing mechanisms for sensing tasks. <em>THMS</em>, <em>55</em>(4), 569-578. (<a href='https://doi.org/10.1109/THMS.2025.3580554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, mobile crowdsensing (MCS) has received widespread attention due to various application demands, such as home chronic rehabilitation via Internet, emergency rescues, and smart cities, where more than one requester (convalescents) need to recruit mobile users simultaneously (doctors) equipped with different sensors to fulfil the tasks, such as labeling, sensing, and diagnosis. However, most of existing MCS works focus on only one requester, and demands of all of mobile requesters (convalescents) are uncertain. Therefore, this article proposed semi-distributed and distributed multirequest pricing mechanisms for multirequest MCS on the basis of MADDPG, and laid theoretical foundations for home chronic rehabilitation via Internet. Extensive simulations demonstrate that our mechanisms outweigh existing benchmarks.},
  archive      = {J_THMS},
  author       = {Jiajun Sun and Dianliang Wu},
  doi          = {10.1109/THMS.2025.3580554},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {569-578},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {MADDPG based distributed multirequest pricing mechanisms for sensing tasks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AVE speech: A comprehensive multimodal dataset for speech recognition integrating audio, visual, and electromyographic signals. <em>THMS</em>, <em>55</em>(4), 559-568. (<a href='https://doi.org/10.1109/THMS.2025.3585165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global aging population faces considerable challenges, particularly in communication, due to the prevalence of hearing and speech impairments. To address these, we introduce the AVE speech, a comprehensive multimodal dataset for speech recognition tasks. The dataset includes a 100-sentence Mandarin corpus with audio signals, lip-region video recordings, and six-channel electromyography data, collected from 100 participants. Each subject read the entire corpus ten times, with each sentence averaging approximately two seconds in duration, resulting in over 55 hours of multimodal speech data per modality. Experiments demonstrate that combining these modalities significantly improves recognition performance, particularly in cross-subject and high-noise environments. To our knowledge, this is the first publicly available sentence-level dataset integrating these three modalities for large-scale Mandarin speech recognition. We expect this dataset to drive advancements in both acoustic and nonacoustic speech recognition research, enhancing cross-modal learning and human–machine interaction.},
  archive      = {J_THMS},
  author       = {Dongliang Zhou and Yakun Zhang and Jinghan Wu and Xingyu Zhang and Liang Xie and Erwei Yin},
  doi          = {10.1109/THMS.2025.3585165},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {559-568},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {AVE speech: A comprehensive multimodal dataset for speech recognition integrating audio, visual, and electromyographic signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A smart glove based on inductive sensors for hand gesture recognition. <em>THMS</em>, <em>55</em>(4), 549-558. (<a href='https://doi.org/10.1109/THMS.2025.3566941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an enhanced gesture recognition system based on inductive sensors for accurately recognizing American Sign Language (ASL) hand gestures. In this system, conductive threads are employed to sew coils onto a regular glove covering the fingers, wrist, palm, and ulnar positions. To improve sensitivity, a data acquisition system measures a tank circuit formed by each sewn coil and external components. The system is rigorously tested on ten subjects. A comparative study of three machine learning algorithms (MLAs), including random forest (RF), support vector machine (SVM), and K-nearest neighbors (KNNs), is conducted to detect 26 ASL letters. To improve the diversity and generalizability of the MLA, the generative adversarial network (GAN) data augmentation method is provided, expanding the dataset to 5050 trials for each gesture. The results demonstrate impressive accuracy rates of 99.67% and 97.46% using five-fold cross-validation (5F-CV) and leave-one-subject-out cross-validation (LOSO-CV), respectively, for the RF algorithm. This exhibits higher sensitivity in detecting similar gestures compared to the previous design. The proposed solution addresses the limitations of existing hand gesture recognition designs and offers a practical and effective approach to human-computer interaction.},
  archive      = {J_THMS},
  author       = {Maryam Ravan and Alma Abbasnia and Shokoufeh Davarzani and Reza K. Amineh},
  doi          = {10.1109/THMS.2025.3566941},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {549-558},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A smart glove based on inductive sensors for hand gesture recognition},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gait recognition via motion difference representation learning and salient feature modeling. <em>THMS</em>, <em>55</em>(4), 539-548. (<a href='https://doi.org/10.1109/THMS.2025.3576484'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a periodic movement, gait contains informative biometric traits formed by individual body structures, motion patterns, and behavioral habits. Previous gait recognition methods mainly focus on mining the appearance cues from gait sequences, while neglecting the dynamic motion characteristics. Motion cues are important complementary information for generating high-quality gait representations that can help models accurately recognize individuals. In this article, we propose a novel gait recognition framework named GaitDS to model dynamic motion information and construct salient gait representations. Specifically, we develop a motion information perception module that can directly represent dynamic regions during walking and extract fine-grained motion features based on the appearance of body parts over time. In addition, since some frames in gait sequences share partial similarities, we present saliency identity representation learning to focus on key frames along the temporal dimension, and integrate salient identity features to enhance sequence-level representations. Furthermore, a channel enhanced module is designed to generate more discriminative gait representations, where motion and temporal salient features can be complemented with global representations. Compared with existing state-of-the-art methods, our model achieves superior average rank-1 recognition accuracy on three benchmark datasets, i.e., 93.7% on CASIA-B, 92.4% on OU-MVLP, and 50.7% on Gait3D.},
  archive      = {J_THMS},
  author       = {Wei Huo and Ke Wang and Jun Tang and Nian Wang and Dong Liang},
  doi          = {10.1109/THMS.2025.3576484},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {539-548},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Gait recognition via motion difference representation learning and salient feature modeling},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Voice familiarity in a voice-reminders app for elderly care recipients and their family caregivers. <em>THMS</em>, <em>55</em>(4), 529-538. (<a href='https://doi.org/10.1109/THMS.2025.3582259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on the effect of voice familiarity in voice reminders between elders and their caregivers. A voice application (for desktops and smartphones) was created to study these effects. Seniors, and their family care providers along with medical providers were consulted for voice application design and improvement opportunities using two user tests with 17 care dyads and 15 medical care providers. Results from qualitative content analysis show that care dyads generally prefer familiar voice reminders for routine tasks (such as taking medication) and generally find caregiver-voiced reminders to be acceptable, while medical care providers have mixed opinions about older adults using recorded reminders.},
  archive      = {J_THMS},
  author       = {Karen P. Valdivia and Jamy Li},
  doi          = {10.1109/THMS.2025.3582259},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {529-538},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Voice familiarity in a voice-reminders app for elderly care recipients and their family caregivers},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An actionability assessment tool for enhancing algorithmic recourse in explainable AI. <em>THMS</em>, <em>55</em>(4), 519-528. (<a href='https://doi.org/10.1109/THMS.2025.3582285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce and evaluate a tool for researchers and practitioners to assess the actionability of information provided to users to support algorithmic recourse. While there are clear benefits of recourse from the user’s perspective, the notion of actionability in explainable AI research remains vague, and claims of ‘actionable’ explainability techniques are based on researchers’ intuitions. Inspired by definitions and instruments for assessing actionability in other domains, we construct a seven-item tool and investigate its effectiveness through two user studies. We show that the tool discriminates actionability across explanation types and that the distinctions align with human judgments. We illustrate the impact of context on actionability assessments, suggesting that domain-specific tool adaptations may foster more human-centred algorithmic systems. This is a valuable step forward for research and practices into actionable explainability and algorithmic recourse, providing the first clear human-centred tool for assessing actionability in explainable AI.},
  archive      = {J_THMS},
  author       = {Ronal Singh and Tim Miller and Liz Sonenberg and Eduardo Velloso and Frank Vetere and Piers Howe},
  doi          = {10.1109/THMS.2025.3582285},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {519-528},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An actionability assessment tool for enhancing algorithmic recourse in explainable AI},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained assessment of upper-limb bradykinesia through multimodal feature enhancement and deep learning. <em>THMS</em>, <em>55</em>(4), 508-518. (<a href='https://doi.org/10.1109/THMS.2025.3570704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bradykinesia is a hallmark symptom of Parkinson’s disease (PD) that significantly affects patients’ functional abilities and quality of life. This study proposed a fine-grained classification method for evaluating the level of bradykinesia in PD patients. Based on inertial signals, surface electromyographic (sEMG) signals, and videos obtained from 40 PD patients and 13 healthy subjects, the proposed data preprocessing method extracts 69-D features from inertial and sEMG (IE) signals, and 7-D skeleton features from videos. A two-stream network, including IE stream, skeleton stream, and decision fusion module, was developed using long short-term memory, full convolutional neural networks, and fully connected neural networks. In addition, the IE stream incorporated a feature shrinking module to process high-dimensional features to reduce redundant features. Furthermore, an LSTM-variational autoencoders method was proposed for data augmentation of categories with fewer samples. The proposed method achieved higher recognition rates (pro/supination movements of hands: 85.51%, finger tapping: 88.06%, hand movements: 90.00%) compared to other methods. With low-cost, compact and lightweight methods, bradykinesia in PD patients can be intelligently assessed, which will enhance patient management and treatment efficiency.},
  archive      = {J_THMS},
  author       = {Fang Lin and Zhelong Wang and Zhenglin Li and Hongyu Zhao and Xin Shi and Ruichen Liu and Jiaxi Li and Daoyong Peng and Bo Ru},
  doi          = {10.1109/THMS.2025.3570704},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {508-518},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Fine-grained assessment of upper-limb bradykinesia through multimodal feature enhancement and deep learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human perception of AI capabilities at classifying perturbed roadway signs. <em>THMS</em>, <em>55</em>(4), 499-507. (<a href='https://doi.org/10.1109/THMS.2025.3573173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is crucial to numerous functions required for driving automation systems, including the computer vision techniques used to detect the roadway environment and make real-time decisions. However, the images used as inputs to the AI system may be maliciously perturbed, or manipulated, causing the AI system to make an incorrect classification. In this study, we examined humans’ perception of the AI’s computer vision capability of classifying various road sign images, including the original images, images with two different types of malicious attacks, and images that are scrambled randomly at the pixel level. Our results showed that participants rated the AI agent to be less capable than themselves of classifying the road signs. However, they overestimated the AI’s computer vision capability for correctly classifying images with malicious attacks that should cause the AI system to misclassify the image. These findings suggest that people lack an accurate understanding of the vulnerabilities of AI computer vision technologies and tend to overtrust AI in driving automation systems.},
  archive      = {J_THMS},
  author       = {Katherine R. Garcia and Jing Chen and Yanru Xiao and Scott Mishler and Cong Wang and Bin Hu},
  doi          = {10.1109/THMS.2025.3573173},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {499-507},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human perception of AI capabilities at classifying perturbed roadway signs},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time supervision and guidance for rehabilitation exercises via computer vision. <em>THMS</em>, <em>55</em>(4), 490-498. (<a href='https://doi.org/10.1109/THMS.2025.3566248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rehabilitation exercises are essential for individuals who have lost their ability to function normally. Typically, a functional rehabilitation program consists of two parts: supervised exercises in a rehabilitation center and additional exercises performed independently at home. Home exercises are as important as in-center exercises and can significantly improve recovery time. However, at home, the patient is unsupervised, with a significant risk of performing exercises incorrectly. This article presents a web application designed for hand rehabilitation exercises that uses a pretrained hand landmark model and an artificial neural network (ANN) classifier to ensure correct exercise performance. The application employs transfer learning to extract features from the user's hand movements, with the ANN classifier, designed using the ML5 library, determining if the positions are performed accurately. The ML5 library, a high-level interface to TensorFlow.js, makes the application suitable for web deployment. Physicians can create new exercises and retrain the ANN classifier without programming skills. An evaluation with 12 participants demonstrated an overall classification accuracy of 90.25% and achieved a system usability scale score of 75.30, indicating high usability and potential for real-world rehabilitation scenarios. This lightweight web application is highly accessible and user friendly and runs seamlessly on any smartphone or tablet. This approach shows great promise in enhancing exercise accuracy and safety, thereby improving the rehabilitation process at home and in clinical settings. In addition, it is adaptable for other models for full-body and face tracking, making it versatile for various rehabilitation applications.},
  archive      = {J_THMS},
  author       = {Mohamed Z. Amrani and Christoph W. Borst and Nouara Achour},
  doi          = {10.1109/THMS.2025.3566248},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {490-498},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Real-time supervision and guidance for rehabilitation exercises via computer vision},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Infrastructure-free indoor localization for cultural heritage sites. <em>THMS</em>, <em>55</em>(4), 480-489. (<a href='https://doi.org/10.1109/THMS.2025.3562438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor positioning systems are crucial for preserving and promoting cultural heritage sites by providing visitors with accurate location information and real-time exhibit details. These systems also offer valuable insights into visitor behavior for museums and cultural institutions to optimize their offerings and enhance the overall visitor experience. Their importance lies in driving engagement, fostering deeper connections with visitors, and providing data for future decision-making. This study proposes a magnetic field-based pedestrian localization algorithm as a solution to the challenge of providing accurate location information for visitors in indoor environments during virtual tours. The proposed approach uses magnetic field sequences to identify unique landmarks and a deep neural network (VGG-16) to analyze magnetic signal features and determine the visitor's location based on the nearest landmark. The study's results indicate that this magnetic field-based approach can be reliable for providing location information in indoor environments.},
  archive      = {J_THMS},
  author       = {Mohamed Amin Benatia and Souleyman Sahnoun and Mourad Messaadia},
  doi          = {10.1109/THMS.2025.3562438},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {480-489},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Infrastructure-free indoor localization for cultural heritage sites},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
