<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCDS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcds">TCDS - 23</h2>
<ul>
<li><details>
<summary>
(2025). Multimodal discriminative network for emotion recognition across individuals. <em>TCDS</em>, <em>17</em>(5), 1323-1335. (<a href='https://doi.org/10.1109/TCDS.2025.3552124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition is gaining significant attention for ability to fuse complementary information from diverse physiological and behavioral signals, which benefits the understanding of emotional disorders. However, challenges arise in multimodal fusion due to uncertainties inherent in different modalities, such as complex signal coupling and modality heterogeneity. Furthermore, the feature distribution drift in intersubject emotion recognition hinders the generalization ability of the method and significantly degrades performance on new individuals. To address the above issues, we propose a cross-subject multimodal emotion robust recognition framework that effectively extracts subject-independent intrinsic emotional identification information from heterogeneous multimodal emotion data. First, we develop a multichannel network with self-attention and cross-attention mechanisms to capture modality-specific and complementary features among different modalities, respectively. Second, we incorporate contrastive loss into the multichannel attention network to enhance feature extraction across different channels, thereby facilitating the disentanglement of emotion-specific information. Moreover, a self-expression learning-based network layer is devised to enhance feature discriminability and subject alignment. It aligns samples in a discriminative space using block diagonal matrices and maps multiple individuals to a shared subspace using a block off-diagonal matrix. Finally, attention is used to merge multichannel features, and multilayer perceptron is employed for classification. Experimental results on multimodal emotion datasets confirm that our proposed approach surpasses the current state-of-the-art in terms of emotion recognition accuracy, with particularly significant gains observed in the challenging cross-subject multimodal recognition scenarios.},
  archive      = {J_TCDS},
  author       = {Minxu Liu and Donghai Guan and Chuhang Zheng and Qi Zhu},
  doi          = {10.1109/TCDS.2025.3552124},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1323-1335},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal discriminative network for emotion recognition across individuals},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIR-HCL: Semantic-inconsistency reasoning and hybrid contrastive learning for efficient cross-emotion anomaly detection. <em>TCDS</em>, <em>17</em>(5), 1310-1322. (<a href='https://doi.org/10.1109/TCDS.2025.3550645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-emotion anomaly detection is an emerging and challenging research topic in cognitive analysis field, which aims at identifying the abnormal emotion pair whose semantic patterns are inconsistent across different emotional modalities. To the best of our knowledge, this topic has yet to be well studied, which could potentially benefit lots of valuable cognitive applications such as autistic children diagnosis and criminal deception detection. To this end, this article proposes an efficient cross-emotion anomaly detection approach via semantic-inconsistency reasoning and hybrid contrastive learning (SIR-HCL), which is the first attempt to detect the anomalous emotional pairs across the audio–visual emotions. First, the proposed framework utilizes dual-branch network to obtain the deep emotional features in each modality, and then employs the shared residual block to derive the semantically compatible features. Subsequently, an efficient hybrid contrastive learning approach is designed to enlarge the semantic-inconsistency among abnormal emotional pair with different affective classes, while enhancing the semantic-consistency and increasing the feature correlation between normal emotional pair from the same affective class. At the same time, an efficient bidirectional learning scheme is employed to significantly improve the data utilization and a two-component Beta Mixture Model is adaptively utilized to reason the anomalous emotion pairs. Extensive experiments evaluated on two benchmark datasets show that the proposed SIR-HCL method can well detect the anomalous emotional pairs across audio-visual emotional data, and brings substantial improvements over the state-of-the-art competing methods.},
  archive      = {J_TCDS},
  author       = {Xin Liu and Qiyan Chen and Yiu-ming Cheung and Shu-Juan Peng},
  doi          = {10.1109/TCDS.2025.3550645},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1310-1322},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SIR-HCL: Semantic-inconsistency reasoning and hybrid contrastive learning for efficient cross-emotion anomaly detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alteration of functional brain networks during lower limb movement in parkinson's disease patients with freezing of gait. <em>TCDS</em>, <em>17</em>(5), 1301-1309. (<a href='https://doi.org/10.1109/TCDS.2025.3551600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal brain structures have been observed in Parkinson's disease (PD) patients with freezing of gait (FoG), but the neural mechanisms behind FoG are still not well understood. This study analyzed EEG data from 13 PD patients with FoG and 13 healthy controls (HCs) during a pedaling task. Eight key brain regions were selected to measure power density and connectivity across different frequency bands and time windows. Using graph theory, the study examined neural changes between FoG patients and HCs, focusing on metrics: clustering coefficient, degree, nodal efficiency, and global efficiency. FoG patients had decreased δ and θ activity in the precentral-L and frontal-mid-L regions and increased β activity in the precentral-R and postcentral regions during motor initiation (0–400 ms postcue). FoG patients also exhibited decreased connectivity in the bilateral frontal-mid, supplementary motor area (SMA), postcentral, and precentral regions in the δ and θ bands during motor initiation. During motor execution (0–1 s postcue), fewer significant connections were observed in the α band in these regions. Furthermore, FoG patients also had a decreased clustering coefficient in δ, θ, and α bands during motor initiation and in the θ band during motor execution in regions such as SMA-L, frontal-mid-R, and postcentral-R. The nodal efficiency increased in the regions of precentral-R and postcentral-R (δ band), postcentral-L (θ band) for motor initiation, and precentral-L (θ band) for motor execution. FoG in PD is characterized by the changes of brain functional network, including the decrease of δ and θ activities and the increase of β activity in some brain regions during gait initiation and the abnormal network reorganization. These findings may help to understand the neural mechanisms of FoG in PD and could guide future brain stimulation studies.},
  archive      = {J_TCDS},
  author       = {Jingting Liang and Xiangguo Yin and Mingxing Lin and Shuqin Wang and Aiqin Song and Wen Chen},
  doi          = {10.1109/TCDS.2025.3551600},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1301-1309},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Alteration of functional brain networks during lower limb movement in parkinson's disease patients with freezing of gait},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An Encoder–Decoder model based on spiking neural networks for address event representation object recognition. <em>TCDS</em>, <em>17</em>(5), 1286-1300. (<a href='https://doi.org/10.1109/TCDS.2025.3548868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Address event representation (AER) object recognition task has attracted extensive attention in neuromorphic vision processing. The spike-based and event-driven computation inherent in the spiking neural network (SNN) provides an energy-saving solution for AER object recognition. However, SNN with spike timing dependent plasticity (STDP) learning rule has not achieved satisfying AER object recognition performance. This work proposes an SNN-based encoder-decoder model to improve the recognition performance of AER objects. An STDP-based locally connected spiking neural network (LC-SNN) is proposed as an encoder to extract rich spatiotemporal features from AER event flows more flexibly. After the encoder extracts and learns primary features, we propose a fully connected spiking neural network (FC-SNN) based on the reward-modulated spike-timing-dependent plasticity (R-STDP) learning rule as a decoder to learn higher-level features for classification. In addition, we improved the winner-take-all (WTA) mechanisms and R-STDP learning rule in the decoder based on the reward and punish decision, enabling the network to perform better. The experiments are performed on the N-MNIST, MNIST-DVS, and the dynamic vision sensor (DVS) gesture datasets, improving the accuracy of the best existing plasticity-based SNN by 0.19%, 0.27%, and 1.35%, respectively.},
  archive      = {J_TCDS},
  author       = {Sichun Du and Haodi Zhu and Yang Zhang and Qinghui Hong},
  doi          = {10.1109/TCDS.2025.3548868},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1286-1300},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An Encoder–Decoder model based on spiking neural networks for address event representation object recognition},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotional intelligence: Abstract cognition innovation in artificial general intelligence systems. <em>TCDS</em>, <em>17</em>(5), 1272-1285. (<a href='https://doi.org/10.1109/TCDS.2025.3547934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the incorporation of abstract emotion-triggering mechanisms into artificial general intelligence (AGI) systems through the nonaxiomatic reasoning system (NARS) framework. Leveraging cognitive appraisal theory, the proposed model facilitates dynamic regulation of cognitive resources by modulating priority and durability based on goal alignment and temporal evaluation. Distinct from conventional emotion models that depend on predefined feedback mechanisms, this framework enables generalized emotional responses, thereby enhancing adaptability to complex temporal and causal dynamics. Experimental validation conducted on flappy bird and airplane combat simulation platforms illustrates the superiority of the emotion-driven NARS, which demonstrates enhanced decision-making efficiency, robust goal prioritization, and superior adaptability compared to its nonemotional counterpart. These findings underscore the potential of emotion-enabled AGI systems to advance applications in high-stakes domains, including autonomous driving and robotics, where real-time adaptability and efficient resource allocation are paramount.},
  archive      = {J_TCDS},
  author       = {Xiang Li and Yongming Li and Luyao Bai and Jingyi Zhao},
  doi          = {10.1109/TCDS.2025.3547934},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1272-1285},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Emotional intelligence: Abstract cognition innovation in artificial general intelligence systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foundational policy acquisition via multitask learning for motor skill generation. <em>TCDS</em>, <em>17</em>(5), 1260-1271. (<a href='https://doi.org/10.1109/TCDS.2025.3543350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a multitask reinforcement learning (RL) algorithm for foundational policy acquisition to generate novel motor skills. Learning the rich representation of the multitask policy is a challenge in dynamic movement generation tasks because the policy needs to cope with changes in goals or environments with different reward functions or physical parameters. Inspired by human sensorimotor adaptation mechanisms, we developed the learning pipeline to construct the encoder–decoder networks and network selection to facilitate foundational policy acquisition under multiple situations. First, we compared the proposed method with previous multitask RL methods in the standard multilocomotion tasks. The results showed that the proposed approach outperformed the baseline methods. Then, we applied the proposed method to the ball heading task using a monopod robot model to evaluate skill generation performance. The results showed that the proposed method was able to adapt to novel target positions or inexperienced ball restitution coefficients but to acquire a foundational policy network, originally learned for heading motion, which can generate an entirely new overhead kicking skill.},
  archive      = {J_TCDS},
  author       = {Satoshi Yamamori and Jun Morimoto},
  doi          = {10.1109/TCDS.2025.3543350},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1260-1271},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Foundational policy acquisition via multitask learning for motor skill generation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-subject and cross-session EEG emotion recognition based on multisource structural deep clustering. <em>TCDS</em>, <em>17</em>(5), 1245-1259. (<a href='https://doi.org/10.1109/TCDS.2025.3545666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual fluctuations and temporal variability of electroencephalogram (EEG) data pose challenges in precisely identifying emotions. Although a model may perform well with data specific to a certain subject or session, the fluctuations in EEG data can significantly impair the model's performance on a different subject or session. To tackle this problem, current approaches synchronize the original and new subject or session feature distributions. Directly matching EEG data across individuals or sessions may undermine the inherent distinguishability due to the heterogeneity in data distribution. Instead of direct alignment, this work utilizes multisource structural deep clustering to identify the inherent structural knowledge of the target itself and regularize it through the distribution of source labels. Furthermore, the method was implemented on the intermediate output utilizing high-confidence features to improve pattern identification in the latent feature space. This led to more distinct differentiations across subdomains with varying labels. Comparative analyses were performed with state of-the-art (SOTA) models on SEED and SEED-IV datasets. The model proposed outperformed other baseline models, reaching an average accuracy of 90.69%/95.05% in a cross-subject/cross session experiment on SEED and 74.35%/78.56% in SEED-IV. This research provides a novel approach to align EEG features without the need for direct distance calculation.},
  archive      = {J_TCDS},
  author       = {Yiyuan Chen and Xiaodong Xu and Xiaowei Qin},
  doi          = {10.1109/TCDS.2025.3545666},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1245-1259},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cross-subject and cross-session EEG emotion recognition based on multisource structural deep clustering},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variability in sensory event-related potential as an early marker of cognitive impairment. <em>TCDS</em>, <em>17</em>(5), 1235-1244. (<a href='https://doi.org/10.1109/TCDS.2025.3541729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mild cognitive impairment (MCI) is a precursor to dementia and poses significant health and economic challenges. Early detection of MCI can slow disease progression and ease the burden on patients and caregivers. This study aimed to explore sensory-perception deficits and fluctuations in MCI using auditory event-related potentials (ERPs) from a prefrontal two-channel electroencephalogram (EEG) device. The study involved 573 MCI patients and 1,295 cognitively normal (CN) individuals, with cognitive decline assessed through the Seoul neuropsychological screening battery (SNSB) and the mini-mental state examination (MMSE). This study analyzed ERPs and trial-to-trial variability using the response variance curve (RVC) in neural responses to eight auditory sounds. While no significant differences were observed in ERP amplitudes and area under the curves (AUCs) for sensory (N1) and perception (P2) processing between MCI and CN groups, MCI patients showed notable differences in trial-to-trial variability, particularly in those aged 70 to 79 years. This variability remained significant even after adjusting for factors such as age, sex, years of education, and MMSE scores. The study suggests that MCI is associated with instability in preattentive auditory detection and higher-order perceptual processing. The findings highlight that people in their 70s may be in a transitional phase associating these sensory-perceptual variabilities with cognitive impairment. Given the large sample size and limitations of current neuropsychological tests, the study underscores the potential of sensory ERP measures as a supplementary tool for MCI screening.},
  archive      = {J_TCDS},
  author       = {Joel Eyamu and Wuon-Shik Kim and Kahye Kim and Kun Ho Lee and Jaeuk U. Kim},
  doi          = {10.1109/TCDS.2025.3541729},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1235-1244},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Variability in sensory event-related potential as an early marker of cognitive impairment},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved dual neural network method based on levy flight for multirobot cooperative area coverage search in 3-D unknown environments. <em>TCDS</em>, <em>17</em>(5), 1223-1234. (<a href='https://doi.org/10.1109/TCDS.2025.3541416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on multirobot collaborative search in unknown 3-D environments, based on bio-inspired neural networks, holds significant value and importance. However, challenges arise in 3-D environments, including excessive turning and vertical movement, as well as the potential for collisions between robots. In response, we propose an improved Glasius bioinspired neural network (GBNN) that mitigates decision conflicts among robots and considers the impact of turning and vertical movement on robot decision-making. Furthermore, to address the issue of robots getting trapped in local deadlocks during the search process, we present a dual neural network algorithm based on Levy flights. In the method, dual GBNN based on Levy flight (LF-DUAL-GBNN) proposed in this article, robots obtain random target points through Levy flights and are then guided by a dual neural network to navigate to the vicinity of the target points, thus breaking free from local deadlock states. Finally, we conducted simulation experiments to validate the algorithm's effectiveness.},
  archive      = {J_TCDS},
  author       = {Fangfang Zhang and Yongqi Wang and Wenhao Wang and Jianbin Xin and Jinzhu Peng and Yaonan Wang},
  doi          = {10.1109/TCDS.2025.3541416},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1223-1234},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An improved dual neural network method based on levy flight for multirobot cooperative area coverage search in 3-D unknown environments},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributional latent variable models with an application in active cognitive testing. <em>TCDS</em>, <em>17</em>(5), 1212-1222. (<a href='https://doi.org/10.1109/TCDS.2025.3548962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive modeling commonly relies on asking participants to complete a battery of varied tests to estimate attention, working memory, and other latent variables. In many cases, these tests result in highly variable observation models. A near-ubiquitous approach is to repeat many observations for each test independently, resulting in a distribution of the outcomes from each test given to each subject. Latent variable models (LVMs), if employed, are only added after data collection. In this article, we explore the usage of LVMs to enable learning across many correlated variables simultaneously. We extend LVMs to the setting where observed data for each subject are a series of observations from many different distributions, rather than simple vectors to be reconstructed. By embedding test battery results for individuals in a latent space that is trained jointly across a population, we can leverage correlations both between disparate test data for a single participant and between multiple participants. We then propose an active learning framework that leverages this model to conduct more efficient cognitive test batteries. We validate our approach by demonstrating with real-time data acquisition that it performs comparably to conventional methods in making item-level predictions with fewer test items.},
  archive      = {J_TCDS},
  author       = {Robert Kasumba and Dom C.P. Marticorena and Anja Pahor and Geetha Ramani and Imani Goffney and Susanne M. Jaeggi and Aaron R. Seitz and Jacob R. Gardner and Dennis L. Barbour},
  doi          = {10.1109/TCDS.2025.3548962},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1212-1222},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Distributional latent variable models with an application in active cognitive testing},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Barrier offset varying-parameter dynamic learning network for solving dual-arms human-like behavior generation. <em>TCDS</em>, <em>17</em>(5), 1199-1211. (<a href='https://doi.org/10.1109/TCDS.2025.3541070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabling robots to imitate human actions and perform tasks with high precision while avoiding potential obstacles in the environment can effectively enhance the interaction between social robots and humans. In this article, to achieve higher precision trajectory tracking and obstacle avoidance for dual-arm humanoid robots, the barrier offset varying-parameter dynamic learning neural (BOVDL) network method is proposed and applied to dual-arm humanoid behavior generation scheme. To do so, a dual-arms humanoid robot model is set up, and transformed into a constrained time-varying quadratic programming (TVQP) problem. Second, by using Lagrangian multiplier method and Karush–Kuhn–Tuchker condition, the inequality constrained TVQP is converted as a time-varying equation with a barrier parameter. Third, a varying-parameter dynamic learning network is presented to solve the time-varying equation with a barrier parameter. Computer simulation experiments are conducted to verify the feasibility, accuracy, and safety of the proposed BOVDL network method. Experimental results show that all 14 joints of the humanoid robot's arms are within the motion range of each real human arm's physical constraints. The maximum position error and velocity error between the desired trajectory and the actual trajectory of the end effector are less than $10^{-6}\ \text{m}$ magnitude and $10^{-7}\ \text{m}$ magnitude, respectively, representing a reduction of five orders of magnitude compared to the traditional varying-parameter convergent-differential neural network. Furthermore, the proposed method also enables the dual-arm humanoid robot to avoid collisions with obstacles while performing tasks, demonstrating the superiority of the proposed BOVDL network scheme.},
  archive      = {J_TCDS},
  author       = {Zhijun Zhang and Mingyang Zhang and Jinjia Guo and Haotian He},
  doi          = {10.1109/TCDS.2025.3541070},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1199-1211},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Barrier offset varying-parameter dynamic learning network for solving dual-arms human-like behavior generation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memristor-based circuit of sequential associative memory with memory interactions and stimulus similarity effect. <em>TCDS</em>, <em>17</em>(5), 1186-1198. (<a href='https://doi.org/10.1109/TCDS.2025.3540591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most proposed memristor-based circuits of associative memory (AM) consider various mechanisms in only one AM. Few works on circuit design of sequential associative memory (SAM) have been reported. In this article, a memristor-based circuit of SAM with memory interactions and stimulus similarity effect is designed. Two associative memories, the prior associative memory (PAM) and the later associative memory (LAM), are formed successively. The PAM modifies the rate of the formation of the LAM, and the LAM, in turn, affects the strength of the PAM, which are two interactions in SAM, called transfer and retroaction. In addition, the magnitudes of transfer and retroaction are determined by the similarity of the conditioned stimuli. The above functions are realized by the ring input processing module, memory module, transfer module, and retroaction module. It has been identified in circuit analysis that the proposed circuit is power efficient and has good robustness. Furthermore, the proposed circuit has promising applications for fire rescue robots.},
  archive      = {J_TCDS},
  author       = {Dongdong Xiong and Xiaoping Wang and Yiming Jiang and Chao Yang and Man Jiang and Jingang Lai and Zhigang Zeng},
  doi          = {10.1109/TCDS.2025.3540591},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1186-1198},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Memristor-based circuit of sequential associative memory with memory interactions and stimulus similarity effect},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RoboGPT: An LLM-based long-term decision-making embodied agent for instruction following tasks. <em>TCDS</em>, <em>17</em>(5), 1163-1174. (<a href='https://doi.org/10.1109/TCDS.2025.3543364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic agents are tasked with mastering common sense and making long-term sequential decisions to execute daily tasks based on natural language instructions. Recent advancements in large language models (LLMs) have catalyzed efforts for complex robotic planning. However, despite their superior generalization and comprehension capabilities, LLM task plans sometimes suffer from issues of accuracy and feasibility. To address these challenges, we propose RoboGPT,11For more details, please refer to our project page https://github.com/Cwb0106/RoboGPT. an embodied agent specifically designed to make long-term decisions for instruction following tasks. RoboGPT integrates three key modules: 1) RoboPlanner, an LLM-based planning module equipped with 67k embodied planning data, breaks down tasks into logical subgoals. We compile a new robotic dataset using a template feedback-based self-instruction method to fine-tune the Llama model. RoboPlanner with strong generalization can plan hundreds of instruction following tasks; 2) RoboSkill, customized for each subgoal to improve navigation and manipulation capabilities; and 3) Re-Plan, a module that dynamically adjusts the subgoals based on real-time environmental feedback. By utilizing the precise semantic map generated by RoboSkill, the target objects can be replaced by calculating the similarity between subgoals and the objects present in the environment. Experimental results demonstrate that RoboGPT exceeds the performance of other state-of-the-art (SOTA) methods, particularly LLM-based methods, in terms of task planning rationality for hundreds of unseen daily tasks and even tasks from other domains.},
  archive      = {J_TCDS},
  author       = {Yaran Chen and Wenbo Cui and Yuanwen Chen and Mining Tan and Xinyao Zhang and Jinrui Liu and Haoran Li and Dongbin Zhao and He Wang},
  doi          = {10.1109/TCDS.2025.3543364},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1163-1174},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {RoboGPT: An LLM-based long-term decision-making embodied agent for instruction following tasks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NuRF: Nudging the particle filter in radiance fields for robot visual localization. <em>TCDS</em>, <em>17</em>(5), 1153-1162. (<a href='https://doi.org/10.1109/TCDS.2025.3553261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can we localize a robot on a map only using monocular vision? This study presents neural radiance field (NuRF), an adaptive and nudged particle filter framework in radiance fields for six degree-of-freedom (6-DoF) robot visual localization. NuRF leverages recent advancements in radiance fields and visual place recognition. Conventional visual place recognition meets the challenges of data sparsity and artifact-induced inaccuracies. By utilizing radiance field-generated novel views, NuRF enhances visual localization performance and combines coarse global localization with the fine-grained pose tracking of a particle filter, ensuring continuous and precise localization. Experimentally, our method converges seven times faster than existing Monte Carlo-based methods and achieves localization accuracy within 1 m, offering an efficient and resilient solution for indoor visual localization.},
  archive      = {J_TCDS},
  author       = {Wugang Meng and Tianfu Wu and Huan Yin and Fumin Zhang},
  doi          = {10.1109/TCDS.2025.3553261},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1153-1162},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {NuRF: Nudging the particle filter in radiance fields for robot visual localization},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoSkill: Hierarchical open-ended skill acquisition for long-horizon manipulation tasks via language-modulated rewards. <em>TCDS</em>, <em>17</em>(5), 1141-1152. (<a href='https://doi.org/10.1109/TCDS.2025.3551298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A desirable property of generalist robots is the ability to both bootstrap diverse skills and solve new long-horizon tasks in open-ended environments without human intervention. Recent advancements have shown that large language models (LLMs) encapsulate vast-scale semantic knowledge about the world to enable long-horizon robot planning. However, they are typically restricted to reasoning high-level instructions and lack world grounding, which makes it difficult for them to coordinately bootstrap and acquire new skills in unstructured environments. To this end, we propose AutoSkill, a hierarchical system that empowers the physical robot to automatically learn to cope with new long-horizon tasks by growing an open-ended skill library without hand-crafted rewards. AutoSkill consists of two key components: 1) an in-context skill chain generation and new skill bootstrapping guided by LLMs that inform the robot of discrete and interpretable skill instructions for skill retrieval and augmentation within the skill library; and 2) a zero-shot language-modulated reward scheme in conjunction with a meta prompter facilitates online new skill acquisition via expert-free supervision aligned with proposed skill directives. Extensive experiments conducted in both simulated and realistic environments demonstrate AutoSkill's superiority over other LLM-based planners as well as hierarchical methods in expediting online learning for novel manipulation tasks.},
  archive      = {J_TCDS},
  author       = {Zhenyang Lin and Yurou Chen and Zhiyong Liu},
  doi          = {10.1109/TCDS.2025.3551298},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1141-1152},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {AutoSkill: Hierarchical open-ended skill acquisition for long-horizon manipulation tasks via language-modulated rewards},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bio-inspired goal-directed cognitive map model for robot navigation and exploration. <em>TCDS</em>, <em>17</em>(5), 1125-1140. (<a href='https://doi.org/10.1109/TCDS.2025.3552085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of a cognitive map (CM), or spatial map, was originally proposed to explain how mammals learn and navigate their environments. Over time, extensive research in neuroscience and psychology has established the CM as a widely accepted model. In this work, we introduce a new goal-directed cognitive map (GDCM) model that takes a nontraditional approach to spatial mapping for robot navigation and path planning. Unlike conventional models, GDCM does not require complete environmental exploration to construct a graph for navigation purposes. Inspired by biological navigation strategies, such as the use of landmarks, Euclidean distance, random motion, and reward-driven behavior. The GDCM can navigate complex, static environments efficiently without needing to explore the entire workspace. The model utilizes known cell types (head direction, speed, border, grid, and place cells) that constitute the CM, arranged in a unique configuration. Each cell model is designed to emulate its biological counterpart in a simple, computationally efficient way. Through simulation-based comparisons, this innovative CM graph-building approach demonstrates more efficient navigation than traditional models that require full exploration. Furthermore, GDCM consistently outperforms several established path planning and navigation algorithms by finding better paths.},
  archive      = {J_TCDS},
  author       = {Matthew A. Hicks and Tingjun Lei and Chaomin Luo and Daniel W. Carruth and Zhuming Bi},
  doi          = {10.1109/TCDS.2025.3552085},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1125-1140},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A bio-inspired goal-directed cognitive map model for robot navigation and exploration},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multitask reinforcement learning via task-specific action correction. <em>TCDS</em>, <em>17</em>(5), 1110-1124. (<a href='https://doi.org/10.1109/TCDS.2025.3543694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask reinforcement learning (MTRL) holds potential for building general-purpose agents, enabling them to generalize across a variety of tasks. However, MTRL may still be susceptible to conflicts between tasks. A primary reason for this problem is that a universal policy struggles to balance short-term and dense learning signals across various tasks, e.g., distinct reward functions in reinforcement learning. In social cognitive theory, internalized future goals, as a form of cognitive representations, can effectively mitigate potential short-term conflicts in multitask settings. Considering the benefits of future goals, we propose a novel and general framework called task-specific action correction (TSAC) from the goal perspective as an orthogonal research to previous MTRL methods. Specifically, to avoid myopia, TSAC introduces goal-oriented sparse rewards and decomposes policy learning into two separate policies: a shared policy (SP) and an action correction policy (ACP). The SP outputs a short-term perspective action based on guiding dense rewards. To alleviate conflicts resulting from excessive focus on specific tasks’ details in SP, the ACP incorporates goal-oriented sparse rewards, enabling an agent to adopt a long-term perspective to output a correction action and achieve generalization across tasks. Finally, the actions output by SP and ACP are combined based on the action correction function to form a final action that interact with the environment. Extensive experiments conducted on meta-world and multitask StarCraft II multiagent scenarios demonstrate that TSAC outperforms existing state-of-the-art methods, achieving significant improvements in sample efficiency, generalization and effective action execution across tasks.},
  archive      = {J_TCDS},
  author       = {Jinyuan Feng and Min Chen and Zhiqiang Pu and Tenghai Qiu and Jianqiang Yi and Jie Zhang},
  doi          = {10.1109/TCDS.2025.3543694},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1110-1124},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Efficient multitask reinforcement learning via task-specific action correction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two heads are better than one: Collaboration-oriented multiagent exploration system. <em>TCDS</em>, <em>17</em>(5), 1098-1109. (<a href='https://doi.org/10.1109/TCDS.2025.3530945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous exploration in unknown environments is a complex and formidable challenge that requires effective collaboration among multiple agents under partially observable conditions. Due to limited observations and inefficient collaboration, multiagent exploration often suffers from excessively long exploration paths. To address this issue, this article proposes a collaboration-oriented multiagent exploration system (COMAE). To effectively understand and leverage the interagent relationships, this article introduces collaboration-oriented observation (COO). In addition to the basic connectivity graph, the COO further constructs collaboration-oriented node features and an interaction graph to enhance the overall strategic understanding of multiagent. To improve collaboration among agents, this article designs an attention-based sequential network (ASN) to predict strategic actions. Additionally, a novel collaborative exploration reward (CER) is proposed to further prevent noncollaborative behaviors during the exploration process. Extensive experiments demonstrate that the proposed method enhances collaboration among agents and significantly reduces exploration distances.},
  archive      = {J_TCDS},
  author       = {Yang Liu and Peng Zhang and Hangyou Yu and Pingping Zhang and Jie Zhao and Dong Wang and Huchuan Lu},
  doi          = {10.1109/TCDS.2025.3530945},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1098-1109},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Two heads are better than one: Collaboration-oriented multiagent exploration system},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human–Robot sharing operation in cotransporting for nonholonomic mobile robot. <em>TCDS</em>, <em>17</em>(5), 1087-1097. (<a href='https://doi.org/10.1109/TCDS.2024.3493116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, human–robot sharing operation in cotransporting for a nonholonomic mobile robot is studied. Sharing operation weights of human and nonholonomic mobile robot are proposed to avoid dynamical obstacles in an unknown environment, which assigns operation weights to human and robot based on sensory information of environment, the balance of task between “following the human” and “autonomous obstacle avoidance” is further realized. A local multimodal obstacle avoidance method based on 2-D Lidar is proposed, which deals with the interference of unrelated obstacles. In addition, we designed a mechanism to estimate human motion intention and integrated it into the nonholonomic mobile robotic platform for following the human. The experimental results show that the proposed method can effectively deal with the problem of human–robot cotransporting for a nonholonomic mobile robot in an unknown environment, can avoid obstacle and ensure human's operation comfort in cotransporting.},
  archive      = {J_TCDS},
  author       = {Nan Feng and Xiong Guo and Xinbo Yu and Shuang Zhang and Wei He},
  doi          = {10.1109/TCDS.2024.3493116},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1087-1097},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Human–Robot sharing operation in cotransporting for nonholonomic mobile robot},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal perception for indoor mobile robotics navigation and safe manipulation. <em>TCDS</em>, <em>17</em>(5), 1074-1086. (<a href='https://doi.org/10.1109/TCDS.2024.3481457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor mobile robotics (IMR) has gained significant attention due to its potential applications in various domains, such as healthcare, logistics, and domestic assistance. However, navigating through indoor environments and performing safe manipulations still pose intractable challenges in terms of navigation accuracy and obstacle avoidance. To solve these issues, this article presents an artificial intelligence (AI) embodied multimodal perception framework for IMR intelligent navigation and safe manipulation. To ensure the navigation accuracy and robustness, we employ the complementary forward RGB camera, downward QR vision sensor, and wheel encoder measurements in a unified framework. The visual residuals and wheel odometry residuals are jointly minimized to estimate the robot states. To guarantee the safety of robotic manipulation tasks, we have developed an AI model that integrates transformer network with convolutional neural network, to associate the long-range RGB & depth patches and aggregate the multiscale obstacle features, enabling the precise detection and segmentation of obstacles in RGB-D images. Afterwards, the depths of detected obstacles are regressed, providing the robot with crucial information for collision avoidance. Eventually, we design a refined robot manipulation system that dynamically adjusts the robot behavior to ensure effective collision avoidance and to minimize potential damage to its mechanical components by constantly evaluating the spatial relationships between the robot and its surroundings. By incorporating advanced obstacle detection and the avoidance mechanism, mobile robots can navigate reliably in indoor environments with a reduced risk of collisions and real-time decision making. The presented method has been evaluated on the developed IMR platform. On the collected dataset, the estimated IMR absolute position and orientation errors are less than 0.18 m and 5${}^{\boldsymbol{\circ}}$, respectively. Besides, it achieves 89% $mAP$ on obstacle detection. The maximum of the estimated obstacle relative depth & orientation errors are less than 0.4 m and 2${}^{\boldsymbol{\circ}}$, respectively, which proves competitiveness against the state-of-the-art in both robot navigation and safe manipulation.},
  archive      = {J_TCDS},
  author       = {Yinlong Zhang and Yuanhao Liu and Shuai Liu and Wei Liang and Chu Wang and Kai Wang},
  doi          = {10.1109/TCDS.2024.3481457},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1074-1086},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal perception for indoor mobile robotics navigation and safe manipulation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CS-SLAM: A lightweight semantic SLAM method for dynamic scenarios. <em>TCDS</em>, <em>17</em>(5), 1061-1073. (<a href='https://doi.org/10.1109/TCDS.2024.3462651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SLAM systems typically rely on the assumption of scene rigidity. However, in real-world applications, robots often need to operate in dynamic environments, presenting unique challenges to the stability of SLAM systems. Efficient and lightweight SLAM systems play an important role in enabling interactions between robots and their environments. To enhance their applicability in dynamic environments, a lightweight semantic dynamic SLAM framework CS-SLAM has been proposed. First, the article designs a lightweight semantic segmentation network, Cross-SegNet, to remove dynamic feature points. This network includes a lightweight feature learning module, Cross Block, which effectively detects dynamic objects while maintaining a lightweight design, thereby improving the processing efficiency and accuracy of the SLAM system. Second, a spatiotemporal consistency-based auxiliary mask algorithm has been proposed, which compares the mask mapped from the previous frame to the current frame and the mask from the Cross-SegNet segmentation. By calculating the intersection over union (IoU), segmentation results are analyzed and supplemented to enhance the efficiency of removing dynamic feature points. Qualitative and quantitative evaluations on public datasets and real-world scenarios demonstrate the robustness and effectiveness of the proposed approach comparing to existing methods compared to existing methods.},
  archive      = {J_TCDS},
  author       = {Zhendong Guo and Na Dong and Zehui Zhang and Xiaoming Mai and Donghui Li},
  doi          = {10.1109/TCDS.2024.3462651},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1061-1073},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {CS-SLAM: A lightweight semantic SLAM method for dynamic scenarios},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Where to learn: Embodied perception learning planned by vision-language models. <em>TCDS</em>, <em>17</em>(5), 1050-1060. (<a href='https://doi.org/10.1109/TCDS.2025.3539665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied learning plays a crucial role in transferring self-learning agents to adapt to the environment. Existing embodied learning methods primarily rely on reinforcement learning (RL) exploration policy to collect inaccurate perceptual result samples for improving perceptual capabilities. However, RL-based exploration policies encounter several challenges such as the need for substantial data for training and the struggle to keep the diversity of the collected samples. In this article, we propose an embodied learning method that employs vision-language models (VLMs) as task planners, code planners, and path planners. Specifically, our method employs layout knowledge of the VLMs to decompose the embodied learning task into multiple subtasks and then convert each subtask into executable code, which will be executed to guide the agent to explore and collect the diverse samples in different types of rooms. Additionally, VLMs incorporate an external database to identify regions that enhance perceptual capabilities, and the agent will explore these poor perception regions to collect samples that can improve the perception performance. Experimental results demonstrate the effectiveness of our approach without the need for additional training.},
  archive      = {J_TCDS},
  author       = {Juan Wang and Di Guo and Huaping Liu},
  doi          = {10.1109/TCDS.2025.3539665},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1050-1060},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Where to learn: Embodied perception learning planned by vision-language models},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Special issue on embodied AI in indoor robotics: Bridging perception, interaction, and autonomy. <em>TCDS</em>, <em>17</em>(5), 1047-1049. (<a href='https://doi.org/10.1109/TCDS.2025.3595370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCDS},
  author       = {Yaran Chen and Chengguang Yang and Chaomin Luo and Dongbin Zhao},
  doi          = {10.1109/TCDS.2025.3595370},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1047-1049},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial: Special issue on embodied AI in indoor robotics: Bridging perception, interaction, and autonomy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
