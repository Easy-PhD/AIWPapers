<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPAMI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpami">TPAMI - 95</h2>
<ul>
<li><details>
<summary>
(2025). Graph spiking attention network: Sparsity, efficiency and robustness. <em>TPAMI</em>, <em>47</em>(11), 10862-10869. (<a href='https://doi.org/10.1109/TPAMI.2025.3593912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Graph Attention Networks (GATs) generally adopt the self-attention mechanism to learn graph edge attention, which usually return dense attention coefficients over all neighbors and thus are prone to be sensitive to graph edge noises. To overcome this problem, sparse GATs are desirable and have garnered increasing interest in recent years. However, existing sparse GATs usually suffer from high training complexity and are also not straightforward for inductive learning tasks. To address these issues, we propose to learn sparse GATs by exploiting spiking neuron (SN) mechanism, termed Graph Spiking Attention (GSAT). Specifically, it is known that spiking neuron can perform inexpensive information processing by transmitting the input data into discrete spike trains and return sparse outputs. Inspired by it, this work attempts to exploit spiking neuron to learn sparse attention coefficients, resulting in edge-sparsified graph for GNNs. Therefore, GSAT can perform message passing on the selective neighbors naturally, which makes GSAT perform compactly and robustly w.r.t graph noises. Moreover, GSAT can be used straightforwardly for inductive learning tasks. Extensive experiments on both transductive and inductive tasks demonstrate the effectiveness, robustness and efficiency of GSAT.},
  archive      = {J_TPAMI},
  author       = {Beibei Wang and Bo Jiang and Jin Tang and Lu Bai and Bin Luo},
  doi          = {10.1109/TPAMI.2025.3593912},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10862-10869},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Graph spiking attention network: Sparsity, efficiency and robustness},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the instance-dependent transition matrix estimation by exploiting self-supervised learning. <em>TPAMI</em>, <em>47</em>(11), 10848-10861. (<a href='https://doi.org/10.1109/TPAMI.2025.3595613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition matrix reveals the transition relationship between clean labels and noisy labels. It plays an important role in building statistically consistent classifiers for learning with noisy labels. However, in real-world applications, the transition matrix is usually unknown and has to be estimated. It is a challenging task to accurately estimate the transition matrix which usually depends on the instance. With both instances and noisy labels at hand, the major difficulty of estimating the transition matrix comes from the absence of clean label information. Recent work suggests that self-supervised learning methods can effectively infer clean label information. These methods could even achieve comparable performance with supervised learning on many benchmark datasets but without requiring any labels. Motivated by this, our paper presents a practical approach that harnesses self-supervised learning to extract clean label information, which reduces the estimation error of the instance-dependent transition matrix. By exploiting the estimated transition matrix, the performance of classifiers is improved. Empirical results on different datasets illustrate that our proposed methodology outperforms existing state-of-the-art methods in terms of both classification accuracy and transition matrix estimation.},
  archive      = {J_TPAMI},
  author       = {Yexiong Lin and Yu Yao and Zhaoqing Wang and Xu Shen and Jun Yu and Bo Han and Tongliang Liu},
  doi          = {10.1109/TPAMI.2025.3595613},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10848-10861},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Improving the instance-dependent transition matrix estimation by exploiting self-supervised learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VLPose: Bridging the domain gap in pose estimation with language-vision tuning. <em>TPAMI</em>, <em>47</em>(11), 10836-10847. (<a href='https://doi.org/10.1109/TPAMI.2025.3594097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to advances in deep learning techniques, Human Pose Estimation (HPE) has achieved significant progress in natural scenarios. However, these models perform poorly in artificial scenarios such as painting and sculpture due to the domain gap, constraining the development of virtual reality and augmented reality. With the growth of model size, retraining the whole model on both natural and artificial data is computationally expensive and inefficient. Our research aims to bridge the domain gap between natural and artificial scenarios with efficient tuning strategies. Leveraging the potential of language models, we enhance the adaptability of traditional pose estimation models across diverse scenarios with a novel framework called VLPose. VLPose leverages the synergy between language and vision to extend the generalization and robustness of pose estimation models beyond the traditional domains. Our approach has demonstrated improvements of 2.26% and 3.74% on HumanArt and MSCOCO, respectively, compared to state-of-the-art tuning strategies.},
  archive      = {J_TPAMI},
  author       = {Jingyao Li and Pengguang Chen and Xuan Ju and Shu Liu and Hong Xu and Jiaya Jia},
  doi          = {10.1109/TPAMI.2025.3594097},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10836-10847},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {VLPose: Bridging the domain gap in pose estimation with language-vision tuning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised skeleton representation learning via actionlet contrast and reconstruct. <em>TPAMI</em>, <em>47</em>(11), 10818-10835. (<a href='https://doi.org/10.1109/TPAMI.2025.3598138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has shown remarkable success in the domain of skeleton-based action recognition. However, the design of data transformations, which is crucial for effective contrastive learning, remains a challenging aspect in the context of skeleton-based action recognition. The difficulty lies in creating data transformations that capture rich motion patterns while ensuring that the transformed data retains the same semantic information. To tackle this challenge, we introduce an innovative framework called ActCLR+ (Actionlet-Dependent Contrastive Learning), which explicitly distinguishes between static and dynamic regions in a skeleton sequence. We begin by introducing the concept of actionlet, connecting self-supervised learning quantitatively with downstream tasks. Actionlets represent regions in the skeleton where features closely align with action prototypes, highlighting dynamic sequences as distinct from static ones. We propose an anchor-based method for unsupervised actionlet discovery, establishing a motion-adaptive data transformation approach based on this discovery. This motion-adaptive data transformation strategy tailors data transformations for actionlet and non-actionlet regions, respectively, introducing more diverse motion patterns while preserving the original motion semantics. Additionally, we incorporate a semantic-aware masked motion modeling technique to enhance the learning of actionlet representations. Our comprehensive experiments on well-established benchmark datasets such as NTU RGB+D and PKUMMD validate the effectiveness of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Lilang Lin and Jiahang Zhang and Jiaying Liu},
  doi          = {10.1109/TPAMI.2025.3598138},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10818-10835},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised skeleton representation learning via actionlet contrast and reconstruct},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISEARLE: Improving textual inversion for zero-shot composed image retrieval. <em>TPAMI</em>, <em>47</em>(11), 10801-10817. (<a href='https://doi.org/10.1109/TPAMI.2025.3593539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a query consisting of a reference image and a relative caption, Composed Image Retrieval (CIR) aims to retrieve target images visually similar to the reference one while incorporating the changes specified in the relative caption. The reliance of supervised methods on labor-intensive manually labeled datasets hinders their broad applicability to CIR. In this work, we introduce a new task, Zero-Shot CIR (ZS-CIR), that addresses CIR without the need for a labeled training dataset. We propose an approach, named iSEARLE (improved zero-Shot composEd imAge Retrieval with textuaL invErsion), that involves mapping the visual information of the reference image into a pseudo-word token in the CLIP token embedding space and combining it with the relative caption. To foster research on ZS-CIR, we present an open-domain benchmarking dataset named CIRCO (Composed Image Retrieval on Common Objects in context), the first CIR dataset where each query is labeled with multiple ground truths and a semantic categorization. The experimental results illustrate that iSEARLE obtains state-of-the-art performance on three different CIR datasets – FashionIQ, CIRR, and the proposed CIRCO – and two additional evaluation settings, namely domain conversion and object composition.},
  archive      = {J_TPAMI},
  author       = {Lorenzo Agnolucci and Alberto Baldrati and Alberto Del Bimbo and Marco Bertini},
  doi          = {10.1109/TPAMI.2025.3593539},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10801-10817},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ISEARLE: Improving textual inversion for zero-shot composed image retrieval},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DREAM: A dual variational framework for unsupervised graph domain adaptation. <em>TPAMI</em>, <em>47</em>(11), 10787-10800. (<a href='https://doi.org/10.1109/TPAMI.2025.3596054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph classification has been a prominent problem in graph machine learning fields. This problem has been investigated by leveraging message passing neural networks (MPNNs) to learn powerful graph representations. However, MPNNs extract topological semantics implicitly under label supervision, which could suffer from domain shift and label scarcity in unsupervised domain adaptation settings. In this paper, we propose an effective solution named Dual Variational Semantics Graph Mining (DREAM) for unsupervised graph domain adaptation by combining graph structural semantics from complementary perspectives. Besides a message passing branch to learn implicit semantics, our DREAM trains a path aggregation branch, which can provide explicit high-order structural semantics as a supplement. To train these two branches conjointly, we employ an expectation-maximization (EM) style variational framework for the maximization of likelihood. In the E-step, we fix the message passing branch and construct a graph-of-graph to indicate the geometric correlation between source and target domains, which would be adopted for the optimization of the other branch. In the M-step, we train the message passing branch and update the graph neural networks on the graph-of-graph with the other branch fixed. The alternative optimization improves the collaboration of knowledge from two branches. Extensive experiments on several benchmark datasets validate the superiority of the proposed DREAM compared with various baselines.},
  archive      = {J_TPAMI},
  author       = {Nan Yin and Li Shen and Mengzhu Wang and Xinwang Liu and Chong Chen and Xian-Sheng Hua},
  doi          = {10.1109/TPAMI.2025.3596054},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10787-10800},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DREAM: A dual variational framework for unsupervised graph domain adaptation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HPformer: Low-parameter transformer with temporal dependency hierarchical propagation for health informatics. <em>TPAMI</em>, <em>47</em>(11), 10770-10786. (<a href='https://doi.org/10.1109/TPAMI.2025.3593657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers based on Self-Attention (SA) mechanism have demonstrated unrivaled superiority in numerous areas. Compared to RNN-based networks, Transformers can learn the temporal dependency representation of an entire sequence in parallel, while efficiently dealing with long-range dependencies. However, the $\mathcal {O}(L^{2})$ ($L$ denotes the length of the sequence) computational complexity of the SA mechanism and the high memory usage make the construction cost of the Transformer-based model prohibitively expensive. To address these challenges, we propose a Transformer-like model, HPformer: Low-Parameter Transformer with Temporal Dependency Hierarchical Propagation. HPformer first chunks the sequence into $K$ ($K = \left\lceil \log {L} \right\rceil + 1$, $\left\lceil \cdot \right\rceil$ denotes ceiling operation) sequence segments, then leverages the hierarchical propagation mechanism with $\mathcal {O}(L)$ computational complexity to learn the temporal dependencies between the segments and within the segments, and ultimately generates $K$ vectors as $Key$ matrices. This reduces the complexity of the SA mechanism from $\mathcal {O}(L^{2})$ to $\mathcal {O}(L\log {L})$. In addition, we employ a strategy of sharing $Key$ and $Value$ matrices between layers to build the HPformer, thus reducing memory usage. Extensive experiments based on public health informatics benchmark and Long-Range Arena (LRA) benchmark have demonstrated that HPformer has advantages over Transformer-based models in terms of memory usage and efficiency.},
  archive      = {J_TPAMI},
  author       = {Wu Lee and Yuliang Shi and Han Yu and Lin Cheng and Xinjun Wang and Zhongmin Yan and Fanyu Kong},
  doi          = {10.1109/TPAMI.2025.3593657},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10770-10786},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HPformer: Low-parameter transformer with temporal dependency hierarchical propagation for health informatics},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward resolution mismatching: Modality-aware feature-aligned network for pan-sharpening. <em>TPAMI</em>, <em>47</em>(11), 10753-10769. (<a href='https://doi.org/10.1109/TPAMI.2025.3594898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panchromatic (PAN) and multi-spectral (MS) remote satellite image fusion, known as pan-sharpening, aims to produce high-resolution MS images by combining the complementary information from the high-resolution, texture-rich PAN and the low-resolution but high spectral-resolution MS counterparts. Despite notable advancements in this field, the current state-of-the-art pan-sharpening techniques do not explicitly address the spatial resolution mismatching problem between the two modalities of PAN and MS images. This mismatching issue can lead to misalignment in feature representation and the creation of blurry artifacts in the model output, ultimately hindering the generation of high-frequency textures and impeding the performance improvement of such methods. To address the aforementioned spatial resolution mismatching problem in pan-sharpening, we propose a novel modality-aware feature-aligned pan-sharpening framework in this paper. The framework comprises three primary stages: modality-aware feature extraction, modality-aware feature aligning, and context integrated image reconstruction. First, we introduce the half-instance normalization strategy as the backbone to filter out the inconsistent features and promote the learning of consistent features between the PAN and MS modalities. Second, a learnable modality-aware feature interpolation is devised to effectively address the misalignment issue. Specifically, the extracted features from the backbone are integrated to predict the transformation offsets of each pixel, which allows for the adaptive selection of custom contextual information and enables the modality-aware features to be more aligned. Finally, within the context of the interactive offset correction, multi-stage information is aggregated to generate the feasible pan-sharpened model output. Extensive experimental results over multiple satellite datasets demonstrate that the proposed algorithm outperforms other state-of-the-art methods both qualitatively and quantitatively, exhibiting great generalization ability to real-world scenes.},
  archive      = {J_TPAMI},
  author       = {Man Zhou and Xuanhua He and Danfeng Hong},
  doi          = {10.1109/TPAMI.2025.3594898},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10753-10769},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Toward resolution mismatching: Modality-aware feature-aligned network for pan-sharpening},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based point cloud compression: An in-depth survey and benchmark. <em>TPAMI</em>, <em>47</em>(11), 10731-10752. (<a href='https://doi.org/10.1109/TPAMI.2025.3594355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the maturity of 3D capture technology, the explosive growth of point cloud data has burdened the storage and transmission process. Traditional hybrid point cloud compression (PCC) tools relying on handcrafted priors have limited compression performance and are increasingly weak in addressing the burden induced by data growth. Recently, deep learning-based PCC methods have been introduced to continue to push the PCC performance boundary. With the thriving of deep PCC, the community urgently demands a systematic overview to conclude the past progress and present future research directions. In this paper, we have a detailed review that covers popular point cloud datasets, algorithm evolution, benchmarking analysis, and future trends. Concretely, we first introduce several widely-used PCC datasets according to their major properties. Then the algorithm evolution of existing studies on deep PCC, including lossy ones and lossless ones proposed for various point cloud types, is reviewed. Apart from academic studies, we also investigate the development of relevant international standards (i.e., MPEG standards and JPEG standards). To help have an in-depth understanding of the advance of deep PCC, we select a representative set of methods and conduct extensive experiments on multiple datasets. Comprehensive benchmarking comparisons and analysis reveal the pros and cons of previous methods. Finally, based on the profound analysis, we highlight the challenges and future trends of deep learning-based PCC, paving the way for further study.},
  archive      = {J_TPAMI},
  author       = {Wei Gao and Liang Xie and Songlin Fan and Ge Li and Shan Liu and Wen Gao},
  doi          = {10.1109/TPAMI.2025.3594355},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10731-10752},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep learning-based point cloud compression: An in-depth survey and benchmark},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human motion video generation: A survey. <em>TPAMI</em>, <em>47</em>(11), 10709-10730. (<a href='https://doi.org/10.1109/TPAMI.2025.3594034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans.},
  archive      = {J_TPAMI},
  author       = {Haiwei Xue and Xiangyang Luo and Zhanghao Hu and Xin Zhang and Xunzhi Xiang and Yuqin Dai and Jianzhuang Liu and Zhensong Zhang and Minglei Li and Jian Yang and Fei Ma and Zhiyong Wu and Changpeng Yang and Zonghong Dai and Fei Richard Yu},
  doi          = {10.1109/TPAMI.2025.3594034},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10709-10730},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Human motion video generation: A survey},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust density peaks clustering for manifold data with multiple peaks. <em>TPAMI</em>, <em>47</em>(11), 10696-10708. (<a href='https://doi.org/10.1109/TPAMI.2025.3594121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering (DPC) is an excellent clustering algorithm that does not need any prior knowledge. However, DPC still has the following shortcomings: (1) The Euclidean distance used by it is not applicable to manifold data with multiple peaks. (2) The local density calculation for DPC is too simple, and the final results may fluctuate due to the cutoff-distance dc. (3) Manually selected centers by decision-graph may lead to a wrong number of clusters and poor performance. To address these shortcomings and improve the performance, a robust density peaks clustering algorithm for manifold data with multiple peaks (RDPCM) is proposed to reduce the sensitivity of clustering results to parameters. Motivated by DPC-GD, RDPCM replaces the Euclidean distance with geodesic distance, which is optimized by the improved mutual K-nearest neighbors. It better considers the local manifold structure of the datasets and obtains excellent results. In addition, the Davies-Bouldin Index based on Minimum Spanning Tree (MDBI) is proposed to select the ideal number of classes adaptively. Numerous experiments have established that RDPCM is more effective and superior than other advanced clustering algorithms.},
  archive      = {J_TPAMI},
  author       = {Ling Ding and Chao Li and Shifei Ding and Xiao Xu and Lili Guo and Xindong Wu},
  doi          = {10.1109/TPAMI.2025.3594121},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10696-10708},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Robust density peaks clustering for manifold data with multiple peaks},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view hand reconstruction with a point-embedded transformer. <em>TPAMI</em>, <em>47</em>(11), 10680-10695. (<a href='https://doi.org/10.1109/TPAMI.2025.3598089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel and generalizable multi-view Hand Mesh Reconstruction (HMR) model, named POEM, designed for practical use in real-world hand motion capture scenarios. The advances of the POEM model consist of two main aspects. First, concerning the modeling of the problem, we propose embedding a static basis point within the multi-view stereo space. A point represents a natural form of 3D information and serves as an ideal medium for fusing features across different views, given its varied projections across these views. Consequently, our method harnesses a simple yet effective idea: a complex 3D hand mesh can be represented by a set of 3D basis points that 1) are embedded in the multi-view stereo, 2) carry features from the multi-view images, and 3) encompass the hand in it. The second advance lies in the training strategy. We utilize a combination of five large-scale multi-view datasets and employ randomization in the number, order, and poses of the cameras. By processing such a vast amount of data and a diverse array of camera configurations, our model demonstrates notable generalizability in the real-world applications. As a result, POEM presents a highly practical, plug-and-play solution that enables user-friendly, cost-effective multi-view motion capture for both left and right hands.},
  archive      = {J_TPAMI},
  author       = {Lixin Yang and Licheng Zhong and Pengxiang Zhu and Xinyu Zhan and Junxiao Kong and Jian Xu and Cewu Lu},
  doi          = {10.1109/TPAMI.2025.3598089},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10680-10695},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-view hand reconstruction with a point-embedded transformer},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SA3Det++: Side-aware quality estimation for semi-supervised 3D object detection. <em>TPAMI</em>, <em>47</em>(11), 10664-10679. (<a href='https://doi.org/10.1109/TPAMI.2025.3594086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised 3D object detection from point cloud aims to train a detector with a small number of labeled data and a large number of unlabeled data. Among existing methods, the pseudo-label based methods have achieved superior performance, and the core lies in how to select high-quality pseudo-labels with the designed quality evaluation criterion. Despite the success of these methods, they all consider the localization and classification quality estimation from a global perspective. For localization quality, they use a global score threshold to filter out low-quality pseudo-labels and assign equal importance to each side during training, ignoring the fact that sides with different localization quality should not be treat equally. Besides, a large number of pseudo-labels are discarded due to the high global threshold, which may also contain some correctly predicted sides that are helpful for model training. For the classification quality, they usually combine the objectness score and classification confidence score to filter out pseudo-labels. The main focus of them is designing effective classification confidence evaluation metrics, neglecting the importance of predicting better objectness score. In this paper, we propose SA3Det++, a side-aware quality estimation method for semi-supervised object detection, which consists of a probabilistic side localization strategy, a side-aware quality estimation strategy, and a soft pseudo-label selection strategy. Extensive results demonstrate that the proposed method consistently outperforms the baseline methods under different scenes and evaluation criterions.},
  archive      = {J_TPAMI},
  author       = {Wenfei Yang and Chuxin Wang and Tianzhu Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TPAMI.2025.3594086},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10664-10679},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SA3Det++: Side-aware quality estimation for semi-supervised 3D object detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GLC++: Source-free universal domain adaptation through global-local clustering and contrastive affinity learning. <em>TPAMI</em>, <em>47</em>(11), 10646-10663. (<a href='https://doi.org/10.1109/TPAMI.2025.3593669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify “known” data belonging to common categories and segregate them from target-private “unknown” data. We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of “unknown” data, impeding the identification of distinct “unknown” categories. To address this, we evolve GLC to GLC++, integrating a contrastive affinity learning strategy. We examine the superiority of GLC and GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by 16.8% and 18.9% in H-score on VisDA, respectively. GLC++ enhances the novel category clustering accuracy of GLC by 4.1% in open-set scenarios on Office-Home. Furthermore, the introduced contrastive learning strategy not only enhances GLC but also significantly facilitates existing methodologies.},
  archive      = {J_TPAMI},
  author       = {Sanqing Qu and Tianpei Zou and Florian Röhrbein and Cewu Lu and Guang Chen and Dacheng Tao and Changjun Jiang},
  doi          = {10.1109/TPAMI.2025.3593669},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10646-10663},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GLC++: Source-free universal domain adaptation through global-local clustering and contrastive affinity learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MTMamba++: Enhancing multi-task dense scene understanding via mamba-based decoders. <em>TPAMI</em>, <em>47</em>(11), 10633-10645. (<a href='https://doi.org/10.1109/TPAMI.2025.3593621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task dense scene understanding, which trains a model for multiple dense prediction tasks, has a wide range of application scenarios. Capturing long-range dependency and enhancing cross-task interactions are crucial to multi-task dense prediction. In this paper, we propose MTMamba++, a novel architecture for multi-task scene understanding featuring with a Mamba-based decoder. It contains two types of core blocks: self-task Mamba (STM) block and cross-task Mamba (CTM) block. STM handles long-range dependency by leveraging state-space models, while CTM explicitly models task interactions to facilitate information exchange across tasks. We design two types of CTM block, namely F-CTM and S-CTM, to enhance cross-task interaction from feature and semantic perspectives, respectively. Extensive experiments on NYUDv2, PASCAL-Context, and Cityscapes datasets demonstrate the superior performance of MTMamba++ over CNN-based, Transformer-based, and diffusion-based methods while maintaining high computational efficiency.},
  archive      = {J_TPAMI},
  author       = {Baijiong Lin and Weisen Jiang and Pengguang Chen and Shu Liu and Ying-Cong Chen},
  doi          = {10.1109/TPAMI.2025.3593621},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10633-10645},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MTMamba++: Enhancing multi-task dense scene understanding via mamba-based decoders},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decouple before align: Visual disentanglement enhances prompt tuning. <em>TPAMI</em>, <em>47</em>(11), 10619-10632. (<a href='https://doi.org/10.1109/TPAMI.2025.3594894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm, has showcased remarkable effectiveness in improving the task-specific transferability of vision-language models. This paper delves into a previously overlooked information asymmetry issue in PT, where the visual modality mostly conveys more context than the object-oriented textual modality. Correspondingly, coarsely aligning these two modalities could result in the biased attention, driving the model to merely focus on the context area. To address this, we propose DAPT, an effective PT framework based on an intuitive decouple-before-align concept. First, we propose to explicitly decouple the visual modality into the foreground and background representation via exploiting coarse-and-fine visual segmenting cues, and then both of these decoupled patterns are aligned with the original foreground texts and the hand-crafted background classes, thereby symmetrically strengthening the modal alignment. To further enhance the visual concentration, we propose a visual pull-push regularization tailored for the foreground-background patterns, directing the original visual representation towards unbiased attention on the region-of-interest object. We demonstrate the power of architecture-free DAPT through few-shot learning, base-to-novel generalization, and data-efficient learning, all of which yield superior performance across prevailing benchmarks.},
  archive      = {J_TPAMI},
  author       = {Fei Zhang and Tianfei Zhou and Jiangchao Yao and Ya Zhang and Ivor W. Tsang and Yanfeng Wang},
  doi          = {10.1109/TPAMI.2025.3594894},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10619-10632},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Decouple before align: Visual disentanglement enhances prompt tuning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified modality separation: A vision-language framework for unsupervised domain adaptation. <em>TPAMI</em>, <em>47</em>(11), 10604-10618. (<a href='https://doi.org/10.1109/TPAMI.2025.3597436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) enables models trained on a labeled source domain to handle new unlabeled domains. Recently, pre-trained vision-language models (VLMs) have demonstrated promising zero-shot performance by leveraging semantic information to facilitate target tasks. By aligning vision and text embeddings, VLMs have shown notable success in bridging domain gaps. However, inherent differences naturally exist between modalities, which is known as modality gap. Our findings reveal that direct UDA with the presence of modality gap only transfers modality-invariant knowledge, leading to suboptimal target performance. To address this limitation, we propose a unified modality separation framework that accommodates both modality-specific and modality-invariant components. During training, different modality components are disentangled from VLM features then handled separately in a unified manner. At test time, modality-adaptive ensemble weights are automatically determined to maximize the synergy of different components. To evaluate instance-level modality characteristics, we design a modality discrepancy metric to categorize samples into modality-invariant, modality-specific, and uncertain ones. The modality-invariant samples are exploited to facilitate cross-modal alignment, while uncertain ones are annotated to enhance model capabilities. Building upon prompt tuning techniques, our methods achieve up to 9% performance gain with 9 times of computational efficiencies. Extensive experiments and analysis across various backbones, baselines, datasets and adaptation settings demonstrate the efficacy of our design.},
  archive      = {J_TPAMI},
  author       = {Xinyao Li and Jingjing Li and Zhekai Du and Lei Zhu and Heng Tao Shen},
  doi          = {10.1109/TPAMI.2025.3597436},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10604-10618},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unified modality separation: A vision-language framework for unsupervised domain adaptation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CycleACR: Cycle modeling of actor-context relations for video action detection. <em>TPAMI</em>, <em>47</em>(11), 10588-10603. (<a href='https://doi.org/10.1109/TPAMI.2025.3595393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relation modeling between actors and scene context advances video action detection where the correlation of multiple actors makes their action recognition challenging. Existing studies model each actor and scene relation to improve action recognition. However, the scene variations and background interference limit their effectiveness. In this paper, we propose to select actor-related scene context, rather than directly laveraging raw video scenario, to improve relation modeling. We develop a Cycle Actor-Context Relation network (CycleACR) where there is a symmetric graph that models the actor and context relations in a bidirectional form. Specifically, our CycleACR is constituted of two modules: 1) Actor-to-Context Reorganization (A2C-R), which adaptively collects actor features for context feature reorganizations, and 2) Context-to-Actor Enhancement (C2A-E), which dynamically utilizes the reorganized context features for actor feature enhancement. Stacking multiple CycleACR modules is able to effectively capture the high-order relation and efficiently exchange useful information between actors and context. To fully exploit time-dependent and holistic context information, we further design a parallel local and global temporal context modeling branch. The outputs of the two branches are integrated as the final context-enhanced actor feature representations. Finally, we propose a context-aware memory bank for long-term relation modeling. The proposed bank can effectively store actor-related scene context from other clips without additional memory overhead. Compared to existing designs that focus on C2A-E, our CycleACR introduces the core design of A2C-R for more effective relation modeling. This cycle modeling enablesour CycleACR to achieve state-of-the-art performance on two popular action detection datasets: AVA (40.6 mAP) and UCF101-24 (84.7 mAP). We also provide ablation studies and visualizations to show how our cycle actor-context relation modeling improves video action detection.},
  archive      = {J_TPAMI},
  author       = {Lei Chen and Zhan Tong and Yibing Song and Gangshan Wu and Limin Wang},
  doi          = {10.1109/TPAMI.2025.3595393},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10588-10603},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CycleACR: Cycle modeling of actor-context relations for video action detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational and statistical guarantees for tensor-on-tensor regression with tensor train decomposition. <em>TPAMI</em>, <em>47</em>(11), 10577-10587. (<a href='https://doi.org/10.1109/TPAMI.2025.3593840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a tensor-on-tensor (ToT) regression model has been proposed to generalize tensor recovery, encompassing scenarios like scalar-on-tensor regression and tensor-on-vector regression. However, the exponential growth in tensor complexity poses challenges for storage and computation in ToT regression. To overcome this hurdle, tensor decompositions have been introduced, with the tensor train (TT)-based ToT model proving efficient in practice due to reduced memory requirements, enhanced computational efficiency, and decreased sampling complexity. Despite these practical benefits, a disparity exists between theoretical analysis and real-world performance. In this paper, we delve into the theoretical and algorithmic aspects of the TT-based ToT regression model. Assuming the regression operator satisfies the restricted isometry property (RIP), we conduct an error analysis for the solution to a constrained least-squares optimization problem. This analysis includes upper error bound and minimax lower bound, revealing that such error bounds polynomially depend on the order $N+M$. To efficiently find solutions meeting such error bounds, we propose two optimization algorithms: the iterative hard thresholding (IHT) algorithm (employing gradient descent with TT-singular value decomposition (TT-SVD)) and the factorization approach using the Riemannian gradient descent (RGD) algorithm. When RIP is satisfied, spectral initialization facilitates proper initialization, and we establish the linear convergence rate of both IHT and RGD. Notably, compared to the IHT, which optimizes the entire tensor in each iteration while maintaining the TT structure through TT-SVD and poses a challenge for storage memory in practice, the RGD optimizes factors in the so-called left-orthogonal TT format, enforcing orthonormality among most of the factors, over the Stiefel manifold, thereby reducing the storage complexity of the IHT. However, this reduction in storage memory comes at a cost: the recovery of RGD is worse than that of IHT, while the error bounds of both algorithms depend on $N+M$ polynomially. Experimental validation substantiates the validity of our theoretical findings.},
  archive      = {J_TPAMI},
  author       = {Zhen Qin and Zhihui Zhu},
  doi          = {10.1109/TPAMI.2025.3593840},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10577-10587},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Computational and statistical guarantees for tensor-on-tensor regression with tensor train decomposition},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMOv2: Pushing 5M vision model frontier. <em>TPAMI</em>, <em>47</em>(11), 10560-10576. (<a href='https://doi.org/10.1109/TPAMI.2025.3596776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5 M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the infrastructure for lightweight CNNs, but no counterparts have been recognized by attention-based design. Our work rethinks the lightweight infrastructure of efficient IRB and practical components in Transformer from a unified perspective, extending CNN-based IRB to attention-based models and abstracting a one-residual Meta Mobile Block (MMBlock) for lightweight model design. Following neat but effective design criterion, we deduce a modern Improved Inverted Residual Mobile Block (i$^{2}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>RMB) and improve a hierarchical Efficient MOdel (EMOv2) with no elaborate complex structures. Considering the imperceptible latency for mobile users when downloading models under 4 G/5 G bandwidth and ensuring model performance, we investigate the performance upper limit of lightweight models with a magnitude of 5 M. Extensive experiments on various vision recognition, dense prediction, and image generation tasks demonstrate the superiority of our EMOv2 over state-of-the-art methods, e.g., EMOv2-1 M/2M/5 M achieve 72.3, 75.8, and 79.4 Top-1 that surpass equal-order CNN-/Attention-based models significantly. At the same time, EMOv2-5 M equipped RetinaNet achieves 41.5 mAP for object detection tasks that surpasses the previous EMO-5 M by +2.6$\uparrow$ . When employing the more robust training recipe, our EMOv2-5M eventually achieves 82.9 Top-1 accuracy, which elevates the performance of 5M magnitude models to a new level.},
  archive      = {J_TPAMI},
  author       = {Jiangning Zhang and Teng Hu and Haoyang He and Zhucun Xue and Yabiao Wang and Chengjie Wang and Yong Liu and Xiangtai Li and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3596776},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10560-10576},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EMOv2: Pushing 5M vision model frontier},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BiBBDM: Bidirectional image translation with brownian bridge diffusion models. <em>TPAMI</em>, <em>47</em>(11), 10546-10559. (<a href='https://doi.org/10.1109/TPAMI.2025.3597667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the challenging realm of image-to-image translation, most traditional methods require separate models for different translation directions, leading to inefficient use of computational resources. This paper introduces the Bidirectional Brownian Bridge Diffusion Model (BiBBDM), a novel approach that leverages Brownian Bridge processes for bidirectional image-to-image translation. Unlike conventional Diffusion Models (DMs) that treat image-to-image translation as a unidirectional conditional generation process, BiBBDM models the translation as a stochastic Brownian Bridge process, enabling simultaneous learning of bidirectional translation between two domains. This innovation allows our method to achieve bidirectional image translation using different sampling directions of a single model, eliminating the need for multiple models for both translation directions. To the best of our knowledge, BiBBDM is the first image translation framework to achieve simultaneous dual-domain sampling with the same model and parameters, based on Brownian Bridge diffusion processes. Extensive experimental results on various benchmarks demonstrate that BiBBDM achieves competitive performance, as evidenced by both visual inspection and quantitative metrics.},
  archive      = {J_TPAMI},
  author       = {Kaitao Xue and Bo Li and Ziyi Liu and Zhifen He and Bin Liu and Congxuan Zhang and Yu-Kun Lai},
  doi          = {10.1109/TPAMI.2025.3597667},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10546-10559},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BiBBDM: Bidirectional image translation with brownian bridge diffusion models},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentangled dynamic intrusion detection. <em>TPAMI</em>, <em>47</em>(11), 10528-10545. (<a href='https://doi.org/10.1109/TPAMI.2025.3595671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, formingthe frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in attacks (e.g., 18% F1 for the MITM and 93% F1 for DDoS by a GCN-based state-of-the-art method), and perform poorly in few-shot intrusion detections (e.g., dramatically drops from 91% to 36% in 3D-IDS, and drops from 89% to 20% in E-GraphSAGE). We reveal that the underlying cause is entangled distributions of flow features. This motivates us to propose DIDS-MFL, a disentangled intrusion detection approach for various scenarios. DIDS-MFL involves two key components: a double Disentanglement-based Intrusion Detection System (DIDS) and a plug-and-play Multi-scale Few-shot Learning-based (MFL) intrusion detection module. Specifically, the proposed DIDS first disentangles traffic features by a non-parameterized optimization, automatically differentiating tens and hundreds of complex features. Such differentiated features will be further disentangled to highlight the attack-specific features. Our DIDS additionally uses a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. Furthermore, the proposed MFL involves an alternating optimization framework to address the entangled representations in few-shot traffic threats with rigorous derivation. MFL first captures multi-scale information in latent space to distinguish attack-specific information and then optimizes the disentanglement term to highlight the attack-specific information. Finally, MFL fuses and alternately solves them in an end-to-end way. To the best of our knowledge, DIDS-MFL takes the first step toward disentangled dynamic intrusion detection under various attack scenarios. Equipped with DIDS-MFL, administrators can effectively identify various attacks in encrypted traffic, including known, unknown, and few-shot threats that are not easily detected. Comprehensive experiments show the superiority of our proposed DIDS-MFL. For few-shot NIDS, our DIDS-MFL achieves a 71.91% –125.19% improvement in average F1-score over 14 baselines and shows versatility in multiple baselines and multiple tasks.},
  archive      = {J_TPAMI},
  author       = {Chenyang Qiu and Guoshun Nan and Hongrui Xia and Zheng Weng and Xueting Wang and Meng Shen and Xiaofeng Tao and Jun Liu},
  doi          = {10.1109/TPAMI.2025.3595671},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10528-10545},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Disentangled dynamic intrusion detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-task relation-aware consistency for weakly supervised temporal action detection. <em>TPAMI</em>, <em>47</em>(11), 10513-10527. (<a href='https://doi.org/10.1109/TPAMI.2025.3594178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action detection aims to predict temporal boundaries and category labels of actions in untrimmed videos. In the past years, many weakly supervised temporal action detection methods have been proposed to relieve the annotation cost of fully supervised methods. Due to the discrepancy between action localization and action classification, the two-branch structure is widely adopted by existing weakly supervised methods, where the classification branch is used to predict category-wise score and the localization branch is used to predict foreground score for each segment. Under the weakly supervised setting, the model training is mainly guided by the video-level or sparse segment-level annotations. As a result, the classification branch tends to focus on the most discriminative segments while ignore less discriminative ones so as to minimize the classification cost, and the localization branch may assign high foreground scores for some negative segments. This phenomenon can severely damage the action detection performance, because the foreground scores and classification scores are combined together in the testing stage for action detection. To deal with this problem, several methods have been proposed to encourage the consistency between the classification branch and localization branch. However, these methods only consider the video-level or segment-level consistency, without considering the relation among different segments to be consistent. In this paper, we propose a Cross-Task Relation-Aware Consistency (CRC) strategy for weakly supervised temporal action detection, including an intra-video consistency module and an inter-video consistency module. The intra-video consistency module can well guarantee the relationship among segments from the same video to be consistent, and the inter-video consistency module guarantees the relationship among segments from different videos to be consistent. These two modules are complementary to each other by combining both intra-video and inter-video consistency. Experimental results show that the proposed CRC strategy can consistently improve the performance of existing weakly supervised methods, including click-level supervised methods (e.g., LACP Lee et al., 2021), video-level supervised methods (e.g., DELU Chen et al., 2022) and unsupervised methods (e.g., BaS-Net Lee et al., 2020), verifying the generality and effectiveness of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Wenfei Yang and Huan Ren and Tianzhu Zhang and Zhe Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TPAMI.2025.3594178},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10513-10527},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Cross-task relation-aware consistency for weakly supervised temporal action detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Out-of-distribution generalization on graphs: A survey. <em>TPAMI</em>, <em>47</em>(11), 10490-10512. (<a href='https://doi.org/10.1109/TPAMI.2025.3593897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph machine learning has been extensively studied in both academia and industry. Although booming with a vast number of emerging methods and techniques, most of the literature is built on the in-distribution hypothesis, i.e., testing and training graph data are identically distributed. However, this in-distribution hypothesis can hardly be satisfied in many real-world graph scenarios where the model performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this critical problem, out-of-distribution (OOD) generalization on graphs, which goes beyond the in-distribution hypothesis, has made great progress and attracted ever-increasing attention from the research community. In this paper, we comprehensively survey OOD generalization on graphs and present a detailed review of recent advances in this area. First, we provide a formal problem definition of OOD generalization on graphs. Second, we categorize existing methods into three classes from conceptually different perspectives, i.e., data, model, and learning strategy, based on their positions in the graph machine learning pipeline, followed by detailed discussions for each category. We also review the theories related to OOD generalization on graphs and introduce the commonly used graph datasets for thorough evaluations. Finally, we share our insights on future research directions.},
  archive      = {J_TPAMI},
  author       = {Haoyang Li and Xin Wang and Ziwei Zhang and Wenwu Zhu},
  doi          = {10.1109/TPAMI.2025.3593897},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10490-10512},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Out-of-distribution generalization on graphs: A survey},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative novel object discovery and box-guided cross-modal alignment for open-vocabulary 3D object detection. <em>TPAMI</em>, <em>47</em>(11), 10475-10489. (<a href='https://doi.org/10.1109/TPAMI.2025.3593580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary 3D Object Detection (OV-3DDet) addresses the detection of objects from an arbitrary list of novel categories in 3D scenes, which remains a very challenging problem. In this work, we propose CoDAv2, a unified framework designed to innovatively tackle both the localization and classification of novel 3D objects, under the condition of limited base categories. For localization, the proposed 3D Novel Object Discovery (3D-NOD) strategy utilizes 3D geometries and 2D open-vocabulary semantic priors to discover pseudo labels for novel objects during training. 3D-NOD is further extended with an Enrichment strategy that significantly enriches the novel object distribution in the training scenes, and then enhances the model’s ability to localize more novel objects. The 3D-NOD with Enrichment is termed 3D-NODE. For classification, the Discovery-driven Cross-modal Alignment (DCMA) module aligns features from 3D point clouds and 2D/textual modalities, employing both class-agnostic and class-specific alignments that are iteratively refined to handle the expanding vocabulary of objects. Besides, 2D box guidance boosts the classification accuracy against complex background noises, which is coined as Box-DCMA. Extensive evaluation demonstrates the superiority of CoDAv2. CoDAv2 outperforms the best-performing method by a large margin ($\text{AP}_{Novel}$ of 9.17 vs. 3.61 on SUN-RGBD and 9.12 vs. 3.74 on ScanNetv2).},
  archive      = {J_TPAMI},
  author       = {Yang Cao and Yihan Zeng and Hang Xu and Dan Xu},
  doi          = {10.1109/TPAMI.2025.3593580},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10475-10489},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Collaborative novel object discovery and box-guided cross-modal alignment for open-vocabulary 3D object detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FiGVCL: Fine-grained benchmark and method for video copy localization. <em>TPAMI</em>, <em>47</em>(11), 10457-10474. (<a href='https://doi.org/10.1109/TPAMI.2025.3595625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-based video copy localization (VCL) aims to detect and locate copied segments in pairs of videos. VCL requires fine-grained video analysis to robustly identify copied segments that have been edited. Despite recent progress, the prohibitive cost of annotating copied segments and the lack of a fine-grained benchmark hinder the development of effective VCL systems. In this work, we annotate a new real-world dataset, FiGVCL, with challenging scenarios designed to evaluate VCL methods. FiGVCL is carefully annotated to preserve the temporal correspondences observed in copied segments. Moreover, we propose a novel fine-grained VCL benchmark metric based on temporal correspondences to improve discriminability. Finally, we design a simple but effective baseline model that uses fine-grained local embeddings for accurate copied segment localization. We also present an unsupervised training strategy that outperforms previous supervised VCL methods.},
  archive      = {J_TPAMI},
  author       = {Wenyang Luo and Yufan Liu and Bing Li and Weiming Hu and Stephen Maybank},
  doi          = {10.1109/TPAMI.2025.3595625},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10457-10474},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FiGVCL: Fine-grained benchmark and method for video copy localization},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constraint-aware zero-shot vision-language navigation in continuous environments. <em>TPAMI</em>, <em>47</em>(11), 10441-10456. (<a href='https://doi.org/10.1109/TPAMI.2025.3594204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the task of Vision-Language Navigation in Continuous Environments (VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly challenging due to the absence of expert demonstrations for training and minimal environment structural prior to guide navigation. To confront these challenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes zero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion process. CA-Nav continuously translates sub-instructions into navigation plans using two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and the Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria for decomposed sub-instructions as constraints and tracks navigation progress by switching sub-instructions in a constraint-aware manner. CVM, guided by CSM’s constraints, generates a value map on the fly and refines it using superpixel clustering to improve navigation stability. CA-Nav achieves the state-of-the-art performance on two VLN-CE benchmarks, surpassing the previous best method by 12% and 13% in Success Rate on the validation unseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates its effectiveness in real-world robot deployments across various indoor scenes and instructions.},
  archive      = {J_TPAMI},
  author       = {Kehan Chen and Dong An and Yan Huang and Rongtao Xu and Yifei Su and Yonggen Ling and Ian Reid and Liang Wang},
  doi          = {10.1109/TPAMI.2025.3594204},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10441-10456},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Constraint-aware zero-shot vision-language navigation in continuous environments},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward few-shot learning in the open world: A review and beyond. <em>TPAMI</em>, <em>47</em>(11), 10420-10440. (<a href='https://doi.org/10.1109/TPAMI.2025.3594686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human intelligence is characterized by our ability to absorb and apply knowledge from the world around us, especially in rapidly acquiring new concepts from minimal examples, underpinned by prior knowledge. Few-shot learning (FSL) aims to mimic this capacity by enabling significant generalizations and transferability. However, traditional FSL frameworks often rely on assumptions of clean, complete, and static data, conditions that are seldom met in real-world environments. Such assumptions falter in the inherently uncertain, incomplete, and dynamic contexts of the open world. This paper presents a comprehensive review of recent advancements designed to adapt FSL to open-world environments. We categorize existing methods into three distinct types of FSL in the open world: those involving varying instances, varying classes, and varying distributions. Each category is discussed in terms of its specific challenges and methods, as well as its strengths and weaknesses. We standardize experimental settings and metric benchmarks across scenarios and provide a comparative analysis of the performance of various methods. In conclusion, we outline potential future research directions for this evolving field. It is our hope that this review will catalyze further development of effective solutions to these complex challenges, thereby advancing the field of artificial intelligence.},
  archive      = {J_TPAMI},
  author       = {Hui Xue and Yuexuan An and Yongchun Qin and Wenqian Li and Yixin Wu and Yongjuan Che and Pengfei Fang and Min-Ling Zhang},
  doi          = {10.1109/TPAMI.2025.3594686},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10420-10440},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Toward few-shot learning in the open world: A review and beyond},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OpenCIR: Conditional image repainting with open condition mixture. <em>TPAMI</em>, <em>47</em>(11), 10406-10419. (<a href='https://doi.org/10.1109/TPAMI.2025.3597936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce OpenCIR, a fully-functional Conditional Image Repainting (CIR) model designed for local image editing. Given an image and a combination of conditions related to geometry, texture, and color, CIR models are required to repaint instances and seamlessly composite them with the original images. Previous CIR models suffer from limited object categories, restricted condition modalities, and demanded geometry precision. In contrast, leveraging the generative priors from pre-trained models, OpenCIR could repaint open object categories. Equipped with redesigned condition injection modules and the condition extension strategy, OpenCIR is able to understand open condition modalities. Adopting the contour refinement strategy, OpenCIR allows users to specify instances with open geometry precision. In addition, we contribute the Open-CIR dataset, which includes detailed annotations, tailored for the comprehensive training and evaluation of the OpenCIR model. Extensive experiments demonstrate that OpenCIR outperforms relevant state-of-the-art methods, achieving superior visual quality, and more favorable results by human evaluators.},
  archive      = {J_TPAMI},
  author       = {Shuchen Weng and Xiaocheng Gong and Haojie Zheng and Xinlong Wang and Si Li and Boxin Shi},
  doi          = {10.1109/TPAMI.2025.3597936},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10406-10419},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {OpenCIR: Conditional image repainting with open condition mixture},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-based visual microphone based on specular reflections off angularly deformed surfaces. <em>TPAMI</em>, <em>47</em>(11), 10396-10405. (<a href='https://doi.org/10.1109/TPAMI.2025.3595008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and demonstrate the event-based visual microphone (EBVM), a passive electro-optical technique for remotely capturing audio signals using an event-based camera without any use of a conventional microphone. The event-based camera records local angular deformations of a surface induced by the sound propagation by observing the changes in the specular reflections at each pixel. By interpreting the timings of the specular incidences deduced from the event stream as signal level-crossings, we reconstruct the audio signal by imposing short-time Fourier sparsity conditions. The recovered audio signal is qualitatively comparable to or better than the prior art (intensity-based visual microphone), while simultaneously expanding the field of view by approximately 25 times and reducing data volume by three orders of magnitude. The proposed EBVM was tested on speech signal reconstruction as well as novel event-based acousto-optical passive ranging.},
  archive      = {J_TPAMI},
  author       = {Matthew Howard and Ryan Jones and Keigo Hirakawa},
  doi          = {10.1109/TPAMI.2025.3595008},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10396-10405},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Event-based visual microphone based on specular reflections off angularly deformed surfaces},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SeCoV2: Semantic connectivity-driven pseudo-labeling for robust cross-domain semantic segmentation. <em>TPAMI</em>, <em>47</em>(11), 10378-10395. (<a href='https://doi.org/10.1109/TPAMI.2025.3596943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo-labeling is a dominant strategy for cross-domain semantic segmentation (CDSS), yet its effectiveness is limited by fragmented and noisy pixel-level predictions under severe domain shifts. To address this, we propose a semantic connectivity-driven pseudo-labeling framework, SeCo, which constructs and refines pseudo-labels at the connectivity level by aggregating high-confidence pixels into coherent semantic regions. The framework includes two key components: Pixel Semantic Aggregation (PSA), which leverages a dual prompting strategy to preserve category-specific granularity, and Semantic Connectivity Correction with Loss Distribution (SCC-LD), which filters noisy regions based on early-loss statistics. Building upon this foundation, we further present SeCoV2, which introduces SCC-Unc, a novel uncertainty-aware correction module that constructs a connectivity graph and enforces relational consistency for robust refinement in ambiguous regions. SeCoV2 also broadens the applicability of SeCo by extending evaluation to more challenging scenarios, including open-set and multimodal adaptation, semi-supervised domain generalization, and by validating compatibility with different interactive foundation segmentation models such as SAM Kirillov et al. 2023, SEEM Zou et al. 2023, and Fast-SAM Zhao et al. 2023. Extensive experiments across six CDSS tasks demonstrate that SeCoV2 achieves consistent improvements over previous methods, with an average performance gain of up to +4.6%, establishing new state-of-the-art results. These findings highlight the effectiveness and generalization ability for robust adaptation in diverse real-world environments.},
  archive      = {J_TPAMI},
  author       = {Dong Zhao and Qi Zang and Nan Pu and Shuang Wang and Nicu Sebe and Zhun Zhong},
  doi          = {10.1109/TPAMI.2025.3596943},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10378-10395},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SeCoV2: Semantic connectivity-driven pseudo-labeling for robust cross-domain semantic segmentation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dubbing movies via hierarchical phoneme modeling and acoustic diffusion denoising. <em>TPAMI</em>, <em>47</em>(11), 10361-10377. (<a href='https://doi.org/10.1109/TPAMI.2025.3597267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a piece of text, a video clip, and reference audio, the movie dubbing (also known as Visual Voice Cloning, V2C) task aims to generate speeches that clone reference voice and align well with the video in both emotion and lip movement, which is more challenging than conventional text-to-speech synthesis tasks. To align the generated speech with the inherent lip motion of the given silent video, most existing works utilize each video frame to query textual phonemes. However, such an attention operation usually leads to mumble speech because different phonemes are fused for video frames corresponding to one phoneme (video frames are finer-grained than phonemes). To address this issue, we propose a diffusion-based movie dubbing architecture, which improves pronunciation by Hierarchical Phoneme Modeling (HPM) and generates better mel-spectrogram through Acoustic Diffusion Denoising (ADD). We term our model as HD-Dubber. Specifically, our HPM bridges the visual information and corresponding speech prosody from three aspects: (1) aligning lip movement with the speech duration based on each phoneme unit by contrastive learning; (2) conveying facial expression to phoneme-level energy and pitch; and (3) injecting global emotions captured from video scenes into prosody. On the other hand, ADD exploits a denoising diffusion framework to transform the noise signal into a mel-spectrogram via a parameterized Markov chain conditioned on textual phonemes and reference audio. ADD has two novel denoisers, the Style-adaptive Residual Denoiser (SRD) and the Phoneme-enhanced U-net Denoiser (PUD), to enhance speaker similarity and improve pronunciation quality. Extensive experimental results on the three benchmark datasets demonstrate the state-of-the-art performance of the proposed method. The source code and trained models will be made available to the public.},
  archive      = {J_TPAMI},
  author       = {Liang Li and Gaoxiang Cong and Yuankai Qi and Zheng-Jun Zha and Qi Wu and Quan Z. Sheng and Qingming Huang and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2025.3597267},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10361-10377},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dubbing movies via hierarchical phoneme modeling and acoustic diffusion denoising},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning an adaptive sparse transformer for efficient image restoration. <em>TPAMI</em>, <em>47</em>(11), 10344-10360. (<a href='https://doi.org/10.1109/TPAMI.2025.3594910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based approaches have shown promising performance in image restoration tasks due to their ability to model long-range dependencies, which are essential for recovering clear images. Although various efficient attention mechanisms have been proposed to address the intensive computational loads of transformers, they often suffer from redundant information and noisy interactions from irrelevant regions, as they consider all available tokens. In this work, we propose an Adaptive Sparse Transformer (AST-v2) to mitigate these issues by reducing noisy interactions in irrelevant areas and removing feature redundancy along channel dimension. AST-v2 incorporates two core components: an Adaptive Sparse Self-Attention (ASSA) block and a Feature Refinement Feed-forward Network (FRFN). ASSA adopts a dual-branch design, where the sparse branch guides the modulation of standard dense attention weights. This paradigm reduces the negative impact of irrelevant token interactions while preserving the important ones. Meanwhile, FRFN utilizes an enhance-and-ease scheme to eliminate feature redundancy across channels, enhancing the restoration of clear images. Experimental results on commonly used benchmarks show the competitive performance of our method for 6 restoration tasks, including rain streak removal, haze removal, shadow removal, snow removal, blur removal, and low-light enhancement. The code is available in the supplementary materials.},
  archive      = {J_TPAMI},
  author       = {Shihao Zhou and Jinshan Pan and Jufeng Yang},
  doi          = {10.1109/TPAMI.2025.3594910},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10344-10360},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning an adaptive sparse transformer for efficient image restoration},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rotation- and permutation-equivariant quantum graph neural network for 3D graph data. <em>TPAMI</em>, <em>47</em>(11), 10329-10343. (<a href='https://doi.org/10.1109/TPAMI.2025.3593371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Equivariant quantum graph neural networks (EQGNNs) offer a potentially powerful method to process graph data. However, existing EQGNN models only consider the permutation symmetry of graphs, and failing to fully exploit the geometric and non-geometric information in graphs, resulting in suboptimal performance when processing 3D graph data. To address these limitations, we derive constraints of rotation and permutation equivariance, and then propose a novel rotation- and permutation-equivariant quantum graph neural network (RP-EQGNN). An equivariant module is designed to extract the geometric information. Then, a convolution and entanglement module is constructed to extract non-geometric information. To improve performance of our model, an edge entanglement strategy is designed to perform distinguishable entanglement operations based on edge heterogeneity. The experiment results demonstrate that RP-EQGNN is significantly better for graph regression on the QM9 dataset and the OC20 dataset than Q3DGL and EQC in MAE and achieves results comparable to those of EquiformerV2, Geoformer, SO3KRATES and HEGNN. It also has advantage for point cloud classification on the ModelNet40 dataset over quantum models, including sQCNN-3D and PI-QSVM. RP-EQGNN introduces an innovative approach for processing 3D graph data, establishing a basis for future investigations into symmetries within graph neural networks.},
  archive      = {J_TPAMI},
  author       = {Wenjie Liu and Yifan Zhu and Ying Zha and Qingshan Wu and Lei Jian and Zhihao Liu},
  doi          = {10.1109/TPAMI.2025.3593371},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10329-10343},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Rotation- and permutation-equivariant quantum graph neural network for 3D graph data},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Replay master: Automatic sample selection and effective memory utilization for continual semantic segmentation. <em>TPAMI</em>, <em>47</em>(11), 10311-10328. (<a href='https://doi.org/10.1109/TPAMI.2025.3594040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual Semantic Segmentation (CSS) extends static semantic segmentation by incrementally introducing new classes for training. To alleviate the catastrophic forgetting issue in this task, replay methods can be adopted, constructing a memory buffer that stores a small number of samples from previous classes for future replay. However, existing replay approaches in CSS often lack a thorough exploration of two critical issues: how to find the most suitable memory samples and how to utilize them for replay more effectively. Common strategies either randomly select samples or rely on hand-crafted, single-factor-driven methods that are hard to be optimal, and often employ conventional training techniques for replay that do not account for class imbalance problem resulting from limited memory capacity. In this work, we tackle these challenges by introducing a novel memory sample selection method that leverages a reinforcement learning framework with innovative state representations and a dual-stage action scheme to automatically learn a selection policy. Additionally, we propose an expert mechanism and a dual-phase training method to address the class imbalance issue, thereby enhancing the effectiveness of replay training by making better use of memory samples. Incorporating the proposed automatic sample selection and effective memory utilization methods, we develop a novel and effective replay-based pipeline for CSS. Our extensive experiments on Pascal VOC 2012 and ADE20 K datasets demonstrate the effectiveness of our approach, which achieves state-of-the-art (SOTA) performance and outperforms previous advanced methods significantly.},
  archive      = {J_TPAMI},
  author       = {Lanyun Zhu and Tianrun Chen and Jianxiong Yin and Simon See and De Wen Soh and Jun Liu},
  doi          = {10.1109/TPAMI.2025.3594040},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10311-10328},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Replay master: Automatic sample selection and effective memory utilization for continual semantic segmentation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing 3D object detection with depth-aware spatial knowledge distillation. <em>TPAMI</em>, <em>47</em>(11), 10295-10310. (<a href='https://doi.org/10.1109/TPAMI.2025.3594805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate 3D object detection from images can be hindered by inherent depth ambiguity. While knowledge distillation (KD) from privileged sensors such as LiDAR offers a promising direction, it often suffers from a critical cross-sensor domain gap. To address this, we introduce DK3D, a novel depth-aware knowledge distillation framework for 3D detection. Our core strategy involves providing the teacher with privileged ground-truth depth during training. This directly avoids the feature representation mismatch and subsequent inefficient knowledge transfer required when distilling from a LiDAR teacher (sparse, geometric) to a camera-based student (dense, semantic). DK3D introduces specialized modules tailored for two primary student paradigms. For depth-assisted models, we employ a channel-wise projection layer (CPL) and an adversarial scoring block (ASB) to align intermediate features at both the pixel and distribution levels. For depth-independent models, a novel vision-depth association module allows the student to implicitly reason about geometry by fusing depth cues with visual features. Both approaches are further enhanced by target-aware spatial response distillation, which captures complex inter-object spatial relationships. Extensive experiments on the KITTI and nuScenes benchmarks demonstrate that DK3D significantly improves performance for both monocular and multi-view 3D detection, outperforming state-of-the-art methods. As a versatile, plug-and-play framework, DK3D boosts existing models without requiring additional training data or increasing the computational cost at inference.},
  archive      = {J_TPAMI},
  author       = {Zizhang Wu and Fan Song and Yuanzhu Gan and Yunzhe Wu and Tianhao Xu and Xiaoquan Wang and Rui Tang and Jian Pu},
  doi          = {10.1109/TPAMI.2025.3594805},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10295-10310},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Advancing 3D object detection with depth-aware spatial knowledge distillation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniAV: Unified audio-visual perception for multi-task video event localization. <em>TPAMI</em>, <em>47</em>(11), 10280-10294. (<a href='https://doi.org/10.1109/TPAMI.2025.3593932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video event localization tasks include temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL). Existing methods tend to over-specialize on individual tasks, neglecting the equal importance of these different events for a complete understanding of video content. In this work, we aim to develop a unified framework to solve TAL, SED and AVEL tasks together to facilitate holistic video understanding. However, it is challenging since different tasks emphasize distinct event characteristics and there are substantial disparities in existing task-specific datasets (size/domain/duration). It leads to unsatisfactory results when applying a naive multi-task strategy. To tackle the problem, we introduce UniAV, a Unified Audio-Visual perception network to effectively learn and share mutually beneficial knowledge across tasks and modalities. Concretely, we propose a unified audio-visual encoder to derive generic representations from multiple temporal scales for videos from all tasks. Meanwhile, task-specific experts are designed to capture the unique knowledge specific to each task. Besides, instead of using separate prediction heads, we develop a novel unified language-aware classifier by utilizing semantic-aligned task prompts, enabling our model to flexibly localize various instances across tasks with an impressive open-set ability to localize novel categories. Extensive experiments demonstrate that UniAV, with its unified architecture, significantly outperforms both single-task models and the naive multi-task baseline across all three tasks. It achieves superior or on-par performances compared to the state-of-the-art task-specific methods on ActivityNet 1.3, DESED and UnAV-100 benchmarks.},
  archive      = {J_TPAMI},
  author       = {Tiantian Geng and Teng Wang and Jinming Duan and Yanfu Zhang and Weili Guan and Feng Zheng and Ling Shao},
  doi          = {10.1109/TPAMI.2025.3593932},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10280-10294},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {UniAV: Unified audio-visual perception for multi-task video event localization},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPL++: Advancing the network performance via image and label perturbations. <em>TPAMI</em>, <em>47</em>(11), 10262-10279. (<a href='https://doi.org/10.1109/TPAMI.2025.3594149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in supervised learning have predominantly focused on regularizations, optimizers, and architectures, yet the potential of simultaneously optimizing data distributions and supervisory signals for training samples remains underexplored. In this paper, we propose a novel paradigm that leverages the benefits of image perturbations for rectifying data distributions. Our method, called DPL (Deep Perturbation Learning), introduces new insights into utilizing image perturbations and focuses on improving generalizability on normal samples, rather than resisting adversarial attacks. DPL formulates a differentiable function w.r.t. image perturbations and implements an alternative optimization process that seamlessly integrates with downstream tasks. However, the limitations of DPL stem from the inefficiency in employing differentiable targets caused by the exclusive optimization of image perturbations, while neglecting the critical role of supervisory signals in training effectiveness. These lead to the excessive necessity of DPL iterations and yield inferior performance-cost trade-off. To track this, we extend DPL to DPL++ with synchronous optimization for image perturbations and label perturbations. In our DPL++ paradigm, the post-hoc application of perturbations to images and labels endows amendments toward both data distributions and supervisory signals, significantly furthering the generalizability of models over various benchmarks. Crucially, the proposed synchronous optimization process shares key differentiable objectives to reduce computational complexity, thereby achieving enhanced effectiveness within fewer optimization iterations. Theoretically, as a generic and flexible approach, DPL++ can be applied to a variety of backbone architectures (e.g., ResNet, DenseNet, and ViT) and downstream tasks (e.g., image classification and object detection). To validate the efficacy of DPL++, we conduct extensive performance experiments and in-depth analytical studies on 2 visual tasks over 5 mainstream benchmarks across 13 backbone networks. The comprehensive results verify the superiority of DPL++ over DPL and demonstrate its promising capabilities for advancing decision-making capacity, risk minimization, class distinguishability, and training convergence.},
  archive      = {J_TPAMI},
  author       = {Zifan Song and Xiao Gong and Guosheng Hu and Shuguang Dou and Qingsong Zhao and Cairong Zhao},
  doi          = {10.1109/TPAMI.2025.3594149},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10262-10279},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DPL++: Advancing the network performance via image and label perturbations},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic directed distance fields for ray-based shape representations. <em>TPAMI</em>, <em>47</em>(11), 10243-10261. (<a href='https://doi.org/10.1109/TPAMI.2025.3594225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern computer vision, the optimal representation of 3D shape remains task-dependent. One fundamental operation applied to such representations is differentiable rendering, which enables learning-based inverse graphics approaches. Standard explicit representations are often easily rendered, but can suffer from limited geometric fidelity, among other issues. On the other hand, implicit representations generally preserve greater fidelity, but suffer from difficulties with rendering, limiting scalability. In this work, we devise Directed Distance Fields (DDFs), which map a ray or oriented point (position and direction) to surface visibility and depth. This enables efficient differentiable rendering, obtaining depth with a single forward pass per pixel, as well as higher-order geometry with only additional backward passes. Using probabilistic DDFs (PDDFs), we can model the inherent discontinuities in the underlying field. We then apply DDFs to single-shape fitting, generative modelling, and 3D reconstruction, showcasing strong performance with simple architectural components via the versatility of our representation. Finally, since the dimensionality of DDFs permits view-dependent geometric artifacts, we conduct a theoretical investigation of the constraints necessary for view consistency. We find a small set of field properties that are sufficient to guarantee a DDF is consistent, without knowing which shape the field is expressing.},
  archive      = {J_TPAMI},
  author       = {Tristan Aumentado-Armstrong and Stavros Tsogkas and Sven Dickinson and Allan Jepson},
  doi          = {10.1109/TPAMI.2025.3594225},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10243-10261},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Probabilistic directed distance fields for ray-based shape representations},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InfoBound: A provable information-bounds inspired framework for both OoD generalization and OoD detection. <em>TPAMI</em>, <em>47</em>(11), 10227-10242. (<a href='https://doi.org/10.1109/TPAMI.2025.3595676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, distribution shifts give rise to the importance of two problems: out-of-distribution (OoD) generalization, which focuses on models’ generalization ability against covariate shifts (i.e., the changes of environments), and OoD detection, which aims to be aware of semantic shifts (i.e., test-time unseen classes). Real-world testing environments often involve a combination of both covariate and semantic shifts. While numerous methods have been proposed to address these critical issues, only a few works tackled them simultaneously. Moreover, prior works often improve one problem but sacrifice the other. To overcome these limitations, we delve into boosting OoD detection and OoD generalization from the perspective of information theory, which can be easily applied to existing models and different tasks. Building upon the theoretical bounds for mutual information and conditional entropy, we provide a unified approach, composed of Mutual Information Minimization (MI-Min) and Conditional Entropy Maximizing (CE-Max). Extensive experiments and comprehensive evaluations on multi-label image classification and object detection have demonstrated the superiority of our method. It successfully mitigates trade-offs between the two challenges compared to competitive baselines.},
  archive      = {J_TPAMI},
  author       = {Lin Zhu and Yifeng Yang and Zichao Nie and Yuan Gao and Jiarui Li and Qinying Gu and Xinbing Wang and Chenghu Zhou and Nanyang Ye},
  doi          = {10.1109/TPAMI.2025.3595676},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10227-10242},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {InfoBound: A provable information-bounds inspired framework for both OoD generalization and OoD detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HAC++: Towards 100X compression of 3D gaussian splatting. <em>TPAMI</em>, <em>47</em>(11), 10210-10226. (<a href='https://doi.org/10.1109/TPAMI.2025.3594066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting (3DGS) has emerged as a promising representation for novel view synthesis, boosting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. In this paper, we propose HAC++, which explicitly minimizes the representation’s entropy during optimization, enabling efficient arithmetic coding after training for compressed storage. Specifically, to reduce entropy, HAC++ leverages the relationships between unorganized anchors and a structured hash grid, utilizing their mutual information for context modeling. Additionally, HAC++ captures intra-anchor contextual relationships to further enhance compression performance. To facilitate entropy coding, we utilize Gaussian distributions to precisely estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Moreover, we incorporate an adaptive masking strategy to eliminate non-effective Gaussians and anchors. Overall, HAC++ achieves a remarkable size reduction of over $100\times$ compared to vanilla 3DGS when averaged on all datasets, while simultaneously improving fidelity. It also delivers more than $20\times$ size reduction compared to Scaffold-GS.},
  archive      = {J_TPAMI},
  author       = {Yihang Chen and Qianyi Wu and Weiyao Lin and Mehrtash Harandi and Jianfei Cai},
  doi          = {10.1109/TPAMI.2025.3594066},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10210-10226},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {HAC++: Towards 100X compression of 3D gaussian splatting},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards universal modal tracking with online dense temporal token learning. <em>TPAMI</em>, <em>47</em>(11), 10192-10209. (<a href='https://doi.org/10.1109/TPAMI.2025.3593543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a universal video-level modality-awareness tracking model with online dense temporal token learning (called UM-ODTrack). It is designed to support various tracking tasks, including RGB, RGB+Thermal, RGB+Depth, and RGB+Event, utilizing the same model architecture and parameters. Specifically, our model is designed with three core goals: Video-level Sampling. We expand the model’s inputs to a video sequence level, aiming to see a richer video context from an near-global perspective. Video-level Association. Furthermore, we introduce two simple yet effective online dense temporal token association mechanisms to propagate the appearance and motion trajectory information of target via a video stream manner. Modality Scalable. We propose two novel gated perceivers that adaptively learn cross-modal representations via a gated attention mechanism, and subsequently compress them into the same set of model parameters via a one-shot training manner for multi-task inference. This new solution brings the following benefits: (i) The purified token sequences can serve as temporal prompts for the inference in the next video frames, whereby previous information is leveraged to guide future inference. (ii) Unlike multi-modal trackers that require independent training, our one-shot training scheme not only alleviates the training burden, but also improves model representation. Extensive experiments on visible and multi-modal benchmarks show that our UM-ODTrack achieves a new SOTA performance.},
  archive      = {J_TPAMI},
  author       = {Yaozong Zheng and Bineng Zhong and Qihua Liang and Shengping Zhang and Guorong Li and Xianxian Li and Rongrong Ji},
  doi          = {10.1109/TPAMI.2025.3593543},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10192-10209},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards universal modal tracking with online dense temporal token learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFC-RNN: Adaptive forgetting-controlled recurrent neural network for pedestrian trajectory prediction. <em>TPAMI</em>, <em>47</em>(11), 10177-10191. (<a href='https://doi.org/10.1109/TPAMI.2025.3594116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction plays a crucial and fundamental role in many computer vision tasks. Most existing works utilize recurrent neural networks to extract temporal features from trajectories because their recursive structure is inherently well-suited for time series data. However, previous methods overlook the forgetting characteristics of pedestrians when modeling historical trajectories, which may cause the model to focus on the wrong positions of historical information. In this paper, we propose a simple yet effective Adaptive Forgetting-Controlled Recurrent Neural Network (AFC-RNN) for pedestrian trajectory prediction. The core idea of AFC-RNN is a novel Adaptive Forgetting Controller (AFC), which controls the forgetting degree of the historical information at each time step explicitly and adaptively. Specifically, AFC first learns memory factors for each time step based on the temporal correlation of observed trajectories using the self-attention mechanism. Then, AFC-RNN applies these memory factors to regulate the forgetting degree of observed features at each time step from RNN. Extensive experiments and ablation studies on ETH, UCY, SDD, and NBA datasets demonstrate that our method outperforms existing state-of-the-art approaches. Additionally, we provide a mathematical analysis to demonstrate the superiority of our adaptive forgetting strategy in the AFC-RNN over traditional RNNs for trajectory forgetting modeling.},
  archive      = {J_TPAMI},
  author       = {Yonghao Dong and Le Wang and Sanping Zhou and Wei Tang and Gang Hua and Changyin Sun},
  doi          = {10.1109/TPAMI.2025.3594116},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10177-10191},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AFC-RNN: Adaptive forgetting-controlled recurrent neural network for pedestrian trajectory prediction},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised hypergraph training framework via structure-aware learning. <em>TPAMI</em>, <em>47</em>(11), 10160-10176. (<a href='https://doi.org/10.1109/TPAMI.2025.3594487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraphs, with their ability to model complex, beyond pair-wise correlations, presents a significant advancement over traditional graphs for capturing intricate relational data across diverse domains. However, the integration of hypergraphs into self-supervised learning (SSL) frameworks has been hindered by the intricate nature of high-order structural variations. This paper introduces the Self-Supervised Hypergraph Training Framework via Structure-Aware Learning (SS-HT), designed to enhance the perception and measurement of these variations within hypergraphs. The SS-HT framework employs a “Masking and Re-Masking” strategy to bolster feature reconstruction in Hypergraph Neural Networks (HGNNs), addressing the limitations of traditional SSL methods. It also introduces a metric strategy for local high-order correlation changes, streamlining the computational efficiency of structural distance calculations. Extensive experiments on 11 datasets demonstrate SS-HT’s superior performance over existing SSL methods for both low-order and high-order data. Notably, the framework significantly reduces data labeling dependency, achieving a 32% improvement over HGNN in the downstream task fine-tuning phase under the 1% labeled data setting in the Cora-CC dataset. Ablation studies further validate SS-HT’s scalability and its capacity to augment the performance of various HGNN methods, underscoring its robustness and applicability in real-world scenarios.},
  archive      = {J_TPAMI},
  author       = {Yifan Feng and Shiquan Liu and Shihui Ying and Shaoyi Du and Zongze Wu and Yue Gao},
  doi          = {10.1109/TPAMI.2025.3594487},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10160-10176},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-supervised hypergraph training framework via structure-aware learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-inverted image pyramid networks for visual perception and multimodal understanding. <em>TPAMI</em>, <em>47</em>(11), 10142-10159. (<a href='https://doi.org/10.1109/TPAMI.2025.3593283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20 K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8 M training data.},
  archive      = {J_TPAMI},
  author       = {Zhaokai Wang and Xizhou Zhu and Xue Yang and Gen Luo and Hao Li and Changyao Tian and Wenhan Dou and Junqi Ge and Lewei Lu and Yu Qiao and Jifeng Dai},
  doi          = {10.1109/TPAMI.2025.3593283},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10142-10159},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Parameter-inverted image pyramid networks for visual perception and multimodal understanding},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified random walk, its induced laplacians and spectral convolutions for deep hypergraph learning. <em>TPAMI</em>, <em>47</em>(11), 10129-10141. (<a href='https://doi.org/10.1109/TPAMI.2025.3593880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraph-based modeling has gained significant attention for capturing complex higher-order interactions among vertices. While random walks serve as fundamental tools for analyzing hypergraphs, existing approaches either fail to fully leverage edge-dependent vertex weights (EDVWs) or lack sufficient expressiveness to model intricate hypergraph structures. To address these limitations, we propose a unified random walk framework that integrates hyperedge degrees and vertex weights, offering a more robust approach to hypergraph modeling. We establish equivalence conditions between hypergraph and graph random walks, leading to a novel unified random-walk-based hypergraph Laplacian that incorporates EDVWs, ensuring expressiveness and desirable spectral properties. Building on this foundation, we introduce the General Hypergraph Spectral Convolution (GHSC) framework, which extends existing Graph Convolutional Neural Networks (GCNNs) for effective hypergraph learning, supporting both edge-independent and edge-dependent vertex weights. Extensive experiments across diverse datasets, including citation networks, visual objects, and protein modeling tasks, demonstrate state-of-the-art performance, with notable improvements in protein structure modeling using EDVW-hypergraphs. This work advances the theoretical understanding of hypergraph random walks and spectral theory while providing a versatile framework for deep hypergraph learning. Code is available at GHSC_H_GNNs.},
  archive      = {J_TPAMI},
  author       = {Jiying Zhang and Fuyang Li and Xi Xiao and Guanzi Chen and Tingyang Xu and Yu Rong and Junzhou Huang and Yatao Bian},
  doi          = {10.1109/TPAMI.2025.3593880},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10129-10141},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A unified random walk, its induced laplacians and spectral convolutions for deep hypergraph learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection, ensemble, and adaptation: Advancing multi-source-free domain adaptation via architecture zoo. <em>TPAMI</em>, <em>47</em>(11), 10112-10128. (<a href='https://doi.org/10.1109/TPAMI.2025.3593943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional Multi-Source Free Domain Adaptation (MSFDA) assumes that each source domain provides a single source model, and all source models adopt a uniform architecture. This paper introduces Zoo-MSFDA, a more general setting that allows each source domain to offer a zoo of multiple source models with different architectures. While it enriches the source knowledge, Zoo-MSFDA risks being dominated by suboptimal/harmful models. To address this issue, we theoretically analyze the model selection problem in Zoo-MSFDA, and introduce two principles: transferability principle and diversity principle. Recognizing the challenge of measuring transferability, we subsequently propose a novel Source-Free Unsupervised Transferability Estimation (SUTE). It enables assessing and comparing transferability across multiple source models with different architectures under domain shift, without requiring target labels and source data. Based on above, we introduce a Selection, Ensemble, and Adaptation (SEA) framework to address Zoo-MSFDA, which consists of: 1) source models selection based on the proposed principles and SUTE; 2) ensemble construction based on SUTE-estimated transferability; 3) target-domain adaptation of the ensemble model. Evaluations demonstrate that our SEA framework, with the introduced Zoo-MSFDA setting, significantly improves adaptation performance in 2D image classification tasks. Additionally, our SUTE achieves state-of-the-art performance in transferability estimation.},
  archive      = {J_TPAMI},
  author       = {Jiangbo Pei and Ruizhe Li and Aidong Men and Yang Liu and Xiahai Zhuang and Qingchao Chen},
  doi          = {10.1109/TPAMI.2025.3593943},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10112-10128},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Selection, ensemble, and adaptation: Advancing multi-source-free domain adaptation via architecture zoo},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep equilibrium object detection and segmentation. <em>TPAMI</em>, <em>47</em>(11), 10094-10111. (<a href='https://doi.org/10.1109/TPAMI.2025.3595380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query-based object detectors and segmenters have made great progress in their respective tasks by employing an iterative refinement decoder. These query-based methods directly represent object instances with a set of learnable queries. These query vectors are progressively refined to stable, meaningful representations through a sequence of decoder layers, and then used to directly predict object locations (mask or box) and categories with customized heads. In this paper, we present a novel query-based object decoder design with infinite refinement (DEQ-Decoder) through a deep equilibrium model (DEQ). Our DEQ-Decoder models the query vector refinement as the fixed point solving of an implicit (DEQ) layer. To be more specific to query refinement, we use a two-step unrolled equilibrium equation to explicitly capture the query vector refinement. Accordingly, we are able to incorporate refinement awareness into the DEQ-Decoder training with the inexact gradient back-propagation (RAG). In addition, to stabilize the training of our DEQ-Decoder and improve its generalization ability, we devise a deep supervision scheme on the optimization path of DEQ-Decoder with refinement-aware perturbation (RAP). To demonstrate the effectiveness of DEQ-Decoder, we apply it to object detection and instance segmentation. For object detection, we propose DEQDet based on our DEQ-Decode. DEQDet converges faster, consumes less memory, and achieves better results than the baseline counterpart (AdaMixer). In particular, our DEQDet with ResNet50 backbone and 300 queries achieves the 49.6 mAP and 33.9 AP$_{s}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mrow/><mml:mi>s</mml:mi></mml:msub></mml:math> on the MS COCO benchmark under $2\times$ training scheme (24 epochs). For instance segmentation, Our DEQSeg achieves much better box mAP metrics and slightly better mask metrics for different mask decoding branches.},
  archive      = {J_TPAMI},
  author       = {Shuai Wang and Yao Teng and Limin Wang},
  doi          = {10.1109/TPAMI.2025.3595380},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10094-10111},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep equilibrium object detection and segmentation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning dual-stream conditional concepts in compositional zero-shot learning. <em>TPAMI</em>, <em>47</em>(11), 10076-10093. (<a href='https://doi.org/10.1109/TPAMI.2025.3597668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositional concepts composed of seen single concepts. One of the problems of CZSL is to model attributes interacting with objects and objects interacting with attributes. In this work, we focus on this problem and propose Dual-Stream Conditional Network (DSCNet) that learns dual-stream conditional concepts as a solution, where the conditional visual and semantic embeddings of attributes and objects are learned. First, we argue that the condition of the attribute or object is supposed to contain the recognized object and input image, or the recognized attribute and input image. Next, for each concept which can either be an attribute or object, in the semantic stream, we propose to encode the recognized object or attribute semantic features and the input image visual features as the encoded condition, which is then injected into all concept semantic embeddings by a semantic cross encoder to acquire conditional semantic embeddings. In the visual stream, the conditional attribute or object visual embeddings are acquired by injecting the semantic features of the recognized object or attribute into the mapped attribute or object visual features. Experimental results on CZSL benchmarks demonstrate the superiority of our proposed method.},
  archive      = {J_TPAMI},
  author       = {Qingsheng Wang and Lingqiao Liu and Chenchen Jing and Peng Wang and Yanning Zhang and Chunhua Shen},
  doi          = {10.1109/TPAMI.2025.3597668},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10076-10093},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning dual-stream conditional concepts in compositional zero-shot learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and accurate multi-view stereo with confidence-aware diffusion model. <em>TPAMI</em>, <em>47</em>(11), 10060-10075. (<a href='https://doi.org/10.1109/TPAMI.2025.3597148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D.},
  archive      = {J_TPAMI},
  author       = {Fangjinhua Wang and Qingshan Xu and Yew-Soon Ong and Marc Pollefeys},
  doi          = {10.1109/TPAMI.2025.3597148},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10060-10075},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Lightweight and accurate multi-view stereo with confidence-aware diffusion model},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic inference by model reduction. <em>TPAMI</em>, <em>47</em>(11), 10047-10059. (<a href='https://doi.org/10.1109/TPAMI.2025.3595670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can agents infer the intentions of others by simply observing their behavior? And how can they generate fast and accurate actions such as grasping a moving object on the fly? Recent advances in Bayesian model reduction have led to innovative, biologically plausible approaches to actively infer the state of affairs of the world and perform planning with continuous signals. However, reducing the surrounding environment into a small set of simpler hypotheses remains a challenge in highly dynamic contexts. In this study, we propose an approach, based on active inference, that employs dynamic priors sampled from reduced versions of a generative model. Each dynamic prior corresponds to an alternative evolution of the world, which the agent can evaluate by accumulating continuous data. We test our approach on two everyday tasks: inferring a trajectory and grasping a moving object. Our findings reveal how agents can smoothly infer and enact dynamic intentions, and emphasize the key role of intentional gain or precision in motor learning.},
  archive      = {J_TPAMI},
  author       = {Matteo Priorelli and Ivilin Peev Stoianov},
  doi          = {10.1109/TPAMI.2025.3595670},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10047-10059},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic inference by model reduction},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simple lifelong learning machines. <em>TPAMI</em>, <em>47</em>(11), 10033-10046. (<a href='https://doi.org/10.1109/TPAMI.2025.3595364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In lifelong learning, data are used to improve performance not only on the present task, but also on past and future (unencountered) tasks. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be to use data to improve performance on both future tasks (forward transfer) and past tasks (backward transfer). In this paper, we show that a simple approach—representation ensembling—demonstrates both forward and backward transfer in a variety of simulated and benchmark data scenarios, including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, Food1k, and CORe50), and speech (spoken digit), in contrast to various reference algorithms, which typically failed to transfer either forward or backward, or both. Moreover, our proposed approach can flexibly operate with or without a computational budget.},
  archive      = {J_TPAMI},
  author       = {Joshua T. Vogelstein and Jayanta Dey and Hayden S. Helm and Will LeVine and Ronak D. Mehta and Tyler M. Tomita and Haoyin Xu and Ali Geisa and Qingyang Wang and Gido M. van de Ven and Chenyu Gao and Weiwei Yang and Bryan Tower and Jonathan Larson and Christopher M. White and Carey E. Priebe},
  doi          = {10.1109/TPAMI.2025.3595364},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10033-10046},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Simple lifelong learning machines},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEMI-CAVA: A causal variational approach to semi-supervised learning. <em>TPAMI</em>, <em>47</em>(11), 10022-10032. (<a href='https://doi.org/10.1109/TPAMI.2025.3594360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has advanced rapidly, but relies heavily on large-labeled datasets for effective training. This is particularly challenging in fields like medicine, where expert labeling is costly, labor-intensive, and prone to bias and error. Semi-supervised learning (SSL) addresses this challenge by reducing reliance on labeled data. SSL is closely tied to the concept of causation. However, recent works relating causality to SSL are limited by modeling only low-dimensional observations or designing a plug-in module to alleviate the class imbalance. In this paper, we take steps towards training causal generative models for semi-supervised learning, combining principles from causality and variational inference. We interpret the Mixup strategy as a stochastic intervention and introduce a consistency loss to promote coherent latent representations. Under reasonable assumptions, we provide theoretical guarantees that the learned latent representations align with true causal factors up to permissible ambiguities. The experimental results show the proposed approach achieves state-of-the-art performance on several medical datasets of different modalities. Additionally, we test our model on standard benchmarking datasets: CIFAR10, CIFAR100, and SVHN, where it achieves competitive performance.},
  archive      = {J_TPAMI},
  author       = {Saptarshi Saha and Pratyush Kumar Sahoo and Utpal Garain},
  doi          = {10.1109/TPAMI.2025.3594360},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10022-10032},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SEMI-CAVA: A causal variational approach to semi-supervised learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NUPES: Non-uniform post-training quantization via power exponent search. <em>TPAMI</em>, <em>47</em>(11), 10012-10021. (<a href='https://doi.org/10.1109/TPAMI.2025.3593987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) deployment has been confined to larger hardware devices due to their expensive computational requirements. This challenge has recently reached another scale with the emergence of large language models (LLMs). In order to reduce both their memory footprint and latency, a promising technique is quantization. It consists in converting floating point representations to low bit-width fixed point representations, usually by assuming a uniform mapping onto a regular grid. This process, referred to in the literature as uniform quantization, may however be ill-suited as most DNN weights and activations follow a bell-shaped distribution. This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values. In this work, we propose an improvement over the most commonly adopted way to tackle this limitation in deep learning models quantization, namely, non-uniform quantization. NUPES leverages automorphisms to preserve the scalar multiplications. Such transformations are derived from power functions. However, the optimization of the exponent parameter and weight values remains a challenging and novel problem which could not be solved with previous post training optimization techniques which only learn to round up or down weight values in order to preserve the predictive function. We circumvent this limitation with a new paradigm: learning new quantized weights over the entire quantized space. Similarly, we enable the optimization of the power exponent, i.e. the optimization of the quantization operator itself during training by alleviating all the numerical instabilities. The resulting predictive function is compatible with integer-only low-bit inference. We show the ability of the method to achieve state-of-the-art compression rates in both, data-free and data-driven configurations. Our empirical benchmarks highlight the ability of NUPES to circumvent the limitations of previous post-training quantization techniques on transformers and large language models in particular.},
  archive      = {J_TPAMI},
  author       = {Edouard Yvinec and Arnaud Dapogny and Kevin Bailly},
  doi          = {10.1109/TPAMI.2025.3593987},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {10012-10021},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NUPES: Non-uniform post-training quantization via power exponent search},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilingual-prompt-guided directional feature learning for weakly supervised video anomaly detection. <em>TPAMI</em>, <em>47</em>(11), 9994-10011. (<a href='https://doi.org/10.1109/TPAMI.2025.3590242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised video anomaly detection has gained attention for its effective performance and cost-efficient annotation, using video-level labels to distinguish between normal and abnormal patterns. However, challenges arise from the diversity and incompleteness of anomalous events, complicating feature learning. Vision-language models offer promising approaches, but designing precise prompts remains difficult. This is because accommodating the diverse range of normal and anomalous scenarios in real-world settings is challenging, and the workload is significant. To tackle these issues, we propose integrating multilingualism and multiple prompts to improve feature learning. By utilizing prompts in various languages to define “anomaly” and “normalcy,” we tackle these concepts across different linguistic domains. In each domain, multiple prompts are employed for adaptive top-K prompt selection of snippets. To enhance visual feature learning, a multi-granularity attention module combining Transformer and Mamba is designed. Mamba’s long-range adaptation selection builds fine-grained temporal correlations among coarse-grained snippets, while Transformer enhances fine-grained information guided by coarse-grained information. Alongside a multilingual prompt guidance loss, we introduce a gradual directional loss to jointly optimize visual feature distribution and the top-K prompt selection. Our method demonstrates effectiveness on four video datasets and provides generalizability analyses on two medical datasets, including EMG and ECG temporal data.},
  archive      = {J_TPAMI},
  author       = {Chizhuo Xiao and Yang Xiao and Joey Tianyi Zhou and Zhiwen Fang},
  doi          = {10.1109/TPAMI.2025.3590242},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9994-10011},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multilingual-prompt-guided directional feature learning for weakly supervised video anomaly detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M3DM-NR: RGB-3D noisy-resistant industrial anomaly detection via multimodal denoising. <em>TPAMI</em>, <em>47</em>(11), 9981-9993. (<a href='https://doi.org/10.1109/TPAMI.2025.3592089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing industrial anomaly detection methods primarily concentrate on unsupervised learning with pristine RGB images. Yet, both RGB and 3D data are crucial for anomaly detection, and the datasets are seldom completely clean in practical scenarios. To address above challenges, this paper initially delves into the RGB-3D multi-modal noisy anomaly detection, proposing a novel noise-resistant M3DM-NR framework to leveraging strong multi-modal discriminative capabilities of CLIP. M3DM-NR consists of three stages: Stage-I introduces the Suspected References Selection module to filter a few normal samples from the training dataset, using the multimodal features extracted by the Initial Feature Extraction, and a Suspected Anomaly Map Computation module to generate a suspected anomaly map to focus on abnormal regions as reference. Stage-II uses the suspected anomaly maps of the reference samples as reference, and inputs image, point cloud, and text information to achieve denoising of the training samples through intra-modal comparison and multi-scale aggregation operations. Finally, Stage-III proposes the Point Feature Alignment, Unsupervised Feature Fusion, Noise Discriminative Coreset Selection, and Decision Layer Fusion modules to learn the pattern of the training dataset, enabling anomaly detection and segmentation while filtering out noise. Extensive experiments show that M3DM-NR outperforms state-of-the-art methods in 3D-RGB multi-modal noisy anomaly detection.},
  archive      = {J_TPAMI},
  author       = {Chengjie Wang and Haokun Zhu and Jinlong Peng and Yue Wang and Ran Yi and Yunsheng Wu and Lizhuang Ma and Jiangning Zhang},
  doi          = {10.1109/TPAMI.2025.3592089},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9981-9993},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {M3DM-NR: RGB-3D noisy-resistant industrial anomaly detection via multimodal denoising},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing high quality raw video using temporal affinity and diffusion prior. <em>TPAMI</em>, <em>47</em>(11), 9966-9980. (<a href='https://doi.org/10.1109/TPAMI.2025.3596623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rich information and original data distribution, RAW data are widely used in many computer vision applications. However, the use of RAW video remains limited because of the high storage costs associated with data collection. Previous works have attempted to reconstruct RAW frames from sRGB data using small sampled metadata from the original RAW frames. Yet, these algorithms struggle with RAW video reconstruction due to the high computational cost of sampling metadata on cameras. To address these issues, we propose a new RAW video reconstruction pipeline that de-renders high-quality RAW videos from sRGB data using only one initial RAW frame as a reference. Specifically, we introduce three new models to achieve this goal. First, we present the Temporal-Affinity Guided De-rendering Network. This network leverages the temporal affinity between adjacent frames to construct a reference RAW image from previous RAW pixels. The corresponding RAW pixels in the previous frame provide valuable information about the original RAW data distribution, aiding in the precise reconstruction of the current frame. Second, to recover the missing RAW pixels caused by camera and foreground movement, we fully exploit the rich prior information from a pre-trained diffusion model and propose the RAW In-painting Model. This model can accurately fill in hollow regions in a RAW image based on the corresponding sRGB image and the surrounding RAW context. Lastly, we present a lightweight content-aware video clipper that automatically adjusts the clip length used for RAW video reconstruction, thereby balancing storage requirements with reconstruction quality. To better evaluate the performance of the proposed framework across different devices, we introduce the first RAW video reconstruction benchmark that comprises RAW videos from six types of camera devices with challenging scenarios. Experimental results demonstrate that our algorithm can accurately reconstruct RAW videos across all the scenarios.},
  archive      = {J_TPAMI},
  author       = {Wencheng Han and Jianbing Shen and David J. Crandall and Cheng-Zhong Xu},
  doi          = {10.1109/TPAMI.2025.3596623},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9966-9980},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reconstructing high quality raw video using temporal affinity and diffusion prior},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPIC-SOUNDS: A large-scale dataset of actions that sound. <em>TPAMI</em>, <em>47</em>(11), 9953-9965. (<a href='https://doi.org/10.1109/TPAMI.2025.3590390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce EPIC-SOUNDS, a large-scale dataset of audio annotations capturing temporal extents and class labels within the audio stream of the egocentric videos. We propose an annotation pipeline where annotators temporally label distinguishable audio segments and describe the action that could have caused this sound. We identify actions that can be discriminated purely from audio, through grouping these free-form descriptions of audio into classes. For actions that involve objects colliding, we collect human annotations of the materials of these objects (e.g., a glass object being placed on a wooden surface), which we verify from video, discarding ambiguities. Overall, EPIC-SOUNDS includes 78.4 k categorised segments of audible events and actions, distributed across 44 classes as well as 39.2 k non-categorised segments. We train and evaluate state-of-the-art audio recognition and detection models on our dataset, for both audio-only and audio-visual methods. We also conduct analysis on: the temporal overlap between audio events, the temporal and label correlations between audio and visual modalities, the ambiguities in annotating materials from audio-only input, the importance of audio-only labels and the limitations of current models to understand actions that sound.},
  archive      = {J_TPAMI},
  author       = {Jaesung Huh and Jacob Chalk and Evangelos Kazakos and Dima Damen and Andrew Zisserman},
  doi          = {10.1109/TPAMI.2025.3590390},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9953-9965},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EPIC-SOUNDS: A large-scale dataset of actions that sound},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inv-adapter: ID customization generation via image inversion and lightweight parameter adapter. <em>TPAMI</em>, <em>47</em>(11), 9938-9952. (<a href='https://doi.org/10.1109/TPAMI.2025.3590321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable advancement in text-to-image generation models significantly boosts the research in ID customization generation. However, existing personalization methods cannot simultaneously satisfy high-fidelity and low-costs requirements. Their main bottleneck lies in the additional prompt image encoder (i.e., CLIP vision encoder), which produces weak alignment signals with the text-to-image model that may lose face information and is not well ‘absorbed’ by the text-to-image model. Towards this end, we propose Inv-Adapter, which first introduces a more reasonable and efficient token representation of ID image features and introduces a lightweight parameter adaptor to inject ID features. Specifically, our Inv-Adapter extracts diffusion-domain representations of ID images utilizing a pre-trained text-to-image model via DDIM image inversion, without an additional image encoder. Benefiting from the high alignment of the extracted ID prompt features and the intermediate features of the text-to-image model, we then introduce a lightweight attention adapter to embed them efficiently into the base text-to-image model. We conduct extensive experiments on different text-to-image models to assess ID fidelity, generation loyalty, speed, training costs, model scale and generalization ability in scenarios of general object, all of which show that the proposed Inv-Adapter is highly competitive in ID customization generation and model scale.},
  archive      = {J_TPAMI},
  author       = {Peng Xing and Ning Wang and Jianbo Ouyang and Zechao Li},
  doi          = {10.1109/TPAMI.2025.3590321},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9938-9952},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Inv-adapter: ID customization generation via image inversion and lightweight parameter adapter},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sel4FT: Annotation selection for pretraining-finetuning with distribution shift. <em>TPAMI</em>, <em>47</em>(11), 9922-9937. (<a href='https://doi.org/10.1109/TPAMI.2025.3591018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pretraining-finetuning paradigm has become dominant in computer vision, yet strategically exploiting limited annotation budgets during finetuning remains unexplored. We introduce active finetuning—a novel task for selecting the most informative samples to annotate within this paradigm. We propose Sel4FT, a unified annotation selection framework that optimizes a parametric model in continuous feature space to identify a subset preserving the entire pool’s distribution while maintaining diversity. To address distribution shifts from data augmentation, we develop Sel4FT++ with augmentation-aware selection mechanisms. We theoretically prove our approach minimizes the Earth Mover’s Distance between selected subset and full data pool. Our framework eliminates iterative retraining and annotation process during selection, providing an efficient solution for real-world deployment. Extensive experiments on image classification, long-tailed recognition, and semantic segmentation demonstrate state-of-the-art performance with over $100\times$ speedup compared to existing methods.},
  archive      = {J_TPAMI},
  author       = {Han Lu and Yichen Xie and Mingyu Ding and Wei Zhan and Xiaokang Yang and Masayoshi Tomizuka and Junchi Yan},
  doi          = {10.1109/TPAMI.2025.3591018},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9922-9937},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sel4FT: Annotation selection for pretraining-finetuning with distribution shift},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards reliable and faithful explanations: A disentanglement-augmented approach for selective rationalization. <em>TPAMI</em>, <em>47</em>(11), 9906-9921. (<a href='https://doi.org/10.1109/TPAMI.2025.3592313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pursuit of model explainability has prompted the selective rationalization (aka, rationale extraction) which can identify important features (i.e., rationales) from the original input to support prediction results. Existing methods typically involve a cascaded approach with a selector responsible for extracting rationales from the input, followed by a predictor that makes predictions based on the selected rationales. However, these approaches often neglect the information contained in the non-rationales, underutilizing the input. Therefore, in our prior work, we introduce the Disentanglement-Augmented Rationale Extraction (DARE) method, which disentangles the input into rationale and non-rationale components, and enhances rationale representations by minimizing the mutual information between them. While DARE demonstrates strong performance in rationalization, it may still rely on shortcuts in the training distribution, leading to unfaithful rationales. To this end, in this paper, we propose Faith-DARE, an extension of DARE that aims to extract more reliable rationales by mitigating shortcut dependencies. Specifically, we treat the non-rationale features identified by DARE as environments that are decorrelated from the predictions. By shuffling and recombining these environments with rationales, we generate counterfactual samples and identify invariant rationales that remain predictive across shifted distributions. Extensive experiments on graph and textual datasets validate the effectiveness of Faith-DARE.},
  archive      = {J_TPAMI},
  author       = {Linan Yue and Qi Liu and YiChao Du and Li Wang and Yanqing An and Enhong Chen},
  doi          = {10.1109/TPAMI.2025.3592313},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9906-9921},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards reliable and faithful explanations: A disentanglement-augmented approach for selective rationalization},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bringing equity to classification: Domain generalization for domain-linked classes. <em>TPAMI</em>, <em>47</em>(11), 9894-9905. (<a href='https://doi.org/10.1109/TPAMI.2025.3593407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) focuses on transferring domain-invariant knowledge from multiple source (training) domains to an a priori unseen target domain(s). This task implicitly requires that classes of interest are expressed in multiple sources (domain-shared) to break spurious domain-class correlations. However, real-world data scarcity challenges may often result in classes present in only a specific domain (domain-linked), which we show leads to extremely poor generalization. In this work, we introduce the domain-linked DG task to the community and develop a methodology to learn useful domain-invariant representations from domain-shared classes for domain-linked ones. Specifically, we propose FOND, a Fairness-inspired and cONtrastive learning objective for Domain-linked DG. Rigorous and reproducible experimental results communicate that FOND accomplishes state-of-the-art improvements for domain-linked classes, given a sufficient number of domain-shared classes and with minimal performance trade-offs. Complementary to these contributions, we theoretically analyze this task and provide practical insights for domain-linked class generalizability.},
  archive      = {J_TPAMI},
  author       = {Kimathi Kaai and Saad Hossain and Sirisha Rambhatla},
  doi          = {10.1109/TPAMI.2025.3593407},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9894-9905},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bringing equity to classification: Domain generalization for domain-linked classes},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating zero-shot NAS with feature map-based proxy and operation scoring function. <em>TPAMI</em>, <em>47</em>(11), 9876-9893. (<a href='https://doi.org/10.1109/TPAMI.2025.3590342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) has been extensively studied due to its ability in automatic architecture engineering. Existing NAS methods rely heavily on the gradients and data labels, which either incur immense computational costs or suffer from discretization discrepancy due to the supernet structure. Moreover, the majority of them are limited in generating diverse architectures. To alleviate these issues, in this paper, we propose a novel zero-cost proxy called $\mathsf {MeCo}$ based on the Pearson correlation matrix of the feature maps. Unlike the previous work, the computation of $\mathsf {MeCo}$ as well as its variant $\mathsf {MeCo_{opt}}$ requires only one random data for a single forward pass. Based on the proposed zero-cost proxy, we further craft a new zero-shot NAS scheme called $\mathsf {FLASH}$, which harnesses a new proxy-based operation scoring function and a greedy heuristic. Compared to the existing methods, $\mathsf {FLASH}$ is highly efficient and can construct diverse model architectures instead of repeated cells. We design comprehensive experiments and extensively evaluate our designs on multiple benchmarks and datasets. The experimental results show that our method is one to six orders of magnitudes more efficient than the state-of-the-art baselines with the highest model accuracy.},
  archive      = {J_TPAMI},
  author       = {Tangyu Jiang and Haodi Wang and Rongfang Bie and Chun Yuan},
  doi          = {10.1109/TPAMI.2025.3590342},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9876-9893},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Accelerating zero-shot NAS with feature map-based proxy and operation scoring function},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GradBias: Unveiling word influence on bias in text-to-image generative models. <em>TPAMI</em>, <em>47</em>(11), 9863-9875. (<a href='https://doi.org/10.1109/TPAMI.2025.3592901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in Text-to-Image (T2I) generative models has enabled high-quality image generation. As performance and accessibility increase, these models are gaining significant attraction and popularity: ensuring their fairness and safety is a priority to prevent the dissemination and perpetuation of biases. However, existing studies in bias detection focus on closed sets of predefined biases (e.g., gender, ethnicity). In this paper, we propose a general framework to identify, quantify, and explain biases in an open set setting, i.e. without requiring a predefined set. This pipeline leverages a Large Language Model (LLM) to propose biases starting from a set of captions. Next, these captions are used by the target generative model for generating a set of images. Finally, Vision Question Answering (VQA) is leveraged for bias evaluation. We show two variations of this framework: OpenBias and GradBias. OpenBias detects and quantifies biases, while GradBias determines the contribution of individual prompt words on biases. OpenBias effectively detects both well-known and novel biases related to people, objects, and animals and highly aligns with existing closed-set bias detection methods and human judgment. GradBias shows that neutral words can significantly influence biases and it outperforms several baselines, including state-of-the-art foundation models.},
  archive      = {J_TPAMI},
  author       = {Moreno D’Incà and Elia Peruzzo and Massimiliano Mancini and Xingqian Xu and Humphrey Shi and Nicu Sebe},
  doi          = {10.1109/TPAMI.2025.3592901},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9863-9875},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GradBias: Unveiling word influence on bias in text-to-image generative models},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient image fusion network exploiting unifying language and mask guidance. <em>TPAMI</em>, <em>47</em>(11), 9845-9862. (<a href='https://doi.org/10.1109/TPAMI.2025.3591930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion aims to merge image pairs collected by different sensors over the same scene, preserving their distinct features. Recent works have often focused on designing various image fusion losses, developing different network architectures, and leveraging downstream tasks (e.g., object detection) for image fusion. However, a few studies have explored how language and semantic masks can serve as guidance to aid image fusion. In this paper, we investigate how the combination of language and masks can guide image fusion tasks, discarding the previously complex frameworks, which rely on downstream tasks, GAN-based cycle training, diffusion models, or deep image priors. Additionally, we exploit a recurrent neural network-like architecture to build a lightweight network that avoids the quadratic-cost of traditional attention mechanisms. To adapt the receptance weighted key value (RWKV) model to an image modality, we modify it into a bidirectional version using an efficient scanning strategy (ESS). To guide image fusion by language and mask features, we introduce a multi-modal fusion module (MFM) to facilitate information exchange. Comprehensive experiments show that the proposed framework achieved state-of-the-art results in various image fusion tasks (i.e., visible-infrared image fusion, multi-focus image fusion, multi-exposure image fusion, medical image fusion, hyperspectral and multispectral image fusion, and pansharpening).},
  archive      = {J_TPAMI},
  author       = {Zi-Han Cao and Yu-Jie Liang and Liang-Jian Deng and Gemine Vivone},
  doi          = {10.1109/TPAMI.2025.3591930},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9845-9862},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An efficient image fusion network exploiting unifying language and mask guidance},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Re-boosting self-collaboration parallel prompt GAN for unsupervised image restoration. <em>TPAMI</em>, <em>47</em>(11), 9827-9844. (<a href='https://doi.org/10.1109/TPAMI.2025.3589606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods have demonstrated state-of-the-art performance in image restoration, especially when trained on large-scale paired datasets. However, acquiring paired data in real-world scenarios poses a significant challenge. Unsupervised restoration approaches based on generative adversarial networks (GANs) offer a promising solution without requiring paired datasets. Yet, these GAN-based approaches struggle to surpass the performance of conventional unsupervised GAN-based frameworks without significantly modifying model structures or increasing the computational complexity. To address these issues, we propose a self-collaboration (SC) strategy for existing restoration models. This strategy utilizes information from the previous stage as feedback to guide subsequent stages, achieving significant performance improvement without increasing the framework’s inference complexity. The SC strategy comprises a prompt learning (PL) module and a restorer ($Res$). It iteratively replaces the previous less powerful fixed restorer $\overline{Res}$ in the PL module with a more powerful $Res$. The enhanced PL module generates better pseudo-degraded/clean image pairs, leading to a more powerful $Res$ for the next iteration. Our SC can significantly improve the $Res$ ’s performance by over 1.5 dB without adding extra parameters or computational complexity during inference. Meanwhile, existing self-ensemble (SE) and our SC strategies enhance the performance of pre-trained restorers from different perspectives. As SE increases computational complexity during inference, we propose a re-boosting module to the SC (Reb-SC) to improve the SC strategy further by incorporating SE into SC without increasing inference time. This approach further enhances the restorer’s performance by approximately 0.3 dB. Additionally, we present a baseline framework that includes parallel generative adversarial branches with complementary “self-synthesis” and “unpaired-synthesis” constraints, ensuring the effectiveness of the training framework. Extensive experimental results on restoration tasks demonstrate that the proposed model performs favorably against existing state-of-the-art unsupervised restoration methods.},
  archive      = {J_TPAMI},
  author       = {Xin Lin and Yuyan Zhou and Jingtong Yue and Chao Ren and Kelvin C.K. Chan and Lu Qi and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2025.3589606},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9827-9844},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Re-boosting self-collaboration parallel prompt GAN for unsupervised image restoration},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable random feature latent variable models. <em>TPAMI</em>, <em>47</em>(11), 9813-9826. (<a href='https://doi.org/10.1109/TPAMI.2025.3589728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random feature latent variable models (RFLVMs) are state-of-the-art tools for uncovering structure in high-dimensional, non-Gaussian data. However, their reliance on Monte Carlo sampling significantly limits scalability, posing challenges for large-scale applications. To overcome these limitations, we develop a scalable RFLVM framework based on variational Bayesian inference (VBI), a deterministic and optimization-based alternative to sampling methods. Applying VBI to RFLVMs is nontrivial due to two key challenges: (i) the lack of an explicit probability density function (PDF) for Dirichlet process (DP) mixing weights, and (ii) the inefficiency of existing VBI approaches when handling the high-dimensional variational parameters of RFLVMs. To address these issues, we adopt the stick-breaking construction for the DP, which provides an explicit and tractable PDF over mixing weights, and propose a novel inference algorithm, block coordinate descent variational inference (BCD-VI), which partitions variational parameters into blocks and applies tailored solvers to optimize them efficiently. The resulting scalable model, referred to as SRFLVM, supports various likelihoods; we demonstrate its effectiveness under Gaussian and logistic settings. Extensive experiments on diverse benchmark datasets show that SRFLVM achieves superior scalability, computational efficiency, and performance in latent representation learning and missing data imputation, consistently outperforming state-of-the-art latent variable models, including deep generative approaches.},
  archive      = {J_TPAMI},
  author       = {Ying Li and Zhidi Lin and Yuhao Liu and Michael Minyi Zhang and Pablo M. Olmos and Petar M. Djurić},
  doi          = {10.1109/TPAMI.2025.3589728},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9813-9826},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Scalable random feature latent variable models},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards OOD object detection with unknown-concept guided feature diffusion. <em>TPAMI</em>, <em>47</em>(11), 9798-9812. (<a href='https://doi.org/10.1109/TPAMI.2025.3590735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, learning plentiful knowledge corresponding to known objects is an important ability for humans. The unknown objects could be assumed to depart from the familiar knowledge. Inspired by this idea, we explore leveraging the extracted knowledge to reason a set of unknown concepts. And they could be used to address unsupervised out-of-distribution object detection (OOD-OD) that aims to detect unseen OOD objects without accessing any auxiliary OOD data during training. To this end, we propose a new approach, i.e., Unknown-Concept Guided Feature Diffusion (UCFD), including an object-related knowledge extractor and an unknown-concept guided diffusor for synthesizing virtual OOD features. Specifically, we define multiple learnable codewords to capture object-relevant visual knowledge from all object categories. To avoid the detection performance degradation of the in-distribution (ID) objects, these codewords are utilized to enhance object features. Next, an unknown-concept pool is constructed by mixing up these extracted codewords. Finally, to reduce the impact of lacking OOD data for supervision, we design an unknown-concept guided diffusor, which leverages the sampled unknown concepts from the pool to guide the reverse process to generate expected OOD features that deviate from the familiar knowledge. The significant performance gains on three different tasks demonstrate the superiorities of our method. Meanwhile, extensive visualization results show that our method could synthesize effective virtual OOD features.},
  archive      = {J_TPAMI},
  author       = {Aming Wu and Cheng Deng},
  doi          = {10.1109/TPAMI.2025.3590735},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9798-9812},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards OOD object detection with unknown-concept guided feature diffusion},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 360VOTS: Visual object tracking and segmentation in omnidirectional videos. <em>TPAMI</em>, <em>47</em>(11), 9785-9797. (<a href='https://doi.org/10.1109/TPAMI.2025.3591725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking and segmentation in omnidirectional videos are challenging due to the wide field-of-view and large spherical distortion brought by 360$^{\circ }$ images. To alleviate these problems, we introduce a novel representation, extended bounding field-of-view (eBFoV), for target localization and use it as the foundation of a general 360 tracking framework which is applicable for both omnidirectional visual object tracking and segmentation tasks. Building upon our previous work on omnidirectional visual object tracking (360VOT), we propose a comprehensive dataset and benchmark that incorporates a new component called omnidirectional video object segmentation (360VOS). The 360VOS dataset includes 290 sequences accompanied by dense pixel-wise masks and covers a broader range of target categories. To support both the development and evaluation of algorithms in this domain, we divide the dataset into a training subset with 170 sequences and a testing subset with 120 sequences. Furthermore, we tailor evaluation metrics for both omnidirectional tracking and segmentation to ensure rigorous assessment. Through extensive experiments, we benchmark state-of-the-art approaches and demonstrate the effectiveness of our proposed 360 tracking framework and training dataset.},
  archive      = {J_TPAMI},
  author       = {Yinzhe Xu and Huajian Huang and Yingshu Chen and Sai-Kit Yeung},
  doi          = {10.1109/TPAMI.2025.3591725},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9785-9797},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {360VOTS: Visual object tracking and segmentation in omnidirectional videos},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial frequency modulation for semantic segmentation. <em>TPAMI</em>, <em>47</em>(11), 9767-9784. (<a href='https://doi.org/10.1109/TPAMI.2025.3592621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis demonstrate that our method effectively alleviates aliasing while successfully retaining details after demodulation. As a result, the proposed approach considerably enhances existing state-of-the-art segmentation models (e.g., Mask2Former-Swin-T +1.5 mIoU, InternImage-T +1.4 mIoU on ADE20 K). Furthermore, ARS also enhances the performance of powerful Deformable Convolution (+0.8 mIoU on Cityscapes) by maintaining relative positional order during non-uniform sampling. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks.},
  archive      = {J_TPAMI},
  author       = {Linwei Chen and Ying Fu and Lin Gu and Dezhi Zheng and Jifeng Dai},
  doi          = {10.1109/TPAMI.2025.3592621},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9767-9784},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spatial frequency modulation for semantic segmentation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Another vertical view: A hierarchical network for heterogeneous trajectory prediction via spectrums. <em>TPAMI</em>, <em>47</em>(11), 9749-9766. (<a href='https://doi.org/10.1109/TPAMI.2025.3590487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast development of AI-related techniques, the applications of trajectory prediction are no longer limited to easier scenes and trajectories. More and more trajectories with different forms, such as coordinates, bounding boxes, and even high-dimensional human skeletons, need to be analyzed and forecasted. Among these heterogeneous trajectories, interactions between different elements within a frame of trajectory, which we call “Dimension-wise Interactions”, would be more complex and challenging. However, most previous approaches focus mainly on a specific form of trajectories, and potential dimension-wise interactions are less concerned. In this work, we expand the trajectory prediction task by introducing the trajectory dimensionality $M$, thus extending its application scenarios to heterogeneous trajectories. We first introduce the Haar transform as an alternative to the Fourier transform to better capture the time-frequency properties of each trajectory-dimension. Then, we adopt the bilinear structure to model and fuse two factors simultaneously, including the time-frequency response and the dimension-wise interaction, to forecast heterogeneous trajectories via trajectory spectrums hierarchically in a generic way. Experiments show that the proposed model outperforms most state-of-the-art methods on ETH-UCY, SDD, nuScenes, and Human3.6 M with heterogeneous trajectories, including 2D coordinates, 2D/3D bounding boxes, and 3D human skeletons.},
  archive      = {J_TPAMI},
  author       = {Beihao Xia and Conghao Wong and Duanquan Xu and Qinmu Peng and Xinge You},
  doi          = {10.1109/TPAMI.2025.3590487},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9749-9766},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Another vertical view: A hierarchical network for heterogeneous trajectory prediction via spectrums},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLAG3D++: A benchmark for 3D fitness activity comprehension with language instruction. <em>TPAMI</em>, <em>47</em>(11), 9731-9748. (<a href='https://doi.org/10.1109/TPAMI.2025.3590012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the rapid development of general human action understanding. However, when applied to real-world applications such as sports analysis, most existing datasets are still unsatisfactory, because of the limitations in rich labels on multiple tasks, language instructions, high-quality 3D data, and diverse environments. In this paper, we present FLAG3D++, a large-scale benchmark for 3D fitness activity comprehension, which contains 180 K sequences of 60 activity categories with language instruction. FLAG3D++ features the following four aspects: 1) fine-grained annotations of the temporal intervals of actions in the untrimmed long sequences and how well these actions are performed, 2) detailed and professional language instruction to describe how to perform a specific activity, 3) accurate and dense 3D human pose captured from advanced MoCap system to handle the complex activity and large movement, 4) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments. In light of the specified features, we present two new practical applications as language-guided repetition action counting (L-RAC) and language-guided action quality assessment (L-AQA), which aim to take the language descriptions as references to count the repetitive times of an action and assess the quality of action respectively. Furthermore, we propose a Hierarchical Language-Guided Graph Convolutional Network (HL-GCN) model to better fuse the language information and skeleton sequences for L-RAC and L-AQA. To be specific, the HL-GCN performs cross-modal alignments by the early fusion of the linguistic feature and the hierarchical node features of the skeleton-based sequences encoded by the multiple intermediate graph convolutional layers. Extensive experiments show the superiority of our HL-GCN on both L-RAC and L-AQA, as well as the great research value of FLAG3D++ for various challenges, such as dynamic human mesh recovery and cross-domain human action recognition. Our dataset, source code, and trained models are made publicly available at FLAG3D++.},
  archive      = {J_TPAMI},
  author       = {Yansong Tang and Aoyang Liu and Jinpeng Liu and Shiyi Zhang and Wenxun Dai and Jie Zhou and Xiu Li and Jiwen Lu},
  doi          = {10.1109/TPAMI.2025.3590012},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9731-9748},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {FLAG3D++: A benchmark for 3D fitness activity comprehension with language instruction},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monocular-to-3D virtual try-on with generative semantic articulated fields. <em>TPAMI</em>, <em>47</em>(11), 9718-9730. (<a href='https://doi.org/10.1109/TPAMI.2025.3591072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a monocular-to-3D virtual try-on network based on a conditional 3D-aware Generative Adversarial Network (3D-GAN) for synthesizing multi-view try-on results from single monocular images. In contrast to previous 3D virtual try-on methods that rely on costly scanned meshes or pseudo-depth maps for supervision, our approach utilizes a conditional 3D-GAN trained solely on 2D images, greatly simplifying dataset construction and enhancing model scalability. Specifically, we propose a Generative monocular-to-3D Virtual Try-ON network (G3D-VTON) that integrates a 3D-aware conditional Parsing Module (3DPM), a U-Net Refinement Module (URM), and a Flow-based 2D Virtual Try-On Module (FTM). In our framework, the 3DPM is designed to generate a 3D representation of the virtual try-on result, thereby enabling multi-view rendering. To accomplish this, it is implemented using conditional generative semantic articulated fields, which leverage the 3D SMPL prior via inverse skinning to learn the Signed Distance Function (SDF) of the try-on results in a canonical pose space. This learned SDF enables the rendering of both a coarse human parsing map and a preliminary try-on output with explicit camera control. Furthermore, within 3DPM, we introduce deferred pose guidance to decouple style and pose conditions during training, thereby facilitating view controllable generation during inference. However, the rendered human parsing and try-on results exhibit imprecise shapes and blurry textures. To address these issues, the URM subsequently refines these rendered outputs using a refinement U-Net, and the FTM integrates the refined results with the 2D warped garment to generate the final try-on output with more accurate and realistic appearance details. Extensive experiments demonstrate that the proposed G3D-VTON effectively manipulates and generates faithful 3D human appearances wearing the desired garment, outperforming both 3D-GAN and depth-based 3D approaches while delivering superior visual results in 2D.},
  archive      = {J_TPAMI},
  author       = {Zhenyu Xie and Fuwei Zhao and Jun Zheng and Xin Dong and Feida Zhu and Xiaodan Liang},
  doi          = {10.1109/TPAMI.2025.3591072},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9718-9730},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Monocular-to-3D virtual try-on with generative semantic articulated fields},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One neuron saved is one neuron earned: On parametric efficiency of quadratic networks. <em>TPAMI</em>, <em>47</em>(11), 9702-9717. (<a href='https://doi.org/10.1109/TPAMI.2025.3588894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by neuronal diversity in the biological neural system, a plethora of studies proposed to design novel types of artificial neurons and introduce neuronal diversity into artificial neural networks. Recently proposed quadratic neuron, which replaces the inner-product operation in conventional neurons with a quadratic one, have achieved great success in many essential tasks. Despite the promising results of quadratic neurons, there is still an unresolved issue: Is the superior performance of quadratic networks simply due to the increased parameters or due to the intrinsic expressive capability? Without clarifying this issue, the performance of quadratic networks is always suspicious. Additionally, resolving this issue is reduced to finding killer applications of quadratic networks. In this paper, with theoretical and empirical studies, we show that quadratic networks enjoy parametric efficiency, thereby confirming that the superior performance of quadratic networks is due to the intrinsic expressive capability. This intrinsic expressive ability comes from that quadratic neurons can easily represent nonlinear interaction, while it is hard for conventional neurons. Theoretically, we derive the approximation efficiency of quadratic networks over conventional ones in terms of real space and manifolds. Moreover, from the perspective of the Barron space, we demonstrate that there exists a functional space whose functions can be approximated by quadratic networks in a dimension-free error, but the approximation error of conventional networks is dependent on dimensions. Empirically, experimental results on synthetic data, classic benchmarks, and real-world applications show that quadratic models broadly enjoy parametric efficiency, and the gain of efficiency depends on the task.},
  archive      = {J_TPAMI},
  author       = {Feng-Lei Fan and Hang-Cheng Dong and Zhongming Wu and Lecheng Ruan and Tieyong Zeng and Yiming Cui and Jing-Xiao Liao},
  doi          = {10.1109/TPAMI.2025.3588894},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9702-9717},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {One neuron saved is one neuron earned: On parametric efficiency of quadratic networks},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ClusMatch: Improving deep clustering by unified positive and negative pseudo-label learning. <em>TPAMI</em>, <em>47</em>(11), 9688-9701. (<a href='https://doi.org/10.1109/TPAMI.2025.3588239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep clustering methods have achieved remarkable results compared to traditional clustering approaches. However, its performance remains constrained by the absence of annotations. A thought-provoking observation is that there is still a significant gap between deep clustering and semi-supervised classification methods. Even with only a few labeled samples, the accuracy of semi-supervised learning is much higher than that of clustering. Given that we can annotate a small number of samples in a certain unsupervised way, the clustering task can be naturally transformed into a semi-supervised setting, thereby achieving comparable performance. Based on this intuition, we propose ClusMatch, a unified positive and negative pseudo-label learning based semi-supervised learning framework, which is pluggable and can be applied to existing deep clustering methods. Specifically, we first leverage the pre-trained deep clustering network to compute predictions for all samples, and then design specialized selection strategies to pick out a few high-quality samples as labeled samples for supervised learning. For the unselected samples, the novel unified positive and negative pseudo-label learning is introduced to provide additional supervised signals for semi-supervised fine-tuning. We also propose an adaptive positive-negative threshold learning strategy to further enhance the confidence of generated pseudo-labels. Extensive experiments on six widely-used datasets and one large-scale dataset demonstrate the superiority of our proposed ClusMatch. For example, ClusMatch achieves a significant accuracy improvement of 5.4% over the state-of-the-art method ProPos on an average of these six datasets.},
  archive      = {J_TPAMI},
  author       = {Jianlong Wu and Zihan Li and Wei Sun and Jianhua Yin and Liqiang Nie and Zhouchen Lin},
  doi          = {10.1109/TPAMI.2025.3588239},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9688-9701},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ClusMatch: Improving deep clustering by unified positive and negative pseudo-label learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced dual-pattern matching with vision-language representation for out-of-distribution detection. <em>TPAMI</em>, <em>47</em>(11), 9673-9687. (<a href='https://doi.org/10.1109/TPAMI.2025.3590717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) detection presents a significant challenge in deploying pattern recognition and machine learning models, as they frequently fail to generalize to data from unseen distributions. Recent advancements in vision-language models (VLMs), particularly CLIP, have demonstrated promising results in OOD detection through their rich multimodal representations. However, current CLIP-based OOD detection methods predominantly rely on single-modality in-distribution (ID) data (e.g., textual cues), overlooking the valuable information contained in ID visual cues. In this work, we demonstrate that incorporating ID visual information is crucial for unlocking CLIP’s full potential in OOD detection. We propose a novel approach, Dual-Pattern Matching (DPM), which effectively adapts CLIP for OOD detection by jointly exploiting both textual and visual ID patterns. Specifically, DPM refines visual and textual features through the proposed Domain-Specific Feature Aggregation (DSFA) and Prompt Enhancement (PE) modules. Subsequently, DPM stores class-wise textual features as textual patterns and aggregates ID visual features as visual patterns. During inference, DPM calculates similarity scores relative to both patterns to identify OOD data. Furthermore, we enhance DPM with lightweight adaptation mechanisms to further boost OOD detection performance. Comprehensive experiments demonstrate that DPM surpasses state-of-the-art methods on multiple benchmarks, highlighting the effectiveness of leveraging multimodal information for OOD detection. The proposed dual-pattern approach provides a simple yet robust framework for leveraging vision-language representations in OOD detection tasks.},
  archive      = {J_TPAMI},
  author       = {Xiang Xiang and Zhuo Xu and Zihan Zhang and Zhigang Zeng and Xilin Chen},
  doi          = {10.1109/TPAMI.2025.3590717},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9673-9687},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Enhanced dual-pattern matching with vision-language representation for out-of-distribution detection},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ins-HOI: Instance aware human-object interactions recovery. <em>TPAMI</em>, <em>47</em>(11), 9655-9672. (<a href='https://doi.org/10.1109/TPAMI.2025.3588268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately modeling detailed interactions between human/hand and object is an appealing yet challenging task. Current multi-view capture systems are only capable of reconstructing multiple subjects into a single, unified mesh, which fails to model the states of each instance individually during interactions. To address this, previous methods use template-based representations to track human/hand and object. However, the quality of the reconstructions is limited by the descriptive capabilities of the templates so these methods inherently struggle with geometric details, pressing deformations and invisible contact surfaces. In this work, we propose an end-to-end Instance-aware Human-Object Interactions recovery (Ins-HOI) framework by introducing an instance-level occupancy field representation. However, the real-captured data is presented as a holistic mesh, unable to provide instance-level supervision. To address this, we further propose a complementary training strategy that leverages synthetic data to introduce instance-level shape priors, enabling the disentanglement of occupancy fields for different instances. Specifically, synthetic data, created by randomly combining individual scans of humans/hands and objects, guides the network to learn a coarse prior of instances. Meanwhile, real-captured data helps in learning the overall geometry and restricting interpenetration in contact areas. As demonstrated in experiments, our method Ins-HOI supports instance-level reconstruction and provides reasonable and realistic invisible contact surfaces even in cases of extremely close interaction. To facilitate research on this task, we collect a large-scale, high-fidelity 3D scan dataset, including 5.2 k high-quality scans with real-world human-chair and hand-object interactions. The code and data will be public for research purposes.},
  archive      = {J_TPAMI},
  author       = {Jiajun Zhang and Yuxiang Zhang and Hongwen Zhang and Xiao Zhou and Boyao Zhou and Ruizhi Shao and Zonghai Hu and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3588268},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9655-9672},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ins-HOI: Instance aware human-object interactions recovery},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protecting feature privacy in person re-identification. <em>TPAMI</em>, <em>47</em>(11), 9637-9654. (<a href='https://doi.org/10.1109/TPAMI.2025.3590979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) is to identify the same person across non-overlapping camera views. After a decade of development, the methods based on deep networks have achieved high performance on benchmarks and become mainstream. In applications, the features of gallery images extracted by deep learning-based methods are stored to speed up the query process and protect the sensitive information contained in the images. Unfortunately, it is demonstrated that turning the images into features cannot properly protect privacy, as these features could be reversed to the corresponding images, revealing the sensitive information they contain. Therefore, for preventing privacy leakage, recent methods learn their features against some feature reversal methods, and most conventional reversal methods focus on minimizing the difference between a reconstruction and its original image. However, there could be many reasonable reconstruction results from a single feature, and the conventional reversal methods will inevitably generate reconstruction results that lie in a different distribution from one of the original images, which cannot properly assess the private information for learning to protect and thus hamper the privacy-protected feature learning. To mitigate this problem, we enforce the reconstructions to follow the same distribution as the original images by the generative adversarial network (GAN). We operate this GAN-based feature reversal module accompanied by the conventional ReID feature extraction module and form a novel GAN-based feature privacy-protected person ReID model, which is expected to protect feature privacy so as against reversal attack and maintain ReID utility. We demonstrate that optimizing ReID model to accommodate privacy protection faces a double adversarial objective and is thus challenging. As a remedy, we design a novel two-step training and lazy update strategy that alternatively optimizes the feature extraction module and stabilizes the update process of the GAN-based feature reversal module. To evaluate the efficiency of the model in balancing its ReID utility and feature privacy protection, we introduce a novel metric called utility-reversibility ratio (URR). Compared with existing privacy-protected feature extraction models, the proposed method achieves a better balance between privacy protection and person ReID performance. Extensive experiments validate that our model can effectively protect feature privacy at a tiny accuracy cost, and validate the effectiveness of our model with the emerging diffusion model.},
  archive      = {J_TPAMI},
  author       = {Xiao Li and Yi-Xing Peng and Wei-Shi Zheng},
  doi          = {10.1109/TPAMI.2025.3590979},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9637-9654},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Protecting feature privacy in person re-identification},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal graph mamba for music-guided dance video synthesis. <em>TPAMI</em>, <em>47</em>(11), 9626-9636. (<a href='https://doi.org/10.1109/TPAMI.2025.3588237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the music-guided dance video synthesis task, i.e., to translate the input music to a dance video. STG-Mamba consists of two translation mappings: music-to-skeleton translation and skeleton-to-video translation. In the music-to-skeleton translation, we introduce a novel spatial-temporal graph Mamba (STGM) block to effectively construct skeleton sequences from the input music, capturing dependencies between joints in both the spatial and temporal dimensions. For the skeleton-to-video translation, we propose a novel self-supervised regularization network to translate the generated skeletons, along with a conditional image, into a dance video. Lastly, we collect a new skeleton-to-video translation dataset from the Internet, containing 54,944 video clips. Extensive experiments demonstrate that STG-Mamba achieves significantly better results than existing methods.},
  archive      = {J_TPAMI},
  author       = {Hao Tang and Ling Shao and Zhenyu Zhang and Luc Van Gool and Nicu Sebe},
  doi          = {10.1109/TPAMI.2025.3588237},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9626-9636},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Spatial-temporal graph mamba for music-guided dance video synthesis},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPOT: Scalable 3D pre-training via occupancy prediction for learning transferable 3D representations. <em>TPAMI</em>, <em>47</em>(11), 9609-9625. (<a href='https://doi.org/10.1109/TPAMI.2025.3586961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotating 3D LiDAR point clouds for perception tasks is fundamental for many applications e.g. autonomous driving, yet it still remains notoriously labor-intensive. Pretraining-finetuning approach can alleviate the labeling burden by fine-tuning a pre-trained backbone across various downstream datasets as well as tasks. In this paper, we propose SPOT, namely Scalable Pre-training via Occupancy prediction for learning Transferable 3D representations under such a label-efficient fine-tuning paradigm. SPOT achieves effectiveness on various public datasets with different downstream tasks, showcasing its general representation power, cross-domain robustness and data scalability which are three key factors for real-world application. Specifically, we both theoretically and empirically show, for the first time, that general representations learning can be achieved through the task of occupancy prediction. Then, to address the domain gap caused by different LiDAR sensors and annotation methods, we develop a beam re-sampling technique for point cloud augmentation combined with class-balancing strategy. Furthermore, scalable pre-training is observed, that is, the downstream performance across all the experiments gets better with more pre-training data. Additionally, such pre-training strategy also remains compatible with unlabeled data. The hope is that our findings will facilitate the understanding of LiDAR points and pave the way for future advancements in LiDAR pre-training.},
  archive      = {J_TPAMI},
  author       = {Xiangchao Yan and Runjian Chen and Bo Zhang and Hancheng Ye and Renqiu Xia and Jiakang Yuan and Hongbin Zhou and Xinyu Cai and Botian Shi and Wenqi Shao and Ping Luo and Yu Qiao and Tao Chen and Junchi Yan},
  doi          = {10.1109/TPAMI.2025.3586961},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9609-9625},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SPOT: Scalable 3D pre-training via occupancy prediction for learning transferable 3D representations},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient visual transformer by learnable token merging. <em>TPAMI</em>, <em>47</em>(11), 9597-9608. (<a href='https://doi.org/10.1109/TPAMI.2025.3588186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Learnable Token Merging (LTM), or LTM-Transformer. LTM-Transformer performs token merging in a learnable scheme. LTM-Transformer is compatible with many popular and compact transformer networks, and it reduces the FLOPs and the inference time of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in popular visual transformers, including MobileViT, EfficientViT, ViT, and Swin, with LTM-Transformer blocks, leading to LTM-Transformer networks with different backbones. The LTM-Transformer is motivated by reduction of Information Bottleneck, and a novel and separable variational upper bound for the IB loss is derived. The architecture of the mask module in our LTM blocks which generates the token merging mask is designed to reduce the derived upper bound for the IB loss. Extensive results on computer vision tasks evidence that LTM-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers.},
  archive      = {J_TPAMI},
  author       = {Yancheng Wang and Yingzhen Yang},
  doi          = {10.1109/TPAMI.2025.3588186},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9597-9608},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient visual transformer by learnable token merging},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Re-GAN: Data-efficient GANs training via architectural reconfiguration. <em>TPAMI</em>, <em>47</em>(11), 9580-9596. (<a href='https://doi.org/10.1109/TPAMI.2025.3590650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The training of Generative Adversarial Networks (GANs) for high-fidelity images has predominantly relied on large-scale datasets. Emerging research, particularly on GANs ‘lottery tickets’, suggests that dense GANs models have sparse sub-networks capable of superior performance with limited data. However, the conventional process to uncover these ‘lottery tickets’ involves a resource-intensive train-prune-retrain cycle. Addressing this, our paper introduces Re-GAN, a novel, data-efficient approach for GANs training that dynamically reconfigures the GANs architecture during training. This method focuses on iterative pruning of non-important connections and regrowing them, thereby preventing premature loss of important features and maintaining the model’s representational strength. Re-GAN provides a more stable and efficient solution for GANs models with limited data, offering an alternative to existing progressive growing methods and GANs tickets. While Re-GAN has already demonstrated its potential in image generation across diverse datasets, domains, and resolutions, in this paper, we significantly expand our study. We incorporate new applications, notably Image-to-Image translation, include additional datasets, provide in-depth analyses, and explore compatibility with data augmentation techniques. This expansion not only broadens the scope of Re-GAN but also establishes it as a generic training methodology, demonstrating its effectiveness and adaptability in different GANs scenarios.},
  archive      = {J_TPAMI},
  author       = {Divya Saxena and Jiannong Cao and Jiahao Xu and Tarun Kulshrestha},
  doi          = {10.1109/TPAMI.2025.3590650},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9580-9596},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Re-GAN: Data-efficient GANs training via architectural reconfiguration},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unifying graph contrastive learning via graph message augmentation. <em>TPAMI</em>, <em>47</em>(11), 9563-9579. (<a href='https://doi.org/10.1109/TPAMI.2025.3586651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning is usually performed by first conducting Graph Data Augmentation (GDA) and then employing a contrastive learning pipeline to train GNNs. As we know that GDA is an important issue for graph contrastive learning. Various GDAs have been developed recently which mainly involve dropping or perturbing edges, nodes, node attributes and edge attributes. However, to our knowledge, it still lacks a universal and effective augmentor that is suitable for different types of graph data. To address this issue, in this paper, we first introduce the graph message representation of graph data. Based on it, we then propose a novel Graph Message Augmentation (GMA), a universal scheme for reformulating many existing GDAs. The proposed unified GMA not only gives a new perspective to understand many existing GDAs but also provides a universal and more effective graph data augmentation for graph self-supervised learning tasks. Moreover, GMA introduces an easy way to implement the mixup augmentor which is natural for images but usually challengeable for graphs. Based on the proposed GMA, we then propose a unified graph contrastive learning, termed Graph Message Contrastive Learning (GMCL), that employs attribution-guided universal GMA for graph contrastive learning. Experiments on many graph learning tasks demonstrate the effectiveness and benefits of the proposed GMA and GMCL approaches.},
  archive      = {J_TPAMI},
  author       = {Ziyan Zhang and Bo Jiang and Jin Tang and Bin Luo},
  doi          = {10.1109/TPAMI.2025.3586651},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9563-9579},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unifying graph contrastive learning via graph message augmentation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aligning text-to-image diffusion models with constrained reinforcement learning. <em>TPAMI</em>, <em>47</em>(11), 9550-9562. (<a href='https://doi.org/10.1109/TPAMI.2025.3590730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reward finetuning has emerged as a powerful technique for aligning diffusion models with specific downstream objectives or user preferences. However, current approaches suffer from a persistent challenge of reward overoptimization, where models exploit imperfect reward feedback at the expense of overall performance. In this work, we identify three key contributors to overoptimization: (1) a granularity mismatch between the multi-step diffusion process and sparse rewards; (2) a loss of plasticity that limits the model’s ability to adapt and generalize; and (3) an overly narrow focus on a single reward objective that neglects complementary performance criteria. Accordingly, we introduce Constrained Diffusion Policy Optimization (CDPO), a novel reinforcement learning framework that addresses reward overoptimization from multiple angles. Firstly, CDPO tackles the granularity mismatch through a temporal policy optimization strategy that delivers step-specific rewards throughout the entire diffusion trajectory, thereby reducing the risk of overfitting to sparse final-step rewards. Then we incorporate a neuron reset strategy that selectively resets overactive neurons in the model, preventing overoptimization induced by plasticity loss. Finally, to avoid overfitting to a narrow reward objective, we integrate constrained reinforcement learning with auxiliary reward objectives serving as explicit constraints, ensuring a balanced optimization across diverse performance metrics.},
  archive      = {J_TPAMI},
  author       = {Ziyi Zhang and Sen Zhang and Li Shen and Yibing Zhan and Yong Luo and Han Hu and Bo Du and Yonggang Wen and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3590730},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9550-9562},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Aligning text-to-image diffusion models with constrained reinforcement learning},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond binary: Improving signed message passing in graph neural networks for multi-class graphs. <em>TPAMI</em>, <em>47</em>(11), 9535-9549. (<a href='https://doi.org/10.1109/TPAMI.2025.3581218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) exhibit satisfactory performance on homophilic networks, where most edges connect two nodes with the same label. However, their effectiveness diminishes as the graphs become heterophilic (low homophily), prompting the exploration of various message-passing schemes. In particular, assigning negative weights to heterophilic edges (signed propagation) for message-passing has gained significant attention, and some studies theoretically confirm its effectiveness. Nevertheless, prior theorems assume binary classification scenarios, which may not hold well for graphs with multiple classes. To solve this limitation, we offer new theoretical insights into GNNs in multi-class environments and identify the drawbacks of employing signed propagation from two perspectives: message-passing and parameter update. We found that signed propagation without considering feature distribution can degrade the separability of dissimilar neighbors, which also increases prediction uncertainty (e.g., conflicting evidence) that can cause instability. To address these limitations, we introduce two novel calibration strategies aiming to improve discrimination power while reducing entropy in predictions. Through theoretical and extensive experimental analysis, we demonstrate that the proposed schemes enhance the performance of both signed and general message-passing neural networks (Choi et al. 2023).},
  archive      = {J_TPAMI},
  author       = {Yoonhyuk Choi and Taewook Ko and Jiho Choi and Chong-Kwon Kim},
  doi          = {10.1109/TPAMI.2025.3581218},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9535-9549},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Beyond binary: Improving signed message passing in graph neural networks for multi-class graphs},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Light field neural rendering. <em>TPAMI</em>, <em>47</em>(11), 9523-9534. (<a href='https://doi.org/10.1109/TPAMI.2023.3316992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical light field rendering for novel view synthesis can accurately reproduce view-dependent effects such as reflection, refraction, and translucency, but requires a dense view sampling of the scene. Methods based on geometric reconstruction need only sparse views, but cannot accurately model non-Lambertian effects. We introduce a model that combines the strengths and mitigates the limitations of these two directions. By operating on a four-dimensional representation of the light field, our model learns to represent view-dependent effects accurately. By enforcing geometric constraints during training and inference, the scene geometry is implicitly learned from a sparse set of views. Concretely, we introduce a two-stage transformer-based model that first aggregates features along epipolar lines, then aggregates features along reference views to produce the color of a target ray. Additionally, we propose modifications that allow the model to generalize to scenes without any fine-tuning. Our model outperforms the state-of-the-art on multiple forward-facing and 360$^\circ$ datasets, with larger margins on scenes with severe view-dependent variations.},
  archive      = {J_TPAMI},
  author       = {Mohammed Suhail and Carlos Esteves and Leonid Sigal and Ameesh Makadia},
  doi          = {10.1109/TPAMI.2023.3316992},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9523-9534},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Light field neural rendering},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-empowered invariant grounding for video question answering. <em>TPAMI</em>, <em>47</em>(11), 9510-9522. (<a href='https://doi.org/10.1109/TPAMI.2023.3303451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Question Answering (VideoQA) is the task of answering questions about a video. At its core is the understanding of the alignments between video scenes and question semantics to yield the answer. In leading VideoQA models, the typical learning objective, empirical risk minimization (ERM), tends to over-exploit the spurious correlations between question-irrelevant scenes and answers, instead of inspecting the causal effect of question-critical scenes, which undermines the prediction with unreliable reasoning. In this work, we take a causal look at VideoQA and propose a modal-agnostic learning framework, named Invariant Grounding for VideoQA (IGV), to ground the question-critical scene, whose causal relations with answers are invariant across different interventions on the complement. With IGV, leading VideoQA models are forced to shield the answering from the negative influence of spurious correlations, which significantly improves their reasoning ability. To unleash the potential of this framework, we further provide a Transformer-Empowered Invariant Grounding for VideoQA (TIGV), a substantial instantiation of IGV framework that naturally integrates the idea of invariant grounding into a transformer-style backbone. Experiments on four benchmark datasets validate our design in terms of accuracy, visual explainability, and generalization ability over the leading baselines. Our code is available at https://github.com/yl3800/TIGV.},
  archive      = {J_TPAMI},
  author       = {Yicong Li and Xiang Wang and Junbin Xiao and Wei Ji and Tat-Seng Chua},
  doi          = {10.1109/TPAMI.2023.3303451},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9510-9522},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transformer-empowered invariant grounding for video question answering},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ego4D: Around the world in 3,600 hours of egocentric video. <em>TPAMI</em>, <em>47</em>(11), 9468-9509. (<a href='https://doi.org/10.1109/TPAMI.2024.3381075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards, with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception.},
  archive      = {J_TPAMI},
  author       = {Kristen Grauman and Andrew Westbury and Eugene Byrne and Vincent Cartillier and Zachary Chavis and Antonino Furnari and Rohit Girdhar and Jackson Hamburger and Hao Jiang and Devansh Kukreja and Miao Liu and Xingyu Liu and Miguel Martin and Tushar Nagarajan and Ilija Radosavovic and Santhosh Kumar Ramakrishnan and Fiona Ryan and Jayant Sharma and Michael Wray and Mengmeng Xu and Eric Zhongcong Xu and Chen Zhao and Siddhant Bansal and Dhruv Batra and Sean Crane and Tien Do and Morrie Doulaty and Akshay Erapalli and Christoph Feichtenhofer and Adriano Fragomeni and Qichen Fu and Abrham Gebreselasie and Cristina González and James Hillis and Xuhua Huang and Yifei Huang and Wenqi Jia and Weslie Khoo and Jáchym Kolář and Satwik Kottur and Anurag Kumar and Federico Landini and Chao Li and Yanghao Li and Zhenqiang Li and Karttikeya Mangalam and Raghava Modhugu and Jonathan Munro and Tullie Murrell and Takumi Nishiyasu and Will Price and Paola Ruiz Puentes and Merey Ramazanova and Leda Sari and Kiran Somasundaram and Audrey Southerland and Yusuke Sugano and Ruijie Tao and Minh Vo and Yuchen Wang and Xindi Wu and Takuma Yagi and Ziwei Zhao and Yunyi Zhu and Pablo Arbeláez and David Crandall and Dima Damen and Giovanni Maria Farinella and Christian Fuegen and Bernard Ghanem and Vamsi Krishna Ithapu and C. V. Jawahar and Hanbyul Joo and Kris Kitani and Haizhou Li and Richard Newcombe and Aude Oliva and Hyun Soo Park and James M. Rehg and Yoichi Sato and Jianbo Shi and Mike Zheng Shou and Antonio Torralba and Lorenzo Torresani and Mingfei Yan and Jitendra Malik},
  doi          = {10.1109/TPAMI.2024.3381075},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9468-9509},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ego4D: Around the world in 3,600 hours of egocentric video},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Burst image restoration and enhancement. <em>TPAMI</em>, <em>47</em>(11), 9454-9467. (<a href='https://doi.org/10.1109/TPAMI.2024.3356188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Burst Image Restoration aims to reconstruct a high-quality image by efficiently combining complementary inter-frame information. However, it is quite challenging since individual burst images often have inter-frame misalignments that usually lead to ghosting and zipper artifacts. To mitigate this, we develop a novel approach for burst image processing named BIPNet that focuses solely on the information exchange between burst frames and filter-out the inherent degradations while preserving and enhancing the actual scene details. Our central idea is to generate a set of pseudo-burst features that combine complementary information from all the burst frames to exchange information seamlessly. However, due to inter-frame misalignment, the information cannot be effectively combined in pseudo-burst. Thus, we initially align the incoming burst features regarding the reference frame using the proposed edge-boosting feature alignment. Lastly, we progressively upscale the pseudo-burst features in multiple stages while adaptively combining the complementary information. Unlike the existing works, that usually deploy single-stage up-sampling with a late fusion scheme, we first deploy a pseudo-burst mechanism followed by the adaptive-progressive feature up-sampling. The proposed BIPNet significantly outperforms the existing methods on burst super-resolution, low-light image enhancement, low-light image super-resolution, and denoising tasks.},
  archive      = {J_TPAMI},
  author       = {Akshay Dudhane and Syed Waqas Zamir and Salman Khan and Fahad Shahbaz Khan and Ming-Husan Yang},
  doi          = {10.1109/TPAMI.2024.3356188},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9454-9467},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Burst image restoration and enhancement},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AnyFace++: A unified framework for free-style text-to-face synthesis and manipulation. <em>TPAMI</em>, <em>47</em>(11), 9438-9453. (<a href='https://doi.org/10.1109/TPAMI.2023.3345866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human faces contain rich semantic information that could hardly be described without a large vocabulary and complex sentence patterns. However, most existing text-to-image synthesis methods could only generate meaningful results based on limited sentence templates with words contained in the training set, which heavily impairs the generalization ability of these models. In this paper, we define a novel ‘free-style’ text-to-face generation and manipulation problem, and propose an effective solution, named AnyFace++, which is applicable to a much wider range of open-world scenarios. The CLIP model is involved in AnyFace++ for learning an aligned language-vision feature space, which also expands the range of acceptable vocabulary as it is trained on a large-scale dataset. To further improve the granularity of semantic alignment between text and images, a memory module is incorporated to convert the description with arbitrary length, format, and modality into regularized latent embeddings representing discriminative attributes of the target face. Moreover, the diversity and semantic consistency of generation results are improved by a novel semi-supervised training scheme and a series of newly proposed objective functions. Compared to state-of-the-art methods, AnyFace++ is capable of synthesizing and manipulating face images based on more flexible descriptions and producing realistic images with higher diversity.},
  archive      = {J_TPAMI},
  author       = {Jianxin Sun and Qiyao Deng and Qi Li and Muyi Sun and Yunfan Liu and Zhenan Sun},
  doi          = {10.1109/TPAMI.2023.3345866},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9438-9453},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AnyFace++: A unified framework for free-style text-to-face synthesis and manipulation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ref-NeRF: Structured view-dependent appearance for neural radiance fields. <em>TPAMI</em>, <em>47</em>(11), 9426-9437. (<a href='https://doi.org/10.1109/TPAMI.2024.3360018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.},
  archive      = {J_TPAMI},
  author       = {Dor Verbin and Peter Hedman and Ben Mildenhall and Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},
  doi          = {10.1109/TPAMI.2024.3360018},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9426-9437},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Ref-NeRF: Structured view-dependent appearance for neural radiance fields},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPro-PnP: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation. <em>TPAMI</em>, <em>47</em>(11), 9413-9425. (<a href='https://doi.org/10.1109/TPAMI.2024.3354997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locating 3D objects from a single RGB image via Perspective-n-Point (PnP) is a long-standing problem in computer vision. Driven by end-to-end deep learning, recent studies suggest interpreting PnP as a differentiable layer, allowing for partial learning of 2D-3D point correspondences by backpropagating the gradients of pose loss. Yet, learning the entire correspondences from scratch is highly challenging, particularly for ambiguous pose solutions, where the globally optimal pose is theoretically non-differentiable w.r.t. the points. In this paper, we propose the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation, which outputs a distribution of pose with differentiable probability density on the SE(3) manifold. The 2D-3D coordinates and corresponding weights are treated as intermediate variables learned by minimizing the KL divergence between the predicted and target pose distribution. The underlying principle generalizes previous approaches, and resembles the attention mechanism. EPro-PnP can enhance existing correspondence networks, closing the gap between PnP-based method and the task-specific leaders on the LineMOD 6DoF pose estimation benchmark. Furthermore, EPro-PnP helps to explore new possibilities of network design, as we demonstrate a novel deformable correspondence network with the state-of-the-art pose accuracy on the nuScenes 3D object detection benchmark.},
  archive      = {J_TPAMI},
  author       = {Hansheng Chen and Wei Tian and Pichao Wang and Fan Wang and Lu Xiong and Hao Li},
  doi          = {10.1109/TPAMI.2024.3354997},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9413-9425},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EPro-PnP: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-shutter optical vibration sensing. <em>TPAMI</em>, <em>47</em>(11), 9402-9412. (<a href='https://doi.org/10.1109/TPAMI.2023.3344650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual vibrometry is a highly useful tool for remote capture of audio, as well as the physical properties of materials, human heart rate, and more. While visually-observable vibrations can be captured directly with a high-speed camera, minute imperceptible object vibrations can be optically amplified by imaging the displacement of a speckle pattern created by shining a laser beam on the vibrating surface. In this paper, we propose a novel method for sensing vibrations at high speeds (up to 63 kHz), for multiple scene sources at once, using sensors rated for only 130 Hz operation. Our method relies on simultaneously capturing the scene with two cameras equipped with rolling and global shutter sensors, respectively. The rolling shutter camera captures distorted speckle images that encode the high-speed object vibrations. The global shutter camera captures undistorted reference images of the speckle pattern, helping to decode the source vibrations. We demonstrate our method by capturing vibration caused by audio sources (e.g., speakers, human voice, and musical instruments) and analyzing the vibration modes of a tuning fork.},
  archive      = {J_TPAMI},
  author       = {Mark Sheinin and Dorian Chan and Matthew O'Toole and Srinivasa G. Narasimhan},
  doi          = {10.1109/TPAMI.2023.3344650},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9402-9412},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual-shutter optical vibration sensing},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to solve hard minimal problems. <em>TPAMI</em>, <em>47</em>(11), 9386-9401. (<a href='https://doi.org/10.1109/TPAMI.2023.3307898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach to solving hard geometric optimization problems in the RANSAC framework. The hard minimal problems arise from relaxing the original geometric optimization problem into a minimal problem with many spurious solutions. Our approach avoids computing large numbers of spurious solutions. We design a learning strategy for selecting a starting problem-solution pair that can be numerically continued to the problem and the solution of interest. We demonstrate our approach by developing a RANSAC solver for the problem of computing the relative pose of three calibrated cameras, via a minimal relaxation using four points in each view. On average, we can solve a single problem in under 70$\mu s.$ We also benchmark and study our engineering choices on the very familiar problem of computing the relative pose of two calibrated cameras, via the minimal case of five points in two views.},
  archive      = {J_TPAMI},
  author       = {Petr Hruby and Timothy Duff and Anton Leykin and Tomas Pajdla},
  doi          = {10.1109/TPAMI.2023.3307898},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9386-9401},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning to solve hard minimal problems},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial: Introduction to the special section on best of CVPR'2022. <em>TPAMI</em>, <em>47</em>(11), 9383-9385. (<a href='https://doi.org/10.1109/TPAMI.2024.3388289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TPAMI},
  author       = {Kristin J. Dana and Gang Hua and Stefan Roth and Dimitris Samaras and Richa Singh},
  doi          = {10.1109/TPAMI.2024.3388289},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {11},
  number       = {11},
  pages        = {9383-9385},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Editorial: Introduction to the special section on best of CVPR'2022},
  volume       = {47},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
